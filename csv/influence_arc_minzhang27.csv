1999.mtsummit-1.54,C94-1020,1,0.828788,"Missing"
1999.mtsummit-1.54,1995.tmi-1.27,0,0.089845,"Missing"
1999.mtsummit-1.54,J93-2003,0,\N,Missing
1999.mtsummit-1.54,C96-2185,1,\N,Missing
1999.tmi-1.23,C96-1030,0,\N,Missing
1999.tmi-1.23,J93-2003,0,\N,Missing
1999.tmi-1.23,C94-1020,1,\N,Missing
1999.tmi-1.23,1995.tmi-1.21,0,\N,Missing
2005.mtsummit-papers.32,J93-2003,0,0.00645684,"pair but using phrases as atomic units rather than words. Words are thus phrases of length one. Therefore, we arrive at the following notation h˜ e, ˜fi(0) for he, fi and h˜ e, ˜fi(N ) for h˜ e, ˜fi. The superscript in the notation denotes level of detail with 0 for the finest and N for the coarsest. Let h˜ e, ˜fi(n) be a sentence pair at an intermediate level between (n) (n) (n) (n) ˜(n) = {˜ 0 and N and e e0 , e˜1 , ...˜ ei , ...˜ el 0 } (n) (n) (n) (n) (n) and ˜f = {f˜ , f˜ , ...f˜ , ...f˜ 0 } be its tu0 (n) e˜0 1 j m (n) ples with and f˜0 represent the special token NULL as suggested in (Brown et al., 1993) and l(n) , m(n) represent the length of the corresponding sentence. Let T (n) be a set of alignments defined over the sentence pair with (n) (n) (n) t˜ij =[˜ ei , f˜j ] as its member. As in our previous work, LOD algorithm transforms h˜ e, ˜fi(0) to h˜ e, ˜fi(N ) iteratively. In every iteration, LOD performs a series of steps, similar to the pipeline followed by many other algorithms, to learn phrase translation at one level more coarse. In the generation step, LOD algorithm forms B(n) , a pool of sub-phrase alignments, as the basis for the generation of phrase alignment candidate. LOD genera"
2005.mtsummit-papers.32,P89-1010,0,0.00810421,"of only two alignments at a time and hands over the candidate generation of more coarse alignment to the subsequent iteration. By considering only two alignments, the LOD model opens the opportunity for noncontiguous phrase translation without the disadvantage of combinatorial explosion in the number of candidates. 3.3 Estimation of Phrase Alignment Candidate Probability Joining the alignment set B(n) derived in Section 3.1 and the coarser level alignment C (n) derived in Section 3.2, we form a candidate alignment set B (n) ∪ C (n) . LOD utilizes information theoretic measure as suggested by (Church and Hanks, 1989) to assess the candidacy of each entry in C (n) . Assuming that there are two alignments x ∈ B (n) ,y ∈ B (n) and a candidate alignment hx, yi ∈ C (n) , we derive the probability p(x) and p(y) from the statistics as the count of x and y normalized by the number of sentence pairs in the corpus, and derive probability p(hx, yi) in a similar way. If there is a genuine association between x and y, then we expect the joint probability p(hx, yi) À p(x)p(y). If there is no interesting relationship between x and y, then p(hx, yi) ≈ p(x)p(y) where we say that x and y are independent. If x and y are in"
2005.mtsummit-papers.32,W03-0314,0,0.01645,"cant. In SMT, phrases are learned without distinguishing non-compositional from compositional ones. Despite this problem, SMT has witnessed great benefits from learning such phrasal units. 243 ploy evidence found in parallel corpora. This is not a new idea, as there is a vast amount of literature that directly addresses learning phrase translation from parallel texts. The identification of meaningful phrase translation includes the learning of the translation of noncompositional compounds (Melamed, 1997), “captoids” and name entities (Moore, 2003) and both gapped and rigid multiword sequence (Kaoru et al., 2003) just to name a few. Specifically for phrase-based SMT, there are many approaches proposed for learning phrase translation, such as the joint model (Marcu and Wong, 2002), ISA (Zhang et al., 2003), alignment templates (Och et al., 1999), HMM paths (Vogel et al., 1996) and projection extensions (Tillmann, 2003). In our previous work (Setiawan et al., 2005), we introduced an agglomerative approach to learn phrase translation for SMT. While the LOD approach works well, a weakness is that it is quite slow, as it suffers from computational inefficiency when calculating translation candidates. In th"
2005.mtsummit-papers.32,N03-1017,0,0.022802,"um phrase length can also be introduced to limit the generation step. This limit retains most of the algorithm’s performance since the count of long phrases decreases gradually. • Scoring. This step calculates a significance score that reflects the interestingness of the candidate for each entry in the candidate pool. There have been numerous metrics used to score candidates; specific examples include mixtures of alignment map, word-based lexicon and language-specific measures (Venugopal et al., 2004), block frequency (Tillmann, 2003), relative frequency (Och et al., 1999), lexical weighting (Koehn et al., 2003) and a set of features 244 reflecting word penalty, length penalty and lexicon score (Zens and Ney, 2004). The scoring function must accommodate phrases of varying length and allow direct comparison between them. Many methods employ a normalization process over the phrase length to enable the comparison, but such normalization may not reflect the actual distribution of the phrase. • Selection. This final step selects the most probable candidates as phrase translations. The algorithm typically explores all candidates and decides their promotion based on their scores. Rank-based methods, using m"
2005.mtsummit-papers.32,W04-3250,0,0.0110362,"f 1,055,167 words and 20,138 unique words. Our experiment is conducted on English-to-French tasks on open testing set-up. We use GIZA++1 as the implementation for the word-based IBM 4 model training and ISI ReWrite2 to translate sentences in testing set. Translation performance is reported as BLEU scores, with appropriate confidence intervals computed. 4.1 Performance versus Original LOD Figure 1 shows the performance of the LOD approach using the simplified algorithm in each iteration and juxtaposes it with that of original algorithm in the English-to-French task. We apply a paired t − test (Koehn, 2004) to examine whether the performance difference is statistically significant. The result of our experiments shows that the performance of our simplified algorithm is comparable and slightly above that of original algorithm in some iterations, although the paired t − test shows that the performance difference is not statistically significant. Initially, we expect to see a reduction in translation performance by simplifying the algorithm since the algorithm operates on a smaller set of alignments sacrificing the recall in generating translation candidates. The results suggest that the simplified"
2005.mtsummit-papers.32,W02-1018,0,0.0120744,"such phrasal units. 243 ploy evidence found in parallel corpora. This is not a new idea, as there is a vast amount of literature that directly addresses learning phrase translation from parallel texts. The identification of meaningful phrase translation includes the learning of the translation of noncompositional compounds (Melamed, 1997), “captoids” and name entities (Moore, 2003) and both gapped and rigid multiword sequence (Kaoru et al., 2003) just to name a few. Specifically for phrase-based SMT, there are many approaches proposed for learning phrase translation, such as the joint model (Marcu and Wong, 2002), ISA (Zhang et al., 2003), alignment templates (Och et al., 1999), HMM paths (Vogel et al., 1996) and projection extensions (Tillmann, 2003). In our previous work (Setiawan et al., 2005), we introduced an agglomerative approach to learn phrase translation for SMT. While the LOD approach works well, a weakness is that it is quite slow, as it suffers from computational inefficiency when calculating translation candidates. In this paper, we modify the original LOD approach by simplifying the learning process and using a simpler translation model while maintaining a comparable level of performanc"
2005.mtsummit-papers.32,W97-0311,0,0.0304183,"notes a statistically significant grouping of words rather than a grouping that is linguistically significant. In SMT, phrases are learned without distinguishing non-compositional from compositional ones. Despite this problem, SMT has witnessed great benefits from learning such phrasal units. 243 ploy evidence found in parallel corpora. This is not a new idea, as there is a vast amount of literature that directly addresses learning phrase translation from parallel texts. The identification of meaningful phrase translation includes the learning of the translation of noncompositional compounds (Melamed, 1997), “captoids” and name entities (Moore, 2003) and both gapped and rigid multiword sequence (Kaoru et al., 2003) just to name a few. Specifically for phrase-based SMT, there are many approaches proposed for learning phrase translation, such as the joint model (Marcu and Wong, 2002), ISA (Zhang et al., 2003), alignment templates (Och et al., 1999), HMM paths (Vogel et al., 1996) and projection extensions (Tillmann, 2003). In our previous work (Setiawan et al., 2005), we introduced an agglomerative approach to learn phrase translation for SMT. While the LOD approach works well, a weakness is that"
2005.mtsummit-papers.32,E03-1035,0,0.0192932,"words rather than a grouping that is linguistically significant. In SMT, phrases are learned without distinguishing non-compositional from compositional ones. Despite this problem, SMT has witnessed great benefits from learning such phrasal units. 243 ploy evidence found in parallel corpora. This is not a new idea, as there is a vast amount of literature that directly addresses learning phrase translation from parallel texts. The identification of meaningful phrase translation includes the learning of the translation of noncompositional compounds (Melamed, 1997), “captoids” and name entities (Moore, 2003) and both gapped and rigid multiword sequence (Kaoru et al., 2003) just to name a few. Specifically for phrase-based SMT, there are many approaches proposed for learning phrase translation, such as the joint model (Marcu and Wong, 2002), ISA (Zhang et al., 2003), alignment templates (Och et al., 1999), HMM paths (Vogel et al., 1996) and projection extensions (Tillmann, 2003). In our previous work (Setiawan et al., 2005), we introduced an agglomerative approach to learn phrase translation for SMT. While the LOD approach works well, a weakness is that it is quite slow, as it suffers from computa"
2005.mtsummit-papers.32,W99-0604,0,0.258622,"is not a new idea, as there is a vast amount of literature that directly addresses learning phrase translation from parallel texts. The identification of meaningful phrase translation includes the learning of the translation of noncompositional compounds (Melamed, 1997), “captoids” and name entities (Moore, 2003) and both gapped and rigid multiword sequence (Kaoru et al., 2003) just to name a few. Specifically for phrase-based SMT, there are many approaches proposed for learning phrase translation, such as the joint model (Marcu and Wong, 2002), ISA (Zhang et al., 2003), alignment templates (Och et al., 1999), HMM paths (Vogel et al., 1996) and projection extensions (Tillmann, 2003). In our previous work (Setiawan et al., 2005), we introduced an agglomerative approach to learn phrase translation for SMT. While the LOD approach works well, a weakness is that it is quite slow, as it suffers from computational inefficiency when calculating translation candidates. In this paper, we modify the original LOD approach by simplifying the learning process and using a simpler translation model while maintaining a comparable level of performance. In the remainder of this paper, we will describe our simplified"
2005.mtsummit-papers.32,I05-1051,1,0.527089,"Singapore 119613 {stuhs,hli,mzhang}@i2r.a-star.edu.sg 2 School of Computing National University of Singapore Singapore 117543 hendrase@comp.nus.edu.sg Abstract One method to address this problem is to emWe propose a simplified Level Of Detail (LOD) algorithm to learn phrase translation for statistical machine translation. In particular, LOD learns unknown phrase translations from parallel texts without linguistic knowledge. LOD uses an agglomerative method to attack the combinatorial explosion that results when generating candidate phrase translations. Although LOD was previously proposed by (Setiawan et al., 2005), we improve the original algorithm in two ways: simplifying the algorithm and using a simpler translation model. Experimental results show that our algorithm provides comparable performance while demonstrating a significant reduction in computation time. 1 Introduction Many natural language processing applications, such as machine translation, treat words as the primitive unit of processing. These units are often treated as a set, discarding ordering information, and reducing an utterance as a bag of words. However, natural language often exhibits a non-compositional property where an utteran"
2005.mtsummit-papers.32,W03-1001,0,0.0536808,"resses learning phrase translation from parallel texts. The identification of meaningful phrase translation includes the learning of the translation of noncompositional compounds (Melamed, 1997), “captoids” and name entities (Moore, 2003) and both gapped and rigid multiword sequence (Kaoru et al., 2003) just to name a few. Specifically for phrase-based SMT, there are many approaches proposed for learning phrase translation, such as the joint model (Marcu and Wong, 2002), ISA (Zhang et al., 2003), alignment templates (Och et al., 1999), HMM paths (Vogel et al., 1996) and projection extensions (Tillmann, 2003). In our previous work (Setiawan et al., 2005), we introduced an agglomerative approach to learn phrase translation for SMT. While the LOD approach works well, a weakness is that it is quite slow, as it suffers from computational inefficiency when calculating translation candidates. In this paper, we modify the original LOD approach by simplifying the learning process and using a simpler translation model while maintaining a comparable level of performance. In the remainder of this paper, we will describe our simplified Level of Detail (LOD) algorithm. Section 2 discusses related work, includi"
2005.mtsummit-papers.32,C96-2141,0,0.0747395,"s a vast amount of literature that directly addresses learning phrase translation from parallel texts. The identification of meaningful phrase translation includes the learning of the translation of noncompositional compounds (Melamed, 1997), “captoids” and name entities (Moore, 2003) and both gapped and rigid multiword sequence (Kaoru et al., 2003) just to name a few. Specifically for phrase-based SMT, there are many approaches proposed for learning phrase translation, such as the joint model (Marcu and Wong, 2002), ISA (Zhang et al., 2003), alignment templates (Och et al., 1999), HMM paths (Vogel et al., 1996) and projection extensions (Tillmann, 2003). In our previous work (Setiawan et al., 2005), we introduced an agglomerative approach to learn phrase translation for SMT. While the LOD approach works well, a weakness is that it is quite slow, as it suffers from computational inefficiency when calculating translation candidates. In this paper, we modify the original LOD approach by simplifying the learning process and using a simpler translation model while maintaining a comparable level of performance. In the remainder of this paper, we will describe our simplified Level of Detail (LOD) algorithm"
2005.mtsummit-papers.32,N04-1033,0,0.0137779,"rithm’s performance since the count of long phrases decreases gradually. • Scoring. This step calculates a significance score that reflects the interestingness of the candidate for each entry in the candidate pool. There have been numerous metrics used to score candidates; specific examples include mixtures of alignment map, word-based lexicon and language-specific measures (Venugopal et al., 2004), block frequency (Tillmann, 2003), relative frequency (Och et al., 1999), lexical weighting (Koehn et al., 2003) and a set of features 244 reflecting word penalty, length penalty and lexicon score (Zens and Ney, 2004). The scoring function must accommodate phrases of varying length and allow direct comparison between them. Many methods employ a normalization process over the phrase length to enable the comparison, but such normalization may not reflect the actual distribution of the phrase. • Selection. This final step selects the most probable candidates as phrase translations. The algorithm typically explores all candidates and decides their promotion based on their scores. Rank-based methods, using maximal separation criteria (Venugopal et al., 2004) and frequency filtering (Tillmann, 2003) are common m"
2007.iwslt-1.8,N03-1017,0,0.00869946,"Missing"
2007.iwslt-1.8,W05-0835,0,0.0370063,"Missing"
2007.iwslt-1.8,P07-2045,0,0.0176492,"Missing"
2007.iwslt-1.8,P02-1038,0,0.254499,"Missing"
2007.iwslt-1.8,J93-2003,0,0.0108048,"Missing"
2007.iwslt-1.8,P03-1021,0,0.0319677,"Missing"
2007.iwslt-1.8,J03-1002,0,0.00866112,"Missing"
2007.iwslt-1.8,2007.mtsummit-papers.71,1,0.82372,"Missing"
2007.iwslt-1.8,2006.iwslt-papers.4,1,0.902165,"Missing"
2007.iwslt-1.8,W06-3110,0,0.310267,"Missing"
2007.iwslt-1.8,takezawa-etal-2002-toward,0,0.150368,"Missing"
2007.iwslt-1.8,W03-1730,0,0.0270664,"Missing"
2007.iwslt-1.8,2006.iwslt-evaluation.7,1,\N,Missing
2007.iwslt-1.8,J96-1002,0,\N,Missing
2007.iwslt-1.8,2005.iwslt-1.11,1,\N,Missing
2007.mtsummit-papers.71,J93-2003,0,0.0134483,"etter than Pharaoh, a state-of-the-art phrase-based SMT system, and other syntax-based methods, such as the synchronous CFG-based method on the small dataset. Keywords: statistical machine translation, syntax-based statistical machine translation, tree-to-tree alignment, synchronous tree-substitution grammar, elementary tree Motivation Phrase-based SMT Phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) to statistical machine translation (SMT) has recently achieved significant improvements in translation accuracy over the original IBM word-alignment-based model (Brown et al., 1993). In phrase-based models, a phrase can be any string of adjacent words without constraints imposed by any syntactic theory. These phrases allow a model to learn local reorderings, translations of multiword expressions, or insertions and deletions that are sensitive to local context. These make it a simple and powerful mechanism for machine translation. However, there exist many open issues to be resolved in phrase-based models. For examples, the handling of discontiguous phrases and modeling of global reordering, estimation of phrase translation probabilities and phrase partition probabilities"
2007.mtsummit-papers.71,W06-1628,0,0.0735041,"Missing"
2007.mtsummit-papers.71,P05-1067,0,0.215492,"Missing"
2007.mtsummit-papers.71,W04-3250,0,0.47186,"02). Hence, all knowledge sources, including source and target string and all hidden variables and any additional knowledge source, such as language model or additional dictionaries, are described as feature functions. In our implementation, we further simplify our model as follows: e1I (7) , PET1K Eq. (5) is the simplified model. Eq. (6) formalizes the modeling process based on log-linear framework. Eq. (7) formulizes the decoding, i.e., the translation process. Finally, for our experiments we use the following seven feature functions that are analogous to the default feature set of Pharaoh (Koehn, 2004a). 1) Bidirectional elementary tree mapping probability: φ (e |f ) = log ∏ k =1 K φ ( f |e) = log ∏ k =1 K N (ξ ek , ξ fk ) N (ξ fk ) N (ξ fk , ξ ek ) N (ξ ek ) 2) Bidirectional elementary tree lexical translation probability: lex ( f |e) and lex(e |f ) . Here, we only consider terminal translation probability and set the non-terminal translation probability to 1. 3) Language model (lm): log ∏ I i =1 p (ei |ei − 2 , ei −1 ) . 4) Number of elementary tree pairs used (pp): K. 5) Number of target words (wp): I. Rule Extraction Rules or PETs are extracted from word-aligned, bi-parsed sentence pai"
2007.mtsummit-papers.71,W04-3312,0,0.0310476,"rs while Huang et al (2006) and Liu et al (2006) work on tree-tostring alignment models. Our method, in terms of modeling, training and decoding algorithms are different from theirs at one or more points. In the rest of this paper, we elaborate our modeling, training and decoding methods and report our experimental results in detail. Tree-to-Tree Alignment-based Model In this section, we first introduce what STSG is and then based on which we define our tree-to-tree alignmentbased SMT model. Finally, we present the modeling process based on log-linear framework. Synchronous TSG (STSG) for SMT Shieber (2004) gives a formal and general definition of STSG. Here we give a more concrete definition of STSG with respect to its application in SMT. A STSG is a septet G =< Σ s, Σ t , Ns, Nt , Ss, St , P > , where: • Σ s and Σ t are source and target terminal alphabets (POSs or lexical words), respectively, and • Ns and Nt are source and target non-terminal alphabets (linguistic phrase tag, i.e., NP/VP…), respectively, and examples of elementary trees which belong to the English parse tree Tt shown in Figure 1. Obviously, a normal subtree (whose leaf nodes must be terminal symbols) is an elementary tree bu"
2007.mtsummit-papers.71,P01-1067,0,0.363922,"n probabilities and phrase partition probabilities are not yet effectively addressed in phrase-based models (Quirk and Menezes, 2006). Much research has been carried out to look into the above issues. One natural extension is to utilize syntax-based structure features for SMT. Syntax-based SMT Recent work in SMT has evolved from the word-based and phrase-based models to syntax-based models, that include hierarchical phrase models (Wu, 1997; Chiang, 2007), bilingual synchronous grammars (Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006;) and other syntax-based models (Yamada and Knight, 2001; Gildea, 2003; Och et al, 2004b; Liu et al., 2006). Wu (1997) and Chiang (2007)’s methods are formally syntax-based, i.e., their methods are not informed by any linguistically syntactic theory. Wu (1997) proposes Inversion Transduction Grammars (ITGs, an instance of synchronous CFGs), treating translation as a process of parallel parsing of the source and target languages via ITGs. Chiang (2007) uses a formal binary synchronous CFG to model hierarchical phrase structures. Yamada and Knight (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Och et al (2004)"
2008.iwslt-evaluation.17,J03-1002,0,0.00397375,"I1 maximizing a loglinear combination of several feature models [4]: - 116 - ( eˆI1 = arg max eI1 M X ) λm hm (eI1 , f1J ) m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. Proceedings of IWSLT 2008, Hawaii - U.S.A. The N gram-based approach regards translation as a stochastic process maximizing the joint probability p(f, e), leading to a decomposition based on bilingual n-grams, socalled tuples, that are extracted from a word-to-word alignment (performed with GIZA++ tool1 and generated by growdiag-final method [5]). Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [6]: languages capturing bilingual context, as described by the following equation: p(S, T ) = K Y p((˜ s, t˜)k |(˜ s, t˜)k−N +1 , ..., (˜ s, t˜)k−1 ) (1) k=1 where s refers to source, t to target, and (˜ s, t˜)k to the k tuple of a given bilingual sentence pair segmented in K tuples. The bilingual TM actually constitutes an n-gram-based language model (LM) of tuples, which approximates the joint probability between the languages under consideration and can be seen here as a LM, where t"
2008.iwslt-evaluation.17,N04-1033,0,0.0316825,"I1 , f1J ) m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. Proceedings of IWSLT 2008, Hawaii - U.S.A. The N gram-based approach regards translation as a stochastic process maximizing the joint probability p(f, e), leading to a decomposition based on bilingual n-grams, socalled tuples, that are extracted from a word-to-word alignment (performed with GIZA++ tool1 and generated by growdiag-final method [5]). Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [6]: languages capturing bilingual context, as described by the following equation: p(S, T ) = K Y p((˜ s, t˜)k |(˜ s, t˜)k−N +1 , ..., (˜ s, t˜)k−1 ) (1) k=1 where s refers to source, t to target, and (˜ s, t˜)k to the k tuple of a given bilingual sentence pair segmented in K tuples. The bilingual TM actually constitutes an n-gram-based language model (LM) of tuples, which approximates the joint probability between the languages under consideration and can be seen here as a LM, where the language is composed by tuples. th • a monotonic segmentation of each bilingual sentence pair is produced • n"
2008.iwslt-evaluation.17,W06-1609,1,0.710642,"2008, Hawaii - U.S.A. 2.5. Statistical Machine Reordering 3.1. Punctuation restoration The conception of the Statistical Machine Reordering (SMR) stems from the idea of using the powerful techniques developed for SMT and to translate the source language (S) into a reordered source language (S’), which more closely matches the order of the target language. To infer more reorderings, it makes use of word classes. To correctly integrate the SMT and SMR systems, both are concatenated by using a word graph which offers weighted reordering hypotheses to the SMT system. The details are described in [8] and [9]. We decided to embed punctuation restoration in the main translation step. For this purpose we preprocessed the training corpus as follows: 2.6. Translation models interpolation The resulting preprocessed training corpus is used to train a standard SMT system (wi stands for the i-th word). During the post-evaluation period we have implemented a TM interpolation strategy following the ideas proposed in [3], where the authors present a promising technique of target LMs linear interpolation. These findings open the way to involve additional monolingual information into the translation pr"
2008.iwslt-evaluation.17,W07-0721,1,0.843062,"awaii - U.S.A. 2.5. Statistical Machine Reordering 3.1. Punctuation restoration The conception of the Statistical Machine Reordering (SMR) stems from the idea of using the powerful techniques developed for SMT and to translate the source language (S) into a reordered source language (S’), which more closely matches the order of the target language. To infer more reorderings, it makes use of word classes. To correctly integrate the SMT and SMR systems, both are concatenated by using a word graph which offers weighted reordering hypotheses to the SMT system. The details are described in [8] and [9]. We decided to embed punctuation restoration in the main translation step. For this purpose we preprocessed the training corpus as follows: 2.6. Translation models interpolation The resulting preprocessed training corpus is used to train a standard SMT system (wi stands for the i-th word). During the post-evaluation period we have implemented a TM interpolation strategy following the ideas proposed in [3], where the authors present a promising technique of target LMs linear interpolation. These findings open the way to involve additional monolingual information into the translation process, a"
2008.iwslt-evaluation.17,2007.mtsummit-papers.29,0,0.0263997,"iod we have implemented a TM interpolation strategy following the ideas proposed in [3], where the authors present a promising technique of target LMs linear interpolation. These findings open the way to involve additional monolingual information into the translation process, and also gives a motivation to interpolate the translation and reordering tables in a linear way. Due to a small amount of available in-domain data (IWSLT training material), we have used an out-of-domain 130K-line subset from the Arabic News, English Translation of Arabic Treebank and Ummah LDC parallel corpora (VIOLIN) [10] to increase the final translation and reordering tables. Both corpus statistics can be found in table 1. Instead of time-consuming iterative TM reconstruction and using the highest BLEU score as an maximization criteria, we adjust the weights as a function of the lowest perplexity estimated by the corresponding interpolated combination of the target-side LMs and generalize the optimization results on the interpolated translation and reordering models. The word-to-word alignment was obtained from the joint database (IWSLT + VIOLIN). Then, we separately computed the translation and reordering t"
2008.iwslt-evaluation.17,P07-2045,0,0.0159161,"ssion Corpus (BTEC) Arabic to English translation task. The model weights were tuned with the 2006 development corpus (Dev6), containing 489 sentences and 6 reference translations and the 2002 development set (500 sentences and 16 reference translations) was used as an internal test, according to which we take a decision about better or worse system performance. 4.1.1. Arabic data preprocessing In this section we present a phrase-based MT system that was used in the evaluation. This system is based on the well-known MOSES2 toolkit, which is nowadays considered as a state-of-the-art SMT system [11]. The training and weights tuning procedures are explained in details in the above-mentioned publication, as well as, on the MOSES web page: http://www.statmt.org/moses/. 2 www.statmt.org/moses/ We used a similar approach to that shown in [12], namely the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of enclitics: w+, f+, b+, k+, l+, Al+ and pronominal enclitics. The -TAGBIES option produces Bies POS tags on all taggable t"
2008.iwslt-evaluation.17,N06-2013,0,0.0588409,"anslations) was used as an internal test, according to which we take a decision about better or worse system performance. 4.1.1. Arabic data preprocessing In this section we present a phrase-based MT system that was used in the evaluation. This system is based on the well-known MOSES2 toolkit, which is nowadays considered as a state-of-the-art SMT system [11]. The training and weights tuning procedures are explained in details in the above-mentioned publication, as well as, on the MOSES web page: http://www.statmt.org/moses/. 2 www.statmt.org/moses/ We used a similar approach to that shown in [12], namely the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of enclitics: w+, f+, b+, k+, l+, Al+ and pronominal enclitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 3 http://www.slc.atr.jp/IWSLT2008/ - 118 - Proceedings of IWSLT 2008, Hawaii - U.S.A. Sentences Words Average sentence length Vocabulary IWSLT Arabic 24.45 K 170.24 K 6.96 10.89 K English 24.45 K 188.54 K 7.71 6.92 K VIOLIN Arabic 130.5"
2008.iwslt-evaluation.17,2005.iwslt-1.8,0,0.0320169,"n”) outperforms BTEC-only system by 1.8 BLEU points and 1.2 METEOR points for the CRR track and by 2.1 BLEU points and by about 1 METEOR points for the ASR track measured on the official evaluation test set. ”Supplied 2” line stands for the results obtained with the TALPtuples system as described in sub-section 4.1.3. - 119 - • TM(s), direct and inverse phrase/word based TM. • Distortion model, which assigns a cost linear to the reordering distance, while the cost is based on the number of source words which are skipped when translating a new source phrase. • Lexicalized word reordering model [15]. • Word and phrase penalties, which count the number of words and phrases in the target string. • Target-side LM. The TM and reordering model were trained using the standard MOSES tools. Weights of feature functions were tuned by using the optimization tools from the MOSES package. The search operation was accomplished by MOSES decoder. The experiments with the Chinese to English MT were carried out on the BTEC Chinese-English data [16] augmented with HIT-corpus4 , Olympic-corpus5 and PKUcorpus6 from Chinese LDC. 20K BTEC sentence pairs were supplied for the IWSLT 2008 evaluation campaign. HI"
2008.iwslt-evaluation.17,takezawa-etal-2002-toward,0,0.0149266,"ng distance, while the cost is based on the number of source words which are skipped when translating a new source phrase. • Lexicalized word reordering model [15]. • Word and phrase penalties, which count the number of words and phrases in the target string. • Target-side LM. The TM and reordering model were trained using the standard MOSES tools. Weights of feature functions were tuned by using the optimization tools from the MOSES package. The search operation was accomplished by MOSES decoder. The experiments with the Chinese to English MT were carried out on the BTEC Chinese-English data [16] augmented with HIT-corpus4 , Olympic-corpus5 and PKUcorpus6 from Chinese LDC. 20K BTEC sentence pairs were supplied for the IWSLT 2008 evaluation campaign. HIT corpus contains 132K sentence pairs in total, and is known as a multi-source ChineseEnglish parallel corpus; Olympic corpus has 54K bilingual sentences mainly from sport and travelling domains; while PKU-corpus has about 200K parallel phrases and is considered as a domain-balanced corpus. Besides, the English part of the Tanaka corpus7 was used as a complementary training 4 http://mitlab.hit.edu.cn/index.php/resources 5 http://www.chin"
2008.iwslt-evaluation.17,W03-1730,0,0.0212439,"OR)/2 0.6016 0.6055 0.6210 0.5892 0.5320 0.5320 0.5473 0.5296 NIST 8.5253 8.5940 8.8772 8.7421 7.2878 7.2808 7.6113 7.5862 Table 2: Official and post-evaluation results for Arabic-English translation. Sentences Words Vocabulary Chinese 19,972 164K 8,506 IWSLT’08 English Spanish 19,972 19,972 182K 147K 8,301 16,953 All additional data Chinese English 379,065 379,065 4,834K 5,036K 57,055 75,156 Table 3: Corpus used during the Chinese-English training material for the target-side LM. The I2R research group performed word segmentation for the Chinese part using ICTCLAS tools8 developed in the ICT [17]. Table 3 reports the basic statistics of the principal and additional corpora that were used to build the Chinese-toEnglish SMT system. Regarding English-to-Spanish translation, no extra corpora were used. “you ’re”, and negations like “don’t”, “wouldn’t” or “can’t” were split as “do n’t”, “would n’t” and “ca n’t”. The output of this system was performed in accordance with the official evaluation specification, without any postprocessing needed. Table 5 shows the results of the EnglishSpanish system trained with the BTEC corpus. 4.2.1. Chinese-English independent results The union of the BTEC"
2008.iwslt-evaluation.17,N04-1022,0,0.0541414,"and “you’re” were split as “we ’ll” and 8 http://www.nlp.org.cn/project/project.php?proj BLEU NIST METEOR id=6 - 120 - Our primary approach to the pivot task was a system cascade. Using the 50-best list of translation hypotheses generated by the decoder for the Chinese-to-English system, a 4-best list was made for each of the first list instances, totally representing a 200-best of possible Spanish translations for each Chinese sentence. From that 200-best list, which is allowed for repetitions, the single-best translation was computed using a Minimum Bayes Risk (MBR) strategy as described in [18]. We used the MOSES implementation of the MBR algorithm. This strategy of 200-best list rescoring performed better than a single-best list selection for both systems, gaining 2.5 BLEU points in the development set. Proceedings of IWSLT 2008, Hawaii - U.S.A. 4.2.4. Secondary submission As an alternative approach to the system cascade, we followed a different strategy for the secondary submission combining the phrase translation probabilities of the two language pairs (Chinese-English and English-Spanish translations) with the strategy proposed in [19] to obtain the translation probabilities for"
2008.iwslt-evaluation.17,P07-1108,0,0.0348074,"m Bayes Risk (MBR) strategy as described in [18]. We used the MOSES implementation of the MBR algorithm. This strategy of 200-best list rescoring performed better than a single-best list selection for both systems, gaining 2.5 BLEU points in the development set. Proceedings of IWSLT 2008, Hawaii - U.S.A. 4.2.4. Secondary submission As an alternative approach to the system cascade, we followed a different strategy for the secondary submission combining the phrase translation probabilities of the two language pairs (Chinese-English and English-Spanish translations) with the strategy proposed in [19] to obtain the translation probabilities for each Chinese-Spanish phrase. The final phrase probabilities are calculated as followed: φ(fi |ei ) = X φ(fi |pi )φ(pi |ei ) (2) pi where φ(fi |ei ) corresponds to the translation probability of the Chinese phrase fi given the Spanish phrase ei , φ(fi |pi ) stands for the translation probability of the Chinese phrase fi given the English phrase pi and φ(pi |ei ) stands for the translation probability of the English phrase pi given the Spanish phrase ei . It is important to mention that the English and Spanish phrases are lowercased in this system and"
2008.iwslt-evaluation.17,carreras-etal-2004-freeling,0,0.029059,"6 sentences with 16 Spanish references for tuning the system. The basic statistics of this corpus can be seen in table 7. 4.3.1. Data preprocessing The Chinese corpus was not preprocessed before translation: the corpus was tokenized by words and the punctuation marks were separated. Note that the TM, as well as the LM and reordering model, was trained with punctuation marks and the official test set that did not contain this information, therefore it was preprocessed with the hidden-ngram tool to restore it. The Spanish part of the corpus was lowercased and tokenized using the Freeling toolkit[20], an open source tool for language analysis. It splitted the enclitics from the Spanish verbs (d´amelo → da +me +lo) and also generated the POS tags that were lately used to estimate a target-side POS LM and in postprocessing. 4.3.2. Data postprocessing Once the decoding process had finished, the output of the system was still lowercased and splitted with the enclitics and the POS tags were generated. Afterwards, a postprocess including two steps was performed: firstly, the original morphological verbs form was restored using the enclitics and POS tags information; on the next step, the case i"
2008.iwslt-evaluation.17,2007.iwslt-1.1,0,\N,Missing
2008.iwslt-evaluation.17,2006.iwslt-evaluation.1,0,\N,Missing
2008.iwslt-evaluation.6,E06-1005,0,0.054695,"system combination methods that we have explored. The performance on development and test sets are reported in detail in the paper. The system has shown competitive performance with respect to the BLEU and METEOR measures in Chinese-English Challenge and BTEC tasks. 1. Introduction This paper describes the machine translation (MT) system and approach explored by the Institute for Infocomm Research (I2R) for the International Workshop on Spoken Language Translation (IWSLT) 2008. We submitted runs under the open data conditions for Chinese-to-English BTEC and Challenge tasks. System combination [1, 2, 3] has demonstrated its advantage in the recent machine translation evaluation campaign [4, 5]. In our system, a multi-pass SMT approach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders"
2008.iwslt-evaluation.6,N07-1029,0,0.0312943,"system combination methods that we have explored. The performance on development and test sets are reported in detail in the paper. The system has shown competitive performance with respect to the BLEU and METEOR measures in Chinese-English Challenge and BTEC tasks. 1. Introduction This paper describes the machine translation (MT) system and approach explored by the Institute for Infocomm Research (I2R) for the International Workshop on Spoken Language Translation (IWSLT) 2008. We submitted runs under the open data conditions for Chinese-to-English BTEC and Challenge tasks. System combination [1, 2, 3] has demonstrated its advantage in the recent machine translation evaluation campaign [4, 5]. In our system, a multi-pass SMT approach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders"
2008.iwslt-evaluation.6,P07-2045,0,0.0105658,"ed its advantage in the recent machine translation evaluation campaign [4, 5]. In our system, a multi-pass SMT approach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders, namely Moses [6] (an open source phrasebased MT system), JosHUa [7] (a hierarchical phrase-based translation system) and Tranyu [8] (an in-house linguistically-annotated BTG-based decoder) to generate 2Nbest lists of hypotheses for each decoder. Then each 2N-best lists are rescored and re-ranked with additional feature functions. The 1-best and top N-best lists of these re-ranked lists are then used in system combination1. Secondly, we construct system combination in two-stage. In the first stage, two strategies are applied. The first strategy is n-gram expansion by which we spawn new translation entries thro"
2008.iwslt-evaluation.6,W08-0402,0,0.0242585,"evaluation campaign [4, 5]. In our system, a multi-pass SMT approach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders, namely Moses [6] (an open source phrasebased MT system), JosHUa [7] (a hierarchical phrase-based translation system) and Tranyu [8] (an in-house linguistically-annotated BTG-based decoder) to generate 2Nbest lists of hypotheses for each decoder. Then each 2N-best lists are rescored and re-ranked with additional feature functions. The 1-best and top N-best lists of these re-ranked lists are then used in system combination1. Secondly, we construct system combination in two-stage. In the first stage, two strategies are applied. The first strategy is n-gram expansion by which we spawn new translation entries through a word-based n-gram language model estimated on"
2008.iwslt-evaluation.6,P06-1066,1,0.866553,"oach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders, namely Moses [6] (an open source phrasebased MT system), JosHUa [7] (a hierarchical phrase-based translation system) and Tranyu [8] (an in-house linguistically-annotated BTG-based decoder) to generate 2Nbest lists of hypotheses for each decoder. Then each 2N-best lists are rescored and re-ranked with additional feature functions. The 1-best and top N-best lists of these re-ranked lists are then used in system combination1. Secondly, we construct system combination in two-stage. In the first stage, two strategies are applied. The first strategy is n-gram expansion by which we spawn new translation entries through a word-based n-gram language model estimated on the input hypotheses. Then input hypotheses and the newly-gener"
2008.iwslt-evaluation.6,J03-1002,0,0.00526627,"Section 3 details the rescoring models. Section 4 describes three system combination strategies: n-gram expansion, simple cascading and weighted voting. Section 5 reports the experimental setups and results while Section 6 concludes the paper. 2. The SMT Models To integrate the advantages of the state-of-the-art translation methods, we use three different SMT models, phrase-based, hierarchical phrase-based and linguistically-annotated BTGbased in the first pass to generate N-best hypotheses. The three methods share the some common features: word alignment of training data obtained from GIZA++ [9], Language model(s) (LM) trained using SRILM toolkit [10] with modified Kneser-Ney smoothing method [11]. 2.1. Phrasal translation system Phrase-based SMT systems are usually modeled through a log-linear framework [12]. By introducing the hidden word alignment variable a [13], the optimal translation can be searched for based on the following criterion: M e * = arg max(∑ m =1 λm hm (e, f , a)) where e is a string of phrases in the target language, the source language string of phrases, feature functions, weights 1 Since our system combination method n-gram expansion [3] is based on a gener"
2008.iwslt-evaluation.6,P02-1038,0,0.0901849,"ion 6 concludes the paper. 2. The SMT Models To integrate the advantages of the state-of-the-art translation methods, we use three different SMT models, phrase-based, hierarchical phrase-based and linguistically-annotated BTGbased in the first pass to generate N-best hypotheses. The three methods share the some common features: word alignment of training data obtained from GIZA++ [9], Language model(s) (LM) trained using SRILM toolkit [10] with modified Kneser-Ney smoothing method [11]. 2.1. Phrasal translation system Phrase-based SMT systems are usually modeled through a log-linear framework [12]. By introducing the hidden word alignment variable a [13], the optimal translation can be searched for based on the following criterion: M e * = arg max(∑ m =1 λm hm (e, f , a)) where e is a string of phrases in the target language, the source language string of phrases, feature functions, weights 1 Since our system combination method n-gram expansion [3] is based on a generative language model that is trained on the input hypotheses lists, the hypotheses quality is very important to the performance of the n-gram expansion method. Therefore, we filter out N-worse hypotheses from the 2N-be"
2008.iwslt-evaluation.6,P03-1021,0,0.044622,"m =1 λm hm (e, f , a)) where e is a string of phrases in the target language, the source language string of phrases, feature functions, weights 1 Since our system combination method n-gram expansion [3] is based on a generative language model that is trained on the input hypotheses lists, the hypotheses quality is very important to the performance of the n-gram expansion method. Therefore, we filter out N-worse hypotheses from the 2N-best lists before passing them to the n-gram expansion model. (1) e ,a λm hm (e, f , a ) f is are are typically optimized to maximize the scoring function [14]. Our phrasal translation system is based on the Moses open source package [6]. IBM word reordering constraints [15] are applied during decoding to reduce the computational - 46 - Proceedings of IWSLT 2008, Hawaii - U.S.A. Figure 1: system architecture complexity. The other models and feature functions employed by Moses decoder are: • Translation model(s) (TM), direct phrase/word based translation model and inverse • Distortion model, which assigns a cost linear to the reordering distance, the cost is based on the number of source words which are skipped when translating a new source phrase •"
2008.iwslt-evaluation.6,2005.iwslt-1.8,0,0.0381389,"s based on the Moses open source package [6]. IBM word reordering constraints [15] are applied during decoding to reduce the computational - 46 - Proceedings of IWSLT 2008, Hawaii - U.S.A. Figure 1: system architecture complexity. The other models and feature functions employed by Moses decoder are: • Translation model(s) (TM), direct phrase/word based translation model and inverse • Distortion model, which assigns a cost linear to the reordering distance, the cost is based on the number of source words which are skipped when translating a new source phrase • Lexicalized word reordering model [16] (RM) • Word and phrase penalties, which count the numbers of words and phrases in the target string The translation model, reordering model and feature weights are trained and optimized using Moses training and tuning toolkits. Two different N-best lists are generated by the same Moses decoder with the same source input but different preprocessing. 2.2. Hierarchical phrase-based translation system Hierarchical phrase-based translation method is a typical formally syntax-based translation modeling method. Empirically, it has demonstrated better performance than the phrase-based method because"
2008.iwslt-evaluation.6,J07-2003,0,0.103377,"ses in the target string The translation model, reordering model and feature weights are trained and optimized using Moses training and tuning toolkits. Two different N-best lists are generated by the same Moses decoder with the same source input but different preprocessing. 2.2. Hierarchical phrase-based translation system Hierarchical phrase-based translation method is a typical formally syntax-based translation modeling method. Empirically, it has demonstrated better performance than the phrase-based method because it permits phrases with gaps by generalizing the normal phrase-based models [17, 7]. Formally, the hierarchical phrase-based translation model is a weighted synchronous context free grammar. In our system combination framework, for the hierarchical phrase-based translation component, we use the default setting as discussed in [17] for training and tuning and use JosHUa [7]’s implementation for decoding. 2.3. Linguistically annotated BTG-based system Tranyu is an in-house formally and linguistically syntaxbased SMT system, which adopts the bracketing transduction grammars (BTG) as the fundamental framework for phrase translation and reordering. The BTG lexical rules (A --&gt; x/"
2008.iwslt-evaluation.6,P08-2038,1,0.868481,"el uses boundary words of neighboring phrases as features [8], which we call the boundary words based reordering model (BWR). The second model uses linguistic annotations of each BTG node as features, which are automatically learned by projecting source-side parse trees onto the corresponding binary trees generated by BTG. We call the second model the linguistically annotated reordering model (LAR). Based on these two reordering models, we developed two variations of Tranyu. The first variation Tranyu1 only uses the BWR model [8] while the second variation Tranyu2 uses both BWR and LAR models [18]. 3. Rescoring models Rescoring operation plays a very important role in our system. A rich global feature functions set benefits our system greatly. The rescoring models are the same ones which were used in our SMT system for IWSLT 2007 [4]. We apply the - 47 - Proceedings of IWSLT 2008, Hawaii - U.S.A. following feature functions. Weights of feature functions are optimized by the MERT tool in Moses package. • direct and inverse IBM model 1 and 3 and two combined systems. The feature weight of each system is tuned over the development set. If all the weights are set to 1, then we call it simp"
2008.iwslt-evaluation.6,2006.iwslt-papers.4,1,0.869059,"system greatly. The rescoring models are the same ones which were used in our SMT system for IWSLT 2007 [4]. We apply the - 47 - Proceedings of IWSLT 2008, Hawaii - U.S.A. following feature functions. Weights of feature functions are optimized by the MERT tool in Moses package. • direct and inverse IBM model 1 and 3 and two combined systems. The feature weight of each system is tuned over the development set. If all the weights are set to 1, then we call it simple voting. • association scores, i.e. hyper-geometric distribution probabilities and mutual information • lexicalized reordering rule [19] • 6-gram target language model and 8-gram target wordclass based LM, word-classes are clustered by GIZA++ • length ratio between source and target sentence • question feature [20] • Linear sum of n-grams (n=1,2,3,4) relative frequencies within all translations [20] • n-gram and sentence length posterior probabilities within the N-best translations [21] 5. Experiments We participated Chinese-to-English BTEC task (BT) and Challenge task (CT) in open data track for IWSLT 2008. 5.1. Preprocessing Preprocessing includes Chinese word segmentation, English tokenization, and transformation of numbers"
2008.iwslt-evaluation.6,J93-2003,0,\N,Missing
2008.iwslt-evaluation.6,C08-1014,1,\N,Missing
2008.iwslt-evaluation.6,2007.iwslt-1.8,1,\N,Missing
2008.iwslt-evaluation.6,P02-1040,0,\N,Missing
2008.iwslt-evaluation.6,W06-3110,0,\N,Missing
2008.iwslt-evaluation.6,takezawa-etal-2002-toward,0,\N,Missing
2008.iwslt-evaluation.6,2005.iwslt-1.11,1,\N,Missing
2008.iwslt-evaluation.6,W03-1730,1,\N,Missing
2009.iwslt-evaluation.7,N07-1029,0,0.0408514,"y optimized to maximize the scoring function [11]. IBM word reordering constraints [13] are applied during decoding to reduce the computational complexity. The other models and feature functions employed by Lavender are: • Translation model(s) (TM), direct and inverse phrase/word based translation model 1. Introduction This paper describes the machine translation (MT) system and approach explored by the Institute for Infocomm Research (I2R) for the International Workshop on Spoken Language Translation (IWSLT) 2009. Basically, our MT system is a system combination framework. System combination [1, 2, 3] has demonstrated its advantage in the recent machine translation evaluation campaign [4, 5]. In our system combination framework, we adopt mainly two kinds of statistical machine translation (SMT) methods: phrase-based SMT and syntax-based SMT. For syntax-based system, we developed three variations. Totally, we applied four SMT systems. Based on outputs of four single systems, we applied rescoring method [4] to incorporate rich global features. Finally, we adopt two kinds of system combination methods, namely, n-gram expansion [3] and weighted voting, on all rescoring outputs. The rest of pap"
2009.iwslt-evaluation.7,J03-1002,0,0.0057466,"Missing"
2009.iwslt-evaluation.7,P02-1038,0,0.373791,"f the different individual systems. Rescoring is applied on each single system output, and system combination is applied on all rescoring outputs. Finally, our system combination framework shows better performance in Chinese-English BTEC task. Lavender [19] is our newly-developed in-house SMT translation platform, including a phrase-based decoder and most of the current linguistically motivated syntax-based system. Its phrase-based component, which functions very similar to Moses [12], is used as the phrase-based decoder for this campaign. Phrase-based SMT usually adopt a log-linear framework [9]. By introducing the hidden word alignment variable a [10], the optimal translation can be searched for based on the following criterion: ~ M e~ * = arg max( λ h (e~, f , a)) e,a ∑m =1 m m ~ where ~ e is a string of phrases in the target language, f is the ~ source language string of phrases, h (e~, f , a) are feature funcm tions, weights λm are typically optimized to maximize the scoring function [11]. IBM word reordering constraints [13] are applied during decoding to reduce the computational complexity. The other models and feature functions employed by Lavender are: • Translation model(s)"
2009.iwslt-evaluation.7,P03-1021,0,0.038379,"-based system. Its phrase-based component, which functions very similar to Moses [12], is used as the phrase-based decoder for this campaign. Phrase-based SMT usually adopt a log-linear framework [9]. By introducing the hidden word alignment variable a [10], the optimal translation can be searched for based on the following criterion: ~ M e~ * = arg max( λ h (e~, f , a)) e,a ∑m =1 m m ~ where ~ e is a string of phrases in the target language, f is the ~ source language string of phrases, h (e~, f , a) are feature funcm tions, weights λm are typically optimized to maximize the scoring function [11]. IBM word reordering constraints [13] are applied during decoding to reduce the computational complexity. The other models and feature functions employed by Lavender are: • Translation model(s) (TM), direct and inverse phrase/word based translation model 1. Introduction This paper describes the machine translation (MT) system and approach explored by the Institute for Infocomm Research (I2R) for the International Workshop on Spoken Language Translation (IWSLT) 2009. Basically, our MT system is a system combination framework. System combination [1, 2, 3] has demonstrated its advantage in the r"
2009.iwslt-evaluation.7,P07-2045,0,0.00954186,"ng method to improve the individual system performance and use system combination method to combine the strengths of the different individual systems. Rescoring is applied on each single system output, and system combination is applied on all rescoring outputs. Finally, our system combination framework shows better performance in Chinese-English BTEC task. Lavender [19] is our newly-developed in-house SMT translation platform, including a phrase-based decoder and most of the current linguistically motivated syntax-based system. Its phrase-based component, which functions very similar to Moses [12], is used as the phrase-based decoder for this campaign. Phrase-based SMT usually adopt a log-linear framework [9]. By introducing the hidden word alignment variable a [10], the optimal translation can be searched for based on the following criterion: ~ M e~ * = arg max( λ h (e~, f , a)) e,a ∑m =1 m m ~ where ~ e is a string of phrases in the target language, f is the ~ source language string of phrases, h (e~, f , a) are feature funcm tions, weights λm are typically optimized to maximize the scoring function [11]. IBM word reordering constraints [13] are applied during decoding to reduce the"
2009.iwslt-evaluation.7,2005.iwslt-1.8,0,0.128635,"] and weighted voting, on all rescoring outputs. The rest of paper is organized as follows. Section 2 presents each individual SMT system used in our framework. Section 3 details the rescoring method. Section 4 describes two system combination strategies: n-gram expansion and weighted voting. Section 5 reports the experimental setups and results while Section 6 concludes the paper. • Distortion model, which assigns a cost linear to the reordering distance, the cost is based on the number of source words which are skipped when translating a new source phrase • Lexicalized word reordering model [14] (RM) • Word and phrase penalties, which count the numbers of words and phrases in the target string The translation model, reordering model and feature weights are trained and optimized using Moses training and tuning toolkits [12]. 2.2. Tranyu: Syntax-based Translation System Tranyu is our another in-house translation platform. It is a formally syntax-based SMT system, which adapts the bracketing transduction grammars (BTG) for phrase translation and reordering. The BTG lexical rules (A --&gt; x/y) are used to translate source phrase x into target phrase y while the BTG merging rules (A --&gt; [A,"
2009.iwslt-evaluation.7,2006.iwslt-papers.4,0,0.0312358,"using boundary words of these examples and finally estimate feature weights. 3. Rescoring Models Rescoring operation plays a very important role in our system. A rich global feature functions set benefits our system greatly. The rescoring models are the same ones which were used in our SMT system for IWSLT 2007 [4]. We apply the following feature functions. Weights of feature functions are optimized by the MERT tool in Moses package. • direct and inverse IBM model 1 and 3 • association scores, i.e. hyper-geometric distribution probabilities and mutual information • lexicalized reordering rule [15] • 6-gram target language model and 8-gram target wordclass based LM, word-classes are clustered by GIZA++ • length ratio between source and target sentence • question feature • Linear sum of n-grams (n=1,2,3,4) relative frequencies within all translations, which favors the hypotheses containing popular n-grams of higher order [16] • Tranyu(LAR). In order to employ more linguistic knowledge in the ITG reordering, we extend boundary word based reordering further by linguistically annotating each node involved in reordering according to the source-side parse tree. We call this linguistically ann"
2009.iwslt-evaluation.7,J93-2003,0,\N,Missing
2009.iwslt-evaluation.7,E06-1005,0,\N,Missing
2009.iwslt-evaluation.7,2007.iwslt-1.8,1,\N,Missing
2009.iwslt-evaluation.7,P09-1036,1,\N,Missing
2009.iwslt-evaluation.7,W06-3110,0,\N,Missing
2009.iwslt-evaluation.7,P06-1066,1,\N,Missing
2009.iwslt-evaluation.7,P08-2038,1,\N,Missing
2009.iwslt-evaluation.7,I08-1066,1,\N,Missing
2009.iwslt-evaluation.7,I05-3025,0,\N,Missing
2009.iwslt-evaluation.7,2005.iwslt-1.11,0,\N,Missing
2009.iwslt-evaluation.7,N06-1014,0,\N,Missing
2009.mtsummit-posters.24,P07-1019,0,0.175412,"cell is p(H) = pin (H) · π(H)λLM where pin (H) is the probability estimated from inside the hypothesis H, λLM is the weight of the language model. Note that this probability is only used for the beam thresholding. 4 Comparison to Previous Work Efficient decoding is of great importance to rapid SMT development and commercial applications. Much of previous work focuses on reducing the overwhelming overhead introduced by the intersection of the m-gram language model and the translation model (phrase-based or syntaxbased). This is the fundamental motivation for cube pruning/growing(Chiang, 2007; Huang and Chiang, 2007), and multi-pass decoding approaches(Venugopal et al., 2007; Zhang and Gildea, 2008). Other efforts have been made for A* decoding using search heuristics (Och et al., 2001; Zhang and Gildea, 2006). The Pharaoh decoder (Koehn, 2004) uses an estimated score of uncovered source sequences as an important component to compare hypotheses. In A* decoding (Och et al., 2001; Zhang and Gildea, 2006), a heuristic function is used to estimate the probability to complete a partial hypothesis. To some extent, both are similar to our LMLA probability. The biggest difference is that we emphasize the effect o"
2009.mtsummit-posters.24,W05-1507,0,0.0135781,"ing to the following two reorderings. If a straight order is preferred (Fig. 2(a)), the language model look-ahead probability πs (H) can be estimated as follows πs (H) = m-gram(T r (s1 ...si−1 ), H l ) ·m-gram(H r , T l (sj+1 ...sn )) where H l/r are the leftmost/rightmost boundary words of H, which both include m0 = min(m − 4 The reason for caching m0 words is to keep the same with what we do for each hypothesis, where m0 words are also stored on the left/right of the hypothesis for the dynamic programming to compute new m-grams in the CKY algorithm intersected with an m-gram language model (Huang et al., 2005). (a) target source 1 i-1 i j j+1 n 1 i-1 i j j+1 n (b) target source Figure 2: Two Reorderings (straight and inverted) for Language Model Look-Ahead. 1, |H|) words. If an inverted order is preferred (Fig. 2(b)), the language model look-ahead probability πi (H) can be estimated as follows πi (H) = m-gram(T r (sj+1 ...sn ), H l ) ·m-gram(H r , T l (s1 ...si−1 )) Since we don’t know which order will be preferred, we take the maximum of the straight and inverted LM look-ahead probability for the hypothesis π(H) = max(πs (H), πi (H)) The final beam thresholding measure for H when compared to the b"
2009.mtsummit-posters.24,2007.mtsummit-papers.43,0,0.0213044,"re of the full uncovered source sequence for both threshold pruning and histogram pruning (the latter). The Pharaoh-style “future cost” can not provide any discriminative information for our pruning since we compare competing hypotheses within the same cell (This means that they have the same future cost). We remains the same as the Pharaoh decoder to find the most probable path through translation options for source words that are not yet translated. But we go further to take into account the interaction of current hypotheses and the most probable path for not yet translated source sequence. Moore and Quirk (2007) present two modifications for beam-search decoding, the Pharaoh decoder in particular by improving the future cost estimation and early pruning out next-phrase translations. Their success and the high efficiency of our beam thresholding methods (verified by experiments in the next section) show that there is much room for search space reduction in widely-used beam-search decoding. 5 Experiments We carried out a series of experiments to examine the effect of our beam thresholding techniques by comparing them with the fixed beam thresholding as well as the cube pruning, and also by combining al"
2009.mtsummit-posters.24,W01-1408,0,0.0611538,"arly stage of decoding. We call this pruning strategy dynamic beam thresholding (DBT). DBT increases the parameter α to tighten the beam when more source words covered. In theory, DBT runs faster than traditional beam thresholding FBT at the same performance level, as our experiments attest. 3 Language Model Look-ahead In traditional beam thresholding used in SMT decoding, only the probability estimated from inside a partial hypothesis is used. This probability does not give information about the probability of the hypothesis in the context of the complete translation. In A* decoding for SMT (Och et al., 2001; Zhang and Gildea, 2006), different heuristic functions are used to estimate a “future” probability for completing a partial hypothesis. In CKY bottom-up parsing, (Goodman, 1997) introduces a prior probability into the beam thresholding. All of these probabilities are capable of capturing the outside context interaction, to some extent. In this paper, we discuss the LM look-ahead (LMLA) and examine the question of whether, given the complicated reordering in SMT, the LM lookahead can obtain a considerable speedup in SMT decoding. The basic idea of the LM look-ahead is to incorporate the langu"
2009.mtsummit-posters.24,P02-1040,0,0.105792,"Missing"
2009.mtsummit-posters.24,N03-1017,0,0.0224318,"Missing"
2009.mtsummit-posters.24,P96-1021,0,0.0346546,"Missing"
2009.mtsummit-posters.24,J97-3002,0,0.0947939,"r success and the high efficiency of our beam thresholding methods (verified by experiments in the next section) show that there is much room for search space reduction in widely-used beam-search decoding. 5 Experiments We carried out a series of experiments to examine the effect of our beam thresholding techniques by comparing them with the fixed beam thresholding as well as the cube pruning, and also by combining all these pruning approaches step by step. We tested them on a Chinese-to-English system with a CKYstyle decoder. The system is based on the Bracketing Transduction Grammars (BTG) (Wu, 1997), which uses the BTG lexical rules (A → x/y) to translate source phrase x into target phrase y and the BTG merging rules (A → [A, A]|hA, Ai) to combine two neighboring phrases with a straight or inverted order. The BTG lexical rules are weighted with several features, such as phrase translation, word penalty and language model, in a log-linear form. For the merging rules, a MaxEnt-based reordering model using boundary words of neighboring phrases as features is used to predict the merging order, similar to (Xiong et al., 2006). All the log-linear model weights are tuned on the development set"
2009.mtsummit-posters.24,P06-1066,1,0.833241,"e decoder. The system is based on the Bracketing Transduction Grammars (BTG) (Wu, 1997), which uses the BTG lexical rules (A → x/y) to translate source phrase x into target phrase y and the BTG merging rules (A → [A, A]|hA, Ai) to combine two neighboring phrases with a straight or inverted order. The BTG lexical rules are weighted with several features, such as phrase translation, word penalty and language model, in a log-linear form. For the merging rules, a MaxEnt-based reordering model using boundary words of neighboring phrases as features is used to predict the merging order, similar to (Xiong et al., 2006). All the log-linear model weights are tuned on the development set to maximize the BLEU score. A CKY-style decoder is developed to generate the best BTG binary tree for each input sentence, which yields the best translation. We used the FBIS corpus (7.06M Chinese words and 9.15M English words) as our bilingual training data, from which a MaxEnt-based reordering model was also trained. The 4-gram language model training data (181.1M words) consists of English texts mostly derived from Xinhua section of the English Gigaword corpus. We used the NIST MT-05 as our test set (27.4 words per sentence"
2009.mtsummit-posters.24,C04-1030,0,0.0441255,"Missing"
2009.mtsummit-posters.24,W06-1627,0,0.0762381,"ding. We call this pruning strategy dynamic beam thresholding (DBT). DBT increases the parameter α to tighten the beam when more source words covered. In theory, DBT runs faster than traditional beam thresholding FBT at the same performance level, as our experiments attest. 3 Language Model Look-ahead In traditional beam thresholding used in SMT decoding, only the probability estimated from inside a partial hypothesis is used. This probability does not give information about the probability of the hypothesis in the context of the complete translation. In A* decoding for SMT (Och et al., 2001; Zhang and Gildea, 2006), different heuristic functions are used to estimate a “future” probability for completing a partial hypothesis. In CKY bottom-up parsing, (Goodman, 1997) introduces a prior probability into the beam thresholding. All of these probabilities are capable of capturing the outside context interaction, to some extent. In this paper, we discuss the LM look-ahead (LMLA) and examine the question of whether, given the complicated reordering in SMT, the LM lookahead can obtain a considerable speedup in SMT decoding. The basic idea of the LM look-ahead is to incorporate the language model interaction of"
2009.mtsummit-posters.24,P08-1025,0,0.0186269,"side the hypothesis H, λLM is the weight of the language model. Note that this probability is only used for the beam thresholding. 4 Comparison to Previous Work Efficient decoding is of great importance to rapid SMT development and commercial applications. Much of previous work focuses on reducing the overwhelming overhead introduced by the intersection of the m-gram language model and the translation model (phrase-based or syntaxbased). This is the fundamental motivation for cube pruning/growing(Chiang, 2007; Huang and Chiang, 2007), and multi-pass decoding approaches(Venugopal et al., 2007; Zhang and Gildea, 2008). Other efforts have been made for A* decoding using search heuristics (Och et al., 2001; Zhang and Gildea, 2006). The Pharaoh decoder (Koehn, 2004) uses an estimated score of uncovered source sequences as an important component to compare hypotheses. In A* decoding (Och et al., 2001; Zhang and Gildea, 2006), a heuristic function is used to estimate the probability to complete a partial hypothesis. To some extent, both are similar to our LMLA probability. The biggest difference is that we emphasize the effect of the language model interaction on the beam thresholding. We neither use the LMLA p"
2009.mtsummit-posters.25,P05-1033,0,0.506363,"be our future work. To calculate P r(hc ), we use a source dependency model. The challenge to our source dependency model is the acquisition of training data: source dependency trees. The source dependency tree used for training has to satisfy two conditions: 1) it is not necessarily linguistically sensible; and 2) it is produced under bilingual context. We observe that we can obtain such dependency trees from word alignments because word alignments implicitly contain hierarchical structures of both source and target language. 1 We inherit the definition of the formally syntax-based MT from (Chiang, 2005). It refers to syntax-based MT which uses synchronous CFG with no linguistic commitment. To uncover the hidden structures from word alignments, we adopt the algorithm of (Zhang et al., 2008). The algorithm decomposes word-aligned sentence pairs into hierarchical trees where leaf nodes are permuted from left to right according to the target language word order. From these trees, we extract the normalized source trees. In order to obtain dependency relations of source words, we annotate nodes with head words and labels. Without introducing conflict with the original definition of dependency tree"
2009.mtsummit-posters.25,D08-1024,0,0.0133445,"et dependency trees in a similar way as the source dependency trees to produce a target dependency model. This is different from (Shen et al., 2008)’s way of inducing a target dependency language model in that we do not require an external dependency parser. 7 Related Work In the formally syntax-based machine translation, Chiang (2005) propose to add a “constituent feature” to the log-linear model in order to reward hypotheses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. All the prior work listed a"
2009.mtsummit-posters.25,P06-1121,0,0.0122002,"ses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. All the prior work listed above requires linguistically motivated parsing. The big distinction from their work is that we build our source dependency model from word alignments without using any monolingual parsing. 8 Conclusion We have presented a novel method to induce a source dependency model from word alignments without using any linguistic analysis. The induced source dependency model captures dependency relations among source words. We employ"
2009.mtsummit-posters.25,2006.amta-papers.8,0,0.0824804,"nstituent feature” to the log-linear model in order to reward hypotheses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. All the prior work listed above requires linguistically motivated parsing. The big distinction from their work is that we build our source dependency model from word alignments without using any monolingual parsing. 8 Conclusion We have presented a novel method to induce a source dependency model from word alignments without using any linguistic analysis. The induced source depend"
2009.mtsummit-posters.25,W04-3250,0,0.0525472,"ns, and applied the “grow-diag-final” refinement rule (Koehn et al., 2005) to produce the final many-to-many word alignments. We trained a fourgram language model using Xinhua section of the English Gigaword corpus (181.1M words) with the SRILM toolkit (Stolcke, 2002). For the efficiency of MERT, we built our development set (580 sentences) using sentences not exceeding 50 characters from the NIST MT-02 set. We evaluated our systems on the NIST MT-05 and MT03 test sets using case-sensitive BLEU-4. Statistical significance in BLEU score differences was assessed by paired bootstrap re-sampling (Koehn, 2004). 5.2 Source Dependency Model Training Because SRA runs very quickly, which produces within a few minutes all decomposition trees from our word-aligned sentence pairs, we can easily train various source dependency models following the steps described in Section 2. In order to allow the normalization step to keep more source words, we removed less probable links in word alignments. First, we calculate lexicon translation probabilities in both directions (P r(C|E) and P r(E|C)) from the original word alignments. Then we search on the source side or the target side for words which are involved in"
2009.mtsummit-posters.25,2005.iwslt-1.8,0,0.0606032,"Missing"
2009.mtsummit-posters.25,P06-1077,0,0.0610526,"opose to add a “constituent feature” to the log-linear model in order to reward hypotheses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. All the prior work listed above requires linguistically motivated parsing. The big distinction from their work is that we build our source dependency model from word alignments without using any monolingual parsing. 8 Conclusion We have presented a novel method to induce a source dependency model from word alignments without using any linguistic analysis. The i"
2009.mtsummit-posters.25,W06-1606,0,0.0217437,"idden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. All the prior work listed above requires linguistically motivated parsing. The big distinction from their work is that we build our source dependency model from word alignments without using any monolingual parsing. 8 Conclusion We have presented a novel method to induce a source dependency model from word alignments without using any linguistic analysis. The induced source dependency model captures dependency relations among source words. We employ this new model in M"
2009.mtsummit-posters.25,P08-1114,0,0.190728,"ethod, we can also obtain target dependency trees in a similar way as the source dependency trees to produce a target dependency model. This is different from (Shen et al., 2008)’s way of inducing a target dependency language model in that we do not require an external dependency parser. 7 Related Work In the formally syntax-based machine translation, Chiang (2005) propose to add a “constituent feature” to the log-linear model in order to reward hypotheses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. A"
2009.mtsummit-posters.25,P00-1056,0,0.223357,"e do not expand the search space which the original decoder has to explore. These two simplifications greatly reduce potential workload caused by introducing a dependency model into decoding. 5 Experiments We carried out experiments to examine the effect of the source dependency model on Chinese-to-English translation tasks. 5.1 Experimental Setup Our baseline is a formally syntax-based system using BTG, developed by following (Xiong et al., 2006). The training data is from FBIS corpus, which contains 6.59M Chinese words and 8.04M English words. To obtain word-level alignments, we ran GIZA++ (Och and Ney, 2000) on the corpus in both directions, and applied the “grow-diag-final” refinement rule (Koehn et al., 2005) to produce the final many-to-many word alignments. We trained a fourgram language model using Xinhua section of the English Gigaword corpus (181.1M words) with the SRILM toolkit (Stolcke, 2002). For the efficiency of MERT, we built our development set (580 sentences) using sentences not exceeding 50 characters from the NIST MT-02 set. We evaluated our systems on the NIST MT-05 and MT03 test sets using case-sensitive BLEU-4. Statistical significance in BLEU score differences was assessed by"
2009.mtsummit-posters.25,P02-1038,0,0.0688259,"ation tasks. 1 P r(h) ≈ P r(hc )P r(he ) Introduction In the word-based translation model introduced by IBM (Brown et al., 1993), the hidden variable, word alignment, associates the source sentence (c) with the target sentence (e). This model has been advanced to the phrase-based and syntax-based models, and the hidden variable has also been transformed into new forms: phrase alignment in phrasebased translation and hierarchical tree in syntaxbased translation. In all cases, we can search the best translation among all possible target sentences and hidden variables through a log-linear model (Och and Ney, 2002) eˆ = argmax(Σi λi fi (c, h, e)) (1) e,h where fi are feature functions which are dependent not only on c and e but also on the hidden variable h. (2) where hc and he are the tree projections of h on the source and target side respectively. Without loss of generality, we discuss P r(h) within the context of Bracketing Transduction Grammar (BTG) (Wu, 1997), which is a binary synchronous CFG widely adopted in machine translation. We focus on P r(hc ) in this paper and leave the modeling of P r(he ) to be our future work. To calculate P r(hc ), we use a source dependency model. The challenge to o"
2009.mtsummit-posters.25,P03-1021,0,0.00596905,"orpus. To deal with the data sparseness problem, we smooth the two probabilities through Witten-Bell interpolation, similar to (Collins, 1999). Table 2 shows the back-off structures for the smoothing. Further, for words occurring less than 5 times in training data, and words in test data which have never been seen in training, we replace them with the ”UNKNOWN” token. As described in the introduction, P r(hc ) is integrated into the log-linear translation model as a new feature. The weight of this new feature, like the weights of other features, is tuned via minimumerror-rate training (MERT) (Och, 2003) on a development set. 4 Decoding with Source Dependency Model Since we use a bottom-up CKY parsing algorithm for decoding, computing the source dependency model score is quite straightforward. When a new node of the hierarchical tree hc is being constructed, we use the predefined POS tag weight table (Table 1) to determine the head. If the BTG lexical rule (A → x/y) is used to produce a leaf node upon a source span, we select the rightmost source word with the highest POS tag weight within the source span as the head word for the leaf node. If the BTG merging rules (A → [A, A]|hA, Ai) are use"
2009.mtsummit-posters.25,P08-1066,0,0.0406647,"ildea, 2005). It is clearly shown in the experiments that noises in word alignments influence the performance of SDM considerably. To address this problem, we would explore hierarchical alignments as they contain hierarchical structures intrinsically and would enable us to obtain high quality source dependency trees without using complicated transformation. • Induce a target dependency model from word alignments. Based on our proposed method, we can also obtain target dependency trees in a similar way as the source dependency trees to produce a target dependency model. This is different from (Shen et al., 2008)’s way of inducing a target dependency language model in that we do not require an external dependency parser. 7 Related Work In the formally syntax-based machine translation, Chiang (2005) propose to add a “constituent feature” to the log-linear model in order to reward hypotheses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance signifi"
2009.mtsummit-posters.25,J97-3002,0,0.186934,"orms: phrase alignment in phrasebased translation and hierarchical tree in syntaxbased translation. In all cases, we can search the best translation among all possible target sentences and hidden variables through a log-linear model (Och and Ney, 2002) eˆ = argmax(Σi λi fi (c, h, e)) (1) e,h where fi are feature functions which are dependent not only on c and e but also on the hidden variable h. (2) where hc and he are the tree projections of h on the source and target side respectively. Without loss of generality, we discuss P r(h) within the context of Bracketing Transduction Grammar (BTG) (Wu, 1997), which is a binary synchronous CFG widely adopted in machine translation. We focus on P r(hc ) in this paper and leave the modeling of P r(he ) to be our future work. To calculate P r(hc ), we use a source dependency model. The challenge to our source dependency model is the acquisition of training data: source dependency trees. The source dependency tree used for training has to satisfy two conditions: 1) it is not necessarily linguistically sensible; and 2) it is produced under bilingual context. We observe that we can obtain such dependency trees from word alignments because word alignment"
2009.mtsummit-posters.25,P06-1066,1,0.892645,"orcedly assigned according to POS tag weights. No probability is involved in this decision. Second, we do not add the head word to the state of hypothesis. This means we do not expand the search space which the original decoder has to explore. These two simplifications greatly reduce potential workload caused by introducing a dependency model into decoding. 5 Experiments We carried out experiments to examine the effect of the source dependency model on Chinese-to-English translation tasks. 5.1 Experimental Setup Our baseline is a formally syntax-based system using BTG, developed by following (Xiong et al., 2006). The training data is from FBIS corpus, which contains 6.59M Chinese words and 8.04M English words. To obtain word-level alignments, we ran GIZA++ (Och and Ney, 2000) on the corpus in both directions, and applied the “grow-diag-final” refinement rule (Koehn et al., 2005) to produce the final many-to-many word alignments. We trained a fourgram language model using Xinhua section of the English Gigaword corpus (181.1M words) with the SRILM toolkit (Stolcke, 2002). For the efficiency of MERT, we built our development set (580 sentences) using sentences not exceeding 50 characters from the NIST M"
2009.mtsummit-posters.25,C08-1136,0,0.0711919,"The source dependency tree used for training has to satisfy two conditions: 1) it is not necessarily linguistically sensible; and 2) it is produced under bilingual context. We observe that we can obtain such dependency trees from word alignments because word alignments implicitly contain hierarchical structures of both source and target language. 1 We inherit the definition of the formally syntax-based MT from (Chiang, 2005). It refers to syntax-based MT which uses synchronous CFG with no linguistic commitment. To uncover the hidden structures from word alignments, we adopt the algorithm of (Zhang et al., 2008). The algorithm decomposes word-aligned sentence pairs into hierarchical trees where leaf nodes are permuted from left to right according to the target language word order. From these trees, we extract the normalized source trees. In order to obtain dependency relations of source words, we annotate nodes with head words and labels. Without introducing conflict with the original definition of dependency tree, we call the annotated tree obtained from the above steps source dependency tree. Using these source dependency trees, we develop our source dependency model. To the best of our knowledge,"
2009.mtsummit-posters.25,2007.mtsummit-papers.71,1,0.770567,"o the log-linear model in order to reward hypotheses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. All the prior work listed above requires linguistically motivated parsing. The big distinction from their work is that we build our source dependency model from word alignments without using any monolingual parsing. 8 Conclusion We have presented a novel method to induce a source dependency model from word alignments without using any linguistic analysis. The induced source dependency model captures d"
2015.mtsummit-papers.3,J07-2003,0,0.24639,"Missing"
2015.mtsummit-papers.3,P11-2031,0,0.0198494,"and applying refinement rule grow-diag-final-and Koehn et al. (2003). A 4-gram language model was trained on the Xinhua section of Gigaword by SRILM toolkit Stolcke et al. (2002). We also extracted SCFG rules from the word-aligned training data. The translation performance was measured by case-insensitive BLEU Papineni et al. (2002). We used minimum error rate training (MERT) (Och.2003a) to tune the log-linear feature weights. As MERT is normally instable, we ran the tuning process three times for all our experiments and presented the average BLEU scores on the three MERT runs as suggested by Clark et al (2011). The open source toolkit DISSECT1 was applied to obtain PMI-based vector space word representations with a context window of 5 words, and Word2Vec2 to acquire neural word representations, with each word represented as a 50-dimensional vector. When adopted Word2Vec, we just set the context window of size 5 and using continuous bag-of-words model. DISSECT was also adpoted to train weights in semantic composition of weighted vector addition. Unsupervised greedy RAE was trained in the way following Socher et al. (2011). In the bilingual projection neural network, 50 hidden units were used in the"
2015.mtsummit-papers.3,N03-1017,0,0.119057,"d sense discrimination (Clark and Pulman,2007) and thesaurus compilation (Yang and Powers,2008). In this paper, we explore how to learn semantic representations of bilingual phrases, rather than monolingual words, in the context of statistical machine translation (SMT) to facilitate the computation of semantic similarity between translation equivalents at the phrase level. We also study whether semantic similarity scores calculated in terms of bilingual distributed phrase representations are complementary to phrase translation probabilities estimated by the conventional counting method in SMT Koehn et al. (2003). Very recently we have witnessed some studies on learning bilingual distributed representations for SMT. Mikolov et al. (2013b) train two neural network (NN) based models to learn word embeddings in the source and the target language, respectively, and then map the embeddings from the source to the target language space using a transformation matrix that is learned ∗Corresponding author Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 32 by minimizing the mapping cost on all word pairs. Zou et al. (2013) introduce bilingual word embeddings into phrase-"
2015.mtsummit-papers.3,P08-1028,0,0.134647,"Missing"
2015.mtsummit-papers.3,P03-1021,0,0.0367911,"Missing"
2015.mtsummit-papers.3,J03-1002,0,0.00795929,"Missing"
2015.mtsummit-papers.3,J07-2002,0,0.0236259,"Missing"
2015.mtsummit-papers.3,P02-1040,0,0.0919351,"Missing"
2015.mtsummit-papers.3,D11-1014,0,0.146961,"f representations at the word level Mikolov et al. (2013b); Zou et al. (2013), so as to keep consistency with the SMT that uses phrases rather than words as basic translation units. • We learn phrase representations from distributed word representations via semantic composition, instead of from raw phrases Gao et al. (2013) in order to avoid the data sparseness issue of directly learning phrase representations from data. Particularly, we empirically compare two different composition methods in our framework, namely, weighted vector addition (Mitchell and Lapata,2008) and recursive autoencoder Socher et al. (2011). • Rather than jointly learning phrase representations with feature weights of the log-linear model of SMT Gao et al. (2013), we take a loose coupling strategy to simplify the learning process. We adopt a neural network to project phrase representations from the source onto the target language semantic space, in separation from the process of feature weight tuning in SMT. • Rather than capturing only the linear transformation between the source and target language semantic space Mikolov et al. (2013b), our neural network for the bilingual projection can model both linear and nonlinear transfo"
2015.mtsummit-papers.3,D12-1110,0,0.106474,"Missing"
2015.mtsummit-papers.3,P10-1040,0,0.0103434,"Missing"
2015.mtsummit-papers.3,P12-1079,1,0.885606,"Missing"
2015.mtsummit-papers.3,D13-1141,0,0.0745415,"ted by the conventional counting method in SMT Koehn et al. (2003). Very recently we have witnessed some studies on learning bilingual distributed representations for SMT. Mikolov et al. (2013b) train two neural network (NN) based models to learn word embeddings in the source and the target language, respectively, and then map the embeddings from the source to the target language space using a transformation matrix that is learned ∗Corresponding author Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 32 by minimizing the mapping cost on all word pairs. Zou et al. (2013) introduce bilingual word embeddings into phrase-based machine translation: Word representations are first learned from language A via an NN-based model, word embeddings in the parallel language B are then initialized according to A’s embeddings and word alignments between A and B, and the final word representations of B are obtained by a further training process that optimizes a combined objective on bilingual data. Gao et al. (2013) extend distributional representations from the word level to the phrase level, adopting a fully connected neural network to transfer bag-of-words vector represen"
2015.mtsummit-papers.3,P14-1011,0,0.0370829,"Missing"
2020.acl-main.143,D16-1250,0,0.0197347,"methods, including word-by-word translation, unsupervised MT, and cross-lingual embedding transformation. On distant language pairs that unsupervised MT struggled to be effective, AT and Bi-view AT perform remarkably better. 2 Related Work The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (U"
2020.acl-main.143,P18-1073,0,0.158206,"hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences1 . 1 Introduction Motivated by a monolingual speaker acquiring translation ability by referring to a bilingual dictionary, we propose a novel MT task that no parallel sentences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency word"
2020.acl-main.143,D18-1399,0,0.0376008,"Missing"
2020.acl-main.143,P19-1019,0,0.062444,"language pairs. 4.5 Experimental Results: with Cross-lingual Pretraining The bottom part of Table 2 reports performances of UNMT with XLM, which conducts the crosslingual pretraining on concatenated non-parallel corpora (Lample and Conneau, 2019), and performances of our AT/Bi-view AT with the anchored cross-lingual pretraining, i.e., ACP. The results show that our proposed AT approaches are still superior when equipped with the cross-lingual pretraining. UNMT obtains great improvement when combined with XLM, achieving state-of-the-art unsupervised MT performance better than Unsupervised SMT (Artetxe et al., 2019) and Unsupervised NMT (Lample et al., 2018b) across close and distant language pairs. 1575 Effect on Bilingual Word Embeddings As shown in Figure 2, we depict the word embeddings of some sampled words in English-Chinese after our Bi-view AT. The dimensions of the embedding vectors are reduced to two by using T-SNE and are visualized by the visualization tool in Tensorflow10 . We sample the English words that are not covered by the dictionary at first, then search their nearest Chinese neighbors in the embedding space. It shows that the words which constitute a new ground-truth translation pair"
2020.acl-main.143,J82-2005,0,0.709748,"Missing"
2020.acl-main.143,D16-1162,0,0.164289,"8b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generated by unsupervised BLI for initializing an unsupervised MT system (Artetxe et al., 2018c,b; Lample et al., 2018a), we use the ground-truth bilingual dictionary and apply it throughout the training process. We propose Anchored Training (AT) to tackle this task. Since word representations are learned over monolingual corpora wi"
2020.acl-main.143,P18-1008,0,0.0261085,"that no parallel sentences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generate"
2020.acl-main.143,E17-1088,0,0.0146801,"of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary. Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and Søgaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017). 3 Our Approach There are multiple freely available bilingual dictionaries such as Muse dictionary2 (Conneau et al., 2018), Wiktionary3 , and PanLex4 . We adopt Muse dictionary which contains 110 large-scale groundtruth bilingual dictionaries. We propose to inject the bilingual dictionary into the MT training by placing anchoring points on the large scale monolingual corpora to drive the semantic spaces of both languages becoming closer so that MT training without parallel sentences becomes easier. We present the proposed Anchored Training (AT) and Bi-view AT in the following. 3.1 Anchored Tr"
2020.acl-main.143,D16-1136,0,0.0209789,"orms remarkably bad on distant language pairs in which aligning the embeddings of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary. Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and Søgaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017). 3 Our Approach There are multiple freely available bilingual dictionaries such as Muse dictionary2 (Conneau et al., 2018), Wiktionary3 , and PanLex4 . We adopt Muse dictionary which contains 110 large-scale groundtruth bilingual dictionaries. We propose to inject the bilingual dictionary into the MT training by placing anchoring points on the large scale monolingual corpora to drive the semantic spaces of both languages becoming closer so that MT training without parallel sentences becomes easier. We present the pr"
2020.acl-main.143,E14-1049,0,0.031871,"aries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al."
2020.acl-main.143,N15-1157,0,0.0293103,"g process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the embeddings of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary. Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and Søgaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017). 3 Our Approach There are multiple freely available bilingual dictionaries such as Muse dictionary2 (Conneau et al., 2018), Wiktionary3 , and PanLex4 . We adopt Muse dictionary which contains 110 large-scale groundtruth bilingual dictionaries. We propose to inject the bilingual dictionary into the MT training by placing anchoring points on the large scale monolingual corpora to drive the semantic spaces of both languages becoming closer s"
2020.acl-main.143,N16-1156,0,0.0134234,"erform remarkably better. 2 Related Work The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionar"
2020.acl-main.143,D18-1549,0,0.0509903,"Missing"
2020.acl-main.143,P15-1027,0,0.0208843,"ive, AT and Bi-view AT perform remarkably better. 2 Related Work The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the"
2020.acl-main.143,N15-1028,0,0.0210411,"orks are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al., 2018c; Lample et"
2020.acl-main.143,P16-1009,0,0.242082,"entences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generated by unsupervised BLI f"
2020.acl-main.143,P16-1162,0,0.424255,"entences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generated by unsupervised BLI f"
2020.acl-main.143,P19-1119,0,0.0252407,"through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al., 2018c; Lample et al., 2018a) or abandon the artificial dictionary by using joint BPE so that multiple BPE units can be shared by both languages (Lample et al., 2018b). We use the ground-truth dictionary instead and apply it throughout a novel training process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the embeddings of both si"
2020.acl-main.143,W14-1613,0,0.0293924,"by both languages (Lample et al., 2018b). We use the ground-truth dictionary instead and apply it throughout a novel training process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the embeddings of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary. Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and Søgaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017). 3 Our Approach There are multiple freely available bilingual dictionaries such as Muse dictionary2 (Conneau et al., 2018), Wiktionary3 , and PanLex4 . We adopt Muse dictionary which contains 110 large-scale groundtruth bilingual dictionaries. We propose to inject the bilingual dictionary into the MT training b"
2020.acl-main.143,N15-1104,0,0.0287906,"uggled to be effective, AT and Bi-view AT perform remarkably better. 2 Related Work The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference i"
2020.acl-main.143,P18-1005,0,0.0184947,"sformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al., 2018c; Lample et al., 2018a) or abandon the artificial dictionary by using joint BPE so that multiple BPE units can be shared by both languages (Lample et al., 2018b). We use the ground-truth dictionary instead and apply it throughout a novel training process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the em"
2020.acl-main.297,W06-1651,0,0.0926525,"k An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, considering SRL is highly related to ORL and has a considerable amount of training data. Inspired by the similarity between ORL and SRL in task definition, Kim and Hovy (2006) and Ruppenhofer et al. (2008) address ORL with a well-trained SRL model by treating"
2020.acl-main.297,P17-4017,0,0.0949806,"TL model effectively boosts the F1 score by 9.29 over the syntaxagnostic baseline. In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT). Our best model achieves 4.34 higher F1 score than the current state-ofthe-art. 1 $ Cardoso Opinion and sentiment analysis has a wide range of real-world applications like social media monitoring (Bollen et al., 2011), stock market prediction (Nguyen et al., 2015), box office prediction (Yu et al., 2010), and general e-commerce applications (Kim et al., 2013; Hu et al., 2017; Cui et al., 2017). In particular, fine-grained opinion analysis aims to identify users’ opinions in a text, including opinion expressions, holders of the opinions, targets of the opinions, target-dependent attitude, and intensity of opinions (Marasovi´c and Frank, 2018), which is very important for understanding political stance, Corresponding author challenge facing Chavez Target is ... Figure 1: An Example of ORL (bottom) and syntactic dependency tree (top) for “Cardoso says challenge facing Chavezis is reestablishing normalcy.” Introduction ∗ says Holder Expression customers’ reviews, marketing trends, and"
2020.acl-main.297,N19-1423,0,0.237545,"cy arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax provides more information while alleviating parsing errors. For the second barrier, considering that the pipeline methods are notorious for the error propagation problem, we introduce multi-task learning (MTL) frameworks, which have been widely used in many NLP models when predictions at various processing levels are needed (Collobert and Weston, 2008; Ruder, 2017). Apart from the syntactic information, contextualized word representations like BERT (Devlin et al., 2019) are widely used to compensate for the sparsity of task-specific training data. They compress distributional semantics of words from large corpora, making the local context fluent and natural. However, the long-distance dependencies between words are often ignored, which is ideally able to be captured by syntactic analysis. In summary, based on previous studies in using syntax to improve various tasks, this work investigates whether syntax can enhance the neural ORL model. Particularly, we try to answer the following three questions. • How to effectively integrate various syntactic information"
2020.acl-main.297,P19-1024,0,0.0190934,"nd the ORL model. We first obtain the edge-weighted graph from the decoder of a well-trained biaffine parser as a data preprocessing step, and then feed the graph into our D EP GCN in the form of an adjacency matrix A 1 . Then we feed the outputs of the ORL BiLSTM-based encoder as the initial inputs h0 to the D EP GCN. Finally, we feed the output of the D EP GCN to the CRF-based decoder, and update the ORL results under the guidance of the syntactic information. Moreover, we introduce dense connections to the multi-layer D EP GCN for extracting more structural information (Huang et al., 2017; Guo et al., 2019). Instead of only adding connections between adjacent layers, we use dense connections from each layer to all the subsequent layers. Formally, the input of node i at the l-th layer is: (l) 4.1 (0) (1) (l−1) xi = hi ⊕ hi ⊕ · · · ⊕ hi Dependency Graph Convolutional Networks (D EP GCN) (3) (l) In this subsection, we propose dependency graph convolutional networks (D EP GCN) to better encode the syntactic information from the edgeweighted graphs. On the one hand, compared with explicit 1-best parse trees, edge-weighted graphs where hi is the output of node i at the l-th layer. We also make residua"
2020.acl-main.297,P18-1192,0,0.0735118,"human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax prov"
2020.acl-main.297,J13-3002,0,0.0940515,"dency relations, do not fully overlap with those from the contextualized word representations like BERT. Our overall model delivers a new stateof-the-art result on the benchmark MPQA corpus, with 4.34 absolute improvement over the previous best result. 2 Related work An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, conside"
2020.acl-main.297,P16-1087,0,0.537352,"s, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, considering SRL is highly related to ORL and has a considerable amount of training data. Inspired by the similarity between ORL and SRL in task definition, Kim and Hovy (2006) and Ruppenhofer et al. (2008) address ORL with a well-trained SRL model by treating opinion expressions as semantic predicates, and op"
2020.acl-main.297,W06-0301,0,0.113222,"from long-distance dependency relations, do not fully overlap with those from the contextualized word representations like BERT. Our overall model delivers a new stateof-the-art result on the benchmark MPQA corpus, with 4.34 absolute improvement over the previous best result. 2 Related work An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issu"
2020.acl-main.297,N18-1054,0,0.470419,"Missing"
2020.acl-main.297,D17-1159,0,0.406534,"ural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representati"
2020.acl-main.297,P16-1105,0,0.0190351,"SRL as an auxiliary task, and employ different MTL frameworks to learn the common grounds between ORL and SRL and distinguish task-specific knowledge. Zhang et al. (2019b) extract neural features from a welltrained SRL model as SRL-aware word representations, and then feed them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Zhang et al. (2019a) and Xia et al. (2019a)"
2020.acl-main.297,P16-1113,0,0.0491495,"res from a welltrained SRL model as SRL-aware word representations, and then feed them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Zhang et al. (2019a) and Xia et al. (2019a) extract the hidden states from the LSTM encoder of the parser model as syntax-aware word representations, and feed them to downstream tasks as extra inputs. In contrast, few works have proved t"
2020.acl-main.297,ruppenhofer-etal-2008-finding,0,0.26963,"ion mining task, opinion role labeling (ORL) aims to identify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Targ"
2020.acl-main.297,D18-1548,0,0.0256489,"ic roles. Marasovi´c and Frank (2018) take SRL as an auxiliary task, and employ different MTL frameworks to learn the common grounds between ORL and SRL and distinguish task-specific knowledge. Zhang et al. (2019b) extract neural features from a welltrained SRL model as SRL-aware word representations, and then feed them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Z"
2020.acl-main.297,P15-1150,0,0.0563738,"them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Zhang et al. (2019a) and Xia et al. (2019a) extract the hidden states from the LSTM encoder of the parser model as syntax-aware word representations, and feed them to downstream tasks as extra inputs. In contrast, few works have proved that syntactic knowledge is useful in the neural ORL models. Yang and Cardie"
2020.acl-main.297,D19-1541,1,0.407543,"ng of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax provides more informat"
2020.acl-main.297,N19-1075,0,0.138213,"ng of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax provides more informat"
2020.acl-main.297,P13-1161,0,0.632022,"ts of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, considering SRL is highly related to ORL and has a considerable amount of training data. Inspired by the similarity between ORL and SRL in task definition, Kim and Hovy (2006) and Ruppenhofer et al. (2008) address ORL with a well-trained SRL model by treating opinion expressions as"
2020.acl-main.297,Q14-1039,0,0.218287,"alleviate the error propagation problem; and 3) contributions from syntactic information, especially from long-distance dependency relations, do not fully overlap with those from the contextualized word representations like BERT. Our overall model delivers a new stateof-the-art result on the benchmark MPQA corpus, with 4.34 absolute improvement over the previous best result. 2 Related work An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targe"
2020.acl-main.297,N19-1118,1,0.170002,"dentify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without"
2020.acl-main.297,D14-1162,0,0.083367,"RT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) to obtain deep contextualized word representations as our extra inputs. In particular, we use BERT-base (uncased) model and extract representations from the top-1 hidden layer. Our experiments show that using the top-1 layer representations performs better than the more common use of aggregating top-4 hidden layers.2 Parameters. We follow the previous works of Zhang et al. (2019b) and Marasovi´c and Frank (2018) without much parameter tuning. Specifically, we use the pretrained 100-dimensional glove embeddings (Pennington et al., 2014). The BiLSTM layer number is set to 3, and the hidden output size is 200. We apply 0.33 dropout to word representation and the hidden states of the BiLSTM. We choose Adam (Kingma and Ba, 2014) to optimize model parameters with a learning rate 10−3 . The entire training instances are trained for 30 epochs with the batch size of 50, and the best-epoch model at the peak performance on the dev corpus is chosen. For the MTL, we train the batches of ORL and parsing in turn since this interleaving training can obtain better performance in our experiments. Besides, we use the corpus weighting trick to"
2020.acl-main.297,N19-1066,0,0.110957,"dentify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without"
2020.acl-main.297,D19-1057,1,0.172248,"dentify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without"
2020.acl-main.302,D17-1171,0,0.048482,"the DL era, due to 2 Though many recent works report higher performance with extra resources, for example contextualized word representations learned from large-scale unlabeled texts under language model loss, they either adopt the same architecture or achieve similar performance under fair comparison. the unmatched speed of CPU and GPU computation. This leads to the second question: can we batchify the inside-outside algorithm and perform computation directly on GPUs? In that case, we can employ efficient TreeCRF as a built-in component in DL toolkits such as PyTorch for wider applications (Cai et al., 2017; Le and Zuidema, 2014). Overall, targeted at the above two questions, this work makes the following contributions. • We for the first time propose second-order TreeCRF for neural dependency parsing. We also propose an efficient and effective triaffine operation for scoring second-order subtrees. • We propose to batchify the inside algorithm via direct large tensor computation on GPUs, leading to very efficient TreeCRF loss computation. We show that the complex outside algorithm is no longer needed for the computation of gradients and marginal probabilities, and can be replaced by the equally"
2020.acl-main.302,D07-1101,0,0.132825,"Most non-neural graph-based parsers adopt the max-margin training algorithm, which first predicts a highest-scoring tree with the current model, and then updates feature weights so that the correct tree has a higher score than the predicted tree. Second, high-order modeling brings significant accuracy gains. The basic first-order model factors the score of a tree into independent scores of single dependencies (McDonald et al., 2005a). Second-order models were soon propose to incorporate scores of dependency pairs, such as adjacent-siblings (McDonald and Pereira, 2006) and grand-parent-child (Carreras, 2007; Koo and Collins, 2010), showing significant accuracy improvement yet with the cost of lower efficiency and more complex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association fo"
2020.acl-main.302,D14-1082,0,0.596848,"plex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics networks for automatically learning combinations of dozens of atomic features similar to Chen and Manning (2014), and for computing subtree scores. They show that incorporating second-order scores of adjacent-sibling subtrees significantly improved performance. Then, both Wang and Chang (2016) and Kiperwasser and Goldberg (2016) propose to utilize BiLSTM as an encoder and use minimal feature sets for scoring single dependencies in a first-order parser. These three representative works all employ global max-margin training. Dozat and Manning (2017) propose a strong and efficient biaffine parser and obtain state-of-the-art accuracy on a variety of datasets and languages. The biaffine parser is also first-"
2020.acl-main.302,N19-1423,0,0.0536989,"” means p &lt; 0.05 and “‡” means p &lt; 0.005. Biaffine17: Dozat and Manning (2017); F&K19: Falenska and Kuhn (2019); Li19: Li et al. (2019); Ji19: Ji et al. (2019); Zhang19: Zhang et al. (2019). sentences per second, which is able to meet the requirements of a real-time system. More discussions on efficiency are presented in Appendix A. 4.2 Main Results Table 1 lists the main results on the dev and test data. The trends on dev and test are mostly consistent. For a fair comparison with previous works, we only consider those without using extra resources such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). We can see that our baseline L OC achieves the best performance on both PTB and CoNLL09. On PTB, both C RF and C RF 2 O fail to improve 3300 94 86 93 85 C RF 2 O C RF L OC 92 91 72 100 200 300 400 70 C RF 2 O C RF L OC 84 83 100 200 300 400 C RF 2 O C RF L OC 68 66 100 200 300 400 Figure 5: Convergence curves (LAS vs. training epochs) on dev data of PTB, CoNLL09, and NLPCC19. SIB R P F UCM LCM PTB L OC 91.16 90.80 90.98 61.59 50.66 C RF 91.24 90.92 91.08 61.92 50.33 C RF 2 O 91.56 91.11 91.33 63.08 50.99 84 92 82 C RF 2 O (Dep) L OC (Dep) C RF 2 O (Sent) L OC (Sent) 1/4 1/2 full C RF 2 O (De"
2020.acl-main.302,K17-3002,0,0.0623887,"Missing"
2020.acl-main.302,N19-1116,0,0.0182699,"incomplete, complete, and sibling spans in the opposite direction from j to i for brevity. Basically, we first pack the scores of same-width spans at different positions (i, j) for all B sentences in the data batch into large tensors. Then we can do computation and aggregation simultaneously on GPUs via efficient large tensor operation. Similarly, we also batchify the decoding algorithm. Due to space limitation, we omit the details. It is noteworthy that the techniques described here are also applicable to other grammar formulations such as CKY-style constituency parsing (Finkel et al., 2008; Drozdov et al., 2019). 3298 3.3 Outside via Back-propagation Eisner (2016) proposes a theoretical proof on the equivalence between the back-propagation mechanism and the outside algorithm in the case of constituency (phrase-structure) parsing. This work empirically verifies this equivalence for dependency parsing. Moreover, we also find that marginal probabilities p(i → j |x) directly correspond to gradients after back-propagation with log Z(x) as the loss: ∂ log Z = ∂s(i, j) X p(y |x) = p(i → j |x) y:(i,j)∈y (11) which can be easily proved. For TreeCRF parsers, we perform MBR decoding (Smith and Smith, 2007) by r"
2020.acl-main.302,W16-5901,0,0.358708,"tion from j to i for brevity. Basically, we first pack the scores of same-width spans at different positions (i, j) for all B sentences in the data batch into large tensors. Then we can do computation and aggregation simultaneously on GPUs via efficient large tensor operation. Similarly, we also batchify the decoding algorithm. Due to space limitation, we omit the details. It is noteworthy that the techniques described here are also applicable to other grammar formulations such as CKY-style constituency parsing (Finkel et al., 2008; Drozdov et al., 2019). 3298 3.3 Outside via Back-propagation Eisner (2016) proposes a theoretical proof on the equivalence between the back-propagation mechanism and the outside algorithm in the case of constituency (phrase-structure) parsing. This work empirically verifies this equivalence for dependency parsing. Moreover, we also find that marginal probabilities p(i → j |x) directly correspond to gradients after back-propagation with log Z(x) as the loss: ∂ log Z = ∂s(i, j) X p(y |x) = p(i → j |x) y:(i,j)∈y (11) which can be easily proved. For TreeCRF parsers, we perform MBR decoding (Smith and Smith, 2007) by replacing scores with marginal probabilities in the de"
2020.acl-main.302,P19-1012,0,0.17469,"9 94.16 94.12 96.08 96.04 96.02 96.14 96.11 94.47 94.34 94.33 94.49 94.46 CoNLL09 88.90 85.38 88.68 85.47 88.77 85.58 89.07 89.04 89.12 89.29 89.44 86.10 86.04 86.12 86.24 86.37 89.15 89.14 89.28 89.49 89.63‡ 85.98 86.06 86.18† 86.39 86.52‡ NLPCC19 L OC 77.01 71.14 C RF w/o MBR 77.40 71.65 C RF 77.34 71.62 C RF 2 O w/o MBR 77.58 71.92 C RF 2 O 78.08 72.32 76.92 77.17 77.53‡ 77.89 78.02‡ 71.04 71.58 71.89‡ 72.25 72.33‡ Table 1: Main results. We perform significance test against L OC on the test data, where “†” means p &lt; 0.05 and “‡” means p &lt; 0.005. Biaffine17: Dozat and Manning (2017); F&K19: Falenska and Kuhn (2019); Li19: Li et al. (2019); Ji19: Ji et al. (2019); Zhang19: Zhang et al. (2019). sentences per second, which is able to meet the requirements of a real-time system. More discussions on efficiency are presented in Appendix A. 4.2 Main Results Table 1 lists the main results on the dev and test data. The trends on dev and test are mostly consistent. For a fair comparison with previous works, we only consider those without using extra resources such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). We can see that our baseline L OC achieves the best performance on both PTB and CoNLL09."
2020.acl-main.302,P08-1109,0,0.132583,"Missing"
2020.acl-main.302,N18-1091,0,0.062174,"Missing"
2020.acl-main.302,P99-1010,0,0.129976,"e also find that marginal probabilities p(i → j |x) directly correspond to gradients after back-propagation with log Z(x) as the loss: ∂ log Z = ∂s(i, j) X p(y |x) = p(i → j |x) y:(i,j)∈y (11) which can be easily proved. For TreeCRF parsers, we perform MBR decoding (Smith and Smith, 2007) by replacing scores with marginal probabilities in the decoding algorithm, leading to a slight but consistent accuracy increase. 3.4 Handling Partial Annotation As an attractive research direction, studies show that it is more effective to construct or even collect partially labeled data (Nivre et al., 2014; Hwa, 1999; Pereira and Schabes, 1992), where a sentence may correspond to a partial tree |y p |&lt; n in the case of dependency parsing. Partial annotation can be very powerful when combined with active learning, because annotation cost can be greatly reduced if annotators only need to annotate sub-structures that are difficult for models. Li et al. (2016) present a detailed survey on this topic. Moreover, Peng et al. (2019) recently released a partially labeled multi-domain Chinese dependency treebank based on this idea. Then, the question is how to train models on partially labeled data. Li et al. (2016"
2020.acl-main.302,P19-1237,0,0.539301,"th Stanford dependencies (Chen and Manning, 2014), and the Chinese data at the CoNLL09 shared task (Hajiˇc et al., 2009). We also adopt the Chinese dataset released at the NLPCC19 cross-domain dependency parsing shared task (Peng et al., 2019), containing one source domain and three target domains. For simplicity, we directly merge the train/dev/test data of the four domains into larger ones respectively. One characteristic of the data is that most sentences are partially annotated based on active learning. Finally, we conduct experiments on Universal Dependencies (UD) v2.2 and v2.3 following Ji et al. (2019) and Zhang et al. (2019) respectively. We adopt the 300d multilingual pretrained word embeddings used in Zeman et al. (2018) and take the CharLSTM representations as input. For UD2.2, to compare with Ji et al. (2019), we follow the raw text setting of the CoNLL18 shared task (Zeman et al., 2018), and directly use their sentence segmentation and tokenization results. For UD2.3, we also report the results of using gold-standard POS tags to compare with Zhang et al. (2019). Evaluation metrics. We use unlabeled and labeled attachment score (UAS/LAS) as the main metrics. Punctuations are omitted fo"
2020.acl-main.302,P18-1252,1,0.858339,"ely perform the inside algorithm for non-neural parsing, due to the inapplicability of the automatic differentiation mechanism. In order to obtain marginal probabilities and then feature weight gradients, we have to realize the more sophisticated outside algorithm, which is usually at least twice slower than the inside algorithm. This may be the major reason for the less popularity of TreeCRF (vs. max-margin training) before the DL era. As far as we know, all previous works on neural TreeCRF parsing explicitly implement the insideoutside algorithm for gradient computation (Zhang et al., 2019; Jiang et al., 2018). To improve efficiency, computation is transferred from GPUs to CPUs with Cython programming. This work shows that the inside algorithm can be effectively batchified to fully utilize the power of GPUs. Figure 3 and Algorithm 1 together illustrate the batchified version of the second-order inside algorithm, which is a direct extension of the secondorder Eisner algorithm in McDonald and Pereira (2006) by replacing max product with sum product. We omit the generations of incomplete, complete, and sibling spans in the opposite direction from j to i for brevity. Basically, we first pack the scores"
2020.acl-main.302,Q16-1023,0,0.24619,"how little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics networks for automatically learning combinations of dozens of atomic features similar to Chen and Manning (2014), and for computing subtree scores. They show that incorporating second-order scores of adjacent-sibling subtrees significantly improved performance. Then, both Wang and Chang (2016) and Kiperwasser and Goldberg (2016) propose to utilize BiLSTM as an encoder and use minimal feature sets for scoring single dependencies in a first-order parser. These three representative works all employ global max-margin training. Dozat and Manning (2017) propose a strong and efficient biaffine parser and obtain state-of-the-art accuracy on a variety of datasets and languages. The biaffine parser is also first-order and employs simpler and more efficient non-structural training via local head selection for each token (Zhang et al., 2017). Observing such contrasting development, we try to make a connection between pre-DL and"
2020.acl-main.302,P10-1001,0,0.470072,"graph-based parsers adopt the max-margin training algorithm, which first predicts a highest-scoring tree with the current model, and then updates feature weights so that the correct tree has a higher score than the predicted tree. Second, high-order modeling brings significant accuracy gains. The basic first-order model factors the score of a tree into independent scores of single dependencies (McDonald et al., 2005a). Second-order models were soon propose to incorporate scores of dependency pairs, such as adjacent-siblings (McDonald and Pereira, 2006) and grand-parent-child (Carreras, 2007; Koo and Collins, 2010), showing significant accuracy improvement yet with the cost of lower efficiency and more complex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association for Computational Linguist"
2020.acl-main.302,N16-1030,0,0.515877,") with two modifications, i.e., using CharLSTM word representation vectors instead of POS tag embeddings, and the first-order Eisner algorithm (Eisner, 2000) for projective decoding instead of the non-projective MST algorithm. Scoring architecture. Figure 2 shows the scoring architecture, consisting of four components. Input vectors. The ith input vector is composed of two parts: the word embedding and the CharLSTM word representation vector of wi . ei = emb(wi ) ⊕ CharLSTM(wi ) (1) where CharLSTM(wi ) is obtained by feeding wi into a BiLSTM and then concatenating the two last hidden vectors (Lample et al., 2016). We find that replacing POS tag embeddings with 3296 s(i, k, j) s(i, j) Biaffine rhi Triaffine rm j MLPh In other words, the model is trained based on simple head selection, without considering the tree structure at all, and losses of all words in a minibatch are accumulated. rhi 0 rsk 0 MLPm MLPh hi MLPs hk rm j MLPm Decoding. Having scores of all dependencies, we adopt the first-order Eisner algorithm with time complexity of O(n3 ) to find the optimal tree.   X y ∗ = arg max s(x, y) ≡ s(i, j) (5) 0 0 hj BiLSTM × 3 y i→j∈y . . . ei . . . ek . . . ej . . . Figure 2: Scoring architecture w"
2020.acl-main.302,D14-1081,0,0.115051,"contrasting development, we try to make a connection between pre-DL and DL techniques for graph-based parsing. Specifically, the first question to be addressed in this work is: can previously useful techniques such as structural learning and high-order modeling further improve the state-of-the-art2 biaffine parser, and if so, in which aspects are they helpful? For structural learning, we focus on the more complex and less popular TreeCRF instead of maxmargin training. The reason is two-fold. First, estimating probability distribution is the core issue in modern data-driven NLP methods (Le and Zuidema, 2014). The probability of a tree, i.e., p(y |x), is potentially more useful than an unbounded score s(x, y) for high-level NLP tasks when utilizing parsing outputs. Second, as a theoretically sound way to measure model confidence of subtrees, marginal probabilities can support Minimum Bayes Risk (MBR) decoding (Smith and Smith, 2007), and are also proven to be crucial for the important research line of token-level active learning based on partial trees (Li et al., 2016). One probable reason for the less popularity of TreeCRF, despite its usefulness, is due to the complexity and inefficiency of the"
2020.acl-main.302,P16-1033,1,0.912898,"ning. The reason is two-fold. First, estimating probability distribution is the core issue in modern data-driven NLP methods (Le and Zuidema, 2014). The probability of a tree, i.e., p(y |x), is potentially more useful than an unbounded score s(x, y) for high-level NLP tasks when utilizing parsing outputs. Second, as a theoretically sound way to measure model confidence of subtrees, marginal probabilities can support Minimum Bayes Risk (MBR) decoding (Smith and Smith, 2007), and are also proven to be crucial for the important research line of token-level active learning based on partial trees (Li et al., 2016). One probable reason for the less popularity of TreeCRF, despite its usefulness, is due to the complexity and inefficiency of the inside-outside algorithm, especially the outside algorithm. As far as we know, all existing works compute the inside and outside algorithms on CPUs. The inefficiency issue becomes more severe in the DL era, due to 2 Though many recent works report higher performance with extra resources, for example contextualized word representations learned from large-scale unlabeled texts under language model loss, they either adopt the same architecture or achieve similar perfo"
2020.acl-main.302,C12-2077,0,0.217529,"nd-order models were soon propose to incorporate scores of dependency pairs, such as adjacent-siblings (McDonald and Pereira, 2006) and grand-parent-child (Carreras, 2007; Koo and Collins, 2010), showing significant accuracy improvement yet with the cost of lower efficiency and more complex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics networks for automatically learning combinations of dozens of atomic features similar to Chen and Manning (2014), and for computing subtree scores. They show that incorporating second-order scores of adjacent-sibling subtrees significantly improved performance. Then, both Wang and Chang (2016) and Kiperwasser and Goldberg (2016) propose to utilize BiLSTM as an encoder and use minimal feature sets for scoring single d"
2020.acl-main.302,P05-1012,0,0.283507,"tures and differs from its neural counterpart in two major aspects. First, structural learning, i.e., explicit awareness of tree structure constraints during training, is indispensable. Most non-neural graph-based parsers adopt the max-margin training algorithm, which first predicts a highest-scoring tree with the current model, and then updates feature weights so that the correct tree has a higher score than the predicted tree. Second, high-order modeling brings significant accuracy gains. The basic first-order model factors the score of a tree into independent scores of single dependencies (McDonald et al., 2005a). Second-order models were soon propose to incorporate scores of dependency pairs, such as adjacent-siblings (McDonald and Pereira, 2006) and grand-parent-child (Carreras, 2007; Koo and Collins, 2010), showing significant accuracy improvement yet with the cost of lower efficiency and more complex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins"
2020.acl-main.302,E06-1011,0,0.854148,"cture constraints during training, is indispensable. Most non-neural graph-based parsers adopt the max-margin training algorithm, which first predicts a highest-scoring tree with the current model, and then updates feature weights so that the correct tree has a higher score than the predicted tree. Second, high-order modeling brings significant accuracy gains. The basic first-order model factors the score of a tree into independent scores of single dependencies (McDonald et al., 2005a). Second-order models were soon propose to incorporate scores of dependency pairs, such as adjacent-siblings (McDonald and Pereira, 2006) and grand-parent-child (Carreras, 2007; Koo and Collins, 2010), showing significant accuracy improvement yet with the cost of lower efficiency and more complex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c"
2020.acl-main.302,H05-1066,0,0.677681,"Missing"
2020.acl-main.302,J14-2001,0,0.0184305,"parsing. Moreover, we also find that marginal probabilities p(i → j |x) directly correspond to gradients after back-propagation with log Z(x) as the loss: ∂ log Z = ∂s(i, j) X p(y |x) = p(i → j |x) y:(i,j)∈y (11) which can be easily proved. For TreeCRF parsers, we perform MBR decoding (Smith and Smith, 2007) by replacing scores with marginal probabilities in the decoding algorithm, leading to a slight but consistent accuracy increase. 3.4 Handling Partial Annotation As an attractive research direction, studies show that it is more effective to construct or even collect partially labeled data (Nivre et al., 2014; Hwa, 1999; Pereira and Schabes, 1992), where a sentence may correspond to a partial tree |y p |&lt; n in the case of dependency parsing. Partial annotation can be very powerful when combined with active learning, because annotation cost can be greatly reduced if annotators only need to annotate sub-structures that are difficult for models. Li et al. (2016) present a detailed survey on this topic. Moreover, Peng et al. (2019) recently released a partially labeled multi-domain Chinese dependency treebank based on this idea. Then, the question is how to train models on partially labeled data. Li e"
2020.acl-main.302,P05-1013,0,0.0852854,"a proportion of random dependencies for each sentence (partial trees). Figure 6 shows the results. We can see that the performance gap is quite steady when we gradually reduce the number of training sentences. In contrast, the gap clearly becomes larger when each training sentence has less annotated dependencies. This shows that C RF 2 O is superior to the basic L OC in utilizing partial annotated data for model training. 4.4 Results on Universal Dependencies Table 3 compares different models on UD datasets, which contain a lot of non-projective trees. We adopt the pseudo-projective approach (Nivre and Nilsson, 2005) for handling the ubiquitous nonprojective trees of most languages. Basically, the idea is to transform non-projective trees into projective ones using more complex labels for postprocessing recovery. We can see that for the basic local parsers, the direct non-projective L OC MST and the pseudoprojective L OC achieve very similar performance. More importantly, both C RF and C RF 2 O produce consistent improvements over the baseline in many languages. On both UD2.2 and UD2.3, Our proposed C RF 2 O model achieves the highest accuracy for 10 languages among 12, and obtains significant improvement"
2020.acl-main.302,P15-1031,0,0.0184933,"econd, high-order modeling brings significant accuracy gains. The basic first-order model factors the score of a tree into independent scores of single dependencies (McDonald et al., 2005a). Second-order models were soon propose to incorporate scores of dependency pairs, such as adjacent-siblings (McDonald and Pereira, 2006) and grand-parent-child (Carreras, 2007; Koo and Collins, 2010), showing significant accuracy improvement yet with the cost of lower efficiency and more complex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics networks for automatically learning combinations of dozens of atomic features similar to Chen and Manning (2014), and for computing subtree scores. They show that incorporating second-order scores of adjacent-sibling subtree"
2020.acl-main.302,P92-1017,0,0.281353,"that marginal probabilities p(i → j |x) directly correspond to gradients after back-propagation with log Z(x) as the loss: ∂ log Z = ∂s(i, j) X p(y |x) = p(i → j |x) y:(i,j)∈y (11) which can be easily proved. For TreeCRF parsers, we perform MBR decoding (Smith and Smith, 2007) by replacing scores with marginal probabilities in the decoding algorithm, leading to a slight but consistent accuracy increase. 3.4 Handling Partial Annotation As an attractive research direction, studies show that it is more effective to construct or even collect partially labeled data (Nivre et al., 2014; Hwa, 1999; Pereira and Schabes, 1992), where a sentence may correspond to a partial tree |y p |&lt; n in the case of dependency parsing. Partial annotation can be very powerful when combined with active learning, because annotation cost can be greatly reduced if annotators only need to annotate sub-structures that are difficult for models. Li et al. (2016) present a detailed survey on this topic. Moreover, Peng et al. (2019) recently released a partially labeled multi-domain Chinese dependency treebank based on this idea. Then, the question is how to train models on partially labeled data. Li et al. (2016) propose to extend TreeCRF"
2020.acl-main.302,N18-1202,0,0.0277633,"L OC on the test data, where “†” means p &lt; 0.05 and “‡” means p &lt; 0.005. Biaffine17: Dozat and Manning (2017); F&K19: Falenska and Kuhn (2019); Li19: Li et al. (2019); Ji19: Ji et al. (2019); Zhang19: Zhang et al. (2019). sentences per second, which is able to meet the requirements of a real-time system. More discussions on efficiency are presented in Appendix A. 4.2 Main Results Table 1 lists the main results on the dev and test data. The trends on dev and test are mostly consistent. For a fair comparison with previous works, we only consider those without using extra resources such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). We can see that our baseline L OC achieves the best performance on both PTB and CoNLL09. On PTB, both C RF and C RF 2 O fail to improve 3300 94 86 93 85 C RF 2 O C RF L OC 92 91 72 100 200 300 400 70 C RF 2 O C RF L OC 84 83 100 200 300 400 C RF 2 O C RF L OC 68 66 100 200 300 400 Figure 5: Convergence curves (LAS vs. training epochs) on dev data of PTB, CoNLL09, and NLPCC19. SIB R P F UCM LCM PTB L OC 91.16 90.80 90.98 61.59 50.66 C RF 91.24 90.92 91.08 61.92 50.33 C RF 2 O 91.56 91.11 91.33 63.08 50.99 84 92 82 C RF 2 O (Dep) L OC (Dep) C RF 2 O (Sent) L OC ("
2020.acl-main.302,D07-1014,0,0.720599,"f so, in which aspects are they helpful? For structural learning, we focus on the more complex and less popular TreeCRF instead of maxmargin training. The reason is two-fold. First, estimating probability distribution is the core issue in modern data-driven NLP methods (Le and Zuidema, 2014). The probability of a tree, i.e., p(y |x), is potentially more useful than an unbounded score s(x, y) for high-level NLP tasks when utilizing parsing outputs. Second, as a theoretically sound way to measure model confidence of subtrees, marginal probabilities can support Minimum Bayes Risk (MBR) decoding (Smith and Smith, 2007), and are also proven to be crucial for the important research line of token-level active learning based on partial trees (Li et al., 2016). One probable reason for the less popularity of TreeCRF, despite its usefulness, is due to the complexity and inefficiency of the inside-outside algorithm, especially the outside algorithm. As far as we know, all existing works compute the inside and outside algorithms on CPUs. The inefficiency issue becomes more severe in the DL era, due to 2 Though many recent works report higher performance with extra resources, for example contextualized word represent"
2020.acl-main.302,P16-1218,0,0.0315983,"and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics networks for automatically learning combinations of dozens of atomic features similar to Chen and Manning (2014), and for computing subtree scores. They show that incorporating second-order scores of adjacent-sibling subtrees significantly improved performance. Then, both Wang and Chang (2016) and Kiperwasser and Goldberg (2016) propose to utilize BiLSTM as an encoder and use minimal feature sets for scoring single dependencies in a first-order parser. These three representative works all employ global max-margin training. Dozat and Manning (2017) propose a strong and efficient biaffine parser and obtain state-of-the-art accuracy on a variety of datasets and languages. The biaffine parser is also first-order and employs simpler and more efficient non-structural training via local head selection for each token (Zhang et al., 2017). Observing such contrasting development, we try to m"
2020.acl-main.302,P19-1454,0,0.064613,"ure 3 Computing TreeCRF Loss Efficiently The key to TreeCRF loss is how to efficiently compute log Z(x), as shown in Equation 8. This problem has been well solved long before the DL era for non-neural dependency parsing. Straightforwardly, we can directly extend the viterbi decoding algorithm by replacing max product with sum 4 Another way is to use one extra MLP for sibling representation, and re-use head and modifier representation from the basic first-order components, which however leads to inferior performance in our preliminary experiments. 5 We have also tried the approximate method of Wang et al. (2019), which uses three biaffine operations to simulate the interactions of three input vectors, but observed inferior performance. We omit the results due to the space limitation. product, and naturally obtain log Z(x) in the same polynomial time complexity. However, it is not enough to solely perform the inside algorithm for non-neural parsing, due to the inapplicability of the automatic differentiation mechanism. In order to obtain marginal probabilities and then feature weight gradients, we have to realize the more sophisticated outside algorithm, which is usually at least twice slower than the"
2020.acl-main.302,K18-2001,0,0.0745696,"Missing"
2020.acl-main.302,P19-1562,0,0.348224,"is not enough to solely perform the inside algorithm for non-neural parsing, due to the inapplicability of the automatic differentiation mechanism. In order to obtain marginal probabilities and then feature weight gradients, we have to realize the more sophisticated outside algorithm, which is usually at least twice slower than the inside algorithm. This may be the major reason for the less popularity of TreeCRF (vs. max-margin training) before the DL era. As far as we know, all previous works on neural TreeCRF parsing explicitly implement the insideoutside algorithm for gradient computation (Zhang et al., 2019; Jiang et al., 2018). To improve efficiency, computation is transferred from GPUs to CPUs with Cython programming. This work shows that the inside algorithm can be effectively batchified to fully utilize the power of GPUs. Figure 3 and Algorithm 1 together illustrate the batchified version of the second-order inside algorithm, which is a direct extension of the secondorder Eisner algorithm in McDonald and Pereira (2006) by replacing max product with sum product. We omit the generations of incomplete, complete, and sibling spans in the opposite direction from j to i for brevity. Basically, we"
2020.acl-main.302,L16-1262,0,\N,Missing
2020.acl-main.302,E17-1063,0,\N,Missing
2020.acl-main.338,P19-1140,0,0.0276113,"present a simpliﬁed graph neural network model, called graph convolutional networks (GCN), which has been exported to several tasks such as scene recognition (Yuan et al., 2019), 3668 semi-supervised node classiﬁcation (Zhang et al., 2019b), text-to-SQL parsing (Bogin et al., 2019) and relation extraction (Sahu et al., 2019). On this basis, some other improved Graph-based Neural Networks are proposed. Morris et al. (2019) propose a generalization of Graph-based Neural Networks, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. Cao et al. (2019) propose a novel Multi-channel Graph Neural Network model to learn alignment-oriented knowledge graph embeddings by robustly encoding two knowledge graphs via multiple channels. More recently, there exist several studies also adopting graph-based neural networks to ASC. For instance, Hou et al. (2019) and Zhang et al. (2019a) build GCN over the dependency tree of a sentence to exploit syntactical information and word dependencies for learning better aspect-related sentence representation for ASC. Different from all the above studies, this paper proposes a novel Cooperative Graph Attention Netw"
2020.acl-main.338,D17-1047,0,0.0234547,"sting studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to using external information to improve the performance of ASC. He et al. (2018b) and Chen and Qian (2019) incorporate the knowledge from document-le"
2020.acl-main.338,P19-1052,0,0.0183307,"information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to using external information to improve the performance of ASC. He et al. (2018b) and Chen and Qian (2019) incorporate the knowledge from document-level sentiment classiﬁcation to improve the performance of ASC. Ma et al. (2018) propose an extension of LSTM to integrate the commonsense knowledge into the recurrent encoder for improving the performance of ASC. In addition, it is worthwhile to note that Hazarika et al. (2018) also investigate the inter-aspect sentiment dependency for ASC, but is limited to capture this information inside a single sentence. In summary, all the above studies ignore the document-level sentiment preference information, which can be leveraged to effectively mitigate the"
2020.acl-main.338,N19-1423,0,0.00741175,"es each new vertex vector h tex vi by considering neighboring vertices’ vectors {hj }Ij=1 with the following formulas: ˆ i = tanh( h I  αij W hj + b) j=1 exp(f (w [W hi ; W hj ])) αij = I t=1 exp(f (w  [W h (1) i ; W ht ])) where αij is the attention weight (i.e., the edge weight) between vertex vi and vertex vj . f (·) is a LeakyReLU activation function. [; ] denotes vector concatenation. W ∈ Rd×d and w ∈ R2d are the trainable parameters. In the following, we will illustrate the ﬁve main components of our CoGAN approach respectively. 3.2 Encoding Block As a text encoding mechanism, BERT (Devlin et al., 2019) can be ﬁne-tuned to create state-of-the-art models for a range of NLP tasks, e.g., text classiﬁcation and natural language inference. In our approach, we use BERT-base2 (uncased) model to encode both the aspect and the sentence as follows. • Aspect Encoding. Since an aspect ak consists of an entity eentity and an attribute eattribute (Pontiki et al., 2015), we process the entity-attribute pair (eentity , eattribute ) into the input pair format of BERT as: [CLS] eentity [SEP] eattribute [SEP] Then, we feed the entity-attribute pair into BERT and regard the mark “[CLS]” representation as the as"
2020.acl-main.338,C18-1096,0,0.0832994,"networks. Aspect Sentiment Classiﬁcation. The ASC task aims to predict the sentiment polarity for each aspect discussed inside a sentence. Existing studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to u"
2020.acl-main.338,P18-2092,0,0.0685403,"networks. Aspect Sentiment Classiﬁcation. The ASC task aims to predict the sentiment polarity for each aspect discussed inside a sentence. Existing studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to u"
2020.acl-main.338,P19-1048,0,0.0578973,"n task to a sentence pair classiﬁcation task. In our implementation, we regard the pair of sentence and its aspect as the input pair of BERT-base model (Devlin et al., 2018) for performing ASC. 8) CADMN. This approach employs attention model to attend on relevant aspects for enhancing the aspect representation. This is a state-of-the-art approach proposed by Song et al. (2019). 9) IMN. This approach is a multi-task learning approach, which employs a novel message passing mechanism to better exploit the correlation among the tasks related to ASC. This is a state-of-the-art approach proposed by He et al. (2019). 10) BERT-QA. This approach is an extension of the above BERT baseline proposed by Sun et al. (2019). In this study, we adopt BERTpair-QA-M in our implementation. This is another state-of-the-art approach for ASC. 11) Sentiue. This is the best-performed system in SemEval-2015 Task 12 (Saias, 2015), which achieves the best accuracy scores in both the laptop15 and restaurant15 domains. 12) XRCE. This is the best-performed system in SemEval-2016 Task 5 (Pontiki et al., 2016), which achieves the best accuracy score in the restaurant16 domain. 13) IIT-TUDA. This is also the best-performed system i"
2020.acl-main.338,S16-1174,0,0.0691356,"Missing"
2020.acl-main.338,P10-1043,1,0.697296,"ITY, polarity = positive - Category = FOOD#PRICES, polarity = positive S3: The lava cake dessert was incredible and I recommend it. - Category = FOOD#QUALITY, polarity = positive Figure 1: Two documents from SemEval 2016 (Pontiki et al. (2016)) datasets, where aspect category is deﬁned as the entity E and attribute A pair (i.e., E#A). Red lines denote the intra-aspect sentiment consistency and blue lines denote the inter-aspect sentiment tendency. Introduction Aspect Sentiment Classiﬁcation (ASC), a ﬁnegrained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang"
2020.acl-main.338,S16-1002,0,0.132215,"Missing"
2020.acl-main.338,S15-2082,0,0.248872,"Missing"
2020.acl-main.338,P19-1423,0,0.043128,"e document-level sentiment preference information, which can be leveraged to effectively mitigate the information deﬁciency problem in ASC. Graph-based Neural Networks. In recent years, graph-based neural networks have received more and more attentions. As a pioneer, Kipf and Welling (2017) present a simpliﬁed graph neural network model, called graph convolutional networks (GCN), which has been exported to several tasks such as scene recognition (Yuan et al., 2019), 3668 semi-supervised node classiﬁcation (Zhang et al., 2019b), text-to-SQL parsing (Bogin et al., 2019) and relation extraction (Sahu et al., 2019). On this basis, some other improved Graph-based Neural Networks are proposed. Morris et al. (2019) propose a generalization of Graph-based Neural Networks, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. Cao et al. (2019) propose a novel Multi-channel Graph Neural Network model to learn alignment-oriented knowledge graph embeddings by robustly encoding two knowledge graphs via multiple channels. More recently, there exist several studies also adopting graph-based neural networks to ASC. For instance, Hou et al. (2019) and Zh"
2020.acl-main.338,S15-2130,0,0.15077,"entation. This is a state-of-the-art approach proposed by Song et al. (2019). 9) IMN. This approach is a multi-task learning approach, which employs a novel message passing mechanism to better exploit the correlation among the tasks related to ASC. This is a state-of-the-art approach proposed by He et al. (2019). 10) BERT-QA. This approach is an extension of the above BERT baseline proposed by Sun et al. (2019). In this study, we adopt BERTpair-QA-M in our implementation. This is another state-of-the-art approach for ASC. 11) Sentiue. This is the best-performed system in SemEval-2015 Task 12 (Saias, 2015), which achieves the best accuracy scores in both the laptop15 and restaurant15 domains. 12) XRCE. This is the best-performed system in SemEval-2016 Task 5 (Pontiki et al., 2016), which achieves the best accuracy score in the restaurant16 domain. 13) IIT-TUDA. This is also the best-performed system in SemEval-2016 Task 5 (Pontiki et al., 2016), while achieving the best accuracy score in the laptop16 domain. 15) CoGAN w/o Intra-Aspect Consistency. Our approach only modeling Inter-Aspect Tendency. 16) CoGAN w/o Inter-Aspect Tendency. Our approach only modeling Intra-Aspect Consistency. 17) CoGAN"
2020.acl-main.338,N19-1035,0,0.360347,"S] Excellent food … [SEP] What do you … Encoding Block  ൌ ܹݎ  ܾ ݒො Layer L ܧ sentence vector ݒ ܵ ݒොଶ ݒොଵ Layer 2 … ܧሾௌாሿ … ܽ [SEP] … Shared BERT ܧଵ ୀଵ Layer 1 ܧሾௌሿ quality … ܧሾௌாሿ … ܧଵ ܵାଵ ௧௧௨௧ [SEP] ൌ ሺሺݒ  ߙ ሺ ߙ ܹ ݒ ሻሻ  ܾሻ ܵ Aspect Input: FOOD#QUALITY ௧௧௬ ூᇲ ሺ௧ሻ ݒො ூ ܵଷ ሺ௧ሻ ݒො ܵଶ ൌ ሺ ߙ ܹఈ ݒ  ܾఈ ሻ ୀଵ Inter-Aspect Tendency Modeling Block Interaction Block Figure 2: The overall framework of our proposed Cooperative Graph Attention Networks (CoGAN). • Sentence Encoding. We borrow the approach proposed by Sun et al. (2019) to generate the aspectrelated sentence representation, which has achieved promising performance for the ASC task. Following Sun et al. (2019), we ﬁrst process the sentence si and its corresponding aspect ak into the input pair format of BERT as: [CLS] si [SEP] question(ak ) [SEP] where question(·) denotes the construction of auxiliary question sentence for aspect ak proposed by Sun et al. (2019). For example, the auxiliary sentence for aspect FOOD#PRICE is constructed as “what do you think of the food and price?”. Then, we similarly feed the above pair into BERT (shared with aspect encoding)"
2020.acl-main.338,C16-1311,0,0.576477,"n Aspect Sentiment Classiﬁcation (ASC), a ﬁnegrained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang et al., 2019c) and Question Answering (Wang et al., 2019a). In the literature, given the ASC datasets (Pontiki et al. (2016)) where aspects (i.e., entity and attribute) are manually annotated comprehensively sentence by sentence, previous studies model the aspect sentiment independently sentence by sentence, which suffer from the problem of ignoring the document-level sentiment preference information. In this study, we argue that such documentlevel sentiment"
2020.acl-main.338,D16-1021,0,0.477432,"n Aspect Sentiment Classiﬁcation (ASC), a ﬁnegrained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang et al., 2019c) and Question Answering (Wang et al., 2019a). In the literature, given the ASC datasets (Pontiki et al. (2016)) where aspects (i.e., entity and attribute) are manually annotated comprehensively sentence by sentence, previous studies model the aspect sentiment independently sentence by sentence, which suffer from the problem of ignoring the document-level sentiment preference information. In this study, we argue that such documentlevel sentiment"
2020.acl-main.338,P19-1345,1,0.819933,"010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang et al., 2019c) and Question Answering (Wang et al., 2019a). In the literature, given the ASC datasets (Pontiki et al. (2016)) where aspects (i.e., entity and attribute) are manually annotated comprehensively sentence by sentence, previous studies model the aspect sentiment independently sentence by sentence, which suffer from the problem of ignoring the document-level sentiment preference information. In this study, we argue that such documentlevel sentiment preference information is crucial to 3667 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3667–3677 c"
2020.acl-main.338,D19-1560,1,0.883168,"Missing"
2020.acl-main.338,D16-1058,0,0.0694742,"hree top-performed systems from SemEval-2015 Task 12 and SemEval2016 Task 5 (Pontiki et al., 2015, 2016). 2 Related Work In this section, we ﬁrst review the Aspect Sentiment Classiﬁcation (ASC) task, and then introduce the related studies on graph-based neural networks. Aspect Sentiment Classiﬁcation. The ASC task aims to predict the sentiment polarity for each aspect discussed inside a sentence. Existing studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information"
2020.acl-main.338,D19-1464,0,0.0482997,"e this information inside a single sentence. In summary, all the above studies ignore the document-level sentiment preference information, which can be leveraged to effectively mitigate the information deﬁciency problem in ASC. Graph-based Neural Networks. In recent years, graph-based neural networks have received more and more attentions. As a pioneer, Kipf and Welling (2017) present a simpliﬁed graph neural network model, called graph convolutional networks (GCN), which has been exported to several tasks such as scene recognition (Yuan et al., 2019), 3668 semi-supervised node classiﬁcation (Zhang et al., 2019b), text-to-SQL parsing (Bogin et al., 2019) and relation extraction (Sahu et al., 2019). On this basis, some other improved Graph-based Neural Networks are proposed. Morris et al. (2019) propose a generalization of Graph-based Neural Networks, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. Cao et al. (2019) propose a novel Multi-channel Graph Neural Network model to learn alignment-oriented knowledge graph embeddings by robustly encoding two knowledge graphs via multiple channels. More recently, there exist several studies"
2020.coling-main.148,D14-1164,0,0.0259654,"ong baseline. In addition to the development of models for sentence encoding, studies on relieving the dependence on human-annotated data are also popular. Distant supervision is proposed by Mintz et al. (2009) to automatically build labeled data for RE. Although a lot of approaches have been proposed to relieve the wrong labeling problem in distant supervised data (Takamatsu et al., 2012; Lin et al., 2016; He et al., 2020), there is a gap between models that trained on supervised data and distant supervision data. Using some carefully selected human-annotated examples as partial supervision, Angeli et al. (2014) combines the reliablity from human-annotated data and the large coverage from distant supervision data. Based on the directionality of relations, Xu et al. (2016) proposes a data augmentation method to alleviate the sparse problem. Vashishth et al. (2018) generates aliases for relation names via phrase-level paraphrases. Beltagy et al. (2019) proposes to combine the distant supervision data with an existing human-annotated RE data. None of the above studies has used the paraphrase sentences. In this paper, we propose to enlarge the coverage of relation expressions by building a relational par"
2020.coling-main.148,N19-1184,0,0.012125,"distant supervised data (Takamatsu et al., 2012; Lin et al., 2016; He et al., 2020), there is a gap between models that trained on supervised data and distant supervision data. Using some carefully selected human-annotated examples as partial supervision, Angeli et al. (2014) combines the reliablity from human-annotated data and the large coverage from distant supervision data. Based on the directionality of relations, Xu et al. (2016) proposes a data augmentation method to alleviate the sparse problem. Vashishth et al. (2018) generates aliases for relation names via phrase-level paraphrases. Beltagy et al. (2019) proposes to combine the distant supervision data with an existing human-annotated RE data. None of the above studies has used the paraphrase sentences. In this paper, we propose to enlarge the coverage of relation expressions by building a relational paraphrase data for an existing RE data. 1695 7 Conclusion In this paper, we show that using the newly built task-specific paraphrase data can have a substantial effect on the performance of relation extraction. In particular, we demonstrate that our proposed system consistently outperforms the strong baseline system using BERT. The gains we find"
2020.coling-main.148,P15-1061,0,0.114513,"show that our approach is effective to improve the performance on relation extraction, even compared with a strong baseline. 1 Introduction Relation Extraction (RE) is an important task in Information Extraction, which identifies semantic relations between entities in text (Zelenko et al., 2003; Zhou et al., 2005; Mintz et al., 2009). The task becomes a typical classification problem if an entity pair in a text is given. In recent years, supervised models have achieved great progress on this task with the help of a massive amount of manually annotated high-quality data (Zeng et al., 2014; dos Santos et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). However, diverse expressions for a same semantic relation are difficult to be fully covered by humanannotated data. For example, sentences (1) “Steve Jobs co-founded Apple Computer.”; (2) “Steve Jobs was the co-founder of Apple Computer.”; and (3) “Steve Jobs started Apple Computer with Wozniak.” express the same semantic relation between person “Steve Jobs” and company “Apple Computer” in different wording. Generally, it is difficult for the supervised model trained on sentence (1) and (2) to recognize the semantic relation in sentence (3). To sol"
2020.coling-main.148,D18-1045,0,0.0224519,"tion types (not no relation) is small (about 13k). And the average number of sentences for each relation fact (an individual triple <head entity, tail entity, relation type&gt;) is less than 2. Hence, generating paraphrase expressions for each labeled sentence is expected to enrich the annotated data. 2.2 Generating Relational Paraphrase Sentences We introduce Neural Machine Translation (NMT) technology with back-translation to help us automatically generate possible paraphrase sentences. Back-translation is an effective method to augment the parallel training corpus in NMT (Fadaee et al., 2017; Edunov et al., 2018). In this procedure, there are two challenges: (1) How to guarantee the variety of paraphrase sentences; (2) How to label the entities in paraphrase sentences. For the first challenge, we view each NMT system as an individual knowledge base which translates a sentence in its own way. Hence, we take more than one public NMT systems to perform back-translation on the training set of TACRED. As the NMT systems provide end-to-end translations, entities in sentences may be replaced by other words after back-translation. As shown in Figure 2, the head entity “all basotho convention” has been transla"
2020.coling-main.148,P17-2090,0,0.0139032,"with meaningful relation types (not no relation) is small (about 13k). And the average number of sentences for each relation fact (an individual triple <head entity, tail entity, relation type&gt;) is less than 2. Hence, generating paraphrase expressions for each labeled sentence is expected to enrich the annotated data. 2.2 Generating Relational Paraphrase Sentences We introduce Neural Machine Translation (NMT) technology with back-translation to help us automatically generate possible paraphrase sentences. Back-translation is an effective method to augment the parallel training corpus in NMT (Fadaee et al., 2017; Edunov et al., 2018). In this procedure, there are two challenges: (1) How to guarantee the variety of paraphrase sentences; (2) How to label the entities in paraphrase sentences. For the first challenge, we view each NMT system as an individual knowledge base which translates a sentence in its own way. Hence, we take more than one public NMT systems to perform back-translation on the training set of TACRED. As the NMT systems provide end-to-end translations, entities in sentences may be replaced by other words after back-translation. As shown in Figure 2, the head entity “all basotho conven"
2020.coling-main.148,P13-1151,0,0.0256184,"scalable, with regard to both time and money. The second one is to adopt the Distant Supervision (DS) mechanism to automatically build a large-scale labeled data (Mintz et al., 2009). However, with the strong assumption that all sentences containing two given entities in a relation triple express the same relation, DS may result in the severe wrong labeling problem. In this paper, we use an alternative solution that uses a paraphrase data which collects sentences conveying the same meaning in different wording. In the literature, there exist many paraphrase datasets, such as Simple Wikipedia (Kauchak, 2013), Twitter URL corpus (Lan et al., 2017), and Para-NMT (Wieting and Gimpel, 2018). However, these general paraphrase datasets do This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1687 Proceedings of the 28th International Conference on Computational Linguistics, pages 1687–1698 Barcelona, Spain (Online), December 8-13, 2020 NMT EN to CN Triple: <All Basotho Convention, org:founded_by, Tom Thabane &gt; CN to EN Entity Alignment RE Data ReP Data Figure 1: Framework of building the relational paraphrase"
2020.coling-main.148,D17-1126,0,0.0269444,"nd money. The second one is to adopt the Distant Supervision (DS) mechanism to automatically build a large-scale labeled data (Mintz et al., 2009). However, with the strong assumption that all sentences containing two given entities in a relation triple express the same relation, DS may result in the severe wrong labeling problem. In this paper, we use an alternative solution that uses a paraphrase data which collects sentences conveying the same meaning in different wording. In the literature, there exist many paraphrase datasets, such as Simple Wikipedia (Kauchak, 2013), Twitter URL corpus (Lan et al., 2017), and Para-NMT (Wieting and Gimpel, 2018). However, these general paraphrase datasets do This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1687 Proceedings of the 28th International Conference on Computational Linguistics, pages 1687–1698 Barcelona, Spain (Online), December 8-13, 2020 NMT EN to CN Triple: <All Basotho Convention, org:founded_by, Tom Thabane &gt; CN to EN Entity Alignment RE Data ReP Data Figure 1: Framework of building the relational paraphrase data. EN=English, CN=Chinese. #1 [tom"
2020.coling-main.148,P16-1200,0,0.0874022,"ol(sx ). x∈B (5) Bag-One. Different from outputting a maximum value on each dimension in Bag-Max method, BagOne outputs the best representation from one of three sentences in B by calculating the probability on its gold relation type after a softmax layer. sB = sx0 , (6) 0 x = argmax p(rx |x, θ), x∈B where p() outputs the probability of relation type rx for the input sentence x under current model parameters θ. Bag-Avg. Similar to Bag-Max, Bag-Avg method adds an averaged pooling layer after encoding sentences in B: 1 X sB = sx . (7) |B| x∈B Bag-Att. Inspired by the attention mechanism used in Lin et al. (2016), we add an attention layer to output bag-level representations for sentences in B. First we generate attention weights α for sentences in B by calculating how well it matches with their gold relation type. Then, we output a weighted sum of representations: X sB = αx sx , (8) x∈B exp(ex ) , x0 ∈B exp(ex0 ) αx = P ex = sx Ar, where ex measures how well sx matches with the query vector r ∈ R2d which is the representation of the gold relation of x, and A ∈ R2d×2d represents a diagonal matrix. 3.3 Relation Extractor After obtaining the relational representations, we build the relation extractor fo"
2020.coling-main.148,D15-1166,0,0.00992827,"or relation “org:founded by” between two entities. not have explicit clues for entities and relations. Our preliminary experimental results show that using such paraphrase datasets harms the performance of relation extraction. Therefore, it is difficult to learn useful information for relation extraction from the general paraphrase data. In this paper, we propose to automatically build a task-specific paraphrase data which has the explicit clues instead of using general paraphrase datasets for relation extraction. Motivated by the recent success of deep neural networks in machine translation (Luong et al., 2015; Wu et al., 2016; Vaswani et al., 2017), we adopt more than one Neural Machine Translation (NMT) systems to generate possible paraphrases via back-translation for each sentence in an existing RE data. The back-translation is the procedure in which a system translates a sentence into another language, and then translates back to the original language. However, we can not convey the annotations of entities during back-translation since word alignment information is unavailable. To solve this problem, we design a contextual similarity based method to align entities between the human-annotated se"
2020.coling-main.148,P09-1113,0,0.754415,"se data. Then, we propose a novel model to learn the information of diverse relation expressions. In our model, we try to capture this information on the paraphrases via a joint learning framework. Finally, we conduct experiments on a widely used dataset and the experimental results show that our approach is effective to improve the performance on relation extraction, even compared with a strong baseline. 1 Introduction Relation Extraction (RE) is an important task in Information Extraction, which identifies semantic relations between entities in text (Zelenko et al., 2003; Zhou et al., 2005; Mintz et al., 2009). The task becomes a typical classification problem if an entity pair in a text is given. In recent years, supervised models have achieved great progress on this task with the help of a massive amount of manually annotated high-quality data (Zeng et al., 2014; dos Santos et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). However, diverse expressions for a same semantic relation are difficult to be fully covered by humanannotated data. For example, sentences (1) “Steve Jobs co-founded Apple Computer.”; (2) “Steve Jobs was the co-founder of Apple Computer.”; and (3) “Steve Jobs started Ap"
2020.coling-main.148,P16-1105,0,0.131733,"h is effective to improve the performance on relation extraction, even compared with a strong baseline. 1 Introduction Relation Extraction (RE) is an important task in Information Extraction, which identifies semantic relations between entities in text (Zelenko et al., 2003; Zhou et al., 2005; Mintz et al., 2009). The task becomes a typical classification problem if an entity pair in a text is given. In recent years, supervised models have achieved great progress on this task with the help of a massive amount of manually annotated high-quality data (Zeng et al., 2014; dos Santos et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). However, diverse expressions for a same semantic relation are difficult to be fully covered by humanannotated data. For example, sentences (1) “Steve Jobs co-founded Apple Computer.”; (2) “Steve Jobs was the co-founder of Apple Computer.”; and (3) “Steve Jobs started Apple Computer with Wozniak.” express the same semantic relation between person “Steve Jobs” and company “Apple Computer” in different wording. Generally, it is difficult for the supervised model trained on sentence (1) and (2) to recognize the semantic relation in sentence (3). To solve the above challenge,"
2020.coling-main.148,W15-1506,0,0.0263959,"oximately equal sets according to the sorted relations, a Short set and a Long set. We find that our system achieves the more significant improvement (+1.55) on the Long set than the Short set (+0.39). The reason might be that the NMT systems have more chances to generate different expressions of relations for the sentences with longer entity distance. 6 Related Work Recent years, with the development of neural networks, the neural models for relation extraction attract many researchers spending time on improving the performance. Zeng et al. (2014) proposes a CNN model with position features. Nguyen and Grishman (2015) improves the CNN model with multiple window sizes. Margin based ranking loss is used instead of cross-entropy in dos Santos et al. (2015). There are also many researchers focusing on other neural networks like RNN (Zhang and Wang, 2015), LSTM (Xu et al., 2015; Tai et al., 2015; Miwa and Bansal, 2016) and GCN (Zhang et al., 2018). Recently, transfer learning from pre-trained model like BERT to downstream supervised tasks is popular. For relation extraction, the main challenge of applying BERT is how to model the input sentences in an entity-aware way. Wang et al. (2019) adds relative position"
2020.coling-main.148,P19-1279,0,0.315773,"four sentences into one input unit. As shown in Figure 4, there are three key components in our system: (1) A sentence encoder, which encodes sentences into distributed representations; (2) A multiinstance learning module, which models three paraphrase sentences from one input unit into a mixed distributed representation; (3) A relation extractor, where the input representations are classified into different relations. 3.1 BERT-based Sentence Encoder When using the pretrained model BERT, the fine-tuning based approach gives impressive performance in many tasks (Devlin et al., 2018). Following Soares et al. (2019), we augment each sentence with four reserved tokens ([E1], [/E1], [E2] and [/E2]) to mark the beginning and ending of subject entity and object entity. Then, we concatenate the outputs of [E1] and [E2] as relational representation. Figure 5 illustrates the input and output of the sentence encoder used in this paper. Formally, we first build the input sentence as x: x = [CLS] x0 . . . [E1] xi . . . xj [/E1] . . . [E2] xk . . . xl [/E2] . . . xn [SEP], (2) where [CLS] and [SEP] are two special tokens used in BERT, and xi . . . xj , xk . . . xl are words of two entities, respectively. Let f (θ)"
2020.coling-main.148,P15-1150,0,0.0938516,"icit statement. 1693 λ F1 0.0 68.94 0.1 69.22 0.2 69.31 0.4 69.25 0.6 69.02 0.8 68.76 1.0 68.53 Table 3: Results of tuning with λ on the development set. λ = 0.0 equals to the baseline system. Systems Baseline (ReP-GOLD) ReP-AUTO ReP-GOLD ∪ ReP-AUTO ReP-GOLD + Google ReP-GOLD + Baidu ReP-GOLD + Xiaoniu ReP-GOLD + Bag-Max ReP-GOLD + Bag-One ReP-GOLD + Bag-Avg ReP-GOLD + Bag-Att F1 68.67 66.75 68.53 69.37 69.12 69.24 69.45 69.46 69.60 69.38 Table 4: Comparison with Baseline on test set. 4.2 Systems CNN-PE † (Zeng et al., 2014) PCNN † (Zeng et al., 2015) SDP-LSTM ‡ (Xu et al., 2015) Tree-LSTM ‡ (Tai et al., 2015) PA-LSTM (Zhang et al., 2017) SA-LSTM+D (Yu et al., 2019) C-GCN + PA-LSTM (Zhang et al., 2018) MTB on BERT large (Soares et al., 2019) Baseline on BERT base ReP-GOLD + Bag-Avg on BERT base Baseline on BERT large ReP-GOLD + Bag-Avg on BERT large F1 61.1 62.0 58.7 62.7 65.1 67.6 68.2 71.5 68.7 69.6 70.2 70.8 Table 5: Comparion with previous results. † marks results reported in Yu et al. (2019); ‡ marks results reported in Zhang et al. (2017). Experimental Results Main results. As shown in Table 4, we compare our systems with the baseline system, where all the systems use BERTbase . The baseline"
2020.coling-main.148,P12-1076,0,0.0328516,". (2019) adds relative position features in a self-attention layer. Soares et al. (2019) directly inserts four reserved tags in sentences to represent borders of entities. We also build our system based on BERT, which is a very strong baseline. In addition to the development of models for sentence encoding, studies on relieving the dependence on human-annotated data are also popular. Distant supervision is proposed by Mintz et al. (2009) to automatically build labeled data for RE. Although a lot of approaches have been proposed to relieve the wrong labeling problem in distant supervised data (Takamatsu et al., 2012; Lin et al., 2016; He et al., 2020), there is a gap between models that trained on supervised data and distant supervision data. Using some carefully selected human-annotated examples as partial supervision, Angeli et al. (2014) combines the reliablity from human-annotated data and the large coverage from distant supervision data. Based on the directionality of relations, Xu et al. (2016) proposes a data augmentation method to alleviate the sparse problem. Vashishth et al. (2018) generates aliases for relation names via phrase-level paraphrases. Beltagy et al. (2019) proposes to combine the d"
2020.coling-main.148,D18-1157,0,0.0167054,". Although a lot of approaches have been proposed to relieve the wrong labeling problem in distant supervised data (Takamatsu et al., 2012; Lin et al., 2016; He et al., 2020), there is a gap between models that trained on supervised data and distant supervision data. Using some carefully selected human-annotated examples as partial supervision, Angeli et al. (2014) combines the reliablity from human-annotated data and the large coverage from distant supervision data. Based on the directionality of relations, Xu et al. (2016) proposes a data augmentation method to alleviate the sparse problem. Vashishth et al. (2018) generates aliases for relation names via phrase-level paraphrases. Beltagy et al. (2019) proposes to combine the distant supervision data with an existing human-annotated RE data. None of the above studies has used the paraphrase sentences. In this paper, we propose to enlarge the coverage of relation expressions by building a relational paraphrase data for an existing RE data. 1695 7 Conclusion In this paper, we show that using the newly built task-specific paraphrase data can have a substantial effect on the performance of relation extraction. In particular, we demonstrate that our proposed"
2020.coling-main.148,P19-1132,0,0.057543,"Missing"
2020.coling-main.148,P18-1042,0,0.0214258,"opt the Distant Supervision (DS) mechanism to automatically build a large-scale labeled data (Mintz et al., 2009). However, with the strong assumption that all sentences containing two given entities in a relation triple express the same relation, DS may result in the severe wrong labeling problem. In this paper, we use an alternative solution that uses a paraphrase data which collects sentences conveying the same meaning in different wording. In the literature, there exist many paraphrase datasets, such as Simple Wikipedia (Kauchak, 2013), Twitter URL corpus (Lan et al., 2017), and Para-NMT (Wieting and Gimpel, 2018). However, these general paraphrase datasets do This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1687 Proceedings of the 28th International Conference on Computational Linguistics, pages 1687–1698 Barcelona, Spain (Online), December 8-13, 2020 NMT EN to CN Triple: <All Basotho Convention, org:founded_by, Tom Thabane &gt; CN to EN Entity Alignment RE Data ReP Data Figure 1: Framework of building the relational paraphrase data. EN=English, CN=Chinese. #1 [tom thabane] , who set up the [all basotho co"
2020.coling-main.148,D15-1206,0,0.166032,"ng experiments without an explicit statement. 1693 λ F1 0.0 68.94 0.1 69.22 0.2 69.31 0.4 69.25 0.6 69.02 0.8 68.76 1.0 68.53 Table 3: Results of tuning with λ on the development set. λ = 0.0 equals to the baseline system. Systems Baseline (ReP-GOLD) ReP-AUTO ReP-GOLD ∪ ReP-AUTO ReP-GOLD + Google ReP-GOLD + Baidu ReP-GOLD + Xiaoniu ReP-GOLD + Bag-Max ReP-GOLD + Bag-One ReP-GOLD + Bag-Avg ReP-GOLD + Bag-Att F1 68.67 66.75 68.53 69.37 69.12 69.24 69.45 69.46 69.60 69.38 Table 4: Comparison with Baseline on test set. 4.2 Systems CNN-PE † (Zeng et al., 2014) PCNN † (Zeng et al., 2015) SDP-LSTM ‡ (Xu et al., 2015) Tree-LSTM ‡ (Tai et al., 2015) PA-LSTM (Zhang et al., 2017) SA-LSTM+D (Yu et al., 2019) C-GCN + PA-LSTM (Zhang et al., 2018) MTB on BERT large (Soares et al., 2019) Baseline on BERT base ReP-GOLD + Bag-Avg on BERT base Baseline on BERT large ReP-GOLD + Bag-Avg on BERT large F1 61.1 62.0 58.7 62.7 65.1 67.6 68.2 71.5 68.7 69.6 70.2 70.8 Table 5: Comparion with previous results. † marks results reported in Yu et al. (2019); ‡ marks results reported in Zhang et al. (2017). Experimental Results Main results. As shown in Table 4, we compare our systems with the baseline system, where all the syste"
2020.coling-main.148,C16-1138,0,0.0186362,"ervision is proposed by Mintz et al. (2009) to automatically build labeled data for RE. Although a lot of approaches have been proposed to relieve the wrong labeling problem in distant supervised data (Takamatsu et al., 2012; Lin et al., 2016; He et al., 2020), there is a gap between models that trained on supervised data and distant supervision data. Using some carefully selected human-annotated examples as partial supervision, Angeli et al. (2014) combines the reliablity from human-annotated data and the large coverage from distant supervision data. Based on the directionality of relations, Xu et al. (2016) proposes a data augmentation method to alleviate the sparse problem. Vashishth et al. (2018) generates aliases for relation names via phrase-level paraphrases. Beltagy et al. (2019) proposes to combine the distant supervision data with an existing human-annotated RE data. None of the above studies has used the paraphrase sentences. In this paper, we propose to enlarge the coverage of relation expressions by building a relational paraphrase data for an existing RE data. 1695 7 Conclusion In this paper, we show that using the newly built task-specific paraphrase data can have a substantial effe"
2020.coling-main.148,C14-1220,0,0.612624,"e experimental results show that our approach is effective to improve the performance on relation extraction, even compared with a strong baseline. 1 Introduction Relation Extraction (RE) is an important task in Information Extraction, which identifies semantic relations between entities in text (Zelenko et al., 2003; Zhou et al., 2005; Mintz et al., 2009). The task becomes a typical classification problem if an entity pair in a text is given. In recent years, supervised models have achieved great progress on this task with the help of a massive amount of manually annotated high-quality data (Zeng et al., 2014; dos Santos et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). However, diverse expressions for a same semantic relation are difficult to be fully covered by humanannotated data. For example, sentences (1) “Steve Jobs co-founded Apple Computer.”; (2) “Steve Jobs was the co-founder of Apple Computer.”; and (3) “Steve Jobs started Apple Computer with Wozniak.” express the same semantic relation between person “Steve Jobs” and company “Apple Computer” in different wording. Generally, it is difficult for the supervised model trained on sentence (1) and (2) to recognize the semantic relatio"
2020.coling-main.148,D15-1203,0,0.0789901,"Missing"
2020.coling-main.148,D17-1004,0,0.33522,"ve the performance on relation extraction, even compared with a strong baseline. 1 Introduction Relation Extraction (RE) is an important task in Information Extraction, which identifies semantic relations between entities in text (Zelenko et al., 2003; Zhou et al., 2005; Mintz et al., 2009). The task becomes a typical classification problem if an entity pair in a text is given. In recent years, supervised models have achieved great progress on this task with the help of a massive amount of manually annotated high-quality data (Zeng et al., 2014; dos Santos et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). However, diverse expressions for a same semantic relation are difficult to be fully covered by humanannotated data. For example, sentences (1) “Steve Jobs co-founded Apple Computer.”; (2) “Steve Jobs was the co-founder of Apple Computer.”; and (3) “Steve Jobs started Apple Computer with Wozniak.” express the same semantic relation between person “Steve Jobs” and company “Apple Computer” in different wording. Generally, it is difficult for the supervised model trained on sentence (1) and (2) to recognize the semantic relation in sentence (3). To solve the above challenge, we can use two possi"
2020.coling-main.148,D18-1244,0,0.205949,"al applications, we directly perform relation extraction by Equation (9) on input sentences, which means no extra paraphrase sentences are required. 4 4.1 Experiments Experimental Settings Datasets and evaluation. In our experiments, we use TACRED and the newly built ReP data. For the baseline system, we use ReP-GOLD (training set of TACRED) as training data. For our approaches, we use the ReP data as training data. We use the development set and test set in TACRED to do evaluation. Statistics of TACRED and the ReP data are described in Table 1. Following previous studies (Zhang et al., 2017; Zhang et al., 2018; Soares et al., 2019), we report micro average F1 scores. We run 3 times with random seeds to initialize the model and report the average results. Hyperparapmeters After tuning the hyperparameters on the development set, we choose the following settings: batch size is 16, learning rate is 3e-5 with Adam, and training epoch is [1, 2, 3, 4, 5]. We use PyTorch (Paszke et al., 2019) as our machine learning library and the architecture of BERT from Wolf et al. (2019). Two versions of pretrained BERT models (Devlin et al., 2018), BERTbase and BERTlarge are used in this paper. Tuning of λ. The hyper"
2020.coling-main.148,P05-1053,1,0.627088,"k-specific paraphrase data. Then, we propose a novel model to learn the information of diverse relation expressions. In our model, we try to capture this information on the paraphrases via a joint learning framework. Finally, we conduct experiments on a widely used dataset and the experimental results show that our approach is effective to improve the performance on relation extraction, even compared with a strong baseline. 1 Introduction Relation Extraction (RE) is an important task in Information Extraction, which identifies semantic relations between entities in text (Zelenko et al., 2003; Zhou et al., 2005; Mintz et al., 2009). The task becomes a typical classification problem if an entity pair in a text is given. In recent years, supervised models have achieved great progress on this task with the help of a massive amount of manually annotated high-quality data (Zeng et al., 2014; dos Santos et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). However, diverse expressions for a same semantic relation are difficult to be fully covered by humanannotated data. For example, sentences (1) “Steve Jobs co-founded Apple Computer.”; (2) “Steve Jobs was the co-founder of Apple Computer.”; and (3) “"
2020.coling-main.183,D16-1070,0,0.0188125,"4; Zhao et al., 2018). In this work, we propose two types of weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representation problem of multi-grained phenomena in the noisy pseudo MWS training data. SWS with heterogeneous data. In recent years, there has been a surge of interest in improving SWS with heterogeneous SWS data. The basic idea is improving SWS by utilizing multiple manually labeled SWS data for training at the same time. Representative works include Li et al. (2016a), Chen et al. (2016), He et al. (2018), Chen et al. (2017) and Yang et al. (2017). Although MWS results can be 3 4 https://github.com/fxsjy/jieba http://pullword.com 2034 obtained by merging multiple SWS outputs, but many overlapped words may generated due to the lack of proper constraints, leading to low precision. In this work, we alleviate this issue by considering MWS as a constituent tree parsing problem. 7 Conclusions This work advances the state-of-the-art MWS research from three perspectives. First, we manually annotate over 9,000 sentences for better evaluation, consisting of both canonical NEWS and non-"
2020.coling-main.183,P17-1110,0,0.0139382,"e propose two types of weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representation problem of multi-grained phenomena in the noisy pseudo MWS training data. SWS with heterogeneous data. In recent years, there has been a surge of interest in improving SWS with heterogeneous SWS data. The basic idea is improving SWS by utilizing multiple manually labeled SWS data for training at the same time. Representative works include Li et al. (2016a), Chen et al. (2016), He et al. (2018), Chen et al. (2017) and Yang et al. (2017). Although MWS results can be 3 4 https://github.com/fxsjy/jieba http://pullword.com 2034 obtained by merging multiple SWS outputs, but many overlapped words may generated due to the lack of proper constraints, leading to low precision. In this work, we alleviate this issue by considering MWS as a constituent tree parsing problem. 7 Conclusions This work advances the state-of-the-art MWS research from three perspectives. First, we manually annotate over 9,000 sentences for better evaluation, consisting of both canonical NEWS and non-canonical BAIKE texts. Second, we empl"
2020.coling-main.183,D16-1001,0,0.305701,"ntly improves the state-of-the-art MWS model by 1.12 on NEWS and by 5.97 on BAIKE in F1. We release all the newly annotated data and the codes at https://github.com/gloria0108/ multi-grained-word-seg. 2 Graph-based Model with Local Loss Given an input sentence, the task of MWS is to retrieve all words of different granularities, which can be naturally organized as a hierarchical tree structure as shown in Figure 1 (right). Gong et al. (2017) propose several MWS approaches and show that treating MWS as constituent parsing leads to the best performance. They adopt the transition-based parser of Cross and Huang (2016), which greedily searches an optimal shift-reduce action sequence to build a tree. In this work, instead of adopting the transition-based parser as Gong et al. (2017), we employ the graph-based parser of Stern et al. (2017) and replace the original global max-margin loss with local span-wise loss (Joshi et al., 2018; Teng and Zhang, 2018) as our basic MWS model due to two considerations: 1) the graph-based parser with local loss gains more efficiency without hurting the performance compared with the transitionbased parser and the graph-based parser with global loss, which will be discussed in"
2020.coling-main.183,D17-1072,1,0.64964,"xpert).” It is worth emphasizing that even for the same task or application, words of different granularities can be useful due to its potential complementarity: fine-grained words capture local features and help reduce data sparseness, whereas coarse-grained words reserve more semantics to perform exacter matching and analysis. This facilitates researchers to employ multiple SWS outputs at the same time in information retrieval (IR) (Liu et al., 2008) and machine translation (MT) (Su et al., 2017). Motivated by above perspectives, multi-grained word segmentation (MWS) is formally proposed by Gong et al. (2017) as a useful and challenging direction for research on word segmentation. Given an input sentence, MWS aims to accommodate all words of different granularities with a hierarchical tree structure. Figure 1 (right) presents an example, where “W” means the spanning characters compose a word. In this example, “二 (two)”, “老 (elder)”, “二老 (two elders)”, “都 (both)”, “是 (are)”, “都是 (both are)”, “令 (make)”, “人 (people)”, “令人 (make people)”, “尊敬 (respect)”, “令人尊敬 (respectable)”, “ 的 (of)”, “科学 (science)”, “家 (expert)”, “科学家 (scientist)” are all the words of different granularities. To solve the issue of"
2020.coling-main.183,2020.acl-main.275,0,0.0509741,"Missing"
2020.coling-main.183,P99-1010,0,0.427512,"ranularities to help IR. Su et al. (2017) propose a lattice-based RNN encoder for neural MT by representing MWS outputs in word lattices, leading to improved translation performance. Due to the lack of MWS model, they obtain MWS outputs from several SWS models independently trained on heterogeneous SWS datasets. Utilizing weakly labeled data. The use of weakly labeled data has been an interesting research direction in NLP for a long time. On the one hand, it is usually much easier and cheaper to perform partial annotation than complete annotation, especially for complex tasks such as parsing (Hwa, 1999; Sassano and Kurohashi, 2010; Li et al., 2016b; Joshi et al., 2018). On the other hand, it is sometimes feasible to automatically extract naturally annotated data. Several works utilize naturally annotated data with word boundaries for training SWS models, by making use of markup information such as anchor texts in web pages (Jiang et al., 2013; Liu et al., 2014; Zhao et al., 2018). In this work, we propose two types of weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representat"
2020.coling-main.183,P13-1075,0,0.0776526,"Missing"
2020.coling-main.183,P18-1110,0,0.0512991,"larities, which can be naturally organized as a hierarchical tree structure as shown in Figure 1 (right). Gong et al. (2017) propose several MWS approaches and show that treating MWS as constituent parsing leads to the best performance. They adopt the transition-based parser of Cross and Huang (2016), which greedily searches an optimal shift-reduce action sequence to build a tree. In this work, instead of adopting the transition-based parser as Gong et al. (2017), we employ the graph-based parser of Stern et al. (2017) and replace the original global max-margin loss with local span-wise loss (Joshi et al., 2018; Teng and Zhang, 2018) as our basic MWS model due to two considerations: 1) the graph-based parser with local loss gains more efficiency without hurting the performance compared with the transitionbased parser and the graph-based parser with global loss, which will be discussed in Section 5.3; 2) 2027 Figure 2: Architecture of our MWS model. more importantly, this work aims to conduct in-depth study on a simple, efficient, and effective way to incorporate weakly labeled data for MWS. The graph-based parser with local loss trains the model directly on individual labeled spans, and thus can acc"
2020.coling-main.183,D16-1072,1,0.853501,") propose a lattice-based RNN encoder for neural MT by representing MWS outputs in word lattices, leading to improved translation performance. Due to the lack of MWS model, they obtain MWS outputs from several SWS models independently trained on heterogeneous SWS datasets. Utilizing weakly labeled data. The use of weakly labeled data has been an interesting research direction in NLP for a long time. On the one hand, it is usually much easier and cheaper to perform partial annotation than complete annotation, especially for complex tasks such as parsing (Hwa, 1999; Sassano and Kurohashi, 2010; Li et al., 2016b; Joshi et al., 2018). On the other hand, it is sometimes feasible to automatically extract naturally annotated data. Several works utilize naturally annotated data with word boundaries for training SWS models, by making use of markup information such as anchor texts in web pages (Jiang et al., 2013; Liu et al., 2014; Zhao et al., 2018). In this work, we propose two types of weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representation problem of multi-grained phenomena in the"
2020.coling-main.183,P16-1033,1,0.845428,") propose a lattice-based RNN encoder for neural MT by representing MWS outputs in word lattices, leading to improved translation performance. Due to the lack of MWS model, they obtain MWS outputs from several SWS models independently trained on heterogeneous SWS datasets. Utilizing weakly labeled data. The use of weakly labeled data has been an interesting research direction in NLP for a long time. On the one hand, it is usually much easier and cheaper to perform partial annotation than complete annotation, especially for complex tasks such as parsing (Hwa, 1999; Sassano and Kurohashi, 2010; Li et al., 2016b; Joshi et al., 2018). On the other hand, it is sometimes feasible to automatically extract naturally annotated data. Several works utilize naturally annotated data with word boundaries for training SWS models, by making use of markup information such as anchor texts in web pages (Jiang et al., 2013; Liu et al., 2014; Zhao et al., 2018). In this work, we propose two types of weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representation problem of multi-grained phenomena in the"
2020.coling-main.183,D08-1111,0,0.212629,"erogeneity of an example sentence (left) with its MWS tree (right): “二 (two) 老 (elder) 都 (both) 是 (are) 令 (make) 人 (people) 尊敬 (respect) 的 (of) 科学 (science) 家 (expert).” It is worth emphasizing that even for the same task or application, words of different granularities can be useful due to its potential complementarity: fine-grained words capture local features and help reduce data sparseness, whereas coarse-grained words reserve more semantics to perform exacter matching and analysis. This facilitates researchers to employ multiple SWS outputs at the same time in information retrieval (IR) (Liu et al., 2008) and machine translation (MT) (Su et al., 2017). Motivated by above perspectives, multi-grained word segmentation (MWS) is formally proposed by Gong et al. (2017) as a useful and challenging direction for research on word segmentation. Given an input sentence, MWS aims to accommodate all words of different granularities with a hierarchical tree structure. Figure 1 (right) presents an example, where “W” means the spanning characters compose a word. In this example, “二 (two)”, “老 (elder)”, “二老 (two elders)”, “都 (both)”, “是 (are)”, “都是 (both are)”, “令 (make)”, “人 (people)”, “令人 (make people)”, “尊"
2020.coling-main.183,D14-1093,0,0.176118,"accumulating training loss, we only consider spans whose gold-standard labels can be directly determined according to the SWS annotation and overlook others, as illustrated in Figure 3, where spans labeled with “W” correspond to the words in the SWS sentence, those labeled with “NW” are definitely non-words since they overlap with existing gold-standard words (i.e., the words annotated in SWS data), and all blank spans without labels are overlooked with no loss. 3.2 DictEx Data as Weakly Labeled Data The use of naturally annotated data has been extensively studied for SWS (Jiang et al., 2013; Liu et al., 2014; Zhao et al., 2018). The basic idea is to derive word boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in sequence labeling models. 2029 In this work, we propose to obtain naturally annotated data with complete word information from the example sentences in dictionary (rather than only boundaries), which are manually constructed by linguistic experts, e.g., Entry: 最佳 (the best) Example sentence: 1: 找到 最佳 途径 2: 这是 最佳 选择 (find the best way) ( this is the best choice) where two DictEx sentences"
2020.coling-main.183,P14-1028,0,0.633797,"y MWS dataset according to our newly compiled annotation guideline, consisting of over 9,000 sentences from two types of texts, i.e., canonical newswire (NEWS) and non-canonical web (BAIKE) data for better evaluation. Detailed evaluation shows that our proposed model with weakly labeled data significantly outperforms the state-of-the-art MWS model by 1.12 and 5.97 on NEWS and BAIKE data in F1. 1 Introduction As a preliminary but critical processing step for Chinese language processing, word segmentation (WS) has been extensively studied for decades and made great progress (Zheng et al., 2013; Pei et al., 2014; Zhang et al., 2016; Yang et al., 2019; He et al., 2020). However, most of previous works adopt the single-grained word segmentation (SWS) formulation, where a sentence corresponds to a single word sequence according to some pre-defined annotation guidelines. As shown in Figure 1 (left), the SWS annotations of the sentence are different according to the guidelines of Penn Chinese Treebank (CTB) (Xue et al., 2005), the People Daily Corpus of the Peking University (PPD) (Yu et al., 2003), and the Microsoft Research WS Corpus (MSR) (Huang et al., 2006). This is largely due to the fact that the b"
2020.coling-main.183,P10-1037,0,0.0403731,"s to help IR. Su et al. (2017) propose a lattice-based RNN encoder for neural MT by representing MWS outputs in word lattices, leading to improved translation performance. Due to the lack of MWS model, they obtain MWS outputs from several SWS models independently trained on heterogeneous SWS datasets. Utilizing weakly labeled data. The use of weakly labeled data has been an interesting research direction in NLP for a long time. On the one hand, it is usually much easier and cheaper to perform partial annotation than complete annotation, especially for complex tasks such as parsing (Hwa, 1999; Sassano and Kurohashi, 2010; Li et al., 2016b; Joshi et al., 2018). On the other hand, it is sometimes feasible to automatically extract naturally annotated data. Several works utilize naturally annotated data with word boundaries for training SWS models, by making use of markup information such as anchor texts in web pages (Jiang et al., 2013; Liu et al., 2014; Zhao et al., 2018). In this work, we propose two types of weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representation problem of multi-grained"
2020.coling-main.183,P17-1076,0,0.236299,"Local Loss Given an input sentence, the task of MWS is to retrieve all words of different granularities, which can be naturally organized as a hierarchical tree structure as shown in Figure 1 (right). Gong et al. (2017) propose several MWS approaches and show that treating MWS as constituent parsing leads to the best performance. They adopt the transition-based parser of Cross and Huang (2016), which greedily searches an optimal shift-reduce action sequence to build a tree. In this work, instead of adopting the transition-based parser as Gong et al. (2017), we employ the graph-based parser of Stern et al. (2017) and replace the original global max-margin loss with local span-wise loss (Joshi et al., 2018; Teng and Zhang, 2018) as our basic MWS model due to two considerations: 1) the graph-based parser with local loss gains more efficiency without hurting the performance compared with the transitionbased parser and the graph-based parser with global loss, which will be discussed in Section 5.3; 2) 2027 Figure 2: Architecture of our MWS model. more importantly, this work aims to conduct in-depth study on a simple, efficient, and effective way to incorporate weakly labeled data for MWS. The graph-based"
2020.coling-main.183,C18-1011,0,0.019846,"be naturally organized as a hierarchical tree structure as shown in Figure 1 (right). Gong et al. (2017) propose several MWS approaches and show that treating MWS as constituent parsing leads to the best performance. They adopt the transition-based parser of Cross and Huang (2016), which greedily searches an optimal shift-reduce action sequence to build a tree. In this work, instead of adopting the transition-based parser as Gong et al. (2017), we employ the graph-based parser of Stern et al. (2017) and replace the original global max-margin loss with local span-wise loss (Joshi et al., 2018; Teng and Zhang, 2018) as our basic MWS model due to two considerations: 1) the graph-based parser with local loss gains more efficiency without hurting the performance compared with the transitionbased parser and the graph-based parser with global loss, which will be discussed in Section 5.3; 2) 2027 Figure 2: Architecture of our MWS model. more importantly, this work aims to conduct in-depth study on a simple, efficient, and effective way to incorporate weakly labeled data for MWS. The graph-based parser with local loss trains the model directly on individual labeled spans, and thus can accommodate weakly labeled"
2020.coling-main.183,P16-1218,0,0.0234966,"n (Pei et al., 2014), we use the concatenation of single character embeddings embci and bigram character embeddings embci−1 ci as the input. xi = embci ⊕ embci−1 ci (1) The encoding layer uses two layers of BiLSTM to encode the sentence and produce contextualized representations. We use fi and bi to denote the hidden vector of the top-layer forward and backward LSTMs for the i-th position. The span representation layer constructs a dense representation vector for each possible span ci ...cj−1 denoted as (i, j): ri,j = (fj − fi ) ⊕ (bi − bj ) (2) which is also known as the LSTM-minus features (Wang and Chang, 2016; Cross and Huang, 2016). The classification layer uses an MLP to compute the labeling scores of each span. oi,j = W2 ReLU(W1 ri,j + b1 ) + b2 (3) where W1 , W2 , b1 , and b2 are parameters. In our task, the dimension of oi,j is 2, w.r.t “W” and “NW” respectively (oi,j [0] is the score of labeling span (i, j) as a word, and oi,j [1] as a non-word). 2.2 Training with Local Span-wise Loss During training, we compute a local cross-entropy loss value for each span, and accumulate all loss values of all possible spans in the input sentence. ∗ eoi,j [yi,j ] L=− log o [0] e i,j + eoi,j [1] 0≤i<j≤n X"
2020.coling-main.183,P17-1078,0,0.138669,"weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representation problem of multi-grained phenomena in the noisy pseudo MWS training data. SWS with heterogeneous data. In recent years, there has been a surge of interest in improving SWS with heterogeneous SWS data. The basic idea is improving SWS by utilizing multiple manually labeled SWS data for training at the same time. Representative works include Li et al. (2016a), Chen et al. (2016), He et al. (2018), Chen et al. (2017) and Yang et al. (2017). Although MWS results can be 3 4 https://github.com/fxsjy/jieba http://pullword.com 2034 obtained by merging multiple SWS outputs, but many overlapped words may generated due to the lack of proper constraints, leading to low precision. In this work, we alleviate this issue by considering MWS as a constituent tree parsing problem. 7 Conclusions This work advances the state-of-the-art MWS research from three perspectives. First, we manually annotate over 9,000 sentences for better evaluation, consisting of both canonical NEWS and non-canonical BAIKE texts. Second, we employ a simple graph-based"
2020.coling-main.183,N19-1278,0,0.0224724,"Missing"
2020.coling-main.183,P16-1040,0,0.0283266,"Missing"
2020.coling-main.183,D13-1061,0,0.153603,"Missing"
2020.coling-main.266,C18-1233,0,0.0180067,"kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL. Syntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018)"
2020.coling-main.266,W05-0620,0,0.133682,"Missing"
2020.coling-main.266,N19-1423,0,0.0299973,"cit to take advantage of heterogeneous syntactic knowledge, which we believe are highly complementary. Our baseline model follows the architecture of He et al. (2018a). Afterwards, we inject the heterogeneous syntactic knowledge into the base model using two proposed methods. For the explicit method, we try to encode the heterogeneous automatic dependency trees with the recent popular graph convolutional networks (GCN) (Kipf and Welling, 2016). For the implicit method, which is inspired by the powerful representations from pre-trained language models, like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), we introduce a method to extract implicit syntactic representations from the dependency parser trained with heterogeneous syntactic treebanks. It is well known that the main reason for the success of pre-trained language model representations is the use of large amounts of natural text. However, it is difficult to obtain and costly to annotate large amounts of syntactic data. Therefore, making full use of existing heterogeneous data is the most feasible and natural idea. Intuitively, the explicit method models the syntactic structure of a sentence, providing valuable syntactic position infor"
2020.coling-main.266,Q19-1019,0,0.0241109,"racting syntactic knowledge from heterogeneous dependency treebanks. First, we introduce the method for encoding singleton dependency trees and then detailedly describe the variations to extract heterogeneous syntactic knowledge. 3.1 Syntactic Representation We employ two different methods to fully encode the homogeneous syntactic trees, i.e., GCN that encodes the syntactic structures and implicit representations that encode the syntactic features. Explicit Method. Graph convolutional networks (GCN) are neural networks that work on graph structures, which have been explored in many NLP tasks (Guo et al., 2019; Zhang et al., 2020). Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as ! X l (3) hlv = ρ Wl hl−1 u +b , u∈N (v) where Wl ∈ Rm×m is the weight matrix, bl ∈ Rm is the bias term, N (v) is the set of all one-hop neighbour nodes of v, and ρ is an activation function. Especially, h0u ∈ Rm is the initial input representation, and m is the representation dimension. In our work, we employ a 1-layer BiLSTM encoder over the input layer1 , and treat the BiLSTM outputs as th"
2020.coling-main.266,P17-1044,0,0.505726,"ct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syntax-agnostic models. Roth and Lapata"
2020.coling-main.266,P18-2058,0,0.091085,"gnificant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syn"
2020.coling-main.266,P18-1192,0,0.575347,"gnificant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syn"
2020.coling-main.266,D19-1538,0,0.183322,"cate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL. Syntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018) propose a multi-task learning framework based on the self-attention encoder, which treats dependency parsing as an auxiliary task. He et al. (2019) propose an argument pruning method based on dependency tree positions for multilingual SRL. Recently, Xia et al. (2019a) propose a similar framework to extract syntactic representation for SRL, but they only focus on Chinese SRL. Xia et al. (2019b) compares four explicit methods to encode automatic dependency trees for SRL. Zhang et al. (2019) present different methods to encode dependency trees and compare various incorporation ways into a self-attention based SRL model. These previous works mainly focus on encoding single-sourced dependency treebank, which can only provide limited syntactic"
2020.coling-main.266,2020.acl-main.744,0,0.187854,"tperforming Ouchi et al. (2018) by +0.2 F1. In the end-to-end setting of the CoNLL-2005 dataset, our model achieves 86.95 and 80.53 F1 scores in the WSJ and Brown test data, respectively. Ablation study on heterogeneous treebanks. Even though integrating syntactic knowledge into SRL brings significant improvements to the CPB1.0 dataset, we also find that it is not obvious on the CoNLL2005 dataset. To know why the improvements on CoNLL-2005 are smaller than on the CPB1.0 dataset, 2986 Dev Models Predefined predicates. He et al. (2018a) He et al. (2018a) (w/ ELMo) Ouchi et al. (2018) (w/ ELMo)? Li et al. (2020) (w/ RoBERTa) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) End-to-end setting. He et al. (2018a) He et al. (2018a) (w/ ELMo) Baseline HybridHDP Baseline(w/ RoBERTa) HybridHDP (w/ RoBERTa) WSJ Brown P R F1 P R F1 P R F1 88.0 87.24 82.28 83.65 86.87 86.99 86.9 87.26 82.76 84.06 87.89 87.41 87.4 87.25 82.52 83.85 87.38 87.20 89.2 88.05 84.21 85.12 88.11 88.43 87.9 88.00 84.39 85.0 88.64 88.75 83.9 87.4 88.5 88.03 84.30 85.06 88.37 88.59 81.0 80.04 74.37 76.3 82.49 83.05 78.4 79.56 73.59 75.42 83.51 83.28 73.7 80.4 79.6 79.80 73.98 75.86 83.00 83.16 81.53 82.95 85.81 85.93 82.15"
2020.coling-main.266,K17-1041,0,0.102309,"s because of the development of deep learning. Previous works can mostly be divided into two kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL. Syntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path em"
2020.coling-main.266,H94-1020,0,0.129263,"019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syntax-agnostic models. Roth and Lapata (2016) propose to use dependency-based embeddings in a neural SRL model for dependency-based SRL. He et al. (2018b) introduce k-order pruning algorithm to prune arguments according to dependency trees. However, previous syntax-aware works mainly employ singleton/homogeneous automatic dependency trees, which are generated by a syntactic parser trained on a specific syntactic treebank, like Penn Treebank (PTB) (Marcus et al., 1994). Our work follows the syntax-aware approach and enhances SRL with heterogeneous syntactic knowledge. We define heterogeneous syntactic treebanks as treebanks that follow different annotation guidelines. All is well known, there exist many published dependency treebanks that follow different annotation guidelines, i.e., English PTB (Marcus et al., 1994), Universal Dependencies (UD) (Silveira et al., 2014), Penn Chinese Treebank (PCTB) (Xue et al., 2005), Chinese Dependency Treebank (CDT) (Che et ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 Internationa"
2020.coling-main.266,D18-1191,0,0.170603,"ate-argument structures. 4.3 Results and Analyses on English SRL Table 5 shows our results on English CoNLL-2005 development and test data, where WSJ is the indomain data and Brown is the out-of-domain data. Our implemented baseline model achieves slightly higher performance than the model (He et al., 2018a) we follow. The proposed methods can further improve our baseline model by +0.76 (p &lt; 1e-4) and +1.88 (p &lt; 1e-4) F1 scores on WSJ and Brown test data, respectively. With the help of RoBERTa representations, our full model achieves 88.59 F1 score on the test WSJ data, slightly outperforming Ouchi et al. (2018) by +0.2 F1. In the end-to-end setting of the CoNLL-2005 dataset, our model achieves 86.95 and 80.53 F1 scores in the WSJ and Brown test data, respectively. Ablation study on heterogeneous treebanks. Even though integrating syntactic knowledge into SRL brings significant improvements to the CPB1.0 dataset, we also find that it is not obvious on the CoNLL2005 dataset. To know why the improvements on CoNLL-2005 are smaller than on the CPB1.0 dataset, 2986 Dev Models Predefined predicates. He et al. (2018a) He et al. (2018a) (w/ ELMo) Ouchi et al. (2018) (w/ ELMo)? Li et al. (2020) (w/ RoBERTa) B"
2020.coling-main.266,D14-1162,0,0.0850612,"Missing"
2020.coling-main.266,N18-1202,0,0.0261584,"rspective of explicit and implicit to take advantage of heterogeneous syntactic knowledge, which we believe are highly complementary. Our baseline model follows the architecture of He et al. (2018a). Afterwards, we inject the heterogeneous syntactic knowledge into the base model using two proposed methods. For the explicit method, we try to encode the heterogeneous automatic dependency trees with the recent popular graph convolutional networks (GCN) (Kipf and Welling, 2016). For the implicit method, which is inspired by the powerful representations from pre-trained language models, like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), we introduce a method to extract implicit syntactic representations from the dependency parser trained with heterogeneous syntactic treebanks. It is well known that the main reason for the success of pre-trained language model representations is the use of large amounts of natural text. However, it is difficult to obtain and costly to annotate large amounts of syntactic data. Therefore, making full use of existing heterogeneous data is the most feasible and natural idea. Intuitively, the explicit method models the syntactic structure of a sentence, providing va"
2020.coling-main.266,P16-1113,0,0.444205,"tic knowledge brings significant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches"
2020.coling-main.266,D16-1212,0,0.0244291,"commonly used Chinese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and M`arquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models are released in https: //github.com/KiroSummer/HDP-SRL. Heterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD2 dependency treebanks for English. We employ BiAffine 2 We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 2983 Dev Models Predefined predicates. Sha et al. (2016) Xia et al. (2017) Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) End-to-end setting. Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) Test P R F1 P R F1 81.62 84.92 87.17 86.93 82.36 85.59 86.65 87.02 83.39 81.99 85.25 86.91 86.97 81.94 84.86 87.48 87.93 80.59 84.45 87.11 87.57 77.69 79.67 83.91 87.54 81.26 84.65 87.29 87.75 80.35 83.4 85.47 86.73 80.89 84.23 86.31 86.41 82.39 85.92 80.62 83.81 85.89 86.57 79.82 83.67 86.30 87.04 78.22 82.89 86.14 86.23 81.73 85.57 7"
2020.coling-main.266,silveira-etal-2014-gold,0,0.054242,"Missing"
2020.coling-main.266,D18-1548,0,0.200322,"ents over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syn"
2020.coling-main.266,P17-1189,0,0.0185216,"ese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and M`arquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models are released in https: //github.com/KiroSummer/HDP-SRL. Heterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD2 dependency treebanks for English. We employ BiAffine 2 We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 2983 Dev Models Predefined predicates. Sha et al. (2016) Xia et al. (2017) Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) End-to-end setting. Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) Test P R F1 P R F1 81.62 84.92 87.17 86.93 82.36 85.59 86.65 87.02 83.39 81.99 85.25 86.91 86.97 81.94 84.86 87.48 87.93 80.59 84.45 87.11 87.57 77.69 79.67 83.91 87.54 81.26 84.65 87.29 87.75 80.35 83.4 85.47 86.73 80.89 84.23 86.31 86.41 82.39 85.92 80.62 83.81 85.89 86.57 79.82 83.67 86.30 87.04 78.22 82.89 86.14 86.23 81.73 85.57 79.01 83.28 86.22 8"
2020.coling-main.266,D19-1541,1,0.823683,"fied as: ! hlv =ρ X Wl rlu l +b , (5) u∈N (v) l where the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden ×d (dl = d + dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, whose effectiveness is shown in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019a), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 3. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations. 1 The ExpHDP module shares the sam"
2020.coling-main.266,N19-1075,0,0.556792,"fied as: ! hlv =ρ X Wl rlu l +b , (5) u∈N (v) l where the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden ×d (dl = d + dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, whose effectiveness is shown in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019a), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 3. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations. 1 The ExpHDP module shares the sam"
2020.coling-main.266,J08-2004,0,0.0450443,"pendency data completes the forward process. 3.3 Hybrid HDP Our model combines the two representations together, according to our intuition that explicit and implicit syntactic representations are highly complementary, which is denoted as “HybridHDP” (Hybrid Heterogeneous Dependency Parsing) in later sections. In detail, we concatenate the two heterogeneous l isr syntactic representations with the SRL input, formulated as xi = embword ⊕ repchar wi wi ⊕ hv ⊕ hi . 4 Experiments and Analysis 4.1 Experimental Setup We conduct experiments on the commonly used Chinese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and M`arquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models are released in https: //github.com/KiroSummer/HDP-SRL. Heterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD2 dependency treebanks for English. We employ BiAffine 2 We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 2983 Dev Models Predefined predicates. Sha et al. (2016) Xia et al. (2017) Xia et al. (2019a) Xia et a"
2020.coling-main.266,C18-1047,0,0.0252679,"yer would be modified as: ! hlv =ρ X Wl rlu l +b , (5) u∈N (v) l where the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden ×d (dl = d + dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, whose effectiveness is shown in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019a), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 3. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations. 1 The ExpHDP mod"
2020.coling-main.266,D19-1057,1,0.801133,"propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018) propose a multi-task learning framework based on the self-attention encoder, which treats dependency parsing as an auxiliary task. He et al. (2019) propose an argument pruning method based on dependency tree positions for multilingual SRL. Recently, Xia et al. (2019a) propose a similar framework to extract syntactic representation for SRL, but they only focus on Chinese SRL. Xia et al. (2019b) compares four explicit methods to encode automatic dependency trees for SRL. Zhang et al. (2019) present different methods to encode dependency trees and compare various incorporation ways into a self-attention based SRL model. These previous works mainly focus on encoding single-sourced dependency treebank, which can only provide limited syntactic knowledge. Our work focus on exploiting heterogeneous dependency benchmarks and the results verify our intuition that heterogeneous syntactic knowledge can provide more valid information. 6 Conclusion We propose to encode heterogeneous syntactic knowledge with explicit and implicit methods to help SRL. For the explicit aspect, we propose ExpHD"
2020.coling-main.266,2020.acl-main.297,1,0.824494,"knowledge from heterogeneous dependency treebanks. First, we introduce the method for encoding singleton dependency trees and then detailedly describe the variations to extract heterogeneous syntactic knowledge. 3.1 Syntactic Representation We employ two different methods to fully encode the homogeneous syntactic trees, i.e., GCN that encodes the syntactic structures and implicit representations that encode the syntactic features. Explicit Method. Graph convolutional networks (GCN) are neural networks that work on graph structures, which have been explored in many NLP tasks (Guo et al., 2019; Zhang et al., 2020). Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as ! X l (3) hlv = ρ Wl hl−1 u +b , u∈N (v) where Wl ∈ Rm×m is the weight matrix, bl ∈ Rm is the bias term, N (v) is the set of all one-hop neighbour nodes of v, and ρ is an activation function. Especially, h0u ∈ Rm is the initial input representation, and m is the representation dimension. In our work, we employ a 1-layer BiLSTM encoder over the input layer1 , and treat the BiLSTM outputs as the input of the GCN mo"
2020.coling-main.266,P15-1109,0,0.0199839,"apparently no because integrating syntactic knowledge into pre-trained language models has attracted some attention (Wang et al., 2020). And unitizing heterogeneous syntactic knowledge would be a direct and natural idea, which we leave for future work. 5 Related Work Recently, SRL has achieved significant improvements because of the development of deep learning. Previous works can mostly be divided into two kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the"
2020.coling-main.282,C18-1048,0,0.108324,"2) in Figure 1 hold a causal relation, where the possible connective “because” is not given. Interactive attention [Arg1: Psyllium’s not a good crop.] [Arg2: You get a rain at the wrong time and the crop is ruined.] Self-attention Figure 1: An example of causally-related arguments. Since the time when IDRR was boiled down to a problem of discourse relation classification (Pitler et al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li, 2016) as well as the perception of discourse relations. During encoding, the self-attention mechanism is able to highlight the latent information of attention-worthy words conditioned on local context (Note: we refer the a"
2020.coling-main.282,P16-1163,0,0.359836,"le, the arguments (i.e., Arg1 and Arg2) in Figure 1 hold a causal relation, where the possible connective “because” is not given. Interactive attention [Arg1: Psyllium’s not a good crop.] [Arg2: You get a rain at the wrong time and the crop is ruined.] Self-attention Figure 1: An example of causally-related arguments. Since the time when IDRR was boiled down to a problem of discourse relation classification (Pitler et al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li, 2016) as well as the perception of discourse relations. During encoding, the self-attention mechanism is able to highlight the latent information of attention-worthy words conditioned"
2020.coling-main.282,N18-1013,0,0.0584646,"the P-value, the higher the significance (Dror et al., 2018). We calculate P-values by comparing the experimental results of IPAL and the updated version (Multihead self-attention+IPAL) with those of others. Similarly, we consider two scenarios in which the GloVE based BiLSTM and BERT respectively cooperate with IPAL. We show the results of significance tests in Table 3, where the P-values which are lower than the threshold are marked with the sign “∗”. 3172 Method Zhang et al. (2015) Chen et al. (2016) Qin et al. (2016) Liu et al. (2016) Liu and Li (2016) Qin et al. (2017) Lan et al. (2017) Dai and Huang (2018) Lei et al. (2018) Guo et al. (2018) Bai and Zhao (2018) Nguyen et al. (2019) He et al (2020) IPAL+Multihead (BERT) COM 33.22 40.17 41.55 37.91 36.70 40.87 40.73 46.79 43.24 40.35 47.85 48.44 47.98 46.75 CON 52.04 54.76 57.32 55.88 54.48 54.56 58.96 57.09 57.82 56.81 54.47 56.84 55.62 59.56 EXP 69.59 71.50 69.97 70.43 72.38 72.47 70.41 72.88 72.11 70.60 73.66 69.37 75.83 TEM 30.54 31.32 35.43 37.17 38.84 36.20 38.50 45.61 29.10 38.65 36.97 38.60 38.94 39.35 Table 4: Comparisons with the state of the art The P-values listed in Table 3 demonstrate that IPAL yields statistically significant impro"
2020.coling-main.282,N19-1423,0,0.172773,"state ht ∈ Rdh and a backward hidden state ht ∈ Rdh . → − ← − → − ← − We concatenate ht and ht to form the synthetic hidden state ht = [ ht , ht ]. Accordingly, the inattentive sentence-level embeddings of the arguments (i.e., inattentive representations) can be represented as follows, where L is the maximum length of an argument: ( Arg1 : H1 ∈ RL×2dh = (h11 , ..., h1L ) (1) Arg2 : H2 ∈ RL×2dh = (h21 , ..., h2L ) The entries of BiLSTM layer are constituted of pretrained GloVE word embeddings (Pennington et al., 2014). In our experiments, we additionally evaluate the effect of fine-tuned BERT (Devlin et al., 2019) which is deployed as the substitution of the GloVE based BiLSTM. 2.2 Classic Self-Attention Mechanism For an argument, we first compute the self-attention vector α ∈ RL merely using intrinsic information in the argument itself. Lin et al. (2017)’s self-attention mechanism is used: ˇ α tanh(Wα H&gt; )) α = softmax(W (2) ˇ α ∈ Rda are learnable parameters, while da is a hyperparameter need to be where Wα ∈ Rda ×2dh and W tuned heuristically. On the basis, the self-attentive argument representation is computed as follows: Hα = αH 2.3 (3) Interactive Attention Propagation We model the interaction be"
2020.coling-main.282,P18-1128,0,0.0130014,"version to form IPAL instead of the classic one. It can be observed that the updated IPAL achieves better performance (see the bottom row in Table 2). 3.4 Discussion 1: Statistical Significance Testing We follow Johnson (1999) to use the sampling-based P-values for examining the significance. Johnson (1999) suggests that the ideal threshold of P-value is 0.05. It indicates that a system achieves significant improvements over others only if P-values are less than 0.05, otherwise insignificant. More importantly, it has been demonstrated that the smaller the P-value, the higher the significance (Dror et al., 2018). We calculate P-values by comparing the experimental results of IPAL and the updated version (Multihead self-attention+IPAL) with those of others. Similarly, we consider two scenarios in which the GloVE based BiLSTM and BERT respectively cooperate with IPAL. We show the results of significance tests in Table 3, where the P-values which are lower than the threshold are marked with the sign “∗”. 3172 Method Zhang et al. (2015) Chen et al. (2016) Qin et al. (2016) Liu et al. (2016) Liu and Li (2016) Qin et al. (2017) Lan et al. (2017) Dai and Huang (2018) Lei et al. (2018) Guo et al. (2018) Bai"
2020.coling-main.282,C18-1046,0,0.0633123,"Missing"
2020.coling-main.282,2020.acl-main.14,0,0.0473335,"Missing"
2020.coling-main.282,D17-1134,0,0.197868,"i.e., Arg1 and Arg2) in Figure 1 hold a causal relation, where the possible connective “because” is not given. Interactive attention [Arg1: Psyllium’s not a good crop.] [Arg2: You get a rain at the wrong time and the crop is ruined.] Self-attention Figure 1: An example of causally-related arguments. Since the time when IDRR was boiled down to a problem of discourse relation classification (Pitler et al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li, 2016) as well as the perception of discourse relations. During encoding, the self-attention mechanism is able to highlight the latent information of attention-worthy words conditioned on local context ("
2020.coling-main.282,D09-1036,0,0.0525221,"closed closed closed closed 783 783 783 783 27 27 27 27 ] ] ] ] Figure 5: Examples of attention weight assignment (Ground-truth attention-worthy words are marked by yellow background, while the predicted ones by blue. A darker color denotes a higher attention weight.) ings, as well as the exploitation of effective features. One of the most important feature engineering approaches uses interrelated word pairs as the reliable features (Marcu and Echihabi, 2002) since they imply semantic relationships. Hereafter, part-of-speech (POS) (Pitler et al., 2009), syntactic structures and dependencies (Lin et al., 2009) and semantic properties (Lei et al., 2018) were used as novel features. Recently, neural networks have been widely studied for argument representation learning (Zhang et al., 2015), which is admitted to be the crucial issue for discourse relation recognition. Due to the capacity of generating low-dimensional continuous representations for arguments, RNNs with Bi-LSTM are used during encoding. Chen et al (2016) couple Bi-LSTM with a gated relevance model. Liu and Li (2016) use multi-layer attention computation over the output of Bi-LSTM. Meanwhile, Liu et al (2016) build a multi-task learning"
2020.coling-main.282,D16-1130,0,0.223023,"al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li, 2016) as well as the perception of discourse relations. During encoding, the self-attention mechanism is able to highlight the latent information of attention-worthy words conditioned on local context (Note: we refer the attention-worthy words to the ones which play the dominant role in signaling discourse relations). By contrast, the interactive attention mechanism introduces external evidence into the identification of attention-worthy words, and similarly, highlighting their latent information. So far, the two kinds of attention computations are performed separately. However, our survey shows th"
2020.coling-main.282,P02-1047,0,0.227096,"B.A.T B.A.T B.A.T rally rally rally rally spread European markets spread European markets spread European markets spread European markets ] ] ] ] ] ] ] ] ] ] ] ] closed closed closed closed 783 783 783 783 27 27 27 27 ] ] ] ] Figure 5: Examples of attention weight assignment (Ground-truth attention-worthy words are marked by yellow background, while the predicted ones by blue. A darker color denotes a higher attention weight.) ings, as well as the exploitation of effective features. One of the most important feature engineering approaches uses interrelated word pairs as the reliable features (Marcu and Echihabi, 2002) since they imply semantic relationships. Hereafter, part-of-speech (POS) (Pitler et al., 2009), syntactic structures and dependencies (Lin et al., 2009) and semantic properties (Lei et al., 2018) were used as novel features. Recently, neural networks have been widely studied for argument representation learning (Zhang et al., 2015), which is admitted to be the crucial issue for discourse relation recognition. Due to the capacity of generating low-dimensional continuous representations for arguments, RNNs with Bi-LSTM are used during encoding. Chen et al (2016) couple Bi-LSTM with a gated rele"
2020.coling-main.282,C16-1205,0,0.0151382,"igure 1: An example of causally-related arguments. Since the time when IDRR was boiled down to a problem of discourse relation classification (Pitler et al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li, 2016) as well as the perception of discourse relations. During encoding, the self-attention mechanism is able to highlight the latent information of attention-worthy words conditioned on local context (Note: we refer the attention-worthy words to the ones which play the dominant role in signaling discourse relations). By contrast, the interactive attention mechanism introduces external evidence into the identification of attention-worthy words, and s"
2020.coling-main.282,P19-1411,0,0.765915,"Missing"
2020.coling-main.282,D14-1162,0,0.0890528,"n respectively. By BiLSTM, each word in an argu→ − ← − ment will be transformed into a forward hidden state ht ∈ Rdh and a backward hidden state ht ∈ Rdh . → − ← − → − ← − We concatenate ht and ht to form the synthetic hidden state ht = [ ht , ht ]. Accordingly, the inattentive sentence-level embeddings of the arguments (i.e., inattentive representations) can be represented as follows, where L is the maximum length of an argument: ( Arg1 : H1 ∈ RL×2dh = (h11 , ..., h1L ) (1) Arg2 : H2 ∈ RL×2dh = (h21 , ..., h2L ) The entries of BiLSTM layer are constituted of pretrained GloVE word embeddings (Pennington et al., 2014). In our experiments, we additionally evaluate the effect of fine-tuned BERT (Devlin et al., 2019) which is deployed as the substitution of the GloVE based BiLSTM. 2.2 Classic Self-Attention Mechanism For an argument, we first compute the self-attention vector α ∈ RL merely using intrinsic information in the argument itself. Lin et al. (2017)’s self-attention mechanism is used: ˇ α tanh(Wα H&gt; )) α = softmax(W (2) ˇ α ∈ Rda are learnable parameters, while da is a hyperparameter need to be where Wα ∈ Rda ×2dh and W tuned heuristically. On the basis, the self-attentive argument representation is"
2020.coling-main.282,P09-1077,0,0.0675507,"elationship between arguments, under the condition that there is lack of a connective signaling the relationship. An argument generally stands for a narrative sentence or clause. For example, the arguments (i.e., Arg1 and Arg2) in Figure 1 hold a causal relation, where the possible connective “because” is not given. Interactive attention [Arg1: Psyllium’s not a good crop.] [Arg2: You get a rain at the wrong time and the crop is ruined.] Self-attention Figure 1: An example of causally-related arguments. Since the time when IDRR was boiled down to a problem of discourse relation classification (Pitler et al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li"
2020.coling-main.282,prasad-etal-2008-penn,0,0.108423,"Given a target relation type r, the set R comprises two (n=2) class labels— rM and rO — which respectively signal a positive sample (argument pair) which holds the target relation and a negative sample which doesn’t hold the relation. During training, we minimize the binary classification loss L. The cost function is the cross-entropy of yr and yˆr for both the class labels rM and rO , where yrM , yrO ∈ {0,1} denotes the ground truth: L = −yrM log(ˆ yrM ) − yrO log(ˆ yrO ) 3 3.1 (8) Experimentation Datasets and Evaluation Metric We follow the common practice to use section 02-20 of PDTB v2.0 (Prasad et al., 2008) as the training set, section 00-01 as the development set, and section 21-22 as the test set. Table 1 shows the statistics of instances in the sets. We use F1-score as the evaluation metric for binary discourse relation classification. Besides, in the discussion sections, P-value (Johnson, 1999) is taken as the evaluation metric for statistical significance, and NDCG@k (J¨arvelin and Kek¨al¨ainen, 2002) is employed for evaluating the integrity of attention-worthy words. 3.2 Hyperparameter Settings We set two groups of hyperparameters in total, which correspond to different word embedding lear"
2020.coling-main.282,D16-1246,0,0.573583,"tion learning, Liu et al (2016) developed a multilayer attention mechanism. Chen et al (2016) integrated both the linear and non-linear interactions. Guo et al (2018) incorporated sparse learning into the interactive attention mechanism. Bai and Zhao (2018) used a feed forward network to model interactive attention and captured the effects on multi-grain linguistic features. Nguyen et al (2019) followed Bai and Zhao (2018)’s framework and conducted knowledge transferring. • For model design, neural networks were mainly used, including the basic ones like CNN, RNN and LSTM (Zhang et al., 2015; Qin et al., 2016; Guo et al., 2018), and the variants such as CRN and CGNN (Chen et al., 2016; Qin et al., 2016), as well as adversarial (Qin et al., 2017) and multi-task learning models (Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). • For feature selection, the embeddings of character, subword, word, sentence and sentence-pair levels (Bai and Zhao, 2018; Nguyen et al., 2019), paragraph-level relation continuity (Dai and Huang, 2018) and topic continuity (Lei et al., 2018) have been successfully applied in this area. Table 4 shows the performance of the previous methods and ours. Compared to the"
2020.coling-main.282,P17-1093,0,0.455204,"teractions. Guo et al (2018) incorporated sparse learning into the interactive attention mechanism. Bai and Zhao (2018) used a feed forward network to model interactive attention and captured the effects on multi-grain linguistic features. Nguyen et al (2019) followed Bai and Zhao (2018)’s framework and conducted knowledge transferring. • For model design, neural networks were mainly used, including the basic ones like CNN, RNN and LSTM (Zhang et al., 2015; Qin et al., 2016; Guo et al., 2018), and the variants such as CRN and CGNN (Chen et al., 2016; Qin et al., 2016), as well as adversarial (Qin et al., 2017) and multi-task learning models (Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). • For feature selection, the embeddings of character, subword, word, sentence and sentence-pair levels (Bai and Zhao, 2018; Nguyen et al., 2019), paragraph-level relation continuity (Dai and Huang, 2018) and topic continuity (Lei et al., 2018) have been successfully applied in this area. Table 4 shows the performance of the previous methods and ours. Compared to the state-of-the-art methods mentioned above, our IPAL is puny as it is isolated from the highly sophisticated learning architectures. More se"
2020.coling-main.282,D15-1266,0,0.768531,"arrative sentence or clause. For example, the arguments (i.e., Arg1 and Arg2) in Figure 1 hold a causal relation, where the possible connective “because” is not given. Interactive attention [Arg1: Psyllium’s not a good crop.] [Arg2: You get a rain at the wrong time and the crop is ruined.] Self-attention Figure 1: An example of causally-related arguments. Since the time when IDRR was boiled down to a problem of discourse relation classification (Pitler et al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li, 2016) as well as the perception of discourse relations. During encoding, the self-attention mechanism is able to highlight the latent information o"
2020.coling-main.338,N19-1009,0,0.0285733,"STM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representations, thus further boosting the performance of cross-domain dependency parsin"
2020.coling-main.338,P16-1231,0,0.0273406,"adiwinoto and Ng, 2017). Given an input sentence s = w1 w2 . . . wn , a dependency tree, as depicted in Figure 1, is defined as d = {(h, m, l), 0 ≤ h ≤ n, 1 ≤ m ≤ n, l ∈ L}, where (h, m, l) is a dependency from the head word wh to the child word wm with the relation label l ∈ L, and w0 is a pseudo word that points to the root word of the sentence. In recent years, neural network based approaches have achieved remarkable improvement and outperformed the traditional discrete-feature based approaches by a large margin in dependency parsing (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Dozat and Manning, 2017). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep BiAffine parser and achieve the state-of-the-art accuracy on a variety of datasets and languages. However, the domain adaptation problem, i.e., how to improve parsing performance on texts that are very different from the training data, remains a key challenge for the parsing community, especially when trying to apply the parsing technique to real-life web data. Taking the examples in Figure 1, we can see that as user-generated texts, the left sentence from the product comment (PC) domain i"
2020.coling-main.338,W17-4712,0,0.0244698,"y the FA technique on neural network, which uses a shared and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representati"
2020.coling-main.338,D18-1017,0,0.0265257,"n neural network, which uses a shared and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representations, thus further"
2020.coling-main.338,D14-1082,0,0.11834,"ration (Park and Kang, 2019), and machine translation (Hadiwinoto and Ng, 2017). Given an input sentence s = w1 w2 . . . wn , a dependency tree, as depicted in Figure 1, is defined as d = {(h, m, l), 0 ≤ h ≤ n, 1 ≤ m ≤ n, l ∈ L}, where (h, m, l) is a dependency from the head word wh to the child word wm with the relation label l ∈ L, and w0 is a pseudo word that points to the root word of the sentence. In recent years, neural network based approaches have achieved remarkable improvement and outperformed the traditional discrete-feature based approaches by a large margin in dependency parsing (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Dozat and Manning, 2017). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep BiAffine parser and achieve the state-of-the-art accuracy on a variety of datasets and languages. However, the domain adaptation problem, i.e., how to improve parsing performance on texts that are very different from the training data, remains a key challenge for the parsing community, especially when trying to apply the parsing technique to real-life web data. Taking the examples in Figure 1, we can see that as user-generated texts, the"
2020.coling-main.338,W03-0407,0,0.0179799,"main adaptation. Self-training is a simple method to incorporate unlabeled data into the new model, which first annotates the unlabeled data with the existing model, and then train a new model with the combination of newly generated data and actual labeled data (Yarowsky, 1995). As a typical unsupervised approach, self-training has proven effective on cross-domain constituency parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015), but there are also many failed works. Charniak (1997) reports either minor improvements or significant damage for parsing by using self-training. Clark et al. (2003) show the same findings on POS-tagging task. Co-training is another way to utilize the unlabeled data (Blum and Mitchell, 1998). It leverages multiple learners to annotate the unlabeled data respectively, and then arguments the training data with the newly labeled data when multiple learners agree on the annotation labels. Sarkar (2001) and Steedman et al. (2003) demonstrate that co-training is helpful for unsupervised cross-domain parsing. However, it still is a challenge to select the appropriate labeled data for self-training and co-training. 5.2 Semi-supervised Domain Adaptation Semi-super"
2020.coling-main.338,D18-1217,0,0.0483108,"enate hinv and hspe as the final contextualized word representation hi , which is used i i for dependency parsing by shared MLP and biaffine operations. In addition, the orthogonality loss is used to divergent the domain-specific and domain-invariant representations. Finally, the entire model is optimized by a joint loss, which is the same defined as L∗f a . 3.4 Fine-tuning BERT with All Target-domain Unlabeled Data Recently proposed contextualized word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) can further improve parsing performance by a large margin (Clark et al., 2018; Li et al., 2019a). Remarkably, BERT has been proven effective on a variety of natural language processing tasks (Devlin et al., 2019). Recently, researchers pay more attention to updating BERT representations with additional corpus and achieve great progress on BERT applications (Gururangan et al., 2020). Motivated by the successful utilization of BERT and BERT’s strong capability of word representations, we propose to fine-tune BERT model parameters with all unlabeled data to obtain more reliable representations. First, we use the released Chinese BERT-Based model as the original BERT model"
2020.coling-main.338,P07-1033,0,0.583474,"Missing"
2020.coling-main.338,N19-1423,0,0.0316635,"iaffine operations. In addition, the orthogonality loss is used to divergent the domain-specific and domain-invariant representations. Finally, the entire model is optimized by a joint loss, which is the same defined as L∗f a . 3.4 Fine-tuning BERT with All Target-domain Unlabeled Data Recently proposed contextualized word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) can further improve parsing performance by a large margin (Clark et al., 2018; Li et al., 2019a). Remarkably, BERT has been proven effective on a variety of natural language processing tasks (Devlin et al., 2019). Recently, researchers pay more attention to updating BERT representations with additional corpus and achieve great progress on BERT applications (Gururangan et al., 2020). Motivated by the successful utilization of BERT and BERT’s strong capability of word representations, we propose to fine-tune BERT model parameters with all unlabeled data to obtain more reliable representations. First, we use the released Chinese BERT-Based model as the original BERT model.2 Then, we finetune BERT on the unlabeled data using the parameters in the original BERT model as the starting point. To save computat"
2020.coling-main.338,P81-1022,0,0.668093,"Missing"
2020.coling-main.338,D18-1498,0,0.0766046,"which uses a shared and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representations, thus further boosting the perfo"
2020.coling-main.338,2020.acl-main.740,0,0.0200745,"mized by a joint loss, which is the same defined as L∗f a . 3.4 Fine-tuning BERT with All Target-domain Unlabeled Data Recently proposed contextualized word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) can further improve parsing performance by a large margin (Clark et al., 2018; Li et al., 2019a). Remarkably, BERT has been proven effective on a variety of natural language processing tasks (Devlin et al., 2019). Recently, researchers pay more attention to updating BERT representations with additional corpus and achieve great progress on BERT applications (Gururangan et al., 2020). Motivated by the successful utilization of BERT and BERT’s strong capability of word representations, we propose to fine-tune BERT model parameters with all unlabeled data to obtain more reliable representations. First, we use the released Chinese BERT-Based model as the original BERT model.2 Then, we finetune BERT on the unlabeled data using the parameters in the original BERT model as the starting point. To save computational resource, we merge all train/unlabeled data of all domains as one unlabeled dataset for fine-tuning BERT once. Thus, the same fine-tuned BERT model is used for all th"
2020.coling-main.338,C16-1038,0,0.613212,"kar, 2001). However, due to the intrinsic difficulty of domain adaptation, progress in this direction is very slow. In the past few years, semi-supervised cross-domain parsing attracts more attention due to the emergence of more labeled data. Particularly, Li et al. (2019b) release large-scale labeled and unlabeled datasets, and they find that their proposed domain embedding (DE) approach is more effective than the direct concatenation (CON) method. The feature augmentation (FA) method, as another typical technique for semi-supervised domain adaptation, is first proposed by Daum´e III (2007). Kim et al. (2016) successfully apply it to a neural model which leverages multiple BiLSTMs to extract shared and private domain features. To learn the differences and commonalities between source and target domains, the DE method uses explicit domain indicators as extra inputs, whereas the FA method employs a shared and two private BiLSTM encoders for the feature separation. This work proposes to improve the contextualized word representation by adversarial learning and fine-tuning BERT, thus further modeling more pure yet effective domain-specific and domain-invariant representations. To alleviate the domain-"
2020.coling-main.338,P17-1119,0,0.0224601,"successfully employ the FA technique on neural network, which uses a shared and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effecti"
2020.coling-main.338,Q16-1023,0,0.0443867,"019), and machine translation (Hadiwinoto and Ng, 2017). Given an input sentence s = w1 w2 . . . wn , a dependency tree, as depicted in Figure 1, is defined as d = {(h, m, l), 0 ≤ h ≤ n, 1 ≤ m ≤ n, l ∈ L}, where (h, m, l) is a dependency from the head word wh to the child word wm with the relation label l ∈ L, and w0 is a pseudo word that points to the root word of the sentence. In recent years, neural network based approaches have achieved remarkable improvement and outperformed the traditional discrete-feature based approaches by a large margin in dependency parsing (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Dozat and Manning, 2017). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep BiAffine parser and achieve the state-of-the-art accuracy on a variety of datasets and languages. However, the domain adaptation problem, i.e., how to improve parsing performance on texts that are very different from the training data, remains a key challenge for the parsing community, especially when trying to apply the parsing technique to real-life web data. Taking the examples in Figure 1, we can see that as user-generated texts, the left sentence from the product c"
2020.coling-main.338,P19-1229,1,0.638182,"sion structures, etc. The key for domain adaptation is how to model differences and commonalities between different domains. Most previous works focus on unsupervised cross-domain parsing, assuming there is no target-domain labeled data. Typical methods include self-training (McClosky and Charniak, 2008; Yu et al., 2015) and co-training (Sarkar, 2001). However, due to the intrinsic difficulty of domain adaptation, progress in this direction is very slow. In the past few years, semi-supervised cross-domain parsing attracts more attention due to the emergence of more labeled data. Particularly, Li et al. (2019b) release large-scale labeled and unlabeled datasets, and they find that their proposed domain embedding (DE) approach is more effective than the direct concatenation (CON) method. The feature augmentation (FA) method, as another typical technique for semi-supervised domain adaptation, is first proposed by Daum´e III (2007). Kim et al. (2016) successfully apply it to a neural model which leverages multiple BiLSTMs to extract shared and private domain features. To learn the differences and commonalities between source and target domains, the DE method uses explicit domain indicators as extra i"
2020.coling-main.338,P08-2026,0,0.052112,"$ $ 会议 Meeting 在 in 北京 Beijing punc 举行 was held 。 . Figure 1: Examples of dependency trees. The left sentence is from the target-domain PC data and the right one is from the source-domain BC data. differences can be represented with both sentence and parse tree distribution changes due to new words and phrases, new expression structures, etc. The key for domain adaptation is how to model differences and commonalities between different domains. Most previous works focus on unsupervised cross-domain parsing, assuming there is no target-domain labeled data. Typical methods include self-training (McClosky and Charniak, 2008; Yu et al., 2015) and co-training (Sarkar, 2001). However, due to the intrinsic difficulty of domain adaptation, progress in this direction is very slow. In the past few years, semi-supervised cross-domain parsing attracts more attention due to the emergence of more labeled data. Particularly, Li et al. (2019b) release large-scale labeled and unlabeled datasets, and they find that their proposed domain embedding (DE) approach is more effective than the direct concatenation (CON) method. The feature augmentation (FA) method, as another typical technique for semi-supervised domain adaptation, i"
2020.coling-main.338,N06-1020,0,0.136708,"e approaches for both unsupervised and semi-supervised domain adaptation. 5.1 Unsupervised Domain Adaptation Due to the lack of target-domain labeled data, previous researches mostly focus on the unsupervised domain adaptation. Self-training is a simple method to incorporate unlabeled data into the new model, which first annotates the unlabeled data with the existing model, and then train a new model with the combination of newly generated data and actual labeled data (Yarowsky, 1995). As a typical unsupervised approach, self-training has proven effective on cross-domain constituency parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015), but there are also many failed works. Charniak (1997) reports either minor improvements or significant damage for parsing by using self-training. Clark et al. (2003) show the same findings on POS-tagging task. Co-training is another way to utilize the unlabeled data (Blum and Mitchell, 1998). It leverages multiple learners to annotate the unlabeled data respectively, and then arguments the training data with the newly labeled data when multiple learners agree on the annotation labels. Sarkar (2001) and Steedman et al. (2003) demonstrate that co-traini"
2020.coling-main.338,H05-1066,0,0.157985,"head word, and rDi as a dependent, and MLPH/D both have a single hidden layer with the ReLU activation function. BiAffine layer. The scores of all dependencies are computed via a BiAffine operation,  score(i ← j) = rDi 1 T Wb rHj (4) where score(i ← j) is the score of the dependency (j, i) and the matrix Wb is a BiAffine parameter. The arc-factorization score of a dependency tree is computed with extra MLPs, which can be seen in Dozat and Manning (2017). After obtaining the scores, the highest-scoring tree can be decoded with the dynamic programming algorithm known as maximum spanning tree (McDonald et al., 2005). Parser loss. Assuming wj is the gold-standard head of wi , the BiAffine parser loss for each position i is escore(i←j) P Lparser = − log (5) escore(i←k) 0≤k≤n,k6=i The BiAffine parser treats the classification of dependency labels as a separate task after finding the highest-scoring dependency tree. 3808 3 Approaches In this work, we propose to improve contextualized word representations by adversarial learning and fine-tuning BERT processes to boost the performance of cross-domain dependency parsing. Concretely, we apply adversarial learning to three typical semi-supervised approaches with"
2020.coling-main.338,P08-1108,0,0.034203,"in parsing. However, it still is a challenge to select the appropriate labeled data for self-training and co-training. 5.2 Semi-supervised Domain Adaptation Semi-supervised domain adaptation assumes the model is trained with all source- and target-domain labeled data. Most recently, Li et al. (2019c) and Yu et al. (2019) reveal that newly generated targetdomain data by self-training or tri-training and model ensemble can improve the cross-domain parsing performance significantly. The model ensemble method is a commonly used strategy to integrate different parsing models in dependency parsing (Nivre and McDonald, 2008). However, all these approaches require to retrain parser repeatedly, making them difficult for practical applications. Daum´e III (2007) for the first time proposes the FA method on sequence labeling task, which distinguishes domain-specific and domain-invariant with different feature extractors. Kim et al. (2016) successfully employ the FA technique on neural network, which uses a shared and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find"
2020.coling-main.338,N18-1202,0,0.0540639,"129 1,300 2,600 291,481 ZX 1,645 500 1,100 33,792 Table 1: Data statistics in sentence number 0 Then, we concatenate hinv and hspe as the final contextualized word representation hi , which is used i i for dependency parsing by shared MLP and biaffine operations. In addition, the orthogonality loss is used to divergent the domain-specific and domain-invariant representations. Finally, the entire model is optimized by a joint loss, which is the same defined as L∗f a . 3.4 Fine-tuning BERT with All Target-domain Unlabeled Data Recently proposed contextualized word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) can further improve parsing performance by a large margin (Clark et al., 2018; Li et al., 2019a). Remarkably, BERT has been proven effective on a variety of natural language processing tasks (Devlin et al., 2019). Recently, researchers pay more attention to updating BERT representations with additional corpus and achieve great progress on BERT applications (Gururangan et al., 2020). Motivated by the successful utilization of BERT and BERT’s strong capability of word representations, we propose to fine-tune BERT model parameters with all unlabeled data to obtain"
2020.coling-main.338,N01-1023,0,0.860666,"xamples of dependency trees. The left sentence is from the target-domain PC data and the right one is from the source-domain BC data. differences can be represented with both sentence and parse tree distribution changes due to new words and phrases, new expression structures, etc. The key for domain adaptation is how to model differences and commonalities between different domains. Most previous works focus on unsupervised cross-domain parsing, assuming there is no target-domain labeled data. Typical methods include self-training (McClosky and Charniak, 2008; Yu et al., 2015) and co-training (Sarkar, 2001). However, due to the intrinsic difficulty of domain adaptation, progress in this direction is very slow. In the past few years, semi-supervised cross-domain parsing attracts more attention due to the emergence of more labeled data. Particularly, Li et al. (2019b) release large-scale labeled and unlabeled datasets, and they find that their proposed domain embedding (DE) approach is more effective than the direct concatenation (CON) method. The feature augmentation (FA) method, as another typical technique for semi-supervised domain adaptation, is first proposed by Daum´e III (2007). Kim et al."
2020.coling-main.338,K17-3007,0,0.0449532,"s another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representations, thus further boosting the performance of cross-domain dependency parsing. 6 Conclusions This work successful"
2020.coling-main.338,E03-1008,0,0.192371,"main constituency parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015), but there are also many failed works. Charniak (1997) reports either minor improvements or significant damage for parsing by using self-training. Clark et al. (2003) show the same findings on POS-tagging task. Co-training is another way to utilize the unlabeled data (Blum and Mitchell, 1998). It leverages multiple learners to annotate the unlabeled data respectively, and then arguments the training data with the newly labeled data when multiple learners agree on the annotation labels. Sarkar (2001) and Steedman et al. (2003) demonstrate that co-training is helpful for unsupervised cross-domain parsing. However, it still is a challenge to select the appropriate labeled data for self-training and co-training. 5.2 Semi-supervised Domain Adaptation Semi-supervised domain adaptation assumes the model is trained with all source- and target-domain labeled data. Most recently, Li et al. (2019c) and Yu et al. (2019) reveal that newly generated targetdomain data by self-training or tri-training and model ensemble can improve the cross-domain parsing performance significantly. The model ensemble method is a commonly used st"
2020.coling-main.338,N19-1075,0,0.0255217,"ns that benefit for the cross-domain dependency parsing. Experiments on a benchmark dataset show that our proposed adversarial approaches achieve consistent improvements, and fine-tuning BERT further boosts the parsing accuracy by a large margin. Our single model achieves the same state-of-the-art performance as the top submitted system in the NLPCC-2019 shared task, which uses ensemble models and BERT. 1 Introduction Dependency parsing aims to capture syntax with a dependency tree and is proven to be helpful for various natural language processing (NLP) tasks, such as semantic role labeling (Xia et al., 2019), natural language generation (Park and Kang, 2019), and machine translation (Hadiwinoto and Ng, 2017). Given an input sentence s = w1 w2 . . . wn , a dependency tree, as depicted in Figure 1, is defined as d = {(h, m, l), 0 ≤ h ≤ n, 1 ≤ m ≤ n, l ∈ L}, where (h, m, l) is a dependency from the head word wh to the child word wm with the relation label l ∈ L, and w0 is a pseudo word that points to the root word of the sentence. In recent years, neural network based approaches have achieved remarkable improvement and outperformed the traditional discrete-feature based approaches by a large margin"
2020.coling-main.338,P95-1026,0,0.749191,"Domain adaptation has been a long-standing yet challenging research topic. Here we try to briefly summarize the representative approaches for both unsupervised and semi-supervised domain adaptation. 5.1 Unsupervised Domain Adaptation Due to the lack of target-domain labeled data, previous researches mostly focus on the unsupervised domain adaptation. Self-training is a simple method to incorporate unlabeled data into the new model, which first annotates the unlabeled data with the existing model, and then train a new model with the combination of newly generated data and actual labeled data (Yarowsky, 1995). As a typical unsupervised approach, self-training has proven effective on cross-domain constituency parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015), but there are also many failed works. Charniak (1997) reports either minor improvements or significant damage for parsing by using self-training. Clark et al. (2003) show the same findings on POS-tagging task. Co-training is another way to utilize the unlabeled data (Blum and Mitchell, 1998). It leverages multiple learners to annotate the unlabeled data respectively, and then arguments the training data with the newly la"
2020.coling-main.338,W15-2201,0,0.61471,"g punc 举行 was held 。 . Figure 1: Examples of dependency trees. The left sentence is from the target-domain PC data and the right one is from the source-domain BC data. differences can be represented with both sentence and parse tree distribution changes due to new words and phrases, new expression structures, etc. The key for domain adaptation is how to model differences and commonalities between different domains. Most previous works focus on unsupervised cross-domain parsing, assuming there is no target-domain labeled data. Typical methods include self-training (McClosky and Charniak, 2008; Yu et al., 2015) and co-training (Sarkar, 2001). However, due to the intrinsic difficulty of domain adaptation, progress in this direction is very slow. In the past few years, semi-supervised cross-domain parsing attracts more attention due to the emergence of more labeled data. Particularly, Li et al. (2019b) release large-scale labeled and unlabeled datasets, and they find that their proposed domain embedding (DE) approach is more effective than the direct concatenation (CON) method. The feature augmentation (FA) method, as another typical technique for semi-supervised domain adaptation, is first proposed b"
2020.coling-main.338,D18-1041,0,0.0444858,"d and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representations, thus further boosting the performance of cross-dom"
2020.coling-main.379,K16-1002,0,0.227105,"d by Sennrich et al. (2016a), different from the standard dropout, the method drops full word by setting zero to word embedding during training. The deficiency is zero vector can not learn representation from its context in the self-attention layer. Drop-Tag (K˚ageb¨ack and Salomonsson, 2016) replaces token with a < dropped > tag. The tag is subsequently treated just like any other word in the vocabulary and has a corresponding word embedding that is trained. We adopt this technique for NMT to learn better feature representation. Unk-Tag replaces token with generic unknown word token < unk >. Bowman et al. (2016) and Yang et al. (2017) apply it to RNN decoder to force model make prediction by latent variable. We found this perfectly suits for NMT system especially on self-attention layers. Better than Drop-Tag method, it need not to add an extra token as well as parameters. 2.2 Replaced Token Detection We propose the Replaced Token Detection task to promote generalization ability of the model encoder. We regard dropped information as self-supervised label, following Clark et al. (2020), we train a discriminator D(G(x)) to detect whether tokens are dropped or not. On account of our dropped tokens are o"
2020.coling-main.379,P18-1163,0,0.0170578,"ahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017). In spite of the impressive performance, NMT models are still vulnerable to perturbations in the input sentences (Belinkov and Bisk, 2018; Cheng et al., 2019) , i.e. a tiny perturbation will affect hidden representation and lead to low quality of translation (Zhao et al., 2018). Moreover, NMT commonly consists of millions of parameters, which making it prone to overfitting especially in low resource scene. A natural way to improve generalization is synthesizing natural noise (Karpukhin et al., 2019) or adopting arbitrary noise (Cheng et al., 2018; Ebrahimi et al., 2018). Another way is exploring regularization techniques to avoid overfitting (Miceli et al., 2017), making model robust to unseen or unfamiliar inputs. However, as discrete data, the text is hard to retain the semantic information after corruption. In this paper, we propose Token Drop to prevent overfitting and improve generalization. Different from standard dropout (Srivastava et al., 2014) that drops neurons in network randomly, we drop tokens of the input sentences. In order to retain semantic information, we replace tokens with a special symbol < unk > . This allows mo"
2020.coling-main.379,P19-1425,0,0.0862911,"ith less information, in this way the model can learn textual representation better. Experiments on Chinese-English and English-Romanian benchmark demonstrate the effectiveness of our approach and our model achieves significant improvements over a strong Transformer baseline1 . 1 Introduction Neural machine translation (NMT) achieved enormous success in advancing the quality of translation (Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017). In spite of the impressive performance, NMT models are still vulnerable to perturbations in the input sentences (Belinkov and Bisk, 2018; Cheng et al., 2019) , i.e. a tiny perturbation will affect hidden representation and lead to low quality of translation (Zhao et al., 2018). Moreover, NMT commonly consists of millions of parameters, which making it prone to overfitting especially in low resource scene. A natural way to improve generalization is synthesizing natural noise (Karpukhin et al., 2019) or adopting arbitrary noise (Cheng et al., 2018; Ebrahimi et al., 2018). Another way is exploring regularization techniques to avoid overfitting (Miceli et al., 2017), making model robust to unseen or unfamiliar inputs. However, as discrete data, the te"
2020.coling-main.379,N19-1423,0,0.168911,"sentences can be explained as data augmentation; On the other hand, our method corrupts input sentences with natural noise can be seen as regularization term for NMT. We investigate two self-supervised objectives: Replaced Token Detection and Dropped Token Prediction. Considering our Token Drop method regularize parameters by weakening model inputs, making NMT suitable for applying self-supervised objective. During training: (1) use a discriminator to detect whether input tokens are dropped or not; (2) leverage hidden state to predict original tokens of dropped tokens inspired by Cloze task (Devlin et al., 2019). Both of them guide model to generate semantically similar representation, leading to a better generalization capacity. ∗ * Corresponding author. Our code is released at https://github.com/zhajiahe/Token_Drop This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. 1 License details: http: 4298 Proceedings of the 28th International Conference on Computational Linguistics, pages 4298–4303 Barcelona, Spain (Online), December 8-13, 2020 2 Token Drop Training Standard dropout prevents overfitting by setting input neurons or hidd"
2020.coling-main.379,C18-1055,0,0.0183391,"; Vaswani et al., 2017; Gehring et al., 2017). In spite of the impressive performance, NMT models are still vulnerable to perturbations in the input sentences (Belinkov and Bisk, 2018; Cheng et al., 2019) , i.e. a tiny perturbation will affect hidden representation and lead to low quality of translation (Zhao et al., 2018). Moreover, NMT commonly consists of millions of parameters, which making it prone to overfitting especially in low resource scene. A natural way to improve generalization is synthesizing natural noise (Karpukhin et al., 2019) or adopting arbitrary noise (Cheng et al., 2018; Ebrahimi et al., 2018). Another way is exploring regularization techniques to avoid overfitting (Miceli et al., 2017), making model robust to unseen or unfamiliar inputs. However, as discrete data, the text is hard to retain the semantic information after corruption. In this paper, we propose Token Drop to prevent overfitting and improve generalization. Different from standard dropout (Srivastava et al., 2014) that drops neurons in network randomly, we drop tokens of the input sentences. In order to retain semantic information, we replace tokens with a special symbol < unk > . This allows model learn hidden represe"
2020.coling-main.379,P15-1162,0,0.0391941,"Missing"
2020.coling-main.379,W16-5307,0,0.0467521,"Missing"
2020.coling-main.379,D19-5506,0,0.0161114,"us success in advancing the quality of translation (Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017). In spite of the impressive performance, NMT models are still vulnerable to perturbations in the input sentences (Belinkov and Bisk, 2018; Cheng et al., 2019) , i.e. a tiny perturbation will affect hidden representation and lead to low quality of translation (Zhao et al., 2018). Moreover, NMT commonly consists of millions of parameters, which making it prone to overfitting especially in low resource scene. A natural way to improve generalization is synthesizing natural noise (Karpukhin et al., 2019) or adopting arbitrary noise (Cheng et al., 2018; Ebrahimi et al., 2018). Another way is exploring regularization techniques to avoid overfitting (Miceli et al., 2017), making model robust to unseen or unfamiliar inputs. However, as discrete data, the text is hard to retain the semantic information after corruption. In this paper, we propose Token Drop to prevent overfitting and improve generalization. Different from standard dropout (Srivastava et al., 2014) that drops neurons in network randomly, we drop tokens of the input sentences. In order to retain semantic information, we replace token"
2020.coling-main.379,D18-1149,0,0.023262,"ent We conduct our approach on two machine translation benchmarks: LDC (ZH-EN) and WMT16 (ENRO)2 3.1 Dataset and Evaluation For ZH-EN translation, we used 1.25M sentence pairs extract from LDC corpus. Byte-pair encoding is employed separate vocabulary of about 42K and 31K tokens with 32K merge operations (Sennrich et al., 2016b). we chose NIST06 as the valid set and NIST02, NIST03, NIST04, NIST05, NIST08 as the test set, which contains 878, 919, 1788, 1082, 1357 sentence pairs respectively. We measure BLEU score with multi-bleu.pl3 . For WMT16 EN-RO data which consists of 610K pairs, we adopt Lee et al. (2018)’s preprocessing. The vocabulary is 35K joint source and target subwords (Sennrich et al., 2016b). we use newstest-2016 as test set and report tokenized BLEU score. 3.2 Models and Settings We adopt the Transformer model (Vaswani et al., 2017) implemented in PyTorch in the fairseq-py toolkit (Ott et al., 2019). We closely followed settings by Vaswani et al. (2017)(dmodel = 512, dhidden = 512, dF F N = 2048, nlayer = 6, nhead = 8) and used dropout of pdropout = 0.3. As for our approach, we set drop rate ps = 0.15 and pt = 0.3 respectively for ZH-EN task. For EN-RO, we set ps = 0.15 and pt = 0.2."
2020.coling-main.379,D17-1156,0,0.0177521,"are still vulnerable to perturbations in the input sentences (Belinkov and Bisk, 2018; Cheng et al., 2019) , i.e. a tiny perturbation will affect hidden representation and lead to low quality of translation (Zhao et al., 2018). Moreover, NMT commonly consists of millions of parameters, which making it prone to overfitting especially in low resource scene. A natural way to improve generalization is synthesizing natural noise (Karpukhin et al., 2019) or adopting arbitrary noise (Cheng et al., 2018; Ebrahimi et al., 2018). Another way is exploring regularization techniques to avoid overfitting (Miceli et al., 2017), making model robust to unseen or unfamiliar inputs. However, as discrete data, the text is hard to retain the semantic information after corruption. In this paper, we propose Token Drop to prevent overfitting and improve generalization. Different from standard dropout (Srivastava et al., 2014) that drops neurons in network randomly, we drop tokens of the input sentences. In order to retain semantic information, we replace tokens with a special symbol < unk > . This allows model learn hidden representation from rest token’s context, and predict target translation condition on latent variable."
2020.coling-main.379,N19-4009,0,0.0248882,"nrich et al., 2016b). we chose NIST06 as the valid set and NIST02, NIST03, NIST04, NIST05, NIST08 as the test set, which contains 878, 919, 1788, 1082, 1357 sentence pairs respectively. We measure BLEU score with multi-bleu.pl3 . For WMT16 EN-RO data which consists of 610K pairs, we adopt Lee et al. (2018)’s preprocessing. The vocabulary is 35K joint source and target subwords (Sennrich et al., 2016b). we use newstest-2016 as test set and report tokenized BLEU score. 3.2 Models and Settings We adopt the Transformer model (Vaswani et al., 2017) implemented in PyTorch in the fairseq-py toolkit (Ott et al., 2019). We closely followed settings by Vaswani et al. (2017)(dmodel = 512, dhidden = 512, dF F N = 2048, nlayer = 6, nhead = 8) and used dropout of pdropout = 0.3. As for our approach, we set drop rate ps = 0.15 and pt = 0.3 respectively for ZH-EN task. For EN-RO, we set ps = 0.15 and pt = 0.2. To train with DTP and RTD objective, we simply set α, β = 1 . 4 Result Models Transformer Zero-Out Drop-Tag Unk-Tag + RTD + DTP + RTD&DTP NIST02 47.17 47.86 48.99 48.96 49.00 49.16 49.52 NIST03 46.78 47.12 48.50 48.52 49.36 49.19 49.29 LDC ZH-EN NIST04 NIST05 47.46 47.97 48.86 47.90 49.81 49.60 49.50 49.49 4"
2020.coling-main.379,E17-2025,0,0.0211574,"ly replaces tokens of input sentence, similar to Masked Language Model (Devlin et al., 2019), which masks then predicts masked tokens by the rest of the tokens, making use of contextual information. Accordingly, we propose Dropped Token Prediction (DTP), predicting dropped tokens by their hidden states. The DTP objective is : X LDT P = − log P (ˆ x|X/d(x) , θM ) (4) x ˆ∈d(x) 4299 P (ˆ x|X/d(x) ) = E(G(X/d(x) )) (5) Where d(x) and X/d(x) denote the dropped tokens and the rest tokens respectively. G is model encoder, E(.) is prediction layer. In our implementation of DTP, we adopt weight tying (Press and Wolf, 2017) , that is to share the same weight matrix between embedding layer and token prediction classifier. At the end we train our model jointly with DTP and RTD objective: L = LM + αLRT D + βLDT P 3 (6) Experiment We conduct our approach on two machine translation benchmarks: LDC (ZH-EN) and WMT16 (ENRO)2 3.1 Dataset and Evaluation For ZH-EN translation, we used 1.25M sentence pairs extract from LDC corpus. Byte-pair encoding is employed separate vocabulary of about 42K and 31K tokens with 32K merge operations (Sennrich et al., 2016b). we chose NIST06 as the valid set and NIST02, NIST03, NIST04, NIS"
2020.coling-main.379,W16-2323,0,0.374803,"ts encoder and decoder architecture, therefore our method drops tokens for both source and target inputs. For the source side, model encoder learns intermediate representation by exponentially different incomplete sentences. For the target side, model decoder generates target translation condition on latent variable, weakening the constraint caused by teacher forcing. Both of them receives incomplete information from inputs, simulating the real situation (e.g unknown or unfamiliar data) at test time. 2.1 Token Drop methods We adopt three drop strategy for Token Drop: Zero-Out is introduced by Sennrich et al. (2016a), different from the standard dropout, the method drops full word by setting zero to word embedding during training. The deficiency is zero vector can not learn representation from its context in the self-attention layer. Drop-Tag (K˚ageb¨ack and Salomonsson, 2016) replaces token with a < dropped > tag. The tag is subsequently treated just like any other word in the vocabulary and has a corresponding word embedding that is trained. We adopt this technique for NMT to learn better feature representation. Unk-Tag replaces token with generic unknown word token < unk >. Bowman et al. (2016) and Y"
2020.coling-main.379,P16-1162,0,0.51272,"ts encoder and decoder architecture, therefore our method drops tokens for both source and target inputs. For the source side, model encoder learns intermediate representation by exponentially different incomplete sentences. For the target side, model decoder generates target translation condition on latent variable, weakening the constraint caused by teacher forcing. Both of them receives incomplete information from inputs, simulating the real situation (e.g unknown or unfamiliar data) at test time. 2.1 Token Drop methods We adopt three drop strategy for Token Drop: Zero-Out is introduced by Sennrich et al. (2016a), different from the standard dropout, the method drops full word by setting zero to word embedding during training. The deficiency is zero vector can not learn representation from its context in the self-attention layer. Drop-Tag (K˚ageb¨ack and Salomonsson, 2016) replaces token with a < dropped > tag. The tag is subsequently treated just like any other word in the vocabulary and has a corresponding word embedding that is trained. We adopt this technique for NMT to learn better feature representation. Unk-Tag replaces token with generic unknown word token < unk >. Bowman et al. (2016) and Y"
2020.coling-main.379,D18-1316,0,0.0227322,"nglish-Romanian benchmark demonstrate the effectiveness of our approach and our model achieves significant improvements over a strong Transformer baseline1 . 1 Introduction Neural machine translation (NMT) achieved enormous success in advancing the quality of translation (Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017). In spite of the impressive performance, NMT models are still vulnerable to perturbations in the input sentences (Belinkov and Bisk, 2018; Cheng et al., 2019) , i.e. a tiny perturbation will affect hidden representation and lead to low quality of translation (Zhao et al., 2018). Moreover, NMT commonly consists of millions of parameters, which making it prone to overfitting especially in low resource scene. A natural way to improve generalization is synthesizing natural noise (Karpukhin et al., 2019) or adopting arbitrary noise (Cheng et al., 2018; Ebrahimi et al., 2018). Another way is exploring regularization techniques to avoid overfitting (Miceli et al., 2017), making model robust to unseen or unfamiliar inputs. However, as discrete data, the text is hard to retain the semantic information after corruption. In this paper, we propose Token Drop to prevent overfitt"
2020.emnlp-main.196,W13-2322,0,0.230043,"Europe"" :name (n2 / name :op1 ""Europe"")))) (c) AMR Linearization ( consider-01 : ARG0 ( country : name ( name : op1 "" China "" ) ) : ARG1 ( partner-01 : ARG1 ( country : name ( name : op1 "" Germany "" ) ) : mod ( important : degree ( most ) ) : mod ( trade-01 ) : location ( continent : name ( name : op1 "" Europe "" ) ) ) ) Figure 1: An example of seq2seq-based AMR parsing. Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these appro"
2020.emnlp-main.196,D19-1393,0,0.671556,"p1 "" Europe "" ) ) ) ) Figure 1: An example of seq2seq-based AMR parsing. Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated da"
2020.emnlp-main.196,2020.acl-main.119,0,0.495375,"en the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated dataset. For example, even recent AMR 2.0 contains only 36.5K training AMRs. To alleviate the effect of such restriction, a previous attempt is to utilize large-scale unlabeled sentences with self-training (Konstas et al., 2017). Alternatively, a more recent feasible solution is to resor"
2020.emnlp-main.196,P13-2131,0,0.470685,"gap 6.9 between their corresponding baselines. • Finally, our approach achieves the best reported performance on AMR 1.0 and the performance on AMR 2.0 is higher than or close to that achieved by previous studies which use BERT. This is very encouraging taking into consideration the fact that our seq2seq model is much simper than the graph-based models proposed in related studies (Zhang et al., 2019a,b; Naseem et al., 2019; Cai and Lam, 2020). Evaluation Metrics For evaluation purpose, we use the AMR-evaluation toolkit to evaluate parsing performance in Smatch and other fine-grained metrics (Cai and Knight, 2013; Damonte et al., 2017). We report results of single models that are tuned on the development set. 4.2 Experimental Results Table 2 presents the comparison of our approach and related studies on the test sets of AMR 1.0 and AMR 2.0. From the results, we have the following observations: • Pre-trained models on a single task (i.e., from #2 to #6) significantly improve the performance of AMR parsing, indicating seq2seq pre-training is helpful for seq2seqbased AMR parsing. We also note that the pre-trained model of NMT achieves the best performance, followed by the pre-trained models on AMR parsin"
2020.emnlp-main.196,D16-1257,0,0.025459,"re-trained model. PTM-SynPar is a seq2seq constituent parsing model. Building such a model requires a training dataset which consists of sentences paired with constituency parse trees. To construct a silver treebank, we parse the English sentences in the bilingual data for MT by using an off-the-shelf parser. Then we linearize the automatic parse trees to get syntax sequences, as illustrated in Figure 2. Note that in the linearization, we let the output contain the words from the source sentence. The motivation here is to regard parsing as a language generation problem, similar to the idea in Choe and Charniak (2016). Intuitively, the above described single pre-trained models can capture linguistic features from different perspectives. One question is whether these models are complementary when they are properly used to initialize a seq2seq-based AMR parser. To empirically answer this question, we propose to build pre-trained models through jointly learning multiple pre-training tasks. Inspired by the zeroshot approach proposed for multi-lingual neural machine translation (Johnson et al., 2017), we add a unique preceding tag to the target side of training data to distinguish the task of each training inst"
2020.emnlp-main.196,E17-1051,0,0.436782,"2seq-based AMR parsing. Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated dataset. For example, even recent AMR 2.0 contains on"
2020.emnlp-main.196,N19-1423,0,0.136025,"ural Language Processing, pages 2501–2511, c November 16–20, 2020. 2020 Association for Computational Linguistics Task machine translation syntactic parsing AMR parsing Dataset gold silver silver Source sentence sentence sentence Target sentence tree sequence AMR sequence Table 1: Three seq2seq learning tasks explored in this paper to obtain pre-trained models. Here silver dataset indicates that the sequences in the target-side are generated automatically . incorporated into the training of an AMR parser. However, the widely used pre-trained models such as ELMO (Peters et al., 2017) and BERT (Devlin et al., 2019) may not work as expected for building a state-of-the-art seq2seq AMR parser. The reasons are two-fold. On the one hand, previous studies on both seq2seq-based AMR parsing and AMR-to-text generation demonstrate the necessity of a shared vocabulary for the source and target sides (Ge et al., 2019; Zhu et al., 2019). Using pretrained models like BERT as pre-trained encoders for AMR parsing, however, will violate the rule of sharing a vocabulary. On the other hand, pretrained models such as BERT are basically tuned for the purpose of representing sentences instead of generating target sequences."
2020.emnlp-main.196,N19-1409,0,0.0154628,"a universal model and then fine-tuning the model on a downstream task have recently become a popular strategy in the field of natural language processing. Previous works on pre-training can be roughly grouped into two categories. One category of approaches is to learn static word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) while the other group builds dynamic pre-trained models that would also be used in downstream tasks. Representative examples in the latter group in2508 clude Dai and Le (2015), CoVe (McCann et al., 2017), ELMo (Peters et al., 2017; Edunov et al., 2019), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). Besides the aforementioned encoder-only (e.g., BERT) or decoderonly (e.g., GPT) pre-training approaches, recent studies also propose approaches to pre-training seq2seq models, such as MASS (Song et al., 2019), PoDA (Wang et al., 2019), PEGASUS (Zhang et al., 2020), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). AMR Parsing. As a semantic parsing task that translates texts into AMR graphs, AMR parsing has received much attention in recent years. Diverse approaches have been applied to the task. Flanigan et al. (2014)"
2020.emnlp-main.196,P14-1134,0,0.581194,"01 ) : location ( continent : name ( name : op1 "" Europe "" ) ) ) ) Figure 1: An example of seq2seq-based AMR parsing. Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much"
2020.emnlp-main.196,P18-1170,0,0.102799,"Missing"
2020.emnlp-main.196,D18-1198,0,0.462111,". Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated dataset. For example, even recent AMR 2.0 contains only 36.5K training A"
2020.emnlp-main.196,P19-1356,0,0.0153463,"models, we merge all the source and target sides of the three pretraining tasks, and construct a shared vocabulary. Moreover, in all the models we share vocabulary embeddings for both the source and target sides. Transformer Encoder Task1: Task2: Task3: s1(1) … sn(1)1 s1(2) … sn(2) 2 s1(3) … sn(3)3 Transformer Decoder t1(1) … t m(1)1 T1 T2 t1(2) … t m(2)2 T3 t1(3) … t m(3)3 Figure 3: Illustration of the joint pre-training approach. PTM-MT is a seq2seq neural machine translation (NMT) model which is trained on a publicly available bilingual dataset. According to findings in Goldberg (2019) and Jawahar et al. (2019), the Transformer encoder is strong in capturing syntax and semantics from source sentences, which is helpful to AMR parsing. baseline system of AMR parsing to process the English sentences in the bilingual MT corpus. Then we adopt the linearization process illustrated in Figure 1 to obtain source-target pairs. Finally, we train a seq2seq-based AMR parsing model on the silver corpus that will be used as a pre-trained model. PTM-SynPar is a seq2seq constituent parsing model. Building such a model requires a training dataset which consists of sentences paired with constituency parse trees. To co"
2020.emnlp-main.196,Q17-1024,0,0.0607601,"Missing"
2020.emnlp-main.196,P17-4012,0,0.0224894,"ut 3.9M training sentence pairs after filtering out long and imbalanced pairs. To obtain syntactic parse trees for the source sentences, we utilize toolkit AllenNLP (Gardner et al., 2017) which is trained on Penn Treebank (Marcus et al., 1993). To obtain AMR graphs for the source sentences, we utilize our baseline AMR parsing system. Then we merge English/German sentences and linearized parse trees, and AMR graphs together and segment all the tokens into subwords by byte pair encoding (BPE) (Sennrich et al., 2016) with 20K operations. We implement above pre-trained models based on OpenNMT-py (Klein et al., 2017).3 For simplicity, we unify parameters of these models as the Transformer-base model in Vaswani et al. (2017). The number of layers in encoder and decoder is 6 while the number of heads is 8. Both the embedding size and the hidden size are 512 while the size of feedforward network is 2048. Moreover, we use Adam optimizer (Kingma and Ba, 2015) with β1 of 0.9 and β2 of 0.998. Warm up step, learning rate, dropout rate and label smoothing epsilon are 16000, 2.0, 0.1 and 0.1 respectively. In addition, we set the batch token-size to 8,192. We train the models for 300K steps and choose the model 2 ht"
2020.emnlp-main.196,P17-1014,0,0.475488,"irected and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated dataset. For example, even recent AMR 2.0 contains only 36.5K training AMRs. To alleviate the effect of such restriction, a previous attempt is to utilize large-scale unlabeled sent"
2020.emnlp-main.196,2020.acl-main.703,0,0.0612792,"2014) while the other group builds dynamic pre-trained models that would also be used in downstream tasks. Representative examples in the latter group in2508 clude Dai and Le (2015), CoVe (McCann et al., 2017), ELMo (Peters et al., 2017; Edunov et al., 2019), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). Besides the aforementioned encoder-only (e.g., BERT) or decoderonly (e.g., GPT) pre-training approaches, recent studies also propose approaches to pre-training seq2seq models, such as MASS (Song et al., 2019), PoDA (Wang et al., 2019), PEGASUS (Zhang et al., 2020), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). AMR Parsing. As a semantic parsing task that translates texts into AMR graphs, AMR parsing has received much attention in recent years. Diverse approaches have been applied to the task. Flanigan et al. (2014) pioneer the research work on AMR parsing by using a a two-stage approach: node identification followed by relation recognition. Werling et al. (2015) improve the first stage in the parser of Flanigan et al. (2014) by generating subgraph aligned to lexical items. To avoid conducting AMR parsing from scratch, Wang et al. (2015b) propose to obtain AMR graphs f"
2020.emnlp-main.196,P17-1064,1,0.825829,"y as the encoder, a more reasonable approach is to utilize BERT as an extra feature or view BERT as an extra encoder. See Section 5.1 for more detailed discussions on the effect of BERT on AMR parsing. In this paper, we propose to pre-train seq2seq models that aim to capture different linguistic knowledge from input sentences. To build such pre-trained models, we explore three different yet relevant seq2seq tasks, as listed in Table 1. Here, machine translation acts as the most representative seq2seq task which takes a bilingual dataset as the training data. According to Shi et al. (2016) and Li et al. (2017), a machine translation system with good performance requires the model to well derive linguistic information from input sentences. The other two tasks require auto-parsed syntactic parse trees and AMR graphs as the training data, respectively. It is worth noting that the pre-training task of AMR parsing is in the similar spirit of selftraining (Konstas et al., 2017). In order to investigate whether various seq2seq pre-trained models are complementary to each other in the sense that they can be learned jointly to achieve better performance, we further explore joint learning of several pre-trai"
2020.emnlp-main.196,P18-1037,0,0.321386,"ds, the BERT tokenizer segments it into a subword sequence x0 = (x01 , · · · , x0m ) with m 2505 # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 AMR 1.0 P. R. F1 None None 69.8 60.2 64.6 Vanilla 78.8 69.5 73.8 PTM-MT MTL 81.1 72.2 76.4 Vanilla 74.3 65.8 69.8 PTM-SynPar MTL 76.7 68.1 72.2 PTM-SemPar Vanilla 80.8 73.5 77.0 Vanilla 79.1 70.5 74.6 PTM-MT-SynPar MTL 81.2 74.0 77.5 Vanilla 82.3 75.4 78.7 PTM-MT-SemPar MTL 82.4 74.6 78.3 Vanilla 81.6 74.0 77.6 PTM-SynPar-SemPar MTL 81.8 74.0 77.7 Vanilla 82.4 75.4 78.7 PTM-MT-SynPar-SemPar MTL 82.6 75.9 79.1 Previous work without extra resources Graph Prediction(Lyu and Titov, 2018) Prediction(Guo and Lu, 2018) Prediction(Groschwitz et al., 2018) Seq2Seq(Ge et al., 2019) Seq2Seq(Cai and Lam, 2019) Graph(Cai and Lam, 2020) 71.2 Previous work with extra resources Seq2Graph(Zhang et al., 2019a)† 70.2 71.3 Seq2Graph(Zhang et al., 2019b)† RL(Naseem et al., 2019)† Seq2Seq(Ge et al., 2019)∗ Graph(Cai and Lam, 2020)† 75.4 Pre-trained Model Fine-Tune AMR 2.0 P. R. F1 75.8 67.7 71.5 80.0 74.3 77.1 81.3 77.1 79.1 76.2 71.5 73.8 78.0 72.8 75.3 80.8 75.2 77.9 79.5 75.0 77.1 81.5 77.6 79.5 82.4 77.3 79.7 82.3 78.0 80.1 81.1 76.3 78.6 81.3 76.8 79.0 82.1 77.6 79.8 82.3 78.3 80.2 74.4 6"
2020.emnlp-main.196,J93-2004,0,0.0824997,"M-MT-SynPar-SemPar, in MTL fine-tuning we only keep the pre-training tasks of MT and syntactic parsing. 4 Experimentation In this section, we report the performance of our seq2seq pre-training approach to AMR parsing. 4.1 Experimental Settings Pre-training Dataset and Pre-trained Models For pre-trained models, we use the WMT14 English-to-German dataset2 which consists of about 3.9M training sentence pairs after filtering out long and imbalanced pairs. To obtain syntactic parse trees for the source sentences, we utilize toolkit AllenNLP (Gardner et al., 2017) which is trained on Penn Treebank (Marcus et al., 1993). To obtain AMR graphs for the source sentences, we utilize our baseline AMR parsing system. Then we merge English/German sentences and linearized parse trees, and AMR graphs together and segment all the tokens into subwords by byte pair encoding (BPE) (Sennrich et al., 2016) with 20K operations. We implement above pre-trained models based on OpenNMT-py (Klein et al., 2017).3 For simplicity, we unify parameters of these models as the Transformer-base model in Vaswani et al. (2017). The number of layers in encoder and decoder is 6 while the number of heads is 8. Both the embedding size and the"
2020.emnlp-main.196,P19-1451,0,0.543462,"we move from single pre-training models to joint pre-training models. For example, based on PTM-MT-SynParSemPar, the performance gap is 1.1 in Smatch F1 scores, much less than the performance gap 6.9 between their corresponding baselines. • Finally, our approach achieves the best reported performance on AMR 1.0 and the performance on AMR 2.0 is higher than or close to that achieved by previous studies which use BERT. This is very encouraging taking into consideration the fact that our seq2seq model is much simper than the graph-based models proposed in related studies (Zhang et al., 2019a,b; Naseem et al., 2019; Cai and Lam, 2020). Evaluation Metrics For evaluation purpose, we use the AMR-evaluation toolkit to evaluate parsing performance in Smatch and other fine-grained metrics (Cai and Knight, 2013; Damonte et al., 2017). We report results of single models that are tuned on the development set. 4.2 Experimental Results Table 2 presents the comparison of our approach and related studies on the test sets of AMR 1.0 and AMR 2.0. From the results, we have the following observations: • Pre-trained models on a single task (i.e., from #2 to #6) significantly improve the performance of AMR parsing, indica"
2020.emnlp-main.196,E17-1035,0,0.0758254,"ims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated dataset. For example, even recent AMR 2.0 contains only 36.5K training AMRs. To alleviate the effect of such restriction, a previous a"
2020.emnlp-main.196,D14-1162,0,0.0822054,"Missing"
2020.emnlp-main.196,P16-1162,0,0.0160956,"rained Models For pre-trained models, we use the WMT14 English-to-German dataset2 which consists of about 3.9M training sentence pairs after filtering out long and imbalanced pairs. To obtain syntactic parse trees for the source sentences, we utilize toolkit AllenNLP (Gardner et al., 2017) which is trained on Penn Treebank (Marcus et al., 1993). To obtain AMR graphs for the source sentences, we utilize our baseline AMR parsing system. Then we merge English/German sentences and linearized parse trees, and AMR graphs together and segment all the tokens into subwords by byte pair encoding (BPE) (Sennrich et al., 2016) with 20K operations. We implement above pre-trained models based on OpenNMT-py (Klein et al., 2017).3 For simplicity, we unify parameters of these models as the Transformer-base model in Vaswani et al. (2017). The number of layers in encoder and decoder is 6 while the number of heads is 8. Both the embedding size and the hidden size are 512 while the size of feedforward network is 2048. Moreover, we use Adam optimizer (Kingma and Ba, 2015) with β1 of 0.9 and β2 of 0.998. Warm up step, learning rate, dropout rate and label smoothing epsilon are 16000, 2.0, 0.1 and 0.1 respectively. In addition"
2020.emnlp-main.196,D16-1159,0,0.0162119,"to using BERT directly as the encoder, a more reasonable approach is to utilize BERT as an extra feature or view BERT as an extra encoder. See Section 5.1 for more detailed discussions on the effect of BERT on AMR parsing. In this paper, we propose to pre-train seq2seq models that aim to capture different linguistic knowledge from input sentences. To build such pre-trained models, we explore three different yet relevant seq2seq tasks, as listed in Table 1. Here, machine translation acts as the most representative seq2seq task which takes a bilingual dataset as the training data. According to Shi et al. (2016) and Li et al. (2017), a machine translation system with good performance requires the model to well derive linguistic information from input sentences. The other two tasks require auto-parsed syntactic parse trees and AMR graphs as the training data, respectively. It is worth noting that the pre-training task of AMR parsing is in the similar spirit of selftraining (Konstas et al., 2017). In order to investigate whether various seq2seq pre-trained models are complementary to each other in the sense that they can be learned jointly to achieve better performance, we further explore joint learnin"
2020.emnlp-main.196,P15-2141,0,0.484496,": ARG1 ( country : name ( name : op1 "" Germany "" ) ) : mod ( important : degree ( most ) ) : mod ( trade-01 ) : location ( continent : name ( name : op1 "" Europe "" ) ) ) ) Figure 1: An example of seq2seq-based AMR parsing. Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in imple"
2020.emnlp-main.196,N15-1040,0,0.543204,": ARG1 ( country : name ( name : op1 "" Germany "" ) ) : mod ( important : degree ( most ) ) : mod ( trade-01 ) : location ( continent : name ( name : op1 "" Europe "" ) ) ) ) Figure 1: An example of seq2seq-based AMR parsing. Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in imple"
2020.emnlp-main.196,D19-1412,0,0.0250793,"vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) while the other group builds dynamic pre-trained models that would also be used in downstream tasks. Representative examples in the latter group in2508 clude Dai and Le (2015), CoVe (McCann et al., 2017), ELMo (Peters et al., 2017; Edunov et al., 2019), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). Besides the aforementioned encoder-only (e.g., BERT) or decoderonly (e.g., GPT) pre-training approaches, recent studies also propose approaches to pre-training seq2seq models, such as MASS (Song et al., 2019), PoDA (Wang et al., 2019), PEGASUS (Zhang et al., 2020), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). AMR Parsing. As a semantic parsing task that translates texts into AMR graphs, AMR parsing has received much attention in recent years. Diverse approaches have been applied to the task. Flanigan et al. (2014) pioneer the research work on AMR parsing by using a a two-stage approach: node identification followed by relation recognition. Werling et al. (2015) improve the first stage in the parser of Flanigan et al. (2014) by generating subgraph aligned to lexical items. To avoid conducting AMR parsing from sc"
2020.emnlp-main.196,P15-1095,0,0.0984235,"Missing"
2020.emnlp-main.196,P19-1009,0,0.47722,"Missing"
2020.emnlp-main.196,D19-1392,0,0.545394,"Missing"
2020.emnlp-main.196,D19-1548,1,0.823865,"d in this paper to obtain pre-trained models. Here silver dataset indicates that the sequences in the target-side are generated automatically . incorporated into the training of an AMR parser. However, the widely used pre-trained models such as ELMO (Peters et al., 2017) and BERT (Devlin et al., 2019) may not work as expected for building a state-of-the-art seq2seq AMR parser. The reasons are two-fold. On the one hand, previous studies on both seq2seq-based AMR parsing and AMR-to-text generation demonstrate the necessity of a shared vocabulary for the source and target sides (Ge et al., 2019; Zhu et al., 2019). Using pretrained models like BERT as pre-trained encoders for AMR parsing, however, will violate the rule of sharing a vocabulary. On the other hand, pretrained models such as BERT are basically tuned for the purpose of representing sentences instead of generating target sequences. According to Zhu et al. (2020), by contrast to using BERT directly as the encoder, a more reasonable approach is to utilize BERT as an extra feature or view BERT as an extra encoder. See Section 5.1 for more detailed discussions on the effect of BERT on AMR parsing. In this paper, we propose to pre-train seq2seq m"
2020.findings-emnlp.179,K18-1048,0,0.0258478,"echanism to locate the most relevant historical customer utterances and seller utterances, and then produce their context representations. • We use a gated strategy to generate the final response by comprehensively considering the different importance of current dialogue and historical dialogues under a hybrid network. • Empirical results show that our proposed approach outperforms state-of-the-art competitors significantly on a real-world multi-turn customer service dialogue dataset with both automatic and manual evaluation. 2 Related Work Previous research on multi-turn dialogue generation (Chaudhuri et al., 2018; Zhou et al., 2018; Olabiyi et al., 2018) has drawn a huge amount of attention from academia and industry, which has broader usage scenario than single-turn dialogue generation (Zhang et al., 2018; Li et al., 2017). Serban et al. (2016); Chen et al. (2018); Wu et al. (2016) proposed a hierarchical encoderdecoder framework to model all the context utterances which can better grasp the overall information of the dialogues. However, these models are difficult to generalize, and their results are unsatisfied since responses maybe vary a lot for the same question towards different occasions and sp"
2020.findings-emnlp.179,D17-1230,0,0.0279764,"ering the different importance of current dialogue and historical dialogues under a hybrid network. • Empirical results show that our proposed approach outperforms state-of-the-art competitors significantly on a real-world multi-turn customer service dialogue dataset with both automatic and manual evaluation. 2 Related Work Previous research on multi-turn dialogue generation (Chaudhuri et al., 2018; Zhou et al., 2018; Olabiyi et al., 2018) has drawn a huge amount of attention from academia and industry, which has broader usage scenario than single-turn dialogue generation (Zhang et al., 2018; Li et al., 2017). Serban et al. (2016); Chen et al. (2018); Wu et al. (2016) proposed a hierarchical encoderdecoder framework to model all the context utterances which can better grasp the overall information of the dialogues. However, these models are difficult to generalize, and their results are unsatisfied since responses maybe vary a lot for the same question towards different occasions and speakers. Recent studies have noticed the problem and try to alleviate it by incorporating helpful external information into response generation, e.g., speakers’ emotional information. (Zhang et al., 2019a,b; Wang et"
2020.findings-emnlp.179,P02-1040,0,0.114129,"Missing"
2020.findings-emnlp.179,P19-1362,0,0.0962756,"et al., 2018; Li et al., 2017). Serban et al. (2016); Chen et al. (2018); Wu et al. (2016) proposed a hierarchical encoderdecoder framework to model all the context utterances which can better grasp the overall information of the dialogues. However, these models are difficult to generalize, and their results are unsatisfied since responses maybe vary a lot for the same question towards different occasions and speakers. Recent studies have noticed the problem and try to alleviate it by incorporating helpful external information into response generation, e.g., speakers’ emotional information. (Zhang et al., 2019a,b; Wang et al., 2020). Zhao et al. (2019) proposed a review response generation model in the E-commerce platform, which used the reinforcement learning and copy mechanism to fuse external product information, thereby generating informative and diverse responses. Zheng et al. (2019) proposed a dialogue generation model considering personality traits such as age, name, and gender. Meng et al. (2019) proposed RefNet, which used background descriptions about the target dialogue and used a copy mechanism to copy tokens or semantic units. However, all these models are difficult to generalize in re"
2020.findings-emnlp.179,P18-1103,0,0.0115415,"ost relevant historical customer utterances and seller utterances, and then produce their context representations. • We use a gated strategy to generate the final response by comprehensively considering the different importance of current dialogue and historical dialogues under a hybrid network. • Empirical results show that our proposed approach outperforms state-of-the-art competitors significantly on a real-world multi-turn customer service dialogue dataset with both automatic and manual evaluation. 2 Related Work Previous research on multi-turn dialogue generation (Chaudhuri et al., 2018; Zhou et al., 2018; Olabiyi et al., 2018) has drawn a huge amount of attention from academia and industry, which has broader usage scenario than single-turn dialogue generation (Zhang et al., 2018; Li et al., 2017). Serban et al. (2016); Chen et al. (2018); Wu et al. (2016) proposed a hierarchical encoderdecoder framework to model all the context utterances which can better grasp the overall information of the dialogues. However, these models are difficult to generalize, and their results are unsatisfied since responses maybe vary a lot for the same question towards different occasions and speakers. Recent stud"
2021.acl-long.201,2020.acl-main.721,0,0.0204073,"dge of one document type cannot be easily transferred into another, so that these models often need to be re-trained once the document type is changed. Thereby the local invariance in general document layout (key-value pairs in a left-right layout, tables in a grid layout, etc.) cannot be fully exploited. To this end, the second direction relies on the deep fusion among textual, visual, and layout information from a great number of unlabeled documents in different domains, where pre-training techniques play an important role in learning the cross-modality interaction in an end-to-end fashion (Lockard et al., 2020; Xu et al., 2020). In this way, the pre-trained 2579 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2579–2591 August 1–6, 2021. ©2021 Association for Computational Linguistics models absorb cross-modal knowledge from different document types, where the local invariance among these layouts and styles is preserved. Furthermore, when the model needs to be transferred into another domain with different document formats, only a few labeled samples would be sufficient to fine-t"
2021.acl-long.201,N18-2074,0,0.0284803,"his paper follows the second direction, and we explore how to further improve the pre-training strategies for the VrDU tasks. In this paper, we present an improved version of LayoutLM (Xu et al., 2020), aka LayoutLMv2. Different from the vanilla LayoutLM model where visual embeddings are combined in the fine-tuning stage, we integrate the visual information in the pre-training stage in LayoutLMv2 by taking advantage of the Transformer architecture to learn the cross-modality interaction between visual and textual information. In addition, inspired by the 1-D relative position representations (Shaw et al., 2018; Raffel et al., 2020; Bao et al., 2020), we propose the spatial-aware self-attention mechanism for LayoutLMv2, which involves a 2-D relative position representation for token pairs. Different from the absolute 2-D position embeddings that LayoutLM uses to model the page layout, the relative position embeddings explicitly provide a broader view for the contextual spatial modeling. For the pre-training strategies, we use two new training objectives for LayoutLMv2 in addition to the masked visual-language modeling. The first is the proposed text-image alignment strategy, which aligns the text li"
2021.acl-long.201,2020.acl-main.580,0,0.256687,"y the style and format of each type as well as the document content. Therefore, to accurately recognize the text fields of interest, it is inevitable to take advantage of the cross-modality nature of visually-rich documents, where the textual, visual, and layout information should be jointly modeled and learned end-to-end in a single framework. The recent progress of VrDU lies primarily in two directions. The first direction is usually built on the shallow fusion between textual and visual/layout/style information (Yang et al., 2017; Liu et al., 2019; Sarkhel and Nandi, 2019; Yu et al., 2020; Majumder et al., 2020; Wei et al., 2020; Zhang et al., 2020). These approaches leverage the pre-trained NLP and CV models individually and combine the information from multiple modalities for supervised learning. Although good performance has been achieved, the domain knowledge of one document type cannot be easily transferred into another, so that these models often need to be re-trained once the document type is changed. Thereby the local invariance in general document layout (key-value pairs in a left-right layout, tables in a grid layout, etc.) cannot be fully exploited. To this end, the second direction relie"
2021.acl-long.201,D16-1264,0,0.127266,"Missing"
2021.acl-long.201,D19-1514,0,0.0179133,"ition representation for token pairs. Different from the absolute 2-D position embeddings that LayoutLM uses to model the page layout, the relative position embeddings explicitly provide a broader view for the contextual spatial modeling. For the pre-training strategies, we use two new training objectives for LayoutLMv2 in addition to the masked visual-language modeling. The first is the proposed text-image alignment strategy, which aligns the text lines and the corresponding image regions. The second is the text-image matching strategy popular in previous vision-language pre-training models (Tan and Bansal, 2019; Lu et al., 2019; Su et al., 2020; Chen et al., 2020; Sun et al., 2019), where the model learns whether the document image and textual content are correlated. We select six publicly available benchmark datasets as the downstream tasks to evaluate the performance of the pre-trained LayoutLMv2 model, which are the FUNSD dataset (Jaume et al., 2019) for form understanding, the CORD dataset (Park et al., 2019) and the SROIE dataset (Huang et al., 2019) for receipt understanding, the Kleister-NDA dataset (Grali´nski et al., 2020) for long document understanding with a complex layout, the RVL-CDIP"
2021.acl-long.222,N18-1118,0,0.0171027,"SR alone. Table 6: Performance (BLEU scores) on dev and test sets of ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model t"
2021.acl-long.222,2012.eamt-1.60,0,0.0139028,"parallel dataset and the monolingual document and segment words into sub-words by a BPE model with 30K (25K) operations (Sennrich et al., 2016). Fine-tuning data settings. For ZH-EN, we have one translation task on news domain. The document-level parallel corpus of training set include 41K documents with 780K sentence pairs.8 We use the NIST MT 2006 dataset as the development set, and combine the NIST MT 2002, 2003, 2004, 2005, 2008 datasets as test set.. For EN-DE, we test three translation tasks in domains of TED talks, News-Commentary and Europarl. • TED, which is from IWSLT 2017 MT track (Cettolo et al., 2012). We combine test2016 and test2017 as our test set while the rest as the development set. Experimentation To test the effect of our approach in leveraging sentence-level parallel dataset and monolingual documents, we carry out experiments on Chineseto-English (ZH-EN) and English-to-German (ENDE) translation. 4.1 Experimental Settings Pre-training data settings. The ZH-EN sentence-level parallel dataset contains 2.0M sentence pairs with 54.8M Chinese words and 60.8M English words.4 We use WMT14 EN-DE 4 It consists of LDC2002E18, LDC2003E07, LDC2003E14, news part of LDC2004T08, LDC2002T01, LDC20"
2021.acl-long.222,Q17-1024,0,0.049404,"Missing"
2021.acl-long.222,W19-5321,0,0.0420252,"Missing"
2021.acl-long.222,2020.emnlp-main.175,0,0.0462114,"ory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the encoder. They simply concatenate sentences within a document int"
2021.acl-long.222,P17-4012,0,0.0297593,"rovement of averaged 1.36 BLEU and 1.72 Meteor. Moreover, when we use documents as translation units, our models (i.e., #3 Ours-doc) achieve further improvement by modeling document-level context. Compared to previous studies, it also shows that our approach surpasses all context-aware baselines on ZH-EN and EN-DE (TED) tasks and achieves the state-ofthe-art on average. In the scenario where both sentence-level parallel dataset and monolingual documents are used,12 similar performance trends also hold. For example, #5 Ours-sent significantly exceeds Transformer Model settings. We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it.11 For all translation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., Ng , Ne , and Nd in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respectively. The number of heads in multi-head attention is 8 and the dropout rate is 0.1. In pre-training, we train the models for 500K steps on four V100 GPUs with batch-size 8192. We use Adam (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98 for optimization, and learning rate as 1, the warm-up step as 16K."
2021.acl-long.222,P07-2045,0,0.0126851,"Missing"
2021.acl-long.222,C18-1050,1,0.815965,"example, will be X respectively. Table 7 compares the performance when the pre-training task is of CA-MSR objective or combination of CA-GSR and CA-MSR.It Related Work We describe related studies in the following two perspectives. 6.1 5.4 Test 50.03 49.73 shows the combining objective achieves better performance than using CA-MSR alone. Table 6: Performance (BLEU scores) on dev and test sets of ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several atte"
2021.acl-long.222,W07-0734,0,0.0412222,"on test sets. Bi-sent/Mo-doc indicates if the models are pretrained on sentence-level parallel dataset or monolingual documents (7 for no and 3 for yes). Ours-sent/Ours-doc indicates that we use sentences or documents as input units, i.e., performing sentence-level NMT or context-aware NMT. Scores are obtained by running their source code with our model settings. • Europarl, which is extracted from the Europarl v7. The training, development and test sets are obtained through randomly splitting the corpus. Evaluation. For evaluation, we use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate translation quality. All above EN-DE document-level parallel datasets are downloaded from Maruf et al. (2019).10 Similar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pretraining datasets. See Appendix A for more statistics of the fine-tuning datasets. Main results. Table 1 shows the performance of our approach, where Ours-sent and Ours-doc indicate the performance achieved by our approach when we use sentences or documents as input units, re"
2021.acl-long.222,N19-1423,0,0.0188033,"context-aware NMT performance in the scenarios where the document-level parallel dataset is scale-limited, or even not available. On the one hand, sentence-level parallel dataset is a natural resource to use. For example, Zhang et al. (2018) propose a two-stage training strategy for context-aware NMT by pre-training the model on a sentencelevel parallel dataset. On the other hand, JunczysDowmunt (2019) leverage large-scale source-side monolingual documents, in which they simply concatenate sentences within a document into a long sequence and explore multi-task training via the BERT-objective (Devlin et al., 2019) on the encoder. Due to that different models are usually required to model sentences and documents, however, it is challenging to effectively take them both in a single model. In order to effectively and simultaneously model 2 We note that not all, but many context-aware NMT models contain a context encoder to extract global context information from the document. 2851 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2851–2861 August 1–6, 2021. ©2021 Association for Computat"
2021.acl-long.222,2020.acl-main.703,0,0.0431285,"tencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the encoder. They simply concatenate sentences within a document into a long sequence, which is different from our approach. 6.2 Pre-training for Document-Level NMT While there are substantial studies on improving sentence-level NMT with pre-training, we limit ourselves here to pre-training for document-level (context-aware) NMT. BART (Lewis et al., 2020) is a denoising auto-encoder model which learns to reconstruct the original document from a noised version. Inspired by BART, mBART (Liu et al., 2858 2020) is a model trained on a mixed corpus containing monolingual documents of different languages. Both BART and mBART concatenate sentences in one document into a long sequence, and thus fall into a standard sequence-to-sequence (seq2seq) framework. This is very different from our cross-task pre-training, in which we combine both context-agnostic learning and context-aware learning in a single model. 7 Conclusion In order to leverage both large"
2021.acl-long.222,2020.wmt-1.71,0,0.0581681,"f ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (20"
2021.acl-long.222,2020.tacl-1.47,0,0.0324015,"Missing"
2021.acl-long.222,P18-1118,0,0.0816908,"cument-level perspectives. Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents. 1 Introduction Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context. Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few). However, the performance of contextaware NMT still suffers from the size of parallel document dataset. On the one hand, unlike ∗ Corresponding Author: Junhui Li. If not specified, monolingual documents are all for sourceside through this paper. 1 sentence-level translation models which could be well trained on large-scale sentence-level parallel datasets, the translation models of context-aware NMT may result in insufficient training. On the other hand, with only scale-limited source-side documents, the context encoders may fail to effectively extract use"
2021.acl-long.222,N19-1313,0,0.223252,". Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents. 1 Introduction Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context. Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few). However, the performance of contextaware NMT still suffers from the size of parallel document dataset. On the one hand, unlike ∗ Corresponding Author: Junhui Li. If not specified, monolingual documents are all for sourceside through this paper. 1 sentence-level translation models which could be well trained on large-scale sentence-level parallel datasets, the translation models of context-aware NMT may result in insufficient training. On the other hand, with only scale-limited source-side documents, the context encoders may fail to effectively extract useful context from the"
2021.acl-long.222,D18-1325,0,0.0263752,"Missing"
2021.acl-long.222,P02-1040,0,0.109336,"rformance (BLEU and Meteor scores) on test sets. Bi-sent/Mo-doc indicates if the models are pretrained on sentence-level parallel dataset or monolingual documents (7 for no and 3 for yes). Ours-sent/Ours-doc indicates that we use sentences or documents as input units, i.e., performing sentence-level NMT or context-aware NMT. Scores are obtained by running their source code with our model settings. • Europarl, which is extracted from the Europarl v7. The training, development and test sets are obtained through randomly splitting the corpus. Evaluation. For evaluation, we use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate translation quality. All above EN-DE document-level parallel datasets are downloaded from Maruf et al. (2019).10 Similar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pretraining datasets. See Appendix A for more statistics of the fine-tuning datasets. Main results. Table 1 shows the performance of our approach, where Ours-sent and Ours-doc indicate the performance achieved by our approach when we use sent"
2021.acl-long.222,P16-1162,0,0.127889,"Missing"
2021.acl-long.222,D19-1168,1,0.714836,"storation. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the enco"
2021.acl-long.222,W17-4811,0,0.022607,"from both sentencelevel and document-level perspectives. Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents. 1 Introduction Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context. Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few). However, the performance of contextaware NMT still suffers from the size of parallel document dataset. On the one hand, unlike ∗ Corresponding Author: Junhui Li. If not specified, monolingual documents are all for sourceside through this paper. 1 sentence-level translation models which could be well trained on large-scale sentence-level parallel datasets, the translation models of context-aware NMT may result in insufficient training. On the other hand, with only scale-limited source-side documents, the context encoders may fail t"
2021.acl-long.222,W17-4802,0,0.0221713,"Missing"
2021.acl-long.222,D19-1164,0,0.0578752,"ed studies, we lowercase English sentences in ZHEN while truecase English and German sentences in EN-DE. 8 It consists of LDC2002T01, LDC2004T07, LDC2005T06, LDC2005T10, LDC2009T02, LDC2009T15, LDC2010T03. Note that they are also included in ZH-EN parallel dataset. 9 http://www.casmacat.eu/corpus/news-co mmentary.html 2855 Model # ZH-EN BLEU Meteor 7 40.32 27.93 7 40.83 28.19 7 41.01 28.37 7 7 40.92 28.25 7 39.64 27.56 7 40.73 27.97 7 41.27 28.46 3 46.30 32.91 3 49.58 35.97 3 50.03 36.50 Bi- Mosent doc DocT (Zhang et al., 2018) 7 HAN (Miculicich et al., 2018) 7 SAN (Maruf et al., 2019) 7 QCN (Yang et al., 2019) 7 MCN (Zheng et al., 2020) 7 #1 Transformer 7 7 #2 Ours-sent #3 Ours-doc 7 #4 Transformer 3 #5 Ours-sent 3 #6 Ours-doc 3 EN-DE (TED) BLEU Meteor 24.00 44.69 24.58 45.48 24.42 45.26 25.19 46.09 25.10 23.02 43.66 24.75 45.83 25.31 46.30 26.94 47.06 28.73 48.80 29.31 49.40 EN-DE (News) BLEU Meteor 23.08 42.40 25.03 44.02 24.84 44.17 22.37 41.88 24.91 22.03 41.37 24.19 43.96 24.70 44.38 26.80 46.99 28.41 48.52 29.01 48.83 EN-DE (Europarl) BLEU Meteor 29.32 46.72 28.60 46.09 29.75 47.22 29.82 47.86 30.40 28.65 45.83 29.10 47.55 30.07 47.93 29.90 47.50 30.61 48.29 31.52 49.02 Avg. BLEU Meteor 29.18"
2021.acl-long.222,2020.tacl-1.23,0,0.0232631,"g et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the encoder. They simply concatenate sentences within a document into a long sequence, which is different from our approach. 6.2 Pre-training for Document-Level NMT While there are substantial studies on improving sentence-level NMT with pre-training, we limit ourselves here to pre-training for document-level (context-aware) NMT. BART (Lewis et al., 2020) is a denoising auto-encoder model which"
2021.acl-long.222,D18-1049,1,0.913664,"o break the corpus bottleneck for context-aware NMT by leveraging both largescale sentence-level parallel dataset and monolingual documents. Specifically, we aim to use the former to boost the performance of translation models while employ the latter to enhance the context encoders’ capability of capturing useful context information. There have been several attempts to boost context-aware NMT performance in the scenarios where the document-level parallel dataset is scale-limited, or even not available. On the one hand, sentence-level parallel dataset is a natural resource to use. For example, Zhang et al. (2018) propose a two-stage training strategy for context-aware NMT by pre-training the model on a sentencelevel parallel dataset. On the other hand, JunczysDowmunt (2019) leverage large-scale source-side monolingual documents, in which they simply concatenate sentences within a document into a long sequence and explore multi-task training via the BERT-objective (Devlin et al., 2019) on the encoder. Due to that different models are usually required to model sentences and documents, however, it is challenging to effectively take them both in a single model. In order to effectively and simultaneously m"
2021.acl-long.222,Q18-1029,0,0.0560714,"Missing"
2021.acl-long.222,D19-1081,0,0.0570404,"el translation as a fine-tuning task. Table 3 compares the performance with respect to different fine-tuning strategies and different input units in inferring. When we use documents as input units in inferring, the joint fine-tuning strategy provides no advantage. However, when the input units are sentences, the joint fine-tuning strategy outperforms the one not including sentence-level translation in fine-tuning. 5.2 Analysis of Discourse Phenomena We also want to examine whether the proposed approach actually learns to utilize document context to resolve discourse inconsistencies. Following Voita et al. (2019b) and Zheng et al. (2020), we use the same datasets to train model and contrastive test set for the evaluation of discourse phenomena for English-Russian by Voita et al. (2019b). There are four test sets in the suite regarding deixis, lexicon consistency, ellipsis (inflection and verb phrase). Each testset contains groups of contrastive examples consisting of a positive translation with correct discourse phenomenon and negative translations with incorrect phenomena. The goal is to figure out if a model is more likely to generate a cor2857 Model Trans. Ours Bi-sent Mo-doc 7 7 7 7 Dev 67.30 68."
2021.acl-long.222,P19-1116,0,0.0302677,"Missing"
2021.acl-long.222,P18-1117,0,0.0178409,"rformance (BLEU scores) on dev and test sets of ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-le"
2021.acl-long.222,D17-1301,0,0.0138548,"ares the performance when the pre-training task is of CA-MSR objective or combination of CA-GSR and CA-MSR.It Related Work We describe related studies in the following two perspectives. 6.1 5.4 Test 50.03 49.73 shows the combining objective achieves better performance than using CA-MSR alone. Table 6: Performance (BLEU scores) on dev and test sets of ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monoli"
2021.acl-long.452,D14-1082,0,0.0607765,"iani and Titov, 2017; Zhang et al., 2018). We follow the implementation of Zhang et al. (2018) and use a two-layer GCN as a more sophisticated way. In order to utilize labels, we extend vanilla GCN to have the same input with LabelCharLSTM, i.e., zk . We obtain the final word representation by performing average pooling over the output vectors of the top-layer GCN. 6.2 Experiments The basic parser applys a BiLSTM over character sequence to obtain word representation. In this part, we propose two simple alternative methods to encode internal structure shown in Figure 1-(c). Settings. Following Chen and Manning (2014), we conduct experiments on CTB5 with the same data split (16,091/803/1,910 sentences) and constituent-to-dependency conversion. Both char/label embeddings are randomly initialized and have the same dimension of 50. For the parsers using gold-standard POS tags, we randomly initialized the POS tagging embeddings and set the dimension to 50. For other hyperparameters, we adopt the default configuration of SuPar, including the pre-trained word embeddings. For multi-char words without annotated internal structure, we use the automatic outputs from the trained parser with BERT in Section 5, so that"
2021.acl-long.452,cheng-etal-2014-parsing,0,0.0464199,"Missing"
2021.acl-long.452,P15-2043,0,0.025891,"tribution measurement. Instead of treating a word as a bag of characters, we experiment with two simple ways to obtain structure-aware word representations. Meanwhile, enhancing their approach with explicit wordinternal structure could be also very interesting. Utilizing word-internal structure. Wordinternal structure have been explored in various NLP tasks. Several works propose to learn wordinternal structure, word segmentation, POS tagging and parsing jointly (Zhang et al., 2013, 2014; Li et al., 2018), demonstrating the effectiveness of word-internal structure in helping downstream tasks. Cheng et al. (2015) attempt to convert words into fine-grained subwords according to the 2 Following this direction, studies tried to explore more character information for better Chinese word representation, such as strokes (Cao et al., 2018) and ideographic shape (Sun et al., 2019). 5825 internal structure of words for better dealing with unknown words during word segmentation. Lin et al. (2020) propose to integrate the representation of word-internal structure into the input of neural machine translation model, leading to improved translation performance. 3 Word-internal Structure Annotation In this section,"
2021.acl-long.452,Q16-1026,0,0.0418986,"y, we can see that most previous studies adopted quite shallow hierarchical structure. In contrast, this work presents a more in-depth investigation on internal structure of Chinese words and employs 11 labels to distinguish different syntactic roles in word formation, as shown in Figure 1-(c). Leveraging character information for better word representation. It has already become a standard way in many NLP tasks to obtain charaware word representation by applying LSTM or CNN to the character sequence of a word, and concatenate it with word embedding as input, such as named entity recognition (Chiu and Nichols, 2016), dependency parsing (Zhang et al., 2020), and constituent parsing (Gaddy et al., 2018). Another research direction is to leverage character information to obtain better word embeddings. Chen et al. (2015) extended the CBOW model and proposed to jointly learn character and word embeddings. Based on Chen et al. (2015), Yu et al. (2017) proposed to jointly learn embeddings of words, characters, and sub-characters.2 However, both studies assume that characters contribute equally to the meaning of a word and directly average embeddings of all characters. To address this, Xu et al. (2016) extended"
2021.acl-long.452,N19-1423,0,0.0943148,"LabelGCN, and show that using the resulting word representation leads to promising gains on the dependency parsing task. We release WIST at https://github.com/ SUDA-LA/ACL2021-wist, and also provide a demo to parse the internal structure of any input word. 2 Related Work Annotating word-internal structure. In the deep learning (DL) era, pretraining techniques are extremely powerful in handling large-scale unlabeled data, including Skip-Gram or CBOW models (Mikolov et al., 2013) for learning contextindependent word embedding in the beginning, and the recent ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019) for learning context-aware word representations. Conversely, in the pre-DL era, there exist few (if any) effective methods for utilizing unlabeled data, and statistical models rely on discrete one-hot features, leading to severe data sparseness for many NLP tasks. This directly motivates annotation of word-internal structure, especially for dealing with rare words. Annotation of shallow internal structure of Chinese words was first mentioned in Zhao (2009), largely based on heuristic rules. Li (2011); Li and Zhou (2012) found that many multi-char words could be divided into two subwords, i.e."
2021.acl-long.452,N18-1091,0,0.0439491,"Missing"
2021.acl-long.452,D17-1072,1,0.796539,"∗ Chen Gong and Saihao Huang make equal contributions to this work. Zhenghua is the corresponding author. words are the minimal units that express a complete semantic concept or play a grammatical role independently (Xia, 2009; Yu et al., 2003).1 Roles played by characters in word formation can be divided into three types. (1) There is a stable and important set of single-char words, such as “你” (you)”, “的” (of), and most punctuation marks. (2) A character having no specific meaning acts as a part of a single-morpheme word, such as “仿 1 There is still a dispute on the word granularity issue (Gong et al., 2017; Lai et al., 2021). Words are defined as a character sequence that is in tight and steady combination. However, the combination intensity is usually yet vaguely qualified according to co-occurrence frequency. We believe this work may also be potentially useful to this direction. 5823 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5823–5833 August 1–6, 2021. ©2021 Association for Computational Linguistics 佛” (like) and “法(fˇa)老(lˇao)” (Pharaoh, transliteration of foreign w"
2021.acl-long.452,W19-7726,0,0.0686994,"each character, and then determine an unlabeled dependency tree, and finally use a POS tag triple as arc label, corresponding to the POS tags of the modifier/head characters and the whole word. However, we argue POS tag triples are only loosely related with word-formation patterns, not to mention the severe difficulty of annotating char-level POS tags in each word. Recently, Lin et al. (2020) extended Zhang et al. (2014) by using an extra label for marking singlemorpheme words, and annotated hierarchical internal structure of 53K words from a Chinese-English machine translation (MT) dataset. Li et al. (2019a) annotated the internal structure of words with 4 dependency relations. In summary, we can see that most previous studies adopted quite shallow hierarchical structure. In contrast, this work presents a more in-depth investigation on internal structure of Chinese words and employs 11 labels to distinguish different syntactic roles in word formation, as shown in Figure 1-(c). Leveraging character information for better word representation. It has already become a standard way in many NLP tasks to obtain charaware word representation by applying LSTM or CNN to the character sequence of a word,"
2021.acl-long.452,P19-1229,1,0.923771,"each character, and then determine an unlabeled dependency tree, and finally use a POS tag triple as arc label, corresponding to the POS tags of the modifier/head characters and the whole word. However, we argue POS tag triples are only loosely related with word-formation patterns, not to mention the severe difficulty of annotating char-level POS tags in each word. Recently, Lin et al. (2020) extended Zhang et al. (2014) by using an extra label for marking singlemorpheme words, and annotated hierarchical internal structure of 53K words from a Chinese-English machine translation (MT) dataset. Li et al. (2019a) annotated the internal structure of words with 4 dependency relations. In summary, we can see that most previous studies adopted quite shallow hierarchical structure. In contrast, this work presents a more in-depth investigation on internal structure of Chinese words and employs 11 labels to distinguish different syntactic roles in word formation, as shown in Figure 1-(c). Leveraging character information for better word representation. It has already become a standard way in many NLP tasks to obtain charaware word representation by applying LSTM or CNN to the character sequence of a word,"
2021.acl-long.452,P11-1141,0,0.0444259,"Missing"
2021.acl-long.452,D12-1132,0,0.0689367,"Missing"
2021.acl-long.452,I17-1007,0,0.0558565,"Missing"
2021.acl-long.452,P18-1130,0,0.0486473,"Missing"
2021.acl-long.452,D17-1159,0,0.0305739,"racter of wi . The final word representation from CharLSTM(wi ) is obtained by concatenating two last-timestamp hidden output vectors of a one-layer BiLSTM. LabelCharLSTM Method. Considering that the word is usually very short and a bare label itself provides rich syntax information, we propose a straightforward extension to CharLSTM, named as LabelCharLSTM, via minor modification. zk = emb(ci,k ) ⊕ emb(li,k ) (3) where li,k represents the label between ci,k and its head in the word-internal structure. LabelGCN method. Previous work show that GCN is very effective in encoding syntactic trees (Marcheggiani and Titov, 2017; Zhang et al., 2018). We follow the implementation of Zhang et al. (2018) and use a two-layer GCN as a more sophisticated way. In order to utilize labels, we extend vanilla GCN to have the same input with LabelCharLSTM, i.e., zk . We obtain the final word representation by performing average pooling over the output vectors of the top-layer GCN. 6.2 Experiments The basic parser applys a BiLSTM over character sequence to obtain word representation. In this part, we propose two simple alternative methods to encode internal structure shown in Figure 1-(c). Settings. Following Chen and Manning (20"
2021.acl-long.452,N18-1202,0,0.0749173,"Missing"
2021.acl-long.452,N19-1277,0,0.0443256,"Missing"
2021.acl-long.452,N16-1119,0,0.173699,"ze of characters is much smaller than that of words. In fact, many NLP researchers have tried to utilize charlevel word-internal structures for better Chinese understanding. Most related to ours, previous studies on syntactic parsing have proposed to annotate word-internal structures to alleviate the data sparseness problem (Zhang et al., 2014; Li et al., 2018). However, their annotations mainly consider flat and shallow word-internal structure, as shown in Figure 1-(a) and (b). Meanwhile, researchers try to make use of character information to learn better word embeddings (Chen et al., 2015; Xu et al., 2016). Without explicitly capturing word-internal structures, these studies have to treat a word as a bag of characters. See Section 2 for more discussion. This paper presents an in-depth study on charlevel internal structure of Chinese words. We endeavour to address three questions. (1) What are the word-formation patterns for Chinese words? (2) Can we train a model to predict deep word-internal structures? (3) Is modeling word-internal structures beneficial for word representation learning? For the first question, we propose to use labeled dependency trees to represent word-internal structures, a"
2021.acl-long.452,D17-1027,0,0.023379,"d representation. It has already become a standard way in many NLP tasks to obtain charaware word representation by applying LSTM or CNN to the character sequence of a word, and concatenate it with word embedding as input, such as named entity recognition (Chiu and Nichols, 2016), dependency parsing (Zhang et al., 2020), and constituent parsing (Gaddy et al., 2018). Another research direction is to leverage character information to obtain better word embeddings. Chen et al. (2015) extended the CBOW model and proposed to jointly learn character and word embeddings. Based on Chen et al. (2015), Yu et al. (2017) proposed to jointly learn embeddings of words, characters, and sub-characters.2 However, both studies assume that characters contribute equally to the meaning of a word and directly average embeddings of all characters. To address this, Xu et al. (2016) extended Chen et al. (2015) and proposed a cross-lingual approach to distinguish contribution of characters for a word. The idea is to translate Chinese words and characters into English words, and use similarities between corresponding English word embeddings for contribution measurement. Instead of treating a word as a bag of characters, we"
2021.acl-long.452,P13-1013,0,0.017553,"ranslate Chinese words and characters into English words, and use similarities between corresponding English word embeddings for contribution measurement. Instead of treating a word as a bag of characters, we experiment with two simple ways to obtain structure-aware word representations. Meanwhile, enhancing their approach with explicit wordinternal structure could be also very interesting. Utilizing word-internal structure. Wordinternal structure have been explored in various NLP tasks. Several works propose to learn wordinternal structure, word segmentation, POS tagging and parsing jointly (Zhang et al., 2013, 2014; Li et al., 2018), demonstrating the effectiveness of word-internal structure in helping downstream tasks. Cheng et al. (2015) attempt to convert words into fine-grained subwords according to the 2 Following this direction, studies tried to explore more character information for better Chinese word representation, such as strokes (Cao et al., 2018) and ideographic shape (Sun et al., 2019). 5825 internal structure of words for better dealing with unknown words during word segmentation. Lin et al. (2020) propose to integrate the representation of word-internal structure into the input of"
2021.acl-long.452,P14-1125,0,0.138989,"ed by two annotators and inconsistencies are handled by a third senior annotator. Second, we present detailed and interesting analysis on WIST to reveal insights on Chinese word formation. Third, we propose word-internal structure parsing as a new task, and conduct benchmark experiments using a competitive dependency parser. Finally, we present two simple ways to encode word-internal structures, leading to promising gains on the sentence-level syntactic parsing task. 1 coordinate right left left 想 方 设 think plan design coordinate coordinate 法 婚 姻 method marriage marriage 老 法 法 law Pharaoh (a) Zhang et al. (2014): labels mark head positions. root root root v-v-v v-v-n v-v-n 想 方 设 think plan design 法 n-n-n 婚 n-n-n n-n-n 姻 method marriage marriage 老 法 法 law Pharaoh (b) Li et al. (2018): labels correspond to POS tag triples. root coo obj att coo obj 想 方 设 think plan design 法 婚 姻 method marriage marriage root root frag 法 法 law 老 Pharaoh (c) Ours: fine-grained structure with 11 labels. Figure 1: Three example words with internal structure under different annotation paradigms. “想(think of) 方(plan) 设(design) 法(method)” is a verb and means “find ways or means to do”. “婚(marriage) 姻(marriage) 法(law)” is a noun"
2021.acl-long.452,2020.acl-main.302,1,0.927215,"ted quite shallow hierarchical structure. In contrast, this work presents a more in-depth investigation on internal structure of Chinese words and employs 11 labels to distinguish different syntactic roles in word formation, as shown in Figure 1-(c). Leveraging character information for better word representation. It has already become a standard way in many NLP tasks to obtain charaware word representation by applying LSTM or CNN to the character sequence of a word, and concatenate it with word embedding as input, such as named entity recognition (Chiu and Nichols, 2016), dependency parsing (Zhang et al., 2020), and constituent parsing (Gaddy et al., 2018). Another research direction is to leverage character information to obtain better word embeddings. Chen et al. (2015) extended the CBOW model and proposed to jointly learn character and word embeddings. Based on Chen et al. (2015), Yu et al. (2017) proposed to jointly learn embeddings of words, characters, and sub-characters.2 However, both studies assume that characters contribute equally to the meaning of a word and directly average embeddings of all characters. To address this, Xu et al. (2016) extended Chen et al. (2015) and proposed a cross-l"
2021.acl-long.452,D18-1244,0,0.0255135,"representation from CharLSTM(wi ) is obtained by concatenating two last-timestamp hidden output vectors of a one-layer BiLSTM. LabelCharLSTM Method. Considering that the word is usually very short and a bare label itself provides rich syntax information, we propose a straightforward extension to CharLSTM, named as LabelCharLSTM, via minor modification. zk = emb(ci,k ) ⊕ emb(li,k ) (3) where li,k represents the label between ci,k and its head in the word-internal structure. LabelGCN method. Previous work show that GCN is very effective in encoding syntactic trees (Marcheggiani and Titov, 2017; Zhang et al., 2018). We follow the implementation of Zhang et al. (2018) and use a two-layer GCN as a more sophisticated way. In order to utilize labels, we extend vanilla GCN to have the same input with LabelCharLSTM, i.e., zk . We obtain the final word representation by performing average pooling over the output vectors of the top-layer GCN. 6.2 Experiments The basic parser applys a BiLSTM over character sequence to obtain word representation. In this part, we propose two simple alternative methods to encode internal structure shown in Figure 1-(c). Settings. Following Chen and Manning (2014), we conduct exper"
2021.acl-long.452,E09-1100,0,0.0874791,"Missing"
2021.acl-long.468,2020.acl-main.688,0,0.112962,"and parameters are learned from the downstream corpus. Along this line, Sato et al. (2020) exploit external monolingual data to construct a new embedding layer and achieve improvements in domain adaptation. This series of studies empirically confirm the necessity of the suitable vocabulary for the finetuning stage. However, these methods have to learn the task-specific embeddings separately before each adaptation, which brings in additional computational cost thus limiting their applicability. Besides, they completely discard the pre-trained embeddings, which have been proved to be useful by Aji et al. (2020). Extra encoder or embedding layer may fail to be well optimized with insufficient downstream resources. Accordingly, Rothe et al. (2020) employ a task-specific vocabulary to retrain M-BERT, which is then used to initialize neural machine translation (NMT) model. Considering more robust approaches, Kudo (2018) and Provilkov et al. (2020) randomly sample segmentations for each sentence at the training time. Unlike the above methods, our goal is to build a plug-andplay component, that involves neither retraining the pre-trained model nor learning task-specific embeddings separately. G(motocycle)"
2021.acl-long.468,W05-0909,0,0.108894,"ntence pieces for downstream tasks, respectively. Machine Translation Considering machine translation, we examine our method on the widely used English-to-German (En⇒De) benchmarks: WMT14. We follow Rothe et al. (2020) and Liu et al. (2020c) to deal this task. Question Generation We use the SQuAD v1.1 (Rajpurkar et al., 2016) dataset for question generation. We follow the common setting to preprocess dataset and train our models (Liu et al., 2020a). The answer and the passage are taken as the model input, while the question is the target output. ROUGE-L (Lin and Hovy, 2003), BLEU, and METEOR (Banerjee and Lavie, 2005) are treated as the assessment metrics. Results As illustrated in Table 3, the randomly initialized NMT model yields comparable results 6007 ¶ || Single NVIDIA v100 GPU with batch size being 32. https://dumps.wikimedia.org 119 0.2 109 0.18 99 0.16 Inference Speed Inference ECE 89 3.3 3.5 3.7 3.9 Averaged Token Length 30 29.6 BLEU 0.22 Inference ECE Inference Speed (sentence/s) 129 29.2 28.8 w/ M-BERT 28.4 +Ours 28 0.14 0 4.1 150 200 Figure 4: Effects of the training steps of embedding generators on BLEU scores of downstream models. Segmented Token with the reported system with the same archite"
2021.acl-long.468,D18-1461,0,0.0208229,"heir pre-trained counterparts. In order to deal with the open-vocabulary problem, it is de-facto standard for pre-trained models to employ heuristic subword segmentation methods (Sennrich et al., 2016; Kudo 6001 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6001–6011 August 1–6, 2021. ©2021 Association for Computational Linguistics and Richardson, 2018). However, the segmentation learns on the upstream corpus other than the finetuned data and is likely to be sub-optimal (Cherry et al., 2018; Provilkov et al., 2020). We argue that these lead to subword discrepancy and bring two defects. Firstly, the pre-trained model usually learns a fine-grained subword segmentation to maintain the coverage of a large amount of diverse vocabulary. Consequently, downstream NLG models may suffer from more serious exposure bias (Bengio et al., 2015) and expensive computational cost caused by the increased sequence lengths. As one example, M-BERT exploits 100 thousand fine-grained subwords to encode hundreds of languages, while most of downstream NLG tasks, in fact, require only one language and its"
2021.acl-long.468,N19-1213,0,0.0166625,"required tokens. • Extensive experiments show that our strategy is able to efficiently decrease the vocabulary gaps in pretrain-finetune paradigm and significantly boost the performance of NLG models. 2 Related Work Recent studies observe that pre-trained models suffer a bottleneck when they are applied to NLG tasks (Edunov et al., 2019; Zhu et al., 2020; Rothe et al., 2020). This problem has been attributed to many reasons. For example, Yang et al. (2019b) point out pretrain-finetune discrepancy caused by the absent masked frames in real data when adopting pretrained masked language models. Chronopoulou et al. (2019) investigate catastrophic forgetting in finetuning stage. It can be said that how to successfully employ pretrain-finetune to enhance NLG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks. Task-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding 6002 Embedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers u"
2021.acl-long.468,N19-1423,0,0.19433,"from different data distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large-scale corpus, which is then finetuned to various downstream tasks via a few adjustments. Due to its simplicity yet impressive performance, pretrain-finetune paradigm becomes the undoubtedly dominant solution"
2021.acl-long.468,N19-1409,0,0.25271,"distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large-scale corpus, which is then finetuned to various downstream tasks via a few adjustments. Due to its simplicity yet impressive performance, pretrain-finetune paradigm becomes the undoubtedly dominant solution for building state-of"
2021.acl-long.468,2020.findings-emnlp.434,0,0.0317193,"LG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks. Task-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding 6002 Embedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020). For example, Zhao et al. (2018) train an embedding generator through reconstructing the original representation of each word from its bag of subwords. Sasaki et al. (2019) progressively improve the generator using attention mechanism. Fukuda et al. (2020) further leverage similar words to enhance this procedure. Our work significantly differs from the above studies in two aspects. Due to the vocabulary is fixed once predefined, the embedding reconstruction can be merely drawn on a few of selected words. By contrast, our generator is able to produce embeddings of any tokens, since these embed"
2021.acl-long.468,2021.acl-long.394,1,0.786399,"Missing"
2021.acl-long.468,W18-2705,0,0.0191607,"us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models. 1 1 Table 1: Segmentation of English sequence “Cenozoic palaeohydrodynamic” learned from different data distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large"
2021.acl-long.468,W18-6325,0,0.0215876,"frequent in downstream task may be segmented end up poorly understood (Provilkov et al., 2020). Considering the English sequence “Cenozoic palaeohydrodynamic” shown in Table 1, all the words are frequent in a thesis domain translation task and can be well preserved in its vocabulary. Nevertheless, they are segmented into under-represented tokens by pre-trained models, preventing the finetuning stage from better learning their compositionality for generation. An alternative solution is reconstructing the pre-trained model by exploiting either a taskspecific vocabulary (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018) or a subword regularization approach (Provilkov et al., 2020). However, retraining the upstream model from scratch for each task is time-consuming and unavailable for largescale models like M-BERT, GPT, etc. To this end, we propose a simple yet generalized pretrain-finetune strategy, where an embedding transfer stage is inserted between pre-training and finetuning to eliminate their token granularity gaps. Unlike the prior strategy using a fixed vocabulary, our vocabulary is changeable and its items including mismatched ones can be easily initialized by the pre-trained embeddings. Concretely,"
2021.acl-long.468,P18-1007,0,0.128231,"However, these methods have to learn the task-specific embeddings separately before each adaptation, which brings in additional computational cost thus limiting their applicability. Besides, they completely discard the pre-trained embeddings, which have been proved to be useful by Aji et al. (2020). Extra encoder or embedding layer may fail to be well optimized with insufficient downstream resources. Accordingly, Rothe et al. (2020) employ a task-specific vocabulary to retrain M-BERT, which is then used to initialize neural machine translation (NMT) model. Considering more robust approaches, Kudo (2018) and Provilkov et al. (2020) randomly sample segmentations for each sentence at the training time. Unlike the above methods, our goal is to build a plug-andplay component, that involves neither retraining the pre-trained model nor learning task-specific embeddings separately. G(motocycle) waiter, ##er, writer, Vocabulary G(motocycle) waiter, ##er, writer, … worker, motocycle waiter, ##er, writer, worker, motocycle … worker, motocycle … Initialize Train Figure 1: Illustration of our pretrain-finetune pipeline. We pretrain an embedding generator for the initialization of embeddings of unseen tok"
2021.acl-long.468,D18-2012,0,0.0347329,"Missing"
2021.acl-long.468,2020.acl-main.703,0,0.180876,"o many reasons. For example, Yang et al. (2019b) point out pretrain-finetune discrepancy caused by the absent masked frames in real data when adopting pretrained masked language models. Chronopoulou et al. (2019) investigate catastrophic forgetting in finetuning stage. It can be said that how to successfully employ pretrain-finetune to enhance NLG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks. Task-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding 6002 Embedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020). For example, Zhao et al. (2018) train an embedding generator through reconstructing the original representation of each word from its bag of subwords. Sasaki et al. (2019) progressively improve the generator using attention mechanism. Fukuda et al. ("
2021.acl-long.468,N03-1020,0,0.281089,"Missing"
2021.acl-long.468,2020.tacl-1.47,0,0.250501,"cient and better performed downstream NLG models. 1 1 Table 1: Segmentation of English sequence “Cenozoic palaeohydrodynamic” learned from different data distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large-scale corpus, which is then finetuned to various downstream tasks via a"
2021.acl-long.468,2020.acl-main.170,0,0.483897,"terparts. In order to deal with the open-vocabulary problem, it is de-facto standard for pre-trained models to employ heuristic subword segmentation methods (Sennrich et al., 2016; Kudo 6001 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6001–6011 August 1–6, 2021. ©2021 Association for Computational Linguistics and Richardson, 2018). However, the segmentation learns on the upstream corpus other than the finetuned data and is likely to be sub-optimal (Cherry et al., 2018; Provilkov et al., 2020). We argue that these lead to subword discrepancy and bring two defects. Firstly, the pre-trained model usually learns a fine-grained subword segmentation to maintain the coverage of a large amount of diverse vocabulary. Consequently, downstream NLG models may suffer from more serious exposure bias (Bengio et al., 2015) and expensive computational cost caused by the increased sequence lengths. As one example, M-BERT exploits 100 thousand fine-grained subwords to encode hundreds of languages, while most of downstream NLG tasks, in fact, require only one language and its associate tokens. Second"
2021.acl-long.468,D16-1264,0,0.0102312,"and decoder. Same as configurations in domain adaptation, we merely perform the embedding transferring in decoder. Since the two language models exploit different segmentation tools, i.e., WordPiece (Wu et al., 2016) and SentencePiece (Kudo, 2018), we set 32K and 10K as the number of word and sentence pieces for downstream tasks, respectively. Machine Translation Considering machine translation, we examine our method on the widely used English-to-German (En⇒De) benchmarks: WMT14. We follow Rothe et al. (2020) and Liu et al. (2020c) to deal this task. Question Generation We use the SQuAD v1.1 (Rajpurkar et al., 2016) dataset for question generation. We follow the common setting to preprocess dataset and train our models (Liu et al., 2020a). The answer and the passage are taken as the model input, while the question is the target output. ROUGE-L (Lin and Hovy, 2003), BLEU, and METEOR (Banerjee and Lavie, 2005) are treated as the assessment metrics. Results As illustrated in Table 3, the randomly initialized NMT model yields comparable results 6007 ¶ || Single NVIDIA v100 GPU with batch size being 32. https://dumps.wikimedia.org 119 0.2 109 0.18 99 0.16 Inference Speed Inference ECE 89 3.3 3.5 3.7 3.9 Avera"
2021.acl-long.468,2020.tacl-1.18,0,0.425062,"ream tasks via a few adjustments. Due to its simplicity yet impressive performance, pretrain-finetune paradigm becomes the undoubtedly dominant solution for building state-of-the-art models in many natural language understanding tasks (Xu et al., 2019; Yang et al., 2019a; Liu et al., 2020b). In comparison, this strategy often achieves disappointing or barely satisfactory performance in natural language generation (NLG) tasks. For example, several studies observe that M-BERT (Devlin et al., 2019) fails to enhance the decoder of a translation model (Edunov et al., 2019; Zhu et al., 2020), while Rothe et al. (2020) reach the same conclusion even when adapting an autoregressive model GPT (Radford et al., 2019). A natural problem arises: What is the crucial bottleneck in current pretrain-finetune framework and how to break it? In this paper, we provide the first answer from the subword discrepancy aspect, namely, the subword vocabulary extracted according to the pretraining data distribution is insufficient to cope with the downstream NLG tasks. Such inflexibility stems from the fact that downstream NLG models have to inherit the vocabulary from their pre-trained counterparts. In order to deal with the op"
2021.acl-long.468,N19-1353,0,0.0819414,"Extensive experiments show that our strategy is able to efficiently decrease the vocabulary gaps in pretrain-finetune paradigm and significantly boost the performance of NLG models. 2 Related Work Recent studies observe that pre-trained models suffer a bottleneck when they are applied to NLG tasks (Edunov et al., 2019; Zhu et al., 2020; Rothe et al., 2020). This problem has been attributed to many reasons. For example, Yang et al. (2019b) point out pretrain-finetune discrepancy caused by the absent masked frames in real data when adopting pretrained masked language models. Chronopoulou et al. (2019) investigate catastrophic forgetting in finetuning stage. It can be said that how to successfully employ pretrain-finetune to enhance NLG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks. Task-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding 6002 Embedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers u"
2021.acl-long.468,2020.findings-emnlp.381,0,0.214001,"nd extensive analyses show that the proposed strategy offers us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models. 1 1 Table 1: Segmentation of English sequence “Cenozoic palaeohydrodynamic” learned from different data distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu"
2021.acl-long.468,P16-1162,0,0.0502572,"es: What is the crucial bottleneck in current pretrain-finetune framework and how to break it? In this paper, we provide the first answer from the subword discrepancy aspect, namely, the subword vocabulary extracted according to the pretraining data distribution is insufficient to cope with the downstream NLG tasks. Such inflexibility stems from the fact that downstream NLG models have to inherit the vocabulary from their pre-trained counterparts. In order to deal with the open-vocabulary problem, it is de-facto standard for pre-trained models to employ heuristic subword segmentation methods (Sennrich et al., 2016; Kudo 6001 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6001–6011 August 1–6, 2021. ©2021 Association for Computational Linguistics and Richardson, 2018). However, the segmentation learns on the upstream corpus other than the finetuned data and is likely to be sub-optimal (Cherry et al., 2018; Provilkov et al., 2020). We argue that these lead to subword discrepancy and bring two defects. Firstly, the pre-trained model usually learns a fine-grained subword segmentation t"
2021.acl-long.468,tian-etal-2014-um,0,0.0197633,"LG tasks: machine translation (MT) and answer-aware question generation (QG). 4.1 Domain Adaptation We conduct experiments on English-to-Chinese (En⇒Zh) domain adaptation translation tasks, where the pretrain-finetune paradigm resort as standard. The pre-training corpus is extracted from an 6005 out-of-domain dataset LDC† , in which 1.25M (M = million), 3K (K = thousand), 3K sentences pairs are randomly sampled as training, development and test set, respectively. We verify the effectiveness of our strategy on two downstream domains: Thesis and Laws, of which data are collected from UM-Corpus (Tian et al., 2014). We follow the same settings as Zeng et al. (2018) and Su et al. (2021) to preprocess two corpus and train models. The translation quality is evaluated by cased BLEU (Papineni et al., 2002), which is caculated by mteval-v13a.pl. Implementation Details All the compared methods are re-implemented on top of FairSeq‡ and built on Transformer (Vaswani et al., 2017). We apply Adam Optimizer (Kingma and Ba, 2015) with β1 and β2 being 0.9 and 0.999, respectively. The dropout ratio is set to 0.3 and each iteration batch consists of 25K tokens. For both pre-training and finetuning, we employ warm-up st"
2021.acl-long.468,I17-2050,0,0.153585,"are in upstream task but frequent in downstream task may be segmented end up poorly understood (Provilkov et al., 2020). Considering the English sequence “Cenozoic palaeohydrodynamic” shown in Table 1, all the words are frequent in a thesis domain translation task and can be well preserved in its vocabulary. Nevertheless, they are segmented into under-represented tokens by pre-trained models, preventing the finetuning stage from better learning their compositionality for generation. An alternative solution is reconstructing the pre-trained model by exploiting either a taskspecific vocabulary (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018) or a subword regularization approach (Provilkov et al., 2020). However, retraining the upstream model from scratch for each task is time-consuming and unavailable for largescale models like M-BERT, GPT, etc. To this end, we propose a simple yet generalized pretrain-finetune strategy, where an embedding transfer stage is inserted between pre-training and finetuning to eliminate their token granularity gaps. Unlike the prior strategy using a fixed vocabulary, our vocabulary is changeable and its items including mismatched ones can be easily initialized by the pre-trained"
2021.acl-long.468,2020.emnlp-main.80,1,0.724909,"free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models. 1 1 Table 1: Segmentation of English sequence “Cenozoic palaeohydrodynamic” learned from different data distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large-scale corpus, whic"
2021.acl-long.468,P02-1040,0,0.109443,"ks, where the pretrain-finetune paradigm resort as standard. The pre-training corpus is extracted from an 6005 out-of-domain dataset LDC† , in which 1.25M (M = million), 3K (K = thousand), 3K sentences pairs are randomly sampled as training, development and test set, respectively. We verify the effectiveness of our strategy on two downstream domains: Thesis and Laws, of which data are collected from UM-Corpus (Tian et al., 2014). We follow the same settings as Zeng et al. (2018) and Su et al. (2021) to preprocess two corpus and train models. The translation quality is evaluated by cased BLEU (Papineni et al., 2002), which is caculated by mteval-v13a.pl. Implementation Details All the compared methods are re-implemented on top of FairSeq‡ and built on Transformer (Vaswani et al., 2017). We apply Adam Optimizer (Kingma and Ba, 2015) with β1 and β2 being 0.9 and 0.999, respectively. The dropout ratio is set to 0.3 and each iteration batch consists of 25K tokens. For both pre-training and finetuning, we employ warm-up strategy where the linear warm-up phase takes 4K steps, reaching its maximum learning rate to 5 × 10−4 . The training of each model is early-stopped to maximize BLEU score on the development s"
2021.acl-long.468,2020.acl-main.278,0,0.0120737,"stigate three problems: Q1: How subword granularity affects NLG models? (§ 5.1) Q2: How embedding transfer benefits to downstream models? (§ 5.2) Q3: Dose our strategy acquire large computational costs? (§ 5.3) Q4: Can our strategy exactly handle under-represented tokens? (§ 5.4) 5.1 100 Training Steps Figure 3: Effects of different token granularities on En⇒De task. As seen, the segmentation granularity remarkably affects inference speed and inference ECE. 5 50 Impact of Subword Granularity Figure 3 visualizes the inference speed and exposure bias (Inference Expected Calibration Error (ECE), Wang et al., 2020) of translation models with different token granularities in their vocabulary. Obviously, for a translation model, neither too small nor too large granularity regarding to subwords can reach a satisfactory performance on inference speed. At the same time, the granularity indeed affects the problem of exposure bias in translation task. The experiments confirm the suitable segmentation strategy can effectively alleviate the problem of exposure bias. Source Reference Translations M-BERT: s dan k bar Ours: dankbar it’s very gratifying to have this kind of reception here. ich bin sehr dankbar f¨ur"
2021.acl-long.468,D17-1010,0,0.022109,"hat how to successfully employ pretrain-finetune to enhance NLG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks. Task-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding 6002 Embedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020). For example, Zhao et al. (2018) train an embedding generator through reconstructing the original representation of each word from its bag of subwords. Sasaki et al. (2019) progressively improve the generator using attention mechanism. Fukuda et al. (2020) further leverage similar words to enhance this procedure. Our work significantly differs from the above studies in two aspects. Due to the vocabulary is fixed once predefined, the embedding reconstruction can be merely drawn on a few of selected words. By contrast, our generator"
2021.acl-long.468,N19-1242,0,0.0163376,"zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large-scale corpus, which is then finetuned to various downstream tasks via a few adjustments. Due to its simplicity yet impressive performance, pretrain-finetune paradigm becomes the undoubtedly dominant solution for building state-of-the-art models in many natural language understanding tasks (Xu et al., 2019; Yang et al., 2019a; Liu et al., 2020b). In comparison, this strategy often achieves disappointing or barely satisfactory performance in natural language generation (NLG) tasks. For example, several studies observe that M-BERT (Devlin et al., 2019) fails to enhance the decoder of a translation model (Edunov et al., 2019; Zhu et al., 2020), while Rothe et al. (2020) reach the same conclusion even when adapting an autoregressive model GPT (Radford et al., 2019). A natural problem arises: What is the crucial bottleneck in current pretrain-finetune framework and how to break it? In this paper, we"
2021.acl-long.468,P19-1226,0,0.116453,"dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large-scale corpus, which is then finetuned to various downstream tasks via a few adjustments. Due to its simplicity yet impressive performance, pretrain-finetune paradigm becomes the undoubtedly dominant solution for building state-of-the-art models in many natural language understanding tasks (Xu et al., 2019; Yang et al., 2019a; Liu et al., 2020b). In comparison, this strategy often achieves disappointing or barely satisfactory performance in natural language generation (NLG) tasks. For example, several studies observe that M-BERT (Devlin et al., 2019) fails to enhance the decoder of a translation model (Edunov et al., 2019; Zhu et al., 2020), while Rothe et al. (2020) reach the same conclusion even when adapting an autoregressive model GPT (Radford et al., 2019). A natural problem arises: What is the crucial bottleneck in current pretrain-finetune framework and how to break it? In this paper, we provide the first"
2021.acl-long.468,2020.coling-main.399,1,0.737658,"es show that the proposed strategy offers us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models. 1 1 Table 1: Segmentation of English sequence “Cenozoic palaeohydrodynamic” learned from different data distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at D"
2021.acl-long.468,D18-1041,1,0.854743,"question generation (QG). 4.1 Domain Adaptation We conduct experiments on English-to-Chinese (En⇒Zh) domain adaptation translation tasks, where the pretrain-finetune paradigm resort as standard. The pre-training corpus is extracted from an 6005 out-of-domain dataset LDC† , in which 1.25M (M = million), 3K (K = thousand), 3K sentences pairs are randomly sampled as training, development and test set, respectively. We verify the effectiveness of our strategy on two downstream domains: Thesis and Laws, of which data are collected from UM-Corpus (Tian et al., 2014). We follow the same settings as Zeng et al. (2018) and Su et al. (2021) to preprocess two corpus and train models. The translation quality is evaluated by cased BLEU (Papineni et al., 2002), which is caculated by mteval-v13a.pl. Implementation Details All the compared methods are re-implemented on top of FairSeq‡ and built on Transformer (Vaswani et al., 2017). We apply Adam Optimizer (Kingma and Ba, 2015) with β1 and β2 being 0.9 and 0.999, respectively. The dropout ratio is set to 0.3 and each iteration batch consists of 25K tokens. For both pre-training and finetuning, we employ warm-up strategy where the linear warm-up phase takes 4K step"
2021.acl-long.468,D18-1059,0,0.0935781,"ly employ pretrain-finetune to enhance NLG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks. Task-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding 6002 Embedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020). For example, Zhao et al. (2018) train an embedding generator through reconstructing the original representation of each word from its bag of subwords. Sasaki et al. (2019) progressively improve the generator using attention mechanism. Fukuda et al. (2020) further leverage similar words to enhance this procedure. Our work significantly differs from the above studies in two aspects. Due to the vocabulary is fixed once predefined, the embedding reconstruction can be merely drawn on a few of selected words. By contrast, our generator is able to produce"
2021.acl-long.73,W13-2322,0,0.107559,"ur knowledge, this is the first study that utilizes such a pre-training approach in cross-lingual AMR research. We also explore and compare four different finetuning methods to answer the question that whether combining AMR parsing and AMR-to-text generation tasks in fine-tuning stage will achieve better performance. Moreover, inspired by the teacherstudent mechanism (Kim and Rush, 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; X"
2021.acl-long.73,2020.emnlp-main.195,0,0.507377,"on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations, and propose to conduct cross-lingual AMR parsing via annotation projection and machine translation. Blloshmi et al. (2020) follow the same line and create large-scale silver data to boost the performance of cross-lingual AMR parsing. Fan and Gardent (2020) focus on multilingual AMR-to-text generation for twenty one different languages. The aforementioned studies consider AMR parsing and AMR-to-text generation separately. In this paper, we formalize both AMR parsing and AMR-to-text generation as sequence-tosequence (seq2seq) learning and propose a novel and effective approach to cross-lingual AMR, which is illustrated in Figure 1. Upon the availability of the English AMR dataset and English-toX parallel datasets ("
2021.acl-long.73,D19-1393,0,0.0119068,"red by the teacherstudent mechanism (Kim and Rush, 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on gra"
2021.acl-long.73,2020.acl-main.119,0,0.0208192,"ouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into t"
2021.acl-long.73,2020.acl-main.640,0,0.0447769,"ouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into t"
2021.acl-long.73,P13-2131,0,0.268815,"m at every 10K (1K) steps. Finally, we obtain final pre-trained (finetuned) models by averaging the last 10 checkpoints. Evaluation. We evaluate on LDC2020T07 (Damonte and Cohen, 2018), a corpus containing human translations of the test portion of 1371 sentences from the AMR 2.0, in German, Spanish, Italian, and Chinese. This data is designed for use in cross-lingual AMR research. Following Fan and Gardent (2020), we only evaluate on languages of German, Spanish and Italian where we have training data from EUROPARL. For AMR parsing evaluation, we utilize Smatch and other fine-grained metrics (Cai and Knight, 2013; Damonte et al., 2017). For AMR-to-text generation, we report performance in BLEU (Papineni et al., 2002). 5.2 We compare the performance of our approach against two baseline systems. Baselinescratch . To build this baseline system, we directly train models from scratch on the finetuning datasets. Taking German AMR parsing as example, we trainthe model on its fine-tuning dataset F DE , F AMR to get Baselinescratch . Fine-Tuning Datasets. We use English AMR2.0 which contains 36,521, 1,368, and 1,371 EnglishAMR pairs for training, development, and testing, respectively. We translate the Englis"
2021.acl-long.73,2020.coling-main.218,0,0.0313791,"Missing"
2021.acl-long.73,N19-1223,0,0.0155329,"t al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models,"
2021.acl-long.73,P17-1176,0,0.136857,"ned while learning for English AMR parsing and text generation could be helpful to the X-language counterparts, and machine translation tasks could act as a good regularizer (Xu et al., 2020). To the best of our knowledge, this is the first study that utilizes such a pre-training approach in cross-lingual AMR research. We also explore and compare four different finetuning methods to answer the question that whether combining AMR parsing and AMR-to-text generation tasks in fine-tuning stage will achieve better performance. Moreover, inspired by the teacherstudent mechanism (Kim and Rush, 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte"
2021.acl-long.73,D16-1139,0,0.180998,"e that knowledge gained while learning for English AMR parsing and text generation could be helpful to the X-language counterparts, and machine translation tasks could act as a good regularizer (Xu et al., 2020). To the best of our knowledge, this is the first study that utilizes such a pre-training approach in cross-lingual AMR research. We also explore and compare four different finetuning methods to answer the question that whether combining AMR parsing and AMR-to-text generation tasks in fine-tuning stage will achieve better performance. Moreover, inspired by the teacherstudent mechanism (Kim and Rush, 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou e"
2021.acl-long.73,N18-1104,0,0.3748,"nverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations, and propose to conduct cross-lingual AMR parsing via annotation projection and machine translation. Blloshmi et al. (2020) follow the same line and create large-scale silver data to boost the performance of cross-lingual AMR parsing. Fan and Gardent (2020) focus on multilingual AMR-to-text generation for twenty one different languages. The aforementioned studies consider AMR parsing and AMR-to-text generation separately. In this paper, we formalize both AMR parsing and AMR-to-text generation as s"
2021.acl-long.73,N19-1366,0,0.0176982,"(Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open ques"
2021.acl-long.73,P17-4012,0,0.0108398,"entences into German, Spanish, and Italian, respectively. We segment all the tokens into subwords by using the BPE model trained on pre-training datasets. Baselinepre-trained . Rather than training models from scratch, we pre-train the models on largescale silver datasets. Taking German AMR parsing as example, we first pre-train the model on the pre DE AMR training dataset, i.e., T , T , then we finetune the pre-trained model on the corresponding  fine-tuning dataset, i.e., F DE , F AMR . Pre-Training and Fine-Tuning Model Settings. We implement above pre-trained models based on OpenNMT-py (Klein et al., 2017). 7 For simplicity, we use the same hyperparameter settings to train all the models in both pre-training and fine-tuning 5 https://www.statmt.org/wmt14/ translation-task.html 6 https://www.statmt.org/europarl/index. html 7 https://github.com/OpenNMT/OpenNMT-py Baseline Systems 5.3 Main Results Table 2 shows the performance of AMR parsing and AMR-to-text generation for German (DE), Spanish (ES), and Italian (IT). 901 AMR Parsing AMR-to-Text DE ES IT DE ES IT Baselinescratch 58.10 60.65 58.67 13.11 17.83 13.59 Baselinepre-trained 64.90 68.05 66.54 19.32 27.17 24.13 XLPT-AMRnone 48.97 59.52 58.13"
2021.acl-long.73,E17-1051,0,0.112665,", 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More"
2021.acl-long.73,2020.emnlp-main.231,0,0.432859,"chiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations, and propose to conduct cross-lingual AMR parsing via annotation projection and machine translation. Blloshmi et al. (2020) follow the same line and create large-scale silver data to boost the performance of cross-lingual AMR parsing. Fan and Gardent (2020) focus on multilingual AMR-to-text generation for twenty one different languages. The aforementioned studies consider AMR parsing and AMR-to-text generation separately. In this paper, we formalize both AMR parsing and AMR-to-text generation as sequence-tosequence (seq2seq) learning and propose a novel and effective approach to cross-lingual AMR, which is illustrated in Figure 1. Upon the availability of the English AMR dataset and English-toX parallel datasets (X ∈ {German, Spanish, Italian} in this paper), our purpose is to boost the performance of zero-shot AMR parsing and text generation in"
2021.acl-long.73,W17-3501,0,0.0171375,"al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advant"
2021.acl-long.73,P17-1014,0,0.262024,"/xdqkid/XLPT-AMR. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a widely used formalism that represents the semantics of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as ∗ Corresponding Author: Junhui Li. “doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et a"
2021.acl-long.73,P18-1037,0,0.0142329,"on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs"
2021.acl-long.73,N16-1087,0,0.14842,"represents the semantics of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as ∗ Corresponding Author: Junhui Li. “doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual seman"
2021.acl-long.73,P14-1134,0,0.0923677,"ode available on github https:// github.com/xdqkid/XLPT-AMR. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a widely used formalism that represents the semantics of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as ∗ Corresponding Author: Junhui Li. “doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annota"
2021.acl-long.73,P18-1170,0,0.0393304,"Missing"
2021.acl-long.73,P02-1040,0,0.109557,"10 checkpoints. Evaluation. We evaluate on LDC2020T07 (Damonte and Cohen, 2018), a corpus containing human translations of the test portion of 1371 sentences from the AMR 2.0, in German, Spanish, Italian, and Chinese. This data is designed for use in cross-lingual AMR research. Following Fan and Gardent (2020), we only evaluate on languages of German, Spanish and Italian where we have training data from EUROPARL. For AMR parsing evaluation, we utilize Smatch and other fine-grained metrics (Cai and Knight, 2013; Damonte et al., 2017). For AMR-to-text generation, we report performance in BLEU (Papineni et al., 2002). 5.2 We compare the performance of our approach against two baseline systems. Baselinescratch . To build this baseline system, we directly train models from scratch on the finetuning datasets. Taking German AMR parsing as example, we trainthe model on its fine-tuning dataset F DE , F AMR to get Baselinescratch . Fine-Tuning Datasets. We use English AMR2.0 which contains 36,521, 1,368, and 1,371 EnglishAMR pairs for training, development, and testing, respectively. We translate the English sentences into German, Spanish, and Italian, respectively. We segment all the tokens into subwords by us"
2021.acl-long.73,D18-1198,0,0.0128812,"arget fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generat"
2021.acl-long.73,E17-1035,0,0.0184233,"hat translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira"
2021.acl-long.73,Q19-1019,0,0.0140853,"ong et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al.,"
2021.acl-long.73,W16-6603,0,0.0201628,"q2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al.,"
2021.acl-long.73,D19-1314,0,0.0124211,"More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajiˇ c et al.,"
2021.acl-long.73,P16-1162,0,0.00918509,"MT14 English-German translation dataset 5 which consists of 3.9M sentence pairs after preprocessing. For Spanish and Italian, we use Europarl parallel datasets,6 which consist of 1.9M English-Spanish and 1.9M English-Italian sentence pairs, respectively. The English sentences of all the datasets are all parsed into AMR graphs via an English AMR parser trained on AMR 2.0 (LDC2017T10) (Appendix A provides more details on the English AMR parser). We merge English, German (Spanish/Italian) sentences and linearized AMRs together and segment all the tokens into subwords by byte pair encoding (BPE) (Sennrich et al., 2016) with 40K (or 30K for both Spanish and Italian) operations. In addition, we also train NMT models to translate English into German, Spanish, and Italian on above parallel datasets with Transformer-big settings (Vaswani et al., 2017). These NMT models will be used in preparing fine-tuning datasets (Appendix B provides more implementation details on the NMT models). by just following the settings for the Transformerbase model in Vaswani et al. (2017). The number of layers in encoder and decoder is 6 while the number of heads is 8. Both the embedding size and the hidden state size are 512 while t"
2021.acl-long.73,2021.eacl-main.30,0,0.39898,"rch. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajiˇ c et al., 2014). Till lately , Damonte and Cohen (2018) demonstrate that a simplified AMR can be used across languages and for the first time they study crosslingual AMR parsing for languages rather than English. Blloshmi et al. (2020) employ large-scale silver parallel AMR data to bridge the gap between different languages and greatly advance the performance of cross-lingual AMR parsing. Sheth et al. (2021) explore annotation projection to leverage existing English AMR and overcome resource shortage in the target language. Furthermore, Fan and Gardent (2020) explore cross-lingual AMR-to-text based on pre-trained cross-lingual language model (XLM) (Lample and Conneau, 2019). In this paper we build strong cross-lingual pre-trained models for both AMR parsing and AMR-to-text generation. Moreover, a nice property of our approach is that for AMR parsing, unlike related studies (Damonte and Cohen, 2018; Blloshmi et al., 2020), we do not need to perform lemmatization, POS tagging, NER, or re-categoriza"
2021.acl-long.73,D19-6313,0,0.017455,"“:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations, and propose to conduct cross-lingual AMR parsing via annotation projection and machine translation. Blloshmi et al. (2020) follow the same line and create large-scale silver data to boost the performance of cross-lingual AMR parsi"
2021.acl-long.73,P17-2002,0,0.0896182,"cs of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as ∗ Corresponding Author: Junhui Li. “doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations"
2021.acl-long.73,2020.acl-main.712,0,0.023251,"lation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajiˇ c et al., 2014). Till lately , Damonte and Cohen (2018) demonstrate that a simplified"
2021.acl-long.73,P18-1150,0,0.0149225,"task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether A"
2021.acl-long.73,2020.findings-emnlp.199,0,0.0227891,"sly. The results also show that our XLPT-AMRT-S models greatly advance the state of art. For example, our XLPT-AMRT-S models outperform Sheth et al. (2021) by 3.4∼7.8 Smatch F1 on AMR parsing of the three languages while surpass Fan and Gardent (2020) by around 10 BLEU on AMR-to-text generation. 902 Table 3 compares the performance of finegrained metrics for AMR parsing. It shows that our XLPT-AMRT-S models achieve the best performance on all the metrics with the only exception of Concepts for Italian AMR parsing. It shows that like English AMR parsing, all models predict Reentrancies poorly (Szubert et al., 2020). It also demonstrates that Negations is another metric which is hard to predict. In future work, we will pay particular attention to the two metrics. Metric Smatch Unlabeled No WSD Concepts Named Ent. Negations Wikification Reentrancies SRL Blloshmi et al. (2020) DE ES IT 53.0 58.0 58.1 57.7 63.0 63.4 53.2 58.4 58.4 58.0 65.9 64.7 66.0 65.9 64.7 11.7 23.4 29.2 60.9 63.1 67.0 39.9 46.6 46.1 47.9 55.2 54.7 Baselinepre-trained DE ES IT 64.90 68.05 66.54 69.53 72.49 71.16 65.16 68.40 66.78 68.79 73.06 78.21 79.12 81.34 68.42 42.69 51.93 48.57 67.40 69.40 71.05 42.40 46.20 44.10 60.50 65.20 63.80"
2021.acl-long.73,N18-2040,0,0.507917,"the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations, and propose to conduct cross-lingual AMR parsing via annotation projection and machine translation. Blloshmi et al. (2020) follow the same line and create large-scale silver data to boost the perf"
2021.acl-long.73,P15-2141,0,0.113751,"https:// github.com/xdqkid/XLPT-AMR. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a widely used formalism that represents the semantics of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as ∗ Corresponding Author: Junhui Li. “doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et"
2021.acl-long.73,N15-1040,0,0.48219,"https:// github.com/xdqkid/XLPT-AMR. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a widely used formalism that represents the semantics of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as ∗ Corresponding Author: Junhui Li. “doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et"
2021.acl-long.73,P15-1095,0,0.0555699,"Missing"
2021.acl-long.73,2020.emnlp-main.196,1,0.922309,"-01 she much medication :ARG1 :quant :ARG1 :ARG2 doctor • We evaluate our approach in three zero-shot languages of AMR and our approach greatly advances the state of the art. make-02 2 good-02 :degree Related Work We describe related studies on AMR from three perspectives: English AMR parsing, English AMRto-text generation, and cross-lingual AMR. more Figure 1: Illustration of cross-lingual AMR parsing and AMR-to-text generation: (a) sentences in different languages sharing the same meaning; (b) AMR graph of the sentences. X-language. To this end, we borrow the idea of joint pre-training from Xu et al. (2020) and explore three types of relevant tasks, including machine translation tasks, AMR parsing and AMR-to-text generation tasks. We conjecture that knowledge gained while learning for English AMR parsing and text generation could be helpful to the X-language counterparts, and machine translation tasks could act as a good regularizer (Xu et al., 2020). To the best of our knowledge, this is the first study that utilizes such a pre-training approach in cross-lingual AMR research. We also explore and compare four different finetuning methods to answer the question that whether combining AMR parsing"
2021.acl-long.73,P19-1009,0,0.0288182,"Missing"
2021.acl-long.73,D19-1392,0,0.0281734,"Missing"
2021.acl-long.73,2020.acl-main.67,0,0.024457,"as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajiˇ c et al., 2014). Till lately , Damonte and Cohen (2018) demonstrate"
2021.acl-long.73,2021.naacl-main.443,0,0.0233173,"task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine tra"
2021.acl-long.73,D16-1065,0,0.360797,", 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; So"
2021.acl-long.73,D19-1548,1,0.884741,"ropose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajiˇ c et al., 2014). Till lately"
2021.blackboxnlp-1.14,J93-2003,0,0.17545,"rent language pairs. Then we evaluate various length prediction strategies and dig into potential influential factors. Finally the correlation between accuracy of length prediction and translation quality is investigated. Introduction 2 Related Work Though Transformer (Vaswani et al., 2017) has Gu et al. (2018) predict target length with fertility promoted conventional autoregressive generation which essentially is an alignment of source and tar(AR) by leveraging multi-head self-attention to get tokens generated by a tool named IBM Model avoid recurrence at training, decoders that generate 2 (Brown et al., 1993). The hidden state of each each token conditioned on previously generated tokens still make it impossible to take full advan- source token is used to predict the number of the aligned target token. Then each source token is tage of parallelism during inference: p(Y |X) = Q T copied one or multiple times as decoder input, dep(y |y |X). Non-autoregressive Machine i ≤i i Translation (NAT) (Gu et al., 2018; Kaiser et al., pending on the corresponding predicted length during inference. However, it may introduce noise 2018) was proposed to parallelize the generation by allowing the prediction of eac"
2021.blackboxnlp-1.14,D19-1633,0,0.130938,"ngchao,zhangmin186,taoshimin,yanghao30}@huawei.com yuxiaw@student.unimelb.edu.au Abstract Length prediction is a special task in a series of NAT models where target length has to be determined before generation. However, the performance of length prediction and its influence on translation quality has seldom been discussed. In this paper, we present comprehensive analyses on length prediction task of NAT, aiming to find the factors that influence performance, as well as how it associates with translation quality. We mainly perform experiments based on Conditional Masked Language Model (CMLM) (Ghazvininejad et al., 2019), a representative NAT model, and evaluate it on two language pairs, En-De and En-Ro. We draw two conclusions: 1) The performance of length prediction is mainly influenced by properties of language pairs such as alignment pattern, word order or intrinsic length ratio, and is also affected by the usage of knowledge distilled data. 2) There is a positive correlation between the performance of the length prediction and the BLEU score. 1 ple tokens can be inserted into an existing unfinished sentence, and the sentence length dynamically changes during insertion (Stern et al., 2019; Gu et al., 2019"
2021.blackboxnlp-1.14,D18-1149,0,0.0904531,"rn, word order or intrinsic length ratio, and is also affected by the usage of knowledge distilled data. 2) There is a positive correlation between the performance of the length prediction and the BLEU score. 1 ple tokens can be inserted into an existing unfinished sentence, and the sentence length dynamically changes during insertion (Stern et al., 2019; Gu et al., 2019; Chan et al., 2019). Another predetermines the target side length N and generates tokens in a fixed length space conditioned on wellpredicted tokens and optionally copied source embedding within N iterations (Gu et al., 2018; Lee et al., 2018; Ghazvininejad et al., 2019). In this paper, we mainly focus on the critical component in the second direction — length prediction which has been rarely discussed. Specifically, we investigate RQ1: what influences accuracy of length prediction, and RQ2: how accuracy of target sentence length prediction affects the translation quality. To answer these questions, we first analyze intrinsic properties of sentence lengths for different language pairs. Then we evaluate various length prediction strategies and dig into potential influential factors. Finally the correlation between accuracy of lengt"
2021.blackboxnlp-1.14,D19-1573,0,0.024467,"Missing"
2021.blackboxnlp-1.14,N19-4009,0,0.0491737,"Missing"
2021.blackboxnlp-1.14,P02-1040,0,0.109865,"Missing"
2021.blackboxnlp-1.14,P16-1162,0,0.00655903,"ed corpora for convenient comparison, including WMT14 En↔De (train=4.5M / valid=3k / test=3k) and WMT16 En↔Ro (train=2.8M/ valid=2k/ test=2k). Following Gu et al. (2018); Zhou et al. (2020), we employ knowledge distilled (KD) data to train our NAT model. The KD data is generated with a pre-trained autoregressive Transformer-big teacher model by completely translating the source text into the target language. We set beam size as five in distillation and only keep the first candidate to make KD data equally sized with raw data. Corpus for all language pairs are tokenized into subwords with BPE (Sennrich et al., 2016), the vocabulary sizes are 42k and 40k for En↔De and En↔Ro respectively. CMLM is a representative NAT model due to both the simple implementation and the impressive performance. It’s built up on the standard Transformer architecture without the decoder selfattention mask. Formally, given source and target sentence X = (x1 , ..., xk ) and Y = (y1 , ...yn ), the model first predicts target length with hidden state of the source sentence: p(L|X), and initializes a sequence of [MASK] ×L accordingly as decoder input. Then it repeats an alternative between predicting and remasking tokens in sequence"
2021.blackboxnlp-1.14,2006.amta-papers.25,0,0.0448754,"Missing"
2021.ccl-1.48,Y96-1018,0,0.306289,"Missing"
2021.ccl-1.48,W16-1702,0,0.0686791,"Missing"
2021.ccl-1.48,P16-1033,1,0.717036,"Missing"
2021.ccl-1.48,J93-2004,0,0.0825203,"Missing"
2021.ccl-1.48,P13-2017,0,0.0894733,"Missing"
2021.ccl-1.48,W03-1507,0,0.320236,"Missing"
2021.ccl-1.48,xia-etal-2000-developing,0,0.717715,"Missing"
2021.ccl-1.48,P19-1426,0,0.0200915,"Missing"
2021.conll-1.23,P05-1022,0,0.363163,"inference, 3) simplifying system architecture by training a single model. In contrast, there have been few work on joint WS-POS-PAR in the DL era, with the exception of Zheng et al. (2015). Similar to this work, they treated the joint problem as a char-level parsing task, which is obviously the most straightforward way for Chinese, as illustrated in Figure 1(b). However, they showed that the joint model fails to outperform the pipeline approach. In the past few years, constituent parsing has achieved significant progress. Compared with preDL parsers based on discrete features (Collins, 1997; Charniak and Johnson, 2005; Petrov and 1 Introduction Klein, 2007), the major characteristic of graphAs shown in Figure 1(a), the results of word seg- based parsers (Stern et al., 2017; Gaddy et al., 2018; mentation (WS), part-of-speech (POS) tagging, and Kitaev and Klein, 2018; Zhang et al., 2020) is that constituent parsing (PAR) can be organized in a the score of a constituent tree is decomposed into unified hierarchical tree, where leaf nodes include scores of labeled spans (i.e., (2, 3, VP) or VP2,3 ), words and their POS tags, and non-terminal nodes without any consideration on production rules (i.e., correspond"
2021.conll-1.23,N06-1022,0,0.0673875,"nson (2005) succeed in reranking n-best to aggregate the outputs of three independently parses with a two-stage method. They predict n- trained models for the three tasks. Zhang et al. best parses with a coarse-grained grammar in the (2013) integrate word segmentation and POS tagfirst stage, then, the best parse is selected from the ging information into parse trees, and then use a n-best parses with the finer-grained second-stage transition-based parser to perform joint learning grammar which makes use of more important con- and decoding. They manually annotated the intratextual information. Charniak et al. (2006) extend structures of words on the CTB5 dataset, which is the basic two-stage coarse-to-fine parsing to multi- proven to be very beneficial. 297 Joint WS-POS-DepPAR. Hatori et al. (2012) propose a transition-based parser that uses extra actions for word segmentation and POS tagging. Zhang et al. (2014) apply their previous work on intra-word structures (Zhang et al., 2013) to dependency parsing. The idea is to compose characterlevel dependency trees by combing inter- and intraword dependencies. Joint modeling in the DL era. Most closely related to our work, Zheng et al. (2015) also focus on jo"
2021.conll-1.23,P97-1003,0,0.407594,"eraction during inference, 3) simplifying system architecture by training a single model. In contrast, there have been few work on joint WS-POS-PAR in the DL era, with the exception of Zheng et al. (2015). Similar to this work, they treated the joint problem as a char-level parsing task, which is obviously the most straightforward way for Chinese, as illustrated in Figure 1(b). However, they showed that the joint model fails to outperform the pipeline approach. In the past few years, constituent parsing has achieved significant progress. Compared with preDL parsers based on discrete features (Collins, 1997; Charniak and Johnson, 2005; Petrov and 1 Introduction Klein, 2007), the major characteristic of graphAs shown in Figure 1(a), the results of word seg- based parsers (Stern et al., 2017; Gaddy et al., 2018; mentation (WS), part-of-speech (POS) tagging, and Kitaev and Klein, 2018; Zhang et al., 2020) is that constituent parsing (PAR) can be organized in a the score of a constituent tree is decomposed into unified hierarchical tree, where leaf nodes include scores of labeled spans (i.e., (2, 3, VP) or VP2,3 ), words and their POS tags, and non-terminal nodes without any consideration on product"
2021.conll-1.23,N19-1423,0,0.00635085,"for joint WS-POS-PAR, which can deal with both challenges. In the coarse labeling stage, the joint model outputs a bracketed tree with coarse labels (i.e., phrase, subphrase, word, subword). The constrained CKY algorithm is used to guarantee that the predicted tree contains no illegal production rules. In the fine labeling stage, the model expands each coarse label into a final fine-grained label (such as VP, VP∗ , VV, VV∗ ). Experiments on three Chinese Treebank (CTB) benchmark datasets show the joint framework is superior to the pipeline framework on both settings of without and with BERT (Devlin et al., 2019), and achieves new state-of-the-art performance. We will release our code at https://github.com/ ironsword666/JointParser. 2 2.1 Joint WS-POS-PAR as Char-Level Tree Parsing From Word-level Tree to Char-level Tree As illustrated in Figure 1, a word-level constituent tree with POS tags in 1(a) is converted into a charlevel tree in 1(b). In a word-level tree, a leaf node corresponds to a word with its POS tag. In contrast, in a char-level tree, a leaf node is always a character, and POS tags become non-terminal nodes. We use flat structures for multi-char words, such as “NN1,3 → 巧 克 力”. IP IP ∗ P"
2021.conll-1.23,N18-1091,0,0.0291423,"Missing"
2021.conll-1.23,I11-1136,0,0.0793158,"Missing"
2021.conll-1.23,P13-2110,0,0.0270986,"he model expands each coarse label into a final label (such as VP, VP∗ , VV, VV∗ ). Experiments on Chinese Penn Treebank 5.1 and 7.0 show that our joint model consistently outperforms the pipeline approach on both settings of without and with BERT, and achieves new state-of-the-art performance. IP IP NP ADVP VP PU VA AD NN 。 美味 很 巧克力 Chocolate1 Very2 Delicious3 .4 (a) Word-level tree NN PU VP NP VP ADVP VP AD VA 味 。 美 很 力 克 巧 Cho-1 -co-2 -late3 Very4 Beautiful5 Taste6 .7 (b) Char-level tree Figure 1: Example WS-POS-PAR trees. The English translation is “The chocolate is very delicious.” 2013; Wang et al., 2013). The motivations are three-fold: 1) alleviating error propagation, 2) promoting knowledge sharing and interaction during inference, 3) simplifying system architecture by training a single model. In contrast, there have been few work on joint WS-POS-PAR in the DL era, with the exception of Zheng et al. (2015). Similar to this work, they treated the joint problem as a char-level parsing task, which is obviously the most straightforward way for Chinese, as illustrated in Figure 1(b). However, they showed that the joint model fails to outperform the pipeline approach. In the past few years, const"
2021.conll-1.23,P12-1110,0,0.0197029,"t parses with a coarse-grained grammar in the (2013) integrate word segmentation and POS tagfirst stage, then, the best parse is selected from the ging information into parse trees, and then use a n-best parses with the finer-grained second-stage transition-based parser to perform joint learning grammar which makes use of more important con- and decoding. They manually annotated the intratextual information. Charniak et al. (2006) extend structures of words on the CTB5 dataset, which is the basic two-stage coarse-to-fine parsing to multi- proven to be very beneficial. 297 Joint WS-POS-DepPAR. Hatori et al. (2012) propose a transition-based parser that uses extra actions for word segmentation and POS tagging. Zhang et al. (2014) apply their previous work on intra-word structures (Zhang et al., 2013) to dependency parsing. The idea is to compose characterlevel dependency trees by combing inter- and intraword dependencies. Joint modeling in the DL era. Most closely related to our work, Zheng et al. (2015) also focus on joint WS-POS-ConPAR. They adopt CNN with pooling layers to encode the input character sequence. They show that utilizing the intra-word structures annotated by Zhang et al. (2013) can brin"
2021.conll-1.23,P82-1020,0,0.778662,"Missing"
2021.conll-1.23,P18-1249,0,0.0844041,"vel parsing task, which is obviously the most straightforward way for Chinese, as illustrated in Figure 1(b). However, they showed that the joint model fails to outperform the pipeline approach. In the past few years, constituent parsing has achieved significant progress. Compared with preDL parsers based on discrete features (Collins, 1997; Charniak and Johnson, 2005; Petrov and 1 Introduction Klein, 2007), the major characteristic of graphAs shown in Figure 1(a), the results of word seg- based parsers (Stern et al., 2017; Gaddy et al., 2018; mentation (WS), part-of-speech (POS) tagging, and Kitaev and Klein, 2018; Zhang et al., 2020) is that constituent parsing (PAR) can be organized in a the score of a constituent tree is decomposed into unified hierarchical tree, where leaf nodes include scores of labeled spans (i.e., (2, 3, VP) or VP2,3 ), words and their POS tags, and non-terminal nodes without any consideration on production rules (i.e., correspond to phrases (or constituents) with their VP2,3 → ADVP2,2 VP3,3 ). syntactic label. Before the deep learning (DL) For char-level graph-based WS-POS-PAR, there era, there had been intensive research interest in exist two severe challenges. (1) The first c"
2021.conll-1.23,P17-1111,0,0.0169357,"al. (2015) also focus on joint WS-POS-ConPAR. They adopt CNN with pooling layers to encode the input character sequence. They show that utilizing the intra-word structures annotated by Zhang et al. (2013) can bring considerable gains to parsing performance. Compared with their work, we simply use left binarization to decide the intra-word structures, and our model achieves much higher performance by adopting the state-of-the-art BiLSTM-based parsing model. Our main contribution is proposing an elegant way to handle word-vs-phrase conflicts, which unfortunately are not mentioned in their work. Kurita et al. (2017) for the first time apply neural networks to jont WS-POS-DepPAR. They enhance the transition-based parser with char/word embeddings and BiLSTM which alleviate the efforts of feature engineering. Li et al. (2018) annotate a character-level dependency treebank for joint WS-POS-DepPAR. They use a neural characterlevel transition-based parsing model to reduce computational cost. Yan et al. (2020) focus on joint WS-DepPAR. They adopt the character-level graphbased parsing approach. For intra-word dependencies, a character is designed to modify its subsequent character with the label of “app”. Wu an"
2021.conll-1.23,N16-1030,0,0.0296863,"ramework We adopt the typical cascaded pipeline framework as our baseline. In the training phase, three separate models (WS/POS/PAR) are separately trained. In the evaluation phase, the first step is word segmentation; then the word sequence is fed into the POS tagger; finally the word sequence is fed into the constituent parser. Constituent parsing. We directly adopt the competitive two-stage CRF parser of Zhang et al. (2020), which is also backbone of our joint WSPOS-PAR model. As a word-level parser, its input is composed of two parts: 1) word embedding and 2) CharLSTM word representation (Lample et al., 2016). (w) ei = emb(wi ) ⊕ CharLSTM(wi ) (11) Other components, such as encoding and span scoring, are the same with those in Section 3.3. POS tagging. The inputs and encoder of the POS tagging model are the same with the above wordcoarse 501×501 where Wc ∈R is the biaffine param- level constituent parser. We then feed hi into a eter matrix for the label c. MLP layer to directly calculate the scores of differAnalogously, for scoring fine-grained labeled ent POS tags. We directly use the local word-level spans (i, j, f ), two extra MLPs and an extra set cross-entropy loss in the training phase, and"
2021.conll-1.23,D11-1109,1,0.760646,"is already remarkably superior to the tems instead of graph-based. previous state-of-the-art joint framework proposed Joint POS-ConPAR. Wang and Xue (2014) inby Zhang et al. (2013). Compared with it, our tegrate POS tagging and constituent parsing based joint framework achieves an absolute improvement on a transition-based parsing model. They modify of 0.57, 0.84, 3.14 on word segmentation, POS the action to assign a POS tag when the word is tagging, and constituent parsing respectively on shifted into the stack, making POS tagging as a CTB5 test. part of parsing naturally. Joint POS-DepPAR. Li et al. (2011) extend 6 Related Work graph-based dependency parsing to handle POS Coarse-to-fine Parsing. Directly performing dy- tagging simultaneously based on a dynamic programming algorithm for joint decoding. Hatori namic programming parsing with fine-grained et al. (2011) combine POS tagging and dependency grammar is computationally expensive from the parsing into a shift-reduce parsing system. aspects of time and memory size. Coarse-to-fine parsers introduce complexity gradually by coarseJoint WS-POS-ConPAR. Qian and Liu (2012) to-fine utilizing a sequence of grammars. Charniak design a graph-based j"
2021.conll-1.23,2020.tacl-1.6,0,0.0110143,"e by adopting the state-of-the-art BiLSTM-based parsing model. Our main contribution is proposing an elegant way to handle word-vs-phrase conflicts, which unfortunately are not mentioned in their work. Kurita et al. (2017) for the first time apply neural networks to jont WS-POS-DepPAR. They enhance the transition-based parser with char/word embeddings and BiLSTM which alleviate the efforts of feature engineering. Li et al. (2018) annotate a character-level dependency treebank for joint WS-POS-DepPAR. They use a neural characterlevel transition-based parsing model to reduce computational cost. Yan et al. (2020) focus on joint WS-DepPAR. They adopt the character-level graphbased parsing approach. For intra-word dependencies, a character is designed to modify its subsequent character with the label of “app”. Wu and Zhang (2021) split joint WS-POS-DepPAR into joint WS-POS and joint WS-DepPAR by using a shared character-level encoder and two independent decoders. They adopt outputs from joint WSPOS as final word segmentation results. 7 Conclusions In this work, we propose a two-stage coarse-to-fine labeling framework of joint WS-POS-PAR, which is shown to be able to handle both challenges for char-level"
2021.conll-1.23,P13-1013,0,0.232749,"Zhang and Clark, 2009; Liu and Zhang, 2017) to split train/dev/test datasets. Because both dev and test datasets of CTB5 contain only about 350 sentences, which is not enough to conduct robust investigations, we adopt another partition of Chinese Penn Treebank 5.1 (CTB5big), which is proposed by Duan et al. (2007). For CTB7, we follow the data split suggested in official guidelines. Table 2 shows the data statistics. Evaluation metrics. Before the evaluation, we convert predicted character-level CNF trees into word-level n-ary trees. Since our joint framework is based on characters, we follow Zhang et al. (2013) to redefine the span by the index of its beginning and ending characters. We adopt the standard constituent-level labeled precision, recall, F-score (Par P/R/F) as the evaluation metrics for constituent parsing, where POS tags are discarded. Metrics of labeled precision, recall, and F-score (Tag P/R/F) of POS tags are used to evaluate the POS tagging task. For word segmentation, the unlabeled precision, recall, F-score (Seg P/R/F) of POS tagging are served as the evaluation metrics. 10 102 85.84 103 85.89 85.8 85.71 85.6 93.6 Word Segmentation. The inputs and encoder of the WS model are the s"
2021.conll-1.23,P14-1125,0,0.0271985,"t parse is selected from the ging information into parse trees, and then use a n-best parses with the finer-grained second-stage transition-based parser to perform joint learning grammar which makes use of more important con- and decoding. They manually annotated the intratextual information. Charniak et al. (2006) extend structures of words on the CTB5 dataset, which is the basic two-stage coarse-to-fine parsing to multi- proven to be very beneficial. 297 Joint WS-POS-DepPAR. Hatori et al. (2012) propose a transition-based parser that uses extra actions for word segmentation and POS tagging. Zhang et al. (2014) apply their previous work on intra-word structures (Zhang et al., 2013) to dependency parsing. The idea is to compose characterlevel dependency trees by combing inter- and intraword dependencies. Joint modeling in the DL era. Most closely related to our work, Zheng et al. (2015) also focus on joint WS-POS-ConPAR. They adopt CNN with pooling layers to encode the input character sequence. They show that utilizing the intra-word structures annotated by Zhang et al. (2013) can bring considerable gains to parsing performance. Compared with their work, we simply use left binarization to decide the"
2021.conll-1.23,W09-3825,0,0.0546649,"to better performance. 294 #Train #Dev #Test #Labels Orig. CNF CNF∗ CTB5 18,104 352 348 57 CTB5-big 16,091 803 1,910 57 CTB7 46,572 2,079 2,796 63 323 313 638 183 179 293 Table 2: Numbers of sentences and constituent labels. “Orig.” indicates the number of labels before CNF. “CNF∗ ” represents the number of remained labels after pruning labels with frequency &lt; α = 10. 97.4 638 600 97.2 400 97.0 293 181 200 1 10 102 140 93.8 86.0 93.83 93.81 93.76 93.82 5 5.1 Experiments Settings Data. We conduct experiments on Chinese Penn Treebank 5.1 (CTB5) and 7 (CTB7). For CTB5, we follow previous works (Zhang and Clark, 2009; Liu and Zhang, 2017) to split train/dev/test datasets. Because both dev and test datasets of CTB5 contain only about 350 sentences, which is not enough to conduct robust investigations, we adopt another partition of Chinese Penn Treebank 5.1 (CTB5big), which is proposed by Duan et al. (2007). For CTB7, we follow the data split suggested in official guidelines. Table 2 shows the data statistics. Evaluation metrics. Before the evaluation, we convert predicted character-level CNF trees into word-level n-ary trees. Since our joint framework is based on characters, we follow Zhang et al. (2013) t"
2021.conll-1.23,Q17-1004,0,0.0153093,"294 #Train #Dev #Test #Labels Orig. CNF CNF∗ CTB5 18,104 352 348 57 CTB5-big 16,091 803 1,910 57 CTB7 46,572 2,079 2,796 63 323 313 638 183 179 293 Table 2: Numbers of sentences and constituent labels. “Orig.” indicates the number of labels before CNF. “CNF∗ ” represents the number of remained labels after pruning labels with frequency &lt; α = 10. 97.4 638 600 97.2 400 97.0 293 181 200 1 10 102 140 93.8 86.0 93.83 93.81 93.76 93.82 5 5.1 Experiments Settings Data. We conduct experiments on Chinese Penn Treebank 5.1 (CTB5) and 7 (CTB7). For CTB5, we follow previous works (Zhang and Clark, 2009; Liu and Zhang, 2017) to split train/dev/test datasets. Because both dev and test datasets of CTB5 contain only about 350 sentences, which is not enough to conduct robust investigations, we adopt another partition of Chinese Penn Treebank 5.1 (CTB5big), which is proposed by Duan et al. (2007). For CTB7, we follow the data split suggested in official guidelines. Table 2 shows the data statistics. Evaluation metrics. Before the evaluation, we convert predicted character-level CNF trees into word-level n-ary trees. Since our joint framework is based on characters, we follow Zhang et al. (2013) to redefine the span by"
2021.conll-1.23,W03-1025,0,0.176447,"e is decomposed into unified hierarchical tree, where leaf nodes include scores of labeled spans (i.e., (2, 3, VP) or VP2,3 ), words and their POS tags, and non-terminal nodes without any consideration on production rules (i.e., correspond to phrases (or constituents) with their VP2,3 → ADVP2,2 VP3,3 ). syntactic label. Before the deep learning (DL) For char-level graph-based WS-POS-PAR, there era, there had been intensive research interest in exist two severe challenges. (1) The first challenge jointly modeling of the three tasks, i.e., WS-POSis high model complexity. On the one hand, a PAR (Luo, 2003; Qian and Liu, 2012; Zhang et al., char sequence is almost twice longer than its word ∗ Corresponding author sequence. On the other hand, the size of the la290 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 290–299 November 10–11, 2021. ©2021 Association for Computational Linguistics bel set is very large (e.g., ≥ 600) after transforming char-level trees into Chomsky normal form (CNF), as illustrated in Figure 2(a). Both factors greatly increase time and memory complexity. (2) The second challenge is rule conflict. Since neural graph-based parsers"
2021.conll-1.23,N07-1051,0,0.0954659,"Joint frameworks Qian and Liu (2012) 97.96 93.81 82.85 Zhang et al. (2013) 97.84 94.80 84.43 Wang et al. (2013) 97.86 94.40 83.42 Zheng et al. (2015) – – 84.22 Pipeline Joint 98.05 95.17 87.47 98.41 95.64 87.57 Table 6: Comparisons with previous works on CTB5 test. stage parsing by constructing multiple levels of grammars. They cluster constituent labels of the raw treebank into coarser categories. For example, in the most coarse grammar, there only exists one label “P” which corresponds to the phases. The proposed clustering idea in their work is similar to that used in our work. The work of Petrov and Klein (2007) also builds a multi-stage parser which constructs a sequence of increasingly refined grammars in a automatic fashion. However, the final grammar is finer than the raw treebank grammar. Joint modeling in the pre-DL era. In this part, we try to briefly discuss works on joint modeling Comparisons with previous works. Finally, we of word segmentation, POS tagging, and parsing, list results of all recent related works of the joint which were conducted in the pre-DL era, and most framework on CTB5. Please note that our pipeline of which are transition-based (shift-reduce) sysframework is already re"
2021.conll-1.23,D12-1046,0,0.406671,"osed into unified hierarchical tree, where leaf nodes include scores of labeled spans (i.e., (2, 3, VP) or VP2,3 ), words and their POS tags, and non-terminal nodes without any consideration on production rules (i.e., correspond to phrases (or constituents) with their VP2,3 → ADVP2,2 VP3,3 ). syntactic label. Before the deep learning (DL) For char-level graph-based WS-POS-PAR, there era, there had been intensive research interest in exist two severe challenges. (1) The first challenge jointly modeling of the three tasks, i.e., WS-POSis high model complexity. On the one hand, a PAR (Luo, 2003; Qian and Liu, 2012; Zhang et al., char sequence is almost twice longer than its word ∗ Corresponding author sequence. On the other hand, the size of the la290 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 290–299 November 10–11, 2021. ©2021 Association for Computational Linguistics bel set is very large (e.g., ≥ 600) after transforming char-level trees into Chomsky normal form (CNF), as illustrated in Figure 2(a). Both factors greatly increase time and memory complexity. (2) The second challenge is rule conflict. Since neural graph-based parsers do not consider pro"
2021.conll-1.23,P17-1076,0,0.158471,"of Zheng et al. (2015). Similar to this work, they treated the joint problem as a char-level parsing task, which is obviously the most straightforward way for Chinese, as illustrated in Figure 1(b). However, they showed that the joint model fails to outperform the pipeline approach. In the past few years, constituent parsing has achieved significant progress. Compared with preDL parsers based on discrete features (Collins, 1997; Charniak and Johnson, 2005; Petrov and 1 Introduction Klein, 2007), the major characteristic of graphAs shown in Figure 1(a), the results of word seg- based parsers (Stern et al., 2017; Gaddy et al., 2018; mentation (WS), part-of-speech (POS) tagging, and Kitaev and Klein, 2018; Zhang et al., 2020) is that constituent parsing (PAR) can be organized in a the score of a constituent tree is decomposed into unified hierarchical tree, where leaf nodes include scores of labeled spans (i.e., (2, 3, VP) or VP2,3 ), words and their POS tags, and non-terminal nodes without any consideration on production rules (i.e., correspond to phrases (or constituents) with their VP2,3 → ADVP2,2 VP3,3 ). syntactic label. Before the deep learning (DL) For char-level graph-based WS-POS-PAR, there e"
2021.conll-1.23,P14-1069,0,0.0169979,"ver, the final grammar is finer than the raw treebank grammar. Joint modeling in the pre-DL era. In this part, we try to briefly discuss works on joint modeling Comparisons with previous works. Finally, we of word segmentation, POS tagging, and parsing, list results of all recent related works of the joint which were conducted in the pre-DL era, and most framework on CTB5. Please note that our pipeline of which are transition-based (shift-reduce) sysframework is already remarkably superior to the tems instead of graph-based. previous state-of-the-art joint framework proposed Joint POS-ConPAR. Wang and Xue (2014) inby Zhang et al. (2013). Compared with it, our tegrate POS tagging and constituent parsing based joint framework achieves an absolute improvement on a transition-based parsing model. They modify of 0.57, 0.84, 3.14 on word segmentation, POS the action to assign a POS tag when the word is tagging, and constituent parsing respectively on shifted into the stack, making POS tagging as a CTB5 test. part of parsing naturally. Joint POS-DepPAR. Li et al. (2011) extend 6 Related Work graph-based dependency parsing to handle POS Coarse-to-fine Parsing. Directly performing dy- tagging simultaneously b"
2021.emnlp-main.262,N18-1118,0,0.0201027,"rimental Carpuat (2009) and Türe et al. (2012) demonstrate that applying “one translation per discourse” con- results on Chinese↔English and English→French translation tasks show that our approach not only straint in SMT leads to better translation quality. achieves higher BLEU scores than various contextMoving to NMT, most of document-level NMT aware NMT models, but also greatly improves lexstudies have proposed various context-aware NMT models to leverage either local context, e.g., previ- ical translation consistency. ous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., Acknowledgments 2018, 2019b; Yang et al., 2019), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; The authors would like to thank the anonymous Maruf et al., 2019; Tan et al., 2019; Xiong et al., reviewers for their constructive feedback. This 2019; Zheng et al., 2020; Kang et al., 2020). How- work was supported by the National Natural Sciever, different from ours, these studies aim to im- ence Foundation of China (Grant No. 62036004 and 61876120). prove the translation accuracy without handling a specific discourse phenomena. Kuang et al. (2017) and Tu et al"
2021.emnlp-main.262,W09-2404,0,0.172438,"s for the shanghai real estate sector last year … #4: … over the previous year , while real estate added value … #7: the annual real estate market added amounted to 67.023 billion yuan … #2 : #3: #4: #7: Unlike sentence-level neural machine translation (NMT), document-level NMT needs to not only model intra-sentence dependencies, but also consider a wide variety of inter-sentence discourse phenomena, such as coreference, lexical cohesion, semantic coherence, discourse relations. Motivated by the success of “one translation per discourse” in statistical machine translation (SMT) (Merkel, 1996; Carpuat, 2009; Türe et al., 2012; Guillou, 2013; Al Khotaba and Al Tarawneh, 2015), in this paper our goal is to encourage lexical translation consistency for document-level NMT. Figure 1 shows an example of an input document and its output translated by a state-of-the-art sentence-level NMT system. The technical term Corresponding author: Junhui Li. Reference … the state 's macro-control policies for the real estate sector … … various real estate sector indexes in shanghai … last year … while value added from the real estate sector … for the full year , the real estate sector registered value added of 67."
2021.emnlp-main.262,P17-4012,0,0.0352629,"rom the beginning may break the balance between optimizing the encoder and the decoder, and make it hard for the training to properly converge. To alleviate this problem, we divide the whole training process into two stages. In the first stage, we train the models to convergence with the cross entropy loss JNMT (θ) only while in the second stage, we combine the consistency constraint loss JCC (θ) and train the models with the joint loss. Actually, the second training stage acts like a fine-tuning, in which we use a smaller learning rate and fewer training steps. Model Setting. We use OpenNMT (Klein et al., 2017) as the implementation of the Transformer and extend it. For the number of linked words with the current word, we set K = 6. The margin size γ in the consistency constraint loss is set to 0.2 while the weight α in joint objective function is set to 0.01. Other model settings are in Appendix B. Evaluation. For all translation tasks, we report case-insensitive BLEU score as calculated by the multi-bleu.perl script. 4.2 Experimental Result Besides sentence-level Transformer, we also compare our approach to three previous Transformerbased context-aware NMT models: HAN (Miculicich et al., 2018),8 S"
2021.emnlp-main.262,2012.eamt-1.60,0,0.0383592,"training data is composed from LDC. We use the NIST2006 dataset as the development set and combine NIST2002, 2003, 2004, 2005 and 2008 as the test set. Note that in the development and test sets every Chinese document has four aligned English documents, thus for ZH→EN translation one Chinese sentence has four references. In turn for EN→ZH translation each English sentence has one reference, and the numbers of sentences in development and test sets are four times those of ZH→EN translation, e.g., 4 × 879 and 4 × 5473, respectively. For ZH↔EN (TED), the dataset is from the IWSLT 2014 and 2015 (Cettolo et al., 2012, 2015) evaluation. We use dev2010 as the development set and combine tst2010-2013 as the test set. For both ZH↔EN translations, every source sentence has one translation reference. For EN→FR, we use IWSLT 2015 (Cettolo et al., 2015) evaluation as training data. For development and testing, we use dev2010 as the development set and combine tst2010-2013 as test set and every source sentence has one translation reference. See Appendix A for more statistics and preprocessing of the experimental datasets. Training Strategy. To compute the consistency constraint loss JCC (θ), sentences are required"
2021.emnlp-main.262,2015.iwslt-evaluation.1,0,0.024531,"ned English documents, thus for ZH→EN translation one Chinese sentence has four references. In turn for EN→ZH translation each English sentence has one reference, and the numbers of sentences in development and test sets are four times those of ZH→EN translation, e.g., 4 × 879 and 4 × 5473, respectively. For ZH↔EN (TED), the dataset is from the IWSLT 2014 and 2015 (Cettolo et al., 2012, 2015) evaluation. We use dev2010 as the development set and combine tst2010-2013 as the test set. For both ZH↔EN translations, every source sentence has one translation reference. For EN→FR, we use IWSLT 2015 (Cettolo et al., 2015) evaluation as training data. For development and testing, we use dev2010 as the development set and combine tst2010-2013 as test set and every source sentence has one translation reference. See Appendix A for more statistics and preprocessing of the experimental datasets. Training Strategy. To compute the consistency constraint loss JCC (θ), sentences are required to be encoded twice, i.e., one for encoding with the wordlink attention sub-layer and the other for encoding without it. Therefore, including this loss function from the beginning may break the balance between optimizing the encoder"
2021.emnlp-main.262,W04-3250,0,0.49596,"Missing"
2021.emnlp-main.262,P07-2045,0,0.0114581,"Missing"
2021.emnlp-main.262,N13-1073,0,0.0387315,"Missing"
2021.emnlp-main.262,P11-1124,0,0.0563196,"Missing"
2021.emnlp-main.262,D11-1084,1,0.767719,"re language model to re-rank sentence-level translation candidates. There has been substantial work in SMT that either 7 Conclusion encourages or enforces lexical translation consistency. For example, Xiao et al. (2011) and Gar- In this paper, we apply “one translation per discia et al. (2014, 2017) propose post-editing ap- course” in NMT, and have proposed an approach to encourage lexical translation consistency. This is proaches to re-translate those source words which have been translated differently in a document. done by first obtaining a word link for each source Tiedemann (2010a,b) and Gong et al. (2011) pro- word in a document, which tells the positions the pose cache-based approaches to remember trans- source word appears at. Then we encourage the translations of words within a link to be consistent lation history. Discriminative learning approaches by both exchanging their context information in (Ma et al., 2011; He et al., 2011) are also proposed to fix lexical translation non-consistency. Besides, encoding, and using an auxiliary loss to constrain their translation being consistent. Experimental Carpuat (2009) and Türe et al. (2012) demonstrate that applying “one translation per discours"
2021.emnlp-main.262,P18-1118,0,0.0941309,"地产业/fang_di_chan_ye 增加值 … 全年 房地产业/fang_di_chan_ye 增加值 为 670.23 亿 元 … 房地产业/fang_di_chan_ye, occurring four times within a document, surprisingly obtains different translations while in its reference (human translation) it is translated consistently. Such inconsistent translations, however, tend to confuse readers in some cases. Recent years have witnessed an increasing interest in document-level NMT, but most previous studies explore various context-aware models for better incorporating document-level context to improve translation performance without handling a specific discourse phenomenon ( Maruf and Haffari 2018; Miculicich et al. 2018; Maruf et al. 2019, to name a few). As a way to encourage lexical translation consistency, Kuang et al. (2017) and Tu et al. (2018) cache recently translated words and/or their translations for translating future sentences. However, cache-based approaches may potentially guide the translation of future sentences in a wrong way since the cached translation could be incorrect. Rather than explicitly presenting lexical translations used 3265 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3265–3277 c November 7–11, 2021. 2021"
2021.emnlp-main.262,W13-3302,0,0.0963033,"tor last year … #4: … over the previous year , while real estate added value … #7: the annual real estate market added amounted to 67.023 billion yuan … #2 : #3: #4: #7: Unlike sentence-level neural machine translation (NMT), document-level NMT needs to not only model intra-sentence dependencies, but also consider a wide variety of inter-sentence discourse phenomena, such as coreference, lexical cohesion, semantic coherence, discourse relations. Motivated by the success of “one translation per discourse” in statistical machine translation (SMT) (Merkel, 1996; Carpuat, 2009; Türe et al., 2012; Guillou, 2013; Al Khotaba and Al Tarawneh, 2015), in this paper our goal is to encourage lexical translation consistency for document-level NMT. Figure 1 shows an example of an input document and its output translated by a state-of-the-art sentence-level NMT system. The technical term Corresponding author: Junhui Li. Reference … the state 's macro-control policies for the real estate sector … … various real estate sector indexes in shanghai … last year … while value added from the real estate sector … for the full year , the real estate sector registered value added of 67.023 billion yuan , … Figure 1: An"
2021.emnlp-main.262,N19-1313,0,0.231626,"e 增加值 为 670.23 亿 元 … 房地产业/fang_di_chan_ye, occurring four times within a document, surprisingly obtains different translations while in its reference (human translation) it is translated consistently. Such inconsistent translations, however, tend to confuse readers in some cases. Recent years have witnessed an increasing interest in document-level NMT, but most previous studies explore various context-aware models for better incorporating document-level context to improve translation performance without handling a specific discourse phenomenon ( Maruf and Haffari 2018; Miculicich et al. 2018; Maruf et al. 2019, to name a few). As a way to encourage lexical translation consistency, Kuang et al. (2017) and Tu et al. (2018) cache recently translated words and/or their translations for translating future sentences. However, cache-based approaches may potentially guide the translation of future sentences in a wrong way since the cached translation could be incorrect. Rather than explicitly presenting lexical translations used 3265 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3265–3277 c November 7–11, 2021. 2021 Association for Computational Linguistics i"
2021.emnlp-main.262,2011.mtsummit-papers.52,0,0.169503,"Missing"
2021.emnlp-main.262,D18-1325,0,0.302013,"… 全年 房地产业/fang_di_chan_ye 增加值 为 670.23 亿 元 … 房地产业/fang_di_chan_ye, occurring four times within a document, surprisingly obtains different translations while in its reference (human translation) it is translated consistently. Such inconsistent translations, however, tend to confuse readers in some cases. Recent years have witnessed an increasing interest in document-level NMT, but most previous studies explore various context-aware models for better incorporating document-level context to improve translation performance without handling a specific discourse phenomenon ( Maruf and Haffari 2018; Miculicich et al. 2018; Maruf et al. 2019, to name a few). As a way to encourage lexical translation consistency, Kuang et al. (2017) and Tu et al. (2018) cache recently translated words and/or their translations for translating future sentences. However, cache-based approaches may potentially guide the translation of future sentences in a wrong way since the cached translation could be incorrect. Rather than explicitly presenting lexical translations used 3265 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3265–3277 c November 7–11, 2021. 2021 Association for Computat"
2021.emnlp-main.262,2020.emnlp-main.175,0,0.0311051,"t of document-level NMT aware NMT models, but also greatly improves lexstudies have proposed various context-aware NMT models to leverage either local context, e.g., previ- ical translation consistency. ous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., Acknowledgments 2018, 2019b; Yang et al., 2019), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; The authors would like to thank the anonymous Maruf et al., 2019; Tan et al., 2019; Xiong et al., reviewers for their constructive feedback. This 2019; Zheng et al., 2020; Kang et al., 2020). How- work was supported by the National Natural Sciever, different from ours, these studies aim to im- ence Foundation of China (Grant No. 62036004 and 61876120). prove the translation accuracy without handling a specific discourse phenomena. Kuang et al. (2017) and Tu et al. (2018) cache recently translated words References and/or their translations which could be used to increase lexical consistency when translate future Eissa Al Khotaba and Khaled Al Tarawneh. 2015. Lexical discourse analysis in translation. Education and sentences. However, cache-based approaches rePractice, 6(3):106–112"
2021.emnlp-main.262,P16-1162,0,0.16299,"Missing"
2021.emnlp-main.262,D19-1168,0,0.0873766,"t to sentence-level Transformer when K is 0. loss) achieves higher LTCR than Transformer over 3271 POS Noun Verb Adv Adj Others All Gold 80.98 57.86 61.23 81.77 75.96 74.24 Trans. 50.74 35.96 30.77 52.83 30.92 43.13 +word-link +CC-loss 58.11 38.19 35.66 56.63 34.11 48.34 +word-link 53.66 38.72 32.68 53.41 32.91 45.01 Model Trans. +word-link w/o SPE Test 68.39 68.97 69.23 all POS tags, especially for nouns. Meanwhile, the performance gap behind that of reference translation suggests that there still exists room for further improvement. Pronoun Translation We follow Miculicich et al. (2018) and Tan et al. (2019) to evaluate coreference and anaphora using the reference-based metric: accuracy of pronoun translation (Werlen and Popescu-Belis, 2017). Table 8 lists the performance of pronoun translation. From it we observe that our approach also improves the performance of pronoun translation while exchanging context information among linked words (i.e., +word-link) contributes more than the consistency constraint loss (i.e., +CCloss). 5.5 Human Evaluation We conduct a human evaluation on 500 sentences randomly selected from our test set. Let us assume that the i-th sentence Si in a document-level paralle"
2021.emnlp-main.262,W10-2602,0,0.0379134,"020) train a context-aware language model to re-rank sentence-level translation candidates. There has been substantial work in SMT that either 7 Conclusion encourages or enforces lexical translation consistency. For example, Xiao et al. (2011) and Gar- In this paper, we apply “one translation per discia et al. (2014, 2017) propose post-editing ap- course” in NMT, and have proposed an approach to encourage lexical translation consistency. This is proaches to re-translate those source words which have been translated differently in a document. done by first obtaining a word link for each source Tiedemann (2010a,b) and Gong et al. (2011) pro- word in a document, which tells the positions the pose cache-based approaches to remember trans- source word appears at. Then we encourage the translations of words within a link to be consistent lation history. Discriminative learning approaches by both exchanging their context information in (Ma et al., 2011; He et al., 2011) are also proposed to fix lexical translation non-consistency. Besides, encoding, and using an auxiliary loss to constrain their translation being consistent. Experimental Carpuat (2009) and Türe et al. (2012) demonstrate that applying “o"
2021.emnlp-main.262,W10-1728,0,0.026898,"020) train a context-aware language model to re-rank sentence-level translation candidates. There has been substantial work in SMT that either 7 Conclusion encourages or enforces lexical translation consistency. For example, Xiao et al. (2011) and Gar- In this paper, we apply “one translation per discia et al. (2014, 2017) propose post-editing ap- course” in NMT, and have proposed an approach to encourage lexical translation consistency. This is proaches to re-translate those source words which have been translated differently in a document. done by first obtaining a word link for each source Tiedemann (2010a,b) and Gong et al. (2011) pro- word in a document, which tells the positions the pose cache-based approaches to remember trans- source word appears at. Then we encourage the translations of words within a link to be consistent lation history. Discriminative learning approaches by both exchanging their context information in (Ma et al., 2011; He et al., 2011) are also proposed to fix lexical translation non-consistency. Besides, encoding, and using an auxiliary loss to constrain their translation being consistent. Experimental Carpuat (2009) and Türe et al. (2012) demonstrate that applying “o"
2021.emnlp-main.262,Q18-1029,0,0.122887,"translations while in its reference (human translation) it is translated consistently. Such inconsistent translations, however, tend to confuse readers in some cases. Recent years have witnessed an increasing interest in document-level NMT, but most previous studies explore various context-aware models for better incorporating document-level context to improve translation performance without handling a specific discourse phenomenon ( Maruf and Haffari 2018; Miculicich et al. 2018; Maruf et al. 2019, to name a few). As a way to encourage lexical translation consistency, Kuang et al. (2017) and Tu et al. (2018) cache recently translated words and/or their translations for translating future sentences. However, cache-based approaches may potentially guide the translation of future sentences in a wrong way since the cached translation could be incorrect. Rather than explicitly presenting lexical translations used 3265 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3265–3277 c November 7–11, 2021. 2021 Association for Computational Linguistics in previous sentences as in cache-based approaches, in this paper we aim at improving lexical translation consiste"
2021.emnlp-main.262,N12-1046,0,0.174156,"hai real estate sector last year … #4: … over the previous year , while real estate added value … #7: the annual real estate market added amounted to 67.023 billion yuan … #2 : #3: #4: #7: Unlike sentence-level neural machine translation (NMT), document-level NMT needs to not only model intra-sentence dependencies, but also consider a wide variety of inter-sentence discourse phenomena, such as coreference, lexical cohesion, semantic coherence, discourse relations. Motivated by the success of “one translation per discourse” in statistical machine translation (SMT) (Merkel, 1996; Carpuat, 2009; Türe et al., 2012; Guillou, 2013; Al Khotaba and Al Tarawneh, 2015), in this paper our goal is to encourage lexical translation consistency for document-level NMT. Figure 1 shows an example of an input document and its output translated by a state-of-the-art sentence-level NMT system. The technical term Corresponding author: Junhui Li. Reference … the state 's macro-control policies for the real estate sector … … various real estate sector indexes in shanghai … last year … while value added from the real estate sector … for the full year , the real estate sector registered value added of 67.023 billion yuan ,"
2021.emnlp-main.262,D19-1081,0,0.0143745,"n lead to the mediocrity of cross-sentence information. 6 Related Work sults in related studies (Zhang et al., 2018; Miculicich et al., 2018) have shown that the improvement of cache-based approaches is limited in BLEU over (sentence-level) Transformer. Our approach is different from cached-based approach as we translate sentences within a document synchronously, and more importantly it does not explicitly suggest any translation. There also exists many studies in NMT that aim to resolve discourse phenomena in post-process. For example, to make translation outputs of a document more coherent, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to re-rank sentence-level translation candidates. There has been substantial work in SMT that either 7 Conclusion encourages or enforces lexical translation consistency. For example, Xiao et al. (2011) and Gar- In this paper, we apply “one translation per discia et al. (2014, 2017) propose post-editing ap- course” in NMT, and have proposed an approach to encourage lexical translation consistency. This is p"
2021.emnlp-main.262,P19-1116,0,0.0228478,"Missing"
2021.emnlp-main.262,P18-1117,0,0.0355587,"Missing"
2021.emnlp-main.262,D17-1301,0,0.0155984,"heir translation being consistent. Experimental Carpuat (2009) and Türe et al. (2012) demonstrate that applying “one translation per discourse” con- results on Chinese↔English and English→French translation tasks show that our approach not only straint in SMT leads to better translation quality. achieves higher BLEU scores than various contextMoving to NMT, most of document-level NMT aware NMT models, but also greatly improves lexstudies have proposed various context-aware NMT models to leverage either local context, e.g., previ- ical translation consistency. ous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., Acknowledgments 2018, 2019b; Yang et al., 2019), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; The authors would like to thank the anonymous Maruf et al., 2019; Tan et al., 2019; Xiong et al., reviewers for their constructive feedback. This 2019; Zheng et al., 2020; Kang et al., 2020). How- work was supported by the National Natural Sciever, different from ours, these studies aim to im- ence Foundation of China (Grant No. 62036004 and 61876120). prove the translation accuracy without handling a specific discourse ph"
2021.emnlp-main.262,W17-4802,0,0.0477661,"Missing"
2021.emnlp-main.262,2011.mtsummit-papers.13,0,0.0470367,"es not explicitly suggest any translation. There also exists many studies in NMT that aim to resolve discourse phenomena in post-process. For example, to make translation outputs of a document more coherent, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to re-rank sentence-level translation candidates. There has been substantial work in SMT that either 7 Conclusion encourages or enforces lexical translation consistency. For example, Xiao et al. (2011) and Gar- In this paper, we apply “one translation per discia et al. (2014, 2017) propose post-editing ap- course” in NMT, and have proposed an approach to encourage lexical translation consistency. This is proaches to re-translate those source words which have been translated differently in a document. done by first obtaining a word link for each source Tiedemann (2010a,b) and Gong et al. (2011) pro- word in a document, which tells the positions the pose cache-based approaches to remember trans- source word appears at. Then we encourage the translations of words within a link to be consistent"
2021.emnlp-main.262,D19-1164,0,0.0137924,"applying “one translation per discourse” con- results on Chinese↔English and English→French translation tasks show that our approach not only straint in SMT leads to better translation quality. achieves higher BLEU scores than various contextMoving to NMT, most of document-level NMT aware NMT models, but also greatly improves lexstudies have proposed various context-aware NMT models to leverage either local context, e.g., previ- ical translation consistency. ous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., Acknowledgments 2018, 2019b; Yang et al., 2019), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; The authors would like to thank the anonymous Maruf et al., 2019; Tan et al., 2019; Xiong et al., reviewers for their constructive feedback. This 2019; Zheng et al., 2020; Kang et al., 2020). How- work was supported by the National Natural Sciever, different from ours, these studies aim to im- ence Foundation of China (Grant No. 62036004 and 61876120). prove the translation accuracy without handling a specific discourse phenomena. Kuang et al. (2017) and Tu et al. (2018) cache recently translated words References and/or thei"
2021.emnlp-main.262,2020.tacl-1.23,0,0.0239371,"at the improvement of cache-based approaches is limited in BLEU over (sentence-level) Transformer. Our approach is different from cached-based approach as we translate sentences within a document synchronously, and more importantly it does not explicitly suggest any translation. There also exists many studies in NMT that aim to resolve discourse phenomena in post-process. For example, to make translation outputs of a document more coherent, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to re-rank sentence-level translation candidates. There has been substantial work in SMT that either 7 Conclusion encourages or enforces lexical translation consistency. For example, Xiao et al. (2011) and Gar- In this paper, we apply “one translation per discia et al. (2014, 2017) propose post-editing ap- course” in NMT, and have proposed an approach to encourage lexical translation consistency. This is proaches to re-translate those source words which have been translated differently in a document. done by first obtaining a word link for each source Tied"
2021.emnlp-main.262,D18-1049,1,0.903212,"ess of this method, we perform a contrastive experiment by replacing multi-head attention function in Eq. 3 with the average pooling function Eq. 8.    (m) (m) (m) Di,j = LayerNorm Avg Ci,j + Bi,j . (8) Table 11 lists the performance of translation when we use different functions to exchange information among linked words. From it we observe that the multi-head attention function performs better. This in turn may suggest that simply averaging hidden states of linked words to exchange information lead to the mediocrity of cross-sentence information. 6 Related Work sults in related studies (Zhang et al., 2018; Miculicich et al., 2018) have shown that the improvement of cache-based approaches is limited in BLEU over (sentence-level) Transformer. Our approach is different from cached-based approach as we translate sentences within a document synchronously, and more importantly it does not explicitly suggest any translation. There also exists many studies in NMT that aim to resolve discourse phenomena in post-process. For example, to make translation outputs of a document more coherent, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistenc"
2021.emnlp-main.360,2020.coling-main.24,0,0.481817,"extraction besides the corresponding sentiment classification in a multi-modal scenario. Note that we also propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Text-based Joint Aspect Terms Extraction and Sentiment Classification. Some studies (Zhang et al., 2020a) have attempted to solve both sub-tasks in a more integrated way, by jointly extracting aspect terms and predicting their sentiment polarities. The most recent and representative are a span-based extract-then-classify approach (Hu et al., 2019) and a directed GCN with syntactic information (Chen et al., 2020). However, all the above studies can not model the visual guidance for both sub-tasks. Different from them, we propose a multi-modal joint framework to handle both MATE and MASC. 3 Joint Multi-modal Aspect-Sentiment Analysis 3.1 Cross-modal Relation Detection Unlike traditional approaches, which take visual information into consideration completely and ignore whether image can bring benefits to text, we incorporate the image-text relation into the model and only retain the auxiliary visual information towards the text. Therefore, we build a relation module by pre-training to properly exploit v"
2021.emnlp-main.360,P19-1052,0,0.0263826,"ent Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulkens and van Cranen- datasets in Twitter for multi-modal target-oriented burgh, 2020; Akhtar et al., 2020). While, multi- (aka aspect-level) sentiment classification and levermodal target-oriented sentiment analysis has be- age BERT as backbone to effectively combine both come more and more vital because of its urgent textual and visual modalities. In the same period, 4396 Yu et al. (2020a) propose a target-sensitive attention and fusion network to address both text-based and multi-modal target-oriented s"
2021.emnlp-main.360,2020.emnlp-main.164,0,0.118715,"(a) RT @ funnytwittingg : [OBAMA]Neg TO [ISRAEL]Neu ? [OBAMA]Neg TO [UKRAINE] ? [OBAMA]Neg TO [USA]Neu ? (b) Figure 1: Two examples for joint multi-modal aspectsentiment analysis. aspect terms from a free text with its accompanying image (Wu et al., 2020a). Second, MASC aims to classify the sentiment polarity of a multi-modal post towards a given aspect in textual modality (Yu and Jiang, 2019). To better satisfy the practical applications, the aspect term-polarity co-extraction, which solves ATE and ASC simultaneously, receives much attention recently in a textual scenario (Wan et al., 2020; Chen and Qian, 2020b; Ying et al., 2020). However, to our best knowledge, in the multi-modal scenario, the joint MATE and MASC, i.e., joint multi-modal aspect-sentiment analysis (JMASA), have never been investigated so far. For this joint multi-modal task, we believe that there exist the following challenges at least. 1 Introduction On the one hand, visual modality may provide no clues for one of sub-tasks. For example, in FigMulti-modal aspect-level (aka target-oriented) senure 1(a), since the image shows most of the content timent analysis (MALSA) is an important and finedescribed in the text, and we can’t inf"
2021.emnlp-main.360,2020.acl-main.340,0,0.174693,"(a) RT @ funnytwittingg : [OBAMA]Neg TO [ISRAEL]Neu ? [OBAMA]Neg TO [UKRAINE] ? [OBAMA]Neg TO [USA]Neu ? (b) Figure 1: Two examples for joint multi-modal aspectsentiment analysis. aspect terms from a free text with its accompanying image (Wu et al., 2020a). Second, MASC aims to classify the sentiment polarity of a multi-modal post towards a given aspect in textual modality (Yu and Jiang, 2019). To better satisfy the practical applications, the aspect term-polarity co-extraction, which solves ATE and ASC simultaneously, receives much attention recently in a textual scenario (Wan et al., 2020; Chen and Qian, 2020b; Ying et al., 2020). However, to our best knowledge, in the multi-modal scenario, the joint MATE and MASC, i.e., joint multi-modal aspect-sentiment analysis (JMASA), have never been investigated so far. For this joint multi-modal task, we believe that there exist the following challenges at least. 1 Introduction On the one hand, visual modality may provide no clues for one of sub-tasks. For example, in FigMulti-modal aspect-level (aka target-oriented) senure 1(a), since the image shows most of the content timent analysis (MALSA) is an important and finedescribed in the text, and we can’t inf"
2021.emnlp-main.360,N19-1423,0,0.135672,"k and a softn=1 be the set of data samples. Given a word sequence X = {x1 , x2 , · · · , xk } max activation function as follows: with length k and an image I, the joint task is to extract a aspect terms list A = {a1 , a2 , · · · , am } pr = softmax(W2 tanh(W1 H)) (5) and classify the aspect sentiment list S = {s1 , s2 , · · · , sm } simultaneously, where m denotes where W1 ∈ R4∗dm ×dm and W2 ∈ Rdm ×2 are the number of aspects. Note that the word embed- two trainable parameter matrices. H means the dings are obtained by pre-processing via BERT concatenation of Ho , Hx , Ho→x and Hx→o . Since (Devlin et al., 2019) due to its excellent ability the relation score can also be binary: 0 or 1, we of textual representation, meanwhile the image re- calculated by equation similarly to equation 5,but gion embeddings are obtained by pre-processing score pr < 0.5 = 0, p < 0.5. Then we try both via ResNet (He et al., 2016) due to its excellent soft and hard relations to guide our multi-modal ability of visual representation. joint tasks. 4397 BERT Embedding Crossmodal Attenion ... Multi-aspect Extraction H ak ... T3  T2  T1  H a3 H o Tk Trm Trm ... Trm Trm Trm ... Trm E1 E2 ... Ek H u1  H s2  u1,1 H s1  u1,"
2021.emnlp-main.360,P19-1051,0,0.30161,"Different from them, we propose to jointly perform aspect terms extraction besides the corresponding sentiment classification in a multi-modal scenario. Note that we also propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Text-based Joint Aspect Terms Extraction and Sentiment Classification. Some studies (Zhang et al., 2020a) have attempted to solve both sub-tasks in a more integrated way, by jointly extracting aspect terms and predicting their sentiment polarities. The most recent and representative are a span-based extract-then-classify approach (Hu et al., 2019) and a directed GCN with syntactic information (Chen et al., 2020). However, all the above studies can not model the visual guidance for both sub-tasks. Different from them, we propose a multi-modal joint framework to handle both MATE and MASC. 3 Joint Multi-modal Aspect-Sentiment Analysis 3.1 Cross-modal Relation Detection Unlike traditional approaches, which take visual information into consideration completely and ignore whether image can bring benefits to text, we incorporate the image-text relation into the model and only retain the auxiliary visual information towards the text. Therefore"
2021.emnlp-main.360,2020.emnlp-main.570,0,0.0372153,"2020a,b) However, all the above studies completely ignore the sentiment polarity analysis dependent on the detected target, which has great facilitates in practical applications, such as e-commerce. Different from them, we propose to jointly perform the corresponding sentiment classification besides aspect terms extraction in a multi-modal scenario. Note that we propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Multi-modal Aspect Sentiment Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulkens and van Cranen"
2021.emnlp-main.360,D19-1468,0,0.0233119,"to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with focus on named entity recognition propose to leverage the whole image information by ResNet encoding to augment each word representation, such as (Moon et al., 2018; Zhang et al., 2018) upon RNN, (Yu et al., 2020b) upon Transformer and (Zhang et al., 2021b) on GNN. Besides, several related studies propose to leveraging the fine-grained visual information by object detection, such as (Wu et al., 2020a,b) However, all the above studies completely ignore the sentiment polarity analysis dependent on the"
2021.emnlp-main.360,P19-1056,0,0.0191113,"odal Aspect Sentiment Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulkens and van Cranen- datasets in Twitter for multi-modal target-oriented burgh, 2020; Akhtar et al., 2020). While, multi- (aka aspect-level) sentiment classification and levermodal target-oriented sentiment analysis has be- age BERT as backbone to effectively combine both come more and more vital because of its urgent textual and visual modalities. In the same period, 4396 Yu et al. (2020a) propose a target-sensitive attention and fusion network to address both text-based and multi-mo"
2021.emnlp-main.360,P19-1344,0,0.0248744,"hes. 2 Related Work need to be applied to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with focus on named entity recognition propose to leverage the whole image information by ResNet encoding to augment each word representation, such as (Moon et al., 2018; Zhang et al., 2018) upon RNN, (Yu et al., 2020b) upon Transformer and (Zhang et al., 2021b) on GNN. Besides, several related studies propose to leveraging the fine-grained visual information by object detection, such as (Wu et al., 2020a,b) However, all the above studies completely ignor"
2021.emnlp-main.360,N18-1078,0,0.0282201,"fication on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with focus on named entity recognition propose to leverage the whole image information by ResNet encoding to augment each word representation, such as (Moon et al., 2018; Zhang et al., 2018) upon RNN, (Yu et al., 2020b) upon Transformer and (Zhang et al., 2021b) on GNN. Besides, several related studies propose to leveraging the fine-grained visual information by object detection, such as (Wu et al., 2020a,b) However, all the above studies completely ignore the sentiment polarity analysis dependent on the detected target, which has great facilitates in practical applications, such as e-commerce. Different from them, we propose to jointly perform the corresponding sentiment classification besides aspect terms extraction in a multi-modal scenario. Note that we p"
2021.emnlp-main.360,2020.aacl-main.33,0,0.0301429,"ection, such as (Wu et al., 2020a,b) However, all the above studies completely ignore the sentiment polarity analysis dependent on the detected target, which has great facilitates in practical applications, such as e-commerce. Different from them, we propose to jointly perform the corresponding sentiment classification besides aspect terms extraction in a multi-modal scenario. Note that we propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Multi-modal Aspect Sentiment Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulke"
2021.emnlp-main.360,P19-1053,0,0.0572799,"Missing"
2021.emnlp-main.360,2020.acl-main.290,0,0.0610892,"Missing"
2021.emnlp-main.360,2020.coling-main.13,0,0.0357248,", all the above studies completely ignore the sentiment polarity analysis dependent on the detected target, which has great facilitates in practical applications, such as e-commerce. Different from them, we propose to jointly perform the corresponding sentiment classification besides aspect terms extraction in a multi-modal scenario. Note that we propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Multi-modal Aspect Sentiment Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulkens and van Cranen- datasets in Twitte"
2021.emnlp-main.360,P19-1272,0,0.0608412,"Missing"
2021.emnlp-main.360,2020.emnlp-main.286,0,0.0998868,"Missing"
2021.emnlp-main.360,2020.acl-main.306,0,0.0900057,"Missing"
2021.emnlp-main.360,2020.emnlp-main.141,0,0.0335747,"s-modal relation detection to control whether the image adds to the text meaning. Second, we leverage the joint hierarchical framework to separately attend to the effective visual information for each sub-task instead of collapsed tagging framework. Finally, we can obtain all potential aspect term-polarity pairs. Extensive experiments and analysis on two multi-modal datasets in Twitter show that our approach performs significantly better than text-based joint approaches and collapsed multi-modal joint approaches. 2 Related Work need to be applied to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the"
2021.emnlp-main.360,2020.findings-emnlp.72,0,0.202055,"ds to the text meaning. Second, we leverage the joint hierarchical framework to separately attend to the effective visual information for each sub-task instead of collapsed tagging framework. Finally, we can obtain all potential aspect term-polarity pairs. Extensive experiments and analysis on two multi-modal datasets in Twitter show that our approach performs significantly better than text-based joint approaches and collapsed multi-modal joint approaches. 2 Related Work need to be applied to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with f"
2021.emnlp-main.360,2020.emnlp-main.291,1,0.842073,"ds to the text meaning. Second, we leverage the joint hierarchical framework to separately attend to the effective visual information for each sub-task instead of collapsed tagging framework. Finally, we can obtain all potential aspect term-polarity pairs. Extensive experiments and analysis on two multi-modal datasets in Twitter show that our approach performs significantly better than text-based joint approaches and collapsed multi-modal joint approaches. 2 Related Work need to be applied to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with f"
2021.emnlp-main.707,P01-1008,0,0.0422567,"e., ciency with augmented data. In summary, we make data augmentation, which addresses both chal- the following contributions. lenges discussed above in a resource-cheap way. • We present a simple and resource-cheap data augThe idea of data augmentation is automatically mentation framework for cross-domain text-togenerating noisy labeled data using some delibSQL parsing with no human intervention.1 erately designed method, and the technique has • As the key component for our framework, we probeen successfully applied to a wide range of NLP pose a hierarchical SQL-to-question generation tasks (Barzilay and McKeown, 2001; Jia and Liang, model to obtain more reliable NL questions. 2016). In our cross-domain text-to-SQL task, we can directly generate labeled data over unseen DBs • In order to improve training efficiency, we proas extra training data. The key of data augmenpose a simple sampling strategy to utilize genertation is how to improve the quality of generated ated data, which is of relatively larger scale than data. As two prior works, Yu et al. (2018a) manuoriginal training data. ally align question tokens and DB elements in the 1 We release the code at https://github.com/ corresponding SQL query, in"
2021.emnlp-main.707,N19-1423,0,0.017965,"r each dataset, the first major row shows previously reported results, and the second major row gives results of our base parsers without and with data augmentation. To compare previous data augmentation methHyper-parameter settings. For each parser, we ods, we also re-implement the flat one-stage genuse default parameter settings in their released code. eration approach (FLAT) proposed by Guo et al. All these parsers are enhanced with vanilla (in (2018). We do not implement the pattern-based contrast to task-specific) pretraining models, i.e., data augmentation approach (PATTERN) of Yu BERT (Devlin et al., 2019), including IRNet-Ext. et al. (2018a) due to its requirement of human inIn order to avoid the effect of performance vitervention. Moreover, their large performance imbrations9 , we run each model for 5 times with provement is obtained over a very weak baseline. 8 The comparison of V2 and V3 is discussed at https: Performance of our baseline parsers. On Wik//github.com/microsoft/rat-sql/issues/12. iSQL, the averaged performance of our SQLova 9 Please see issues proposed at the github of RATSQL parser is lower than their reported performance by model, such as https://github.com/microsoft/ rat-sq"
2021.emnlp-main.707,P17-2090,0,0.0298472,"ta of parsing models. We conduct this experiment augmentation has been widely and successfully on the Spider dataset using IRNet model based on adopted in the computer vision field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge base, semantic parsing aims to generate a"
2021.emnlp-main.707,P16-1154,0,0.0353237,"mainly considers as illustrated by the last two examples in Figure 3. the informativeness aspect of generated NL questions. We leave such evaluation and analysis as From the perspective of SQL syntax, HAVING and GROUP_BY, are naturally bundled together, future work, which will certainly help us better understand our proposed approach. and thus are put into one clause, as shown in the third example of Figure 3. LIMIT and ORDER_BY 2.3 Clause-to-subquestion Translation Model are similarly handled. We adopt the standard Seq2Seq model with From the second perspective, some keywords copy mechanism (Gu et al., 2016) for clause-toare not explicitly expressed in NL questions. In subquestion translation, which is also used in our other words, there is a mismatch between intents expressed in NL questions and the implementa- baseline, i.e., flat SQL-to-question translation, with tion details in SQL queries. To better align them, the same hyper-parameter settings. In the input layer, we represent every SQL towe follow IRNet (Guo et al., 2019) and combine GROUP_BY with either SELECT or ORDER_BY. ken by concatenating two embeddings, i.e., word (token as string) embedding, and token type (colFor a nested SQL quer"
2021.emnlp-main.707,D18-1188,0,0.105257,"generated questions, since very complex questions are rare in the training data. Please kindly note that our simple ASTG-based generation procedure can produce a lot of patterns unseen in the original data, because our generation is at production rule level. This is advantageous from the data variety perspective. Moreover, given a DB, we only keep executable SQL queries for correctness check. 2.2 Hierarchical SQL-to-Question Generation Given an SQL query, especially a complex one, it is difficult to generate an NL question that represents exactly same meaning. In their data augmentation work, Guo et al. (2018) use a vanilla Seq2Seq model to translate SQL queries into NL questions and obtain performance boost on WikiSQL consisting of simple queries. However, as shown in Table 2, we find performance consistently drops on all datasets over our strong baselines, which is largely due to the quality issue of generated NL questions, as illustrated in Table 3. This work proposes a hierarchical SQL-toBeing a program language, all SQL queries can be represented as nested tree structures, as depicted in Figure 2-B according to some context-free grammar. In fact, most text-to-SQL parsers proposed recently adop"
2021.emnlp-main.707,P19-1444,0,0.336098,"ikiSQL consisting of simple queries. However, as shown in Table 2, we find performance consistently drops on all datasets over our strong baselines, which is largely due to the quality issue of generated NL questions, as illustrated in Table 3. This work proposes a hierarchical SQL-toBeing a program language, all SQL queries can be represented as nested tree structures, as depicted in Figure 2-B according to some context-free grammar. In fact, most text-to-SQL parsers proposed recently adopt the abstract syntax tree representation at the decoding stage (Yin and Neubig, 2018; Yu et al., 2018a; Guo et al., 2019; Wang et al., 2020a). Following those works, we design a general ASTG that can cover all SQL patterns in our adopted benchmark datasets. Due to space limita2 As discussed in the logic form-based semantic parsing tion, Figure 2 shows a fraction of the production work of Herzig and Berant (2019), distribution mismatch is rules. mainly caused by insufficient coverage of logical form temAccording to our ASTG, the SQL query in Fig- plates. 8976 question generation model to produce higher- much easier to translate clauses to subquestions quality NL questions. The idea is motivated by compared with"
2021.emnlp-main.707,D19-1394,0,0.0189092,"ing a program language, all SQL queries can be represented as nested tree structures, as depicted in Figure 2-B according to some context-free grammar. In fact, most text-to-SQL parsers proposed recently adopt the abstract syntax tree representation at the decoding stage (Yin and Neubig, 2018; Yu et al., 2018a; Guo et al., 2019; Wang et al., 2020a). Following those works, we design a general ASTG that can cover all SQL patterns in our adopted benchmark datasets. Due to space limita2 As discussed in the logic form-based semantic parsing tion, Figure 2 shows a fraction of the production work of Herzig and Berant (2019), distribution mismatch is rules. mainly caused by insufficient coverage of logical form temAccording to our ASTG, the SQL query in Fig- plates. 8976 question generation model to produce higher- much easier to translate clauses to subquestions quality NL questions. The idea is motivated by compared with direct SQL-to-question translation. our observation that there is a strong segment-level We use a standard copy-based Seq2Seq model (Gu mapping between SQL queries and corresponding et al., 2016) for clause-to-subquestion generation. questions, as shown in Figure 3. For example, the The details"
2021.emnlp-main.707,2020.acl-main.677,0,0.257833,"uSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement. Figure 1: An example of the text-to-SQL parsing task. where all question/SQL pairs of train/dev/test sets are generated against the same DB. In order to deal with the more realistic setting where DBs in the evaluation phase are unseen in the training data, researchers propose several cross-domain datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) in English, and DuSQL (Wang et al., 2020b) in Chinese. All three datasets adopt the DB-level data splitting, meaning that a DB and all its corresponding question/SQL pairs can appear in only one of the train/dev/test sets. Cross-domain text-to-SQL parsing has two major challenges. First, unseen DBs usually introduce 1 Introduction new schemas, such as new table/column names and unknown semantics of inter-table relationships. Given a natural language (NL) question and a relaTherefore, it is crucial for a parsing model to have tional database (DB), the text-to-SQL parsing task strong generalization ability. The second challenge aims t"
2021.emnlp-main.707,2020.emnlp-main.562,1,0.725224,"uSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement. Figure 1: An example of the text-to-SQL parsing task. where all question/SQL pairs of train/dev/test sets are generated against the same DB. In order to deal with the more realistic setting where DBs in the evaluation phase are unseen in the training data, researchers propose several cross-domain datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) in English, and DuSQL (Wang et al., 2020b) in Chinese. All three datasets adopt the DB-level data splitting, meaning that a DB and all its corresponding question/SQL pairs can appear in only one of the train/dev/test sets. Cross-domain text-to-SQL parsing has two major challenges. First, unseen DBs usually introduce 1 Introduction new schemas, such as new table/column names and unknown semantics of inter-table relationships. Given a natural language (NL) question and a relaTherefore, it is crucial for a parsing model to have tional database (DB), the text-to-SQL parsing task strong generalization ability. The second challenge aims t"
2021.emnlp-main.707,2020.acl-main.398,0,0.0547679,"Missing"
2021.emnlp-main.707,D15-1306,0,0.0153771,"sion field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge base, semantic parsing aims to generate a semantically equivalent formal representation, such as SQL query, logic form (LF), or task-oriented dialogue slots. Based on LF-based representation, Jia and Liang (2016"
2021.emnlp-main.707,C18-1105,0,0.102154,"ne the corresponding subquestion as the shortest question segment that contains all DB elements in the clause. Finally, we discard low-confidence clause/subquestion pairs to reduce noises, such as subquestions having large overlap with others. We keep overlapping subquestions, unless one subquestion fully contains another. In that case, we only keep the shorter subquestion. We find that a portion of collected clauses have multiple subquestion translations. For example, the clause “ORDER_BY age ASC” are translated as both “in ascending order of the age” and “from youngest to oldest”. We follow Hou et al. (2018) and use them as two independent clause/subquestion pairs for training. 2.4 Three Strategies for Utilizing Generated Data Given a set of DBs, the generated question/SQL pairs are usually of larger scale than the original training data (see Table 1), which may greatly increase training time. In this work, we compare the following three strategies for parser training. • The pre-training strategy first pre-trains the model with only generated data, and then finetunes the model with labeled training data. • The directly merging strategy trains the model with all generated data and labeled training"
2021.emnlp-main.707,D19-1670,0,0.0162761,"ve way the number of augmented pairs affects the accuracy to address the sparseness of labeled data, data of parsing models. We conduct this experiment augmentation has been widely and successfully on the Spider dataset using IRNet model based on adopted in the computer vision field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentat"
2021.emnlp-main.707,P17-1089,0,0.156978,"ion ability. The second challenge aims to produce a legal and executable SQL query is that the scale of labeled data is quite small for to get the correct answer (Date and Darwen, 1997), such a complex task, since it is extremely diffias depicted in Figure 1. A DB usually consists of cult to construct DBs and manually annotate cormultiple tables interconnected via foreign keys. responding question/SQL pairs. For example, the Early research on text-to-SQL parsing mainly Spider dataset has only 200 DBs and 10K quesfocuses on the in-domain setting (Li and Jagadish, tion/SQL pairs in total. 2014; Iyer et al., 2017; Yaghmazadeh et al., 2017), To deal with the first challenge, many previous * Work done during an internship at Baidu Inc. works focus on how to better encode the matching 8974 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8974–8983 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 2: An overview of our approach containing 3 stages: SQL query generation based on ASTG (Section §2.1), question hierarchical generation according to SQL structure (Section §2.2), model training via data augmentation (Section §2.4). among que"
2021.emnlp-main.707,P16-1002,0,0.0206393,"t al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge base, semantic parsing aims to generate a semantically equivalent formal representation, such as SQL query, logic form (LF), or task-oriented dialogue slots. Based on LF-based representation, Jia and Liang (2016) train a synchronous"
2021.emnlp-main.707,D18-1545,0,0.0126,"ess the sparseness of labeled data, data of parsing models. We conduct this experiment augmentation has been widely and successfully on the Spider dataset using IRNet model based on adopted in the computer vision field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge"
2021.emnlp-main.707,2020.acl-main.745,0,0.0314504,"Missing"
2021.emnlp-main.707,D18-1193,0,0.351191,".e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement. Figure 1: An example of the text-to-SQL parsing task. where all question/SQL pairs of train/dev/test sets are generated against the same DB. In order to deal with the more realistic setting where DBs in the evaluation phase are unseen in the training data, researchers propose several cross-domain datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) in English, and DuSQL (Wang et al., 2020b) in Chinese. All three datasets adopt the DB-level data splitting, meaning that a DB and all its corresponding question/SQL pairs can appear in only one of the train/dev/test sets. Cross-domain text-to-SQL parsing has two major challenges. First, unseen DBs usually introduce 1 Introduction new schemas, such as new table/column names and unknown semantics of inter-table relationships. Given a natural language (NL) question and a relaTherefore, it is crucial for a parsing model to have tional database (DB), the text-to-SQL parsing task strong generali"
2021.emnlp-main.707,D18-1425,0,0.0470955,"Missing"
2021.findings-acl.260,W13-2233,0,0.0279993,"riments show that our mechanism consistently improves performances over robust BLI baselines on all language pairs by averagely improving 3.2 points in the supervised setting, and 3.1 points in the unsupervised setting1 . 1 Introduction Bilingual Lexicon Induction (BLI) aims to find bilingual translation lexicons from monolingual corpora in two languages (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Conneau et al., 2017), and is applied on numerous NLP tasks such as POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018). Most work on BLI learns a mapping between two static word embedding spaces, which are pretrained on large monolingual corpora (Ruder et al., 2019). Both linear mapping (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017) and non-linear mapping (Mohiuddin et al., ∗ 1 Corresponding Author. Code is released at https://github.com/zjpbinary/CSCBLI 2020) methods have been studied to align the two spaces. Recently, other than the static word embeddings, contextual representations are used for BLI due to the significant progresses on cross-lingual app"
2021.findings-acl.260,N18-2084,0,0.0288025,"onsistently improves performances over robust BLI baselines on all language pairs by averagely improving 3.2 points in the supervised setting, and 3.1 points in the unsupervised setting1 . 1 Introduction Bilingual Lexicon Induction (BLI) aims to find bilingual translation lexicons from monolingual corpora in two languages (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Conneau et al., 2017), and is applied on numerous NLP tasks such as POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018). Most work on BLI learns a mapping between two static word embedding spaces, which are pretrained on large monolingual corpora (Ruder et al., 2019). Both linear mapping (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017) and non-linear mapping (Mohiuddin et al., ∗ 1 Corresponding Author. Code is released at https://github.com/zjpbinary/CSCBLI 2020) methods have been studied to align the two spaces. Recently, other than the static word embeddings, contextual representations are used for BLI due to the significant progresses on cross-lingual applications (Aldarma"
2021.findings-acl.260,D18-1330,0,0.254928,"tic word embeddings come from the same data. We use the bilingual dictionaries released by Muse project5 in our experiments. Note that some words in these dictionaries do not necessarily appear in our monolingual corpora, we have to recompose the training, validation, and test sets such that all words in these sets are included in our monolingual corpora. In the end, we have 5000 entries with unique source words in the training set, and 1500 entries with unique source words in both the validation set and the test set for all language pairs. • RCSLS7 : In addition to use CSLS during inference, Joulin et al. (2018) minimize a convex relaxation of CSLS loss during training, and improve the supervised BLI performance. 2 https://github.com/attardi/wikiextractor https://github.com/facebookresearch/xlm. We use the MLM model of 15 languages with tokenize + lowercase + no accent + BPE. 4 https://github.com/facebookresearch/fastText/ 5 https://github.com/facebookresearch/MUSE 6 https://github.com/artetxem/vecmap 7 https://github.com/facebookresearch/ fastText/tree/master/alignment 2947 3 EN-ES → ← EN-AR → ← EN-ZH → ← EN-DE → ← EN-FR → ← avg Muse(Conneau et al., 2017) VecMap(Artetxe et al., 2018a) RCSLS(Joulin e"
2021.findings-acl.260,P99-1067,0,0.518758,"On BLI benchmark sets of multiple language pairs, our combination mechanism performs significantly better than systems using only the static word embeddings and systems using only the contextual representations. Our mechanism improves over robust BLI baselines on all language pairs, achieving average 3.2 points improvement in the supervised setting, and average 3.1 points improvement in the unsupervised setting. 2 Background The early works on Bilingual Lexicon Induction (BLI) date back to several decades ago, including feature-based retrieval (Fung and Yee, 1998), distributional hypothesis (Rapp, 1999; Vuli´c and Moens, 2013), and decipherment (Ravi and Knight, 2011). Following Mikolov et al. (2013), which pioneered the embedding based BLI method, word representation based method becomes the dominant approach, and can be categorized into two classes: static word embedding based method, and contextual representation based method. • Static Word Embedding Based Method Word embeddings of different languages are pre-trained in large monolingual corpora independently. Then a mapping function is applied to align the embedding spaces of the two languages (Mikolov et al., 2013; Xing et al., 2015; A"
2021.findings-acl.260,P07-2045,0,0.0100519,"ive Procrustes alignment for supervised BLI. • VecMap6 : Artetxe et al. (2018a) use a multistep framework consisting of several steps: whitening, orthogonal mapping, re-weighting, de-whitening, and dimensionality reduction. Experiments We test our combination mechanism in supervised and unsupervised BLI tasks on EnglishEspanish (EN-ES), English-Arabic (EN-AR), English-Chinese (EN-ZH), English-German (ENDE), and English-French (EN-FR). 4.1 to train them. For that reason, we use WikiExtractor2 to extract plain text from Wikipedia dumps, and preprocess the resulting corpora using standard Moses (Koehn et al., 2007) tools by applying sentence splitting, punctuation normalization, tokenization, and lowercasing. On these corpora, we use the cross-lingual pre-training system XLM (Lample and Conneau, 2019)3 to compute the contextual representations. Meanwhile, we also use these corpora to train the static word embeddings by using fastText4 to ensure that both the contextual representations and the static word embeddings come from the same data. We use the bilingual dictionaries released by Muse project5 in our experiments. Note that some words in these dictionaries do not necessarily appear in our monolingua"
2021.findings-acl.260,2020.findings-emnlp.83,0,0.0221593,"try assumption by combining the contextual representations with the word embeddings to compensate the shortage of the overly strong assumption. • Contextual Representation Based Method Contextual representations can be obtained through multilingual pre-training, which encodes whole sentence and outputs contextual representation for each word (Devlin et al., 2019; Lample and Conneau, 2019). Due to the rich context information contained in the contextual representations, there are endeavors to align them in different languages (Schuster et al., 2019; Aldarmaki and Diab, 2019; Wang et al., 2020; Kulshreshtha et al., 2020; Cao et al., 2020). Since a word may appear in different sentences with different contexts, Schuster et al. (2019) use an average anchor to summarize multiple contexts for a word type and align the anchors of different languages, while other works aim to align each individual context representation based on parallel corpora, including learning alignment on sentence level representations and applying the learned mapping on word level contextual representations (Aldarmaki and Diab, 2019), using word alignments in a parallel corpora to learn the mapping for word contextual representations (Wang"
2021.findings-acl.260,2020.tacl-1.47,0,0.0170673,"es, it is complementary to “Unified”. When “Contextual” is combined with “Unified” through the interpolation, the performance is further improved, achieving the best performance among all systems. It shows that our combination mechanism is effective to utilize the merits of both the static word embeddings and the contextual representations. In the unsupervised task, we achieve the significant improvements over the baselines. “Unified” is Analyses 4.5.1 XLM v.s. mBART Our results in Table 1 are based on using XLM for obtaining the contextual representations. In this section, we also use mBART (Liu et al., 2020) to compare with XLM. Table 2 shows the comparison result. XLM pre-trains the Transformer encoder through the masking mechanism, while mBART pre-trains the full Transformer encoderdecoder through multilingual denoising. Regarding BLI task, we obtain the contextual representations from the encoder. Table 2 shows that XLM and mBART get similar BLI performances since only encoder is used. In some directions, mBART performs slightly better than XLM, while in other directions, XLM is slightly better. According to the average performance, XLM ties with mBART in the supervised task, and is slightly b"
2021.findings-acl.260,2020.emnlp-main.215,0,0.0131184,"h mapping functions are constrained to be orthogonal during training by setting Wx = U and Wy = V , where U ΣV T = X T Y is the singular value decomposition of X T Y . Such orthogonal constraint is based on the assumption that the source embedding space and the target embedding space are isometric, which is a particularly strong assumption that does not hold in all conditions (Zhang et al., 2017b; Søgaard et al., 2018). To depart from the isometry assumption, Patra et al. (2019) uses a semi-supervised technique that leverages both seed dictionary and a larger set of unaligned word embeddings, Mohiuddin et al. (2020) uses a non-linear mapping function that is not constrained to be orthogonal. We propose another method to relax the isometry assumption by combining the contextual representations with the word embeddings to compensate the shortage of the overly strong assumption. • Contextual Representation Based Method Contextual representations can be obtained through multilingual pre-training, which encodes whole sentence and outputs contextual representation for each word (Devlin et al., 2019; Lample and Conneau, 2019). Due to the rich context information contained in the contextual representations, ther"
2021.findings-acl.260,N19-1386,0,0.0160062,"“VecMap” (Artetxe et al., 2018b). • BLISS8 : Patra et al. (2019) use a semisupervised method that leverages both the bilingual dictionary and a larger set of unaligned word embeddings. Unsupervised BLI task, which is not allowed to use any parallel resources for training and validation. The baseline systems are: • Muse: Unsupervised Muse (Conneau et al., 2017) uses adversarial training and iterative Procrustes refinement. • VecMap: Artetxe et al. (2018b) use careful initialization, robust self-learning procedure, and symmetric re-weighting to improve the unsupervised mapping result. • Ad.9 : Mohiuddin and Joty (2019) include regularization terms for adversarial auto-encoder for the unsupervised BLI. 4.3 Experimental Settings We use fastText to train the word embeddings for BLI. The dimension of the word embeddings is 300. The contextual representations are extracted from XLM, and the dimension of the contextual representations is 1024. For each word type, we randomly select ten sentences containing the word from the monolingual corpora to do the averaging 8 9 https://github.com/joelmoniz/BLISS https://github.com/taasnim/unsup-word-translation/ to get the contextual representation. The influence of the num"
2021.findings-acl.260,P19-1018,0,0.0681192,"dictionary 2944 is initialized and iteratively updated through training Wx and Wy according to equation (1) (Artetxe et al., 2018b). Both mapping functions are constrained to be orthogonal during training by setting Wx = U and Wy = V , where U ΣV T = X T Y is the singular value decomposition of X T Y . Such orthogonal constraint is based on the assumption that the source embedding space and the target embedding space are isometric, which is a particularly strong assumption that does not hold in all conditions (Zhang et al., 2017b; Søgaard et al., 2018). To depart from the isometry assumption, Patra et al. (2019) uses a semi-supervised technique that leverages both seed dictionary and a larger set of unaligned word embeddings, Mohiuddin et al. (2020) uses a non-linear mapping function that is not constrained to be orthogonal. We propose another method to relax the isometry assumption by combining the contextual representations with the word embeddings to compensate the shortage of the overly strong assumption. • Contextual Representation Based Method Contextual representations can be obtained through multilingual pre-training, which encodes whole sentence and outputs contextual representation for each"
2021.findings-acl.260,P11-1002,0,0.0375065,"combination mechanism performs significantly better than systems using only the static word embeddings and systems using only the contextual representations. Our mechanism improves over robust BLI baselines on all language pairs, achieving average 3.2 points improvement in the supervised setting, and average 3.1 points improvement in the unsupervised setting. 2 Background The early works on Bilingual Lexicon Induction (BLI) date back to several decades ago, including feature-based retrieval (Fung and Yee, 1998), distributional hypothesis (Rapp, 1999; Vuli´c and Moens, 2013), and decipherment (Ravi and Knight, 2011). Following Mikolov et al. (2013), which pioneered the embedding based BLI method, word representation based method becomes the dominant approach, and can be categorized into two classes: static word embedding based method, and contextual representation based method. • Static Word Embedding Based Method Word embeddings of different languages are pre-trained in large monolingual corpora independently. Then a mapping function is applied to align the embedding spaces of the two languages (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017). We follow one robust BLI"
2021.findings-acl.260,N19-1162,0,0.207856,"learns a mapping between two static word embedding spaces, which are pretrained on large monolingual corpora (Ruder et al., 2019). Both linear mapping (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017) and non-linear mapping (Mohiuddin et al., ∗ 1 Corresponding Author. Code is released at https://github.com/zjpbinary/CSCBLI 2020) methods have been studied to align the two spaces. Recently, other than the static word embeddings, contextual representations are used for BLI due to the significant progresses on cross-lingual applications (Aldarmaki and Diab, 2019; Schuster et al., 2019). Although the static word embeddings and the contextual representations exhibit properties suited for alignment, there is no works to combine the two paradigms. On one hand, the static word embeddings have been widely used for BLI, but one specific embedding mapping function does not ensure that in all conditions, words in a translation pair are nearest neighbors in the mapped common space. On the other hand, the contextual representations contain rich semantic information beneficial for alignment, but the dynamic contexts of word tokens pose a challenge for aligning word types. In this paper"
2021.findings-acl.260,P16-1162,0,0.0327078,"xtual representation d0 to the dimension of the word embedding d. Equations (4-5) list the network structure of both sides. A1x = φ(θx0 (Ax )), A1y = φ(θy0 (Ay )) (4) A2x A2y (5) = φ(θx1 (A1x )), = φ(θy1 (A1y )) where φ denotes the Tanh activation, and θ denotes the feedforward layer. A2x/y is the output of the spring network, and fulfills as the offset distance to compensate the deviation of words in each translation pair in the mapped word embedding space. Since we use cross-lingual pre-training (Lample and Conneau, 2019) to generate the contextual representations, which are actually BPE’s (Sennrich et al., 2016) contextual representations, we have to form the contextual representations in the word level. Suppose a word x has q BPEs, 2946 • In the supervised contrastive training, given a bilingual dictionary with I translation pairs, the contrastive loss is: Lsup I X =− (J × cos(uix , uiy ) − i=1 J X cos(uix , ujy¯)) (6) j=1 where uix and uiy are the unified representations corresponding to the ith entry of the given bilingual dictionary. In equation (6), (uix , uiy ) is the positive translation pair according to the given dictionary, and the cosine similarity of this pair is maximized during training"
2021.findings-acl.260,2021.acl-long.67,0,0.0429294,"Missing"
2021.findings-acl.260,P18-1072,0,0.0426031,"Missing"
2021.findings-acl.260,D13-1168,0,0.0703757,"Missing"
2021.findings-acl.260,W14-1613,0,0.032827,"and unsupervised BLI benchmark settings. Experiments show that our mechanism consistently improves performances over robust BLI baselines on all language pairs by averagely improving 3.2 points in the supervised setting, and 3.1 points in the unsupervised setting1 . 1 Introduction Bilingual Lexicon Induction (BLI) aims to find bilingual translation lexicons from monolingual corpora in two languages (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Conneau et al., 2017), and is applied on numerous NLP tasks such as POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018). Most work on BLI learns a mapping between two static word embedding spaces, which are pretrained on large monolingual corpora (Ruder et al., 2019). Both linear mapping (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017) and non-linear mapping (Mohiuddin et al., ∗ 1 Corresponding Author. Code is released at https://github.com/zjpbinary/CSCBLI 2020) methods have been studied to align the two spaces. Recently, other than the static word embeddings, contextual representations are used for"
2021.findings-acl.260,N15-1104,0,0.0866738,"Missing"
2021.findings-acl.260,P17-1179,0,0.137555,"e contextual representations to utilize the advantages of both paradigms. We test the combination mechanism on various language pairs under the supervised and unsupervised BLI benchmark settings. Experiments show that our mechanism consistently improves performances over robust BLI baselines on all language pairs by averagely improving 3.2 points in the supervised setting, and 3.1 points in the unsupervised setting1 . 1 Introduction Bilingual Lexicon Induction (BLI) aims to find bilingual translation lexicons from monolingual corpora in two languages (Haghighi et al., 2008; Xing et al., 2015; Zhang et al., 2017a; Artetxe et al., 2017; Conneau et al., 2017), and is applied on numerous NLP tasks such as POS tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014), and machine translation (Irvine and Callison-Burch, 2013; Qi et al., 2018). Most work on BLI learns a mapping between two static word embedding spaces, which are pretrained on large monolingual corpora (Ruder et al., 2019). Both linear mapping (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017) and non-linear mapping (Mohiuddin et al., ∗ 1 Corresponding Author. Code is released at https://github.com/zjpbinar"
2021.findings-acl.93,2020.acl-main.449,0,0.368694,"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1078–1090 August 1–6, 2021. ©2021 Association for Computational Linguistics Structuresensitive LSTM Tree-LSTM Transformer LSTM + SBT Transformer + SBT SiT Long-term dependency Feat-model match X X X X X X X X X Table 2: Comparison of the previous models with proposed SiT model. The last column refers to whether input features match with the corresponding model. sequences due to its poor long-term dependency. For instance, a normal snippet of Java as shown in Table 1 usually has hundreds of tokens. More recently, Ahmad et al. (2020) used an enhanced Transformer-based model to capture long-term and non-sequential information of source code, which outperformed previous RNN-based models by a large margin. On the other hand, in the light of the structural nature of programming languages, structure clues are supposed to greatly enhance programming language processing task like code summarization (Fernandes et al., 2019). Indeed, substantial empirical studies showed that Abstract Syntax Tree may help models better comprehend code snippets and achieve more sensible generation results. Previous approaches could be divided into t"
2021.findings-acl.93,2020.emnlp-main.19,0,0.0703139,"Missing"
2021.findings-acl.93,P19-1285,0,0.0136142,"t al. (2020) introduced type information to assist training, which also gained promising results. Additionally, reinforce learning (Wan et al., 2018) and dual learning (Wei et al., 2019; Ye et al., 2020) are also shown effective to boost model performance. Transformer-based Approaches It is known that RNN-based models may encounter bottleneck when modeling long code sequences. Ahmad et al. (2020) proposed an enhanced Transformer with copy attention and relative position encoding while Gupta (2020); Dowdell and Zhang (2020) proposed to use Transformer (Vaswani et al., 2017) and Transformer-XL (Dai et al., 2019), all of which outperformed previous RNN-based models by a large margin. 1085 Structure-based Approaches Recent works on code summarization pay more and more attention on structural information, which usually treats the source code in form of its Abstract Syntax Tree (AST). Hu et al. (2018a); LeClair et al. (2019); Uri et al. (2019) leveraged flattened ASTs as inputs and trained with LSTMs. Mou et al. (2016); Bui et al. (2021a); Shido et al. (2019); Harer et al. (2019) proposed TBCNN, TreeCaps, Tree-LSTM and Tree-Transformer to directly encode tree-style inputs. Differ from modeling code with"
2021.findings-acl.93,N19-1423,0,0.298404,"ingasan/astruc We try to adjust the weights of three views, showing little performance variant, which suggests that self-attention network itself may balance the relative significance between the three. 2 (b) Window (c) Random (d) Structure-induced Figure 5: Comparison of different types of selfattention pattern. (b) Window attention with w = 2. (c) Random attention with r = 2. Pre-trained Language Model We also compare our model with CodeBERT (Feng et al., 2020), a pre-trained language model on both natural and programming languages. It is pre-trained over six programming languages with MLM (Devlin et al., 2019) and RTD (Clark et al., 2020). 3.3 Baselines (a) Full Training Details We train our model on a single nVidia Titan RTX with batch size in {32, 64}. The learning rate is in {3e-5, 5e-5} with warm-up rate of 0.06 and L2 weight decay of 0.01. The maximum number of epochs is set to 150 for Transformer and 30 for CodeBERT. For validation, we simply use greedy search, while for evaluation, we use beam search with beam size in {4, 5, 8} and choose the best result3 . 3.4 Main Results Scores Table 3 shows the overall results on Java and Python benchmarks. The Transformer baseline is strong enough as it"
2021.findings-acl.93,P16-1195,0,0.314901,"ional Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. In early days, code summarization was a derivative problem of information retrieval (Haiduc et al., 2010; Eddy et al., 2013; Wong et al., 2013, 2015) by matching the most similar code snippets which are labeled with summaries. Such method lacks generalization and performs unsatisfactorily. Thus in recent years, researchers treated code summarization as a task of language generation (Iyer et al., 2016; Liang and Zhu, 2018), which usually depends on RNN-based Seq2Seq models (Cho et al., 2014; Bahdanau et al., 2015). It is already known that RNN-based models may encounter bottleneck when modeling long 1078 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1078–1090 August 1–6, 2021. ©2021 Association for Computational Linguistics Structuresensitive LSTM Tree-LSTM Transformer LSTM + SBT Transformer + SBT SiT Long-term dependency Feat-model match X X X X X X X X X Table 2: Comparison of the previous models with proposed SiT model. The last column refers to wheth"
2021.findings-acl.93,P16-1078,0,0.112802,"robustness and avoid over-pruning, we introduce structure-induced module, which is a stack of two layers, SAN and Si-SAN. In each module, SAN is followed by Si-SAN and the output is the combination of both layers. Specifically, given input sequence X = (x1 , . . . , xl ), where l denotes sequence length, we first pass it through an SAN layer to obtain hidden representation denoted as H = (h1 , . . . , hl ): H = Concat(SAN1 (X), . . . , SANh (X)) (4) where h refers to number of heads of multi-head attention while SANi refers to self-attention of 1081 Model CODE-NN (Iyer et al., 2016) Tree2Seq (Eriguchi et al., 2016) Hybrid2Seq (Wan et al., 2018) DeepCom (Hu et al., 2018a) API + Code (Hu et al., 2018b) Dual Model (Wei et al., 2019) Transformer (Ahmad et al., 2020) Transformer∗ (Ahmad et al., 2020) SiT CodeBERT∗ † (Feng et al., 2020) SiT on CodeBERT† BLEU 27.60 37.88 38.22 39.75 41.31 42.39 44.58 44.87 45.76(↑1.18) 43.33 45.19(↑0.61) Java ROUGE-L 41.10 51.50 51.91 52.67 52.25 53.61 54.76 54.95 55.58(↑0.82) 54.64 55.87(↑1.11) METEOR 12.61 22.55 22.75 23.06 23.73 25.77 26.43 26.58 27.58(↑1.15) 26.20 27.52(↑1.09) BLEU 17.36 20.07 19.28 20.78 15.36 21.80 32.52 32.85 34.11(↑1.59) 33.47 34.31(↑1.79) Python ROUGE"
2021.findings-acl.93,2021.ccl-1.108,0,0.0217344,"Missing"
2021.findings-acl.93,2020.emnlp-main.339,0,0.0608038,"Missing"
2021.findings-acl.93,P17-1099,0,0.0272805,"er vocabulary than natural language, including vast operators and identifiers. We have to introduce vast out-of-vocabulary (OOV) tokens (usually replaced by hUNKi) (Hu et al., 2018a) to keep it in a regular size. To avoid OVV problem, we apply CamelCase and snake case tokenizers (Ahmad et al., 2020) to reduce code vocabulary and remove all extra nodes which do not correspond to specific tokens. 3.2 We take all three categories of state-of-the-art models as our baselines for comparison. Transformer We refer to the enhanced Transformer in (Ahmad et al., 2020) which equipped with copy attention (See et al., 2017) and relative position encoding (RPE) (Shaw et al., 2018). For fair enough comparison, we run their model on our machine under the same environment with SiT. Note that we also utilize RPE in SiT because of its better capability in capturing long sequences, while we do not utilize copy attention. LSTM This group includes all relevant LSTM models with sequential and non-sequential inputs (Iyer et al., 2016; Eriguchi et al., 2016; Wan et al., 2018; Hu et al., 2018a,b; Wei et al., 2019). 1 https://github.com/gingasan/astruc We try to adjust the weights of three views, showing little performance va"
2021.findings-acl.93,N18-2074,0,0.0226337,"ators and identifiers. We have to introduce vast out-of-vocabulary (OOV) tokens (usually replaced by hUNKi) (Hu et al., 2018a) to keep it in a regular size. To avoid OVV problem, we apply CamelCase and snake case tokenizers (Ahmad et al., 2020) to reduce code vocabulary and remove all extra nodes which do not correspond to specific tokens. 3.2 We take all three categories of state-of-the-art models as our baselines for comparison. Transformer We refer to the enhanced Transformer in (Ahmad et al., 2020) which equipped with copy attention (See et al., 2017) and relative position encoding (RPE) (Shaw et al., 2018). For fair enough comparison, we run their model on our machine under the same environment with SiT. Note that we also utilize RPE in SiT because of its better capability in capturing long sequences, while we do not utilize copy attention. LSTM This group includes all relevant LSTM models with sequential and non-sequential inputs (Iyer et al., 2016; Eriguchi et al., 2016; Wan et al., 2018; Hu et al., 2018a,b; Wei et al., 2019). 1 https://github.com/gingasan/astruc We try to adjust the weights of three views, showing little performance variant, which suggests that self-attention network itself"
2021.findings-acl.93,2020.acl-main.451,0,0.025819,"h is different from CPG. which is original for C/C++ only and we do not find an appropriate analysis platform for other languages. 1 Expression Num 2.2 print(b) Structure-induced Transformer Call print Name b Name Figure 2: A Python code sample of multi-view graph used in Si-SAN. The code snippet is referred from Liu et al. (2020), which is original in Java. Followed by appropriate structure representation and graph construction, we now propose our structure-induced Transformer (SiT) for code summarization, which is a structure-sensitive transformer (Zhang et al., 2020b; Narayan et al., 2020; Xu et al., 2020) model and is able to comprehend code snippets both semantically and syntac1080 ×n Value Transformer Decoder Attention Query Value Code Attention Query + Key Key + Summary Figure 3: Overall architecture of Structure-induced Transformer (SiT). tically. Meanwhile, we do not introduce extra parameters in SiT so that guarantee the training efficiency. In this section, we first review the selfattention network (SAN) of Transformer in terms of attention graph. Then we correspondingly propose structure-induced self-attention to build the structure-induced Transformer. Vanilla Self-Attention Transform"
2021.findings-emnlp.149,D19-1188,0,0.0211495,", achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Recently, a number of works attempt to use the parameter generation network to improve the cross-domain or cross-language performance (Cai et al., 2019; Stoica et al., 2020; Jin et al., 2020; Nekvinda and Dusek, 2020). Particularly, Jia et al. (2019) propose to generate BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. D"
2021.findings-emnlp.149,D14-1082,0,0.188043,"Missing"
2021.findings-emnlp.149,D18-1217,0,0.0295431,"ark for the English language. In order to obtain competitive performance, supervised dependency parsing models rely on a sufficient amount of training data, which is inevitably dominated to several fixed domains. When the test data is sourced from similar domains, good performance could be achieved. However, the performance could be decreased significantly when the test data is from a different domain which has a large gap between the training domains. Thus domain adaptation for dependency parsing has been concerned by a number of studies (Koo et al., 2008; Yu et al., 2013; Sato et al., 2017; Clark et al., 2018; Li et al., 2020b). These works mostly focus on single-source cross-domain dependency parsing, 1 Introduction assuming the training data is from a single source Dependency parsing aims to derive syntactic and domain (Yu et al., 2013; Sato et al., 2017). In fact, semantic tree structures over input words (Mc- multi-source cross-domain dependency parsing is a Donald et al., 2013). Given an input sentence more practical setting, considering that several des = w1 w2 . . . wn , a dependency tree, as depicted pendency parsing corpora from different domains in Figure 1, is defined as d = {(h, m, l),"
2021.findings-emnlp.149,P07-1033,0,0.407388,"Missing"
2021.findings-emnlp.149,D18-1498,0,0.02412,"target-domain training data is small. Most recently, Li et al. (2019b) propose to leverage an extra domain embedding to indicate domain source and achieve better performance on semi-supervised domain adaptation. In this work, we adjust the domain embedding method as our strong baseline. Multi-source domain adaptation. Multisource domain adaptation assumes the training data comes from multiple source domains. Many approaches of multi-source domain adaptation focus on leveraging domain knowledge to extract domainrelated features, thus boosting the performance of target domain (Daumé III, 2007; Guo et al., 2018; Li et al., 2020a; Wright and Augenstein, 2020). Zeng et al. (2018) design a domain classifier and an adversarial network to capture domain-specific and domain-invariant features, achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the sour"
2021.findings-emnlp.149,W09-1201,0,0.0382346,"rce domain which is a balanced corpus (BC) from news-wire, three target domains which are the product comments (PC) data from Taobao, the product blog (PB) data from Taobao headline, and a web fiction data named “ZhuXian” (ZX). Table 1 shows the detailed illustration of the data statistics. In this work, we pick one target dataset as the target domain, and the rest are the source domains. For example, if the target domain is PC, source domains are BC, PB, and ZX. Evaluation. We use unlabeled attachment score (UAS) and labeled attachment score (LAS) to evaluate the dependency parsing accuracy (Hajic et al., 2009). Each model is trained for at most 1, 000 iterations, and the performance is evaluated on the dev data after each iteration for model selection. We stop the training if the peak performance does not increase in 100 consecutive iterations. Baseline models. To verify the effectiveness and advantage of our proposed model, we select the following approaches as our strong baselines. 1 http://hlt.suda.edu.cn/index.php/ Nlpcc-2019-shared-task • Parameter generation network (PGN). Motivated by Jia et al. (2019), we exploit the PGN based on distributed domain representations to generate domain-related"
2021.findings-emnlp.149,P19-1236,0,0.37234,"n , E) n 0 = BiLSTM (x0 . . . xn , V = W ⊗ E) (6) where ⊗ denotes matrix multiplication; W ∈ RU×D is a parameter matrix to be trained; E ∈ RD is distributed domain-aware sentence representation vector and will be explained later. Distributed domain-aware sentence representation. The distributed domain-aware sentence representation vector can be regarded as a sum of weighted domain embeddings, where higher weights are expected to be assigned to domains that are more similar to the input sentence. First, we compute domain distribution probabilities of each word via simple domain classification. Jia et al. (2019) first propose PGN to generate BiLSTM parameters based on fixed task and domain embeddings for NER domain adaptation, finding  that the PGN can effectively extract domain differzi = softmax MLP hdom (7) i ences. However, the vanilla PGN requires crosswhere hdom is the representation vector of the i-th domain language model task as a bridge to help i word generated by a separated standard BiLSTM. fixed domain embeddings training. Considering the development of pre-training techniques with Then, we compute a distributed domain-aware language model loss and computational complex- word represent"
2021.findings-emnlp.149,2020.acl-main.625,0,0.0360117,"e translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Recently, a number of works attempt to use the parameter generation network to improve the cross-domain or cross-language performance (Cai et al., 2019; Stoica et al., 2020; Jin et al., 2020; Nekvinda and Dusek, 2020). Particularly, Jia et al. (2019) propose to generate BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. Domain adaptation has been extensively s"
2021.findings-emnlp.149,P17-1060,0,0.0166215,"te BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. Domain adaptation has been extensively studied in many research areas, including machine learning (Wang et al., 2017; Kim et al., 2017), computer vision (Ganin and Lempitsky, 2015; Rozantsev et al., 2019) and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (C"
2021.findings-emnlp.149,C16-1038,0,0.0166719,"main NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. Domain adaptation has been extensively studied in many research areas, including machine learning (Wang et al., 2017; Kim et al., 2017), computer vision (Ganin and Lempitsky, 2015; Rozantsev et al., 2019) and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (Charniak, 1997; Steedman et al., 2003; Reichart and Rappoport, 2007; Yu et al., 2015), co-training (Sarkar, 2001), or tr"
2021.findings-emnlp.149,Q16-1023,0,0.0267691,"eveloped (Peng et al., 2019). Intuitively, n, 1 ≤ m ≤ n, l ∈ L}, where (h, m, l) is a depen- an effective exploration of all these corpora can dency from the head word wh to the child word give better performance for the target domain comwm with the relation label l ∈ L. pared with the single-source domain adaptation. Recently, supervised neural dependency parsSeparating domain-invariant and domaining models have achieved great success, leading specific features is one popular way for domain to impressive performance (Chen and Manning, adaptation to distinguish the similarity and discrep2014; Kiperwasser and Goldberg, 2016; Dozat and ancy of different domains (Daumé III, 2007; Kim Manning, 2017; Li et al., 2019a). Remarkably, the et al., 2016; Sato et al., 2017). Domain-invariant BiAffine parsing model can obtain a UAS of 96.67 features indicate the shared feature space across ∗ Corresponding author domains, which have been widely-adopted as 1724 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1724–1733 November 7–11, 2021. ©2021 Association for Computational Linguistics knowledge transferring. Domain-specific features imply the differences between domains, which could be helpful if"
2021.findings-emnlp.149,P08-1068,0,0.0956171,"s. and a LAS of 95.03 on standard Penn Treebank benchmark for the English language. In order to obtain competitive performance, supervised dependency parsing models rely on a sufficient amount of training data, which is inevitably dominated to several fixed domains. When the test data is sourced from similar domains, good performance could be achieved. However, the performance could be decreased significantly when the test data is from a different domain which has a large gap between the training domains. Thus domain adaptation for dependency parsing has been concerned by a number of studies (Koo et al., 2008; Yu et al., 2013; Sato et al., 2017; Clark et al., 2018; Li et al., 2020b). These works mostly focus on single-source cross-domain dependency parsing, 1 Introduction assuming the training data is from a single source Dependency parsing aims to derive syntactic and domain (Yu et al., 2013; Sato et al., 2017). In fact, semantic tree structures over input words (Mc- multi-source cross-domain dependency parsing is a Donald et al., 2013). Given an input sentence more practical setting, considering that several des = w1 w2 . . . wn , a dependency tree, as depicted pendency parsing corpora from diff"
2021.findings-emnlp.149,2020.coling-main.338,1,0.883628,"language. In order to obtain competitive performance, supervised dependency parsing models rely on a sufficient amount of training data, which is inevitably dominated to several fixed domains. When the test data is sourced from similar domains, good performance could be achieved. However, the performance could be decreased significantly when the test data is from a different domain which has a large gap between the training domains. Thus domain adaptation for dependency parsing has been concerned by a number of studies (Koo et al., 2008; Yu et al., 2013; Sato et al., 2017; Clark et al., 2018; Li et al., 2020b). These works mostly focus on single-source cross-domain dependency parsing, 1 Introduction assuming the training data is from a single source Dependency parsing aims to derive syntactic and domain (Yu et al., 2013; Sato et al., 2017). In fact, semantic tree structures over input words (Mc- multi-source cross-domain dependency parsing is a Donald et al., 2013). Given an input sentence more practical setting, considering that several des = w1 w2 . . . wn , a dependency tree, as depicted pendency parsing corpora from different domains in Figure 1, is defined as d = {(h, m, l), 0 ≤ h ≤ have bee"
2021.findings-emnlp.149,P19-1229,1,0.815406,"exploration of all these corpora can dency from the head word wh to the child word give better performance for the target domain comwm with the relation label l ∈ L. pared with the single-source domain adaptation. Recently, supervised neural dependency parsSeparating domain-invariant and domaining models have achieved great success, leading specific features is one popular way for domain to impressive performance (Chen and Manning, adaptation to distinguish the similarity and discrep2014; Kiperwasser and Goldberg, 2016; Dozat and ancy of different domains (Daumé III, 2007; Kim Manning, 2017; Li et al., 2019a). Remarkably, the et al., 2016; Sato et al., 2017). Domain-invariant BiAffine parsing model can obtain a UAS of 96.67 features indicate the shared feature space across ∗ Corresponding author domains, which have been widely-adopted as 1724 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1724–1733 November 7–11, 2021. ©2021 Association for Computational Linguistics knowledge transferring. Domain-specific features imply the differences between domains, which could be helpful if the domain gaps could be accurately measured and effectively modeled. The learning of dom"
2021.findings-emnlp.149,P13-2017,0,0.0950635,"Missing"
2021.findings-emnlp.149,D17-1155,0,0.0228326,") propose to generate BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. Domain adaptation has been extensively studied in many research areas, including machine learning (Wang et al., 2017; Kim et al., 2017), computer vision (Ganin and Lempitsky, 2015; Rozantsev et al., 2019) and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples"
2021.findings-emnlp.149,2020.emnlp-main.639,0,0.0244174,"l. Most recently, Li et al. (2019b) propose to leverage an extra domain embedding to indicate domain source and achieve better performance on semi-supervised domain adaptation. In this work, we adjust the domain embedding method as our strong baseline. Multi-source domain adaptation. Multisource domain adaptation assumes the training data comes from multiple source domains. Many approaches of multi-source domain adaptation focus on leveraging domain knowledge to extract domainrelated features, thus boosting the performance of target domain (Daumé III, 2007; Guo et al., 2018; Li et al., 2020a; Wright and Augenstein, 2020). Zeng et al. (2018) design a domain classifier and an adversarial network to capture domain-specific and domain-invariant features, achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Rece"
2021.findings-emnlp.149,W15-2201,0,0.0135495,") and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (Charniak, 1997; Steedman et al., 2003; Reichart and Rappoport, 2007; Yu et al., 2015), co-training (Sarkar, 2001), or tri-training (Li et al., 2019c). However, selecting high confidence samples is a challenge. Thanks to large-scale labeled web data released by parsing communities, recent existing works pay 6 Conclusion more attention to semi-supervised scenario. Yu et al. (2013) give detailed error analysis on cross- This work for the first time apply the APGN apdomain dependency parsing and solve the ambigu- proach to multi-source cross-domain dependency ous features problem. Sato et al. (2017) propose to parsing, obtaining better performance than multiple 1731 baselines, eve"
2021.findings-emnlp.149,D18-1041,0,0.0243399,"019b) propose to leverage an extra domain embedding to indicate domain source and achieve better performance on semi-supervised domain adaptation. In this work, we adjust the domain embedding method as our strong baseline. Multi-source domain adaptation. Multisource domain adaptation assumes the training data comes from multiple source domains. Many approaches of multi-source domain adaptation focus on leveraging domain knowledge to extract domainrelated features, thus boosting the performance of target domain (Daumé III, 2007; Guo et al., 2018; Li et al., 2020a; Wright and Augenstein, 2020). Zeng et al. (2018) design a domain classifier and an adversarial network to capture domain-specific and domain-invariant features, achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Recently, a number of wo"
2021.findings-emnlp.149,D18-1039,0,0.028153,"ging domain knowledge to extract domainrelated features, thus boosting the performance of target domain (Daumé III, 2007; Guo et al., 2018; Li et al., 2020a; Wright and Augenstein, 2020). Zeng et al. (2018) design a domain classifier and an adversarial network to capture domain-specific and domain-invariant features, achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Recently, a number of works attempt to use the parameter generation network to improve the cross-domain or cross-language performance (Cai et al., 2019; Stoica et al., 2020; Jin et al., 2020; Nekvinda and Dusek, 2020). Particularly, Jia et al. (2019) propose to generate BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation co"
2021.findings-emnlp.149,P07-1078,0,0.0809956,", 2015; Rozantsev et al., 2019) and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (Charniak, 1997; Steedman et al., 2003; Reichart and Rappoport, 2007; Yu et al., 2015), co-training (Sarkar, 2001), or tri-training (Li et al., 2019c). However, selecting high confidence samples is a challenge. Thanks to large-scale labeled web data released by parsing communities, recent existing works pay 6 Conclusion more attention to semi-supervised scenario. Yu et al. (2013) give detailed error analysis on cross- This work for the first time apply the APGN apdomain dependency parsing and solve the ambigu- proach to multi-source cross-domain dependency ous features problem. Sato et al. (2017) propose to parsing, obtaining better performance than multiple 1"
2021.findings-emnlp.149,N01-1023,0,0.169558,"ng (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (Charniak, 1997; Steedman et al., 2003; Reichart and Rappoport, 2007; Yu et al., 2015), co-training (Sarkar, 2001), or tri-training (Li et al., 2019c). However, selecting high confidence samples is a challenge. Thanks to large-scale labeled web data released by parsing communities, recent existing works pay 6 Conclusion more attention to semi-supervised scenario. Yu et al. (2013) give detailed error analysis on cross- This work for the first time apply the APGN apdomain dependency parsing and solve the ambigu- proach to multi-source cross-domain dependency ous features problem. Sato et al. (2017) propose to parsing, obtaining better performance than multiple 1731 baselines, even when all models are enhanc"
2021.findings-emnlp.149,K17-3007,0,0.320371,"enn Treebank benchmark for the English language. In order to obtain competitive performance, supervised dependency parsing models rely on a sufficient amount of training data, which is inevitably dominated to several fixed domains. When the test data is sourced from similar domains, good performance could be achieved. However, the performance could be decreased significantly when the test data is from a different domain which has a large gap between the training domains. Thus domain adaptation for dependency parsing has been concerned by a number of studies (Koo et al., 2008; Yu et al., 2013; Sato et al., 2017; Clark et al., 2018; Li et al., 2020b). These works mostly focus on single-source cross-domain dependency parsing, 1 Introduction assuming the training data is from a single source Dependency parsing aims to derive syntactic and domain (Yu et al., 2013; Sato et al., 2017). In fact, semantic tree structures over input words (Mc- multi-source cross-domain dependency parsing is a Donald et al., 2013). Given an input sentence more practical setting, considering that several des = w1 w2 . . . wn , a dependency tree, as depicted pendency parsing corpora from different domains in Figure 1, is define"
2021.findings-emnlp.406,W13-2322,0,0.0531625,"itive results with the SOTA model, and the speed is an order of magnitude faster. Detailed analyses are conducted to gain more insights into our proposed model and the effectiveness of the pre-training technique. 1 face-01 ARG0 nature ARG1 poss really humankind caprice Figure 1: AMR example of the sentence “Facing the caprice of nature, humankind is really insignificant.” Abstract meaning representation (AMR) parsing aims to abstract semantics from a natural language sentence into a rooted, directed, and labeled graph, where the nodes represent concepts and edges represent semantic relations (Banarescu et al., 2013). Figure 1 gives an example. One main challenge of AMR parsing is the lack of large-scale annotated data, which limits the model representative ability. To alleviate the problem and boost the performance, early works propose to use silver (pseudo) data that are generated from some released AMR parsing models Corresponding author. condition degree ARG1 Introduction ∗ significant-02 polarity (van Noord and Bos, 2017; Konstas et al., 2017). Apart from AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data"
2021.findings-emnlp.406,D19-1393,0,0.18002,"t the performance, early works propose to use silver (pseudo) data that are generated from some released AMR parsing models Corresponding author. condition degree ARG1 Introduction ∗ significant-02 polarity (van Noord and Bos, 2017; Konstas et al., 2017). Apart from AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data and machine translation data. With the development of pre-trained language models, recent works try to use pre-trained language models to enhance the model input representative ability (Cai and Lam, 2019; Zhou et al., 2021). Most of the them use pre-trained models in the model encoder side since it naturally provides powerful contextualized representations for sentences. Recently, Bevilacqua et al. (2021) propose a seq2seq AMR parser based on BART (Lewis et al., 2020), which is one encoder-decoder fashion pre-trained language model. They first convert the AMR graph into a text sequence with symbols indicating the concepts’ graph positions. Then, they propose to fine-tune the sentence sequence and AMR graph sequence on BART, achieving large improvements compared with previous works, including"
2021.findings-emnlp.406,2020.acl-main.119,0,0.143065,"The remaining question is how we do concept generation and edge classification with Transformer, which is usually used for encoding sequences. Our answer is giving the Transformer attention mechanisms more meanings. In detail, we try to demonstrate that the self-attention in the decoder captures the semantic relation that can guide establishing the connections for the concepts and the cross-attention implicitly links the concept with its surface word, which is similar to the core of attention-based machine translation. Based on the inspirations, we use the copy mechanism (Zhang et al., 2019b; Cai and Lam, 2020) to copy words or lemmas as candidate concepts for concept prediction, in which we treat the cross-attention between the encoder and decoder as the probability. Another source of candidate concepts is the extracted concept vocabulary from the training data. For edge classification, we directly treat part of the decoder self-attention values as the edge scores between concept nodes. and the speed is an order of magnitude faster. Our contributions are threefold: (I) We propose a simple Transformer-based AMR parser, which only needs to add one external bi-affine scorer for the relation classifica"
2021.findings-emnlp.406,E17-1051,0,0.0189556,"tization, part-of-speech tagging, and named entity tagging. The dependency relations are obtained by the bi-affine dependency parser (Dozat and Manning, 2017) implemented in SuPar (Zhang et al., 2020). Previous works (Zhang et al., 2019b; Cai and Lam, 2020) usually use graph recategorization to reduce the complexity and sparseness of the AMR graph. In this work, we use the same script from Cai and Lam (2020) for preand post-processing. Our models are trained with Adam (Kingma and Ba, 2015) optimizer and learning rate with warm-up same as to Vaswani et al. (2017). We use the evaluation tool of Damonte et al. (2017) to test our model. Training Criterion. We train our models for at most 2,020 epochs and choose the best model to evaluate the test data according to the performance on development data. Methods Zhang et al. (2019a) Zhang et al. (2019b) Cai and Lam (2020) TAMR Smatch Unlabeled-Smatch 76.3 – 77.0 80.0 80.2 82.8 80.3 83.5 Table 2: Smatch scores of our model TAMR and comparison with previous seq2graph-based models on AMR2.0 test data. Silver Data Dev Test JAMR – 67.0? TAMR 80.6 80.3 S PRING – 83.8 Pre-train Dev Test 70.5 70.8 81.5 81.2 83.0 82.7 Fine-tune Dev Test 80.8 81.0 82.4 82.2 83.8 83.7 Ta"
2021.findings-emnlp.406,N19-1423,0,0.0334058,"he model architecture in detail and show how to adapt the AMR parsing process into Transformer in the following sections. 3.2 Input Layer. Encoder Input. The model input of each word wi in the sentence s is composed of its character representation which is generated by a convolutional neural network (CNN) (Kalchbrenner et al., 2014), randomly initialized lemma, part-of-speech tag, named entity tag, and dependency label embeddings (Xia et al., 2019), which is denoted as lem P oS NE fi = repchar wi ⊕ embwi ⊕ embwi ⊕ embwi ⊕ DL embwi , where ⊕ means the concatenation operation. We also use BERT (Devlin et al., 2019) to enhance the word representation. To get the wordbased representations, we make average pooling to sub-word-based representations. And due to the GPU limitation, we fix the BERT model parameters as Zhang et al. (2019b). The final model input representation for wi is computed as xw i = √ wi |s dim ∗ (MLP(fi ) + MLP(repBERT )) + embpi , where dim is the embedding dimension and embpi is the i-th sinusoidal position embedding. Decoder Input. In the decoder, we use the concatenation of the concept character representation and the randomly initialized concept embedding denoted as √ as the concept"
2021.findings-emnlp.406,P14-1134,0,0.0332947,"-based AMR parser, which only needs to add one external bi-affine scorer for the relation classification. (II) We investigate how to ensemble different models via the proposed stack pre-training method. (III) Detailed analyses show more insights into our model and several interesting findings of utilizing the silver data. 2 Related Work AMR parsing approaches can mostly be categorized into four classes: pipeline-based, transitionbased, seq2seq-based, and seq2graph-based approaches. Pipeline-based approaches mainly consist of two steps: 1) concept identification and 2) relation identification. Flanigan et al. (2014) is the first AMR parsing work (JAMR) that treats concept identification as a sequence labeling problem and relation identification as a maximum-scoring connected graph searching problem, in which they also propose an influential rule-based aligner for aligning the concepts and words. Lyu and Titov (2018) treat Second, to achieve competitive performance the alignment as latent variables and propose a joint with the current SOTA model, we seek to use sil- model for AMR parsing. Zhang et al. (2019a) first ver data to enhance the model representative abil- use the attention-based copy mechanism t"
2021.findings-emnlp.406,P14-1062,0,0.0281972,"≤ i ≤ m, 1 ≤ j ≤ m, r ∈ R} is the set of edges in the graph. R is the set of AMR relations. Overall, our Transformer-based model consists of the following modules, i.e., input layer, encoder layer, decoder layer, concept generator, edge generator, and relation classifier. We will describe the model architecture in detail and show how to adapt the AMR parsing process into Transformer in the following sections. 3.2 Input Layer. Encoder Input. The model input of each word wi in the sentence s is composed of its character representation which is generated by a convolutional neural network (CNN) (Kalchbrenner et al., 2014), randomly initialized lemma, part-of-speech tag, named entity tag, and dependency label embeddings (Xia et al., 2019), which is denoted as lem P oS NE fi = repchar wi ⊕ embwi ⊕ embwi ⊕ embwi ⊕ DL embwi , where ⊕ means the concatenation operation. We also use BERT (Devlin et al., 2019) to enhance the word representation. To get the wordbased representations, we make average pooling to sub-word-based representations. And due to the GPU limitation, we fix the BERT model parameters as Zhang et al. (2019b). The final model input representation for wi is computed as xw i = √ wi |s dim ∗ (MLP(fi ) +"
2021.findings-emnlp.406,P17-1014,0,0.292417,"ntics from a natural language sentence into a rooted, directed, and labeled graph, where the nodes represent concepts and edges represent semantic relations (Banarescu et al., 2013). Figure 1 gives an example. One main challenge of AMR parsing is the lack of large-scale annotated data, which limits the model representative ability. To alleviate the problem and boost the performance, early works propose to use silver (pseudo) data that are generated from some released AMR parsing models Corresponding author. condition degree ARG1 Introduction ∗ significant-02 polarity (van Noord and Bos, 2017; Konstas et al., 2017). Apart from AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data and machine translation data. With the development of pre-trained language models, recent works try to use pre-trained language models to enhance the model input representative ability (Cai and Lam, 2019; Zhou et al., 2021). Most of the them use pre-trained models in the model encoder side since it naturally provides powerful contextualized representations for sentences. Recently, Bevilacqua et al. (2021) propose a seq2seq AMR parser ba"
2021.findings-emnlp.406,2020.acl-main.703,0,0.537065,"AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data and machine translation data. With the development of pre-trained language models, recent works try to use pre-trained language models to enhance the model input representative ability (Cai and Lam, 2019; Zhou et al., 2021). Most of the them use pre-trained models in the model encoder side since it naturally provides powerful contextualized representations for sentences. Recently, Bevilacqua et al. (2021) propose a seq2seq AMR parser based on BART (Lewis et al., 2020), which is one encoder-decoder fashion pre-trained language model. They first convert the AMR graph into a text sequence with symbols indicating the concepts’ graph positions. Then, they propose to fine-tune the sentence sequence and AMR graph sequence on BART, achieving large improvements compared with previous works, including those with BERT. However, it makes the model relatively slower, which parses 31 tokens per second. We think there are two main reasons: 1) the 12-layer Transformer decoder and 2) the longer converted graph sequences that include the added symbols. In this work, we inve"
2021.findings-emnlp.406,2020.emnlp-main.196,1,0.846884,"d, directed, and labeled graph, where the nodes represent concepts and edges represent semantic relations (Banarescu et al., 2013). Figure 1 gives an example. One main challenge of AMR parsing is the lack of large-scale annotated data, which limits the model representative ability. To alleviate the problem and boost the performance, early works propose to use silver (pseudo) data that are generated from some released AMR parsing models Corresponding author. condition degree ARG1 Introduction ∗ significant-02 polarity (van Noord and Bos, 2017; Konstas et al., 2017). Apart from AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data and machine translation data. With the development of pre-trained language models, recent works try to use pre-trained language models to enhance the model input representative ability (Cai and Lam, 2019; Zhou et al., 2021). Most of the them use pre-trained models in the model encoder side since it naturally provides powerful contextualized representations for sentences. Recently, Bevilacqua et al. (2021) propose a seq2seq AMR parser based on BART (Lewis et al., 2020), which is one"
2021.findings-emnlp.406,P19-1009,0,0.0365327,"Missing"
2021.findings-emnlp.406,D19-1392,0,0.0256032,"Missing"
2021.findings-emnlp.406,2020.acl-main.302,1,0.801999,"00 BERT-base-cased 4 8 1024 100 Table 1: Hyper-parameter settings. representation and BERT is fixed in our work due to the GPU memory limitation. The encoder and decoder consist of 4 and 8 Transformer blocks, respectively. Each Transformer block has 8 heads, the feed-forward hidden size is 1024, and the hidden size is 512. Implementation Details. We use Stanford CoreNLP (Manning et al., 2014) for tokenization, lemmatization, part-of-speech tagging, and named entity tagging. The dependency relations are obtained by the bi-affine dependency parser (Dozat and Manning, 2017) implemented in SuPar (Zhang et al., 2020). Previous works (Zhang et al., 2019b; Cai and Lam, 2020) usually use graph recategorization to reduce the complexity and sparseness of the AMR graph. In this work, we use the same script from Cai and Lam (2020) for preand post-processing. Our models are trained with Adam (Kingma and Ba, 2015) optimizer and learning rate with warm-up same as to Vaswani et al. (2017). We use the evaluation tool of Damonte et al. (2017) to test our model. Training Criterion. We train our models for at most 2,020 epochs and choose the best model to evaluate the test data according to the performance on developmen"
2021.findings-emnlp.406,P18-1037,0,0.065492,"he silver data. 2 Related Work AMR parsing approaches can mostly be categorized into four classes: pipeline-based, transitionbased, seq2seq-based, and seq2graph-based approaches. Pipeline-based approaches mainly consist of two steps: 1) concept identification and 2) relation identification. Flanigan et al. (2014) is the first AMR parsing work (JAMR) that treats concept identification as a sequence labeling problem and relation identification as a maximum-scoring connected graph searching problem, in which they also propose an influential rule-based aligner for aligning the concepts and words. Lyu and Titov (2018) treat Second, to achieve competitive performance the alignment as latent variables and propose a joint with the current SOTA model, we seek to use sil- model for AMR parsing. Zhang et al. (2019a) first ver data to enhance the model representative abil- use the attention-based copy mechanism to predict ity. Specifically, we employ three different per- concepts in a BiLSTM encoder-decoder framework formance AMR models (denoted as “father” mod- and then use the bi-affine scorer for edge and relaels) to generate three different performance silver tion prediction based on the predicted concepts. d"
2021.findings-emnlp.406,P14-5010,0,0.00312283,"4733 Input Layer character lemma PoS tag NER tag dependency label concept BERT Encoder Layer Transformer encoder Decoder Layer Transformer decoder Concept Generator hidden size Relation Classifier hidden size 32 300 32 16 64 300 BERT-base-cased 4 8 1024 100 Table 1: Hyper-parameter settings. representation and BERT is fixed in our work due to the GPU memory limitation. The encoder and decoder consist of 4 and 8 Transformer blocks, respectively. Each Transformer block has 8 heads, the feed-forward hidden size is 1024, and the hidden size is 512. Implementation Details. We use Stanford CoreNLP (Manning et al., 2014) for tokenization, lemmatization, part-of-speech tagging, and named entity tagging. The dependency relations are obtained by the bi-affine dependency parser (Dozat and Manning, 2017) implemented in SuPar (Zhang et al., 2020). Previous works (Zhang et al., 2019b; Cai and Lam, 2020) usually use graph recategorization to reduce the complexity and sparseness of the AMR graph. In this work, we use the same script from Cai and Lam (2020) for preand post-processing. Our models are trained with Adam (Kingma and Ba, 2015) optimizer and learning rate with warm-up same as to Vaswani et al. (2017). We use"
2021.findings-emnlp.406,P19-1451,0,0.0698639,"e bi-affine scorer for edge and relaels) to generate three different performance silver tion prediction based on the predicted concepts. data and try to investigate several questions which Transition-based methods aim to design a seare seldom discussed in previous works: 1) What ries of actions to generate the AMR graph. Wang are the best learning schedules to build pre-trained et al. (2016) propose to transform the sentence’s demodels with silver data and later fine-tune with pendency tree into its AMR graph. Ballesteros and the gold-standard data, respectively? 2) Are all Al-Onaizan (2017); Naseem et al. (2019) use Stackthe different performance silver data beneficial for LSTM transition-based AMR parser that transour model, even its father model lags behind our forms the sentence into the AMR graph, which model? and 3) Whether using multiple different is different from Wang et al. (2016). With the performance silver data can provide more informa- rise of Transformer, Astudillo et al. (2020); Zhou tion than the best performance one or not, i.e., can et al. (2021) propose to use Stack-Transformer for the higher performance silver data benefits from transition-based AMR parsing. lower performance silv"
2021.findings-emnlp.406,S16-1181,0,0.0190748,"of actions to generate the AMR graph. Wang are the best learning schedules to build pre-trained et al. (2016) propose to transform the sentence’s demodels with silver data and later fine-tune with pendency tree into its AMR graph. Ballesteros and the gold-standard data, respectively? 2) Are all Al-Onaizan (2017); Naseem et al. (2019) use Stackthe different performance silver data beneficial for LSTM transition-based AMR parser that transour model, even its father model lags behind our forms the sentence into the AMR graph, which model? and 3) Whether using multiple different is different from Wang et al. (2016). With the performance silver data can provide more informa- rise of Transformer, Astudillo et al. (2020); Zhou tion than the best performance one or not, i.e., can et al. (2021) propose to use Stack-Transformer for the higher performance silver data benefits from transition-based AMR parsing. lower performance silver data? Based on the anSeq2seq-based approaches convert the AMR swers to these questions, which are shown in Secgraph generation problem into a symbolic sequence tion 6.2, we propose a stack pre-training technique generation problem, where the hierarchy structure for effectively us"
2021.inlg-1.16,N19-1423,0,0.0164563,"dependency between adjacent tokens: p(Y |X) = QT p(y |X), enabling to generate all outputs in i i parallel. Recent years have witnessed impressive advances in non-autoregressive models, such as fullyNAT and its variants (Gu et al., 2018; Guo et al., 2019; Wang et al., 2019), insertion-based models (Stern et al., 2019; Gu et al., 2019) and iterative refinement models (Lee et al., 2018; Ghazvininejad et al., 2019). Mask-predict CMLM (CMLM) stands out of them owing to both significantly-fast inference and remarkable performance (Ghazvininejad et al., 2019). It extends the masked language model (Devlin et al., 2019) and enables it to solving generation tasks with iterative refinement. In each step, the model decodes target conditioned on m well-predicted tokens with high confidence and (L − m)×[MASK], where L is the length of target (see Section 2 for details). This mechanism leads to the issue that it’s liable to generate repeated tokens and slow down the convergence in early-stage iterations. We speculate this is because the proportion of useful tokens, i.e. m → 0, is too small to provide enough information for the next step prediction. Intuitively, the model tends to predict similar or even identical"
2021.inlg-1.16,D19-1633,0,0.0554664,"on (NMT), autoregressive QT models decode tokens one-by-one: p(Y |X) = i p(yi |y≤i |X), which ensures the robustness of intrinsic language models but slows down the inference. Non-autoregressive models break the dependency between adjacent tokens: p(Y |X) = QT p(y |X), enabling to generate all outputs in i i parallel. Recent years have witnessed impressive advances in non-autoregressive models, such as fullyNAT and its variants (Gu et al., 2018; Guo et al., 2019; Wang et al., 2019), insertion-based models (Stern et al., 2019; Gu et al., 2019) and iterative refinement models (Lee et al., 2018; Ghazvininejad et al., 2019). Mask-predict CMLM (CMLM) stands out of them owing to both significantly-fast inference and remarkable performance (Ghazvininejad et al., 2019). It extends the masked language model (Devlin et al., 2019) and enables it to solving generation tasks with iterative refinement. In each step, the model decodes target conditioned on m well-predicted tokens with high confidence and (L − m)×[MASK], where L is the length of target (see Section 2 for details). This mechanism leads to the issue that it’s liable to generate repeated tokens and slow down the convergence in early-stage iterations. We specul"
2021.inlg-1.16,D18-1149,0,0.0822622,"machine translation (NMT), autoregressive QT models decode tokens one-by-one: p(Y |X) = i p(yi |y≤i |X), which ensures the robustness of intrinsic language models but slows down the inference. Non-autoregressive models break the dependency between adjacent tokens: p(Y |X) = QT p(y |X), enabling to generate all outputs in i i parallel. Recent years have witnessed impressive advances in non-autoregressive models, such as fullyNAT and its variants (Gu et al., 2018; Guo et al., 2019; Wang et al., 2019), insertion-based models (Stern et al., 2019; Gu et al., 2019) and iterative refinement models (Lee et al., 2018; Ghazvininejad et al., 2019). Mask-predict CMLM (CMLM) stands out of them owing to both significantly-fast inference and remarkable performance (Ghazvininejad et al., 2019). It extends the masked language model (Devlin et al., 2019) and enables it to solving generation tasks with iterative refinement. In each step, the model decodes target conditioned on m well-predicted tokens with high confidence and (L − m)×[MASK], where L is the length of target (see Section 2 for details). This mechanism leads to the issue that it’s liable to generate repeated tokens and slow down the convergence in earl"
2021.inlg-1.16,N19-4009,0,0.0237977,"d for tokenization with the vocabulary size set to 42k and 40k for En↔De and En↔Ro. Model Configurations We apply the same weight initialization method and configurations on hyperparameters as prior work: nlayers = 12, nheads = 8, dhidden = 512, dF F N = 2048 (Ghazvininejad et al., 2019; Vaswani et al., 2017). Our model is trained on 4 Tesla V100 GPUs with the max batch size of 8k tokens per card. Adam (Kingma and Ba, 2015) is used for optimization. The learning rate warms-up for 20k steps to 5e-4 and decays with the inversed-sqrt schedular. We implement models in the experiment with fairseq (Ott et al., 2019). 4.2 Results and Analysis Table 1 shows the performance of the proposed HI-CMLM with the BLEU score (Papineni et al., 2002). For each language pair, the model obtains consistent improvements with the Fence Replace, but no gains with another two. 169 Model En-De De-En En-Ro Ro-En Transformer (Vaswani et al., 2017) Transformer (Our Implementation) 27.30 27.72 32.04 34.03 33.93 CMLM (Ghazvininejad et al., 2019) CMLM (Our Implementation) 27.03 26.89 30.53 30.71 33.08 32.94 33.31 33.07 27.01 26.76 26.81 27.42 (+0.53) 30.74 30.82 30.79 31.32 (+0.61) 32.89 32.74 32.80 33.36 (+0.42) 33.03 32.95 33.14"
2021.inlg-1.16,P02-1040,0,0.111659,"ame weight initialization method and configurations on hyperparameters as prior work: nlayers = 12, nheads = 8, dhidden = 512, dF F N = 2048 (Ghazvininejad et al., 2019; Vaswani et al., 2017). Our model is trained on 4 Tesla V100 GPUs with the max batch size of 8k tokens per card. Adam (Kingma and Ba, 2015) is used for optimization. The learning rate warms-up for 20k steps to 5e-4 and decays with the inversed-sqrt schedular. We implement models in the experiment with fairseq (Ott et al., 2019). 4.2 Results and Analysis Table 1 shows the performance of the proposed HI-CMLM with the BLEU score (Papineni et al., 2002). For each language pair, the model obtains consistent improvements with the Fence Replace, but no gains with another two. 169 Model En-De De-En En-Ro Ro-En Transformer (Vaswani et al., 2017) Transformer (Our Implementation) 27.30 27.72 32.04 34.03 33.93 CMLM (Ghazvininejad et al., 2019) CMLM (Our Implementation) 27.03 26.89 30.53 30.71 33.08 32.94 33.31 33.07 27.01 26.76 26.81 27.42 (+0.53) 30.74 30.82 30.79 31.32 (+0.61) 32.89 32.74 32.80 33.36 (+0.42) 33.03 32.95 33.14 33.51 (+0.44) HI-CMLM + All Mask HI-CMLM + All Copied HI-CMLM + Weighted Sum HI-CMLM + Fence Replace Table 1: The performan"
2021.inlg-1.16,P16-1162,0,0.058109,"ing the subset of the original masked token with the copied source embedding, so that the proportion of corrupted tokens can be unchanged. We apply this method to train the model under all hybrid 4 4.1 Experiments Experimental Setup We evaluate HI-CMLMs with the proposed hybrid strategies on standard machine translation benchmarks including WMT14 En↔De and WMT16 En↔Ro in both directions. Datasets The sizes of the dataset are 4.5M and 610k for En↔De and En↔Ro respectively. We create the knowledge distilled data as suggested in (Gu et al., 2018; Zhou et al., 2020) with same configurations. BPE (Sennrich et al., 2016) is used for tokenization with the vocabulary size set to 42k and 40k for En↔De and En↔Ro. Model Configurations We apply the same weight initialization method and configurations on hyperparameters as prior work: nlayers = 12, nheads = 8, dhidden = 512, dF F N = 2048 (Ghazvininejad et al., 2019; Vaswani et al., 2017). Our model is trained on 4 Tesla V100 GPUs with the max batch size of 8k tokens per card. Adam (Kingma and Ba, 2015) is used for optimization. The learning rate warms-up for 20k steps to 5e-4 and decays with the inversed-sqrt schedular. We implement models in the experiment with fa"
2021.mtsummit-research.12,W05-0909,0,0.127451,"rm-up and inverse-sqrt warm-up strategy. We use 0.3 for dropout probability, 0.1 for label smoothing (Pereyra et al., 2017), Adam (Kingma and Ba, 2015) is used as the optimizer. For the VLTransformer, we use the parameter of fairseq pre-trained Transformer to initialize the backbone and text related embeddings, vision related parameters are initialized randomly. The model is fine-tuned on a Tesla V100 GPU with fp16 enabled and converges in less than 20 minutes for 10 epochs. The baseline method is the pre-trained Transformer without fine-tuning. We use BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) as evaluation metrics with lowercased text. 5 Analysis We compare our results with another six latest methods in Table 1. As the goal of newlyproposed NMTUVR (Zhang et al., 2020) is to improve universal NMT with multimodality, direct comparison with ours is unfair. As expectation, the pre-trained Transformer set a very strong baseline, which demonstrate that a well-trained text-only NMT model has been able to produce satisfying translations in the absence of word and phrase ambiguitity. At the same time, the profit of fine-tuning the Transformer is significant, even with only textual inputs."
2021.mtsummit-research.12,W18-6402,0,0.072682,"lied to MMT, concretely transferring from monomodal to multimodal tasks. Constant attention has been paid on MMT task (Specia et al., 2016a) in the Conference of Machine Translation (WMT) in recent years (2016-2018). Formally, it aims to learn a function mapping: X × I → Y, which takes source text and an image as input and translate them into the target text as shown in Figure 1. Additional modality is to disambiguate the source sentence, with the reference of image. However, the effectiveness of the visual context has been questioned by prior work (Specia et al., 2016b; Elliott et al., 2017; Barrault et al., 2018; Caglayan et al., 2019). They show that visual context is not convincingly useful and the marginal gain is pretty modest, which is speculated to be resulted from the limitation of available datasets — the Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 139 Figure 1: An example of the multimodal translation. (Specia et al., 2016b) Large-scale text-only bilingual corpus Pre-Training + Vision Fine-Tuning Limited multimodal bilingual corpus Text + Image Figure 2: The multimodal transfer learning solution. 1) Initializ"
2021.mtsummit-research.12,W16-2360,0,0.0432324,"nsformer (VLTransformer) which is compatible for both monomodal and multimodal inputs. The model achieves competitive results on Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 140 Multi30k En-De and En-Fr tasks. • We present a method of fine-tuning a pretrained monomodal MT model in the multimodal MT task, which is implemented by appropriately masking elements in both modalities to encourage the model to make full use of the input information. 2 Related Work There are a spectrum of prior works investigating MMT. (Caglayan et al., 2016; Calixto and Liu, 2017) used standard RNN encoder-decoder with attention (Bahdanau et al., 2015) to fuse textual and visual features. Both of them employed pretrained image classification models like VGG and ResNet to extract visual features and combine with textual features with different schemes of attentions. Imaginet is proposed to predict the visual feature conditioned on textual inputs, which is used to improve the quality of the representation of contexts (Elliott and K´ad´ar, 2017), where they decompose the MMT task into two sub-tasks where each can be trained separately with large ex"
2021.mtsummit-research.12,N19-1422,0,0.0123878,"transferring from monomodal to multimodal tasks. Constant attention has been paid on MMT task (Specia et al., 2016a) in the Conference of Machine Translation (WMT) in recent years (2016-2018). Formally, it aims to learn a function mapping: X × I → Y, which takes source text and an image as input and translate them into the target text as shown in Figure 1. Additional modality is to disambiguate the source sentence, with the reference of image. However, the effectiveness of the visual context has been questioned by prior work (Specia et al., 2016b; Elliott et al., 2017; Barrault et al., 2018; Caglayan et al., 2019). They show that visual context is not convincingly useful and the marginal gain is pretty modest, which is speculated to be resulted from the limitation of available datasets — the Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 139 Figure 1: An example of the multimodal translation. (Specia et al., 2016b) Large-scale text-only bilingual corpus Pre-Training + Vision Fine-Tuning Limited multimodal bilingual corpus Text + Image Figure 2: The multimodal transfer learning solution. 1) Initialize a vanilla Transformer."
2021.mtsummit-research.12,W18-6439,0,0.0362125,"Missing"
2021.mtsummit-research.12,N19-1423,0,0.491326,"robust MMT model. Compared with the translation corpus on news such as Common Crawl and UN corpus, commonly-used MMT dataset Multi30k (Elliott et al., 2016) is too small to train large-capacity models with millions of parameters. Therefore, it is imperative to put efforts on methods in low-resource MMT. For the text-only NMT tasks, the Transformer (Vaswani et al., 2017) provides a novel architecture on language generation which supersedes RNN architectures rapidly with enhanced parallelizability. Meanwhile, the framework of pre-training and fine-tuning becomes a standard pipeline since BERT (Devlin et al., 2019) achieved the SOTA performances over a bunch of natural language understanding tasks. This to some extent suggests that transfer learning could effectively solve NLP tasks which requires deep understanding on the semantics but have limited size of in-domain data. Therefore, in this paper, we will investigate whether it’s feasible to apply transfer learning to MMT task, i.e. transferring the prior knowledge learned from monomodal task into a multimodal task, as shown in Figure 2. The contribution of our work can be summarized as follows: • We propose the Visual Language Transformer (VLTransform"
2021.mtsummit-research.12,W17-4718,0,0.223572,"can be effectively applied to MMT, concretely transferring from monomodal to multimodal tasks. Constant attention has been paid on MMT task (Specia et al., 2016a) in the Conference of Machine Translation (WMT) in recent years (2016-2018). Formally, it aims to learn a function mapping: X × I → Y, which takes source text and an image as input and translate them into the target text as shown in Figure 1. Additional modality is to disambiguate the source sentence, with the reference of image. However, the effectiveness of the visual context has been questioned by prior work (Specia et al., 2016b; Elliott et al., 2017; Barrault et al., 2018; Caglayan et al., 2019). They show that visual context is not convincingly useful and the marginal gain is pretty modest, which is speculated to be resulted from the limitation of available datasets — the Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 139 Figure 1: An example of the multimodal translation. (Specia et al., 2016b) Large-scale text-only bilingual corpus Pre-Training + Vision Fine-Tuning Limited multimodal bilingual corpus Text + Image Figure 2: The multimodal transfer learning"
2021.mtsummit-research.12,W16-3210,0,0.0508269,"Missing"
2021.mtsummit-research.12,I17-1014,0,0.038454,"Missing"
2021.mtsummit-research.12,N19-3012,0,0.0144846,"iu, 2017) used standard RNN encoder-decoder with attention (Bahdanau et al., 2015) to fuse textual and visual features. Both of them employed pretrained image classification models like VGG and ResNet to extract visual features and combine with textual features with different schemes of attentions. Imaginet is proposed to predict the visual feature conditioned on textual inputs, which is used to improve the quality of the representation of contexts (Elliott and K´ad´ar, 2017), where they decompose the MMT task into two sub-tasks where each can be trained separately with large external corpus. Hirasawa et al. (2019) extends the work of Imagination by converting the decoding process into a similarity based searching between the predicted embedding and the embedding of the vocabulary, which is achieved by optimizing a marginal loss on pre-trained word embeddings with predicted word embeddings. Besides, (Specia et al., 2016b; Elliott et al., 2017; Barrault et al., 2018) make comprehensive summaries on the MMT tasks from MMT 2016 to 2018, which shows two major findings from the task: 1). The effectiveness of the additional modality is still questionable or limited, which encourages researchers to go further"
2021.mtsummit-research.12,W08-0510,0,0.0573683,"y prevents the model from neglecting visual contexts by CMM in training. See Figure 3 for more intuitive details. 4 4.1 Experiment Dataset In the experiment, we use the Multi30k (Elliott et al., 2016) dataset to evaluate our method. The sizes of the dataset are 29000:1014:1000:1000 for training, validation, test2016 and test2017 set, each instance in form of triples (source, target, image). English descriptions are provided as source texts, German and French corpus are provided as target texts. All corresponding images are from Flickr30k (Young et al., 2014) dataset. We use the Moses toolkit (Hoang and Koehn, 2008) to pre-process the data with lowercasing, tokenizing and punctuation normalization. For image features, we use BUTD (Anderson et al., 2018) to extract 4 groups of features for each object, including pooled ROI feature vector, object class, object attribute and bounding box. Maximum of 36 detected objects are reserved with the prediction probability higher than 0.5. The BUTD model is not fine-tuned in the translation task. 4.2 Setup We use the pre-trained transformer model provided by fairseq (Ott et al., 2019) which is implemented with PyTorch (Paszke et al., 2019). The En-De model (Transform"
2021.mtsummit-research.12,N19-4009,0,0.0480418,"Missing"
2021.mtsummit-research.12,P02-1040,0,0.110858,"-rate of 1e-4 with 4000 steps of warm-up and inverse-sqrt warm-up strategy. We use 0.3 for dropout probability, 0.1 for label smoothing (Pereyra et al., 2017), Adam (Kingma and Ba, 2015) is used as the optimizer. For the VLTransformer, we use the parameter of fairseq pre-trained Transformer to initialize the backbone and text related embeddings, vision related parameters are initialized randomly. The model is fine-tuned on a Tesla V100 GPU with fp16 enabled and converges in less than 20 minutes for 10 epochs. The baseline method is the pre-trained Transformer without fine-tuning. We use BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) as evaluation metrics with lowercased text. 5 Analysis We compare our results with another six latest methods in Table 1. As the goal of newlyproposed NMTUVR (Zhang et al., 2020) is to improve universal NMT with multimodality, direct comparison with ours is unfair. As expectation, the pre-trained Transformer set a very strong baseline, which demonstrate that a well-trained text-only NMT model has been able to produce satisfying translations in the absence of word and phrase ambiguitity. At the same time, the profit of fine-tuning the Transformer is signif"
2021.mtsummit-research.12,P16-1162,0,0.0154936,"ine-tuned in the translation task. 4.2 Setup We use the pre-trained transformer model provided by fairseq (Ott et al., 2019) which is implemented with PyTorch (Paszke et al., 2019). The En-De model (Transformer-Large) is trained on WMT’19 corpus and En-Fr (Transformer-Big) model is trained on WMT’14 corpus. Both models share the vocabulary between source and target language, resulting in sizes of 42020 and 44508 for En-De and En-Fr vocabularies. The parameters of the embedding layer as well as the output projection layer are also shared for the encoder and the decoder in both models. The BPE (Sennrich et al., 2016) is applied to create the vocabulary. The model of En-De is slightly larger (270M) than the En-Fr (222M) model, because of the difference of the dimenProceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 144 zwei zwei h@@ 0.8 unde h@@ spielen spielen zusammen auf 0.6 zusammen auf grü@@ grü@@ 0.4 ne@@ 0.6 0.4 ne@@ m gr@@ 0.2 gr@@ as . . dog@ dog@ tail@ collar@ tongue@ two dogs are playing together on green grass . <eos&gt; as 0.2 dog@ dog@ tail@ collar@ tongue@ two <unk&gt; are playing together on green grass . <eos&gt; m 0.8 unde"
2021.mtsummit-research.12,W16-2346,0,0.0477796,"Missing"
2021.mtsummit-research.12,Q14-1006,0,0.0373813,"nd recover the corrupted vectors. We find it effectively prevents the model from neglecting visual contexts by CMM in training. See Figure 3 for more intuitive details. 4 4.1 Experiment Dataset In the experiment, we use the Multi30k (Elliott et al., 2016) dataset to evaluate our method. The sizes of the dataset are 29000:1014:1000:1000 for training, validation, test2016 and test2017 set, each instance in form of triples (source, target, image). English descriptions are provided as source texts, German and French corpus are provided as target texts. All corresponding images are from Flickr30k (Young et al., 2014) dataset. We use the Moses toolkit (Hoang and Koehn, 2008) to pre-process the data with lowercasing, tokenizing and punctuation normalization. For image features, we use BUTD (Anderson et al., 2018) to extract 4 groups of features for each object, including pooled ROI feature vector, object class, object attribute and bounding box. Maximum of 36 detected objects are reserved with the prediction probability higher than 0.5. The BUTD model is not fine-tuned in the translation task. 4.2 Setup We use the pre-trained transformer model provided by fairseq (Ott et al., 2019) which is implemented with"
2021.mtsummit-research.12,D18-1400,0,0.017031,"edding matrix in the experiment. Va ∈ Rm×400 are attribute class one-hot vectors, and the bounding boxes Vbbox ∈ Rm×4 represents for normalized coordinates (x0 , y0 , x1 , y1 ) of detected objects. Coordinates are normalized into [0, 1] with the size of the image, i.e. x/ximg , y/yimg . φ represents for linear transformations to scale the dimensionality along with the original Transformer dmodel . The summation of 4 types of features simultaneously encodes most of necessary visual information, which is more fine-grained and informative comparing with previous works (Elliott and K´ad´ar, 2017; Zhou et al., 2018; Caglayan et al., 2016) which only uses pooled ResNet (He et al., 2016) features or pooled object embeddings (Gr¨onroos et al., 2018). 3.2 Fusion of Image and Text In order to take the advantage of pre-trained NMT models and avoid overfitting using largecapacity network with limited multimodal labelled training data, we introduce parameters that needs to be trained from scratch as few as possible into the model. Therefore, instead of using architectures like LXMERT (Tan and Bansal, 2019) and the model proposed in (Zhang et al., 2020), where large sets of newly initialized parameters will be i"
2021.naacl-main.144,P17-4017,0,0.0467976,"Missing"
2021.naacl-main.144,N19-1423,0,0.126883,"ng et al. (2019a) extract the semanthat our proposed unified model achieves superior tic representations from a pre-trained SRL model performance compared with previously proposed BMESO-based works. Our contributions are: (i) and feed them into the opinion mining model, achieving substantial improvements. Zhang et al. we propose a unified span-based model for opinion (2020) incorporate the powerful contextual repremining in the end-to-end fashion that also supports sentations of bi-directional encoder representations the given-expression setting, (ii) we successfully from Transformers (BERT) (Devlin et al., 2019) integrate syntactic constituents knowledge into our and external dependency syntactic knowledge. model with MTL and GCN, achieving promising improvements, (iii) detailed analyses demonstrate To solve or alleviate the weaknesses of the prethe effectiveness of our unified model and the use- viously proposed BMESO-based models, we profulness of integrating constituent syntactic knowl- pose a new method to unifiedly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classifica"
2021.naacl-main.144,Q19-1019,0,0.0464782,"Missing"
2021.naacl-main.144,P18-2058,0,0.0219271,"edly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classification as an MTL problem. Besides, to boost the opinion mining performance and motivated by the span-based task formalism, we explore to incorporate syntactic constituents into our model. Utilizing span-based representations have been investigated for many other NLP tasks, such as named entity recognition (NER) (Tan et al., 2020), constituency parsing (Kitaev and Klein, 2018), and semantic role labeling (SRL) (He et al., 2018). Generally, NER is a single span classification problem, constituency parsing is a span-based structure prediction problem, and SRL is a word-span classification problem. Different from them, in our methodology, OM is a span-span classification problem. 3 The S PAN OM Model 3.1 Task Definition. Given an input sentence s = w1 , w2 , ..., wn , our model aims to predict the gold-standard opinion structures Y ⊆ E × O × R, where E = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of expressions, O = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of opinion roles , and R is the set of opinion relations (holder"
2021.naacl-main.144,P14-1062,0,0.0186777,"char representation, and contextual word representation to compose the model input, denoted as: xi = embword ⊕ repchar ⊕ repcontext , (1) wi wi wi |s Encoder Layer. + M LPbexp (hb ) + M LPeexp (he ), srol = M LP rol (spanrol b,e ) (5) + M LPbrol (hb ) + M LPerol (he ). We can observe that for a sentence with n words, the numbers of candidate spans for expressions and roles are both n∗(n+1) , while the number of 2 gold expressions and roles are much fewer. To alleviate the unbalanced number of gold samples where ⊕ means the concatenate operation. We use the convolutional neural networks (CNN) (Kalchbrenner et al., 2014) to generate the character representations over the characters of words. 1797 1 We omit the process of span boundary module in Figure 2 for clarity. O Classification Layer MLP Holder OM Target MLP Encoder MLP Representation Layer OM Constituent MTL Input seriously needs equipment for detecting drugs GCN Constituent Encoder Input Layer GCN Input OM Encoder Layer Encoder Input MTL+GCN GCN Figure 2: The model architecture of our unified span-based opinion mining model (left) and syntactic constituent integration methods (right). and negative samples, we adapt the focal loss that is widely used in"
2021.naacl-main.144,P16-1087,0,0.342622,"rporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinio"
2021.naacl-main.144,D14-1162,0,0.0862699,"62.04 53.27 57.76 Proportional F1 Holder Target Overall 46.62 34.29 55.62 41.65 48.90 61.20 49.88 55.68 Table 1: Experimental results of our span-based opinion mining model and comparison with previous works on the MPQA2.0 dataset in the end-to-end setting. “-” means results are not reported in their paper. Exact P R F1 Zhang et al. (2019b) 60.21 48.52 53.04 S PAN OM 64.85 52.60 58.06 S PAN OM+BERT 67.15 60.63 63.71 Models Table 2: Results and comparison of the expression prediction on the exact metric in the end-to-end setting. 5.2 Hyper-parameters. We employ the 300-dimension GloVe vector (Pennington et al., 2014) as our pre-trained word embeddings. The character embeddings are randomly initialized and a CNN with kernel sizes of 3, 4, 5 is used to capture the character representations. For the contextual representations, we extract the representations from the base BERT by making a weighted summation over the last four layer outputs. The hidden size of the BiLSTM layer is set to 300 and we employ 2-layer BiLSTMs to encode the input representations. The dimension of opinion expression and role representations is 300 and the hidden size of expression, role, and relation classifiers is 150. We use 3-layer"
2021.naacl-main.144,P13-1161,0,0.210861,"thod. In addition, incorporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap"
2021.naacl-main.144,Q14-1039,0,0.0244727,"n Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1795–1804 June 6–11, 2021. ©2021 Association for Computational Linguistics belongs to an expression, 0 otherwise), thus one sample is expanded n times if one sentence has n expressions, which is inefficient (Marasovi´c and Frank, 2018; Zhang et al., 2020). 2) The BMESObased method is weak to capture long-range dependencies and prefers to predict shorter opinion role spans (Zhang et al., 2020). 2 Related Work There are several task settings for opinion mining in the community: 1) Breck et al. (2007); Yang and Cardie (2014) focus on labeling the expressions. 2) Katiyar and Cardie (2016); Zhang et al. (2019b); Quan et al. (2019) discover the opinion structures in the end-to-end setting, i.e, based on the systemMotivated by the span-based representations of atic expressions. 3) Marasovi´c and Frank (2018); opinion expressions and roles, we propose a unified Zhang et al. (2019a, 2020) identify the opinion span-based opinion mining model (S PAN OM) that roles based on the given expressions. Our work can solve or alleviate the aforementioned weak- follows the end-to-end setting and also supports nesses. First, we tre"
2021.naacl-main.144,P18-1249,0,0.388081,"ing constituent syntactic knowl- pose a new method to unifiedly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classification as an MTL problem. Besides, to boost the opinion mining performance and motivated by the span-based task formalism, we explore to incorporate syntactic constituents into our model. Utilizing span-based representations have been investigated for many other NLP tasks, such as named entity recognition (NER) (Tan et al., 2020), constituency parsing (Kitaev and Klein, 2018), and semantic role labeling (SRL) (He et al., 2018). Generally, NER is a single span classification problem, constituency parsing is a span-based structure prediction problem, and SRL is a word-span classification problem. Different from them, in our methodology, OM is a span-span classification problem. 3 The S PAN OM Model 3.1 Task Definition. Given an input sentence s = w1 , w2 , ..., wn , our model aims to predict the gold-standard opinion structures Y ⊆ E × O × R, where E = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of expressions, O = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of opinion ro"
2021.naacl-main.144,2020.acl-main.297,1,0.617524,"promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinion structures between different expresOpi"
2021.naacl-main.144,N19-1066,0,0.12942,"etting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinion structures between different expresOpinion mining (OM), which aims to find the opin- sions in one sentence. Figure 1 gives an example, ion structures of “Who expressed what opinions in which some overlapped opinion relations have towards what.” in one sentence, has achieved much been discarded by previous works (Katiyar and attention in recent years (Katiyar and Cardie, 2016; Cardie, 2016), such as [happy, he loves being EnMarasovi´c and Frank, 2018; Zhang et al., 2019b, derly Park, Target] and [loves, he, Holder]. There 2020). The opinion analysis has many NLP appli- are also other works which focus only on predicting cations, such as social media monitoring (Bollen opinions roles based on the gold-standard expreset al., 2011) and e-commerce applications (Cui sions, which also follow the BMESO-based method et al., 2017). The commonly used benchmark (Marasovi´c and Frank, 2018; Zhang et al., 2020). However, they also suffer from some weaknesses: ∗ Rui Wang’s contributions were carried out while at 1) the expressions are usually fed into the model inAlibaba"
2021.naacl-main.144,N18-1054,0,0.041995,"Missing"
2021.naacl-main.144,J93-2004,0,0.0741147,"F1 score of 67.66. Finally, we try to combine the two kinds of methods and the results are shown in the last major row. It is clear that combining the MTL method with OntoNotes and the GCN method with ParserPTB achieves better results than the reversed one. Therefore, our constituent-enhanced opinion mining model follows this combination. Besides, we can also see the relative lower results of “OntoNotes+PTB” in “+MTL” and “+GCN” settings, which is strange Which source of constituent knowledge is better? There are two main constituent syntax corpus in the community, i.e., Penn Treebank (PTB) (Marcus et al., 1993) and OntoNotes5.0 (Weischedel et al., 2013). The PTB corpus contains about 39k training data and mainly focuses on news data, while the OntoNotes5.0 corpus contains about 75k training data and focuses on multi-domain data (news, web, telephone conversation, and etc.). It is a worthy question to explore which is better for our span-based OM model, or what kind of combination is better. We compare them with various combinations on the BERT-based model, whose results are shown in Table 5. First, the sec7 We use the code of Kitaev and Klein (2018) to train the OntoNotes conond major row shows the"
2021.naacl-main.144,D18-1244,0,0.0229945,"information to expressions and roles. 4.2 The GCN Method. The MTL method enhances our OM model from the aspect of model representative ability by jointly modeling opinion mining and partial constituency parsing. We argue that modeling the syntactic constituent structure is also beneficial for OM because it provides valuable syntactic information for a sentence. Therefore, we try to employ the recently popular GCN (Kipf and Welling, 2016) to encode the constituent structure. However, the conventional GCN is not suitable for constituency trees, because it usually works on the dependency trees (Zhang et al., 2018, 2020) where the nodes are the surface words in a sentence. While, in constituent trees, there exists a certain number of non-terminal nodes3 , such as “NP”, “VP”, “SBAR” and so on. So it is hard to directly apply conventional GCN on the constituent trees. In the following, we first introduce the definition and workflow of typical GCN and then describe our modification. Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as: ! X l hlv = ρ Wl hl−1 (11) u +b , u∈N (v) 3"
C02-1060,P94-1038,0,\N,Missing
C02-1060,P93-1022,0,\N,Missing
C02-1060,P90-1034,0,\N,Missing
C02-1060,P97-1009,0,\N,Missing
C02-1060,P98-2148,0,\N,Missing
C02-1060,C98-2143,0,\N,Missing
C04-1103,W03-0317,0,0.138161,"Missing"
C04-1103,P98-2220,0,0.225691,"Missing"
C04-1103,kang-choi-2000-automatic,0,\N,Missing
C04-1103,C02-1099,0,\N,Missing
C04-1103,C00-1056,0,\N,Missing
C04-1103,W03-1508,0,\N,Missing
C04-1103,C98-2215,0,\N,Missing
C04-1103,P02-1051,0,\N,Missing
C08-1014,J93-2003,0,0.0123935,"Missing"
C08-1014,2006.iwslt-papers.4,1,0.884309,"d either from different methods or same decoder with different models, local feature functions of each hypothesis are not directly comparable, and thus inadequate for rescoring. We hence exploit rich global feature functions in the rescoring models to compensate the loss of local feature functions. We apply the following 10 feature functions and optimize the weight of each feature function using the tool in Moses package. • direct and inverse IBM model 1 and 3 • association score, i.e. hyper-geometric distribution probabilities and mutual information • lexicalized word/block reordering rules (Chen et al., 2006) • 6-gram target LM • 8-gram target word-class based LM, wordclasses are clustered by GIZA++ • length ratio between source and target sentence • question feature (Chen et al., 2005) • linear sum of n-grams relative frequencies within N-best translations (Chen et al., 2005) • n-gram posterior probabilities within the Nbest translations (Zens and Ney, 2006) • sentence length posterior probabilities (Zens and Ney, 2006) 108 5 Experiments 5.1 data Tasks Train We carried out two sets of experiments on two different datasets. One is in spoken language domain while the other is on newswire corpus. Bo"
C08-1014,2007.mtsummit-papers.15,1,0.910881,"are generated by the decoder and the 1-best translation is returned after rescored with additional feature functions. e,a m =1 where e is a string of phrases in the target language, f is the source language string of phrases, Figure 2: Structure of a three-pass machine translation system with the new regeneration pass. The original N-best translations list (Nbest1) is expanded to generate a new N-best translations list (N-best2) before the rescoring pass. lation and reordering models that are trained on the source-to-target N-best translations generated in the first pass. N-gram expansion (Chen et al., 2007) regenerates more hypotheses by continuously expanding the partial hypotheses through an n-gram language model trained on the original N-best translations. And confusion network generates new hypotheses based on confusion network decoding (Matusov et al., 2006), where the confusion network is built on the original N-best translations. Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b). Researchers have used confusion network to compute cons"
C08-1014,N03-1017,0,0.0070794,"le system. We explore three different methods to implement the regeneration process: redecoding, n-gram expansion, and confusion network-based regeneration. Experiments on Chinese-to-English NIST and IWSLT tasks show that all three methods obtain consistent improvements. Moreover, the combination of the three strategies achieves further improvements and outperforms the baseline by 0.81 BLEU-score on IWSLT’06, 0.57 on NIST’03, 0.61 on NIST’05 test set respectively. 1 Introduction State-of-the-art Statistical Machine Translation (SMT) systems usually adopt a two-pass search strategy (Och, 2003; Koehn, et al., 2003) as shown in Figure 1. In the first pass, a decoding algorithm is applied to generate an N-best list of translation hypotheses, while in the second pass, the final translation is selected by rescoring and re-ranking the N-best translations through additional feature functions. The fundamental assumption behind using a second pass is that the generated N-best list may contain better transla© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. tions than the best choice foun"
C08-1014,takezawa-etal-2002-toward,0,0.0637026,"ure (Chen et al., 2005) • linear sum of n-grams relative frequencies within N-best translations (Chen et al., 2005) • n-gram posterior probabilities within the Nbest translations (Zens and Ney, 2006) • sentence length posterior probabilities (Zens and Ney, 2006) 108 5 Experiments 5.1 data Tasks Train We carried out two sets of experiments on two different datasets. One is in spoken language domain while the other is on newswire corpus. Both experiments are on Chinese-to-English translation. Experiments on spoken language domain were carried out on the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2002) Chinese- to-English data augmented with HITcorpus 1 . BTEC is a multilingual speech corpus which contains sentences spoken by tourists. 40K sentence-pairs are used in our experiment. HITcorpus is a balanced corpus and has 500K sentence-pairs in total. We selected 360K sentencepairs that are more similar to BTEC data according to its sub-topic. Additionally, the English sentences of Tanaka corpus 2 were also used to train our LM. We ran experiments on an IWSLT 3 challenge track which uses IWSLT2006 4 DEV clean text set as development set and IWSLT-2006 TEST clean text as test set. Table 1 summ"
C08-1014,W06-3110,0,0.342533,"stem can be improved from two aspects, i.e. scoring models and the quality of the N-best hypotheses. Rescoring pass improves the performance of machine translation by enhancing the scoring models with more global sophisticated and discriminative feature functions. The idea for applying two passes instead of one is that some global feature functions cannot be easily decomposed into local scores and computed during decoding. Furthermore, rescoring allows some feature functions, such as word and n-gram posterior probabilities, to be estimated on the N-best list (Ueffing, 2003; Chen et al., 2005; Zens and Ney, 2006). In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b). We have instead chosen to regenerate new hypotheses from the original N-best list, a technique which we call regeneration. Regeneration is an intermediate pass bet"
C08-1014,P07-2045,0,0.0336091,"Missing"
C08-1014,C04-1183,1,0.757767,"are ranked, choosing the first best hypothesis as the skeleton is straightforward in our work. Aligning words: As a confusion network can be easily built from a one-to-one alignment, we develop our algorithm based on the one-to-one assumption and use competitive linking algorithm (Melamed, 2000) for our word alignment. Firstly, an association score is computed for every possible word pair from the skeleton and sentence to be aligned. Then a greedy algorithm is applied to select the best word-alignment. In this paper, we use a linear combination of multiple association scores, as suggested in (Kraif and Chen, 2004). As the two sentences to be aligned are in the same language, the association scores are computed on the following four clues. They are cognate (S1), word class (S2), synonyms (S3), and position difference (S4). The four scores are linearly combined with empirically determined weights as shown is Equation 2. 4 S ( f j , e i ) = ∑ λk × S k (2) k =1 Reordering words: After word alignment, the words in all other hypotheses are reordered to match the word order of the skeleton. The aligned words are reordered according to their alignment indices. The unaligned words are reordered in two strategie"
C08-1014,E06-1005,0,0.607335,"posed into local scores and computed during decoding. Furthermore, rescoring allows some feature functions, such as word and n-gram posterior probabilities, to be estimated on the N-best list (Ueffing, 2003; Chen et al., 2005; Zens and Ney, 2006). In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b). We have instead chosen to regenerate new hypotheses from the original N-best list, a technique which we call regeneration. Regeneration is an intermediate pass between decoding and rescoring as depicted in Figure 2. Given the original N-best list (N-best1) generated by the decoder, this regeneration pass creates new translation hypotheses from this list to form another N-best list (N-best2). These two N-best lists are then combined and given to the rescoring pass to derive the best translation. We implement three methods to regener"
C08-1014,J00-2004,0,0.0187612,"ther hypotheses on average as the skeleton. Bangalore et al. (2001) used a WER based alignment and Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate 107 (TER) based alignment to build the confusion network. Choosing alignment reference: Since the Nbest translations are ranked, choosing the first best hypothesis as the skeleton is straightforward in our work. Aligning words: As a confusion network can be easily built from a one-to-one alignment, we develop our algorithm based on the one-to-one assumption and use competitive linking algorithm (Melamed, 2000) for our word alignment. Firstly, an association score is computed for every possible word pair from the skeleton and sentence to be aligned. Then a greedy algorithm is applied to select the best word-alignment. In this paper, we use a linear combination of multiple association scores, as suggested in (Kraif and Chen, 2004). As the two sentences to be aligned are in the same language, the association scores are computed on the following four clues. They are cognate (S1), word class (S2), synonyms (S3), and position difference (S4). The four scores are linearly combined with empirically determi"
C08-1014,P03-1021,0,0.120474,"from a single system. We explore three different methods to implement the regeneration process: redecoding, n-gram expansion, and confusion network-based regeneration. Experiments on Chinese-to-English NIST and IWSLT tasks show that all three methods obtain consistent improvements. Moreover, the combination of the three strategies achieves further improvements and outperforms the baseline by 0.81 BLEU-score on IWSLT’06, 0.57 on NIST’03, 0.61 on NIST’05 test set respectively. 1 Introduction State-of-the-art Statistical Machine Translation (SMT) systems usually adopt a two-pass search strategy (Och, 2003; Koehn, et al., 2003) as shown in Figure 1. In the first pass, a decoding algorithm is applied to generate an N-best list of translation hypotheses, while in the second pass, the final translation is selected by rescoring and re-ranking the N-best translations through additional feature functions. The fundamental assumption behind using a second pass is that the generated N-best list may contain better transla© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. tions tha"
C08-1014,J03-1002,0,0.00995435,"Missing"
C08-1014,P02-1040,0,0.0763935,"Missing"
C08-1014,N07-1029,0,0.441059,"ing decoding. Furthermore, rescoring allows some feature functions, such as word and n-gram posterior probabilities, to be estimated on the N-best list (Ueffing, 2003; Chen et al., 2005; Zens and Ney, 2006). In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b). We have instead chosen to regenerate new hypotheses from the original N-best list, a technique which we call regeneration. Regeneration is an intermediate pass between decoding and rescoring as depicted in Figure 2. Given the original N-best list (N-best1) generated by the decoder, this regeneration pass creates new translation hypotheses from this list to form another N-best list (N-best2). These two N-best lists are then combined and given to the rescoring pass to derive the best translation. We implement three methods to regenerate new hypotheses: re-decoding, n-gra"
C08-1014,P07-1040,0,0.487111,"ing decoding. Furthermore, rescoring allows some feature functions, such as word and n-gram posterior probabilities, to be estimated on the N-best list (Ueffing, 2003; Chen et al., 2005; Zens and Ney, 2006). In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b). We have instead chosen to regenerate new hypotheses from the original N-best list, a technique which we call regeneration. Regeneration is an intermediate pass between decoding and rescoring as depicted in Figure 2. Given the original N-best list (N-best1) generated by the decoder, this regeneration pass creates new translation hypotheses from this list to form another N-best list (N-best2). These two N-best lists are then combined and given to the rescoring pass to derive the best translation. We implement three methods to regenerate new hypotheses: re-decoding, n-gra"
C08-1127,P05-1033,0,0.10108,"model. We also present an annotation algorithm that captures syntactic information for BTG nodes. The experiments show that the LABTG approach significantly outperforms a baseline BTGbased system and a state-of-the-art phrasebased system on the NIST MT-05 Chineseto-English translation task. Moreover, we empirically demonstrate that the proposed method achieves better translation selection and phrase reordering. 1 Introduction Formal grammar used in statistical machine translation (SMT), such as Bracketing Transduction Grammar (BTG) proposed by (Wu, 1997) and the synchronous CFG presented by (Chiang, 2005), provides a natural platform for integrating linguistic knowledge into SMT because hierarchical structures produced by the formal grammar resemble linguistic structures.1 Chiang (2005) attempts to integrate linguistic information into his formally c 2008.  Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 We inherit the definitions of formal and linguistic from (Chiang, 2005) which makes a distinction between formally syntax-based SMT and linguistically syntax-based SMT."
C08-1127,P05-1067,0,0.0858963,"Missing"
C08-1127,P03-2041,0,0.0607656,"dge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is significant because we annotate standard plain phrases using lingui"
C08-1127,P06-1077,0,0.0216025,"and scheme are capable of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is significant becaus"
C08-1127,W06-1606,0,0.021123,"016 Manchester, August 2008 annotated reordering model. The second is that our proposed annotation algorithm and scheme are capable of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain"
C08-1127,P00-1056,0,0.208745,"prior probability which can be set based on the order preference of the language pairs. In MEBTG (Xiong et al., 2006), however, the probability is calculated more sophisticatedly using a MaxEnt-based classification model with boundary words as its features. by projecting source-side syntax tree to BTG tree, and finally extract rules from these annotated BTG trees. This way restricts learning space to only the best BTG trees2 , and leads to the loss of many useful annotated rules. Therefore, we use an alternative way to extract the annotated rules as illustrated below. Firstly, we run GIZA++ (Och and Ney, 2000) on the training corpus in both directions and then apply the ogrow-diag-finalp refinement rule (Koehn et al., 2003) to obtain many-to-many word alignments. Secondly, we extract bilingual phrases from the word-aligned corpus, then annotate their source sides with linguistic elements to obtain the annotated lexical rules.3 Finally, we learn reordering examples (Xiong et al., 2006), annotate their two neighboring sub-phrases and whole phrases, and then generalize them in the annotated merging rules. Although this alternative way may also miss reorderings due to word alignment errors, it is still"
C08-1127,P03-1021,0,0.0302069,"features for the reordering model PRb (shared by both MEBTG and LABTG systems) using the right boundary words of phrases and 85K features for the annotated reordering model PRa (only included in the LABTG system) using linguistic annotations. We ran the MaxEnt toolkit (Zhang, 2004) to tune reordering feature weights with iteration number being set to 100 and Gaussian prior to 1 to avoid overfitting. We built our four-gram language model using Xinhua section of the English Gigaword corpus (181.1M words) with the SRILM toolkit (Stolcke, 2002). For the efficiency of minimum-error-rate training (Och, 2003), we built our development set (580 sentences) using sentences not exceeding 50 characters from the NIST MT-02 evaluation test data. 5.1 LABTG vs. phrase-based SMT and BTG-based SMT We compared the LABTG system with two baseline systems. The results are given in Table 2. The LABTG outperforms Moses and MEBTG by 2.81 and 1.69 absolute BLEU points, respectively. These significant improvements indicate that BTG formal structures can be successfully extended with linguistic knowledge extracted from syntactic structures without losing the strength of phrasebased method. 5.2 The Effect of Different"
C08-1127,P05-1034,0,0.0431644,"nnotation algorithm and scheme are capable of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is"
C08-1127,P07-1090,1,0.871143,"significant because we annotate standard plain phrases using linguistic elements on the source side. Compared with the target side annotation which improves fluency and grammaticality of translation output, linguistic annotation on the source side helps to improve translation adequacy. Recently, some researchers have extended and created several variations of BTG/ITG. Zhang et al. (2005) propose lexicalized ITG for better word alignment. Xiong et al. (2006) demonstrate that their MEBTG, a BTG variation with MaxEntbased reordering model, can improve phrase reordering significantly. Similarly, Setiawan et al. (2007) use an enhanced BTG variation with function words for reordering. LABTG differs from these BTG variations in that the latter does not use any external linguistic knowledge. Zhang et al. (2007) describe a phrase reordering model based on BTG-style rules which integrates source-side syntactic knowledge. Our annotated reordering model of LABTG differs from their work in two key aspects. Firstly, we allow any phrase reorderings while they only reorder syntactic phrases. In their model, only syntactic phrases can use linguistic knowledge from parse trees for reordering while non-syntactic phrases"
C08-1127,P96-1021,0,0.76653,"labelling both syntactic and non-syntactic phrases. The linguistic elements extracted from parse trees capture both internal lexical content and external context of phrases. With these linguistic annotations, we expect the LABTG to address two traditional issues of standard phrase-based SMT (Koehn et al., 2003) in a more effective manner. They are (1) phrase translation: translating phrases according to their contexts; (2) phrase reordering: incorporating richer linguistic features for better reordering. The proposed LABTG displays two unique characteristics when compared with BTG-based SMT (Wu, 1996; Xiong et al., 2006). The first is that two linguistically-informed sub-models are introduced for better phrase translation and reordering: annotated phrase translation model and 1009 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1009–1016 Manchester, August 2008 annotated reordering model. The second is that our proposed annotation algorithm and scheme are capable of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain t"
C08-1127,J97-3002,0,0.63124,"otivated phrase translation model and reordering model. We also present an annotation algorithm that captures syntactic information for BTG nodes. The experiments show that the LABTG approach significantly outperforms a baseline BTGbased system and a state-of-the-art phrasebased system on the NIST MT-05 Chineseto-English translation task. Moreover, we empirically demonstrate that the proposed method achieves better translation selection and phrase reordering. 1 Introduction Formal grammar used in statistical machine translation (SMT), such as Bracketing Transduction Grammar (BTG) proposed by (Wu, 1997) and the synchronous CFG presented by (Chiang, 2005), provides a natural platform for integrating linguistic knowledge into SMT because hierarchical structures produced by the formal grammar resemble linguistic structures.1 Chiang (2005) attempts to integrate linguistic information into his formally c 2008.  Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 We inherit the definitions of formal and linguistic from (Chiang, 2005) which makes a distinction between formally sy"
C08-1127,P07-1037,0,0.0285055,"Missing"
C08-1127,I05-1007,1,0.844533,"Missing"
C08-1127,N06-1031,0,0.0770157,"in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is significant because we annotate standard plain phrases using linguistic elements on the source side. Compared with the target side annotation which improves fluency and grammaticality of translation output, linguistic annotation on the source side helps to improve translation adequacy. Rece"
C08-1127,P06-1066,1,0.861375,"both syntactic and non-syntactic phrases. The linguistic elements extracted from parse trees capture both internal lexical content and external context of phrases. With these linguistic annotations, we expect the LABTG to address two traditional issues of standard phrase-based SMT (Koehn et al., 2003) in a more effective manner. They are (1) phrase translation: translating phrases according to their contexts; (2) phrase reordering: incorporating richer linguistic features for better reordering. The proposed LABTG displays two unique characteristics when compared with BTG-based SMT (Wu, 1996; Xiong et al., 2006). The first is that two linguistically-informed sub-models are introduced for better phrase translation and reordering: annotated phrase translation model and 1009 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1009–1016 Manchester, August 2008 annotated reordering model. The second is that our proposed annotation algorithm and scheme are capable of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we es"
C08-1127,2006.amta-papers.8,0,0.0165304,"able of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is significant because we annotate standar"
C08-1127,N03-1017,0,0.134062,"c parse trees of source or target language. Along this line, we propose a novel approach: Linguistically Annotated BTG (LABTG) for SMT. The LABTG annotates BTG rules with linguistic elements that are learned from syntactic parse trees on the source side through an annotation algorithm, which is capable of labelling both syntactic and non-syntactic phrases. The linguistic elements extracted from parse trees capture both internal lexical content and external context of phrases. With these linguistic annotations, we expect the LABTG to address two traditional issues of standard phrase-based SMT (Koehn et al., 2003) in a more effective manner. They are (1) phrase translation: translating phrases according to their contexts; (2) phrase reordering: incorporating richer linguistic features for better reordering. The proposed LABTG displays two unique characteristics when compared with BTG-based SMT (Wu, 1996; Xiong et al., 2006). The first is that two linguistically-informed sub-models are introduced for better phrase translation and reordering: annotated phrase translation model and 1009 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1009–1016 Manchester,"
C08-1127,D07-1091,0,0.0285451,"tures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is significant because we annotate standard plain phrases using linguistic elements on the source side. Compared"
C08-1127,P07-2045,0,0.0111875,"Missing"
C08-1127,D07-1056,0,0.267646,"ause we annotate standard plain phrases using linguistic elements on the source side. Compared with the target side annotation which improves fluency and grammaticality of translation output, linguistic annotation on the source side helps to improve translation adequacy. Recently, some researchers have extended and created several variations of BTG/ITG. Zhang et al. (2005) propose lexicalized ITG for better word alignment. Xiong et al. (2006) demonstrate that their MEBTG, a BTG variation with MaxEntbased reordering model, can improve phrase reordering significantly. Similarly, Setiawan et al. (2007) use an enhanced BTG variation with function words for reordering. LABTG differs from these BTG variations in that the latter does not use any external linguistic knowledge. Zhang et al. (2007) describe a phrase reordering model based on BTG-style rules which integrates source-side syntactic knowledge. Our annotated reordering model of LABTG differs from their work in two key aspects. Firstly, we allow any phrase reorderings while they only reorder syntactic phrases. In their model, only syntactic phrases can use linguistic knowledge from parse trees for reordering while non-syntactic phrases"
C08-1127,P05-1059,0,0.11755,"Missing"
C08-1127,W06-3119,0,0.0229064,"rdering model. The second is that our proposed annotation algorithm and scheme are capable of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The differ"
C08-1138,koen-2004-pharaoh,0,0.150361,"rivations3 that could lead to the same target tree T (e1I ) , the mapping probability Pr (T (e1I ) |T ( f1J )) is obtained by summing over the probabilities of all derivations. The probability of each derivation θ is given by the product of the probabilities of all the rules p (ri ) used in the derivation (here we assume that a rule is applied independently in a derivation). Pr (e1I |f1J ) = Pr (T (e1I ) |T ( f1J )) = ∑∏ p (ri ) θ (1) ri ∈θ The model is implemented under log-linear framework. We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2004): 1) bidirectional rule mapping probabilities; 2) bidirectional lexical translation probabilities; 3) the target language model; 4) the number of rules used and 5) the number of target words. Besides, we define two new features: 1) the number of lexical words in a rule to control the model’s preference for lexicalized rules over un-lexicalized rules and 2) the average tree height in a rule to balance the usage of hierarchical rules and more flat rules. 2) SCFG-based tree-to-tree model when α s = The overall training process is similar to the process in the phrase-based system (koehn et al., α"
C08-1138,J93-2003,0,0.00973221,"mainly de© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. termined by the grammar. Many grammars, such as finite-state grammars (FSG), bracket/inversion transduction grammars (BTG/ITG) (Wu, 1997), context-free grammar (CFG), tree substitution grammar (TSG) (Comon et al., 2007) and their synchronous versions, have been explored in SMT. Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Cowan et al., 2006; Eisner, 2003; Ding and Palmer, 2005; Zhang et al., 2007; Bod, 2007; Quirk wt al., 2005; Poutsma, 2000; Hearne and Way, 2003) and so on. Although many achievements have been obtained by these advances, it is still unclear which of these important pursuits is able to best explai"
C08-1138,P07-2045,0,0.00915683,"s our development set and the NIST MT-2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ and the heuristics “grow-diagfinal” to generate m-to-n word alignments. For the MER training, we modified Koehn’s MER trainer (Koehn, 2004) for our STSSG-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We compared four SMT systems: Moses (Koehn et al., 2007), SCFG-based, STSG-based and STSSGbased tree-to-tree translation models. For Moses, we used its default settings. For the others, we implemented them on the STSSG platform by adopting the same settings as used in the synchronous parsing. We optimized the decoding parameters on the development sets empirically. 4.2 Experimental Results SCFG STSG a larger span than SCFG. It reconfirms that only allowing sibling nodes reordering as done in SCFG may be inadequate for translational equivalence modeling (Galley et al., 2004)4. 3) All the three models on the FBIS corpus show much lower performance th"
C08-1138,P06-1077,0,0.629179,"nported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. termined by the grammar. Many grammars, such as finite-state grammars (FSG), bracket/inversion transduction grammars (BTG/ITG) (Wu, 1997), context-free grammar (CFG), tree substitution grammar (TSG) (Comon et al., 2007) and their synchronous versions, have been explored in SMT. Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Cowan et al., 2006; Eisner, 2003; Ding and Palmer, 2005; Zhang et al., 2007; Bod, 2007; Quirk wt al., 2005; Poutsma, 2000; Hearne and Way, 2003) and so on. Although many achievements have been obtained by these advances, it is still unclear which of these important pursuits is able to best explain human translation data, as each has its advantages and disadvantages. Therefore, it has grea"
C08-1138,P07-1089,0,0.805014,"ttp://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. termined by the grammar. Many grammars, such as finite-state grammars (FSG), bracket/inversion transduction grammars (BTG/ITG) (Wu, 1997), context-free grammar (CFG), tree substitution grammar (TSG) (Comon et al., 2007) and their synchronous versions, have been explored in SMT. Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Cowan et al., 2006; Eisner, 2003; Ding and Palmer, 2005; Zhang et al., 2007; Bod, 2007; Quirk wt al., 2005; Poutsma, 2000; Hearne and Way, 2003) and so on. Although many achievements have been obtained by these advances, it is still unclear which of these important pursuits is able to best explain human translation data, as each has its advantages and disadvantages. Therefore, it has great meaning in both t"
C08-1138,J94-4004,0,0.194206,"Missing"
C08-1138,W02-1039,0,0.253408,"ous factors in this study, including the genera of corpora (newspaper domain via spoken domain), the accuracy of word alignments and syntax parsing (automatically vs. manually). We report our experimental settings, experimental results and our findings in detail in the rest of the paper, which is organized as follows: Section 2 reviews previous work. Section 3 elaborates the general framework while Section 4 reports the experimental results. Finally, we conclude our work in Section 5. 2 Previous Work There are only a few of previous work related to the study of translation grammar comparison. Fox (2002) is the first to look at how well proposed translation models fit actual translation data empirically. She examined the issue of phrasal cohesion between English and French and discovered that while there is less cohesion than one might desire, there is still a large amount of regularity in the constructions where breakdowns occur. This suggests that reordering words by phrasal movement is a reasonable strategy (Fox, 2002). She has also examined the differences in cohesion between Treebank-style parse trees, trees with flattened verb phrases, and dependency structures. Their experimental resul"
C08-1138,P06-1121,0,0.093015,"Missing"
C08-1138,P04-1083,0,0.0235041,"arget ones. To speed up the decoder, we utilize several thresholds to limit the search beams for each span, such as the number of rules used and the number of hypotheses generated. 3.4 Synchronous Parsing A synchronous parser is an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the correspondence relation between these structures. When a parser’s input can have fewer dimensions than the parser’s grammar, we call it a translator. When a parser’s grammar can have fewer dimensions than the parser’s input, we call it a synchronizer (Melamed, 2004). Therefore, synchronous parsing and MT are closed to each other. In this paper, we use synchronous parsing to compare the ability of different grammars in translational equivalence modeling. Given a bilingual sentence pair f1J and e1I , the synchronous parser is to find a derivation θ that generates &lt; T ( f1J ) , T (e1I ) >. Our synchronous parser is similar to the synchronous CKY parser presented at (Melamed, 2004). The difference is that we implement it based on our STSSG decoder. Therefore, in nature the parser is a standard synchronous chart parser but constrained by the rules of the STSS"
C08-1138,P05-1034,0,0.108909,"Missing"
C08-1138,N04-1035,0,0.716904,"cohesion between English and French and discovered that while there is less cohesion than one might desire, there is still a large amount of regularity in the constructions where breakdowns occur. This suggests that reordering words by phrasal movement is a reasonable strategy (Fox, 2002). She has also examined the differences in cohesion between Treebank-style parse trees, trees with flattened verb phrases, and dependency structures. Their experimental results indicate that the highest degree of cohesion is present in dependency structures. Motivated by the same problem raised by Fox (2002), Galley et al. (2004) study what rule can better explain human translation data. They first propose a theory that gives formal semantics to word-level alignments defined over parallel corpora, and then use the theory to introduce a linear algorithm that is used to derive from wordaligned, parallel corpora the minimal set of syntactically motivated transformation rules to explain human translation data. Their basic idea is to create transformation rules that condition on larger fragments of tree structure. Their experimental results suggest that their proposed rules provide a good, realistic indicator of the comple"
C08-1138,J97-3002,0,0.252765,"process to describe and build these alignments using mathematical models. Thus, the study of TEM is highly relevant to Statistical Machine Translation (SMT). Grammar is the most important infrastructure for TEM and SMT since translation models’ expressive and generative abilities are mainly de© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. termined by the grammar. Many grammars, such as finite-state grammars (FSG), bracket/inversion transduction grammars (BTG/ITG) (Wu, 1997), context-free grammar (CFG), tree substitution grammar (TSG) (Comon et al., 2007) and their synchronous versions, have been explored in SMT. Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Co"
C08-1138,P01-1067,0,0.0407236,"ghts reserved. termined by the grammar. Many grammars, such as finite-state grammars (FSG), bracket/inversion transduction grammars (BTG/ITG) (Wu, 1997), context-free grammar (CFG), tree substitution grammar (TSG) (Comon et al., 2007) and their synchronous versions, have been explored in SMT. Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Cowan et al., 2006; Eisner, 2003; Ding and Palmer, 2005; Zhang et al., 2007; Bod, 2007; Quirk wt al., 2005; Poutsma, 2000; Hearne and Way, 2003) and so on. Although many achievements have been obtained by these advances, it is still unclear which of these important pursuits is able to best explain human translation data, as each has its advantages and disadvantages. Therefore, it has great meaning in both theory and practice to do comparison studies among these grammar"
C08-1138,2003.mtsummit-papers.22,0,0.0433645,"een explored in SMT. Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Cowan et al., 2006; Eisner, 2003; Ding and Palmer, 2005; Zhang et al., 2007; Bod, 2007; Quirk wt al., 2005; Poutsma, 2000; Hearne and Way, 2003) and so on. Although many achievements have been obtained by these advances, it is still unclear which of these important pursuits is able to best explain human translation data, as each has its advantages and disadvantages. Therefore, it has great meaning in both theory and practice to do comparison studies among these grammars and SMT models to see which of them are capable of better describing parallel translation data. This is a fundamental issue worth exploring in multilingual information processing. However, little effort in previous work has been put in this point. To address this issue"
C08-1138,zhang-etal-2004-interpreting,0,0.0967943,"less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT-2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ and the heuristics “grow-diagfinal” to generate m-to-n word alignments. For the MER training, we modified Koehn’s MER trainer (Koehn, 2004) for our STSSG-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We compared four SMT systems: Moses (Koehn et al., 2007), SCFG-based, STSG-based and STSSGbased tree-to-tree translation models. For Moses, we used its default settings. For the others, we implemented them on the STSSG platform by adopting the same settings as used in the synchronous parsing. We optimized the decoding parameters on the development sets empirically. 4.2 Experimental Results SCFG STSG a larger span than SCFG. It reconfirms that only allowing sibling nodes reordering as done in SCFG may be inadequate for translational equivalence modeling (Galley et al., 2004)4. 3) All the thre"
C08-1138,2006.amta-papers.8,0,\N,Missing
C08-1138,2007.mtsummit-papers.8,0,\N,Missing
C08-1138,P00-1057,0,\N,Missing
C08-1138,C00-2092,0,\N,Missing
C08-1138,P03-1054,0,\N,Missing
C08-1138,P02-1040,0,\N,Missing
C08-1138,P06-1123,0,\N,Missing
C08-1138,P05-1067,0,\N,Missing
C08-1138,P03-2041,0,\N,Missing
C08-1138,W06-1628,0,\N,Missing
C08-1138,W06-1606,0,\N,Missing
C08-1138,P05-1033,0,\N,Missing
C08-1138,N03-1017,0,\N,Missing
C10-1118,D08-1092,0,0.0217423,"search helps avoid this asymmetric problem. 4 Feature Functions In this section, we introduce a variety of feature functions to capture the semantically equivalent 1048 counterparts and structural divergence across languages. For the semantic equivalence, we define lexical and word alignment feature functions. Since those feature functions are directional, we describe most of these functions as conditional feature functions based on the conditional lexical probabilities. We also introduce the tree structural features to deal with the structural divergence of bilingual parse trees. Inspired by Burkett and Klein (2008), we introduce the feature functions in an internal-external manner based on the fact that the feature scores for an aligned sub-tree pair tend to be high inside both sub-trees, while they tend to be low inside one sub-tree and outside the other. 4.1 ∑ ( |) ∑ (∏ ) ( Although the word alignment information within bilingual sentence pairs is to some extent not reliable, the links of word alignment account much for the co-occurrence of the aligned terms. We define the internal word alignment features as follows: ( ∑ ) ( ) ( ) ( |) ( ( ) {( ) | ∑ ( ) ) {( | ) ( )|) 4.4 Internal-External Word Align"
C10-1118,P06-1121,0,0.026109,"idely used word alignment. Experimental results on benchmark data show that sub-tree alignment benefits both systems by relaxing the constraint of the word alignment. 1 Introduction Recent research in Statistical Machine Translation (SMT) tends to incorporate more linguistically grammatical information into the translation model known as linguistically motivated syntax-based models. To develop such models, the phrasal structure parse tree is usually adopted as the representation of bilingual sentence pairs either on the source side (Huang et al., 2006; Liu et al., 2006) or on the target side (Galley et al., 2006; Marcu et al., 2006), or even on both sides (Graehl and Knight, 2004; Zhang et al., 2007). Most of the above models either construct a pipeline to transform from/to tree structure, or synchronously generate two trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rule extraction has difficulty in capturing such non-local phenom"
C10-1118,N04-1014,0,0.0133631,"show that sub-tree alignment benefits both systems by relaxing the constraint of the word alignment. 1 Introduction Recent research in Statistical Machine Translation (SMT) tends to incorporate more linguistically grammatical information into the translation model known as linguistically motivated syntax-based models. To develop such models, the phrasal structure parse tree is usually adopted as the representation of bilingual sentence pairs either on the source side (Huang et al., 2006; Liu et al., 2006) or on the target side (Galley et al., 2006; Marcu et al., 2006), or even on both sides (Graehl and Knight, 2004; Zhang et al., 2007). Most of the above models either construct a pipeline to transform from/to tree structure, or synchronously generate two trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rule extraction has difficulty in capturing such non-local phenomena and will fully propagate the word alignment error to the later st"
C10-1118,C04-1154,0,0.0821934,"agate the word alignment error to the later stage of rule extraction. Alternatively, some initial attempts have been made to directly conduct syntactic structure alignment. As mentioned in Tinsley et al. (2007), the early work usually constructs the structure alignment by hand, which is time-consuming. Recent research tries to automatically align the bilingual syntactic sub-trees. However, most of these works suffer from the following problems. Firstly, the alignment is conducted based on heuristic rules, which may lose extensibility and generality in spite of accommodating some common cases (Groves et al., 2004). Secondly, various similarity computation methods are used based merely on lexical translation probabilities (Tinsley et al., 2007; Imamura, 2001) regardless of structural features. We believe the structure information is an important issue to capture the non-local structural divergence of languages by modeling beyond the plain text. To address the above issues, we present a statistical framework based on Maximum Entropy (MaxEnt) model. Specifically, we consider subtree alignment as a binary classification problem and use Maximum Entropy model to classify each instance as aligned or unaligned"
C10-1118,2006.amta-papers.8,0,0.0488805,"T systems. We then compare the performance with that of the widely used word alignment. Experimental results on benchmark data show that sub-tree alignment benefits both systems by relaxing the constraint of the word alignment. 1 Introduction Recent research in Statistical Machine Translation (SMT) tends to incorporate more linguistically grammatical information into the translation model known as linguistically motivated syntax-based models. To develop such models, the phrasal structure parse tree is usually adopted as the representation of bilingual sentence pairs either on the source side (Huang et al., 2006; Liu et al., 2006) or on the target side (Galley et al., 2006; Marcu et al., 2006), or even on both sides (Graehl and Knight, 2004; Zhang et al., 2007). Most of the above models either construct a pipeline to transform from/to tree structure, or synchronously generate two trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rul"
C10-1118,2007.mtsummit-papers.62,0,0.543965,"trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rule extraction has difficulty in capturing such non-local phenomena and will fully propagate the word alignment error to the later stage of rule extraction. Alternatively, some initial attempts have been made to directly conduct syntactic structure alignment. As mentioned in Tinsley et al. (2007), the early work usually constructs the structure alignment by hand, which is time-consuming. Recent research tries to automatically align the bilingual syntactic sub-trees. However, most of these works suffer from the following problems. Firstly, the alignment is conducted based on heuristic rules, which may lose extensibility and generality in spite of accommodating some common cases (Groves et al., 2004). Secondly, various similarity computation methods are used based merely on lexical translation probabilities (Tinsley et al., 2007; Imamura, 2001) regardless of structural features. We beli"
C10-1118,2007.mtsummit-papers.71,1,0.820315,"ment benefits both systems by relaxing the constraint of the word alignment. 1 Introduction Recent research in Statistical Machine Translation (SMT) tends to incorporate more linguistically grammatical information into the translation model known as linguistically motivated syntax-based models. To develop such models, the phrasal structure parse tree is usually adopted as the representation of bilingual sentence pairs either on the source side (Huang et al., 2006; Liu et al., 2006) or on the target side (Galley et al., 2006; Marcu et al., 2006), or even on both sides (Graehl and Knight, 2004; Zhang et al., 2007). Most of the above models either construct a pipeline to transform from/to tree structure, or synchronously generate two trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rule extraction has difficulty in capturing such non-local phenomena and will fully propagate the word alignment error to the later stage of rule extractio"
C10-1118,P08-1064,1,0.808428,"we use Moses (Koehn et al, 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in an SMT system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement the STSG/STSSG based model in Pisces decoder with the same features and settings in Sun et al. (2009). The STSSG based decoder translates each span iteratively in a bottom up manner which guarantees that when translating a source span, any of its sub-spans has already been translated. The STSG based experiment can be easily achieved by restricting the translation rule set in the STSSG decoder to be elementary tree pairs only. For the alignment setting of the baselines, we use the word alignment trained on the entire FBIS(240k) corpus by GIZA++ with heuristic grow-diag-final for"
C10-1118,P03-1054,0,0.0499734,"/index.php/resources.html . We licensed the corpus from them for research usage. 1050 experiment due to problems of domain divergence, annotation discrepancy (Chinese parse tree adopts a different grammar from Penn Treebank annotations) and degree of tolerance for parsing errors. Due to the above issues, we annotate a new data set to apply the sub-tree alignment in machine translation. We randomly select 300 bilingual sentence pairs from the Chinese-English FBIS corpus with the length in both the source and target sides. The selected plain sentence pairs are further parsed by Stanford parser (Klein and Manning, 2003) on both the English and Chinese sides. We manually annotate the sub-tree alignment for the automatically parsed tree pairs according to the definition in Section 2. To be fully consistent with the definition, we strictly preserve the semantic equivalence for the aligned sub-trees to keep a high precision. In other words, we do not conduct any doubtful links. The corpus is further divided into 200 aligned tree pairs for training and 100 for testing. Some initial statistic of the automatically parsed corpus is shown in Table 2. Train Test # of Sentence pair Avg. Sentence Length Avg. # of sub-tr"
C10-1118,P07-2045,0,0.00569676,"f data to train word alignment b. Precision/Recall for HIT test set. c. F-score for HIT test set. d. Precision/Recall for FBIS test set. e. F-score for FBIS test set. sentences with less than 50 characters from the NIST MT-2002 test set as the development set (to speed up tuning for syntax based system) and the NIST MT-2005 test set as our test set. We use the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is case-sensitive BLEU-4. For the phrase based system, we use Moses (Koehn et al, 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in an SMT system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement"
C10-1118,N03-1017,0,0.00642613,"le set in the STSSG decoder to be elementary tree pairs only. For the alignment setting of the baselines, we use the word alignment trained on the entire FBIS(240k) corpus by GIZA++ with heuristic grow-diag-final for Moses and the syntax systems and perform rule extraction constrained on the word alignment. As for the experiments adopting sub-tree alignment, we use the above word alignment to learn lexical/word alignment features, and train the sub-tree alignment model with FBIS training data (200). 7.2 Experimental results Utilizing the syntactic rules only has been argued to be ineffective (Koehn et al., 2003). Therefore, instead of using the sub-tree aligned rules only, we try to improve the word alignment constrained rule set by sub-tree alignment as shown in Table 5. Firstly, we try to Directly Concatenate (DirC) the sub-tree alignment constraint rule set 3 to the original syntax/phrase rule set based on word alignment. Then we re-train the MT model based 3 For syntax based system, it’s just the sub-tree pairs deducted from the sub-tree alignment; for phrase based system, it&apos;s the phrases with context equivalent to the aligned sub-tree pairs. 1053 250 PP3 IP2 P CP VP VP NP DEC AD VV NR 反对 以色列的 非"
C10-1118,P06-1077,0,0.0530366,"ompare the performance with that of the widely used word alignment. Experimental results on benchmark data show that sub-tree alignment benefits both systems by relaxing the constraint of the word alignment. 1 Introduction Recent research in Statistical Machine Translation (SMT) tends to incorporate more linguistically grammatical information into the translation model known as linguistically motivated syntax-based models. To develop such models, the phrasal structure parse tree is usually adopted as the representation of bilingual sentence pairs either on the source side (Huang et al., 2006; Liu et al., 2006) or on the target side (Galley et al., 2006; Marcu et al., 2006), or even on both sides (Graehl and Knight, 2004; Zhang et al., 2007). Most of the above models either construct a pipeline to transform from/to tree structure, or synchronously generate two trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rule extraction has di"
C10-1118,W06-1606,0,0.0257243,"ment. Experimental results on benchmark data show that sub-tree alignment benefits both systems by relaxing the constraint of the word alignment. 1 Introduction Recent research in Statistical Machine Translation (SMT) tends to incorporate more linguistically grammatical information into the translation model known as linguistically motivated syntax-based models. To develop such models, the phrasal structure parse tree is usually adopted as the representation of bilingual sentence pairs either on the source side (Huang et al., 2006; Liu et al., 2006) or on the target side (Galley et al., 2006; Marcu et al., 2006), or even on both sides (Graehl and Knight, 2004; Zhang et al., 2007). Most of the above models either construct a pipeline to transform from/to tree structure, or synchronously generate two trees in parallel (i.e., synchronous parsing). Both cases require syntactically rich translational equivalences to handle non-local reordering. However, most current works obtain the syntactic translational equivalences by initially conducting alignment on the word level. To employ word alignment as a hard constraint for rule extraction has difficulty in capturing such non-local phenomena and will fully pr"
C10-1118,J03-1002,0,0.00907158,"kipped over, the hypothesis is chosen as a sure link. Heuristic span1 postpones the selection of the hypotheses on the POS level. Since the highest-scoring hypotheses tend to appear on the leaf nodes, it may introduce ambiguity when conducting the alignment for a POS node whose child word appears twice in a sentence. The baseline method proposes two score functions based on the lexical translation probability. They also compute the score function by splitting the tree into the internal and external components. Tinsley et al. (2007) adopt the lexical translation probabilities dumped by GIZA++ (Och and Ney, 2003) to compute the span based scores for each pair of sub-trees. Although all of their heuristics combinations are re-implemented in our study, we only present the best result among them with the highest Recall and F-value as our baseline, denoted as skip2_s1_span12. Baseline approach We implement the work in Tinsley et al. (2007) as our baseline methodology. Given a tree pair , the baseline approach first takes all the links between the sub-tree pairs as alignment hypotheses, i.e., the Cartesian product of the two sub-tree sets: { } { } By using the lexical translation probabilities, each hypoth"
C10-1118,P03-2041,0,\N,Missing
C10-1118,J08-3004,0,\N,Missing
C10-1118,P09-1103,1,\N,Missing
C10-2073,J93-2003,0,0.0248122,"Missing"
C10-2073,C02-1011,0,0.0292421,"rminology can be extracted directly from these corpora, evolving or emerging terminologies can be captured much faster than lexicography and this would facilitate many tasks and applications in accessing cross-lingual information. There remain challenges in term alignment for comparable corpora. The structures of texts, paragraphs and sentences can be very different. The similarity of content in two documents varies through they talk about the same subject matter. Recent research in using transliteration (Udupa et. al., 2008; Knight and Graehl, 1998), context information (Morin et. al., 2007; Cao and Li, 2002; Fung, 1998), part-of-speech tagging, frequency distribution (Tao and Zhai, 2005) or some hybrid methods (Klementiev and Roth, 2006; Sadat et. al., 2003) have shone some light in dealing with comparable corpora. In particular, context information seems to be popular since it is ubiquitous and can be retrieved from corpora easily. In this paper, we propose an EM-based hybrid model for term alignment to address the issue. Through this model, we hope to discover new bilingual terminology from comparable corpora without supervision. In the following sections, the model will be explained in detail"
C10-2073,W04-3208,0,0.0118325,"ilingual terminology with limited usage of dictionaries. 1 Introduction Bilingual terminology extraction or term alignment has been well studied in parallel corpora. Due to the coherent nature of parallel corpora, various statistical methods, like EM algorithm (Brown et. al., 1993) have been proven to be effective and have achieved excellent performance in term of precision and recall. The limitation of parallel corpora in all domains and languages has led some researchers to explore ways to automate the parallel sentence extraction process from non-parallel corpora (Munteanu and Marcu, 2005; Fung and Cheung, 2004) before proceeding to the usual term alignment extraction using the existing techniques for parallel corpora. Nevertheless, the coverage is limited since parallel sentences in non-parallel corpora are minimal. Meanwhile, some researchers have started to exploit comparable corpora directly in a new manner. The motivations for such an approach are obvious: comparable corpora are abundantly available, from encyclopedia to daily newspapers, and the human effort is reduced in either generating or collecting these corpora. If bilingual terminology can be extracted directly from these corpora, evolvi"
C10-2073,P07-1084,0,0.026925,"Missing"
C10-2073,J05-4003,0,0.0252037,"pability to discover new bilingual terminology with limited usage of dictionaries. 1 Introduction Bilingual terminology extraction or term alignment has been well studied in parallel corpora. Due to the coherent nature of parallel corpora, various statistical methods, like EM algorithm (Brown et. al., 1993) have been proven to be effective and have achieved excellent performance in term of precision and recall. The limitation of parallel corpora in all domains and languages has led some researchers to explore ways to automate the parallel sentence extraction process from non-parallel corpora (Munteanu and Marcu, 2005; Fung and Cheung, 2004) before proceeding to the usual term alignment extraction using the existing techniques for parallel corpora. Nevertheless, the coverage is limited since parallel sentences in non-parallel corpora are minimal. Meanwhile, some researchers have started to exploit comparable corpora directly in a new manner. The motivations for such an approach are obvious: comparable corpora are abundantly available, from encyclopedia to daily newspapers, and the human effort is reduced in either generating or collecting these corpora. If bilingual terminology can be extracted directly fr"
C10-2073,W03-1108,0,0.0463416,"Missing"
C10-2073,I08-2084,1,0.857902,"Missing"
C10-2073,E09-1096,1,0.884232,"Missing"
C10-2073,P06-1103,0,\N,Missing
C10-2073,J98-4003,0,\N,Missing
C10-2086,J97-3002,0,0.313212,"Missing"
C10-2086,J98-4004,0,0.399118,"d of phrasal/constituent level model. In our model, with the help of the alignment and the head-modifier dependency based relationship in the source side, the reordering type of each target word with alignment in source side is identified as one of pre-defined reordering types. With these reordering types, the reordering of phrase in translation is estimated on word level. 748 Coling 2010: Poster Volume, pages 748–756, Beijing, August 2010 Fig 1. An Constituent based Parse Tree 2 Baseline ing capacity of the translation model. Instead of directly employing the parse tree fragments (Bod, 1992; Johnson, 1998) in reordering rules (Huang and Knight, 2006; Liu 2006; Zhang and Jiang 2008), we make a mapping from trees to ebest = argmaxe p ( e |f ) pLM ( e ) ωlength(e) (1) sets of head-modifier dependency relations (Collins 1996 ) which can be obtained from the where p(e|f) can be computed using phrase constituent based parse tree with the help of translation model, distortion model and lexical head rules ( Bikel, 2004 ). reordering model. pLM(e) can be computed using the language model. ωlength(e) is word penalty 3.1 Head-modifier Relation model. Among the above models, there are three According to Kl"
C10-2086,2006.amta-papers.8,0,0.0359001,"Missing"
C10-2086,J07-2003,0,0.119531,"Missing"
C10-2086,P06-1077,0,0.0454593,"Missing"
C10-2086,P01-1067,0,0.0775597,"uccessfully applied to SMT to improve translation performance. Research in applying syntax information to SMT has been carried out in two aspects. On the one hand, the syntax knowledge is employed by directly integrating the syntactic structure into the translation rules i.e. syntactic translation rules. On this perspective, the word order of the target translation is modeled by the syntax structure explicitly. Chiang (2005), Wu (1997) and Xiong (2006) learn the syntax rules using the formal grammars. While more research is conducted to learn syntax rules with the help of linguistic analysis (Yamada and Knight, 2001; Graehl and Knight, 2004). However, there are some challenges to these models. Firstly, the linguistic analysis is far from perfect. Most of these methods require an off-the-shelf parser to generate syntactic structure, which makes the translation results sensitive to the parsing errors to some extent. To tackle this problem, n-best parse trees and parsing forest (Mi and Huang, 2008; Zhang, 2009) are proposed to relieve the error propagation brought by linguistic analysis. Secondly, some phrases which violate the boundary of linguistic analysis are also useful in these models ( DeNeefe et al."
C10-2086,P08-1114,0,0.0221605,"To tackle this problem, n-best parse trees and parsing forest (Mi and Huang, 2008; Zhang, 2009) are proposed to relieve the error propagation brought by linguistic analysis. Secondly, some phrases which violate the boundary of linguistic analysis are also useful in these models ( DeNeefe et al., 2007; Cowan et al. 2006). Thus, a tradeoff needs to be found between linguistic sense and formal sense. On the other hand, instead of using syntactic translation rules, some previous work attempts to learn the syntax knowledge separately and then integrated those knowledge to the original constraint. Marton and Resnik (2008) utilize the language linguistic analysis that is derived from parse tree to constrain the translation in a soft way. By doing so, this approach addresses the challenges brought by linguistic analysis through the log-linear model in a soft way. Starting from the state-of-the-art phrase based model Moses ( Koehn e.t. al, 2007), we propose a head-modifier relation based reordering model and use the proposed model as a soft syntax constraint in the phrase-based translation framework. Compared with most of previous soft constraint models, we study the way to utilize the constituent based parse tre"
C10-2086,P08-1066,0,0.0281483,"Missing"
C10-2086,P03-1054,0,0.0259913,"8) in reordering rules (Huang and Knight, 2006; Liu 2006; Zhang and Jiang 2008), we make a mapping from trees to ebest = argmaxe p ( e |f ) pLM ( e ) ωlength(e) (1) sets of head-modifier dependency relations (Collins 1996 ) which can be obtained from the where p(e|f) can be computed using phrase constituent based parse tree with the help of translation model, distortion model and lexical head rules ( Bikel, 2004 ). reordering model. pLM(e) can be computed using the language model. ωlength(e) is word penalty 3.1 Head-modifier Relation model. Among the above models, there are three According to Klein and Manning (2003) and reordering-related components: language model, Collins (1999), there are two shortcomings in nlexical reordering model and distortion model. ary Treebank grammar. Firstly, the grammar is The language model can reorder the local target too coarse for parsing. The rules in different words within a fixed window in an implied way. context always have different distributions. SeThe lexical reordering model and distortion condly, the rules learned from training corpus reordering model tackle the reordering problem cannot cover the rules in testing set. Currently, the state-of-the-art parsing al"
C10-2086,P96-1025,0,0.203111,"ide is identified as one of pre-defined reordering types. With these reordering types, the reordering of phrase in translation is estimated on word level. 748 Coling 2010: Poster Volume, pages 748–756, Beijing, August 2010 Fig 1. An Constituent based Parse Tree 2 Baseline ing capacity of the translation model. Instead of directly employing the parse tree fragments (Bod, 1992; Johnson, 1998) in reordering rules (Huang and Knight, 2006; Liu 2006; Zhang and Jiang 2008), we make a mapping from trees to ebest = argmaxe p ( e |f ) pLM ( e ) ωlength(e) (1) sets of head-modifier dependency relations (Collins 1996 ) which can be obtained from the where p(e|f) can be computed using phrase constituent based parse tree with the help of translation model, distortion model and lexical head rules ( Bikel, 2004 ). reordering model. pLM(e) can be computed using the language model. ωlength(e) is word penalty 3.1 Head-modifier Relation model. Among the above models, there are three According to Klein and Manning (2003) and reordering-related components: language model, Collins (1999), there are two shortcomings in nlexical reordering model and distortion model. ary Treebank grammar. Firstly, the grammar is The l"
C10-2086,P96-1021,0,0.0531225,"stituents in parse tree. Chiang(2005), Marton and Resnik(2008) explored the constituent match/violation in hiero; Xiong (2009 a) added constituent parse tree based linguistic analysis into BTG model; Xiong (2009 b) added source dependency structure to BTG; Zhang(2009) added tree-kernel to BTG model. All these studies show promising results. Making soft constrain is an easy and 755 efficient way in adding linguistic analysis into formal sense SMT model. In modeling the reordering, most of previous studies are on phrase level. In Moses, the lexical reordering is modeled on adjacent phrases. In (Wu, 1996; Xiong, 2006), the reordering is also modeled on adjacent translated phrases. In hiero, the reordering is modeled on the segments of the unmotivated translation rules. The tree-tostring models (Yamada et al. 2001; Liu et al.2006) are model on phrases with syntax representations. All these studies show excellent performance, while there are few studies on word level model in recent years. It is because, we consider, the alignment in word level model is complex which limits the reordering capacity of word level models. However, our work exploits a new direction in reordering that, by utilizing"
C10-2086,P06-1066,0,0.0524097,"Missing"
C10-2086,P09-1036,1,0.886552,"Missing"
C10-2086,2009.mtsummit-posters.25,1,0.792708,"Missing"
C10-2086,P07-2045,0,0.00524484,"Missing"
C10-2086,N03-1017,0,0.0551136,"Missing"
C10-2086,W06-1628,0,0.0343592,"Missing"
C10-2086,N04-1014,0,\N,Missing
C10-2086,koen-2004-pharaoh,0,\N,Missing
C10-2086,D08-1022,0,\N,Missing
C10-2086,J02-1005,0,\N,Missing
C10-2086,J03-4003,0,\N,Missing
C10-2086,P09-1020,1,\N,Missing
C10-2086,P08-1064,1,\N,Missing
C10-2086,P08-2038,1,\N,Missing
C10-2086,P05-1033,0,\N,Missing
C10-2086,D07-1079,0,\N,Missing
C10-2086,P00-1056,0,\N,Missing
C10-2112,J96-1002,0,0.0572198,"Missing"
C10-2112,P07-1016,1,0.857524,"Missing"
C10-2112,P04-1024,0,0.0390586,"Missing"
C10-2112,I08-1008,1,0.23473,"Missing"
C10-2112,J98-4003,0,\N,Missing
C10-2165,P02-1051,0,0.0827426,"king parts of the world, machine transliteration play a crucial role in most multilingual NLP, MT and CLIR applications (Hermjakob et al., 2008; Mandl and Womser-Hacker, 2004). This is because proper names account for the majority of OOV issues and translation lexicons (even derived from large parallel corpora) usually fail to provide good coverage over diverse, dynamically increasing names across languages. Much research effort has been done to address the transliteration issue in the research community (Knight and Graehl, 1998; Wan and Verspoor, 1998; Kang and Choi, 2000; Meng et al., 2001; Al-Onaizan and Knight, 2002; Gao et al., 2004; Klementiev and Roth, 2006; Sproat, 2006; Zelenko and Aone, 2006; Li et al., 2004, 2009a, 2009b; Sherif and Kondrak, 2007; Bertoldi et al., 2008; Goldwasser and Roth, 2008). These previous work can be categorized into three classes, i.e., graphemebased, phoneme-based and hybrid methods. Grapheme-based method (Li et al., 2004) treats transliteration as a direct orthographic mapping process and only uses orthographyrelated features while phoneme-based method (Knight and Graehl, 1998) treats transliteration as a phonetic mapping issue, converting source grapheme to source phone"
C10-2165,2008.iwslt-papers.1,0,0.141767,"2004). This is because proper names account for the majority of OOV issues and translation lexicons (even derived from large parallel corpora) usually fail to provide good coverage over diverse, dynamically increasing names across languages. Much research effort has been done to address the transliteration issue in the research community (Knight and Graehl, 1998; Wan and Verspoor, 1998; Kang and Choi, 2000; Meng et al., 2001; Al-Onaizan and Knight, 2002; Gao et al., 2004; Klementiev and Roth, 2006; Sproat, 2006; Zelenko and Aone, 2006; Li et al., 2004, 2009a, 2009b; Sherif and Kondrak, 2007; Bertoldi et al., 2008; Goldwasser and Roth, 2008). These previous work can be categorized into three classes, i.e., graphemebased, phoneme-based and hybrid methods. Grapheme-based method (Li et al., 2004) treats transliteration as a direct orthographic mapping process and only uses orthographyrelated features while phoneme-based method (Knight and Graehl, 1998) treats transliteration as a phonetic mapping issue, converting source grapheme to source phoneme followed by a mapping from source phoneme to target phoneme/grapheme. Hybrid method in machine transliteration refers to the combination of several different mo"
C10-2165,W06-1672,0,0.171672,"l NLP, MT and CLIR applications (Hermjakob et al., 2008; Mandl and Womser-Hacker, 2004). This is because proper names account for the majority of OOV issues and translation lexicons (even derived from large parallel corpora) usually fail to provide good coverage over diverse, dynamically increasing names across languages. Much research effort has been done to address the transliteration issue in the research community (Knight and Graehl, 1998; Wan and Verspoor, 1998; Kang and Choi, 2000; Meng et al., 2001; Al-Onaizan and Knight, 2002; Gao et al., 2004; Klementiev and Roth, 2006; Sproat, 2006; Zelenko and Aone, 2006; Li et al., 2004, 2009a, 2009b; Sherif and Kondrak, 2007; Bertoldi et al., 2008; Goldwasser and Roth, 2008). These previous work can be categorized into three classes, i.e., graphemebased, phoneme-based and hybrid methods. Grapheme-based method (Li et al., 2004) treats transliteration as a direct orthographic mapping process and only uses orthographyrelated features while phoneme-based method (Knight and Graehl, 1998) treats transliteration as a phonetic mapping issue, converting source grapheme to source phoneme followed by a mapping from source phoneme to target phoneme/grapheme. Hybrid met"
C10-2165,kang-choi-2000-automatic,0,\N,Missing
C10-2165,N10-1065,0,\N,Missing
C10-2165,C04-1103,1,\N,Missing
C10-2165,P04-1021,1,\N,Missing
C10-2165,J96-1002,0,\N,Missing
C10-2165,P07-1108,0,\N,Missing
C10-2165,P06-1103,0,\N,Missing
C10-2165,N07-1061,0,\N,Missing
C10-2165,P07-1119,0,\N,Missing
C10-2165,P06-1010,0,\N,Missing
C10-2165,W09-3501,1,\N,Missing
C10-2165,P98-2220,0,\N,Missing
C10-2165,C98-2215,0,\N,Missing
C10-2165,D08-1037,0,\N,Missing
C10-2165,P08-1045,0,\N,Missing
C10-2165,N03-1017,0,\N,Missing
C10-2165,J03-1002,0,\N,Missing
C10-2165,W09-3506,0,\N,Missing
C10-2165,P09-1018,0,\N,Missing
C10-2165,P07-1092,0,\N,Missing
C10-2165,W09-3502,1,\N,Missing
C10-2165,P09-1016,1,\N,Missing
C10-2165,I08-8003,0,\N,Missing
C12-1077,W06-1620,0,0.0711409,"Missing"
C12-1077,D10-1117,0,0.318588,"Missing"
C12-1077,D08-1023,0,0.0744051,"Missing"
C12-1077,P06-1109,0,0.0621186,"Missing"
C12-1077,I11-1049,0,0.442582,"Missing"
C12-1077,C04-1180,0,0.0263471,"Missing"
C12-1077,W03-1013,0,0.0795372,"Missing"
C12-1077,J07-4004,0,0.0426405,"Missing"
C12-1077,N09-1062,0,0.0721982,"Missing"
C12-1077,P12-2004,0,0.0210837,"Missing"
C12-1077,P07-1037,0,0.0764807,"Missing"
C12-1077,N09-1012,0,0.120672,"Missing"
C12-1077,P02-1043,0,0.557258,"Missing"
C12-1077,J07-3004,0,0.19408,"Missing"
C12-1077,Y12-1061,1,0.810194,"Missing"
C12-1077,W07-2416,0,0.0568271,"Missing"
C12-1077,N09-1036,0,0.0595578,"Missing"
C12-1077,N10-1074,0,0.0577776,"Missing"
C12-1077,P04-1061,0,0.39535,"Missing"
C12-1077,P02-1017,0,0.107895,"Missing"
C12-1077,J93-2004,0,0.0448677,"Missing"
C12-1077,D10-1120,0,0.17543,"Missing"
C12-1077,W97-1010,0,0.178027,"Missing"
C12-1077,P07-3002,0,0.0483296,"Missing"
C12-1077,P07-1049,0,0.0538554,"Missing"
C12-1077,W10-2902,0,0.0457999,"Missing"
C12-1077,C00-2139,0,0.13246,"Missing"
C12-1077,W99-0909,0,0.100813,"Missing"
C12-1077,D07-1071,0,0.0843043,"Missing"
C12-1077,D11-1106,0,0.0495706,"Missing"
C12-1077,N10-1016,1,\N,Missing
C12-1077,N09-1009,0,\N,Missing
C12-1103,P11-1048,0,0.0128095,"cient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction between morphology and syntax and achieve gains on both subtasks. (Rush et al., 2010) propose dual decomposition (DD) for integrating different NLP subtasks at the test phase. They experiment with two cases, one integrating a phrase-structure parser and a dependency parser, and the other integrating a phrase-structure parser and a POS tagger. Both cases show that DD can help the individual subtasks. (Auli and Lopez, 2011) conduct an extensive comparison of LBP and DD for joint CCG supertagging and parsing. They show that LBP and DD achieves similar parsing accuracy improvement but has largely different convergence characteristics. Moreover, their work focuses on integrating two separately-trained sub-models, and they find that training the integrated model on LBP leads to large improvement drops compared with separately-trained models. 3 Pipeline POS tagging and dependency parsing The pipeline method treats POS tagging and dependency parsing as two cascaded problems. First, an optimal POS tag sequence ˆt is de"
C12-1103,C10-1011,0,0.178642,"Dependency features fdep (x, t, h, m, l) Sibling features fsib (x, t, h, m, l, s) Grandchild features fgrd (x, t, h, m, l, g) Atomic features incorporated l, wh, w m , t h , t m , t h±1 , t m±1 , t b , d i r(h, m), d ist(h, m) l, wh, ws , w m , t h , t m , t s , t h±1 , t m±1 , t s±1 , d i r(h, m), d ist(h, m) l, wh, w m , w g , t h , t m , t g , t h±1 , t m±1 , t g±1 , d i r(h, m), d i r(m, g) Table 1: Brief illustration of the syntactic features. b is an index between h and m. d i r(i, j) and d ist(i, j) denote the direction and distance of the dependency (i, j). Please refer to Table 4 of (Bohnet, 2010) for the complete feature list. CRF-based POS tagging. We adopt the first-order CRF to build our baseline POS tagger. As a conditional log-linear probabilistic model, CRF defines the probability of a tag sequence as X exp(Scorepos (x, t′ )) P(t|x) = exp(Scorepos (x, t))/ Scorepos (x, t) = wpos · fpos (x, t) = X 1≤i≤n t′ wpu · fpu (x, t i ) + wpb · fpb (x, t i−1 , t i ) (3) where fpos/pu/pb (.) refers to the feature vectors and wpos/pu/pb is the corresponding weight vectors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt"
C12-1103,D12-1133,0,0.229202,"peline models in the parsing accuracy but lead to substantial tagging accuracy drop. Compared with their work, we propose a better training algorithm for the joint models that can improve both tagging and parsing accuracies. In addition, our joint model adopts richer features and handles labeled dependency parsing. (Hatori et al., 2011) propose the first transition-based joint model for Chinese POS tagging and unlabeled dependency parsing and gain large improvement in the parsing accuracy. However, their joint models only slightly improve the tagging accuracy over a sequential tagging model. (Bohnet and Nivre, 2012) propose a transition-based joint model which can handle labeled non-projective dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy. Differently, we are the first work on joint POS tagging and dependency parsing that achieves large improvement in the tagging accuracy. (Smith and Eisner, 2008) apply loopy belief propagation (LBP) to dependency parsing and points out that LBP can naturally represent POS tags as laten"
C12-1103,D07-1101,0,0.509725,"t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing accuracy on a variety of languages (Koo and Collins, 2010; Bohnet, 2010). The score of a dependency tree is factored into scores of the three kinds of subtrees in Figure 2. Scoresyn (x, t, d) = wsyn · fsyn (x, t, d) = X {(h,m,l)}⊆d + X wdep · fdep (x, t, h, m, l) {(h,m,l),(h,s)}⊆d + X wsib · fsib (x, t, h, m, l, s) {(h,m,l),(m,g)}⊆d (4) wgrd · fgrd (x, t, h, m, l, g) For syntactic features, we adopt those of (Bohnet, 2010) which include three categories corres"
C12-1103,W08-2102,0,0.0186802,"Missing"
C12-1103,P05-1022,0,0.0567146,"Missing"
C12-1103,P12-2003,1,0.88498,"Missing"
C12-1103,W02-1001,0,0.0361869,"ted by x = w1 ...w n , part-of-speech (POS) tagging aims to find an optimal tag sequence t = t 1 ...t n , where t i ∈ T (1 ≤ i ≤ n) and T is a predefined tag set. POS tags are designed to represent word classes so that words of the same POS tag play a similar role in syntactic structures. The size of T is usually much less than the vocabulary size. Typically, POS tagging is treated as a sequence labeling problem, and has been previously addressed by machine learning algorithms, such as maximum-entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). Figure 1 gives an example sentence from Penn Chinese Treebank 5.1 (CTB5). The lowest three rows present the n-best POS tags for each word, produced by a state-ofthe-art CRF model. Looking at the 1-best POS tags, we can see that the CRF model makes four errors, i.e. de/DEC→DEG, ouwen/NR→NN, xiaoli/VV→NN, and liwupudui/NR→NN. In fact, (DEC,DEG) and (NN,VV) ambiguities, which usually require long-distance syntactic knowledge to resolve, are very difficult for the sequential labeling models. NMOD ROOT DEP VMOD VMOD  1 SUB AMOD á 2 193 gang man just turned 19 AD VV CD JJ 19 P VMOD  4 VMOD  &apos;"
C12-1103,W09-1201,0,0.0527682,"Missing"
C12-1103,I11-1136,0,0.318758,"is most closely related to (Li et al., 2011) who present the first work on joint models for Chinese POS tagging and unlabeled dependency parsing. Similar to us, their joint models are based on graph-based dependency parsing. They find that the joint models largely outperform the pipeline models in the parsing accuracy but lead to substantial tagging accuracy drop. Compared with their work, we propose a better training algorithm for the joint models that can improve both tagging and parsing accuracies. In addition, our joint model adopts richer features and handles labeled dependency parsing. (Hatori et al., 2011) propose the first transition-based joint model for Chinese POS tagging and unlabeled dependency parsing and gain large improvement in the parsing accuracy. However, their joint models only slightly improve the tagging accuracy over a sequential tagging model. (Bohnet and Nivre, 2012) propose a transition-based joint model which can handle labeled non-projective dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy."
C12-1103,P10-1110,0,0.190632,"the discriminative power of the POS features in resolving such syntaxinsensitive POS ambiguities are suppressed in the joint models when trained with AP or PA. Compared with AP and PA, SPA raises the weight of the POS features and can better utilize the disambiguation power of both the POS and syntactic features, leading to large tagging accuracy boost. On the other hand, better tagging results can further help parsing. 6 Experiments Data. We conduct experiments on CTB5 (Xue et al., 2005). Following the standard practice, we adopt the data split of (Duan et al., 2007; Zhang and Clark, 2008b; Huang and Sagae, 2010) and adopt Penn2Malt2 for constituent-to-dependency conversion with the head-finding rules of (Zhang and Clark, 2008b). We also evaluate our models on another version of CTB5 used in (Bohnet and Nivre, 2012) to compare with their joint model. We thank Bernd Bohnet for sharing their dataset. We refer to their dataset as CTB5-Bohnet. We carefully compare CTB5 with CTB5-Bohnet and find that except for the mismatch of about 30 sentence, the datasets differ in both dependency structures and dependency labels. After discussions with Bernd Bohnet, we find out that they adopt Yue Zhang’s constituent-t"
C12-1103,D08-1008,0,0.0157962,") , ˆt, d) AP wjoint (7) PA computes the update step τjoint by considering the loss of the best result, the score distance, and the feature vector distance.  ˆ − Scorejoint (x( j) , t( j) , d( j) ) + ρpos (t( j) , ˆt) + ρsyn (d( j) , d) ˆ Scorejoint (x( j) , ˆt, d)  τjoint = ( j) ( j) ( j) ( j) ˆ ˆ 2 kfjoint (x , t , d ) − fjoint (x , t, d)k (8) PA  (k+1) (k) ˆ w =w + τjoint (fjoint (x( j) , t( j) , d( j) ) − fjoint (x( j) , ˆt, d)) joint joint ( j) ˆ is the where ρpos (t , ˆt) is the incorrect POS tag number in ˆt according to t( j) , and ρsyn (d( j) , d) ˆ according to d( j) . Following (Johansson and Nugues, 2008), dependency error number in d ˆ increases by 1 for an incorrect dependency and by 0.5 for a correct dependency ρsyn (d( j) , d) with a wrong label. Theoretically, Eq. 8 computes the smallest update that makes the correct hypothesis outscores the returned highest-scoring hypothesis by the overall error. We can see that AP and PA use the same update step for the POS features fpos (.) and syntactic features fsyn (.). Therefore, the weights of the POS features and the syntactic features are of the same scale after training is completed. We argue that this is problematic since the number of the sy"
C12-1103,P08-1068,0,0.333682,"Missing"
C12-1103,P10-1001,0,0.392309,"igram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing accuracy on a variety of languages (Koo and Collins, 2010; Bohnet, 2010). The score of a dependency tree is factored into scores of the three kinds of subtrees in Figure 2. Scoresyn (x, t, d) = wsyn · fsyn (x, t, d) = X {(h,m,l)}⊆d + X wdep · fdep (x, t, h, m, l) {(h,m,l),(h,s)}⊆d + X wsib · fsib (x, t, h, m, l, s) {(h,m,l),(m,g)}⊆d (4) wgrd · fgrd (x, t, h, m, l, g) For syntactic features, we adopt those of (Bohnet, 2010) which include three categories corresponding to the three typ"
C12-1103,P11-1089,0,0.0153191,"ctive dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy. Differently, we are the first work on joint POS tagging and dependency parsing that achieves large improvement in the tagging accuracy. (Smith and Eisner, 2008) apply loopy belief propagation (LBP) to dependency parsing and points out that LBP can naturally represent POS tags as latent variables so that the POS tags can be inferred jointly with the parse. (Lee et al., 2011) extend the LBP based approach of (Smith and Eisner, 2008) and study joint morphological disambiguation and dependency parsing for morphologically-rich languages including Latin, Czech, Ancient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction between morphology and syntax and achieve gains on both subtasks. (Rush et al., 2010) propose dual decomposition (DD) for integrating different NLP subtasks at the test phase. They experiment with two cases, one inte"
C12-1103,D11-1109,1,0.93535,"score that are previously defined in the pipeline models. Scorejoint (x, t, d) = Scorepos (x, t) + Scoresyn (x, t, d) = wpos · fpos (x, t) + wsyn · fsyn (x, t, d) (6) = wpos⊕syn · fpos⊕syn (x, t, d) = wjoint · fjoint (x, t, d) where ⊕ denotes vector concatenation. Note that our joint model incorporates the same POS and syntactic features with the pipeline models. Under the joint model, the weights of POS and syntactic features, denoted by wpos⊕syn or wjoint , are simultaneously learned. Therefore, they can interact with each other to determine an optimal joint result. 4.1 Decoding Similar to (Li et al., 2011), we extend the parsing algorithm of (Carreras, 2007) using the idea of (Eisner, 2000) and propose a dynamic programming (DP) based decoding algorithm for our joint model. Figure 3 illustrates the basic DP structures and operations. The key idea is to augment the basic DP structures in the parsing algorithm (namely spans) with a few POS tags. A span means a partially built structure spanning a sub-sentence. For example, the leftside span in Figure 3(a), which is called an incomplete span and is denoted by I(h,m,l)(t h ,t m ) , represents a partial tree spanning wh...w m with wh being tagged as"
C12-1103,D10-1004,0,0.0469191,"Missing"
C12-1103,P05-1012,0,0.236675,"ctors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing accuracy on a variety of languages (Koo and Collins, 2010; Bohnet, 2010). The score of a dependency tree is factored into scores of the three kinds of subtrees in Figure 2. Scoresyn (x, t, d) = wsyn · fsyn (x, t, d) = X {(h,m,l)}⊆d + X wdep · fdep (x, t, h, m, l) {(h,m,l),(h,s)}⊆d + X wsib · fsib (x, t, h, m, l, s) {(h,m,l),(m,g)}⊆d (4) wgrd · fgrd (x, t, h, m, l, g) For syntactic features, we adopt those of (Bohnet, 2010) which include three c"
C12-1103,N07-1051,0,0.0863404,"Missing"
C12-1103,W96-0213,0,0.889178,"NG 2012, Mumbai, December 2012. 1681 1 Introduction Given an input sentence of n words, denoted by x = w1 ...w n , part-of-speech (POS) tagging aims to find an optimal tag sequence t = t 1 ...t n , where t i ∈ T (1 ≤ i ≤ n) and T is a predefined tag set. POS tags are designed to represent word classes so that words of the same POS tag play a similar role in syntactic structures. The size of T is usually much less than the vocabulary size. Typically, POS tagging is treated as a sequence labeling problem, and has been previously addressed by machine learning algorithms, such as maximum-entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). Figure 1 gives an example sentence from Penn Chinese Treebank 5.1 (CTB5). The lowest three rows present the n-best POS tags for each word, produced by a state-ofthe-art CRF model. Looking at the 1-best POS tags, we can see that the CRF model makes four errors, i.e. de/DEC→DEG, ouwen/NR→NN, xiaoli/VV→NN, and liwupudui/NR→NN. In fact, (DEC,DEG) and (NN,VV) ambiguities, which usually require long-distance syntactic knowledge to resolve, are very difficult for the sequential labeling models. NMOD ROOT DEP VMO"
C12-1103,D10-1001,0,0.012255,"nts out that LBP can naturally represent POS tags as latent variables so that the POS tags can be inferred jointly with the parse. (Lee et al., 2011) extend the LBP based approach of (Smith and Eisner, 2008) and study joint morphological disambiguation and dependency parsing for morphologically-rich languages including Latin, Czech, Ancient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction between morphology and syntax and achieve gains on both subtasks. (Rush et al., 2010) propose dual decomposition (DD) for integrating different NLP subtasks at the test phase. They experiment with two cases, one integrating a phrase-structure parser and a dependency parser, and the other integrating a phrase-structure parser and a POS tagger. Both cases show that DD can help the individual subtasks. (Auli and Lopez, 2011) conduct an extensive comparison of LBP and DD for joint CCG supertagging and parsing. They show that LBP and DD achieves similar parsing accuracy improvement but has largely different convergence characteristics. Moreover, their work focuses on integrating tw"
C12-1103,D08-1016,0,0.0105209,"e parsing accuracy. However, their joint models only slightly improve the tagging accuracy over a sequential tagging model. (Bohnet and Nivre, 2012) propose a transition-based joint model which can handle labeled non-projective dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy. Differently, we are the first work on joint POS tagging and dependency parsing that achieves large improvement in the tagging accuracy. (Smith and Eisner, 2008) apply loopy belief propagation (LBP) to dependency parsing and points out that LBP can naturally represent POS tags as latent variables so that the POS tags can be inferred jointly with the parse. (Lee et al., 2011) extend the LBP based approach of (Smith and Eisner, 2008) and study joint morphological disambiguation and dependency parsing for morphologically-rich languages including Latin, Czech, Ancient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction"
C12-1103,D09-1058,0,0.154484,"Missing"
C12-1103,P08-1101,0,0.469583,"list. CRF-based POS tagging. We adopt the first-order CRF to build our baseline POS tagger. As a conditional log-linear probabilistic model, CRF defines the probability of a tag sequence as X exp(Scorepos (x, t′ )) P(t|x) = exp(Scorepos (x, t))/ Scorepos (x, t) = wpos · fpos (x, t) = X 1≤i≤n t′ wpu · fpu (x, t i ) + wpb · fpb (x, t i−1 , t i ) (3) where fpos/pu/pb (.) refers to the feature vectors and wpos/pu/pb is the corresponding weight vectors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing ac"
C12-1103,D08-1059,0,0.230453,"list. CRF-based POS tagging. We adopt the first-order CRF to build our baseline POS tagger. As a conditional log-linear probabilistic model, CRF defines the probability of a tag sequence as X exp(Scorepos (x, t′ )) P(t|x) = exp(Scorepos (x, t))/ Scorepos (x, t) = wpos · fpos (x, t) = X 1≤i≤n t′ wpu · fpu (x, t i ) + wpb · fpb (x, t i−1 , t i ) (3) where fpos/pu/pb (.) refers to the feature vectors and wpos/pu/pb is the corresponding weight vectors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing ac"
C12-1103,J11-1005,0,0.00551549,"Missing"
C12-1103,P11-2033,0,0.150722,"Missing"
C12-2041,W05-0909,0,0.0336729,"Missing"
C12-2041,P92-1033,0,0.285973,"owever, there is a big difference in the usage of tenses in different languages. In inflectional languages like English, tense is often expressed by verb inflections and thus can be easily recognized. Whereas, some of the major Eastern Asian languages such as Chinese, Vietnamese and Thai, do not have the grammatical category of tense, and their tense is indicated by content words such as adverbs of time. So mapping a correct tense from source-side into target-side in this case is difficult and thus it poses challenges on current machine translation tasks. In some Interlingua-based MT systems (Dorr, 1992; Olsen et al., 2001; Wang and Seneff, 2006), tense information of the source language can be firstly transformed into an abstract languageindependent representation and passed to the target language. However, such research works have not been thoroughly studied since the definition of an Interlingua is already very difficult esp. for a wider domain. With the popularity of SMT, corpus-based methods of addressing tense problems for MT have been introduced (Ye and Zhang, 2005; Liu et al., 2011; Lee, 2011). However, these works just resolve tense recognition and do not address how to integrate th"
C12-2041,D12-1026,1,0.443298,"firstly transformed into an abstract languageindependent representation and passed to the target language. However, such research works have not been thoroughly studied since the definition of an Interlingua is already very difficult esp. for a wider domain. With the popularity of SMT, corpus-based methods of addressing tense problems for MT have been introduced (Ye and Zhang, 2005; Liu et al., 2011; Lee, 2011). However, these works just resolve tense recognition and do not address how to integrate their works into a realistic SMT system. There is little work on resolving tense error for SMT. Gong et al. (2012) propose target N-grambased tense models to improve translation performance. However, it is not reliable enough since they only consider the target-side tense information and SMT systems often generate abnormal outputs. For the source language with weak inflections or even without inflections, its contexts can provide valuable cues about tense. For example, some Chinese words, such as “~(JinChang, often)” and “…U(ZuoTian, yesterday)”, can obviously indicate present tense and past tense respectively. Our proposed SMT system, both source-side and target-side tense cues are employed. Aiming at t"
C12-2041,N03-1017,0,0.125254,"al. (2011), our predication is special in two aspects: (1) we only consider the major tense of source-side sentence and thus our classification accuracy is high enough; (2) we exploit more useful features for this task. Furthermore, we integrate our tense model into a popular SMT system and improve the translation results. 2.2 The system framework The work described in this paper is based on a modified Moses, a state-of-the-art phrase-based SMT system. The major modified parts for Moses are input and output modules in order to translate using document-level information. Our SMT system follows Koehn et al. (2003) and adopts similar six groups of features. Besides, the log-linear model(Och and Ney, 2000) is employed to linearly interpolate these features according to formula(1): e best = arg max e M X m=1 λm hm (e, f ) (1) where hm (e, f ) is a feature function, and λm is the weight of hm (e, f ) optimized by a discriminative training method on a held-out development data (Och, 2003). Classifier-based tense model can be easily integrated into the formula(1) by the following special features: ¨ 1 i f (t g = t s ) F2 = P(Ts_i |f ) F1 = 0 else F1 is a binary feature which encourages the decoder to prefer"
C12-2041,I11-1125,0,0.116943,"cult and thus it poses challenges on current machine translation tasks. In some Interlingua-based MT systems (Dorr, 1992; Olsen et al., 2001; Wang and Seneff, 2006), tense information of the source language can be firstly transformed into an abstract languageindependent representation and passed to the target language. However, such research works have not been thoroughly studied since the definition of an Interlingua is already very difficult esp. for a wider domain. With the popularity of SMT, corpus-based methods of addressing tense problems for MT have been introduced (Ye and Zhang, 2005; Liu et al., 2011; Lee, 2011). However, these works just resolve tense recognition and do not address how to integrate their works into a realistic SMT system. There is little work on resolving tense error for SMT. Gong et al. (2012) propose target N-grambased tense models to improve translation performance. However, it is not reliable enough since they only consider the target-side tense information and SMT systems often generate abnormal outputs. For the source language with weak inflections or even without inflections, its contexts can provide valuable cues about tense. For example, some Chinese words, such"
C12-2041,P03-1021,0,0.00601257,"based on a modified Moses, a state-of-the-art phrase-based SMT system. The major modified parts for Moses are input and output modules in order to translate using document-level information. Our SMT system follows Koehn et al. (2003) and adopts similar six groups of features. Besides, the log-linear model(Och and Ney, 2000) is employed to linearly interpolate these features according to formula(1): e best = arg max e M X m=1 λm hm (e, f ) (1) where hm (e, f ) is a feature function, and λm is the weight of hm (e, f ) optimized by a discriminative training method on a held-out development data (Och, 2003). Classifier-based tense model can be easily integrated into the formula(1) by the following special features: ¨ 1 i f (t g = t s ) F2 = P(Ts_i |f ) F1 = 0 else F1 is a binary feature which encourages the decoder to prefer hypothesis translations which have tense form conforming to the source-side contexts. Since tense of source-side sentence sometimes is not reliable enough, F2 is introduced to inform the decoder to what extent it can trust F1 , it indicates the confidence of the source-side tense classifier. It is worth noting that we don’t introduce the similar feature for the target side s"
C12-2041,N04-1021,0,0.0554589,"Missing"
C12-2041,P00-1056,0,0.504692,"f source-side sentence and thus our classification accuracy is high enough; (2) we exploit more useful features for this task. Furthermore, we integrate our tense model into a popular SMT system and improve the translation results. 2.2 The system framework The work described in this paper is based on a modified Moses, a state-of-the-art phrase-based SMT system. The major modified parts for Moses are input and output modules in order to translate using document-level information. Our SMT system follows Koehn et al. (2003) and adopts similar six groups of features. Besides, the log-linear model(Och and Ney, 2000) is employed to linearly interpolate these features according to formula(1): e best = arg max e M X m=1 λm hm (e, f ) (1) where hm (e, f ) is a feature function, and λm is the weight of hm (e, f ) optimized by a discriminative training method on a held-out development data (Och, 2003). Classifier-based tense model can be easily integrated into the formula(1) by the following special features: ¨ 1 i f (t g = t s ) F2 = P(Ts_i |f ) F1 = 0 else F1 is a binary feature which encourages the decoder to prefer hypothesis translations which have tense form conforming to the source-side contexts. Since"
C12-2041,2001.mtsummit-papers.47,0,0.0462034,"Missing"
C12-2041,P02-1040,0,0.085781,"Missing"
C12-2041,2006.amta-papers.25,0,0.0529701,"Missing"
C12-2041,vilar-etal-2006-error,0,0.0462898,"Missing"
C12-2041,I05-1077,0,0.0171145,"n this case is difficult and thus it poses challenges on current machine translation tasks. In some Interlingua-based MT systems (Dorr, 1992; Olsen et al., 2001; Wang and Seneff, 2006), tense information of the source language can be firstly transformed into an abstract languageindependent representation and passed to the target language. However, such research works have not been thoroughly studied since the definition of an Interlingua is already very difficult esp. for a wider domain. With the popularity of SMT, corpus-based methods of addressing tense problems for MT have been introduced (Ye and Zhang, 2005; Liu et al., 2011; Lee, 2011). However, these works just resolve tense recognition and do not address how to integrate their works into a realistic SMT system. There is little work on resolving tense error for SMT. Gong et al. (2012) propose target N-grambased tense models to improve translation performance. However, it is not reliable enough since they only consider the target-side tense information and SMT systems often generate abnormal outputs. For the source language with weak inflections or even without inflections, its contexts can provide valuable cues about tense. For example, some C"
C14-1075,C10-1011,0,0.0510777,"small subtrees p. X Score(x, d; w) = w · f (x, d) = Score(x, p; w) (1) p⊆d We adopt the second-order model of McDonald and Pereira (2006) as our core parsing algorithm,1 which defines the score of a dependency tree as: X X Score(x, d; w) = wdep · fdep (x, h, m) + wsib · fsib (x, h, s, m) (2) {(h,m)}⊆d {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are feature vectors corresponding to two kinds of subtree; wdep/sib are the feature weight vectors; the dot product gives the scores contributed by the corresponding subtrees. We adopt the state-of-the-art syntactic features proposed in Bohnet (2010). 3.2 Probabilistic CRF-based GParser Previous work on dependency parsing mostly adopts linear models and online perceptron training, which lack probabilistic explanations of dependency trees and likelihood of the training data. Instead, we build a log-linear CRF-based probabilistic dependency parser, which defines the probability of a dependency tree as: X exp{Score(x, d; w)} p(d|x; w) = ; Z(x; w) = exp{Score(x, d′ ; w)} (3) Z(x; w) ′ d ∈Y(x) where Z(x) is the normalization factor and Y(x) is the set of all legal dependency trees for x. 3.3 Likelihood and Gradient of Training Data with Ambigu"
C14-1075,D08-1092,0,0.0257035,"anually construct treebanks. Therefore, lots of recent work has been devoted to get help from bilingual constraints. The motivation behind are two-fold. First, a difficult syntactic ambiguity in one language may be very easy to resolve in another language. Second, a more accurate parser on one language may help an inferior parser on another language, where the performance difference may be due to the intrinsic complexity of languages or the scale of accessible labeled resources. Following the above research line, much effort has been done recently to explore bilingual constraints for parsing. Burkett and Klein (2008) propose a reranking based method for joint constituent parsing of bitext, which can make use of structural correspondence features in both languages. Their method needs bilingual treebanks with manually labeled syntactic trees on both sides for training. Huang et al. (2009) compose useful parsing features based on word reordering information in source-language sentences. Chen et al. (2010a) derive bilingual subtree constraints with auto-parsed source-language sentences. During training, both Huang et al. (2009) and Chen et al. (2010a) require bilingual text with target-language gold-standard"
C14-1075,W10-2906,0,0.0518478,"ginal probabilities to throw away bad projections, which turns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om et al. (2012) derive crosslingual clusters from bitext to help delexicalized parser transfer. Naseem et al. (2012) propose selectively sharing to better explore multi-source transfer information. 6 In the previous draft of this paper, we directly use the projected dat"
C14-1075,D07-1101,0,0.304917,"re 1(c). We introduce three new dependencies to compose candidate heads for the unattached word “ 3 ”. Note that it is illegal to add the dependency “ 1 y 3 ” since it would cross the projected dependency “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors the score of a dependency tree into s"
C14-1075,N13-1006,0,0.0145899,"ojections, which turns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om et al. (2012) derive crosslingual clusters from bitext to help delexicalized parser transfer. Naseem et al. (2012) propose selectively sharing to better explore multi-source transfer information. 6 In the previous draft of this paper, we directly use the projected data with in previous subsection for si"
C14-1075,P10-1003,1,0.918629,"the intrinsic complexity of languages or the scale of accessible labeled resources. Following the above research line, much effort has been done recently to explore bilingual constraints for parsing. Burkett and Klein (2008) propose a reranking based method for joint constituent parsing of bitext, which can make use of structural correspondence features in both languages. Their method needs bilingual treebanks with manually labeled syntactic trees on both sides for training. Huang et al. (2009) compose useful parsing features based on word reordering information in source-language sentences. Chen et al. (2010a) derive bilingual subtree constraints with auto-parsed source-language sentences. During training, both Huang et al. (2009) and Chen et al. (2010a) require bilingual text with target-language gold-standard dependency trees. All above work shows significant performance gain ∗ Correspondence author This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 783 Proceedings of COLING 2014, the 25th International Conference on Computational Lingu"
C14-1075,P10-1065,0,0.130343,"the intrinsic complexity of languages or the scale of accessible labeled resources. Following the above research line, much effort has been done recently to explore bilingual constraints for parsing. Burkett and Klein (2008) propose a reranking based method for joint constituent parsing of bitext, which can make use of structural correspondence features in both languages. Their method needs bilingual treebanks with manually labeled syntactic trees on both sides for training. Huang et al. (2009) compose useful parsing features based on word reordering information in source-language sentences. Chen et al. (2010a) derive bilingual subtree constraints with auto-parsed source-language sentences. During training, both Huang et al. (2009) and Chen et al. (2010a) require bilingual text with target-language gold-standard dependency trees. All above work shows significant performance gain ∗ Correspondence author This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 783 Proceedings of COLING 2014, the 25th International Conference on Computational Lingu"
C14-1075,D11-1005,0,0.191529,"Missing"
C14-1075,P11-1061,0,0.0488018,"data to learn model parameters. Second, their work measures the reliability of a projected dependencies only from the perspective of alignment probability, while we adopt a probabilistic parsing model and use target-side marginal probabilities to throw away bad projections, which turns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om et al. (2012) derive crosslingual clusters fr"
C14-1075,D07-1098,0,0.0508734,"Missing"
C14-1075,P08-1109,0,0.0570156,"e second term within O(n3 ) time complexity, where n is the length of the input sentence. Similarly, the first term can be solved by running the InsideOutside algorithm in the constrained search space Fi . 3.4 Stochastic Gradient Descent (SGD) Training With the likelihood gradients, we apply L2-norm regularized SGD training to iteratively learn the feature weights w for our CRF-based baseline and bitext-enhanced parsers. We follow the implementation in CRFsuite.2 At each step, the algorithm approximates a gradient with a small subset of training examples, and then updates the feature weights. Finkel et al. (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. Moreover, it is very convenient to parallel SGD since computation among examples in the same batch is mutually independent. Once the feature weights w are learnt, we can parse the test data and try to find the optimal parse tree with the Viterbi decoding algorithm in O(n3 ) parsing time (Eisner, 2000; McDonald and Pereira, 2006). d∗ = arg max p(d|x; w) d∈Y(x) (8) 4 Experiments and Analysis To verify the effectiveness of our proposed method, we carry out experiments on Eng"
C14-1075,I11-1030,0,0.0271017,"s the reliability of a projected dependencies only from the perspective of alignment probability, while we adopt a probabilistic parsing model and use target-side marginal probabilities to throw away bad projections, which turns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om et al. (2012) derive crosslingual clusters from bitext to help delexicalized parser transfer. Nase"
C14-1075,P09-1042,0,0.0398834,"ks as follows. First, we train a parser on sourcelanguage treebank, called a source parser. Then, we use the source parser to produce automatic syntactic structures on the source side of bitext. Next, with the help of automatic word alignments, we project the source-side syntactic structures into the target side. Finally, the target-side structures are used as goldstandard to train new parsing models of target language. Previous work on syntax projection mostly focuses on unsupervised grammar induction where no labeled data exists for target language (Hwa et al., 2005; Spreyer and Kuhn, 2009; Ganchev et al., 2009; Liu et al., 2013). Smith and Eisner (2009) propose quasi-synchronous grammar for cross-lingual parser projection and assume the existence of hundreds of target language annotated sentences. Similar to our work in this paper, Jiang et al. (2010) try to explore projected structures to further improve the performance of statistical parsers trained on full-scale monolingual treebanks (see Section 4.4 for performance comparison). The major issues for syntax projection are 1) errors from the source-language parser and unsupervised word aligner; 2) intrinsic syntactic non-isomorphism between langua"
C14-1075,D09-1127,0,0.0214918,"parser on one language may help an inferior parser on another language, where the performance difference may be due to the intrinsic complexity of languages or the scale of accessible labeled resources. Following the above research line, much effort has been done recently to explore bilingual constraints for parsing. Burkett and Klein (2008) propose a reranking based method for joint constituent parsing of bitext, which can make use of structural correspondence features in both languages. Their method needs bilingual treebanks with manually labeled syntactic trees on both sides for training. Huang et al. (2009) compose useful parsing features based on word reordering information in source-language sentences. Chen et al. (2010a) derive bilingual subtree constraints with auto-parsed source-language sentences. During training, both Huang et al. (2009) and Chen et al. (2010a) require bilingual text with target-language gold-standard dependency trees. All above work shows significant performance gain ∗ Correspondence author This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creative"
C14-1075,P10-1002,0,0.205903,"Missing"
C14-1075,P10-1001,0,0.149505,"oduce three new dependencies to compose candidate heads for the unattached word “ 3 ”. Note that it is illegal to add the dependency “ 1 y 3 ” since it would cross the projected dependency “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors the score of a dependency tree into scores of small subtrees"
C14-1075,C12-1103,1,0.877484,"throw away bad projections, which turns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om et al. (2012) derive crosslingual clusters from bitext to help delexicalized parser transfer. Naseem et al. (2012) propose selectively sharing to better explore multi-source transfer information. 6 In the previous draft of this paper, we directly use the projected data with in previou"
C14-1075,N06-1014,0,0.120943,"Missing"
C14-1075,P13-1105,0,0.0280801,"Missing"
C14-1075,E06-1011,0,0.785748,"cies, as illustrated in Figure 1(c). We introduce three new dependencies to compose candidate heads for the unattached word “ 3 ”. Note that it is illegal to add the dependency “ 1 y 3 ” since it would cross the projected dependency “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors the score of a depend"
C14-1075,P05-1012,0,0.13372,"any projected dependencies, as illustrated in Figure 1(c). We introduce three new dependencies to compose candidate heads for the unattached word “ 3 ”. Note that it is illegal to add the dependency “ 1 y 3 ” since it would cross the projected dependency “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method f"
C14-1075,D11-1006,0,0.166925,"Missing"
C14-1075,P12-1066,0,0.0414152,"Missing"
C14-1075,W03-3017,0,0.129615,"ected dependency “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors the score of a dependency tree into scores of small subtrees p. X Score(x, d; w) = w · f (x, d) = Score(x, p; w) (1) p⊆d We adopt the second-order model of McDonald and Pereira (2006) as our core parsing algorithm,1 which"
C14-1075,D09-1086,0,0.0212304,"n sourcelanguage treebank, called a source parser. Then, we use the source parser to produce automatic syntactic structures on the source side of bitext. Next, with the help of automatic word alignments, we project the source-side syntactic structures into the target side. Finally, the target-side structures are used as goldstandard to train new parsing models of target language. Previous work on syntax projection mostly focuses on unsupervised grammar induction where no labeled data exists for target language (Hwa et al., 2005; Spreyer and Kuhn, 2009; Ganchev et al., 2009; Liu et al., 2013). Smith and Eisner (2009) propose quasi-synchronous grammar for cross-lingual parser projection and assume the existence of hundreds of target language annotated sentences. Similar to our work in this paper, Jiang et al. (2010) try to explore projected structures to further improve the performance of statistical parsers trained on full-scale monolingual treebanks (see Section 4.4 for performance comparison). The major issues for syntax projection are 1) errors from the source-language parser and unsupervised word aligner; 2) intrinsic syntactic non-isomorphism between languages; 3) incomplete parse trees after project"
C14-1075,P11-2120,0,0.18399,"Missing"
C14-1075,W09-1104,0,0.530339,"projection typically works as follows. First, we train a parser on sourcelanguage treebank, called a source parser. Then, we use the source parser to produce automatic syntactic structures on the source side of bitext. Next, with the help of automatic word alignments, we project the source-side syntactic structures into the target side. Finally, the target-side structures are used as goldstandard to train new parsing models of target language. Previous work on syntax projection mostly focuses on unsupervised grammar induction where no labeled data exists for target language (Hwa et al., 2005; Spreyer and Kuhn, 2009; Ganchev et al., 2009; Liu et al., 2013). Smith and Eisner (2009) propose quasi-synchronous grammar for cross-lingual parser projection and assume the existence of hundreds of target language annotated sentences. Similar to our work in this paper, Jiang et al. (2010) try to explore projected structures to further improve the performance of statistical parsers trained on full-scale monolingual treebanks (see Section 4.4 for performance comparison). The major issues for syntax projection are 1) errors from the source-language parser and unsupervised word aligner; 2) intrinsic syntactic non-isom"
C14-1075,N12-1052,0,0.0614031,"Missing"
C14-1075,N13-1126,0,0.262539,"Missing"
C14-1075,P13-1106,0,0.0167994,"urns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om et al. (2012) derive crosslingual clusters from bitext to help delexicalized parser transfer. Naseem et al. (2012) propose selectively sharing to better explore multi-source transfer information. 6 In the previous draft of this paper, we directly use the projected data with in previous subsection for simplicity, and find t"
C14-1075,W03-3023,0,0.113252,"ince it would cross the projected dependency “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors the score of a dependency tree into scores of small subtrees p. X Score(x, d; w) = w · f (x, d) = Score(x, p; w) (1) p⊆d We adopt the second-order model of McDonald and Pereira (2006) as our core parsing algor"
C14-1075,N01-1026,0,0.040213,"se these partial structures as extra training data to learn model parameters. Second, their work measures the reliability of a projected dependencies only from the perspective of alignment probability, while we adopt a probabilistic parsing model and use target-side marginal probabilities to throw away bad projections, which turns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om e"
C14-1075,I08-3008,0,0.358131,"Missing"
C14-1075,P11-2033,0,0.0847711,"ncy “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors the score of a dependency tree into scores of small subtrees p. X Score(x, d; w) = w · f (x, d) = Score(x, p; w) (1) p⊆d We adopt the second-order model of McDonald and Pereira (2006) as our core parsing algorithm,1 which defines the score of a d"
C14-1075,petrov-etal-2012-universal,0,\N,Missing
C14-1078,P05-1001,0,0.00996289,"cluster-based features for dependency parsing models. Suzuki et al. (2009) adapted a Semi-supervised Structured Conditional Model (SS-SCM) to dependency parsing. Suzuki et al. (2011) reported the best results so far on the standard test sets of PTB using a condensed feature representation combined with the word cluster-based features of Koo et al. (2008). Chen et al. (2013) mapped the base features into predefined types using the information of frequencies counted in large amounts of auto-parsed data. The work of Suzuki et al. (2011) and Chen et al. (2013) were to perform feature clustering. Ando and Zhang (2005) presented a semi-supervised learning algorithm named alternating structure optimization for text chunking. They used a large projection matrix to map sparse base features into a small number of high level features over a large number of auxiliary problems. One of the advantages of our approach is that it is simpler and more general than that of Ando and Zhang (2005). Our approach can easily be applied to other tasks by defining new feature contexts. 7 Conclusion In this paper, we have presented an approach to learning feature embeddings for dependency parsing from large amounts of raw data. B"
C14-1078,D12-1133,0,0.0372811,"ph-based parsing model proposed by McDonald et al. (2005). 2.1 Dependency parsing Given an input sentence x = (w0 , w1 , ..., wi , ..., wm ), where w0 is ROOT and wi (i = 0) refers to a word, the task of dependency parsing is to find y ∗ which has the highest score for x, y ∗ = arg max score(x, y) y∈Y (x) where Y (x) is the set of all the valid dependency trees for x. There are two major models (Nivre and McDonald, 2008): the transition-based model and graph-based model, which showed comparable accuracies for a wide range of languages (Nivre et al., 2007; Bohnet, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). We apply feature embeddings to a graph-based model in this paper. 2.2 Graph-based parsing model We use an ordered pair (wi , wj ) ∈ y to define a dependency relation in tree y from word wi to word wj (wi is the head and wj is the dependent), and Gx to define a graph that consists of a set of nodes Vx = {w0 , w1 , ..., wi , ..., wm } and a set of arcs (edges) Ex = {(wi , wj )|i = j, wi ∈ Vx , wj ∈ (Vx − {w0 })}. The parsing model of McDonald et al. (2005) searches for the maximum spanning tree (MST) in Gx . We denote Y (Gx ) as the set of all the subgraphs of Gx that are valid spanning trees"
C14-1078,C10-1011,0,0.102072,"hidden-class representations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline. 1 Introduction Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007; Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k sentences for dependency parsing), the chance of a feature occurring in the training data but not in the test data can be high. Another limitation on features is that many are typically derived by (manual) combination of atomic features. For example, given the head word (wh ) and part-of-speech tag (ph ), dependent word (wd ) and part-of-speech tag (pd ), and the label (l) of"
C14-1078,D07-1101,0,0.120651,"ned features but also benefit from the hidden-class representations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline. 1 Introduction Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007; Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k sentences for dependency parsing), the chance of a feature occurring in the training data but not in the test data can be high. Another limitation on features is that many are typically derived by (manual) combination of atomic features. For example, given the head word (wh ) and part-of-speech tag (ph ), dependent word (wd ) and part-of-s"
C14-1078,D09-1060,1,0.725213,"Missing"
C14-1078,D13-1129,1,0.599637,"in the Gigaword corpus. We report the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all punctuation tokens) with the correct HEAD. We also report the scores on complete dependency tree matches (COMP). 1 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We excluded the texts of PTB from the BLLIP WSJ Corpus. 3 We excluded the texts of CTB5 from the Gigaword data. 2 822 Baseline Baseline+BrownClu M2 Koo and Collins (2010) Zhang and Nivre (2011) Koo et al. (2008) Suzuki et al. (2009) Chen et al. (2009) Zhou et al. (2011) Suzuki et al. (2011) Chen et al. (2013) UAS 92.78 93.37 93.74 93.04 92.9 93.16 93.79 93.16 92.64 94.22 93.77 Table 5: Results on English data. N/A=Not Available. COMP 48.08 49.26 50.82 N/A 48.0 N/A N/A 47.15 46.61 N/A 51.36 Baseline M2 Li et al. (2011a) Hatori et al. (2011) Li et al. (2012) Chen et al. (2013) POS 93.61 93.61 93.08 93.94 94.51 N/A UAS 81.04 82.94 80.74 81.33 81.21 83.08 COMP 29.73 31.72 29.11 29.90 N/A 32.21 Table 6: Results on Chinese data. N/A=Not Available. 5.2 Development experiments In this section, we use the English development data to investigate the effects of different vector sizes of feature embeddings, a"
C14-1078,J81-4005,0,0.781478,"Missing"
C14-1078,P05-1004,0,0.174592,"d ; pd ], [wh ; ph ; wd ], and so on, in addition to the atomic features: [wh ], [ph ], etc. Such combination is necessary for high accuracies because the dominant approach uses linear models, which can not capture complex correlations between atomic features. We tackle the above issues by borrowing solutions from word representations, which have been intensely studied in the NLP community (Turian et al., 2010). In particular, distributed representations of words have been used for many NLP problems, which represent a word by information from the words it frequently co-occurs with (Lin, 1997; Curran, 2005; Collobert et al., 2011; Bengio, 2009; Mikolov et al., 2013b). The representation can be learned from large amounts of raw sentences, and hence used to reduce OOV rates in test data. In addition, since the representation of each word carries information about its context words, it can also be used to calculate word similarity (Mikolov et al., 2013a), or used as additional semantic features (Koo et al., 2008). In this paper, we show that a distributed representation can be learned for features also. Learned from large amount of automatically parsed data, the representation of each feature can"
C14-1078,I11-1136,0,0.0100531,"auto-parsed data. Our target is to learn feature embeddings that can not only make full use of well-established hand-designed features but also benefit from the hidden-class representations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline. 1 Introduction Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007; Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k sentences for dependency parsing), the chance of a feature occurring in the training data but not in the test data can be high. Another limitation on features is that many are typically derived by (manual) combination of ato"
C14-1078,P10-1001,0,0.0136798,"also benefit from the hidden-class representations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline. 1 Introduction Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007; Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k sentences for dependency parsing), the chance of a feature occurring in the training data but not in the test data can be high. Another limitation on features is that many are typically derived by (manual) combination of atomic features. For example, given the head word (wh ) and part-of-speech tag (ph ), dependent word (wd ) and part-of-speech tag (pd ), and th"
C14-1078,P08-1068,0,0.883169,", 2010). In particular, distributed representations of words have been used for many NLP problems, which represent a word by information from the words it frequently co-occurs with (Lin, 1997; Curran, 2005; Collobert et al., 2011; Bengio, 2009; Mikolov et al., 2013b). The representation can be learned from large amounts of raw sentences, and hence used to reduce OOV rates in test data. In addition, since the representation of each word carries information about its context words, it can also be used to calculate word similarity (Mikolov et al., 2013a), or used as additional semantic features (Koo et al., 2008). In this paper, we show that a distributed representation can be learned for features also. Learned from large amount of automatically parsed data, the representation of each feature can be defined on the ∗ Corresponding author This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 816 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 816–826, Dublin, Ireland, August 23-29"
C14-1078,P09-1058,0,0.0231128,"Missing"
C14-1078,I11-1171,0,0.0397526,"Missing"
C14-1078,D11-1109,1,0.812188,"Missing"
C14-1078,C12-1103,1,0.858885,"Missing"
C14-1078,P97-1009,0,0.39586,"wh ; ph ; wd ; pd ], [wh ; ph ; wd ], and so on, in addition to the atomic features: [wh ], [ph ], etc. Such combination is necessary for high accuracies because the dominant approach uses linear models, which can not capture complex correlations between atomic features. We tackle the above issues by borrowing solutions from word representations, which have been intensely studied in the NLP community (Turian et al., 2010). In particular, distributed representations of words have been used for many NLP problems, which represent a word by information from the words it frequently co-occurs with (Lin, 1997; Curran, 2005; Collobert et al., 2011; Bengio, 2009; Mikolov et al., 2013b). The representation can be learned from large amounts of raw sentences, and hence used to reduce OOV rates in test data. In addition, since the representation of each word carries information about its context words, it can also be used to calculate word similarity (Mikolov et al., 2013a), or used as additional semantic features (Koo et al., 2008). In this paper, we show that a distributed representation can be learned for features also. Learned from large amount of automatically parsed data, the representation of eac"
C14-1078,D07-1013,0,0.103419,"et al. (2013a) and Mikolov et al. (2013b) introduce efficient models to learn high-quality word embeddings from extremely large amounts of raw text, which offer a possible solution to the efficiency issue of learning feature embeddings. We adapt their approach for learning feature embeddings, showing how an unordered feature context can be used to learn the representation of a set of complex features. Using this method, a large number of embeddings are trained from automatically parsed texts, based on which a set of new features are designed and incorporated into a graph-based parsing model (McDonald and Nivre, 2007). We conduct experiments on the standard data sets of the Penn English Treebank and the Chinese Treebank V5.1. The results indicate that our proposed approach significantly improves parsing accuracies. 2 Background In this section, we introduce the background of dependency parsing and build a baseline parser based on the graph-based parsing model proposed by McDonald et al. (2005). 2.1 Dependency parsing Given an input sentence x = (w0 , w1 , ..., wi , ..., wm ), where w0 is ROOT and wi (i = 0) refers to a word, the task of dependency parsing is to find y ∗ which has the highest score for x,"
C14-1078,P05-1012,0,0.474326,"f complex features. Using this method, a large number of embeddings are trained from automatically parsed texts, based on which a set of new features are designed and incorporated into a graph-based parsing model (McDonald and Nivre, 2007). We conduct experiments on the standard data sets of the Penn English Treebank and the Chinese Treebank V5.1. The results indicate that our proposed approach significantly improves parsing accuracies. 2 Background In this section, we introduce the background of dependency parsing and build a baseline parser based on the graph-based parsing model proposed by McDonald et al. (2005). 2.1 Dependency parsing Given an input sentence x = (w0 , w1 , ..., wi , ..., wm ), where w0 is ROOT and wi (i = 0) refers to a word, the task of dependency parsing is to find y ∗ which has the highest score for x, y ∗ = arg max score(x, y) y∈Y (x) where Y (x) is the set of all the valid dependency trees for x. There are two major models (Nivre and McDonald, 2008): the transition-based model and graph-based model, which showed comparable accuracies for a wide range of languages (Nivre et al., 2007; Bohnet, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). We apply feature embeddings to a"
C14-1078,P08-1108,0,0.0123186,"icate that our proposed approach significantly improves parsing accuracies. 2 Background In this section, we introduce the background of dependency parsing and build a baseline parser based on the graph-based parsing model proposed by McDonald et al. (2005). 2.1 Dependency parsing Given an input sentence x = (w0 , w1 , ..., wi , ..., wm ), where w0 is ROOT and wi (i = 0) refers to a word, the task of dependency parsing is to find y ∗ which has the highest score for x, y ∗ = arg max score(x, y) y∈Y (x) where Y (x) is the set of all the valid dependency trees for x. There are two major models (Nivre and McDonald, 2008): the transition-based model and graph-based model, which showed comparable accuracies for a wide range of languages (Nivre et al., 2007; Bohnet, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). We apply feature embeddings to a graph-based model in this paper. 2.2 Graph-based parsing model We use an ordered pair (wi , wj ) ∈ y to define a dependency relation in tree y from word wi to word wj (wi is the head and wj is the dependent), and Gx to define a graph that consists of a set of nodes Vx = {w0 , w1 , ..., wi , ..., wm } and a set of arcs (edges) Ex = {(wi , wj )|i = j, wi ∈ Vx , wj ∈"
C14-1078,W04-2407,0,0.0218123,"Missing"
C14-1078,W96-0213,0,0.194199,"Missing"
C14-1078,P13-1045,0,0.057052,"heir tagger includes rich external resources. 823 6 Related work Learning feature embeddings are related to two lines of research: deep learning models for NLP, and semi-supervised dependency parsing. Recent studies used deep learning models in a variety of NLP tasks. Turian et al. (2010) applied word embeddings to chunking and Named Entity Recognition (NER). Collobert et al. (2011) designed a unified neural network to learn distributed representations that were useful for part-of-speech tagging, chunking, NER, and semantic role labeling. They tried to avoid task-specific feature engineering. Socher et al. (2013) proposed a Compositional Vector Grammar, which combined PCFGs with distributed word representations. Zheng et al. (2013) investigated Chinese character embeddings for Chinese word segmentation and part-of-speech tagging. Wu et al. (2013) directly applied word embeddings to Chinese dependency parsing. In most cases, words or characters were the inputs to the learning systems and word/character embeddings were used for the tasks. Our work is different in that we explore distributed representations at the feature level and we can make full use of well-established hand-designed features. We use l"
C14-1078,D09-1058,0,0.174023,"egmentation and tagging, and the Baseline parser was used to parse the sentences in the Gigaword corpus. We report the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all punctuation tokens) with the correct HEAD. We also report the scores on complete dependency tree matches (COMP). 1 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We excluded the texts of PTB from the BLLIP WSJ Corpus. 3 We excluded the texts of CTB5 from the Gigaword data. 2 822 Baseline Baseline+BrownClu M2 Koo and Collins (2010) Zhang and Nivre (2011) Koo et al. (2008) Suzuki et al. (2009) Chen et al. (2009) Zhou et al. (2011) Suzuki et al. (2011) Chen et al. (2013) UAS 92.78 93.37 93.74 93.04 92.9 93.16 93.79 93.16 92.64 94.22 93.77 Table 5: Results on English data. N/A=Not Available. COMP 48.08 49.26 50.82 N/A 48.0 N/A N/A 47.15 46.61 N/A 51.36 Baseline M2 Li et al. (2011a) Hatori et al. (2011) Li et al. (2012) Chen et al. (2013) POS 93.61 93.61 93.08 93.94 94.51 N/A UAS 81.04 82.94 80.74 81.33 81.21 83.08 COMP 29.73 31.72 29.11 29.90 N/A 32.21 Table 6: Results on Chinese data. N/A=Not Available. 5.2 Development experiments In this section, we use the English development data"
C14-1078,P11-2112,0,0.217599,"o parse the sentences in the Gigaword corpus. We report the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all punctuation tokens) with the correct HEAD. We also report the scores on complete dependency tree matches (COMP). 1 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We excluded the texts of PTB from the BLLIP WSJ Corpus. 3 We excluded the texts of CTB5 from the Gigaword data. 2 822 Baseline Baseline+BrownClu M2 Koo and Collins (2010) Zhang and Nivre (2011) Koo et al. (2008) Suzuki et al. (2009) Chen et al. (2009) Zhou et al. (2011) Suzuki et al. (2011) Chen et al. (2013) UAS 92.78 93.37 93.74 93.04 92.9 93.16 93.79 93.16 92.64 94.22 93.77 Table 5: Results on English data. N/A=Not Available. COMP 48.08 49.26 50.82 N/A 48.0 N/A N/A 47.15 46.61 N/A 51.36 Baseline M2 Li et al. (2011a) Hatori et al. (2011) Li et al. (2012) Chen et al. (2013) POS 93.61 93.61 93.08 93.94 94.51 N/A UAS 81.04 82.94 80.74 81.33 81.21 83.08 COMP 29.73 31.72 29.11 29.90 N/A 32.21 Table 6: Results on Chinese data. N/A=Not Available. 5.2 Development experiments In this section, we use the English development data to investigate the effects of different vector sizes of fe"
C14-1078,P10-1040,0,0.227454,"rt-of-speech tag (ph ), dependent word (wd ) and part-of-speech tag (pd ), and the label (l) of a dependency arc, state-of-the-art dependency parsers can have the combined features: [wh ; ph ], [wh ; ph ; wd ; pd ], [wh ; ph ; wd ], and so on, in addition to the atomic features: [wh ], [ph ], etc. Such combination is necessary for high accuracies because the dominant approach uses linear models, which can not capture complex correlations between atomic features. We tackle the above issues by borrowing solutions from word representations, which have been intensely studied in the NLP community (Turian et al., 2010). In particular, distributed representations of words have been used for many NLP problems, which represent a word by information from the words it frequently co-occurs with (Lin, 1997; Curran, 2005; Collobert et al., 2011; Bengio, 2009; Mikolov et al., 2013b). The representation can be learned from large amounts of raw sentences, and hence used to reduce OOV rates in test data. In addition, since the representation of each word carries information about its context words, it can also be used to calculate word similarity (Mikolov et al., 2013a), or used as additional semantic features (Koo et"
C14-1078,W13-5708,0,0.0202883,"Missing"
C14-1078,W03-3023,0,0.258589,"Missing"
C14-1078,D08-1059,1,0.690511,"d from large amounts of auto-parsed data. Our target is to learn feature embeddings that can not only make full use of well-established hand-designed features but also benefit from the hidden-class representations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline. 1 Introduction Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007; Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k sentences for dependency parsing), the chance of a feature occurring in the training data but not in the test data can be high. Another limitation on features is that many are typically derived by (manu"
C14-1078,P11-2033,1,0.861683,"epresentations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline. 1 Introduction Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007; Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k sentences for dependency parsing), the chance of a feature occurring in the training data but not in the test data can be high. Another limitation on features is that many are typically derived by (manual) combination of atomic features. For example, given the head word (wh ) and part-of-speech tag (ph ), dependent word (wd ) and part-of-speech tag (pd ), and the label (l) of a dependency arc, state"
C14-1078,D13-1061,0,0.0137224,"esearch: deep learning models for NLP, and semi-supervised dependency parsing. Recent studies used deep learning models in a variety of NLP tasks. Turian et al. (2010) applied word embeddings to chunking and Named Entity Recognition (NER). Collobert et al. (2011) designed a unified neural network to learn distributed representations that were useful for part-of-speech tagging, chunking, NER, and semantic role labeling. They tried to avoid task-specific feature engineering. Socher et al. (2013) proposed a Compositional Vector Grammar, which combined PCFGs with distributed word representations. Zheng et al. (2013) investigated Chinese character embeddings for Chinese word segmentation and part-of-speech tagging. Wu et al. (2013) directly applied word embeddings to Chinese dependency parsing. In most cases, words or characters were the inputs to the learning systems and word/character embeddings were used for the tasks. Our work is different in that we explore distributed representations at the feature level and we can make full use of well-established hand-designed features. We use large amounts of raw data to infer feature embeddings. There are several previous studies relevant to using raw data for d"
C14-1078,P11-1156,0,0.0129294,"Missing"
C14-1078,J92-4003,0,\N,Missing
C14-1078,D07-1096,0,\N,Missing
C14-1176,D10-1117,0,0.0238802,"s by sampling them from the posterior P (T |S), where T is a latent synchronous tree of a sentence pair S. As presented in the beginning of section 3, the posterior depends on P (αij,pq |Bij,pq = b) and P (βij,pq |Bij,pq = b), on which we put the PYP prior and the Dirichlet prior respectively. Because of integrating out all Gs in all of the priors, interdependency between samples of αij,pq |Bij,pq = b or βij,pq |Bij,pq = b is introduced, resulting in simultaneously obtaining multiple samples impractical. On the other hand, blocked sampling, which obtains sentence-level samples simultaneously (Blunsom and Cohn, 2010; Cohn et al., 2010; Johnson et al., 2007a) is attractive for the fast mixing speed and the easy application of standard dynamic programming algorithms. 4.1 Metropolis-Hastings (MH) Sampler We apply a MH sampler similar to (Johnson et al., 2007a) to overcome the difficulty of obtaining multiple samples simultaneously from posterior. The MH sampler is a MCMC technique that draws samples from a true distribution by first drawing samples simultaneously from a proposal distribution, and then correcting the samples to the true distribution by using an accept/reject test. In practical, the proposal"
C14-1176,J93-2003,0,0.0585004,"roves the quality of SMT results. 1 Introduction Traditional Statistical Machine Translation (SMT) learns translation model from bilingual corpus that is sentence aligned. No large-scale hand aligned structures inside the parallel sentences are usually available to the SMT community, while the aligned structures are essential for training the translation model. Thus, various unsupervised methods had been explored to automatically obtain aligned structures inside the parallel sentences. Currently, the dominant method is a two step pipeline that obtains word alignments by unsupervised learning (Brown et al., 1993) at the first step, then obtains aligned structures at the second step by heuristically extracting all bilingual structures that are consistent with the word alignments. The second step in this two step pipeline is problematic due to its obtained aligned structures, whose counts are heuristically collected and violate valid translation derivations, while most SMT decoders perform translation via valid translation derivations. This problem leads to researches on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction ai"
C14-1176,D09-1037,0,0.0224342,". The second step in this two step pipeline is problematic due to its obtained aligned structures, whose counts are heuristically collected and violate valid translation derivations, while most SMT decoders perform translation via valid translation derivations. This problem leads to researches on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction aims to directly obtain aligned structures by using one statistically sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level (Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al., 2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or discriminative modeling. This paper departs from using the above two traditional CFGs-based grammars, and proposes Synchronous Constituent Context Model (SCCM) which models synchronous constituents and contexts directly so that bilin"
C14-1176,P13-1077,0,0.184589,"on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction aims to directly obtain aligned structures by using one statistically sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level (Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al., 2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or discriminative modeling. This paper departs from using the above two traditional CFGs-based grammars, and proposes Synchronous Constituent Context Model (SCCM) which models synchronous constituents and contexts directly so that bilingual translational equivalences can be directly modeled. The proposed SCCM is inspired by researches on monolingual grammar induction, whose experience is valuable to the synchronous grammar induction community due to its standard evaluation on released monolingual treebanks, while no hand ann"
C14-1176,P07-1035,0,0.0339036,"priors, the Pitman-Yor-Process (PYP) prior, is defined on αij,pq |Bij,pq . The PYP prior can produce the power-law distribution (Goldwater et al., 2009) that is commonly observed in natural languages, and can flexibly model distributions on layer structures due to its defined distribution on distribution hierarchy. The PYP prior had been successfully applied on many NLP tasks such as language modeling (YeeWhye, 2006), word segmentation (Johnson et al., 2007b; Goldwater et al., 2011), dependency grammar induction (Cohen et al., 2008; Cohn et al., 2010), grammar refinement (Liang et al., 2007; Finkel et al., 2007) and Tree-Substitution Grammar induction (Cohn et al., 2010). We use the PYP to model the constituents’ layered structure by using the PYP’s distribution hierarchy. On βij,pq |Bij,pq , we use the Dirichlet distribution for its simplicity because contexts appear in much fewer kinds of surface strings than those of constituents. 3.1 The PYP Prior over Bilingual Constituents Constituents consist of both words and POS tags. Though in much monolingual grammar induction works, only POS tag sequences were used as the observed constituents for their significant hints of phrases (Klein and Manning, 200"
C14-1176,N07-1018,0,0.513429,"contexts, the proposed Bayesian prior over αij,pq |Bij,pq is more complicate than that over βij,pq |Bij,pq . Specifically, one of the non-parametric Bayesian priors, the Pitman-Yor-Process (PYP) prior, is defined on αij,pq |Bij,pq . The PYP prior can produce the power-law distribution (Goldwater et al., 2009) that is commonly observed in natural languages, and can flexibly model distributions on layer structures due to its defined distribution on distribution hierarchy. The PYP prior had been successfully applied on many NLP tasks such as language modeling (YeeWhye, 2006), word segmentation (Johnson et al., 2007b; Goldwater et al., 2011), dependency grammar induction (Cohen et al., 2008; Cohn et al., 2010), grammar refinement (Liang et al., 2007; Finkel et al., 2007) and Tree-Substitution Grammar induction (Cohn et al., 2010). We use the PYP to model the constituents’ layered structure by using the PYP’s distribution hierarchy. On βij,pq |Bij,pq , we use the Dirichlet distribution for its simplicity because contexts appear in much fewer kinds of surface strings than those of constituents. 3.1 The PYP Prior over Bilingual Constituents Constituents consist of both words and POS tags. Though in much mon"
C14-1176,P02-1017,0,0.399976,"orresponding Author This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1865 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1865–1874, Dublin, Ireland, August 23-29 2014. grammar induction. According to the evaluation results, the state-of-the-art monolingual grammar induction was achieved by Bayesian modeling of the Constituent Context Model (CCM) (Duan et al., 2013; Klein and Manning, 2002), while traditional CFGs based monolingual grammar induction methods perform well below the CCM. In view of the significant achievements of the CCM in monolingual grammar induction, we propose the SCCM to apply the CCM to the bilingual case. The tremendous possible constituents and contexts incurred in this bilingual case put a challenge for the SCCM to model such kind of sparse variables. We further propose a non-parametric Bayesian Modeling of the SCCM to cope with the sparse variables. Experiments on Chinese-English machine translation show that meaningful synchronous phrases can be detecte"
C14-1176,N03-1017,0,0.0725834,"used metric: the alignment error rate (AER) to evaluate our proposed alignments (a) against hand-annotated alignments, which are marked with sure (s) and possible (p) alignments. The AER is given by (the lower the better): AER(a, s, p) = 1 − |a ∩ s |+ |a ∩ p| |a |+ |s| In the HIT corpus, only sure alignments were annotated, possible alignments were bypassed because of the strict annotation standard of semantic equivalence. The word alignments evaluation results are reported in Table 2. The baseline was GIZA++ model 4 in both directions with symmetrization by the grow-diag-final-and heuristic (Koehn et al., 2003). A 1 The initialization with different random split bi-trees results in marginal variance of performances. HIT corpus is designed and constructed by HIT-MITLAB. http://mitlab.hit.edu.cn/index.php/resources.html 3 We did not use annotated tree node alignments for synchronous structure evaluation because the coverage of tree nodes that can be aligned is quite low. The reason of low coverage is that Chinese and English exhibit great syntax divergences from monolingual treebank point of view. 2 1871 released induction system - PIALIGN (Neubig et al., 2011)4 was also experimented to compare with o"
C14-1176,D12-1021,0,0.0932313,"and violate valid translation derivations, while most SMT decoders perform translation via valid translation derivations. This problem leads to researches on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction aims to directly obtain aligned structures by using one statistically sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level (Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al., 2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or discriminative modeling. This paper departs from using the above two traditional CFGs-based grammars, and proposes Synchronous Constituent Context Model (SCCM) which models synchronous constituents and contexts directly so that bilingual translational equivalences can be directly modeled. The proposed SCCM is inspired by researches on monolingual grammar induction, who"
C14-1176,D07-1072,0,0.0323014,"-parametric Bayesian priors, the Pitman-Yor-Process (PYP) prior, is defined on αij,pq |Bij,pq . The PYP prior can produce the power-law distribution (Goldwater et al., 2009) that is commonly observed in natural languages, and can flexibly model distributions on layer structures due to its defined distribution on distribution hierarchy. The PYP prior had been successfully applied on many NLP tasks such as language modeling (YeeWhye, 2006), word segmentation (Johnson et al., 2007b; Goldwater et al., 2011), dependency grammar induction (Cohen et al., 2008; Cohn et al., 2010), grammar refinement (Liang et al., 2007; Finkel et al., 2007) and Tree-Substitution Grammar induction (Cohn et al., 2010). We use the PYP to model the constituents’ layered structure by using the PYP’s distribution hierarchy. On βij,pq |Bij,pq , we use the Dirichlet distribution for its simplicity because contexts appear in much fewer kinds of surface strings than those of constituents. 3.1 The PYP Prior over Bilingual Constituents Constituents consist of both words and POS tags. Though in much monolingual grammar induction works, only POS tag sequences were used as the observed constituents for their significant hints of phrases ("
C14-1176,P11-1064,0,0.30999,"m leads to researches on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction aims to directly obtain aligned structures by using one statistically sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level (Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al., 2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or discriminative modeling. This paper departs from using the above two traditional CFGs-based grammars, and proposes Synchronous Constituent Context Model (SCCM) which models synchronous constituents and contexts directly so that bilingual translational equivalences can be directly modeled. The proposed SCCM is inspired by researches on monolingual grammar induction, whose experience is valuable to the synchronous grammar induction community due to its standard evaluation on released monolingual tre"
C14-1176,P03-1021,0,0.0215303,"onsists of a parallel corpus extracted from the Basic T ravel Expression Corpus (BTEC), which had been used in evaluation campaigns of the yearly International Workshop on Spoken Language Translation (IWSLT). Table 3 lists statistics of the corpus used in the experiment. Table 3: Statistics of the corpus used by IWSLT sent word avg. len. ch 23k 190k 8.3 en 213k 9.2 We used CSTAR03 as development set, used IWSLT04 and IWSLT05 official test set for test. A 4-gram language model with modified Kneser-Ney smoothing was trained on English side of parallel corpus. We use minimum error rate training (Och, 2003) with nbest list size 100 to optimize the feature weights for maximum development BLEU. Experimental results were evaluated by case-insensitive BLEU-4 (Papineni et al., 2001). Closest reference sentence length was used for brevity penalty. 5.3.2 Results Following (Levenberg et al., 2012; Neubig et al., 2011; Cohn and Haffari, 2013), we evaluate our model by using the SCCM’s output word alignments to construct a phrase table. As a baseline, we train a phrase-based model using the moses toolkit 5 based on the word alignments obtained using GIZA++ 4 5 http://www.phontron.com/pialign/ http://www.s"
C14-1176,W09-3804,0,0.0198619,"al/word analysis, while CCM has obtained state-of-the-art performance on the more complex unsupervised task - inducing syntactic trees. In view of CCM’s successful monolingual application, we generalize it to bilingual case. In depth comparison: our SCCM deals with both consituents and distituents, and contexts of them, while PIALIGN only deals with constituents. Furthermore, SCCM does not model non-terminal rewriting rules, while PIALIGN model those rules which can rewrite a non-terminal into a complete subtree as adaptor grammars does. In addition, PIALIGN adopts a beam search algorithm of (Saers et al., 2009). Through setting small beam size, PIALIGN’s time complexity is almost O(n3 ). But as critisized by (Cohn and Haffari, 2013), their heuristic beam search algorithm does not meet either of the Markov Chain Monte Carlo (MCMC) criteria of ergodicity or detailed balance. Our method adopts MCMC sampling (Johnson et al., 2007a) which meets the MCMC criteria. We can see that the two induction systems perform significantly better than GIZA++, and our proposed SCCM performs better than PIALIGN. Manual evaluation for the quality of the phrase pairs generated from word alignments is also reported in Tabl"
C14-1176,D13-1026,0,0.018532,"while most SMT decoders perform translation via valid translation derivations. This problem leads to researches on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction aims to directly obtain aligned structures by using one statistically sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level (Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al., 2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or discriminative modeling. This paper departs from using the above two traditional CFGs-based grammars, and proposes Synchronous Constituent Context Model (SCCM) which models synchronous constituents and contexts directly so that bilingual translational equivalences can be directly modeled. The proposed SCCM is inspired by researches on monolingual grammar induction, whose experience is valuable to the synchrono"
C14-1176,C12-1176,0,0.0152293,"lation derivations, while most SMT decoders perform translation via valid translation derivations. This problem leads to researches on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction aims to directly obtain aligned structures by using one statistically sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level (Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al., 2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or discriminative modeling. This paper departs from using the above two traditional CFGs-based grammars, and proposes Synchronous Constituent Context Model (SCCM) which models synchronous constituents and contexts directly so that bilingual translational equivalences can be directly modeled. The proposed SCCM is inspired by researches on monolingual grammar induction, whose experience is va"
C16-1136,P05-1074,0,0.026393,"l models, which can help to reproduce training data of monolingual model. Che et al. (2013) exploited the complementary cues between two languages as bilingual constraints to help detect errors in a mono-lingual tagger task, which can improve the annotation quality of named entities. Zhu et al. (2013) translated English sentences into Chinese sentences (with the same topic) in ACE 2005 evaluation data with google machine translation system as a second text representation feature 1448 so as to alleviate the data sparseness problem effectively. Our method is also related to paraphrase learning (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Snover et al., 2009; Ganitkevitch et al., 2013). However, there are two significant differences. First, paraphrase learning translates phrases strictly via word alignments while we use word alignments to find phrase spans on the target language. Second, our purpose is to obtain structured phrases (with syntactic constraints) rather than plain phrases as structured phrases can help us find new phrase structures as shown in Section 3. 7 Conclusion and Future Work We have presented a bilingual structure projection algorithm that explores structural diver"
C16-1136,W10-2906,0,0.0962151,"n word phrase (e.g., “sitins”) or phrases starting with a noun (e.g., “disobedience of order”), even a passive form phrase structure like “rallies held (in)”. In order to address this issue, we propose a simple yet effective bilingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn new event phrases and their syntactic structures, we map phrases2 back and ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 “xcomp” is a dependency relation between a verb or a"
C16-1136,D08-1021,0,0.0361866,"uce training data of monolingual model. Che et al. (2013) exploited the complementary cues between two languages as bilingual constraints to help detect errors in a mono-lingual tagger task, which can improve the annotation quality of named entities. Zhu et al. (2013) translated English sentences into Chinese sentences (with the same topic) in ACE 2005 evaluation data with google machine translation system as a second text representation feature 1448 so as to alleviate the data sparseness problem effectively. Our method is also related to paraphrase learning (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Snover et al., 2009; Ganitkevitch et al., 2013). However, there are two significant differences. First, paraphrase learning translates phrases strictly via word alignments while we use word alignments to find phrase spans on the target language. Second, our purpose is to obtain structured phrases (with syntactic constraints) rather than plain phrases as structured phrases can help us find new phrase structures as shown in Section 3. 7 Conclusion and Future Work We have presented a bilingual structure projection algorithm that explores structural divergences between languag"
C16-1136,W09-2307,0,0.0251991,"auses, LC: localizers, PU: punctuations, CD: cardinal numbers, MSP: some particles. 1445 Method H&R’s Iter #4 Iteration 1 Iteration 2 Iteration 3 Iteration 4 Phrases EP:623 PP:569 EP:1096 PP:2219 EP:4273 PP:4597 EP:8041 PP:9169 EP:9868 PP:11705 Recall Precision F1 71 88 79 76.2 86.5 81.1 79.2 86.0 82.5 79.2 86.0 82.5 79.2 86.0 82.5 Table 1: Results of the projection method using H&R’s phrase lists as seed phrases for expansion and projection Figure 5: F1-score curve against the number of iterations LDC2004T07. We ran Giza++ (Och, 2003) and Stanford dependency parser (De Marneffe et al., 2006; Chang et al., 2009) on the parallel sentence pairs to obtain word alignments and dependency trees. In addition, we used the same evaluation method and data as H&R’s. The evaluation data contains 400 news articles that were randomly sampled from the English Gigaword Fifth Edition corpora (Parker et al., 2011). Each article contains one of six commonly used civil unrest keywords or their morphological variations. The development set contains 100 documents and the rest 300 documents are used as the test set. 4.2 Event Recognition with Expanded Phrases We examine the effectiveness of our bilingual structure projecti"
C16-1136,N13-1006,0,0.136997,", “disobedience of order”), even a passive form phrase structure like “rallies held (in)”. In order to address this issue, we propose a simple yet effective bilingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn new event phrases and their syntactic structures, we map phrases2 back and ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 “xcomp” is a dependency relation between a verb or an adjective and its open clausal complement in a dependency tree"
C16-1136,W09-2209,0,0.180693,"e yet effective bilingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn new event phrases and their syntactic structures, we map phrases2 back and ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 “xcomp” is a dependency relation between a verb or an adjective and its open clausal complement in a dependency tree. In sentence “Workers came out to demonstrate”, the relation between verb “came” and verb “demonstrate” is “xcomp”. 2 We start with initial p"
C16-1136,P11-1061,0,0.0145497,"allies held in”. In addition, we have seen some new verb structures in English phrases that consist of a single verb or a verb with complex objects as shown in Table 5. 6 Related Work Recent years have witnessed increasing interests in leveraging bilingual corpora or resources to improve performance of monolingual NLP tasks. Generally, The introduction of bilingual corpora or resources serves two purposes. The first purpose is to alleviate the problem that we have few labeled instances in some resource-impoverished languages by a resource-rich language (Hwa et al., 2005; Ganchev et al., 2009; Das and Petrov, 2011; He et al., 2015). The second purpose is to leverage divergences found in different languages to obtain complementary cues (Li et al., 2012; Wang et al., 2013; Che et al., 2013) or extra information (Snyder et al., 2009; Burkett et al., 2010) from another language. Our projection method follows the latter. In the first purpose, Das and Petrov (2011) explored existing abundant English labeled resources as features to assist building tools for eight European languages. Different to projecting labels as feature, Wang and Manning (2014) proposed a method that projected model expectations as featu"
C16-1136,de-marneffe-etal-2006-generating,0,0.0528503,"Missing"
C16-1136,P09-1042,0,0.0313473,"voiced verb phrase “rallies held in”. In addition, we have seen some new verb structures in English phrases that consist of a single verb or a verb with complex objects as shown in Table 5. 6 Related Work Recent years have witnessed increasing interests in leveraging bilingual corpora or resources to improve performance of monolingual NLP tasks. Generally, The introduction of bilingual corpora or resources serves two purposes. The first purpose is to alleviate the problem that we have few labeled instances in some resource-impoverished languages by a resource-rich language (Hwa et al., 2005; Ganchev et al., 2009; Das and Petrov, 2011; He et al., 2015). The second purpose is to leverage divergences found in different languages to obtain complementary cues (Li et al., 2012; Wang et al., 2013; Che et al., 2013) or extra information (Snyder et al., 2009; Burkett et al., 2010) from another language. Our projection method follows the latter. In the first purpose, Das and Petrov (2011) explored existing abundant English labeled resources as features to assist building tools for eight European languages. Different to projecting labels as feature, Wang and Manning (2014) proposed a method that projected model"
C16-1136,N13-1092,0,0.0388965,"Missing"
C16-1136,georgi-etal-2012-measuring,0,0.024285,"d are ignored by the proposed algorithm. For instance, a verb phrase where two verbs are connected with a particular dependency relation “xcomp”1 , (e.g., “came out to demonstrate”) is one of these structures. Civil unrest events can also be invoked by some noun structure phrases, such as just a noun word phrase (e.g., “sitins”) or phrases starting with a noun (e.g., “disobedience of order”), even a passive form phrase structure like “rallies held (in)”. In order to address this issue, we propose a simple yet effective bilingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn"
C16-1136,N13-1005,1,0.716283,"languages (Chinese and English) and mines new phrases with new syntactic structures, which have been ignored in the previous work. Experiments show that our approach can successfully find novel event phrases and structures, e.g., phrases headed by nouns. Furthermore, the newly mined phrases are capable of recognizing additional event descriptions and increasing the recall of event recognition. 1 Introduction Event recognition aims to identify documents that describe a specific type of event. Accurate event recognition is challenging due to ambiguities of event keywords. In the previous work, Huang and Riloff (2013) (hereafter H&R) proposed multi-faceted event recognition method that uses event expressions as well as event defining characteristics (aka “event facets”, such as “agents” and “purpose”) to achieve high accuracy in identifying civil unrest events. They also presented a bootstrapping solution that can learn event expressions and event facet phrases from unannotated texts. However, to achieve high quality phrases, strict syntactic constraints have been enforced and their bootstrapping algorithm can only learn two particular types of V-O (Verb-Object) Structure for both event expressions and fac"
C16-1136,P03-1021,0,0.0690839,"Missing"
C16-1136,P09-1009,0,0.0352952,"g interests in leveraging bilingual corpora or resources to improve performance of monolingual NLP tasks. Generally, The introduction of bilingual corpora or resources serves two purposes. The first purpose is to alleviate the problem that we have few labeled instances in some resource-impoverished languages by a resource-rich language (Hwa et al., 2005; Ganchev et al., 2009; Das and Petrov, 2011; He et al., 2015). The second purpose is to leverage divergences found in different languages to obtain complementary cues (Li et al., 2012; Wang et al., 2013; Che et al., 2013) or extra information (Snyder et al., 2009; Burkett et al., 2010) from another language. Our projection method follows the latter. In the first purpose, Das and Petrov (2011) explored existing abundant English labeled resources as features to assist building tools for eight European languages. Different to projecting labels as feature, Wang and Manning (2014) proposed a method that projected model expectations as feature for training. He et al. (2015) transferred the sentiment information of a resource-rich language to replenish the lost information of the target language. In the second purpose, Chen and Ji (2009) proposed a bootstrap"
C16-1136,P09-1027,0,0.0438148,"ingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn new event phrases and their syntactic structures, we map phrases2 back and ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 “xcomp” is a dependency relation between a verb or an adjective and its open clausal complement in a dependency tree. In sentence “Workers came out to demonstrate”, the relation between verb “came” and verb “demonstrate” is “xcomp”. 2 We start with initial phrases lear"
C16-1136,Q14-1005,0,0.0156564,"resource-rich language (Hwa et al., 2005; Ganchev et al., 2009; Das and Petrov, 2011; He et al., 2015). The second purpose is to leverage divergences found in different languages to obtain complementary cues (Li et al., 2012; Wang et al., 2013; Che et al., 2013) or extra information (Snyder et al., 2009; Burkett et al., 2010) from another language. Our projection method follows the latter. In the first purpose, Das and Petrov (2011) explored existing abundant English labeled resources as features to assist building tools for eight European languages. Different to projecting labels as feature, Wang and Manning (2014) proposed a method that projected model expectations as feature for training. He et al. (2015) transferred the sentiment information of a resource-rich language to replenish the lost information of the target language. In the second purpose, Chen and Ji (2009) proposed a bootstrap framework of co-training among two languages, which uses Chinese event extraction as a case study and bilingual texts as a new source of information. Burkett et al. (2010) attached a bilingual model as a second view (Balcan and Blum, 2005; Ganchev et al., 2012) onto original monolingual models, and used rich features"
C16-1136,P08-1089,0,0.0303828,"onolingual model. Che et al. (2013) exploited the complementary cues between two languages as bilingual constraints to help detect errors in a mono-lingual tagger task, which can improve the annotation quality of named entities. Zhu et al. (2013) translated English sentences into Chinese sentences (with the same topic) in ACE 2005 evaluation data with google machine translation system as a second text representation feature 1448 so as to alleviate the data sparseness problem effectively. Our method is also related to paraphrase learning (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Snover et al., 2009; Ganitkevitch et al., 2013). However, there are two significant differences. First, paraphrase learning translates phrases strictly via word alignments while we use word alignments to find phrase spans on the target language. Second, our purpose is to obtain structured phrases (with syntactic constraints) rather than plain phrases as structured phrases can help us find new phrase structures as shown in Section 3. 7 Conclusion and Future Work We have presented a bilingual structure projection algorithm that explores structural divergences between languages and can effectiv"
C16-1202,P15-1098,0,0.0299232,"Figure 4: MAE vs number of training reviews per user (Amazon) identify the feature words directly extracted from comments. Recently some researchers try to learn latent aspects from comments (Wu and Ester, 2015). Wang et al. (2011) builds a regression model over the topic results from LDA. Their approach does not try to identify user profiles, and thus only applicable to score prediction on the text comments. Wang and Blei (2011) combine the merits of traditional collaborative filtering and topic modeling which provides a latent structure to recommend scientific articles in citation networks. Tang et al. (2015) learn representations of users and item for sentiment classification. Although similar concepts of user and item representations are used in (Tang et al., 2015), there are two major differences. Firstly, the problem we try to solve is completely different: they work on review sentiment classification, while our work focuses on score prediction without text. Secondly, the models are different: their approach includes user/product representation in CNN to enhance score prediction accuracy over text review, while out proposal attempts to maximize the likelihood of individual words. McAuley and L"
C16-1203,D08-1007,0,0.0248665,"cies for SMT. Selectional preferences place semantic restrictions on words, with which words can co-occur in different syntactic patterns. To be more specific, the SPs of a verb can characterize the semantic restrictions that the verb imposes on its arguments. Violating these restrictions inevitably makes sentence senses odd or implausible. For example, in the sentence “The ball drinks a potato.”, both subject and object preferences for the verb “drink” are violated. SPs have proven useful for numerous applications, e.g., semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997) and many more. Therefore, we have sufficient theoretical foundation to believe that SPs between verbs and arguments can be used to alleviate translation errors that we pointed out above. Our work consists of two parts: modeling SPs for verbs and incorporating SPs into an SMT system. In particular, we focus on the verb-object (v, obj) and verb-subject (v, subj) selectional preference instances which can be extracted from our target-side corpus. SPs are computed in two ways: conditionally ∗ Corresponding author T"
C16-1203,J07-2003,0,0.106199,"e fly. As for unseen object headword wun of a verb, we use Eq. (6) to model SPs when integrating conditional probability-based SP model and Eq. (3) to model SPs when integrating topicbased SP model. We store the selectional strength of (v, wun ) for each unknown word so as to avoid repetitive computation. Figure 2 shows the architecture of the SMT system equipped with verbal SPs translation model. Since the system we used is based on a CKY-style decoder, the integration algorithm introduced here can be easily adapted to other CKY-based decoding systems such as the hierarchical phrasal system (Chiang, 2007). 6 Experiments In order to validate the effectiveness of our SMT system enhanced with SPs, we perform a series of experiments on Chinese-to-English translation, which are trained with massive data. Specially, we aim at investigating: • Whether integrating SPs into SMT can improve the system translation accuracy. • Which can achieve better performance, conditionally probabilistic SP model or topic-based SP model? • Whether semantic similarity-based approach is more reasonable than assigning a uniform value as the selectional strength that a verb imposes on its unseen argument headwords. • Whet"
C16-1203,P07-1028,0,0.0110378,"nal association that uses WordNet synsets to provide conceptual classes for nouns co-occurring with a specific predicate in a particular relation. Li and Abe (1998) also rely on WordNet and use the principle of Minimum Description Length to find a suitable generalization level of a noun. But entirely relying on WordNet to generalize nouns to semantic classes has a fatal disadvantage because WordNet is lack of coverage of proper nouns. Therefore, Rooth et al. (1999) propose a probabilistic latent variable model using Expectation-Maximization (EM) clustering algorithm to induce class-based SPs. Erk (2007) investigates a similarity-based model which takes advantage of a corpus-based distributional similarity metrics between arguments for SPs. More recently, a number of researchers come up with methods modeling SPs via unsupervised topic models where topics express a set of latent classes for preferences with different grammatical relations. S´eaghdha (2010) describes a model using latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute SPs composed of a predicate and a single argument. In contrast, Ritter et al. (2010) study acquiring selectional preferences of a predicate and multiple"
C16-1203,J02-3001,0,0.0613325,"ces (SPs) to handle these verb-argument dependencies for SMT. Selectional preferences place semantic restrictions on words, with which words can co-occur in different syntactic patterns. To be more specific, the SPs of a verb can characterize the semantic restrictions that the verb imposes on its arguments. Violating these restrictions inevitably makes sentence senses odd or implausible. For example, in the sentence “The ball drinks a potato.”, both subject and object preferences for the verb “drink” are violated. SPs have proven useful for numerous applications, e.g., semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997) and many more. Therefore, we have sufficient theoretical foundation to believe that SPs between verbs and arguments can be used to alleviate translation errors that we pointed out above. Our work consists of two parts: modeling SPs for verbs and incorporating SPs into an SMT system. In particular, we focus on the verb-object (v, obj) and verb-subject (v, subj) selectional preference instances which can be extracted from our target-side corpus. SPs are computed in two w"
C16-1203,C10-1081,0,0.0242483,"encies between arguments and their dominating verbs. Verbs and arguments are often either incorrectly translated or not translated at all according to the error study by Wu and Fung (2009a). In order to address this issue, predicate-argument structures (PAS), which identify semantic frames within sentences by marking predicates, and labeling arguments with semantic roles, have been explored for SMT via various approaches in recent years. Wu and Fung (2009b) employ target-side PAS to pick out the most suitable translations among translation candidates after the decoding procedure is completed. Gildea (2010) integrates the PAS knowledge into decoding through projecting source-side PAS to the target-side via word alignments. In this paper, we are particularly interested in long-distance dependencies between verbs and their arguments in a predicate-argument structure. We propose to utilize selectional preferences (SPs) to handle these verb-argument dependencies for SMT. Selectional preferences place semantic restrictions on words, with which words can co-occur in different syntactic patterns. To be more specific, the SPs of a verb can characterize the semantic restrictions that the verb imposes on"
C16-1203,J98-2002,0,0.0250297,"nown words in SPs. Section 5 describes how we integrate the verbal SPs into SMT. Section 6 reports the experimental results. In the last section, we conclude with future directions. 2 Related Work Recent two decades have witnessed increasing efforts on automatic acquisition of SPs for verbs as well as wide applications of SPs in NLP tasks. Resnik (1996) is a pioneer on the induction of SPs from corpus, proposing a class-based approach named selectional association that uses WordNet synsets to provide conceptual classes for nouns co-occurring with a specific predicate in a particular relation. Li and Abe (1998) also rely on WordNet and use the principle of Minimum Description Length to find a suitable generalization level of a noun. But entirely relying on WordNet to generalize nouns to semantic classes has a fatal disadvantage because WordNet is lack of coverage of proper nouns. Therefore, Rooth et al. (1999) propose a probabilistic latent variable model using Expectation-Maximization (EM) clustering algorithm to induce class-based SPs. Erk (2007) investigates a similarity-based model which takes advantage of a corpus-based distributional similarity metrics between arguments for SPs. More recently,"
C16-1203,P10-1113,0,0.0311158,"s ) NIST04 36.40 36.93∗ 37.09∗ 36.89 36.99∗ 37.15∗∗ NIST05 33.69 34.22∗∗ 34.43∗∗ 34.19∗ 34.37∗∗ 34.21∗∗ Table 2: Results of conditionally probabilistic SPs with two selectional relations: (v, obj) and (v, sub). **/*: significantly better than the baseline at p &lt; 0.01 and p &lt; 0.05 respectively. Our 4-gram language model was trained on the Xinhua section of the English Gigaword corpus using the SRILM toolkit with modified Kneser-Ney smoothing. In order to automatically learn SPs for verbs, we first parsed all source sentences using Stanford Parser and then ran the Chinese semantic role labeler (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbs. At the same time, we ran SENNA on the target side to not only parse all target sentences but also conduct semantic role labeling for all verbs. It is easy to extract (vt , objt ) pairs or (vt , subt ) pairs after we obtained semantic roles on both sides. As for extracting (vs , objt ) selectional tuples, we first extracted (vs , objs ) pairs from source sentences with PAS and then used word alignments to get the target-side translation objt of objs . We used GibbsLDA++ to infer topics for our topic-based SP models. We set the"
C16-1203,P03-1021,0,0.0393616,"of topics according to results on our development set. We trained word embeddings with word2vec using continuous bag-of-words model (Mikolov et al., 2013). The word vector dimensionality was set to 200 and we set the value of threshold for occurrence of words to 0.00001. Values of other parameters such as the training algorithm and the size of the window were all set by default. We adopted the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We used the case-insensitive BLEU-4 (Papineni et al., 2002) to evaluate translation quality and run MERT (Och, 2003) three times. We finally recorded average BLEU scores over the three runs for all our experiments and used MultEval toolkit3 to perform the significance test. 6.2 Results Our first group of experiments is to investigate whether a simple conditional probability method for modeling SPs is able to improve translation accuracy in terms of BLEU. Moreover, we also would like to know whether the similarity-based SP model for unseen argument headwords will achieve further improvements. Experimental results are shown in Table 1. From the experiments which are conducted only using monolingual SPs, we ca"
C16-1203,N07-1071,0,0.0265878,"e semantic restrictions on words, with which words can co-occur in different syntactic patterns. To be more specific, the SPs of a verb can characterize the semantic restrictions that the verb imposes on its arguments. Violating these restrictions inevitably makes sentence senses odd or implausible. For example, in the sentence “The ball drinks a potato.”, both subject and object preferences for the verb “drink” are violated. SPs have proven useful for numerous applications, e.g., semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997) and many more. Therefore, we have sufficient theoretical foundation to believe that SPs between verbs and arguments can be used to alleviate translation errors that we pointed out above. Our work consists of two parts: modeling SPs for verbs and incorporating SPs into an SMT system. In particular, we focus on the verb-object (v, obj) and verb-subject (v, subj) selectional preference instances which can be extracted from our target-side corpus. SPs are computed in two ways: conditionally ∗ Corresponding author This work is licensed under a Creative Com"
C16-1203,P02-1040,0,0.102474,"50 to 350 with an incremental interval 50. We found the best number of topics according to results on our development set. We trained word embeddings with word2vec using continuous bag-of-words model (Mikolov et al., 2013). The word vector dimensionality was set to 200 and we set the value of threshold for occurrence of words to 0.00001. Values of other parameters such as the training algorithm and the size of the window were all set by default. We adopted the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We used the case-insensitive BLEU-4 (Papineni et al., 2002) to evaluate translation quality and run MERT (Och, 2003) three times. We finally recorded average BLEU scores over the three runs for all our experiments and used MultEval toolkit3 to perform the significance test. 6.2 Results Our first group of experiments is to investigate whether a simple conditional probability method for modeling SPs is able to improve translation accuracy in terms of BLEU. Moreover, we also would like to know whether the similarity-based SP model for unseen argument headwords will achieve further improvements. Experimental results are shown in Table 1. From the experime"
C16-1203,W97-0209,0,0.710203,"s can co-occur in different syntactic patterns. To be more specific, the SPs of a verb can characterize the semantic restrictions that the verb imposes on its arguments. Violating these restrictions inevitably makes sentence senses odd or implausible. For example, in the sentence “The ball drinks a potato.”, both subject and object preferences for the verb “drink” are violated. SPs have proven useful for numerous applications, e.g., semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997) and many more. Therefore, we have sufficient theoretical foundation to believe that SPs between verbs and arguments can be used to alleviate translation errors that we pointed out above. Our work consists of two parts: modeling SPs for verbs and incorporating SPs into an SMT system. In particular, we focus on the verb-object (v, obj) and verb-subject (v, subj) selectional preference instances which can be extracted from our target-side corpus. SPs are computed in two ways: conditionally ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence"
C16-1203,P10-1044,0,0.22079,"Missing"
C16-1203,P99-1014,0,0.150282,"wide applications of SPs in NLP tasks. Resnik (1996) is a pioneer on the induction of SPs from corpus, proposing a class-based approach named selectional association that uses WordNet synsets to provide conceptual classes for nouns co-occurring with a specific predicate in a particular relation. Li and Abe (1998) also rely on WordNet and use the principle of Minimum Description Length to find a suitable generalization level of a noun. But entirely relying on WordNet to generalize nouns to semantic classes has a fatal disadvantage because WordNet is lack of coverage of proper nouns. Therefore, Rooth et al. (1999) propose a probabilistic latent variable model using Expectation-Maximization (EM) clustering algorithm to induce class-based SPs. Erk (2007) investigates a similarity-based model which takes advantage of a corpus-based distributional similarity metrics between arguments for SPs. More recently, a number of researchers come up with methods modeling SPs via unsupervised topic models where topics express a set of latent classes for preferences with different grammatical relations. S´eaghdha (2010) describes a model using latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute SPs compose"
C16-1203,P10-1045,0,0.0397088,"Missing"
C16-1203,D14-1004,0,0.656145,"Missing"
C16-1203,2009.eamt-1.30,0,0.0203047,"hine translation (SMT). Although phrase-based SMT can deal with local context dependencies well, it performs rather poorly with long-distance dependencies and therefore causes a lot of lexical translation errors. Verbs and their arguments form such long-distance dependencies and play important roles in translation as they build skeletons of sentences. However, many SMT systems are not sufficient to capture long-distance dependencies between arguments and their dominating verbs. Verbs and arguments are often either incorrectly translated or not translated at all according to the error study by Wu and Fung (2009a). In order to address this issue, predicate-argument structures (PAS), which identify semantic frames within sentences by marking predicates, and labeling arguments with semantic roles, have been explored for SMT via various approaches in recent years. Wu and Fung (2009b) employ target-side PAS to pick out the most suitable translations among translation candidates after the decoding procedure is completed. Gildea (2010) integrates the PAS knowledge into decoding through projecting source-side PAS to the target-side via word alignments. In this paper, we are particularly interested in long-d"
C16-1203,N09-2004,0,0.0278013,"hine translation (SMT). Although phrase-based SMT can deal with local context dependencies well, it performs rather poorly with long-distance dependencies and therefore causes a lot of lexical translation errors. Verbs and their arguments form such long-distance dependencies and play important roles in translation as they build skeletons of sentences. However, many SMT systems are not sufficient to capture long-distance dependencies between arguments and their dominating verbs. Verbs and arguments are often either incorrectly translated or not translated at all according to the error study by Wu and Fung (2009a). In order to address this issue, predicate-argument structures (PAS), which identify semantic frames within sentences by marking predicates, and labeling arguments with semantic roles, have been explored for SMT via various approaches in recent years. Wu and Fung (2009b) employ target-side PAS to pick out the most suitable translations among translation candidates after the decoding procedure is completed. Gildea (2010) integrates the PAS knowledge into decoding through projecting source-side PAS to the target-side via word alignments. In this paper, we are particularly interested in long-d"
C16-1203,J97-3002,0,0.39082,"seen headword w to compute the value. sim(wun , w) is calculated with word2vec1 and the similarity metric: Cosine. After each word on the target-side corpus is projected into a multidimensional vector space, sim(wun , w) is computed as follows. P (ai × bi ) − − → → − wun • w i − − → → − (7) Sim(wun , w ) = −−−→ = rP − P ||swun ||× ||→ w || a 2× b 2 i i i i where ai and bi are the value of ith dimension of their word embeddings. 5 Decoding In this section, we mainly elaborate how to integrate the proposed SP models into a phrase-based SMT system built on bracketing transduction grammars (BTG) (Wu, 1997). Before we introduce the integration algorithm for SP models, we define two functions F and G on a source sentence and its predicateargument structure following Xiong et al. (2012). We use the sentence in Figure 1 as an example to make the two functions easier to be understood. • F (i, j): The function is used to find positions of all verbs and their object headwords pairs from the predicate-argument structure. These pairs are completely located within the source span (i, j). For example, in Figure 1, F (0, 4)={(2,3)}, F (0, 10)={(2,3), (4,10)} while F (0, 2)={} because the object headword “{"
C16-1203,P06-1066,1,0.472515,"to-English translation, which are trained with massive data. Specially, we aim at investigating: • Whether integrating SPs into SMT can improve the system translation accuracy. • Which can achieve better performance, conditionally probabilistic SP model or topic-based SP model? • Whether semantic similarity-based approach is more reasonable than assigning a uniform value as the selectional strength that a verb imposes on its unseen argument headwords. • Whether bilingual SPs are more effective than monolingual SPs for SMT. 6.1 Setup The baseline is a state-of-the-art BTG-based phrasal system (Xiong et al., 2006). Our training data corpora2 consist of 2.9M sentence pairs with 80.9M Chinese words and 86.4M English words. We ran GIZA++ on these corpora in both directions and then applied the “grow-diag-final” refinement rule to obtain final word alignments. Then we used all these word-aligned corpora to generate our phrase table. 2 The corpora include LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 2159 Model Base Base+Pc (objt |vt ) Base+Pc (objt |vt )+Pc (objtun |vt ) Base+Pc (subt |vt ) Base+Pc (subt |vt )+Pc (subtun |vt ) Base+Pcbil (objt |vs ) NIST04 36"
C16-1203,P12-1095,1,0.894388,"s for word sense disambiguation. Zapirain et al. (2009) employ SPs to process semantic role classification in a large dataset. Many researchers apply SPs to conduct pseudo-disambiguation tasks (Van de Cruys, 2014; Erk, 2007) in order to evaluate the performance of their methods of acquiring SPs. In contrast to plenty of applications of SPs in monolingual tasks, rather few efforts are devoted to incorporate SPs into SMT. To the best of our knowledge, we are the first to model SPs in the context of SMT. 2155 From the perspective of verb and argument translation, the most related work to ours is Xiong et al. (2012). They propose two translation models to incorporate source-side PAS into SMT. One is the predicate translation model exploring both lexical and semantic contexts to predict target-side predicates. The other is the argument reordering model which estimates the direction of target-side arguments movement relative to their predicates. The significant difference is that they separately model the translation of verbs and arguments while we model them in a unified fashion via SPs. 3 Selectional Preference Model Most approaches represent SPs for verbs as a function σ : (v, r, c) → s that maps each v"
C16-1203,P09-2019,0,0.0599932,"Missing"
C16-1240,J07-2003,0,0.0691293,"following intrinsic and extrinsic evaluations are based on this similarity measurement. 6 Experiments In this section, we carried out a series of experiments to validate the effectiveness of our proposed bilingual autoencoders on NIST Chinese-English translation tasks using large-scale bilingual training data. In particular, we investigated 1) whether our model is able to distinguish parallel sentence pairs from nonparallel sentences, and 2) whether our model can improve machine translation quality. 6.1 Setup Our translation decoder is a state-of-the-art hierarchical phrased-based SMT system (Chiang, 2007). The bilingual training data is the combination of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News), which contains 2.9M sentence pairs with 80.9M Chinese words and 86.4M English words. We used a 4-gram language model which was trained on the Xinhua section of the English Gigaword corpus (306M words) using the SRILM4 toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. In addition to the baseline decoder, we also compared our bilingual autoencoder against the abovementioned BBoWAE model (Chandar et al., 2014). To train this model, we used the"
C16-1240,P11-2031,0,0.0194853,"site direction (AS2MM). Throughout all experiments, we set B = 100, α = 0.100, λL = 10−6 , λW = 10−3 and m = 2d. With respect to the dimensionality of word embeddings, we tried four different dimensions from 25 to 100 with an increment of 25 each time. We used the NIST evaluation set of MT05 as our development set, and sets of MT06/MT08 as the test sets. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (MERT) (Och, 2003) to optimize the feature weights. In order to alleviate the instability of MERT, we followed Clark et al. (2011) to run MERT three times and report average BLEU scores over the three runs for all our MT experiments. 6.2 Intrinsic Evaluation: Semantic Analysis The first group of experiments aims at analyzing the ability of our model in distinguishing parallel sequences from nonparallel sequences, at both the phrase and sentence level. For convenience, we used MM2AS model and set d = 25 in all the following experiments. 4 http://www.speech.sri.com/projects/srilm/download.html http://www.sarathchandar.in/crl.html 6 We make this choice due to the following two reasons: 1) the correlation term is meaningless"
C16-1240,P13-1088,0,0.18244,"continuous-valued latent representations for parallel sentences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual sem"
C16-1240,P14-1006,0,0.0212393,"ntence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words embeddings (e.g. the avg representation) and hence are weak in capturing fine-grained complex linguistic phenomena. Therefore we believe that"
C16-1240,W13-3214,0,0.105523,"epresentations for parallel sentences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have pr"
C16-1240,P14-1062,0,0.0595323,"ences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, fo"
C16-1240,D14-1181,0,0.0300596,"intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following eff"
C16-1240,C12-1089,0,0.0339658,"; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are re"
C16-1240,P15-2029,0,0.0710106,"nd extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual"
C16-1240,P03-1021,0,0.00997899,"and max as the input descriptors, while avg and std as the target descriptors (MM2AS); and 2) the opposite direction (AS2MM). Throughout all experiments, we set B = 100, α = 0.100, λL = 10−6 , λW = 10−3 and m = 2d. With respect to the dimensionality of word embeddings, we tried four different dimensions from 25 to 100 with an increment of 25 each time. We used the NIST evaluation set of MT05 as our development set, and sets of MT06/MT08 as the test sets. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (MERT) (Och, 2003) to optimize the feature weights. In order to alleviate the instability of MERT, we followed Clark et al. (2011) to run MERT three times and report average BLEU scores over the three runs for all our MT experiments. 6.2 Intrinsic Evaluation: Semantic Analysis The first group of experiments aims at analyzing the ability of our model in distinguishing parallel sequences from nonparallel sequences, at both the phrase and sentence level. For convenience, we used MM2AS model and set d = 25 in all the following experiments. 4 http://www.speech.sri.com/projects/srilm/download.html http://www.sarathch"
C16-1240,P02-1040,0,0.0961855,"erms of input and target descriptors, we investigated two variants of the proposed bilingual autoencoder: 1) min and max as the input descriptors, while avg and std as the target descriptors (MM2AS); and 2) the opposite direction (AS2MM). Throughout all experiments, we set B = 100, α = 0.100, λL = 10−6 , λW = 10−3 and m = 2d. With respect to the dimensionality of word embeddings, we tried four different dimensions from 25 to 100 with an increment of 25 each time. We used the NIST evaluation set of MT05 as our development set, and sets of MT06/MT08 as the test sets. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (MERT) (Och, 2003) to optimize the feature weights. In order to alleviate the instability of MERT, we followed Clark et al. (2011) to run MERT three times and report average BLEU scores over the three runs for all our MT experiments. 6.2 Intrinsic Evaluation: Semantic Analysis The first group of experiments aims at analyzing the ability of our model in distinguishing parallel sequences from nonparallel sequences, at both the phrase and sentence level. For convenience, we used MM2AS model and set d = 25 in all the"
C16-1240,D11-1014,0,0.313555,"our bilingual antoencoder is able to learn continuous-valued latent representations for parallel sentences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence represent"
C16-1240,D15-1146,1,0.9177,"ed. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words embeddings (e.g."
C16-1240,P15-1150,0,0.163365,"uations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings ("
C16-1240,W11-0329,0,0.0345195,"arallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words embeddings (e.g. the avg representation) and hence are weak in capturing"
C16-1240,P14-1011,0,0.325772,"e language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words"
C16-1240,D15-1266,1,0.931462,"ical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2"
C16-1240,P15-1042,0,0.107821,"ally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words embeddings (e.g. the avg representation) and hence are weak in capturing fine-grained complex linguistic phenomena. Therefore we believe that modeling parallel se"
C16-1240,D13-1141,0,0.0312817,"ost of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and"
C18-1183,P16-1039,0,0.0252383,"utilize partial annotation learning and reinforcement learning to perform new-type named entity recognition in new domains. Several previous studies relevant to our approach have been conducted. NER. Most early studies treated NER task as the sequence labeling problem based on a large annotated corpus with supervised methods, such as HMM, MEMM (Hai and Ng, 2002) and CRF (Lafferty et al., 2001). Recently, neural networks have been explored by researchers (Collobert et al., 2011; Lample et al., 2016), and applied to reduce the weakness of feature sparsity problem and heavy feature engineering (Cai and Zhao, 2016). Those models have the similar architecture for decoding and feature extraction, which is chosen as our baseline model. In order to overcome the challenge of data deficient, some approaches based on weakly supervised learning (Nadeau et al., 2006; Riloff and Jones, 1999) have been proposed and successfully expand training data and feature space. However, it is difficult to implement these methods on Chinese tasks because of the lack of morphological variations such as capitalization and in particular the uncertainty in word segmentation, and it may cause large number of matching errors. 2166"
C18-1183,J81-4005,0,0.737702,"Missing"
C18-1183,W10-4204,0,0.0186873,"or the first time apply the CRF-PA model to NER, and employ distance supervision to produce partially annotated NE data. Reinforcement Learning. In recent years, reinforcement learning has become an issue in research, and applied successfully to many tasks. In text generation community, a deep Q-learning is served by Guo (2015) as generative model to improve the seq2seq model, which completes the process of decoding by Iteration. Li et al. (2016a) show how to apply deep reinforcement learning to model future reward in chatbot dialogue and capture the impact of this conversation in the future. Dethlefs (2010) aim to optimize the integration of NLG tasks that are inherently different in nature by learning a generation policy with reinforcement learning. For computer vision, Yeung et al. (2017) propose a reinforcement learning-based formulation select the right examples for training a classifier from noisy web search results. To more fine-grainedly select high-quality training sentences from noisy data, Feng et al. (2018) train an instance selector based on a policy function with reinforcement learning, which is inspirational to our model. 6 Conclusion This paper presents a new approach to utilize t"
C18-1183,N09-1037,0,0.0293189,"ficient, some approaches based on weakly supervised learning (Nadeau et al., 2006; Riloff and Jones, 1999) have been proposed and successfully expand training data and feature space. However, it is difficult to implement these methods on Chinese tasks because of the lack of morphological variations such as capitalization and in particular the uncertainty in word segmentation, and it may cause large number of matching errors. 2166 Under reasonable assumptions, OOV features should not be forced into certain tag. What’s more, joint models have also obtained great performance (Qian and Liu, 2013; Finkel and Manning, 2009). Learning from PAs. Learning from PAs has always been an attractive idea, since it usually requires much less or even none human annotation effort to obtain partially annotated data than fully annotated data, especially for complex tasks like sequence labeling. Li et al. (2012) propose to only manually annotate the most uncertain word boundaries in a sentence for Chinese word segmentation in order to reduce annotation cost. Tsuboi et al. (2008) extend the standard CRF to directly learn from incomplete annotations for sequence labeling tasks. This work refers to their model as CRF-PA. Jiang et"
C18-1183,C02-1025,0,0.205429,"Missing"
C18-1183,P13-1075,0,0.0718746,"Missing"
C18-1183,N16-1030,0,0.625965,"-generated annotations. In experiments, we create two datasets for Chinese named entity recognition in two domains with the help of distant supervision. The experimental results show that the proposed approach obtains better performance than the comparison systems on both two datasets. 1 Introduction In recent years, deep learning approaches have achieved great progress in the task of named entity recognition (NER) (Collobert et al., 2011; Chiu and Nichols, 2015). The standardized approach is that using BiLSTMs for encoding and then applying CRF for jointly label decoding (Huang et al., 2015; Lample et al., 2016). In addition, BiLSTMs and CNNs are employed to model character- or word-level representations (Ma and Hovy, 2016). Most previous studies on NER focus on a certain set of predefined NER types, such as organization, location, person, date, and so on, where a certain amount of labeled data is provided to train the models. However, different applications require particular entity types, such as “Brand” and “Product” in Ecommerce domain, and “Company” for finance industry. Considering the high cost of human annotation, it may not be feasible to annotate large amounts of labeled data for each new N"
C18-1183,W06-0115,0,0.0172484,"ontains 2,400 sentences tagged by annotators. We split the data into three sets: 1,200 sentences for training, 400 for dev, and 800 for testing. We collect a list of entities to construct dictionary from the training data. To reduce the effect of ambiguities, we remove the entry that belongs to more than one type, or it is a number or single character. Finally, the dictionary has 927 entries (included as EC.dic in supplementary materials). We perform distant supervision on raw data to obtain 2,500 sentences. NEWS: For news domain, we use a NER data from MSRA, which was used in Sighan-bakeoff (Levow, 2006). We only test our systems on the type of PERSON. We randomly select 3,000 sentences as training dataset, 3,328 as dev data, and 3,186 as testing data. The rest set is used as raw data, having 36,602 sentences. We collect a list of person names from the training data. To increase the coverage, we add an additional names to the list. Finally, the list has 71,664 entries (included as NEWS.dic in supplementary materials). We perform distant supervision on raw data to obtain 3,722 sentences. Embedding: In our approach, we need to map Chinese characters into vector representations by the lookup tab"
C18-1183,C12-2067,0,0.0237183,"variations such as capitalization and in particular the uncertainty in word segmentation, and it may cause large number of matching errors. 2166 Under reasonable assumptions, OOV features should not be forced into certain tag. What’s more, joint models have also obtained great performance (Qian and Liu, 2013; Finkel and Manning, 2009). Learning from PAs. Learning from PAs has always been an attractive idea, since it usually requires much less or even none human annotation effort to obtain partially annotated data than fully annotated data, especially for complex tasks like sequence labeling. Li et al. (2012) propose to only manually annotate the most uncertain word boundaries in a sentence for Chinese word segmentation in order to reduce annotation cost. Tsuboi et al. (2008) extend the standard CRF to directly learn from incomplete annotations for sequence labeling tasks. This work refers to their model as CRF-PA. Jiang et al. (2013) propose to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in Chinese word segmentation. Liu et al. (2014) and Yang and Vozila (2014) further im"
C18-1183,D16-1127,0,0.0115603,"e labeling tasks. This work refers to their model as CRF-PA. Jiang et al. (2013) propose to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in Chinese word segmentation. Liu et al. (2014) and Yang and Vozila (2014) further improve the work of Jiang et al. (2013) by employing the more sophisticated CRF-PA model. Marcheggiani and Arti`eres (2014) systematically compare a dozen uncertainty metrics in token-wise active learning with CRF-PA for several sequence labeling tasks. Li et al. (2016b) propose a coupled sequence labeling approach for exploiting heterogeneous data by treating the single-sided annotations as PAs for the task of joint word segmentation and POS tagging. In this work, we for the first time apply the CRF-PA model to NER, and employ distance supervision to produce partially annotated NE data. Reinforcement Learning. In recent years, reinforcement learning has become an issue in research, and applied successfully to many tasks. In text generation community, a deep Q-learning is served by Guo (2015) as generative model to improve the seq2seq model, which completes"
C18-1183,D16-1072,1,0.845342,"e labeling tasks. This work refers to their model as CRF-PA. Jiang et al. (2013) propose to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in Chinese word segmentation. Liu et al. (2014) and Yang and Vozila (2014) further improve the work of Jiang et al. (2013) by employing the more sophisticated CRF-PA model. Marcheggiani and Arti`eres (2014) systematically compare a dozen uncertainty metrics in token-wise active learning with CRF-PA for several sequence labeling tasks. Li et al. (2016b) propose a coupled sequence labeling approach for exploiting heterogeneous data by treating the single-sided annotations as PAs for the task of joint word segmentation and POS tagging. In this work, we for the first time apply the CRF-PA model to NER, and employ distance supervision to produce partially annotated NE data. Reinforcement Learning. In recent years, reinforcement learning has become an issue in research, and applied successfully to many tasks. In text generation community, a deep Q-learning is served by Guo (2015) as generative model to improve the seq2seq model, which completes"
C18-1183,D14-1093,0,0.0748401,"complex tasks like sequence labeling. Li et al. (2012) propose to only manually annotate the most uncertain word boundaries in a sentence for Chinese word segmentation in order to reduce annotation cost. Tsuboi et al. (2008) extend the standard CRF to directly learn from incomplete annotations for sequence labeling tasks. This work refers to their model as CRF-PA. Jiang et al. (2013) propose to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in Chinese word segmentation. Liu et al. (2014) and Yang and Vozila (2014) further improve the work of Jiang et al. (2013) by employing the more sophisticated CRF-PA model. Marcheggiani and Arti`eres (2014) systematically compare a dozen uncertainty metrics in token-wise active learning with CRF-PA for several sequence labeling tasks. Li et al. (2016b) propose a coupled sequence labeling approach for exploiting heterogeneous data by treating the single-sided annotations as PAs for the task of joint word segmentation and POS tagging. In this work, we for the first time apply the CRF-PA model to NER, and employ distance supervision to produc"
C18-1183,P16-1101,0,0.146503,"ith the help of distant supervision. The experimental results show that the proposed approach obtains better performance than the comparison systems on both two datasets. 1 Introduction In recent years, deep learning approaches have achieved great progress in the task of named entity recognition (NER) (Collobert et al., 2011; Chiu and Nichols, 2015). The standardized approach is that using BiLSTMs for encoding and then applying CRF for jointly label decoding (Huang et al., 2015; Lample et al., 2016). In addition, BiLSTMs and CNNs are employed to model character- or word-level representations (Ma and Hovy, 2016). Most previous studies on NER focus on a certain set of predefined NER types, such as organization, location, person, date, and so on, where a certain amount of labeled data is provided to train the models. However, different applications require particular entity types, such as “Brand” and “Product” in Ecommerce domain, and “Company” for finance industry. Considering the high cost of human annotation, it may not be feasible to annotate large amounts of labeled data for each new NER type, but small-scale data is available at some time. As an alternative solution, distant supervision can autom"
C18-1183,D14-1097,0,0.0461429,"Missing"
C18-1183,P09-1113,0,0.228853,"beled data is provided to train the models. However, different applications require particular entity types, such as “Brand” and “Product” in Ecommerce domain, and “Company” for finance industry. Considering the high cost of human annotation, it may not be feasible to annotate large amounts of labeled data for each new NER type, but small-scale data is available at some time. As an alternative solution, distant supervision can automatically generate large-scale labeled data for new-type NER without human-cost. The idea of distant supervision has widely used in the task of relation extraction (Mintz et al., 2009; Riedel et al., 2010; Zeng et al., 2015). For relation extraction, at first we have a knowledge base. If two entities e1 and e2 have relation r according to the knowledge base, then we populate this knowledge and assume the relation between e1 and e2 is r in the sentences that contain the both entities. In this way, we can produce a lot of labeled data for model training. Similarly, in our task, we first acquire a dictionary containing a list of the new-type entities. Then, we automatically generate large-scale labeled data by assuming that each entity mention in a sentence is a positive inst"
C18-1183,D15-1064,0,0.511038,"where “XX” is the type of entities. 2.2 The Baseline LSTM-CRF Given a sentence x = c1 c2 · · · cn , the goal is to assign an unique tag yi for Chinese character ci in the sentence. In general, the model predicts the entities in the sentence x by estimating the probability p(y|x), where y is a possible label sequence for sentence x. The final output ymax of the system for one sentence is the label sequence with the maximum probability. Here, we present a new NE tagger based on the LSTM-CRF model of Lample et al. (2016), which achieves the state-of-the-art performance in the NER task. Following Peng and Dredze (2015a), we represent Chinese characters as vectors and feed them into BiLSTM layer in the Chinese NER task. The left part of Figure 2 shows the framework of our baseline LSTM-CRF based NE tagger. The input layer. For each input sentence x = c1 c2 · · · cn , we map serialized characters into a list of vectors x1 x2 · · · xn with an embedding layer including a lookup table as its key parameter. Following Lample et al. (2016), the lookup table is initialized with embeddings pre-trained on a large-scale raw corpus and is further fine-tuned during our training process. The BiLSTM layer. With vector seq"
C18-1183,P13-2110,0,0.0244274,"challenge of data deficient, some approaches based on weakly supervised learning (Nadeau et al., 2006; Riloff and Jones, 1999) have been proposed and successfully expand training data and feature space. However, it is difficult to implement these methods on Chinese tasks because of the lack of morphological variations such as capitalization and in particular the uncertainty in word segmentation, and it may cause large number of matching errors. 2166 Under reasonable assumptions, OOV features should not be forced into certain tag. What’s more, joint models have also obtained great performance (Qian and Liu, 2013; Finkel and Manning, 2009). Learning from PAs. Learning from PAs has always been an attractive idea, since it usually requires much less or even none human annotation effort to obtain partially annotated data than fully annotated data, especially for complex tasks like sequence labeling. Li et al. (2012) propose to only manually annotate the most uncertain word boundaries in a sentence for Chinese word segmentation in order to reduce annotation cost. Tsuboi et al. (2008) extend the standard CRF to directly learn from incomplete annotations for sequence labeling tasks. This work refers to thei"
C18-1183,C08-1113,0,0.375003,"ork shoes)” is a product, but only the first two characters “工装(fatigue clothes)” are matched by the dictionary because “工装鞋(work shoes)” is not included in the dictionary. Obviously, such false labeled examples certainly provide wrong supervision during model training if we directly use the automatically generated data. In this paper, we propose an approach to handle the two problems of distantly supervised NER data. As for the incomplete annotation problem, we treat the data as partially annotated data based on the extended CRF-PA model that can directly learn from partial annotations (PA) (Tsuboi et al., 2008). The noisy annotation problem is also ubiquitous in distantly supervised for relation extraction, and researchers try to address this issue by using reinforcement learning (RL) technology to select positive instances (Feng et al., 2018). Inspired of their work, we design an instance selector to obtain clean instances from distantly supervised NER data. In summary, we make the following contributions: • We propose a novel approach for new-type named entity recognition, which firstly combines the advantages of both partial annotation learning and reinforcement learning, to handle the problems o"
C18-1183,D14-1010,0,0.0703814,"quence labeling. Li et al. (2012) propose to only manually annotate the most uncertain word boundaries in a sentence for Chinese word segmentation in order to reduce annotation cost. Tsuboi et al. (2008) extend the standard CRF to directly learn from incomplete annotations for sequence labeling tasks. This work refers to their model as CRF-PA. Jiang et al. (2013) propose to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in Chinese word segmentation. Liu et al. (2014) and Yang and Vozila (2014) further improve the work of Jiang et al. (2013) by employing the more sophisticated CRF-PA model. Marcheggiani and Arti`eres (2014) systematically compare a dozen uncertainty metrics in token-wise active learning with CRF-PA for several sequence labeling tasks. Li et al. (2016b) propose a coupled sequence labeling approach for exploiting heterogeneous data by treating the single-sided annotations as PAs for the task of joint word segmentation and POS tagging. In this work, we for the first time apply the CRF-PA model to NER, and employ distance supervision to produce partially annotated NE da"
C18-1183,D15-1203,0,0.0506862,"s. However, different applications require particular entity types, such as “Brand” and “Product” in Ecommerce domain, and “Company” for finance industry. Considering the high cost of human annotation, it may not be feasible to annotate large amounts of labeled data for each new NER type, but small-scale data is available at some time. As an alternative solution, distant supervision can automatically generate large-scale labeled data for new-type NER without human-cost. The idea of distant supervision has widely used in the task of relation extraction (Mintz et al., 2009; Riedel et al., 2010; Zeng et al., 2015). For relation extraction, at first we have a knowledge base. If two entities e1 and e2 have relation r according to the knowledge base, then we populate this knowledge and assume the relation between e1 and e2 is r in the sentences that contain the both entities. In this way, we can produce a lot of labeled data for model training. Similarly, in our task, we first acquire a dictionary containing a list of the new-type entities. Then, we automatically generate large-scale labeled data by assuming that each entity mention in a sentence is a positive instance of the corresponding type according"
C18-1215,D15-1075,0,0.185054,"c resources (Yih et al., 2013), tree edit distance (Yao and Durme, 2013) and named entities (Severyn and Moschitti, 2013). 1 https://www.taobao.com/ 2541 2 In deep learning methods, some neural network algorithms are employed to train learning models. Briefly, these methods could be categorized into three categories, i.e., siamense networks, attentive networks and compare-aggregate networks. In siamense networks, related studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In attentive networks, instead of using the final time step of LSTM to represent a sentence, related studies use the attention strategy to get the weight of overall time steps and then use the weight to represent the sentence. (Tan et al., 2016; Hermann et al., 2015, Yin et al., 2015). In compare-aggregate networks, related studies use different matching strategy to get relationships within words. (He and Lin, 2016; Wang et al., 2017; Wang and Jiang, 2016; Trischler et al., 2016; Parikn et al., 2016.). However, all above approaches are similar to our One vs. One Matching model which deals wi"
C18-1215,N16-1108,0,0.0177049,"studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In attentive networks, instead of using the final time step of LSTM to represent a sentence, related studies use the attention strategy to get the weight of overall time steps and then use the weight to represent the sentence. (Tan et al., 2016; Hermann et al., 2015, Yin et al., 2015). In compare-aggregate networks, related studies use different matching strategy to get relationships within words. (He and Lin, 2016; Wang et al., 2017; Wang and Jiang, 2016; Trischler et al., 2016; Parikn et al., 2016.). However, all above approaches are similar to our One vs. One Matching model which deals with the matching measurement between one sentence (or one piece of text) and another sentence (or another piece of text). In contrast, our approach is a One vs. Many Matching model which deals with the matching measurement between one sentence (or one piece of text) and multiple sentences (or multiple pieces of text). 3 Data Collection and Annotation We collect 4,060 question-answer pairs from “Asking All” in Taobao,"
C18-1215,D16-1244,0,0.0724571,"Missing"
C18-1215,D13-1044,0,0.0493654,"Missing"
C18-1215,P16-1044,0,0.189583,"mine whether an answer is answering a given question. For instance, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filmed in 2009 in Charl"
C18-1215,P16-1041,0,0.0278233,"ce, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filmed in 2009 in Charleston. A2: The film was released on May 25, 2010 on DVD. A: The respon"
C18-1215,C10-1131,0,0.083737,"Missing"
C18-1215,P16-1122,0,0.0269575,"Missing"
C18-1215,D15-1237,0,0.283339,"is a task to determine whether an answer is answering a given question. For instance, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filme"
C18-1215,N13-1106,0,0.0393979,"Missing"
C18-1215,P13-1171,0,0.02644,"erent from the above corpora, the question-answer pairs in our corpus are informal text. 2.2 Matching methods Generally speaking, QA matching methods could be split into two categories: shallow learning methods and deep learning methods. In shallow learning methods, some shallow learning algorithms, such as CRF, SVM and MaxEnt, are employed to train the learning models (Wang et al., 2010). Besides the learning algorithms, the related studies on shallow learning methods mainly focus on feature engineering, using linguistic tools and using external resources, such as lexical semantic resources (Yih et al., 2013), tree edit distance (Yao and Durme, 2013) and named entities (Severyn and Moschitti, 2013). 1 https://www.taobao.com/ 2541 2 In deep learning methods, some neural network algorithms are employed to train learning models. Briefly, these methods could be categorized into three categories, i.e., siamense networks, attentive networks and compare-aggregate networks. In siamense networks, related studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In atten"
C18-1257,W17-4710,0,0.0158788,"st sets. We use the caseinsensitive 4-gram NIST BLEU score (Papineni et al., 2002) for validation and evaluation, measured by mteval-v11b.pl script. EN-DE Translation. The EN-DE training data is provided by the standard benchmark ACL WMT 2017 2 , which consists of 5.85M sentence pairs with 141.42M English words and 134.83M German words respectively. We combine news-test-2012 and news-test-2013 as development set (6003 sentence pairs), and use news-test-2014 (News14), news-test-2015 (News15) and news-test-2016 (News16) as test sets (3003, 2169, and 2999 sentence pairs, respectively). Following Barone et al. (2017), we use the validation cross-entropy to choose the best model on the development set and use the case-sensitive 4-gram BLEU score for evaluation on test sets, measured by multi-bleu.perl script. 4.2 Training Details and Systems We train each model with sentences of length up to 50 words for ZH-EN and 60 words (tokens) for EN-DE. The source and target word embedding dimension is 620. The size of the hidden layer is 1000. We use Adam (Kingma and Ba, 2014) to optimize model parameters with a learning rate of 0.0002, and the mini-batch size of 80. For efficient training the neural networks, in ZH"
C18-1257,W14-4012,0,0.155806,"Missing"
C18-1257,D14-1179,0,0.0470279,"Missing"
C18-1257,P17-1106,0,0.0217228,"ant work to ours is Tu et al. (2017), in which they propose context gate to control the ratios of the source context (i.e., ct ) and target context (i.e., yt−1 and st−1 ) for computing next target state st . On the contrary, we use gate units to regulate information flow in computing the output state ot . Moreover, we propose adaptive weighting for GRU through gate units. Interpretation for neural networks. Attention mechanism (Bahdanau et al., 2015; Lin et al., 2017; Vaswani et al., 2017) offers a way of understanding the contribution of every source words to the generation of a target word. Ding et al. (2017) propose to use layer-wise relevance propagation (LRP) to 3046 interpret the internal workings of NMT and analyze translation errors. Moreover, Karpathy et al. (2015) and Li et al. (2016) propose to visualize and understand RNNs for natural language processing. In this work, we use the proposed gates in both encoder and decoder to analyze what types of information encoded in the encoder and what types of information influences the generation of a target word. 7 Conclusion In this paper, we present an approach to regulate the information flow in neural machine translation model explicitly. This"
C18-1257,P17-1012,0,0.0208119,"0.92 BLEU points for the two translation tasks. Moreover, we discuss in-depth on what type of information is encoded in the encoder and how information influences the generation of target words in the decoder. 1 Introduction Recent advances in neural machine translation (NMT) have achieved remarkable success over the stateof-the-art of statistical machine translation (SMT) on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Wu et al., 2016; Vaswani et al., 2017). In the neural networks of seq2seq models, either RNN-based (Bahdanau et al., 2015), CNN-based (Gehring et al., 2017), or full attention-based (Vaswani et al., 2017), there exist many scenarios in both encoder and decoder where a weighted sum model (WSM) takes a set of inputs and generate one output. As shown in Eq. 1, the WSM first combines k inputs (x1 , · · · , xk ) with k respective weights (w1 , · · · , wk ) and then non-linearizes it through an activation function f , such as tanh, sigmoid function, ReLU , and so on. In this paper we omit bias terms to make the equations less cluttered. o=f k X ! wi x i (1) i=1 Note that the above weights (w1 , · · · , wk ) are independent of each other and once the mo"
C18-1257,W15-3014,0,0.0206371,"English-to-German translation demonstrates that the proposed adaptive weighting is able to much improve translation accuracy by achieving significant improvement of 1.49 and 0.92 BLEU points for the two translation tasks. Moreover, we discuss in-depth on what type of information is encoded in the encoder and how information influences the generation of target words in the decoder. 1 Introduction Recent advances in neural machine translation (NMT) have achieved remarkable success over the stateof-the-art of statistical machine translation (SMT) on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Wu et al., 2016; Vaswani et al., 2017). In the neural networks of seq2seq models, either RNN-based (Bahdanau et al., 2015), CNN-based (Gehring et al., 2017), or full attention-based (Vaswani et al., 2017), there exist many scenarios in both encoder and decoder where a weighted sum model (WSM) takes a set of inputs and generate one output. As shown in Eq. 1, the WSM first combines k inputs (x1 , · · · , xk ) with k respective weights (w1 , · · · , wk ) and then non-linearizes it through an activation function f , such as tanh, sigmoid function, ReLU , and so on. In this pa"
C18-1257,W04-3250,0,0.391355,"Missing"
C18-1257,N16-1082,0,0.0360974,"target state st . On the contrary, we use gate units to regulate information flow in computing the output state ot . Moreover, we propose adaptive weighting for GRU through gate units. Interpretation for neural networks. Attention mechanism (Bahdanau et al., 2015; Lin et al., 2017; Vaswani et al., 2017) offers a way of understanding the contribution of every source words to the generation of a target word. Ding et al. (2017) propose to use layer-wise relevance propagation (LRP) to 3046 interpret the internal workings of NMT and analyze translation errors. Moreover, Karpathy et al. (2015) and Li et al. (2016) propose to visualize and understand RNNs for natural language processing. In this work, we use the proposed gates in both encoder and decoder to analyze what types of information encoded in the encoder and what types of information influences the generation of a target word. 7 Conclusion In this paper, we present an approach to regulate the information flow in neural machine translation model explicitly. This is done by employing adaptive weights through gate units. We apply adaptive weighting for both GRU and the output intermediate state. Experimental results on Chinese-to-English and Engli"
C18-1257,P17-1064,1,0.761036,"er the AER or SAER, the better the alignment quality. 5.2 Effects on Long Sentences Following Bahdanau et al. (2015), we group sentences of similar lengths together and compute BLEU scores. Figure 3 presents the BLEU scores over different lengths of input sentences. It shows that systems with adaptive weighting consistently outperform baseNMT on all sentence lengths. It also shows that the performance drops substantially when the length of input sentences increases from 20. This performance trend over the length is consistent with the translation output in (Wang et al., 2017; Tu et al., 2016; Li et al., 2017). 5.3 Quantitative Analysis Due to the continuous representations and non-linearity of neural networks. It is difficult to interpret the internal workings of NMT model (Ding et al., 2017). Fortunately, adaptive weights provide a mechanism to analyze what inputs of a WSM are more important than others. Next, we make insights on what type of 3044 information encoded in the encoder and what types of information has a greater impact on the generation of a target word. Content words Function words Chinese POS All verb adjective adverb geographical name temporal noun general noun person name preposi"
C18-1257,D15-1166,0,0.0540251,"ranslation demonstrates that the proposed adaptive weighting is able to much improve translation accuracy by achieving significant improvement of 1.49 and 0.92 BLEU points for the two translation tasks. Moreover, we discuss in-depth on what type of information is encoded in the encoder and how information influences the generation of target words in the decoder. 1 Introduction Recent advances in neural machine translation (NMT) have achieved remarkable success over the stateof-the-art of statistical machine translation (SMT) on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Wu et al., 2016; Vaswani et al., 2017). In the neural networks of seq2seq models, either RNN-based (Bahdanau et al., 2015), CNN-based (Gehring et al., 2017), or full attention-based (Vaswani et al., 2017), there exist many scenarios in both encoder and decoder where a weighted sum model (WSM) takes a set of inputs and generate one output. As shown in Eq. 1, the WSM first combines k inputs (x1 , · · · , xk ) with k respective weights (w1 , · · · , wk ) and then non-linearizes it through an activation function f , such as tanh, sigmoid function, ReLU , and so on. In this paper we omit bias ter"
C18-1257,J03-1002,0,0.0128954,"entation vectors (i.e., hidden states) of source words. We conjecture that better word representation will help the decoder be able to attend to appropriate source words in decoding. To test this hypothesis, we carry out experiments of the word alignment task on the evaluation dataset from Liu and Sun (2015), which contains 900 manually aligned Chinese-English sentence pairs. We force the decoder to output reference translations, as to get automatic alignments between input sentences and their reference translations. To evaluate alignment performance, we report the alignment error rate (AER) (Och and Ney, 2003) and the soft AER (SAER) (Tu et al., 2016) in Table 3. It shows that adaptive weighting improves the attention model. System baseNMT +Both AER 43.0 40.9 SAER 55.7 54.6 Table 3: Evaluation of word alignment for ZH-EN translation. The lower the AER or SAER, the better the alignment quality. 5.2 Effects on Long Sentences Following Bahdanau et al. (2015), we group sentences of similar lengths together and compute BLEU scores. Figure 3 presents the BLEU scores over different lengths of input sentences. It shows that systems with adaptive weighting consistently outperform baseNMT on all sentence len"
C18-1257,P02-1040,0,0.101609,"e tasks of Chinese-to-English (ZH-EN) and Englishto-German (EN-DE) machine translations. All source code is available on github. 1 4.1 Dataset and Evaluation Metrics ZH-EN Translation. Our training data for ZH-EN translation consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively. NIST MT 06 (1664 sentence pairs) is chosen as the development set while NIST MT 02, 03, 04, 05, and 08 datasets (878, 919, 1788, 1082 and 1357 sentence pairs, respectively) are used as our test sets. We use the caseinsensitive 4-gram NIST BLEU score (Papineni et al., 2002) for validation and evaluation, measured by mteval-v11b.pl script. EN-DE Translation. The EN-DE training data is provided by the standard benchmark ACL WMT 2017 2 , which consists of 5.85M sentence pairs with 141.42M English words and 134.83M German words respectively. We combine news-test-2012 and news-test-2013 as development set (6003 sentence pairs), and use news-test-2014 (News14), news-test-2015 (News15) and news-test-2016 (News16) as test sets (3003, 2169, and 2999 sentence pairs, respectively). Following Barone et al. (2017), we use the validation cross-entropy to choose the best model"
C18-1257,P16-1162,0,0.118344,"Missing"
C18-1257,P16-1008,0,0.0149852,"urce words. We conjecture that better word representation will help the decoder be able to attend to appropriate source words in decoding. To test this hypothesis, we carry out experiments of the word alignment task on the evaluation dataset from Liu and Sun (2015), which contains 900 manually aligned Chinese-English sentence pairs. We force the decoder to output reference translations, as to get automatic alignments between input sentences and their reference translations. To evaluate alignment performance, we report the alignment error rate (AER) (Och and Ney, 2003) and the soft AER (SAER) (Tu et al., 2016) in Table 3. It shows that adaptive weighting improves the attention model. System baseNMT +Both AER 43.0 40.9 SAER 55.7 54.6 Table 3: Evaluation of word alignment for ZH-EN translation. The lower the AER or SAER, the better the alignment quality. 5.2 Effects on Long Sentences Following Bahdanau et al. (2015), we group sentences of similar lengths together and compute BLEU scores. Figure 3 presents the BLEU scores over different lengths of input sentences. It shows that systems with adaptive weighting consistently outperform baseNMT on all sentence lengths. It also shows that the performance d"
C18-1257,Q17-1007,0,0.0890719,"Note that the above weights (w1 , · · · , wk ) are independent of each other and once the model is tuned, the weights are fixed for all inputs, suggesting that by ignoring different needs of the inputs, the WSM lacks effective control on the influence of each input. Let us take a concrete scenario in seq2seq model as an example. Figure 1(a) shows a typical illustration of generation of t-th target word where the decoder takes three inputs, i.e., source context ct , previous target word yt−1 and current target state st while generating one output yt via output state ot . However, the study in Tu et al. (2017) suggests that different target words require inconsistent contributions from source context (ct ) and target context (i.e., yt−1 and st ). For example, to generate translation Xinhua News Agency , Hong Kong, the first word y1 xinhua is highly related to its source context c1 while the ∗ Min Zhang is Corresponding Author. This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/ License details: http: 3038 Proceedings of the 27th International Conference on Computational Linguistics, pages 3038–3048 Santa Fe, New Mexico, USA, A"
D07-1076,P05-1053,1,0.924919,"” conveys the ACE-style relation “EMPLOYMENT.exec” between the entities “Bill Gates” (person name) and “Microsoft Corporation” (organization name). Extraction of semantic relations between entities can be very useful in many applic ations such as question answering, e.g. to answer the query “Who is the president of the United States?”, and information retrieval, e.g. to expand the query “George W. Bush”with “the president of the United States”via his relationship with “the United States”. Many researches have been done in relation extraction. Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entityrelated information to syntactic parse trees, dependency trees and semantic information. However, it is difficult for them to effectively capture structured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction. As an alternative to feature-based methods, tree kernel-based methods provide an elegant solution to explore implicitly structured features by directly computing the similarity between two trees. Alth"
D07-1076,P06-1016,1,0.776606,"Missing"
D07-1076,H05-1091,0,0.866971,"Missing"
D07-1076,P04-1054,0,\N,Missing
D07-1076,P05-1052,0,\N,Missing
D07-1076,P06-1104,1,\N,Missing
D07-1076,P01-1017,0,\N,Missing
D09-1073,P03-2041,0,0.182765,"eatures over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases."
D09-1073,D08-1089,0,0.0619998,"Missing"
D09-1073,H05-1021,0,0.0168165,"ject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly g"
D09-1073,P07-1091,0,0.0761394,"nce syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrase-based SMT system with BTG (Bracketing Transduction Grammar) constraints (Wu, 1997). Word and phrase reordering is a crucial component in a SMT system. In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear"
D09-1073,P07-1089,0,0.01819,"t-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of"
D09-1073,D08-1022,0,0.0322171,"tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phr"
D09-1073,P04-1043,0,0.183998,"chine learning method that can implicitly explore (structured) features in a high dimensional feature space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability. Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003). However, to our knowledge, such technique still remains unexplored for phrase reordering. In this paper, we look into the phrase reordering problem in two aspects: 1) how to model and optimize structured features, and 2) how to combine the structured features with other linear features and further integrate them into the loglinear model-based translation framework. Our study shows that: 1) the structured syntactic features are very useful a"
D09-1073,P06-1090,0,0.0388921,"Missing"
D09-1073,P02-1038,0,0.302388,"Missing"
D09-1073,P03-1021,0,0.0185461,"ining set, the NIST MT-2002 test set as development (dev) set and the NIST MT-2005 test set as test set. The Stanford parser (Klein and Manning, 2003) is used to parse Chinese sentences on the training, dev and test sets. GIZA++ (Och and Ney, 2004) and the heuristics “growdiag-final-and” are used to generate m-to-n word alignments. The translation model is trained on the FBIS corpus and a 4-gram language model is trained on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kenser and Ney, 1995). For the MER training (Och, 2003), we modify Koehn’s MER trainer (Koehn, 2004) to train our system. For significance test, we use Zhang et al’s implementation (Zhang et al, 2004). Baseline Systems: we set three baseline systems: B1) Moses (Koehn et al., 2007) that uses lexicalized unigram reordering model to predict three orientations: monotone, swap and discontinuous; B2) MaxEnt-based reordering model with lexical boundary word features only (Xiong et al., 2006); B3) Linguistically annotated reordering model for BTG-based (LABTG) SMT (Xiong et al., 2008). For Moses, we used the default settings. We build a CKY-style decoder"
D09-1073,J03-1002,0,0.00624359,"Missing"
D09-1073,J04-4002,0,0.599379,"the context of statistical machine translation. Our study reveals that the structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-ba"
D09-1073,P02-1040,0,0.0787709,"0450736/maxent.html A normal SMT decoder filters a translation model according to the source sentences, whereas in forced decoding, a translation model is filtered based on both source sentence and target references. In other words, in forced decoding, the decoder is forced to use those phrases whose translations are already in the references. 3 703 training, dev and test data. By forced decoding, we aim to isolate the reordering problem from those of OOV and lexical selections resulting from imperfect translation model in the context of a real SMT task. Besides the the case-sensitive BLEU-4 (Papineni et al., 2002) used in the two experiments, we design another evaluation metrics Reordering Accuracy (RAcc) for forced decoding evaluation. RAcc is the percentage of the adjacent word pairs with correct word order 4 over all words in one-best translation results. Similar to BLEU score, we also use the similar Brevity Penalty BP (Papineni et al., 2002) to penalize the short translations in computing RAcc. Finally, please note for the three evaluation metrics, the higher values represent better performance. reordering since only structured information is used in the tree kernel5. The CTs performs the worst am"
D09-1073,P07-1090,1,0.884642,"act of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica1 Thi"
D09-1073,P08-1066,0,0.0260045,"ell captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phras"
D09-1073,N04-4026,0,0.0407902,"rformance is subject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally in"
D09-1073,D07-1077,0,0.0420116,"Missing"
D09-1073,J97-3002,0,0.46112,"als that the structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased"
D09-1073,P08-2038,1,0.916679,"red by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica1 This paper follows the term convention of global reordering and local reordering of Li et al. (2007), between which the distinction is solely de"
D09-1073,P01-1067,0,0.126493,"he structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-synta"
D09-1073,P06-1006,0,0.0225869,"space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability. Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003). However, to our knowledge, such technique still remains unexplored for phrase reordering. In this paper, we look into the phrase reordering problem in two aspects: 1) how to model and optimize structured features, and 2) how to combine the structured features with other linear features and further integrate them into the loglinear model-based translation framework. Our study shows that: 1) the structured syntactic features are very useful and 2) our kernel-based model can well explore diverse knowledge, including previously-used linear featur"
D09-1073,C04-1030,0,0.0213454,"icitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning view"
D09-1073,W06-3108,0,0.0417611,"based method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica1 This paper follows the term convention of global reordering and local reordering o"
D09-1073,P06-1104,1,0.942046,"d) features in a high dimensional feature space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability. Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003). However, to our knowledge, such technique still remains unexplored for phrase reordering. In this paper, we look into the phrase reordering problem in two aspects: 1) how to model and optimize structured features, and 2) how to combine the structured features with other linear features and further integrate them into the loglinear model-based translation framework. Our study shows that: 1) the structured syntactic features are very useful and 2) our kernel-based model can well explore diverse knowledge,"
D09-1073,D07-1056,0,0.675501,"rdering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of how to utilize struc"
D09-1073,P07-1026,1,0.628254,"rdering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of how to utilize struc"
D09-1073,2007.mtsummit-papers.71,1,0.856955,"rdering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of how to utilize struc"
D09-1073,C08-1138,1,0.857272,"Missing"
D09-1073,C04-1073,0,\N,Missing
D09-1073,P03-1054,0,\N,Missing
D09-1073,P08-1064,1,\N,Missing
D09-1073,P06-1066,0,\N,Missing
D09-1073,W06-1628,0,\N,Missing
D09-1073,W06-1606,0,\N,Missing
D09-1073,W06-1609,0,\N,Missing
D09-1073,P05-1033,0,\N,Missing
D09-1073,N03-1017,0,\N,Missing
D09-1073,zhang-etal-2004-interpreting,0,\N,Missing
D09-1108,N04-1035,0,0.124929,"to h. A non-terminal node in a packed forest can be represented as “label [start, stop]”, where “label” is its syntax category and “[start, stop]” is the range of words it covers. For example, the node in Fig. 5 pointed by the dark arrow is labelled as “NP[3,4]”, where NP is its label and [3,4] means that it covers the span from the 3rd word to the 4th word. In forest-based translation, rule matching is much more complicated than the tree-based one. XNA declaration is related to some regulation Figure 2. A packed forest Figure 1. A tree-to-string translation process. The tree-to-string model (Galley et al. 2004; Liu et al. 2006) views the translation as a structure map1038 Zhang et al. (2009) reduce the tree sequence problem into tree problem by introducing virtual node and related forest conversion algorithms, so the algorithm proposed in this paper is also applicable to the tree sequence-based models. Figure 3. Tree 1 (T1) 3 Figure 4. Tree 2 (T2) Matching Methods in Previous Work In this section, we discuss the two typical rule matching algorithms used in previous work. 3.1 For example, if we want to extract useful rules for node NP[3,4] in Fig 5, we have to generate all the tree fragments rooted"
D09-1108,P01-1044,0,0.145454,"tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence. Fig. 1 illustrates this process. In real translation, the number of possible tree fragment segmentations for a given input tree is exponential in the number of tree nodes. 2.2 Forest-based translation To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. A packed forest (Tomita 1987; Klein and Manning, 2001; Huang and Chiang, 2005) is a compact representation of many possible parse trees of a sentence, which can be for, where V is mally described as a triple the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence represented as an ordered word sequence. A hyper-edge in a packed forest is a group of edges in a tree which connects a father node to all its children nodes, representing a CFG-based parse rule. Fig. 2 is a packed forest incorporating two parse trees T1 and T2 of a sentence as shown in Fig. 3 and Fig. 4. Given a hyper-edge e, let h be its father node, then we say"
D09-1108,J99-4005,0,0.0555456,"anslation rules are extracted from the entire rule set by matching the source parse tree/forest. The second step is to decode the source sentence into its target one using the extracted translation rules. Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as 1 Given a source structure (either a parse tree or a parse forest), a translation rule is applicable if and only if the left hand side of the translation rule exactly matches a tree fragment of the given source structure. an NP-hard search problem (Knight, 1999). In the SMT research community, the second step has been well studied and many methods have been proposed to speed up the decoding process, such as node-based or span-based beam search with different pruning strategies (Liu et al., 2006; Zhang et al., 2008a, 2008b) and cube pruning (Huang and Chiang, 2007; Mi et al., 2008). However, the first step attracts less attention. The previous solution to this problem is to do exhaustive searching with heuristics on each tree/forest node or on each source span. This solution becomes computationally infeasible when it is applied to packed forests with"
D09-1108,P07-2045,0,0.0122955,"Missing"
D09-1108,P06-1077,0,0.15997,"node in a packed forest can be represented as “label [start, stop]”, where “label” is its syntax category and “[start, stop]” is the range of words it covers. For example, the node in Fig. 5 pointed by the dark arrow is labelled as “NP[3,4]”, where NP is its label and [3,4] means that it covers the span from the 3rd word to the 4th word. In forest-based translation, rule matching is much more complicated than the tree-based one. XNA declaration is related to some regulation Figure 2. A packed forest Figure 1. A tree-to-string translation process. The tree-to-string model (Galley et al. 2004; Liu et al. 2006) views the translation as a structure map1038 Zhang et al. (2009) reduce the tree sequence problem into tree problem by introducing virtual node and related forest conversion algorithms, so the algorithm proposed in this paper is also applicable to the tree sequence-based models. Figure 3. Tree 1 (T1) 3 Figure 4. Tree 2 (T2) Matching Methods in Previous Work In this section, we discuss the two typical rule matching algorithms used in previous work. 3.1 For example, if we want to extract useful rules for node NP[3,4] in Fig 5, we have to generate all the tree fragments rooted at node NP[3,4] as"
D09-1108,P07-1089,0,0.149621,"Missing"
D09-1108,P08-1023,0,0.272369,"Missing"
D09-1108,D08-1022,0,0.407315,"d tree-to-string translation model which serves as the translation platform in this paper. 2.1 Tree-to-string model ping process, which first breaks the source syntax tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence. Fig. 1 illustrates this process. In real translation, the number of possible tree fragment segmentations for a given input tree is exponential in the number of tree nodes. 2.2 Forest-based translation To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. A packed forest (Tomita 1987; Klein and Manning, 2001; Huang and Chiang, 2005) is a compact representation of many possible parse trees of a sentence, which can be for, where V is mally described as a triple the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence represented as an ordered word sequence. A hyper-edge in a packed forest is a group of edges in a tree which connects a father node to all its children nodes, representing a CFG-based parse rule."
D09-1108,J03-1002,0,0.00845557,"Missing"
D09-1108,J87-1004,0,0.554845,"source syntax tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence. Fig. 1 illustrates this process. In real translation, the number of possible tree fragment segmentations for a given input tree is exponential in the number of tree nodes. 2.2 Forest-based translation To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. A packed forest (Tomita 1987; Klein and Manning, 2001; Huang and Chiang, 2005) is a compact representation of many possible parse trees of a sentence, which can be for, where V is mally described as a triple the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence represented as an ordered word sequence. A hyper-edge in a packed forest is a group of edges in a tree which connects a father node to all its children nodes, representing a CFG-based parse rule. Fig. 2 is a packed forest incorporating two parse trees T1 and T2 of a sentence as shown in Fig. 3 and Fig. 4. Given a hyper-edge e, let h be its"
D09-1108,zhang-etal-2004-interpreting,0,0.105268,"Missing"
D09-1108,A00-2018,0,\N,Missing
D09-1108,C08-1138,1,\N,Missing
D09-1108,W05-1506,0,\N,Missing
D09-1108,P02-1040,0,\N,Missing
D09-1108,P09-1020,1,\N,Missing
D09-1108,P08-1064,1,\N,Missing
D09-1108,P07-1019,0,\N,Missing
D09-1108,W01-1812,0,\N,Missing
D09-1108,P03-1021,0,\N,Missing
D09-1161,E03-1005,0,0.0765955,"Missing"
D09-1161,P06-1055,0,0.260545,"a head-driven lexicalized model and a latent-annotation-based un-lexicalized model. Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively. 1 Introduction Statistical models have achieved great success in language parsing and obtained the state-of-theart results in a variety of languages. In general, they can be divided into two major categories, namely lexicalized models (Collins 1997, 1999; Charniak 1997, 2000) and un-lexicalized models (Klein and Manning 2003; Matsuzaki et al. 2005; Petrov et al. 2006; Petrov and Klein 2007). In lexicalized models, word information play a key role in modeling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual parsing models, many system combination meth"
D09-1161,N07-1051,0,0.289935,"lized model and a latent-annotation-based un-lexicalized model. Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively. 1 Introduction Statistical models have achieved great success in language parsing and obtained the state-of-theart results in a variety of languages. In general, they can be divided into two major categories, namely lexicalized models (Collins 1997, 1999; Charniak 1997, 2000) and un-lexicalized models (Klein and Manning 2003; Matsuzaki et al. 2005; Petrov et al. 2006; Petrov and Klein 2007). In lexicalized models, word information play a key role in modeling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual parsing models, many system combination methods for parsing have bee"
D09-1161,P02-1035,0,0.0204866,"versity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual parsing models, many system combination methods for parsing have been proposed (Henderson and Brill 1999; Zeman and Žabokrtský 2005; Sagae and Lavie 2006) and promising performance improvements have been reported. In addition, parsing re-ranking (Collins 2000; Riezler et al. 2002; Charniak and Johnson 2005; Huang 2008) has also been shown to be another effective technique to improve parsing performance. This technique utilizes a bunch of linguistic features to re-rank the k-best (Huang and Chiang 2005) output on the forest level or tree level. In prior work, system combination was applied on multiple parsers while re-ranking was applied on the k-best outputs of individual parsers. In this paper, we propose a linear model-based general framework for multiple parsers combination. The proposed framework leverages on the strengths of previous system combination and rerank"
D09-1161,P97-1003,0,0.239574,"both the Chinese and English Penn Treebank syntactic parsing task by combining two stateof-the-art parsing models, a head-driven lexicalized model and a latent-annotation-based un-lexicalized model. Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively. 1 Introduction Statistical models have achieved great success in language parsing and obtained the state-of-theart results in a variety of languages. In general, they can be divided into two major categories, namely lexicalized models (Collins 1997, 1999; Charniak 1997, 2000) and un-lexicalized models (Klein and Manning 2003; Matsuzaki et al. 2005; Petrov et al. 2006; Petrov and Klein 2007). In lexicalized models, word information play a key role in modeling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural"
D09-1161,N06-2033,0,0.822122,"ling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual parsing models, many system combination methods for parsing have been proposed (Henderson and Brill 1999; Zeman and Žabokrtský 2005; Sagae and Lavie 2006) and promising performance improvements have been reported. In addition, parsing re-ranking (Collins 2000; Riezler et al. 2002; Charniak and Johnson 2005; Huang 2008) has also been shown to be another effective technique to improve parsing performance. This technique utilizes a bunch of linguistic features to re-rank the k-best (Huang and Chiang 2005) output on the forest level or tree level. In prior work, system combination was applied on multiple parsers while re-ranking was applied on the k-best outputs of individual parsers. In this paper, we propose a linear model-based general framework"
D09-1161,W02-1001,0,0.114872,"and English Penn Treebank corpus. Experimental results show that our final results, an F-Score of 92.62 on English and 85.45 on Chinese, outperform the previously best-reported systems by 0.52 point and 1.21 point, respectively. This convincingly demonstrates the effectiveness of our proposed framework. Our study also shows that the simulated-annealing algorithm (Kirkpatrick et al. 1983) is more effective 1552 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1552–1560, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP than the perceptron algorithm (Collins 2002) for feature weight tuning. The rest of this paper is organized as follows. Section 2 briefly reviews related work. Section 3 discusses our method while section 4 presents the feature weight tuning algorithm. In Section 5, we report our experimental results and then conclude in Section 6. 2 Related Work As discussed in the previous section, system combination and re-ranking are two techniques to improve parsing performance by postprocessing parsers’ k-best outputs. Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an ent"
D09-1161,P08-1067,0,0.121123,"rom each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual parsing models, many system combination methods for parsing have been proposed (Henderson and Brill 1999; Zeman and Žabokrtský 2005; Sagae and Lavie 2006) and promising performance improvements have been reported. In addition, parsing re-ranking (Collins 2000; Riezler et al. 2002; Charniak and Johnson 2005; Huang 2008) has also been shown to be another effective technique to improve parsing performance. This technique utilizes a bunch of linguistic features to re-rank the k-best (Huang and Chiang 2005) output on the forest level or tree level. In prior work, system combination was applied on multiple parsers while re-ranking was applied on the k-best outputs of individual parsers. In this paper, we propose a linear model-based general framework for multiple parsers combination. The proposed framework leverages on the strengths of previous system combination and reranking methods and is open to any type of f"
D09-1161,W01-1812,0,0.0341403,"Briefly speaking, latent-annotation model views each non-terminal in the Treebank as a non-terminal followed by a set of latent variables, and uses EM algorithms to automatically learn the latent variables’ probability functions to maximize the probability of the given training data. Take the following binarized rule as example, could be viewed as the set of rules The process of computing the probability of a normal tree is to first binarized all the rules in it, and then replace each rule to the corresponding set of rules with latent variables. Now the previous tree becomes a packed forest (Klein and Manning 2001; Petrov et al. 2007) in the latentannotation model, and its probability is the inside probability of the root node. This model is quite different from the head-driven model in which 1554 the probability of a tree is just the product all the rules’ probability. 3.3 Constituent Counts Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). A constituent is a non-terminal node covering a special span. For example, “NP[2,4]” means a constituent labelled as “NP” which covers the span from the second w"
D09-1161,P03-1054,0,0.0220626,"combining two stateof-the-art parsing models, a head-driven lexicalized model and a latent-annotation-based un-lexicalized model. Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively. 1 Introduction Statistical models have achieved great success in language parsing and obtained the state-of-theart results in a variety of languages. In general, they can be divided into two major categories, namely lexicalized models (Collins 1997, 1999; Charniak 1997, 2000) and un-lexicalized models (Klein and Manning 2003; Matsuzaki et al. 2005; Petrov et al. 2006; Petrov and Klein 2007). In lexicalized models, word information play a key role in modeling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual p"
D09-1161,W99-0623,0,0.81626,"ized models, word information play a key role in modeling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity. Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper). Therefore, it is natural to combine the two models for better parsing performance. Besides individual parsing models, many system combination methods for parsing have been proposed (Henderson and Brill 1999; Zeman and Žabokrtský 2005; Sagae and Lavie 2006) and promising performance improvements have been reported. In addition, parsing re-ranking (Collins 2000; Riezler et al. 2002; Charniak and Johnson 2005; Huang 2008) has also been shown to be another effective technique to improve parsing performance. This technique utilizes a bunch of linguistic features to re-rank the k-best (Huang and Chiang 2005) output on the forest level or tree level. In prior work, system combination was applied on multiple parsers while re-ranking was applied on the k-best outputs of individual parsers. In this paper,"
D09-1161,J93-2004,0,0.0341485,"Missing"
D09-1161,N06-1020,0,0.0614305,"two parameter estimation algorithms with significant test; “SA.” is simulated annealing, “AP.” is averaged perceptron, “P-value” is the significant test p-value. 5.6 Table 6. F1 score on 50-best combination with different feature configuration. “I” means the constituent count, “B” means Berkeley parser confidence score and “C” means Charniak parser confidence score. 5.5 Algo. Lang Performance-Enhanced Parsers on English Individual For Charniak’s lexicalized parser, there are two techniques to improve its performance. One is reranking as explained in section 2. The other is the self-training (McClosky et al. 2006) which first parses and reranks the NANC corpus, and then use them as additional training data to retrain the model. In this sub-section, we apply our method to combine the Berkeley parser and the enhanced Charniak parser by using the new model confidence score output from the enhanced Charniak parser. Table 9 and Table 10 show that the Charniak parser enhanced by re-ranking and self-training is able to help to further improve the performance of our method. This is because that the enhanced Charniak parser provides more accurate model confidence score. 1558 parser accuracy P <=40 R words F P A"
D09-1161,A00-2018,0,\N,Missing
D09-1161,W05-1506,0,\N,Missing
D09-1161,J03-4003,0,\N,Missing
D09-1161,P05-1022,0,\N,Missing
D09-1161,W05-1518,0,\N,Missing
D09-1161,P05-1010,0,\N,Missing
D09-1161,D08-1092,0,\N,Missing
D10-1043,N04-1035,0,0.750571,"es to convert the source structures into target structures iteratively and recursively while from decoding viewpoint a syntax-based system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations. Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination. For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 & 2008; Liu et al., 2009). However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule. Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models. This may largely compromise the modeling ability o"
D10-1043,P08-1067,0,0.225468,"ide (string to tree model). There is no well study in considering both the source side information and the compatibility between different target syntactic structures during combination. In addition, it is well known that the traditional tree-to-tree models suffer heavily from the data sparseness issue in training and the spurious-ambiguity translation path issue (the same translation with different syntactic structures) in decoding. In addition, because of the performance limitation of automatic syntactic parser, researchers propose using packed forest (Tomita, 1987; Klein and Manning, 2001; Huang, 2008)1 instead of 1-best parse tree to carry out training (Mi and Huang, 2008) and decoding (Mi et al., 2008) in order to reduce the side effect caused by parsing errors of the one-best tree. However, when we apply the tree-to-tree model to the bilingual forest structures, both training and decoding become very complicated. In this paper, to address the first issue, we propose a framework to model the non-isomorphic translation process from source tree fragment to target tree sequence, allowing any one source frontier non-terminal node to be translated into any number of target frontier non-termina"
D10-1043,W01-1812,0,0.0401448,"onal Linguistics target side (string to tree model). There is no well study in considering both the source side information and the compatibility between different target syntactic structures during combination. In addition, it is well known that the traditional tree-to-tree models suffer heavily from the data sparseness issue in training and the spurious-ambiguity translation path issue (the same translation with different syntactic structures) in decoding. In addition, because of the performance limitation of automatic syntactic parser, researchers propose using packed forest (Tomita, 1987; Klein and Manning, 2001; Huang, 2008)1 instead of 1-best parse tree to carry out training (Mi and Huang, 2008) and decoding (Mi et al., 2008) in order to reduce the side effect caused by parsing errors of the one-best tree. However, when we apply the tree-to-tree model to the bilingual forest structures, both training and decoding become very complicated. In this paper, to address the first issue, we propose a framework to model the non-isomorphic translation process from source tree fragment to target tree sequence, allowing any one source frontier non-terminal node to be translated into any number of target fronti"
D10-1043,2003.mtsummit-papers.6,0,0.12256,"Missing"
D10-1043,D09-1076,0,0.125793,"tively and recursively while from decoding viewpoint a syntax-based system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations. Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination. For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 & 2008; Liu et al., 2009). However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule. Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models. This may largely compromise the modeling ability of translation rules. For the second problem, currently, the combinatio"
D10-1043,P03-2041,0,0.29126,"a syntax-based system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations. Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination. For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 & 2008; Liu et al., 2009). However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule. Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models. This may largely compromise the modeling ability of translation rules. For the second problem, currently, the combination is driven by only the source side (bot"
D10-1043,P07-1089,0,0.0907619,"ganized as following. Section 2 reviews the related work. In section 3 and section 4, we discuss the proposed forest-based rule extraction (non-isomorphic mapping) and decoding algorithms (target syntax information usage). Finally we report the experimental results in section 5 and conclude the paper in section 6. 2 Related Work Much effort has been done in the syntax-based translation modeling. Yamada and Knight (2001) propose a string to tree model. Galley et al. (2004) propose the GHKM scheme to model the string-to-tree mapping. Liu et al. (2006) propose a tree-to-string translation model. Liu et al. (2007) propose the tree sequence to string model to capture rules covered by continuous sequence of trees. Shieber (2007), DeNeefe and Knight (2009) and Carreras and Collins (2009) propose synchronous tree adjoin grammar to capture more tree-string mapping beyond the GHKM scheme. Zhang et al. (2009a) propose the concept of virtual node to reform a tree sequence as a tree, and design efficient algorithms for tree sequence model in forest context. All these works only consider either the source side or the target side syntax information. To capture both side syntax contexts, Eisner (2003) studies the"
D10-1043,P09-1063,0,0.509039,"ree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations. Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination. For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 & 2008; Liu et al., 2009). However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule. Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models. This may largely compromise the modeling ability of translation rules. For the second problem, currently, the combination is driven by only the source side (both tree-to-string model and tree-to-tree model"
D10-1043,D08-1022,0,0.35328,"both the source side information and the compatibility between different target syntactic structures during combination. In addition, it is well known that the traditional tree-to-tree models suffer heavily from the data sparseness issue in training and the spurious-ambiguity translation path issue (the same translation with different syntactic structures) in decoding. In addition, because of the performance limitation of automatic syntactic parser, researchers propose using packed forest (Tomita, 1987; Klein and Manning, 2001; Huang, 2008)1 instead of 1-best parse tree to carry out training (Mi and Huang, 2008) and decoding (Mi et al., 2008) in order to reduce the side effect caused by parsing errors of the one-best tree. However, when we apply the tree-to-tree model to the bilingual forest structures, both training and decoding become very complicated. In this paper, to address the first issue, we propose a framework to model the non-isomorphic translation process from source tree fragment to target tree sequence, allowing any one source frontier non-terminal node to be translated into any number of target frontier non-terminal nodes. For the second issue, we propose a technology to model the combi"
D10-1043,P03-1021,0,0.0713388,"of our method is consistent across data set of different size. We use the NIST 2002 test set as our dev set, and NIST 2003 and NIST 2005 test sets as our test set. A 3-gram language model is trained on the target side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We train Charniak’s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate m-to-n word alignments. For the MER training (Och, 2003), Koehn’s MER trainer (Koehn, 2007) is modified for our system. For significance test, we use Zhang et al.’s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bidirectional lexical translation probability, 4) target language model, 5) source tree probability 6) the av447 erage number of unmatched nodes in the target forest. 7) the length of the target translation, 8) the number of glue ru"
D10-1043,J03-1002,0,0.00671401,"on a set of parallel data with 30K sentence pairs, and then do experiment on a larger data set to ensure that the effectiveness of our method is consistent across data set of different size. We use the NIST 2002 test set as our dev set, and NIST 2003 and NIST 2005 test sets as our test set. A 3-gram language model is trained on the target side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We train Charniak’s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate m-to-n word alignments. For the MER training (Och, 2003), Koehn’s MER trainer (Koehn, 2007) is modified for our system. For significance test, we use Zhang et al.’s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bidirectional lexical translation probability, 4) target language model, 5) source tree probability 6) the av447"
D10-1043,P02-1040,0,0.0862314,"t side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We train Charniak’s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate m-to-n word alignments. For the MER training (Och, 2003), Koehn’s MER trainer (Koehn, 2007) is modified for our system. For significance test, we use Zhang et al.’s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bidirectional lexical translation probability, 4) target language model, 5) source tree probability 6) the av447 erage number of unmatched nodes in the target forest. 7) the length of the target translation, 8) the number of glue rules used. 5.2 Empirical Study on Small Data We set forest pruning threshold (Mi et al., 2008) to 8 on both source and target forests for rule extraction. For each source sub-tree, we set its height up to 3, width up to 7 and"
D10-1043,D09-1021,0,0.0309264,"Missing"
D10-1043,P01-1067,0,0.192485,"o target structures iteratively and recursively while from decoding viewpoint a syntax-based system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations. Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination. For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 & 2008; Liu et al., 2009). However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule. Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models. This may largely compromise the modeling ability of translation rules. For the second problem"
D10-1043,D09-1108,1,0.366976,"the paper in section 6. 2 Related Work Much effort has been done in the syntax-based translation modeling. Yamada and Knight (2001) propose a string to tree model. Galley et al. (2004) propose the GHKM scheme to model the string-to-tree mapping. Liu et al. (2006) propose a tree-to-string translation model. Liu et al. (2007) propose the tree sequence to string model to capture rules covered by continuous sequence of trees. Shieber (2007), DeNeefe and Knight (2009) and Carreras and Collins (2009) propose synchronous tree adjoin grammar to capture more tree-string mapping beyond the GHKM scheme. Zhang et al. (2009a) propose the concept of virtual node to reform a tree sequence as a tree, and design efficient algorithms for tree sequence model in forest context. All these works only consider either the source side or the target side syntax information. To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. Zhang et al. (2008) propose tree sequence-based tree-to-tree modeling. Liu et al. (2009) propose efficient algorithms for tree-to-tree model in the forest-based training and decoding scheme. One common limitation of the above work"
D10-1043,2007.mtsummit-papers.71,1,0.74833,"system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations. Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination. For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 & 2008; Liu et al., 2009). However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule. Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models. This may largely compromise the modeling ability of translation rules. For the second problem, currently, the combination is driven by only the source side (both tree-to-string mod"
D10-1043,zhang-etal-2004-interpreting,0,0.0257989,"05 test sets as our test set. A 3-gram language model is trained on the target side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We train Charniak’s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate m-to-n word alignments. For the MER training (Och, 2003), Koehn’s MER trainer (Koehn, 2007) is modified for our system. For significance test, we use Zhang et al.’s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bidirectional lexical translation probability, 4) target language model, 5) source tree probability 6) the av447 erage number of unmatched nodes in the target forest. 7) the length of the target translation, 8) the number of glue rules used. 5.2 Empirical Study on Small Data We set forest pruning threshold (Mi et al., 2008) to 8 on both source and target forests for rule e"
D10-1043,A00-2018,0,\N,Missing
D10-1043,W05-1506,0,\N,Missing
D10-1043,P09-1020,1,\N,Missing
D10-1043,J87-1004,0,\N,Missing
D10-1043,P08-1023,0,\N,Missing
D10-1043,P06-1077,0,\N,Missing
D10-1043,P08-1064,1,\N,Missing
D10-1043,P07-2045,0,\N,Missing
D10-1043,P07-1019,0,\N,Missing
D10-1043,J07-2003,0,\N,Missing
D11-1007,D08-1092,0,0.318764,"design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual tre"
D11-1007,D07-1101,0,0.144195,"sponding to “技巧(jiqiao)/skill” is a grandchild of the word “play” corresponding to “发挥(fahui)/demonstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x an"
D11-1007,D09-1060,1,0.927841,"list of the target monolingual subtrees 1 For the second order features, Dir is the combination of the directions of two dependencies. or bilingual subtrees, this constraint will probably be reliable. We first parse the large-scale unannotated monolingual and bilingual data. Subsequently, we extract the monolingual and bilingual subtrees from the parsed data. We then verify the bilingual constraints using the extracted subtrees. Finally, we generate the bilingual features based on the verified results for the parsing models. 5.1 Verified constraint functions 5.1.1 Monolingual target subtrees Chen et al. (2009) proposed a simple method to extract subtrees from large-scale monolingual data and used them as features to improve monolingual parsing. Following their method, we parse large unannotated data with the Parsert and obtain the subtree list (STt ) on the target side. We extract two types of subtrees: bigram (two words) subtree and trigram (three words) subtree. 5.1.2 Verified target constraint function: Fvt (rtk ) We use the extracted target subtrees to verify the rtk of the bilingual constraints. In fact, rtk is a candidate subtree. If the rtk is included in STt , function Fvt (rtk ) = T ype(rt"
D11-1007,P10-1003,1,0.243719,"based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree structures on both si"
D11-1007,P07-1003,0,0.126522,"ale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a future study. Suppose that"
D11-1007,D09-1127,0,0.0657088,"Missing"
D11-1007,N03-1017,0,0.0160094,"Missing"
D11-1007,P10-1001,0,0.0121108,"ley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a future study. Suppose that we have a (candidate) dependency relation rs that can be a bigram or trigram dependency. We examine whether the corresponding words of the source words of rs have a dependency relation rt in the target trees. We also consider the direction of the dependency relation. The corresponding word of the head should also be the head in rt . We define a binary function for this bilingual constraint: Fbn (rsn : rtk ), where n and k refers to the types of the dependencies (2 for bigram and 3 for trigram). For example, in rs2 : rt3 , rs2 is a bigram dependency on the sour"
D11-1007,P09-1058,1,0.820133,"//www.itl.nist.gov/iad/mig//tests/mt/2008/ 4 data. To extract English subtrees, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ texts. We used the MXPOST tagger (Ratnaparkhi, 1996) trained on training data to assign POS tags and used the first-order Parsert to process the sentences of the BLLIP corpus. To extract bilingual subtrees, we used the FBIS corpus and an additional bilingual corpus containing 800,000 sentence pairs from the training data of NIST MT08 evaluation campaign. On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al., 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. On the English side, we used the same procedure as we did for the BLLIP corpus. Word alignment was performed using the Berkeley Aligner. We reported the parser quality by the UAS, i.e., the percentage of tokens (excluding all punctuation tokens) with correct HEADs. 6.1 Experimental settings For baseline systems, we used the monolingual features mentioned in Section 3. We called these features basic features. To compare the results of (Burk"
D11-1007,N06-1014,0,0.079112,"ed by using large-scale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a fu"
D11-1007,P10-5002,0,0.0237031,"able is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree structures on both sides. Huang et al. (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. It uses another type of bilingual treebanks that have tree structures on the source sentences and their human-translated sentences. Chen"
D11-1007,J93-2004,0,0.0430468,"ingual treebanks that have tree structures on the source sentences and their human-translated sentences. Chen et al. (2010) also used bilingual treebanks and made use of tree structures on the target side. However, the bilingual treebanks are hard to obtain, partly because of the high cost of human translation. Thus, in their experiments, they applied their methods to a small data set, the manually translated portion of the Chinese Treebank (CTB) which contains only about 3,000 sentences. On the other hand, many large-scale monolingual treebanks exist, such as the Penn English Treebank (PTB) (Marcus et al., 1993) (about 40,000 sentences in Version 3) and the latest version of CTB (over 50,000 sentences in Version 7). In this paper, we propose a bitext parsing approach in which we produce the bilingual constraints on existing monolingual treebanks with the help of SMT systems. In other words, we aim to improve source-language parsing with the help of automatic translations. In our approach, we first use an SMT system to translate the sentences of a source monolingual treebank into the target language. Then, the target sentences are parsed by a parser trained on a target monolingual treebank. We then ob"
D11-1007,E06-1011,0,0.17547,"ure, the word “skills” corresponding to “技巧(jiqiao)/skill” is a grandchild of the word “play” corresponding to “发挥(fahui)/demonstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the in"
D11-1007,W03-3017,0,0.0373708,"monstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x and the feature weight vector w are the parameters to be learned by using MIRA (Crammer and Si"
D11-1007,J03-1002,0,0.00306085,"ased on the bilingual constraints verified by using large-scale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order"
D11-1007,W96-0213,0,0.0608525,"e trained first-order and second-order Parsert on the training data. The unlabeled attachment score (UAS) of the second-order Parsert was 91.92, indicating state-of-the-art accuracy on the test data. We used the second-order Parsert to parse the autotranslated/human-made target sentences in the CTB 3 http://www.statmt.org/moses/ http://www.speech.sri.com/projects/srilm/download.html 5 http://www.itl.nist.gov/iad/mig//tests/mt/2008/ 4 data. To extract English subtrees, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ texts. We used the MXPOST tagger (Ratnaparkhi, 1996) trained on training data to assign POS tags and used the first-order Parsert to process the sentences of the BLLIP corpus. To extract bilingual subtrees, we used the FBIS corpus and an additional bilingual corpus containing 800,000 sentence pairs from the training data of NIST MT08 evaluation campaign. On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al., 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. On the English side, we used the same proced"
D11-1007,W04-3207,0,0.0134497,"ify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their"
D11-1007,W03-3023,0,0.0472776,"his is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x and the feature weight vector w are the parameters to be learned by using MIRA (Crammer and Singer, 2003) during training."
D11-1007,P09-1007,0,0.0415887,"for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree s"
D11-1084,2002.tmi-tmiw.2,0,0.00920517,"ation in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.81 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation. 1 Introduction During last decade, tremendous work has been done to improve the quality of statistical machine __________________ * Corresponding author. translation (SMT) systems. However, there is still a huge performance gap between the state-of-theart SMT systems and human translators. Bond (2002) suggested nine ways to improve machine translation by imitating the best practices of human translators (Nida, 1964), with parsing the entire document before translation as the first priority. However, most SMT systems still treat parallel corpora as a list of independent sentence-pairs and ignore document-level information. Document-level information can and should be used to help document-level machine translation. At least, the topic of a document can help choose specific translation candidates, since when taken out of the context from their document, some words, phrases and even sentences"
D11-1084,2005.eamt-1.19,0,0.0713323,"Missing"
D11-1084,N03-1017,0,0.118031,"Missing"
D11-1084,W04-3250,0,0.561859,"Missing"
D11-1084,D07-1036,0,0.0639858,"Missing"
D11-1084,W02-1018,0,0.0730207,"Missing"
D11-1084,W04-3225,0,0.0279222,"e is analogous to “cache memory” in hardware terminology, which tracks short-term fluctuation (Iyer et al., 1999). As the cache changes with different documents, the documentlevel information should be capable of influencing SMT. Previous cache-based approaches mainly point to cache-based language modeling (Kuhn and Mori, 1990), which uses a large global language model to mix with a small local model estimated from recent history data. However, applying such a language model in SMT is very difficult due to the risk of introducing extra noise (Raab, 2007). For cache-based translation modeling, Nepveu et al. (2004) explored user-edited translations in the context of interactive machine translation. Tiedemann (2010) proposed to fill the cache with bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document. Both Nepveu et al. (2004) and Tiedemann (2010) also explored traditional cache-based language models and found that a cache-based language model often contributes much more than a cache-based translation model. 3 Cache-based document-level SMT Given a test document, our system works as follows: 1) clears the static, topic and dynamic caches when switching to"
D11-1084,P03-1021,0,0.102446,"translation candidates, since when taken out of the context from their document, some words, phrases and even sentences may be rather ambiguous and thus difficult to understand. Another advantage of document-level machine translation is its ability in keeping a consistent translation. However, document-level translation has drawn little attention from the SMT research community. The reasons are manifold. First of all, most of parallel corpora lack the annotation of document boundaries (Tam, 2007). Secondly, although it is easy to incorporate a new feature into the classical log-linear model (Och, 2003), it is difficult to capture document-level information and model it via some simple features. Thirdly, reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts. This makes the evaluation of document-level SMT systems extremely difficult. Tiedemann (2010) showed that the repetition and consistency are very important when modeling natural language and translation. He proposed to employ cache-based language and translation models in a phrase-based SMT system for domain 909 Proceedings of the 2011 Confere"
D11-1084,W09-2404,0,0.354359,"employing an adaptive language model with the advantage of avoiding the interpolation of a global language model with a specific domain language model. The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 presents our cache-based approach to documentlevel SMT. Section 4 presents the experimental results. Session 5 gives new insights on cachebased document-level translation. Finally, we conclude this paper in Section 6. 2 Related work There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009). Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT. Tam et al. (2007) proposed a bilingual-LSA model on the basis of a parallel document corpus and built a topic-based language model for each language. By automatically building the correspondence between the source and target language models, this me"
D11-1084,P00-1056,0,0.134938,"Missing"
D11-1084,D08-1033,0,0.0156691,"Missing"
D11-1084,P02-1040,0,0.0870566,"ords extracted from target-side documents 4 Experimentation We have systematically evaluated our cache-based approach to document-level SMT on the ChineseEnglish translation task. 4.1 Experimental Setting Here, we use SRI language modeling toolkit to train a trigram general language model on English newswire text, mostly from the Xinhua portion of the Gigaword corpus (2007) and performed word alignment on the training parallel corpus using GIZA++(Och and Ney,2000) in two directions. For evaluation, the NIST BLEU script (version 13) with the default setting is used to calculate the Bleu score (Papineni et al. 2002), which measures case-insensitive matching of n-grams with n up to 4. To see whether an improvement is statistically significant, we also conduct significance tests using the paired bootstrap approach (Koehn, 2004)2. In this paper, ‘***’, ‘**’, and ‘*’ denote p-values less than or equal to 0.01, in-between (0.01, 0.05), and bigger than 0.05, which mean significantly better, moderately better and slightly better, respectively. 2 http://www.ark.cs.cmu.edu/MT 914 In this paper, we use FBIS as the training data, the 2003 NIST MT evaluation test data as the development data, and the 2005 NIST MT te"
D11-1084,H92-1045,0,0.580187,"topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT. Tam et al. (2007) proposed a bilingual-LSA model on the basis of a parallel document corpus and built a topic-based language model for each language. By automatically building the correspondence between the source and target language models, this method can match the topic-based language model and improve the performance of SMT. Carpuat (2009) revisited the “one sense per discourse” hypothesis of Gale et al. (1992) and gave a detailed comparison and analysis of the “one translation per discourse” hypothesis. However, she failed to propose an effective way to integrate document-level information into a SMT system. For example, she simply recommended some translation candidates to replace some target words in the post-process stage. In principle, the cache-based approach can be well suited for document-level translation. Basically, the cache is analogous to “cache memory” in hardware terminology, which tracks short-term fluctuation (Iyer et al., 1999). As the cache changes with different documents, the do"
D11-1084,W10-2602,0,0.776373,"tention from the SMT research community. The reasons are manifold. First of all, most of parallel corpora lack the annotation of document boundaries (Tam, 2007). Secondly, although it is easy to incorporate a new feature into the classical log-linear model (Och, 2003), it is difficult to capture document-level information and model it via some simple features. Thirdly, reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts. This makes the evaluation of document-level SMT systems extremely difficult. Tiedemann (2010) showed that the repetition and consistency are very important when modeling natural language and translation. He proposed to employ cache-based language and translation models in a phrase-based SMT system for domain 909 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 909–919, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics adaptation. Especially, the cache in the translation model dynamically grows up by adding bilingual phrase pairs from the best translation hypotheses of previous sentences. One problem"
D11-1084,P10-1049,0,0.0175437,"Missing"
D11-1084,C04-1059,0,0.0672583,"Missing"
D11-1084,J93-2003,0,\N,Missing
D11-1084,P06-2124,0,\N,Missing
D11-1109,D07-1101,0,0.442565,"d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for third-order models. In their paper, they implement two versions of third-order models, Model 1 and Model 2 according to their naming. Model 1 incorporates only grand-sibling parts, while Model 2 incorporates both grand-sibling and tri-sibling parts. Their experiments on English and Czech show that Model 1 and Model 2 obtain nearly the same parsing ac"
D11-1109,P05-1022,0,0.355797,"Missing"
D11-1109,C10-1019,1,0.878284,"Missing"
D11-1109,D07-1022,0,0.0164053,"Missing"
D11-1109,W02-1001,0,0.285248,"et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score of a tag sequence is Scorepos (x, t) = wpos · fpos (x, t) 2.2 Dependency Parsing Recently"
D11-1109,N09-1046,0,0.0525872,"Missing"
D11-1109,C96-1058,0,0.797918,"me with Model 1 in Koo and Collins (2010), but without using grand-sibling features.2 • The third-order model (O3): the same with Model 1 in Koo and Collins (2010). We adopt linear models to define the score of a dependency tree. For the third-order model, the score of a dependency tree is represented as: ∑ wdep · fdep (x, t, h, m) Scoresyn (x, t, d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for"
D11-1109,N09-1037,0,0.0607158,"Missing"
D11-1109,P08-1043,0,0.0977694,"Missing"
D11-1109,P10-1110,0,0.400604,"tructures. On the contrary, joint models of version 2 can incorporate both aforementioned feature sets, but have higher complexity. These two versions of models will be thoroughly compared in the experiments. 1185 We then define the allowable candidate POS tags of the word wi to be Ti (x) = {t : t ∈ T , P (ti = t|x) ≥ λt × pmaxi (x)} where λt is the pruning threshold. Ti (x) is used to constrain the POS search space by replacing T in Algorithm 1. 5 Experiments We use the Penn Chinese Treebank 5.1 (CTB5) (Xue et al., 2005). Following the setup of Duan et al. (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 11371147) sets. We use the head-finding rules of Zhang and Clark (2008b) to turn the bracketed sentences into dependency structures. We use the standard tagging accuracy to evaluate POS tagging. For dependency parsing, we use word accuracy (also known as dependency accuracy), root accuracy and complete match rate (all excluding punctuation) . For the averaged training, we train each model for 15 iterations and select the parameters that perform best on the development"
D11-1109,P08-1102,0,0.111307,"Missing"
D11-1109,P10-1001,0,0.270992,"2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score of a tag sequence is Scorepos (x, t) = wpos · fpos (x, t) 2.2 Dependency Parsing Recently, graph-based dependency parsing has gained more and more interest due to its state-ofthe-art accuracy. Graph-based dependency parsing views the problem as finding the highest scoring tree from a directed graph. Based on dynamic programming decoding, it can efficiently find an optimal tree in a huge search space. In a graph-based model, the score"
D11-1109,P09-1058,0,0.227854,"Missing"
D11-1109,P03-1056,0,0.0427835,"where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is an artificial root token which is used to simplify the formalization of the problem. The pipelined method treats POS tagging and dependency parsing as two cascaded problems. First, 1 It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). In addition, Rush et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 20"
D11-1109,P10-1113,0,0.0383995,"Missing"
D11-1109,C10-1080,0,0.0394632,"Missing"
D11-1109,E06-1011,0,0.689881,"the third-order model, the score of a dependency tree is represented as: ∑ wdep · fdep (x, t, h, m) Scoresyn (x, t, d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for third-order models. In their paper, they implement two versions of third-order models, Model 1 and Model 2 according to their naming. Model 1 incorporates only grand-sibling parts, while Model 2 incorporates both grand-sibling and tri-sibling parts"
D11-1109,P05-1012,0,0.930686,"der model (O3): the same with Model 1 in Koo and Collins (2010). We adopt linear models to define the score of a dependency tree. For the third-order model, the score of a dependency tree is represented as: ∑ wdep · fdep (x, t, h, m) Scoresyn (x, t, d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for third-order models. In their paper, they implement two versions of third-order models, Model 1 and Model 2 ac"
D11-1109,J08-4003,0,0.155196,"Missing"
D11-1109,N07-1051,0,0.116748,"Missing"
D11-1109,W96-0213,0,0.524192,"d as non-terminals in the constituent structure (Levy and Manning, 2003). In addition, Rush et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score o"
D11-1109,D10-1001,0,0.0739428,"POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is an artificial root token which is used to simplify the formalization of the problem. The pipelined method treats POS tagging and dependency parsing as two cascaded problems. First, 1 It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). In addition, Rush et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002"
D11-1109,P07-1096,0,0.0207597,"s determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score of a tag sequence is Scorepos (x, t) = wpos · fpos (x, t) 2.2 Dependency Parsing Recently, graph-based dependency parsing has gained more and more interest due to its state-ofthe-art accuracy. Graph-based dependency parsing views the problem as finding the highest scoring tree from a directed graph. Based on dynamic programming de"
D11-1109,W08-2121,0,0.0983419,"Missing"
D11-1109,P09-1055,0,0.0150803,"Missing"
D11-1109,C10-1135,0,0.0414692,"Missing"
D11-1109,P08-1101,0,0.518379,"of this paper is organized as follows. Section 2 describes the pipelined method, including the POS tagging and parsing models. Section 3 discusses the joint models and the decoding algorithms, while Section 4 presents the pruning techniques. Section 5 reports the experimental results and error analysis. We review previous work closely related to our method in Section 6, and conclude this paper in Section 7. an optimal POS tag sequence ˆt is determined. 2 where fpos (x, t) refers to the feature vector and wpos is the corresponding weight vector. For POS tagging features, we follow the work of Zhang and Clark (2008a). Three feature sets are considered: POS unigram, bigram and trigram features. For brevity, we will refer to the three sets as wi ti , ti−1 ti and ti−2 ti−1 ti . Given wpos , we adopt the Viterbi algorithm to get the optimal tagging sequence. The Baseline Pipelined Method Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is"
D11-1109,D08-1059,0,0.828377,"of this paper is organized as follows. Section 2 describes the pipelined method, including the POS tagging and parsing models. Section 3 discusses the joint models and the decoding algorithms, while Section 4 presents the pruning techniques. Section 5 reports the experimental results and error analysis. We review previous work closely related to our method in Section 6, and conclude this paper in Section 7. an optimal POS tag sequence ˆt is determined. 2 where fpos (x, t) refers to the feature vector and wpos is the corresponding weight vector. For POS tagging features, we follow the work of Zhang and Clark (2008a). Three feature sets are considered: POS unigram, bigram and trigram features. For brevity, we will refer to the three sets as wi ti , ti−1 ti and ti−2 ti−1 ti . Given wpos , we adopt the Viterbi algorithm to get the optimal tagging sequence. The Baseline Pipelined Method Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is"
D11-1109,W09-1201,0,\N,Missing
D12-1026,vilar-etal-2006-error,0,0.0507574,"in a sentence and thus supervise SMT to reduce tense inconsistency errors against Observations (1) and (2) in the sentence-level. In comparison, Observation (3) actually reflects the tense distributions among one document. After extracting each main tense for each sentence, we build another tense ngram model in the document-level. For clarity, this paper denotes document-level tense as “inter-tense” and sentence-level tense as “intra-tense”. After that, we propose to integrate such tense models into SMT systems in a dynamic way. It is well known there are many errors in the current MT output (David et al., 2006). Unlike previously making trouble with reference texts, the BLEU-4 score cannot be influenced obviously by modifying a small part of abnormal sentences in a static way. In our system, both inter-tense and intra-tense model are integrated into a SMT system via additional features and thus can supervise the decoding procedure. During decoding, once some words with correct tense can be determined, with the help of language model and other related features, the small component–“tense”–can affect surrounding words and improve the performance of the whole sentence. Our experimental results (see the"
D12-1026,P92-1033,0,0.823893,"Missing"
D12-1026,D11-1084,1,0.858213,"ure. During decoding, once some words with correct tense can be determined, with the help of language model and other related features, the small component–“tense”–can affect surrounding words and improve the performance of the whole sentence. Our experimental results (see the examples in Sec277 tion 6.4) show the effectiveness of this way. Rather than the rule-based model, our models are fully statistical-based. So they can be easily scaled up and integrated into either phrase-based or syntaxbased SMT systems. In this paper, we employ a strong phrase-based SMT baseline system, as proposed in Gong et al. (2011), which uses document as translation unit, for better incorporating documentlevel information. The rest of this paper is organized as follows: Section 2 reviews the related work. Section 3 and 4 are related to tense models. Section 3 describes the preprocessing work for building tense models. Section 4 presents how to build target-side tense models and discuss their characteristics. Section 5 shows our way of integrating such tense models into a SMT system. Session 6 gives the experimental results. Finally, we conclude this paper in Section 7. 2 Related Work In this section, we focus on relate"
D12-1026,N03-1017,0,0.125285,"Missing"
D12-1026,W04-3250,0,0.0420808,"tal Setting for SMT In our experiment, SRI language modeling toolkit was used to train a 5-Gram general language model on the Xinhua portion of the Gigaword corpus. Word alignment was performed on the training parallel corpus using GIZA++ ( Och and Ney, 2000) in two directions. For evaluation, the NIST BLEU script (version 13) with the default setting is used to calculate the BLEU score (Papineni et al., 2002), which measures case-insensitive matching of 4-grams. To see whether an improvement is statistically significant, we also conduct significance tests using the paired bootstrap approach (Koehn, 2004). In this paper, “***” and “**” denote p-values equal to 0.05, and bigger than 0.05, which mean significantly better, moderately better respectively. Role Train Dev Test Corpus Name FBIS NIST2003 NIST2005 Sentences Documents 228455 919 1082 10000 100 100 6.3 Experimental Results All the experiment results are showed on the table 3. Our Baseline is a modified Moses. The major modification is input and output module in order to translate using document as unit. The performance of our baseline exceeds the baseline reported by Gong et al. (2011) about 2 percent based on the similar training and te"
D12-1026,I11-1125,0,0.141759,"nd subordinate clauses connected with some special temporal marker words, such as “after” and “before”, and employed them in temporal inference. Another typical task is cross-lingual tense predication. Some languages, such as English, are inflectional, whose verbs can express tense via certain stems or suffix, while others, such as Chinese often lack inflectional forms. Take Chinese to English translation as example, if Chinese text contains particle word “ (Le)”, the nearest Chinese verb prefers to be translated into English verb with the past tense. Ye and Zhang (2005), Ye et al. (2007) and Liu et al. (2011) focus on labeling the tenses for keywords in source-side language. 3 Ye and Zhang (2005) first built a small amount of manually-labeled data, which provide the tense mapping from Chinese text to English text. Then, they trained a CRF-based tense classifier to label tense on Chinese documents. Ye et al. (2007) further reported that syntactic features contribute most to the marking of aspectual information. Liu et al. (2011) proposed a parallel mapping method to automatically generate annotated data. In particular, they used English verbs to label tense information for Chinese verbs via a paral"
D12-1026,P00-1056,0,0.0454932,"s into SMT In this section, we discuss how to integrate the previous tense models into a SMT system. 5.1 Basic phrase-based SMT system It is well known that the translation process of SMT can be modeled as obtaining the best translation e of the source sentence f by maximizing following posterior probability(Brown et al., 1993): ebest = arg max P (e|f ) e = arg max P (f |e)Plm (e) (2) e where P (e|f ) is a translation model and Plm is a language model. Our baseline is a modified Moses, which follows Koehn et al. (2003) and adopts similar six groups of features. Besides, the log-linear model ( Och and Ney, 2000) is employed to linearly interpolate these features for obtaining the best translation according to the formula 3: ebest = arg max e M X λm hm (e, f ) (3) m=1 where hm (e, f ) is a feature function, and λm is the weight of hm (e, f ) optimized by a discriminative training method on a held-out development data(Och, 2003). 5.2 first obtains tense sequence for such hypothesis and computes intra-tense feature Fs (see Section 5.3). At the same time, it recognizes the main tense of this hypothesis and associate the main tense of previous sentence to compute inter-tense feature Fm (see Section 5.3)."
D12-1026,2002.tmi-tutorials.2,0,0.0350901,"nse sequence is about 2.5, we mainly consider intra-tense bigram model and thus n equals to 2. 5 5.4 Determining Tense For SMT Output The current SMT systems often produce odd translations partly because of abnormal word ordering and uncompleted text etc. For these abnormal translated texts, the syntactic parser cannot work well in our initial experiments, so the previous method to parse main tense and tense sequence of regular texts cannot be applied here too. Fortunately, the solely utilization of Stanford POS tagger for our SMT output is not bad although it has the same issues described in Och et al. (2002). The reason may be that phrase-based SMT contains short contexts that POS tagger can utilize while the syntax parser fails. Once obtaining a completed hypothesis, the decoder will pass it to the Stanford POS tagger and according to tense verbs to get all tense sequence for this hypothesis. However, since the POS tagger can not return the information about level structures, the decoder cannot recognize the main tense from such tense sequence. Liu et al. (2011) once used target-side verbs to label tense of source-side verbs. It is natural to consider whether Chinese verbs can provide similar cl"
D12-1026,P03-1021,0,0.0376538,": ebest = arg max P (e|f ) e = arg max P (f |e)Plm (e) (2) e where P (e|f ) is a translation model and Plm is a language model. Our baseline is a modified Moses, which follows Koehn et al. (2003) and adopts similar six groups of features. Besides, the log-linear model ( Och and Ney, 2000) is employed to linearly interpolate these features for obtaining the best translation according to the formula 3: ebest = arg max e M X λm hm (e, f ) (3) m=1 where hm (e, f ) is a feature function, and λm is the weight of hm (e, f ) optimized by a discriminative training method on a held-out development data(Och, 2003). 5.2 first obtains tense sequence for such hypothesis and computes intra-tense feature Fs (see Section 5.3). At the same time, it recognizes the main tense of this hypothesis and associate the main tense of previous sentence to compute inter-tense feature Fm (see Section 5.3). Next, the decoder uses such two additional feature values to re-score this hypothesis automatically and choose one hypothesis with highest score as the final translation. After translating one sentence, the decoder caches its main tense and pass it to the next sentence. When one document has been processed, the decoder"
D12-1026,2001.mtsummit-papers.47,0,0.174117,"Missing"
D12-1026,P02-1040,0,0.0831374,"Missing"
D12-1026,I05-1077,0,0.0561652,"particular, they trained models on main and subordinate clauses connected with some special temporal marker words, such as “after” and “before”, and employed them in temporal inference. Another typical task is cross-lingual tense predication. Some languages, such as English, are inflectional, whose verbs can express tense via certain stems or suffix, while others, such as Chinese often lack inflectional forms. Take Chinese to English translation as example, if Chinese text contains particle word “ (Le)”, the nearest Chinese verb prefers to be translated into English verb with the past tense. Ye and Zhang (2005), Ye et al. (2007) and Liu et al. (2011) focus on labeling the tenses for keywords in source-side language. 3 Ye and Zhang (2005) first built a small amount of manually-labeled data, which provide the tense mapping from Chinese text to English text. Then, they trained a CRF-based tense classifier to label tense on Chinese documents. Ye et al. (2007) further reported that syntactic features contribute most to the marking of aspectual information. Liu et al. (2011) proposed a parallel mapping method to automatically generate annotated data. In particular, they used English verbs to label tense i"
D12-1026,2007.mtsummit-papers.69,0,0.839163,"ined models on main and subordinate clauses connected with some special temporal marker words, such as “after” and “before”, and employed them in temporal inference. Another typical task is cross-lingual tense predication. Some languages, such as English, are inflectional, whose verbs can express tense via certain stems or suffix, while others, such as Chinese often lack inflectional forms. Take Chinese to English translation as example, if Chinese text contains particle word “ (Le)”, the nearest Chinese verb prefers to be translated into English verb with the past tense. Ye and Zhang (2005), Ye et al. (2007) and Liu et al. (2011) focus on labeling the tenses for keywords in source-side language. 3 Ye and Zhang (2005) first built a small amount of manually-labeled data, which provide the tense mapping from Chinese text to English text. Then, they trained a CRF-based tense classifier to label tense on Chinese documents. Ye et al. (2007) further reported that syntactic features contribute most to the marking of aspectual information. Liu et al. (2011) proposed a parallel mapping method to automatically generate annotated data. In particular, they used English verbs to label tense information for Chi"
D12-1026,J93-2003,0,\N,Missing
D12-1026,N04-1021,0,\N,Missing
D12-1026,W06-0107,0,\N,Missing
D13-1129,P05-1001,0,0.0728574,"peech tags, and dependency trees, and learn the preference of features via adjusting feature weights. ∗ Corresponding author Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (McClosky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008; Chen et al., 2009). In this paper, we propose an alternative approach to semi-supervised dependency parsing via feature transformation (Ando and Zhang, 2005). More 1303 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics specifically, we transform base features to a higherlevel space. The base features defined over surface words, part-of-speech tags, and dependency trees are high dimensional and have been explored in the above previous studies. The higher-level features, which we call meta features, are low dimensional, and newly defined in this paper. The key idea behind is that we build connections b"
D13-1129,C10-1011,0,0.194357,"cy Parsing † Wenliang Chen† , Min Zhang†∗, and Yue Zhang‡ School of Computer Science and Technology, Soochow University, China ‡ Singapore University of Technology and Design, Singapore {wlchen, mzhang}@suda.edu.cn yue zhang@sutd.edu.sg Abstract In the supervised learning scenarios, many previous studies explore rich feature representation that leads to significant improvements. McDonald and Pereira (2006) and Carreras (2007) define secondorder features over two adjacent arcs in secondorder graph-based models. Koo and Collins (2010) use third-order features in a third-order graph-based model. Bohnet (2010) considers information of more surrounding words for the graph-based models, while Zhang and Nivre (2011) define a set of rich features including the word valency and the third-order context features for transition-based models. All these models utilize richer and more complex feature representations and achieve better performance than the earlier models that utilize the simpler features (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). However, the richer feature representations result in a high-dimensional feature space. Features in such a space may suffer from the"
D13-1129,W06-2920,0,0.054748,"elp of a large amount of automatically parsed data. The meta features are used together with base features in our final parser. Our studies indicate that our proposed approach is very effective in processing unseen data and features. Experiments on Chinese and English data sets show that the final parser achieves the best-reported accuracy on the Chinese data and comparable accuracy with the best known parsers on the English data. 1 Introduction In recent years, supervised learning models have achieved lots of progress in the dependency parsing task, as can be found in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). The supervised models take annotated data as training data, utilize features defined over surface words, part-of-speech tags, and dependency trees, and learn the preference of features via adjusting feature weights. ∗ Corresponding author Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (McClosky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008;"
D13-1129,D07-1101,0,0.0389168,"odel scores each subgraph using a linear representation. Then scoring function score(x, g) is, score(x, g) = f(x, g) · w (2) where f(x, g) is a high-dimensional feature vector based on features defined over g and x and w refers to the weights for the features. The maximum spanning tree is the highest scoring tree in Y (Gx ). The task of decoding algorithms in the parsing model for an input sentence x is to find y ∗ , where y ∗ = arg max score(x, y) y∈Y (Gx ) = arg max ∑ score(x, g) y∈Y (Gx ) g∈y = arg max ∑ f(x, g) · w (3) y∈Y (Gx ) g∈y In our system, we use the decoding algorithm proposed by Carreras (2007), which is a secondorder CKY-style algorithm (Eisner, 1996) and feature weights w are learned during training using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; McDonald et al., 2005). 2.2 Base features Previous studies have defined different sets of features for the graph-based parsing models, such as the first-order features defined in McDonald et al. (2005), the second-order parent-siblings features defined in McDonald and Pereira (2006), and the second-order parent-child-grandchild features defined in Carreras (2007). Bohnet (2010) explorers a richer set of featur"
D13-1129,D09-1060,1,0.941947,"Nivre et al., 2007). The supervised models take annotated data as training data, utilize features defined over surface words, part-of-speech tags, and dependency trees, and learn the preference of features via adjusting feature weights. ∗ Corresponding author Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (McClosky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008; Chen et al., 2009). In this paper, we propose an alternative approach to semi-supervised dependency parsing via feature transformation (Ando and Zhang, 2005). More 1303 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics specifically, we transform base features to a higherlevel space. The base features defined over surface words, part-of-speech tags, and dependency trees are high dimensional and have been explored in the above previous studies. The higher-level feat"
D13-1129,P12-1023,1,0.629615,"us systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo10 refers to the thirdorder parser with model1 of Koo and Collins (2010), Zhang11 refers to the parser of Zhang and Nivre (2011), Li12 refers to the unlabeled parser of Li et al. (2012), Koo08 refers to the parser of Koo et al. (2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09 refers to the parser of Chen et al. (2009), Zhou11 refers to the parser of Zhou et al. (2011), Suzuki11 refers to the parser of Suzuki et al. (2011), and Chen12 refers to the parser of Chen et al. (2012). The results showed that our meta parser outperformed most of the previous systems and obtained the comparable accuracy with the best result of Suzuki11 (Suzuki et al., 2011) which combined the clustering-based word representations of Koo et al. (2008) and a condensed feature representation. However, our approach is much simpler than theirs and we believe that our meta parser can be further improved by combining their methods. Sup Semi System McDonald06 Koo10 Zhang11 Li12 Our Baseline Koo08 Suzuki09 Chen09 Zhou11 Suzuki11 Chen12 MetaParser UAS 91.5 93.04 92.9 93.12 92.76 93.16 93.79 93.16 92."
D13-1129,C96-1058,0,0.0106219,"scoring function score(x, g) is, score(x, g) = f(x, g) · w (2) where f(x, g) is a high-dimensional feature vector based on features defined over g and x and w refers to the weights for the features. The maximum spanning tree is the highest scoring tree in Y (Gx ). The task of decoding algorithms in the parsing model for an input sentence x is to find y ∗ , where y ∗ = arg max score(x, y) y∈Y (Gx ) = arg max ∑ score(x, g) y∈Y (Gx ) g∈y = arg max ∑ f(x, g) · w (3) y∈Y (Gx ) g∈y In our system, we use the decoding algorithm proposed by Carreras (2007), which is a secondorder CKY-style algorithm (Eisner, 1996) and feature weights w are learned during training using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; McDonald et al., 2005). 2.2 Base features Previous studies have defined different sets of features for the graph-based parsing models, such as the first-order features defined in McDonald et al. (2005), the second-order parent-siblings features defined in McDonald and Pereira (2006), and the second-order parent-child-grandchild features defined in Carreras (2007). Bohnet (2010) explorers a richer set of features than the above sets. We further extend the features defi"
D13-1129,I11-1136,0,0.350016,"003) and the Chinese head rules of Zhang and Clark (2008). We followed the standard data splits as shown in Table 3. Following the work of Koo et al. (2008), we used a tagger trained on training data to provide part-of-speech (POS) tags for the development and test sets, and used 10-way jackknifing to generate part-of-speech tags for the training set. We used the MXPOST (Ratnaparkhi, 1996) tagger for English and the CRF-based tagger for Chinese. We used gold standard segmentation in the CTB5. The data partition of Chinese were chosen to match previous work (Duan et al., 2007; Li et al., 2011; Hatori et al., 2011). I ate the meat with a fork I!!!!ate!!!!the!!!!meat!!!!with!!!!a!!!!fork!!!!. Tk:!hw,!dw,!cw,!d(h,d,c) Fb:!ate,!meat,!with,!RIGHTSIB &quot; (fb)=Mk [Mk];![Mk],!VV;![Mk],!ate Figure 1: An example of generating meta features ing to the mapping function, we obtain the mapped value Mk . Finally, we have the three meta features “[Mk ]”, “[Mk ], V V ”, and “[Mk ], ate”, where V V is the part-of-speech tag of word “ate”. In this way, we can generate all the meta features for the graphbased model. PTB (sections) CTB5 (files) train 2-21 dev 22 test 23 001-815 1001-1136 886-931 1148-1151 816-885 1137-1147 T"
D13-1129,P10-1001,0,0.0356174,"Missing"
D13-1129,P08-1068,0,0.756769,"z and Marsi, 2006; Nivre et al., 2007). The supervised models take annotated data as training data, utilize features defined over surface words, part-of-speech tags, and dependency trees, and learn the preference of features via adjusting feature weights. ∗ Corresponding author Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (McClosky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008; Chen et al., 2009). In this paper, we propose an alternative approach to semi-supervised dependency parsing via feature transformation (Ando and Zhang, 2005). More 1303 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics specifically, we transform base features to a higherlevel space. The base features defined over surface words, part-of-speech tags, and dependency trees are high dimensional and have been explored in the above previous studies. T"
D13-1129,P09-1058,0,0.0289575,"(Xue et al., 2005) for Chinese. The tool “Penn2Malt”1 was used 1 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html 1307 For the unannotated data in English, we used the BLLIP WSJ corpus (Charniak et al., 2000) containing about 43 million words.2 We used the MXPOST tagger trained on the training data to assign part-ofspeech tags and used the Baseline parser to process the sentences of the Brown corpus. For the unannotated data in Chinese, we used the Xinhua portion of Chinese Gigaword3 Version 2.0 (LDC2009T14) (Huang, 2009), which has approximately 311 million words. We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse the sentences in the Gigaword data. In collecting the base features, we removed the features which occur only once in the English data and less than four times in the Chinese data. The feature occurrences of one time and four times are based on the development data performance. We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of to2 We ensured that the text used for building the meta features did not include the sentences of the Penn Treeb"
D13-1129,D11-1109,1,0.948858,"and Matsumoto (2003) and the Chinese head rules of Zhang and Clark (2008). We followed the standard data splits as shown in Table 3. Following the work of Koo et al. (2008), we used a tagger trained on training data to provide part-of-speech (POS) tags for the development and test sets, and used 10-way jackknifing to generate part-of-speech tags for the training set. We used the MXPOST (Ratnaparkhi, 1996) tagger for English and the CRF-based tagger for Chinese. We used gold standard segmentation in the CTB5. The data partition of Chinese were chosen to match previous work (Duan et al., 2007; Li et al., 2011; Hatori et al., 2011). I ate the meat with a fork I!!!!ate!!!!the!!!!meat!!!!with!!!!a!!!!fork!!!!. Tk:!hw,!dw,!cw,!d(h,d,c) Fb:!ate,!meat,!with,!RIGHTSIB &quot; (fb)=Mk [Mk];![Mk],!VV;![Mk],!ate Figure 1: An example of generating meta features ing to the mapping function, we obtain the mapped value Mk . Finally, we have the three meta features “[Mk ]”, “[Mk ], V V ”, and “[Mk ], ate”, where V V is the part-of-speech tag of word “ate”. In this way, we can generate all the meta features for the graphbased model. PTB (sections) CTB5 (files) train 2-21 dev 22 test 23 001-815 1001-1136 886-931 1148-11"
D13-1129,C12-1103,1,0.727694,"Figure 2 shows the average accuracy scores of the Baseline parsers against to the bins. From the figure, we found that for both two languages the Baseline parsers performed worse while the sentences contained more unknown features. 48.0 48.05 47.15 46.61 51.36 100 Table 10: Relevant results for English. Sup denotes the supervised parsers, Semi denotes the parsers with semisupervised methods. 4.5.2 Chinese Table 11 shows the comparative results, where Li11 refers to the parser of Li et al. (2011), Hatori11 refers to the parser of Hatori et al. (2011), and Li12 refers to the unlabeled parser of Li et al. (2012). The reported scores on this data were produced by the supervised learning methods and our Baseline (supervised) parser provided the comparable accuracy. We found that the score of our meta parser for this data was the best reported so far and significantly higher than the previous scores. Note that we used the auto-assigned POS tags in the test set to match the above previous studies. System Li11 Hatori11 Li12 Our Baseline MetaParser UAS 80.79 81.33 81.21 81.01 83.08 COMP 29.11 29.90 29.71 32.21 Table 11: Relevant results for Chinese 4.6 Analysis Here, we analyzed the effect of the meta feat"
D13-1129,J93-2004,0,0.0515184,"first step, we use a baseline parser to parse a large amount of unannotated sentences. Then we collect the base features from the parse trees. The collected features are transformed into predefined discrete values via a transformation function. Based on the transformed values, we define a set of meta features. Finally, the meta features are incorporated directly into parsing models. To demonstrate the effectiveness of the proposed approach, we apply it to the graph-based parsing models (McDonald and Nivre, 2007). We conduct experiments on the standard data split of the Penn English Treebank (Marcus et al., 1993) and the Chinese Treebank Version 5.1 (Xue et al., 2005). The results indicate that the approach significantly improves the accuracy. In summary, we make the following contributions: • We define a simple yet useful transformation function to transform base features to meta features automatically. The meta features build connections between known and unknown base features, and relieve the data sparseness problem. • Compared to the base features, the number of meta features is remarkably small. • We build semi-supervised dependency parsers that achieve the best accuracy on the Chinese data and c"
D13-1129,P06-1043,0,0.247812,"1 Introduction In recent years, supervised learning models have achieved lots of progress in the dependency parsing task, as can be found in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). The supervised models take annotated data as training data, utilize features defined over surface words, part-of-speech tags, and dependency trees, and learn the preference of features via adjusting feature weights. ∗ Corresponding author Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (McClosky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008; Chen et al., 2009). In this paper, we propose an alternative approach to semi-supervised dependency parsing via feature transformation (Ando and Zhang, 2005). More 1303 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics specifically, we transform base features to a higherlevel spac"
D13-1129,D07-1013,0,0.343727,"y problems. In our approach, the base features are grouped and each group relates to a meta feature. In the first step, we use a baseline parser to parse a large amount of unannotated sentences. Then we collect the base features from the parse trees. The collected features are transformed into predefined discrete values via a transformation function. Based on the transformed values, we define a set of meta features. Finally, the meta features are incorporated directly into parsing models. To demonstrate the effectiveness of the proposed approach, we apply it to the graph-based parsing models (McDonald and Nivre, 2007). We conduct experiments on the standard data split of the Penn English Treebank (Marcus et al., 1993) and the Chinese Treebank Version 5.1 (Xue et al., 2005). The results indicate that the approach significantly improves the accuracy. In summary, we make the following contributions: • We define a simple yet useful transformation function to transform base features to meta features automatically. The meta features build connections between known and unknown base features, and relieve the data sparseness problem. • Compared to the base features, the number of meta features is remarkably small."
D13-1129,E06-1011,0,0.00735014,"Gx ) = arg max ∑ score(x, g) y∈Y (Gx ) g∈y = arg max ∑ f(x, g) · w (3) y∈Y (Gx ) g∈y In our system, we use the decoding algorithm proposed by Carreras (2007), which is a secondorder CKY-style algorithm (Eisner, 1996) and feature weights w are learned during training using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; McDonald et al., 2005). 2.2 Base features Previous studies have defined different sets of features for the graph-based parsing models, such as the first-order features defined in McDonald et al. (2005), the second-order parent-siblings features defined in McDonald and Pereira (2006), and the second-order parent-child-grandchild features defined in Carreras (2007). Bohnet (2010) explorers a richer set of features than the above sets. We further extend the features defined by Bohnet (2010) by introducing more lexical features as the base features. The base feature templates are listed in Table 1, where h, d refer to the head, the dependent respectively, c refers to d’s sibling or child, b refers to the word between h and d, +1 (−1) refers to the next (previous) word, w and p refer to the surface word and part-of-speech tag respectively, [wp] refers to the surface word or p"
D13-1129,P05-1012,0,0.657697,"006) and Carreras (2007) define secondorder features over two adjacent arcs in secondorder graph-based models. Koo and Collins (2010) use third-order features in a third-order graph-based model. Bohnet (2010) considers information of more surrounding words for the graph-based models, while Zhang and Nivre (2011) define a set of rich features including the word valency and the third-order context features for transition-based models. All these models utilize richer and more complex feature representations and achieve better performance than the earlier models that utilize the simpler features (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). However, the richer feature representations result in a high-dimensional feature space. Features in such a space may suffer from the data sparseness problem and thus have less discriminative power on unseen data. If input sentences contain unknown features that are not included in training data, the parsers can usually give lower accuracy. In current dependency parsing models, conventional features (i.e. base features) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data"
D13-1129,C04-1010,0,0.0156794,"s over two adjacent arcs in secondorder graph-based models. Koo and Collins (2010) use third-order features in a third-order graph-based model. Bohnet (2010) considers information of more surrounding words for the graph-based models, while Zhang and Nivre (2011) define a set of rich features including the word valency and the third-order context features for transition-based models. All these models utilize richer and more complex feature representations and achieve better performance than the earlier models that utilize the simpler features (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). However, the richer feature representations result in a high-dimensional feature space. Features in such a space may suffer from the data sparseness problem and thus have less discriminative power on unseen data. If input sentences contain unknown features that are not included in training data, the parsers can usually give lower accuracy. In current dependency parsing models, conventional features (i.e. base features) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus exhibit less discriminat"
D13-1129,W96-0213,0,0.490388,"re with the right direction. In the auto-parsed data, this feature occurs 200 times and ranks between TOP10 and TOP30. Accordto convert the data into dependency structures with the English head rules of Yamada and Matsumoto (2003) and the Chinese head rules of Zhang and Clark (2008). We followed the standard data splits as shown in Table 3. Following the work of Koo et al. (2008), we used a tagger trained on training data to provide part-of-speech (POS) tags for the development and test sets, and used 10-way jackknifing to generate part-of-speech tags for the training set. We used the MXPOST (Ratnaparkhi, 1996) tagger for English and the CRF-based tagger for Chinese. We used gold standard segmentation in the CTB5. The data partition of Chinese were chosen to match previous work (Duan et al., 2007; Li et al., 2011; Hatori et al., 2011). I ate the meat with a fork I!!!!ate!!!!the!!!!meat!!!!with!!!!a!!!!fork!!!!. Tk:!hw,!dw,!cw,!d(h,d,c) Fb:!ate,!meat,!with,!RIGHTSIB &quot; (fb)=Mk [Mk];![Mk],!VV;![Mk],!ate Figure 1: An example of generating meta features ing to the mapping function, we obtain the mapped value Mk . Finally, we have the three meta features “[Mk ]”, “[Mk ], V V ”, and “[Mk ], ate”, where V V"
D13-1129,D07-1111,0,0.737038,"t years, supervised learning models have achieved lots of progress in the dependency parsing task, as can be found in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). The supervised models take annotated data as training data, utilize features defined over surface words, part-of-speech tags, and dependency trees, and learn the preference of features via adjusting feature weights. ∗ Corresponding author Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (McClosky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008; Chen et al., 2009). In this paper, we propose an alternative approach to semi-supervised dependency parsing via feature transformation (Ando and Zhang, 2005). More 1303 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics specifically, we transform base features to a higherlevel space. The base features defi"
D13-1129,P08-1076,0,0.0200172,"0 1 2 3 BIN 4 5 Figure 4: Improvement relative to numbers of active meta features on Chinese (average per word) Several previous studies relevant to our approach have been conducted. Koo et al. (2008) used a word clusters trained on a large amount of unannotated data and designed a set of new features based on the clusters for dependency parsing models. Chen et al. (2009) extracted subtree structures from a large amount of data and represented them as the additional features to improve dependency parsing. Suzuki et al. (2009) extended a Semi-supervised Structured Conditional Model (SSSCM) of Suzuki and Isozaki (2008) to the dependency parsing problem and combined their method with the word clustering feature representation of Koo et al. (2008). Chen et al. (2012) proposed an approach to representing high-order features for graphbased dependency parsing models using a dependency language model and beam search. In future work, we may consider to combine their methods with ours to improve performance. 1311 Several previous studies used co-training/selftraining methods. McClosky et al. (2006) presented a self-training method combined with a reranking algorithm for constituency parsing. Sagae and Tsujii (2007)"
D13-1129,D09-1058,0,0.449073,"ctive meta features on English (average per word) 50 Better Worse Percentage 40 30 20 10 0 1 2 3 BIN 4 5 Figure 4: Improvement relative to numbers of active meta features on Chinese (average per word) Several previous studies relevant to our approach have been conducted. Koo et al. (2008) used a word clusters trained on a large amount of unannotated data and designed a set of new features based on the clusters for dependency parsing models. Chen et al. (2009) extracted subtree structures from a large amount of data and represented them as the additional features to improve dependency parsing. Suzuki et al. (2009) extended a Semi-supervised Structured Conditional Model (SSSCM) of Suzuki and Isozaki (2008) to the dependency parsing problem and combined their method with the word clustering feature representation of Koo et al. (2008). Chen et al. (2012) proposed an approach to representing high-order features for graphbased dependency parsing models using a dependency language model and beam search. In future work, we may consider to combine their methods with ours to improve performance. 1311 Several previous studies used co-training/selftraining methods. McClosky et al. (2006) presented a self-training"
D13-1129,P11-2112,0,0.19353,". Table 1309 Table 10 shows the performance of the previous systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo10 refers to the thirdorder parser with model1 of Koo and Collins (2010), Zhang11 refers to the parser of Zhang and Nivre (2011), Li12 refers to the unlabeled parser of Li et al. (2012), Koo08 refers to the parser of Koo et al. (2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09 refers to the parser of Chen et al. (2009), Zhou11 refers to the parser of Zhou et al. (2011), Suzuki11 refers to the parser of Suzuki et al. (2011), and Chen12 refers to the parser of Chen et al. (2012). The results showed that our meta parser outperformed most of the previous systems and obtained the comparable accuracy with the best result of Suzuki11 (Suzuki et al., 2011) which combined the clustering-based word representations of Koo et al. (2008) and a condensed feature representation. However, our approach is much simpler than theirs and we believe that our meta parser can be further improved by combining their methods. Sup Semi System McDonald06 Koo10 Zhang11 Li12 Our Baseline Koo08 Suzuki09 Chen09 Zhou11 Suzuki11 Chen12 MetaParse"
D13-1129,W03-3023,0,0.167445,") define secondorder features over two adjacent arcs in secondorder graph-based models. Koo and Collins (2010) use third-order features in a third-order graph-based model. Bohnet (2010) considers information of more surrounding words for the graph-based models, while Zhang and Nivre (2011) define a set of rich features including the word valency and the third-order context features for transition-based models. All these models utilize richer and more complex feature representations and achieve better performance than the earlier models that utilize the simpler features (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). However, the richer feature representations result in a high-dimensional feature space. Features in such a space may suffer from the data sparseness problem and thus have less discriminative power on unseen data. If input sentences contain unknown features that are not included in training data, the parsers can usually give lower accuracy. In current dependency parsing models, conventional features (i.e. base features) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus"
D13-1129,D08-1059,1,0.331194,"procedure using template Tk = “hw , dw , cw , d(h, d, c)” (the second template of Table 1-(c) ), which contains the surface forms of the head, the dependent, its sibling, and the directions of the dependencies among h, d, and c. We can have a base feature “ate, meat, with, RIGHTSIB”, where “RIGHTSIB” refers to the parent-siblings structure with the right direction. In the auto-parsed data, this feature occurs 200 times and ranks between TOP10 and TOP30. Accordto convert the data into dependency structures with the English head rules of Yamada and Matsumoto (2003) and the Chinese head rules of Zhang and Clark (2008). We followed the standard data splits as shown in Table 3. Following the work of Koo et al. (2008), we used a tagger trained on training data to provide part-of-speech (POS) tags for the development and test sets, and used 10-way jackknifing to generate part-of-speech tags for the training set. We used the MXPOST (Ratnaparkhi, 1996) tagger for English and the CRF-based tagger for Chinese. We used gold standard segmentation in the CTB5. The data partition of Chinese were chosen to match previous work (Duan et al., 2007; Li et al., 2011; Hatori et al., 2011). I ate the meat with a fork I!!!!ate"
D13-1129,P11-2033,1,0.339716,"ogy, Soochow University, China ‡ Singapore University of Technology and Design, Singapore {wlchen, mzhang}@suda.edu.cn yue zhang@sutd.edu.sg Abstract In the supervised learning scenarios, many previous studies explore rich feature representation that leads to significant improvements. McDonald and Pereira (2006) and Carreras (2007) define secondorder features over two adjacent arcs in secondorder graph-based models. Koo and Collins (2010) use third-order features in a third-order graph-based model. Bohnet (2010) considers information of more surrounding words for the graph-based models, while Zhang and Nivre (2011) define a set of rich features including the word valency and the third-order context features for transition-based models. All these models utilize richer and more complex feature representations and achieve better performance than the earlier models that utilize the simpler features (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). However, the richer feature representations result in a high-dimensional feature space. Features in such a space may suffer from the data sparseness problem and thus have less discriminative power on unseen data. If input sentences conta"
D13-1129,P11-1156,0,0.436585,"Missing"
D13-1129,D07-1096,0,\N,Missing
D13-1163,J08-1001,0,0.0522055,"mong the three cohesion models proposed by Xiong et al. (2013). Modeling Coherence in Document-Level SMT In discourse analysis, cohesion is often studied together with coherence which is another dimension of the linguistic structure of a text (Barzilay and Elhadad, 1997). Cohesion is related to the surface structure of a text while coherence is concerned with the underlying meaning connectedness in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choic"
D13-1163,W13-3304,0,0.0103188,"nts our large-scale experiments and results. Finally, we conclude with future directions in Section 6. 1564 2 Related Work Recent years have witnessed growing research interests in document-level statistical machine translation. Such research efforts can be roughly divided into two groups: 1) general document-level machine translation that does not explore or explores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language top"
D13-1163,2007.tmi-papers.6,0,0.00909198,"ess in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on"
D13-1163,D07-1007,0,0.0489016,"ess in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on"
D13-1163,W09-2404,0,0.168473,"e model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining algorithm (See Section 4.1). 3 Background: Lexical Chain and Chain Computation Lexical chains are sequences of semantically related words (Morris and Hirst, 1991). They represent the lexical cohesion structure of a text. Figure 2 displays six lexical chains computed from the Chinese new"
D13-1163,P07-1005,0,0.0358991,"ith cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining alg"
D13-1163,C10-3004,0,0.0083293,"as only two sub-classes at level i + 1. Actually, they have multiple sub-classes. tended Cilin contains 77,343 Chinese words, which are organized in a hierarchical structure containing 5 levels as shown in Figure 3. In the 5th level, each node represents an atomic concept which consists of a set of synonyms. These atomic concepts are just like synsets in WordNet. We use them to represent senses of words in the disambiguation graph. We select nouns, verbs, abbreviations and idioms as candidate words for the disambiguation graph. These words are identified by a Chinese part-tospeech tagger LTP (Che et al., 2010) in a preprocessing step. In order to build the disambiguation graph, we first build an array indexed by the atomic concepts of Cilin, then insert a copy of each candidate word into its all concept (sense) entries in the array. After that, we create all semantic links among senses of different candidate words in the disambiguation graph following Galley and McKeown (2003). In the second step, we use the principle of one sense per discourse to perform WSD for each candidate word in the disambiguation graph. We sum the weights of all semantic links under the different senses of the candidate wor"
D13-1163,J07-2003,0,0.0588006,"which show the number of documents (#Doc) and sentences (#Sent), the number of lexical chains extracted from the source documents (#Chain), the average number of lexical chains per document (#AvgC) and the average number of words per lexical chain (#AvgW). Figure 4: Architecture of an SMT system with the lexical chain based cohesion model. 5 Experiments In this section, we conducted a series of experiments to validate the effectiveness of the proposed lexical chain based cohesion models for Chinese-to-English document-level machine translation. We used a hierarchical phrased-based SMT system (Chiang, 2007) trained on large-scale data. In particular, we aim at: • Measuring the impact of the threshold  on the probability cohesion model and selecting the best threshold on a development test set. • Investigating the effect of the two lexical-chain based cohesion models. • Comparing our lexical chain based cohesion models against the previous lexical cohesion device based models (Xiong et al., 2013). 5.1 Setup We collected our bilingual training data from LDC, which includes the corpus LDC2002E18, LDC2003E07, LDC2003E14, LDC2004E12, LDC2004T07, LDC2004T08 (Only Hong Kong News), LDC2005T06 and LDC20"
D13-1163,P11-2031,0,0.0155574,"C2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 3 Available at: http://homepages.inf.ed.ac.uk/lzhang10/ maxent toolkit.html  0.05 0.1 0.2 0.3 0.4 System Baseline LexChainCount(top 1) LexChainCount LexChainProb MT06 30.53 31.64 31.45 30.73 31.01 Table 2: BLEU scores of the probability cohesion model Mp (TDt , { LCtk }N k=1 ) with different values for the threshold . et al., 2002) as our evaluation metric. As MERT is normally instable, we ran the tuning process three times for all our experiments and presented the average BLEU scores on the three MERT runs as suggested by Clark et al (2011). 5.2 Setting the Threshold  As the two lexical chain based cohesion models are built on the super target lexical chains that are associated with a parameter , we need to tune the threshold parameter  on the development test set NIST MT06. We conducted a group of experiments using the probability cohesion model defined in Eq. (5) to find the best threshold. Experiment results are shown in Table 2. If we set the threshold too small (e.g., 0.05), the super target lexical chains may contain too many noisy words that are not the translations of source lexical chain words, which may jeopardise t"
D13-1163,H92-1045,0,0.467758,"s defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining algorithm (See Section 4.1). 3 Background: Lexical Chain and Chain Computation Lexical chains are sequences of semantically related words (Morris and Hirst, 1991). They represent the lexical cohesion structure of a text. Figure 2 displays six lexical chains computed from the Chinese news article shown in Figure 1. Words in these lexical chains have lexica"
D13-1163,D11-1084,1,0.745373,"Missing"
D13-1163,W13-3302,0,0.591796,"models. Section 5 presents our large-scale experiments and results. Finally, we conclude with future directions in Section 6. 1564 2 Related Work Recent years have witnessed growing research interests in document-level statistical machine translation. Such research efforts can be roughly divided into two groups: 1) general document-level machine translation that does not explore or explores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a to"
D13-1163,D12-1108,0,0.0625948,"l. (2012) soften this consistency constraint by integrating three counting features into decoder. Using Lexical Cohesion Devices in DocumentLevel SMT Lexical cohesion devices are semantically related words, including word repetition, synonyms/near-synonyms, hyponyms and so on. They are also the cohesion-building elements in lexical chains. Wong and Kit (2012) use lexical cohesion device based metrics to improve machine translation evaluation at the document level. These metrics measure the proportion of content words that are used as lexical cohesion devices in machine-generated translations. Hardmeier et al. (2012) propose a documentwide phrase-based decoder and integrate a semantic language model into the decoder. They argue that their semantic language model can capture lexical cohesion by exploring n-grams that cross sentence boundaries. Most recently Xiong et al. (2013) integrate three categories of lexical cohesion devices into document-level machine translation. They define three cohesion models based on lexical cohesion devices: a direct reward model, a conditional probability model and a mutual information trigger model. The latter two models measure the strength of lexical cohesion relation bet"
D13-1163,D12-1106,0,0.0318609,"els proposed by Xiong et al. (2013). Modeling Coherence in Document-Level SMT In discourse analysis, cohesion is often studied together with coherence which is another dimension of the linguistic structure of a text (Barzilay and Elhadad, 1997). Cohesion is related to the surface structure of a text while coherence is concerned with the underlying meaning connectedness in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the"
D13-1163,J91-1002,0,0.949629,"tween text units, namely co-reference, ellipsis, substitution, conjunction and lexical cohesion that is realized via semantically related words. The former four cohesion relations can be grouped as grammatical cohesion. Generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion (Barzilay and Elhadad, 1997). As most SMT systems translate a text in a sentence-by-sentence fashion, they tend to build less lexical cohesion than human translators (Wong and Kit, 2012). We therefore study lexical cohesion for document-level translation. We use lexical chains (Morris and Hirst, 1991) to capture lexical cohesion in a text. Lexical chains are connected graphs that represent the lexical cohesion structure of a text. They have been successfully used for information retrieval (Stairmand, 1996), document summarization (Barzilay and Elhadad, 1997) and so on. In this paper, we investigate how lexical chains can be used to incorporate lexical cohesion into document-level translation. Our basic assumption is that the lexical chains of a target document are direct correspondences of the lexical chains of its counterpart source document. This assumption is reasonable as the target do"
D13-1163,P02-1040,0,0.0861266,"Missing"
D13-1163,W10-2602,0,0.175477,"arch interests in document-level statistical machine translation. Such research efforts can be roughly divided into two groups: 1) general document-level machine translation that does not explore or explores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language topic words. Xiao et al. (2011) study the translation consistency issue in document-level machine translation. They use a hard constraint to consistently tran"
D13-1163,N12-1046,0,0.287225,"Missing"
D13-1163,D12-1097,0,0.648458,"76). Cohesion is a surface-level property of wellformed texts. It deals with five categories of relationships between text units, namely co-reference, ellipsis, substitution, conjunction and lexical cohesion that is realized via semantically related words. The former four cohesion relations can be grouped as grammatical cohesion. Generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion (Barzilay and Elhadad, 1997). As most SMT systems translate a text in a sentence-by-sentence fashion, they tend to build less lexical cohesion than human translators (Wong and Kit, 2012). We therefore study lexical cohesion for document-level translation. We use lexical chains (Morris and Hirst, 1991) to capture lexical cohesion in a text. Lexical chains are connected graphs that represent the lexical cohesion structure of a text. They have been successfully used for information retrieval (Stairmand, 1996), document summarization (Barzilay and Elhadad, 1997) and so on. In this paper, we investigate how lexical chains can be used to incorporate lexical cohesion into document-level translation. Our basic assumption is that the lexical chains of a target document are direct corr"
D13-1163,2011.mtsummit-papers.13,0,0.396437,"this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language topic words. Xiao et al. (2011) study the translation consistency issue in document-level machine translation. They use a hard constraint to consistently translate ambiguous source words into the most frequent translation options. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Using Lexical Cohesion Devices in DocumentLevel SMT Lexical cohesion devices are semantically related words, including word repetition, synonyms/near-synonyms, hyponyms and so on. They are also the cohesion-building elements in lexical chains. Wong and Kit (2012) use lexical cohesion device b"
D13-1163,W97-0703,0,\N,Missing
D15-1146,D14-1082,0,0.10557,"units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phr"
D15-1146,P14-1013,0,0.0880714,"ressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us"
D15-1146,P14-1129,0,0.0900992,"Missing"
D15-1146,P14-1066,0,0.0451391,"l “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us learn better phrase representations si"
D15-1146,D14-1176,0,0.224828,"on for x, we employ a greedy algorithm (Socher et al., 2011c) to minimize the sum of reconstruction error at each node in the binary tree T (x): X 1 Erec (x; θ) = k [c1 ; c2 ]n −[c01 ; c02 ]n k2 (3) 2 n∈T (x) where θ denotes model parameters and n represents a node in T (x). 2.2 BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different semantic representations. Zhang et al. (2014) use this intuition to constrain semantic pharse embedding learning. As shown in Figure 2, in addition to the abovementioned reconstruction error, BRAE introduces a max-semantic-margin error to minimize the semantic distance between translation equivalents and maximize the semantic distance between non(5) where Esem (f |e, θ) is defined as the semantic distance between the learned vector representations of f and e, denoted by pf and pe , respectively. Since phrase embeddings for the source and target language are learned separately in different vec(3) tor spaces, a transformation matrix Wf ∈ R"
D15-1146,P14-1006,0,0.0826745,"Missing"
D15-1146,D13-1176,0,0.0714049,"tures, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn b"
D15-1146,P14-1062,0,0.234774,"ation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and targ"
D15-1146,D14-1181,0,0.0346093,"ed from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases."
D15-1146,D13-1054,1,0.947567,"tations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal stru"
D15-1146,P13-1078,0,0.088091,"ic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more c"
D15-1146,P14-1140,0,0.190425,"on for x, we employ a greedy algorithm (Socher et al., 2011c) to minimize the sum of reconstruction error at each node in the binary tree T (x): X 1 Erec (x; θ) = k [c1 ; c2 ]n −[c01 ; c02 ]n k2 (3) 2 n∈T (x) where θ denotes model parameters and n represents a node in T (x). 2.2 BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different semantic representations. Zhang et al. (2014) use this intuition to constrain semantic pharse embedding learning. As shown in Figure 2, in addition to the abovementioned reconstruction error, BRAE introduces a max-semantic-margin error to minimize the semantic distance between translation equivalents and maximize the semantic distance between non(5) where Esem (f |e, θ) is defined as the semantic distance between the learned vector representations of f and e, denoted by pf and pe , respectively. Since phrase embeddings for the source and target language are learned separately in different vec(3) tor spaces, a transformation matrix Wf ∈ R"
D15-1146,P02-1038,0,0.273486,"es a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reordering model with the following parameters: iteration number iter=200 and gaussian prior g=1.0. Following Xiong et al. (2006), we use only boundary words of blocks to trigger the reordering model. The whole translation model is organized in a log-linear framework (Och and Ney, 2002). The adopted sub-models mainly include: (1) rule translation probabilities in two directions, (2) lexical weights in two directions, (3) targets-side word number, (4) phrase number, (5) language model score, and (6) the score of maximal entropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combina"
D15-1146,J03-1002,0,0.00554671,"consistency encoded in bilingual phrase structure learning, which is the basis of our model. Then, we describe the objective function which is composed of three types of errors. Finally, we provide details on the training of our model. 3.1 Structural Alignment Consistency We adapt word alignment to structural alignment and introduce some related concepts. Given a bilingual phrase (f, e) with its binary tree structures (Tf , Te ), if the source node nf¯ ∈ Tf covers a source-side sub-phrase f¯, and there exists a target-side sub-phrase e¯ such that (f¯, e¯) are consistent with word alignments (Och and Ney, 2003), we say nf¯ satisfies the structural alignment consistency, and it is referred to as a structuralalignment-consistent (SAC) node. Further, if e¯ is covered by a target node ne¯ ∈ Te , we say ne¯ is the aligned node of nf¯. In this way, several different target nodes may be all aligned to the same source node because of null alignments. For this, we choose the target node with the smallest span as the aligned one for the considered source node. This is because a smaller span reflects a stronger semantic relevance in most situations. Likewise, we have similar definitions for target nodes. Note"
D15-1146,P03-1021,0,0.0252438,"he reordering model with the following parameters: iteration number iter=200 and gaussian prior g=1.0. Following Xiong et al. (2006), we use only boundary words of blocks to trigger the reordering model. The whole translation model is organized in a log-linear framework (Och and Ney, 2002). The adopted sub-models mainly include: (1) rule translation probabilities in two directions, (2) lexical weights in two directions, (3) targets-side word number, (4) phrase number, (5) language model score, and (6) the score of maximal entropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram"
D15-1146,P02-1040,0,0.0961193,"es in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4 . Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html Parameter α β γ λL λrec λcon λlcrec BRAE 0.119 4.95 ×10−5 2.64 ×10−7 9.31 ×10−5 BCorrRAE 0.121 0.6331 0.2459 3.13 ×10−5 2.05 ×10−5 7.32 ×10−6 5.25 ×10−6 Table 1: Hyper-parameters for BCorrRAE and BRAE model. Method BCorrRAESM BCorrRAEST d 2"
D15-1146,W04-3250,0,0.191213,"reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4 . Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html Parameter α β γ λL λrec λcon λlcrec BRAE 0.119 4.95 ×10−5 2.64 ×10−7 9.31 ×10−5 BCorrRAE 0.121 0.6331 0.2459 3.13 ×10−5 2.05 ×10−5 7.32 ×10−6 5.25 ×10−6 Table 1: Hyper-parameters for BCorrRAE and BRAE model. Method BCorrRAESM BCorrRAEST d 25 50 75 100 25 50 75 100 MT06 30.81 30.58↓ 30.50 30.34"
D15-1146,D11-1014,0,0.360999,"Missing"
D15-1146,P14-2037,0,0.0449862,"Missing"
D15-1146,P13-1045,0,0.253422,". However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual c"
D15-1146,D13-1170,0,0.0484163,". However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual c"
D15-1146,D14-1003,0,0.014634,"f BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initia"
D15-1146,P14-1138,0,0.0135589,"e the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, man"
D15-1146,D14-1175,0,0.0183492,"learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits different levels of correspondence relations inside bilingual phrases instead of only the top level of entire"
D15-1146,D14-1023,0,0.0803451,"both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning rep"
D15-1146,D14-1015,0,0.0152033,"pecially the BCorrRAEST model, tends to choose shorter translations that are consistent with word alignments. 6 Related Work A variety of efforts have been devoted to learning vector representations for words/phrases with deep neural networks. According to the difference of learning contexts, previous work mainly include the following two strands. (1) Monolingual Word/Phrase Embeddings. The straightforward approach to represent word/phrases is to learn their hidden representations with traditional feature vectors, which requires manual and task-dependent feature engineering (Cui et al., 2014; Wu et al., 2014; 1255 Source Phrase 䌓㥎 (advocate) 惮䜃 坝揔 (serious challenge) 䋺㟙 䀛 嗪䛢 (data released) BRAE to advocate the in preaching the the promotion of as well as severe challenges a serious challenge to a serious challenge from by the figures published by the the statistics released by data published by the BCorrRAESM out to advocate been encouraging an advocate of rigorous challenges as well as severe challenges of severe challenges to the estimates announced at the figures published the statistics released by BCorrRAEST encouraging claimed advocate rigorous challenge enormous challenge severe challenge"
D15-1146,J97-3002,0,0.606435,"de n, and Cf and Ce count the number of nodes in the source and target tree structure respectively. Note that if we only compute the similarity for root nodes in the bilingual tree of (f, e), the structural similarity equals to the semantic similarity in Eq. (19). 5 Experiments We conducted experiments on NIST ChineseEnglish translation task to validate the effectiveness of BCorrRAE. 5.1 System Overview Our baseline decoder is a state-of-the-art phrasebased translation system equipped with a maximum entropy based reordering model (MEBTG). It adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006): merging 1253 rules A → [A1 , A2 ]|hA1 , A2 i which are used to merge two neighboring blocks2 A1 and A2 in a straight|inverted order, and lexical rule A → f /e used to translate a source phrase f into a target phrase e. The MEBTG system features a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reor"
D15-1146,P10-1049,0,0.0513633,"ntropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4 . Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html Pa"
D15-1146,P06-1066,1,0.784584,"Cf and Ce count the number of nodes in the source and target tree structure respectively. Note that if we only compute the similarity for root nodes in the bilingual tree of (f, e), the structural similarity equals to the semantic similarity in Eq. (19). 5 Experiments We conducted experiments on NIST ChineseEnglish translation task to validate the effectiveness of BCorrRAE. 5.1 System Overview Our baseline decoder is a state-of-the-art phrasebased translation system equipped with a maximum entropy based reordering model (MEBTG). It adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006): merging 1253 rules A → [A1 , A2 ]|hA1 , A2 i which are used to merge two neighboring blocks2 A1 and A2 in a straight|inverted order, and lexical rule A → f /e used to translate a source phrase f into a target phrase e. The MEBTG system features a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reordering model with the"
D15-1146,P13-1017,0,0.0236294,"nt levels of semantic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer exp"
D15-1146,P14-1011,0,0.106705,"rucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us learn better phrase"
D15-1146,D13-1141,0,0.299247,"n bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for success"
D15-1164,2011.eamt-1.38,0,0.0142119,"en et al. (2009) penalize substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels. Similar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima’an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012). Li et al. (2012) incorporate head information extracted from source-side dependency structures into translation rules. Besides, semantic knowledge is also used to refine nonterminals. Gao and Vogel (2011) utilize target-side semantic roles to form SRL-aware SCFG rules. Most of approaches introduced here explicitly require syntactic or semantic parsers trained on manually labeled data. On the other hand, efforts have also been directed towards attaching distributional linguistic knowledge to nonterminals. Venugopal et al. (2009) propose a preference grammar to annotate no"
D15-1164,P14-1062,0,0.0151151,"Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT? Learning semantic representations for terminals (words, multi-word phrases or sentences) from unlabeled data has achieved substantial progress in recent years (Mitchell and Lapata, 2008; Turian et al., 2010; Socher et al., 2010; Mikolov et al., 2013c; Blunsom et al., 2014). These representations have been used successfully in various NLP tasks. However, there is no attempt to learn semantic representations for nonterminals from unlabeled data. In this paper we propose a framework to learn semantic representations for nonterminal Xs in translation rules. Our framework is established on the basis of realvalued vector representations learned for multiword phrases, which are substituted with nonterminal Xs during hierarchical rule extraction. We propose a weighted mean value and a minimum distance method to obtain nonterminal representations from representations of"
D15-1164,C14-1210,0,0.0112383,"g distributional linguistic knowledge to nonterminals. Venugopal et al. (2009) propose a preference grammar to annotate nonterminals based on preference distributions of syntactic categories. Huang et al. (2010) learn la1392 tent syntactic distributions for each nonterminal. They use these distributions to decorate nonterminal Xs in SCFG rules with a real-valued feature vectors and utilize these vectors to measure the similarities between source phrases and applied rules. Similar to this work, Huang et al. (2013) utilize treebank tags based on dependency parsing to learn latent distributions. Cao et al. (2014) attach translation rules with dependency knowledge, which contains both dependency relations inside rules and dependency relations between rules and their contexts. The difference of our work from these studies is that our semantic representations are learned from unlabeled bilingual (or monolingual) data and do not depend on any linguistic resources, e.g., parsers. We also believe that our model is able to exploit both syntactic and semantic information for nonterminals since vector representations learned in our way are able to capture both syntactic and semantic properties (Turian et al.,"
D15-1164,W09-2307,0,0.0164908,"l weights, a word count, a phrase count and a glue rule count. In order to compare our proposed models with previous methods on nonterminal refinement, we re-implemented a syntax mismatch model (SynMis) which was used by Huang et al. (2013) and integrated it into hierarchical phrase-based system. Syn-Mis model decorates each nonterminal with a distribution of head POS tags and uses this distribution to measure the degree of syntactic compatibility of translation rules with corresponding source spans. In order to obtain head POS tags for Syn-Mis model, we used the Stanford dependency parser 6 (Chang et al., 2009) to parse Chinese sentences in our training corpus and NIST development/test sets. 5 We choose bilingual sentences because we want to obtain bilingual training examples to train our projection neural network as described in Section 3.3. 6 http://nlp.stanford.edu/software/lex-parser.shtml Baseline Syn-Mis MV + CS α = 1.0 MV + CS α = 0 MV + CS α = -1.0 MD + ED β = 0 MD + ED β = 0.5 MD + ED β = 1.0 MT06 30.54 31.23∗ 31.44+ 31.63∗ 31.13 31.02+ 31.35+ 31.06 MT08 23.58 24.38∗ 24.23∗ 24.51∗ 24.07∗ 23.74 24.08∗ 23.90+ Avg 27.06 27.81 27.84 28.07 27.60 27.38 27.72 27.48 Table 1: BLEU scores of our mode"
D15-1164,P10-1146,0,0.0228455,"here is a data sparseness problem in this model due to thousands of extracted syntactic categories. One solution to address this issue is to reduce the number of syntactic categories. Zollmann and Vogel (2011) use word tags, generated by either POS tagger or unsupervised word class induction, instead of syntactic categories. Hanneman and Lavie (2013) coarsen the label set by introducing a label collapsing algorithm to SAMT grammars (Zollmann and Venugopal, 2006). Yet another solution is easing restrictions on label matching. Shen et al. (2009) penalize substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels. Similar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima’an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012). Li et al. (2012) incorporate head i"
D15-1164,P11-2031,0,0.014447,". We used NIST MT03 as our development set, NIST MT06 as our development test set and MT08 as our final test set. We ran Giza++ on the training corpus in both Chinese-to-English and English-to-Chinese directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. We used the SRI Language Modeling Toolkit2 (Stolcke and others, 2002) to train our language models. MERT (Och, 2003) was adopted to tune feature weights of the decoder. We used the case-insensitive BLEU3 as our evaluation metric. In order to alleviate the instability of MERT , we followed Clark et al. (2011) to perform three runs of MERT and reported average BLEU scores over the three runs for all our experiments. We used word2vec toolkit4 to train our word embeddings and set the vector dimension d to 30. In our training experiment, we used the continuous bag-of-words model with a context window of size 5. The monolingual corpus, which was used to pre-train word embeddings, is extracted from 1 The corpora include LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 2 http://www.speech.sri.com/projects/srilm/download.html 3 ftp://jaguar.ncsl.nist.gov/mt/res"
D15-1164,W11-1012,0,0.103294,"manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules. In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information ∗ Corresponding author from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT? Learning semantic representations for terminals (words, multi-word phrases or sentences) from unlabeled data has achieved substantial progress in recen"
D15-1164,N13-1029,0,0.0723068,"007) explores formal synchronous context free grammar (SCFG) rules for translation. Two types of nonterminal symbols are used in translation rules: nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules. In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information ∗ Corresponding author from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mi"
D15-1164,W10-1761,0,0.0141531,"his issue is to reduce the number of syntactic categories. Zollmann and Vogel (2011) use word tags, generated by either POS tagger or unsupervised word class induction, instead of syntactic categories. Hanneman and Lavie (2013) coarsen the label set by introducing a label collapsing algorithm to SAMT grammars (Zollmann and Venugopal, 2006). Yet another solution is easing restrictions on label matching. Shen et al. (2009) penalize substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels. Similar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima’an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012). Li et al. (2012) incorporate head information extracted from source-side dependency structures into translation rules. Besides, semantic knowledge is also used to refine"
D15-1164,D10-1014,0,0.0178928,"xtracted from source-side dependency structures into translation rules. Besides, semantic knowledge is also used to refine nonterminals. Gao and Vogel (2011) utilize target-side semantic roles to form SRL-aware SCFG rules. Most of approaches introduced here explicitly require syntactic or semantic parsers trained on manually labeled data. On the other hand, efforts have also been directed towards attaching distributional linguistic knowledge to nonterminals. Venugopal et al. (2009) propose a preference grammar to annotate nonterminals based on preference distributions of syntactic categories. Huang et al. (2010) learn la1392 tent syntactic distributions for each nonterminal. They use these distributions to decorate nonterminal Xs in SCFG rules with a real-valued feature vectors and utilize these vectors to measure the similarities between source phrases and applied rules. Similar to this work, Huang et al. (2013) utilize treebank tags based on dependency parsing to learn latent distributions. Cao et al. (2014) attach translation rules with dependency knowledge, which contains both dependency relations inside rules and dependency relations between rules and their contexts. The difference of our work f"
D15-1164,D13-1053,0,0.181292,"ry SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules. In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information ∗ Corresponding author from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT? Learning semantic representati"
D15-1164,N03-1017,0,0.0607536,"er for integrating the target-side semantic model into translation, projection or two-pass decoding? 3. Does the combination of source and target semantic nonterminal refinement models provide further improvement? 6.1 Setup Our training corpus contains 2.9M sentence pairs with 80.9M Chinese words and 86.4M English words from LDC data1 . We used NIST MT03 as our development set, NIST MT06 as our development test set and MT08 as our final test set. We ran Giza++ on the training corpus in both Chinese-to-English and English-to-Chinese directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. We used the SRI Language Modeling Toolkit2 (Stolcke and others, 2002) to train our language models. MERT (Och, 2003) was adopted to tune feature weights of the decoder. We used the case-insensitive BLEU3 as our evaluation metric. In order to alleviate the instability of MERT , we followed Clark et al. (2011) to perform three runs of MERT and reported average BLEU scores over the three runs for all our experiments. We used word2vec toolkit4 to train our word embeddings and set the vector dimension d to 30. In our training experiment, we used the continuous bag-of-wor"
D15-1164,P12-2007,0,0.0318201,"Missing"
D15-1164,P08-1028,0,0.0287859,"se efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT? Learning semantic representations for terminals (words, multi-word phrases or sentences) from unlabeled data has achieved substantial progress in recent years (Mitchell and Lapata, 2008; Turian et al., 2010; Socher et al., 2010; Mikolov et al., 2013c; Blunsom et al., 2014). These representations have been used successfully in various NLP tasks. However, there is no attempt to learn semantic representations for nonterminals from unlabeled data. In this paper we propose a framework to learn semantic representations for nonterminal Xs in translation rules. Our framework is established on the basis of realvalued vector representations learned for multiword phrases, which are substituted with nonterminal Xs during hierarchical rule extraction. We propose a weighted mean value and"
D15-1164,P11-1065,0,0.0395414,"Missing"
D15-1164,P03-1021,0,0.0635561,"cond pass, we decode source sentence with our target semantic nonterminal refinement model using learned target phrase vector representations. If a target phrase appears in the collected set, the target-side semantic nonterminal refinement model will calculate the semantic similarity between the target phrase and the corresponding nonterminal on the target semantic space; otherwise the model will give a penalty. This is because this phrase is not a desirable phrase as it is not used in 100-best translations. The weights of these two features are tuned by the Minimum Error Rate Training (MERT)(Och, 2003), together with weights of other sub-models on a development set. Figure 2 shows the architecture of SMT system with the proposed semantic nonterminal refinement model. 2. Can the target-side semantic nonterminal refinement model improve translation quality? And which method is better for integrating the target-side semantic model into translation, projection or two-pass decoding? 3. Does the combination of source and target semantic nonterminal refinement models provide further improvement? 6.1 Setup Our training corpus contains 2.9M sentence pairs with 80.9M Chinese words and 86.4M English w"
D15-1164,D09-1008,0,0.0152513,"trees to augment nonterminals in hierarchical rules. Unfortunately, there is a data sparseness problem in this model due to thousands of extracted syntactic categories. One solution to address this issue is to reduce the number of syntactic categories. Zollmann and Vogel (2011) use word tags, generated by either POS tagger or unsupervised word class induction, instead of syntactic categories. Hanneman and Lavie (2013) coarsen the label set by introducing a label collapsing algorithm to SAMT grammars (Zollmann and Venugopal, 2006). Yet another solution is easing restrictions on label matching. Shen et al. (2009) penalize substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels. Similar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima’an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al."
D15-1164,N09-1027,0,0.147556,"nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules. In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information ∗ Corresponding author from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT? Learning"
D15-1164,W12-3127,0,0.0147002,"e substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels. Similar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima’an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012). Li et al. (2012) incorporate head information extracted from source-side dependency structures into translation rules. Besides, semantic knowledge is also used to refine nonterminals. Gao and Vogel (2011) utilize target-side semantic roles to form SRL-aware SCFG rules. Most of approaches introduced here explicitly require syntactic or semantic parsers trained on manually labeled data. On the other hand, efforts have also been directed towards attaching distributional linguistic knowledge to nonterminals. Venugopal et al. (2009) propose a preference grammar to annotate nonterminals based on p"
D15-1164,D07-1071,0,0.0358701,"ear projection methods for integrating the target-side semantic nonterminal refinement model in terms of BLEU scores. /*” and /+” : significantly better than Baseline at significance level p < 0.01 and p < 0.05 respectively. 6.3 1 2 • Two-pass decoding achieves the highest BLEU scores, which are higher than those of the baseline by 0.75 and 0.66 BLEU points on MT06 and MT08 respectively. The reason may be that noisy translation candidates are filtered out in the first pass. This finding is consistent with many other multiple-pass systems in natural language processing, e.g., two-pass parsing (Zettlemoyer and Collins, 2007). • Nonlinear projection achieves an improvement of 0.62 BLEU points over the baseline on MT06. It outperforms linear projection method on both sets. These empirical results support our assumption that nonlinear relations between languages are more reasonable than linear relations. MT08 23.58 24.38∗ 24.51∗ 24.11∗ 24.72∗ Avg 27.06 27.81 28.07 27.64 28.22 (MV + CS α = 0) is used. Nonlinear Projection is used. Table 3: BLEU scores of the combination of the source- and target-side semantic nonterminal refine model. /*” and /+” : significantly better than Baseline at significance level p < 0.01 and"
D15-1164,W06-3119,0,0.299692,"est sets. 1 Introduction Hierarchical phrase-based translation (Chiang, 2007) explores formal synchronous context free grammar (SCFG) rules for translation. Two types of nonterminal symbols are used in translation rules: nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules. In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information ∗ Corresponding author from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a lar"
D15-1164,P11-1001,0,0.128495,"rchical phrase-based translation (Chiang, 2007) explores formal synchronous context free grammar (SCFG) rules for translation. Two types of nonterminal symbols are used in translation rules: nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules. In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information ∗ Corresponding author from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled dat"
D15-1164,D11-1014,0,0.558278,"We employ a neural method, specifically the continuous bag-of-words model (Mikolov et al., 2013a) to learn high-quality vector representations for words. Once we complete the training of the continuous bag-of-words model, word embeddings form an embedding matrix M ∈ Rd×|V |, where d is a pre-determined embedding dimensionality and each word w in the vocabulary V corresponds to a vector ~v ∈ Rd . Given the embedding matrix M , mapping words to vectors can be done by simply looking up their respective columns in M . We further feed these learned word embeddings to recursive autoencoders (RAE) (Socher et al., 2011) for learning phrase representations. In traditional RAE (shown in Figure 1), given two input children representation vectors c~1 ∈ Rd and c~2 ∈ Rd , their parent representation p~ can be calculated as follows: p~ = f (1) (W (1) [c~1 ; c~2 ] + b(1) ) (1) where [c~1 ; c~2 ] ∈ R2d is the concatenation of vectors of two children, W (1) ∈ Rd×2d is a weight matrix, b(1) ∈ Rd is a bias term, and f (1) is an element-wise activation function such as tanh. The above output representation p~ can be used as a child vector to construct the representation for a larger subphrase. This process is repeated un"
D15-1164,P10-1040,0,0.181821,"rating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT? Learning semantic representations for terminals (words, multi-word phrases or sentences) from unlabeled data has achieved substantial progress in recent years (Mitchell and Lapata, 2008; Turian et al., 2010; Socher et al., 2010; Mikolov et al., 2013c; Blunsom et al., 2014). These representations have been used successfully in various NLP tasks. However, there is no attempt to learn semantic representations for nonterminals from unlabeled data. In this paper we propose a framework to learn semantic representations for nonterminal Xs in translation rules. Our framework is established on the basis of realvalued vector representations learned for multiword phrases, which are substituted with nonterminal Xs during hierarchical rule extraction. We propose a weighted mean value and a minimum distance m"
D15-1164,J07-2003,0,\N,Missing
D16-1037,D15-1262,0,0.0464737,"hich we describe in succession. Implicit Discourse Relation Recognition Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015"
D16-1037,P15-2015,0,0.0197483,"(Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from theirs significantly, which can be summarized in the following three as"
D16-1037,C14-1088,0,0.0210779,"3,4 , Rongrong Ji1 , Hong Duan1 , Min Zhang2 Xiamen University, Xiamen, China 3610051 Provincial Key Laboratory for Computer Information Processing Technology Soochow University, Suzhou, China 2150062 ADAPT Centre, School of Computing, Dublin City University3 Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences4 zb@stu.xmu.edu.cn, {jssu, rrji, hduan}@xmu.edu.cn qun.liu@dcu.ie, {dyxiong, minzhang}@suda.edu.cn Abstract other relevant natural language processing tasks, such as text summarization (Yoshida et al., 2014), conversation (Higashinaka et al., 2014), question answering (Verberne et al., 2007) and information extraction (Cimiano et al., 2005). Generally, discourse relations can be divided into two categories: explicit and implicit, which can be illustrated in the following example: Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse"
D16-1037,Q15-1024,0,0.411507,"and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic space. It is this latent variable that generates both discourse arguments and the corresponding relation"
D16-1037,N16-1037,0,0.0279266,"implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from theirs significantly, which can be summarized in the following three aspects: 1) they employ the recurrent neural network to represent the discourse arguments, while we use the simple feedforward neural network; 2) they treat the discourse relations directly as latent variables, rather than the underlying semantic representation of discourses; 3) their model is optimized in terms of the data likelihood, since the discourse relations are observed during training. However, VarNDRR is optimized under the variational theory. Variational Neural Model In the presence"
D16-1037,P13-1047,0,0.0602548,"ies (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from theirs significantly,"
D16-1037,D09-1036,0,0.36099,"sample z from qφ (z|x, y) that not only bridges the gap between the recognizer and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic space. It is this l"
D16-1037,W10-4310,0,0.0179631,"nition and variational neural model, which we describe in succession. Implicit Discourse Relation Recognition Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud"
D16-1037,W12-1614,0,0.0150111,"Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capab"
D16-1037,D13-1094,0,0.0931866,"Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestig"
D16-1037,P09-1077,0,0.340564,"rization technique to sample z from qφ (z|x, y) that not only bridges the gap between the recognizer and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic"
D16-1037,prasad-etal-2008-penn,0,0.672371,"us making the setting z˜ = µ0 during testing reasonable. The second term is the approximate expectation of Eqφ (z|x,y) [log pθ (x, y|z)], which is also differentiable. As the objective function in Eq. (13) is differentiable, we can optimize both the model parameters θ and variational parameters φ jointly using standard gradient ascent techniques. The training procedure for VarNDRR is summarized in Algorithm 1. 4 Experiments We conducted experiments on English implicit DRR task to validate the effectiveness of VarNDRR.4 4.1 Dataset We used the largest hand-annotated discourse corpus PDTB 2.05 (Prasad et al., 2008) (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work (Pitler et al., 2009; Zhou et al., 2010; Lan et 4 Source code is available https://github.com/DeepLearnXMU/VarNDRR. 5 http://www.seas.upenn.edu/ pdtb/ at Model R & X (2015) J & E (2015) SVM SCNN VarNDRR Acc 70.27 63.10 60.42 63.30 P 22.79 22.00 24.00 R 64.47 67.76 71.05 F1 41.00 35.93 33.68 33.22 35.88 Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR (a) C OM vs Other Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR Acc 6"
D16-1037,E14-1068,0,0.0315926,"Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have bee"
D16-1037,N15-1081,0,0.502665,"h02 = dm = dhy = 400, dy = 2 for all experiments.7 . All parameters of VarNDRR are initialized by a Gaussian distribution (µ = 0, σ = 0.01). For Adam, we set β1 = 0.9, β2 = 0.999 with a learning rate 0.001. Additionally, we tied the following parameters in practice: Wh1 and Wh2 , Wx01 and Wx02 . We compared VarNDRR against the following two different baseline methods: • SVM: a support vector machine (SVM) classifier8 trained with several manual features. • SCNN: a shallow convolutional neural network proposed by Zhang et al. (2015). We also provide results from two state-of-the-art systems: • Rutherford and Xue (2015) convert explicit discourse relations into implicit instances. • Ji and Eisenstein (2015) augment discourse representations via entity connections. 7 8 http://nlp.stanford.edu/software/corenlp.shtml 387 There is one dimension in dx1 and dx2 for unknown words. http://svmlight.joachims.org/ 45 40 35 30 25 20 15 10 5 0 80 70 60 50 40 30 20 10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 117 218 19 13 420 521 622 723 824 25 9 26 10 27 11 28 1229 1330 1431 1532 1633 1734 1835 1936 2037 2138 1 39 22 2340 2441 2542 43 26 44 27 45 28 46 2947 3048 3149 3250 3351 3452 3553 3654 -1270.24 25.3012 -207.21 26.0"
D16-1037,C12-1168,0,0.0167503,"en exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from thei"
D16-1037,D14-1196,0,0.0259392,", Deyi Xiong2∗, Jinsong Su1 , Qun Liu3,4 , Rongrong Ji1 , Hong Duan1 , Min Zhang2 Xiamen University, Xiamen, China 3610051 Provincial Key Laboratory for Computer Information Processing Technology Soochow University, Suzhou, China 2150062 ADAPT Centre, School of Computing, Dublin City University3 Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences4 zb@stu.xmu.edu.cn, {jssu, rrji, hduan}@xmu.edu.cn qun.liu@dcu.ie, {dyxiong, minzhang}@suda.edu.cn Abstract other relevant natural language processing tasks, such as text summarization (Yoshida et al., 2014), conversation (Higashinaka et al., 2014), question answering (Verberne et al., 2007) and information extraction (Cimiano et al., 2005). Generally, discourse relations can be divided into two categories: explicit and implicit, which can be illustrated in the following example: Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models a"
D16-1037,D15-1266,1,0.82891,"also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic space. It is this latent variable that generates both discourse arguments and the corresponding relation, i.e. p(x, y|z). The"
D16-1037,C10-2172,0,0.205752,"differentiable, we can optimize both the model parameters θ and variational parameters φ jointly using standard gradient ascent techniques. The training procedure for VarNDRR is summarized in Algorithm 1. 4 Experiments We conducted experiments on English implicit DRR task to validate the effectiveness of VarNDRR.4 4.1 Dataset We used the largest hand-annotated discourse corpus PDTB 2.05 (Prasad et al., 2008) (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work (Pitler et al., 2009; Zhou et al., 2010; Lan et 4 Source code is available https://github.com/DeepLearnXMU/VarNDRR. 5 http://www.seas.upenn.edu/ pdtb/ at Model R & X (2015) J & E (2015) SVM SCNN VarNDRR Acc 70.27 63.10 60.42 63.30 P 22.79 22.00 24.00 R 64.47 67.76 71.05 F1 41.00 35.93 33.68 33.22 35.88 Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR (a) C OM vs Other Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR Acc 69.80 60.71 63.00 57.36 P 65.89 56.29 56.46 Acc 76.95 62.62 63.00 53.82 P 39.14 39.80 35.39 R 72.40 75.29 88.53 F1 53.80 52.78 50.82 52.04 50.56 R 68.24 62.35 97.65 F1 33.30 27.63 24.73 30.54 29.54 (b) C ON"
D16-1050,D16-1025,0,0.0129473,"ts that gain 0.86 and 1.35 BLEU points over Moses and GroundHog respectively. Besides, without the KL objective, VNMT w/o KL obtains even worse results than GroundHog. These results indicate the following two points: 1) explicitly modeling underlying semantics by a latent variable indeed benefits neural machine translation, and 2) the improvements of our model are not from enlarging the network. https://github.com/DeepLearnXMU/VNMT. 527 Results on Long Sentences We further testify VNMT on long sentence translation where the vanilla NMT usually suffers from attention failures (Tu et al., 2016; Bentivogli et al., 2016). We believe that the global latent variable can play an important role on long sentence translation. Our first experiment is carried out on 6 disjoint groups according to the length of source sentences in our test sets. Figure 3 shows the BLEU scores of two neural models. We find that the performance curve of our VNMT model always appears to be on top of that of GroundHog with a certain margin. Specifically, on the final group with the longest source sentences, our VNMT obtains the biggest improvement (3.55 BLEU points). Overall, these obvious improvements on all groups in terms of the length"
D16-1050,P15-1001,0,0.416218,"t2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 (Papineni et al., 2002) metric to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test. We compared our model against two state-of-theart SMT and NMT systems: • Moses (Koehn et al., 2007): a phrase-based SMT system. 4 This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT’14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/ 526 • GroundHog (Bahdanau et al., 2014): attention-based NMT system. an Additionally, we also compared with a variant of VNMT, which does not contain the KL part in the objective (VNMT w/o KL). This is achieved by setting hz to µ0 . For Moses, we adopted all the default settings except for the language model. We trained a 4-gram language model on the Xinhua section of the English Gigaword corpus (306M words) using the SRILM7 toolkit with modified Kneser-Ney smoothing. Important"
D16-1050,D13-1176,0,0.0324504,"the vanilla neural machine translation baselines. 1 Introduction Neural machine translation (NMT) is an emerging translation paradigm that builds on a single and unified end-to-end neural network, instead of using a variety of sub-models tuned in a long training pipeline. It requires a much smaller memory than ∗ Corresponding author phrase- or syntax-based statistical machine translation (SMT) that typically has a huge phrase/rule table. Due to these advantages over traditional SMT system, NMT has recently attracted growing interests from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016). Current NMT models mainly take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into distributed representations, and a neural decoder generates the corresponding target sentence y according to these representations1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Typically, the underlying semantic representations of source and target sentences are learned in an impl"
D16-1050,P07-2045,0,0.0263654,"the NIST MT02/03/04/06/08 datasets as the test sets for the Chinese-English task. Our English-German training data5 consists of 4.5M sentence pairs with 116M English words and 110M German words6 . We used the newstest2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 (Papineni et al., 2002) metric to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test. We compared our model against two state-of-theart SMT and NMT systems: • Moses (Koehn et al., 2007): a phrase-based SMT system. 4 This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT’14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/ 526 • GroundHog (Bahdanau et al., 2014): attention-based NMT system. an Additionally, we also compared with a variant of VNMT, which does not contain the KL part in the objective (VNMT w/o KL). This is achieved by setting hz to µ0 . For Moses, we adopted all the defau"
D16-1050,W04-3250,0,0.0315104,"80.9M Chinese words and 86.4M English words respectively. We used the NIST MT05 dataset as the development set, and the NIST MT02/03/04/06/08 datasets as the test sets for the Chinese-English task. Our English-German training data5 consists of 4.5M sentence pairs with 116M English words and 110M German words6 . We used the newstest2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 (Papineni et al., 2002) metric to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test. We compared our model against two state-of-theart SMT and NMT systems: • Moses (Koehn et al., 2007): a phrase-based SMT system. 4 This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT’14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/ 526 • GroundHog (Bahdanau et al., 2014): attention-based NMT system. an Additionally, we also compared with a variant of VNMT, which does not con"
D16-1050,P09-1067,0,0.0159698,"hat allows the iterative construction of complex images. Very recently, Miao et al. (2015) propose a generic variational inference framework for generative and conditional models of text. The most related work is that of Bowman et 529 al. (2015), where they develop a variational autoencoder for unsupervised generative language modeling. The major difference is that they focus on the monolingual language model, while we adapt this technique to bilingual translation. Although variational neural models have been widely used in NLP tasks and the variational decoding has been investigated for SMT (Li et al., 2009), the adaptation and utilization of variational neural model to neural machine translation, to the best of our knowledge, has never been investigated before. 6 Conclusion and Future Work In this paper, we have presented a variational model for neural machine translation that incorporates a continuous latent variable to model the underlying semantics of sentence pairs. We approximate the posterior distribution with neural networks and reparameterize the variational lower bound. This enables our model to be an end-to-end neural network that can be optimized through the stochastic gradient algori"
D16-1050,D15-1166,0,0.478478,") is an emerging translation paradigm that builds on a single and unified end-to-end neural network, instead of using a variety of sub-models tuned in a long training pipeline. It requires a much smaller memory than ∗ Corresponding author phrase- or syntax-based statistical machine translation (SMT) that typically has a huge phrase/rule table. Due to these advantages over traditional SMT system, NMT has recently attracted growing interests from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016). Current NMT models mainly take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into distributed representations, and a neural decoder generates the corresponding target sentence y according to these representations1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Typically, the underlying semantic representations of source and target sentences are learned in an implicit way in this framework, which heavily relies on the attention mechanism (Bahdanau"
D16-1050,P15-1002,0,0.0600673,"Missing"
D16-1050,P02-1040,0,0.0970109,"n translation tasks. Our Chinese-English training data4 consists of 2.9M sentence pairs, with 80.9M Chinese words and 86.4M English words respectively. We used the NIST MT05 dataset as the development set, and the NIST MT02/03/04/06/08 datasets as the test sets for the Chinese-English task. Our English-German training data5 consists of 4.5M sentence pairs with 116M English words and 110M German words6 . We used the newstest2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 (Papineni et al., 2002) metric to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test. We compared our model against two state-of-theart SMT and NMT systems: • Moses (Koehn et al., 2007): a phrase-based SMT system. 4 This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT’14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/ 526 • GroundHog (Bahdanau et al., 2014): attention-based NMT"
D16-1050,P16-5005,0,0.196025,"-end neural network, instead of using a variety of sub-models tuned in a long training pipeline. It requires a much smaller memory than ∗ Corresponding author phrase- or syntax-based statistical machine translation (SMT) that typically has a huge phrase/rule table. Due to these advantages over traditional SMT system, NMT has recently attracted growing interests from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016). Current NMT models mainly take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into distributed representations, and a neural decoder generates the corresponding target sentence y according to these representations1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Typically, the underlying semantic representations of source and target sentences are learned in an implicit way in this framework, which heavily relies on the attention mechanism (Bahdanau et al., 2014) to identify semantic alignments between source and target words"
D16-1050,D07-1091,0,\N,Missing
D16-1072,P09-1059,0,0.123204,", and syntactic structures (Xue et al., 2005; Xia, 2000), whereas People’s Daily corpus (PD)1 is a large-scale corpus annotated with words and POS tags, containing about 300 thousand sentences from the first half of 1998 of People’s Daily newspaper (Yu et al., 2003). Table 1 gives an example with both CTB and PD annotations. We can see that CTB and PD differ in both word boundary standards and POS tag sets. Previous work on exploiting heterogeneous data mainly focuses on indirect guide-feature methods. The basic idea is to use one resource to generate extra guide features on another resource (Jiang et al., 2009; Sun and Wan, 2012), which is similar to stacked learning (Nivre and McDonald, 2008). Li et al. (2015) propose a coupled sequence labeling approach that can directly learn and predict two heterogeneous annotations simultaneously. The basic idea is to transform a single-side tag into a set of bundled tags for weak supervision based on the idea of ambiguous labeling. Due to the huge size of the bundled tag space, their coupled model is extremely inefficient. They then carefully design tag-to-tag 1 http://icl.pku.edu.cn/icl_groups/ corpustagging.asp 753 Proceedings of the 2016 Conference on Empi"
D16-1072,P13-1075,0,0.31356,"Missing"
D16-1072,P10-1001,0,0.055785,"Missing"
D16-1072,P12-1071,1,0.860867,"where such heterogeneous resources are ubiquitous due to historical reasons. Jiang et al. (2009) first propose the guide-feature approach, which is similar to stacked learning (Nivre and McDonald, 2008), for joint WS&POS tagging on CTB and PD. Sun and Wan (2012) further extend the guide-feature method and propose a more complex sub-word stacking approach. Qiu et al. (2013) propose a linear coupled model similar to that of Li et al. (2015). The key difference is that the model of Qiu et al. (2013) only uses separate features, while Li et al. (2015) and this work explore joint features as well. Li et al. (2012a) apply the guide-feature idea to dependency parsing on CTB and PD. Zhang et al. (2014a) extend a shift-reduce dependency parsing model in order to simultaneously learn and produce two heterogeneous parse trees, which however assumes the existence of training data with both-side annotations. Our context-aware pruning approach is similar to coarse-to-fine pruning in parsing community (Koo and Collins, 2010; Rush and Petrov, 2012), which is a useful technique that allows us to use very complex parsing models without too much efficiency cost. The idea is first to use a simple and basic off-shelf"
D16-1072,C12-1103,1,0.830618,"where such heterogeneous resources are ubiquitous due to historical reasons. Jiang et al. (2009) first propose the guide-feature approach, which is similar to stacked learning (Nivre and McDonald, 2008), for joint WS&POS tagging on CTB and PD. Sun and Wan (2012) further extend the guide-feature method and propose a more complex sub-word stacking approach. Qiu et al. (2013) propose a linear coupled model similar to that of Li et al. (2015). The key difference is that the model of Qiu et al. (2013) only uses separate features, while Li et al. (2015) and this work explore joint features as well. Li et al. (2012a) apply the guide-feature idea to dependency parsing on CTB and PD. Zhang et al. (2014a) extend a shift-reduce dependency parsing model in order to simultaneously learn and produce two heterogeneous parse trees, which however assumes the existence of training data with both-side annotations. Our context-aware pruning approach is similar to coarse-to-fine pruning in parsing community (Koo and Collins, 2010; Rush and Petrov, 2012), which is a useful technique that allows us to use very complex parsing models without too much efficiency cost. The idea is first to use a simple and basic off-shelf"
D16-1072,P15-1172,1,0.724149,"Missing"
D16-1072,D14-1093,0,0.302157,"Missing"
D16-1072,P08-1108,0,0.0427893,"ly corpus (PD)1 is a large-scale corpus annotated with words and POS tags, containing about 300 thousand sentences from the first half of 1998 of People’s Daily newspaper (Yu et al., 2003). Table 1 gives an example with both CTB and PD annotations. We can see that CTB and PD differ in both word boundary standards and POS tag sets. Previous work on exploiting heterogeneous data mainly focuses on indirect guide-feature methods. The basic idea is to use one resource to generate extra guide features on another resource (Jiang et al., 2009; Sun and Wan, 2012), which is similar to stacked learning (Nivre and McDonald, 2008). Li et al. (2015) propose a coupled sequence labeling approach that can directly learn and predict two heterogeneous annotations simultaneously. The basic idea is to transform a single-side tag into a set of bundled tags for weak supervision based on the idea of ambiguous labeling. Due to the huge size of the bundled tag space, their coupled model is extremely inefficient. They then carefully design tag-to-tag 1 http://icl.pku.edu.cn/icl_groups/ corpustagging.asp 753 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 753–762, c Austin, Texas, Novembe"
D16-1072,D13-1062,0,0.2279,"d stacking approach of Sun and Wan (2012) can be understood as a more complex 760 A lot of research has been devoted to design an effective way to exploit non-overlapping heterogeneous labeled data, especially in Chinese language processing, where such heterogeneous resources are ubiquitous due to historical reasons. Jiang et al. (2009) first propose the guide-feature approach, which is similar to stacked learning (Nivre and McDonald, 2008), for joint WS&POS tagging on CTB and PD. Sun and Wan (2012) further extend the guide-feature method and propose a more complex sub-word stacking approach. Qiu et al. (2013) propose a linear coupled model similar to that of Li et al. (2015). The key difference is that the model of Qiu et al. (2013) only uses separate features, while Li et al. (2015) and this work explore joint features as well. Li et al. (2012a) apply the guide-feature idea to dependency parsing on CTB and PD. Zhang et al. (2014a) extend a shift-reduce dependency parsing model in order to simultaneously learn and produce two heterogeneous parse trees, which however assumes the existence of training data with both-side annotations. Our context-aware pruning approach is similar to coarse-to-fine pr"
D16-1072,P02-1035,0,0.214026,"Missing"
D16-1072,N12-1054,0,0.0575587,"Missing"
D16-1072,P12-1025,0,0.0799453,"ctures (Xue et al., 2005; Xia, 2000), whereas People’s Daily corpus (PD)1 is a large-scale corpus annotated with words and POS tags, containing about 300 thousand sentences from the first half of 1998 of People’s Daily newspaper (Yu et al., 2003). Table 1 gives an example with both CTB and PD annotations. We can see that CTB and PD differ in both word boundary standards and POS tag sets. Previous work on exploiting heterogeneous data mainly focuses on indirect guide-feature methods. The basic idea is to use one resource to generate extra guide features on another resource (Jiang et al., 2009; Sun and Wan, 2012), which is similar to stacked learning (Nivre and McDonald, 2008). Li et al. (2015) propose a coupled sequence labeling approach that can directly learn and predict two heterogeneous annotations simultaneously. The basic idea is to transform a single-side tag into a set of bundled tags for weak supervision based on the idea of ambiguous labeling. Due to the huge size of the bundled tag space, their coupled model is extremely inefficient. They then carefully design tag-to-tag 1 http://icl.pku.edu.cn/icl_groups/ corpustagging.asp 753 Proceedings of the 2016 Conference on Empirical Methods in Nat"
D16-1072,N13-1126,0,0.0214313,"Missing"
D16-1072,D14-1010,0,0.214771,"Missing"
D16-1072,P08-1101,0,0.0348463,"plates, and return local feature vectors for tagging wi−1 as t′ and wi as t. Traditional single-side tagging models can only exploit a single set of separate features fsep_a (.) or fsep_b (.). In contrast, the coupled model makes use of all three sets of features. Li et al. (2015) demonstrate that the joint features fjoint (.) capture the implicit mappings between heterogeneous annotations, and the separate features function as back-off features for alleviating the data sparseness problem of the joint features. For the feature templates, we follow Li et al. (2015) and adopt those described in Zhang and Clark (2008) for POS tagging, and use those described in Zhang et al. (2014b) for joint WS&POS tagging. derivations are as follows: ∂log Z(x, S; θ) ∂θ ∑ ∂ t∈S eScore(x,t;θ) 1 = × Z(x, S; θ) ∂θ ( ) ∑ eScore(x,t;θ) ∂Score(x, t; θ) = × Z(x, S; θ) ∂θ t∈S ∑ = p(t|x, S; θ) × f(x, t) (5) t∈S 2.2 =Et|x,S;θ [f(x, t)] Learn from Incomplete Data The key challenge for coupled sequence labeling is that both CTB and PD are non-overlapping and each contains only one-side annotations. Based on the idea of ambiguous labeling, Li et al. (2015) first concatenate a single-side tag with many possible second-side tags, and the"
D16-1072,C14-1051,0,0.0481396,"wi as t. Traditional single-side tagging models can only exploit a single set of separate features fsep_a (.) or fsep_b (.). In contrast, the coupled model makes use of all three sets of features. Li et al. (2015) demonstrate that the joint features fjoint (.) capture the implicit mappings between heterogeneous annotations, and the separate features function as back-off features for alleviating the data sparseness problem of the joint features. For the feature templates, we follow Li et al. (2015) and adopt those described in Zhang and Clark (2008) for POS tagging, and use those described in Zhang et al. (2014b) for joint WS&POS tagging. derivations are as follows: ∂log Z(x, S; θ) ∂θ ∑ ∂ t∈S eScore(x,t;θ) 1 = × Z(x, S; θ) ∂θ ( ) ∑ eScore(x,t;θ) ∂Score(x, t; θ) = × Z(x, S; θ) ∂θ t∈S ∑ = p(t|x, S; θ) × f(x, t) (5) t∈S 2.2 =Et|x,S;θ [f(x, t)] Learn from Incomplete Data The key challenge for coupled sequence labeling is that both CTB and PD are non-overlapping and each contains only one-side annotations. Based on the idea of ambiguous labeling, Li et al. (2015) first concatenate a single-side tag with many possible second-side tags, and then use the set of bundled tags as possibly-correct references du"
D16-1072,P14-1125,0,0.062006,"wi as t. Traditional single-side tagging models can only exploit a single set of separate features fsep_a (.) or fsep_b (.). In contrast, the coupled model makes use of all three sets of features. Li et al. (2015) demonstrate that the joint features fjoint (.) capture the implicit mappings between heterogeneous annotations, and the separate features function as back-off features for alleviating the data sparseness problem of the joint features. For the feature templates, we follow Li et al. (2015) and adopt those described in Zhang and Clark (2008) for POS tagging, and use those described in Zhang et al. (2014b) for joint WS&POS tagging. derivations are as follows: ∂log Z(x, S; θ) ∂θ ∑ ∂ t∈S eScore(x,t;θ) 1 = × Z(x, S; θ) ∂θ ( ) ∑ eScore(x,t;θ) ∂Score(x, t; θ) = × Z(x, S; θ) ∂θ t∈S ∑ = p(t|x, S; θ) × f(x, t) (5) t∈S 2.2 =Et|x,S;θ [f(x, t)] Learn from Incomplete Data The key challenge for coupled sequence labeling is that both CTB and PD are non-overlapping and each contains only one-side annotations. Based on the idea of ambiguous labeling, Li et al. (2015) first concatenate a single-side tag with many possible second-side tags, and then use the set of bundled tags as possibly-correct references du"
D17-1072,D15-1141,0,0.0233762,"1 gives an example sentence segmented in different guidelines. Meanwhile, WS approaches gradually evolve from maximum matching based on lexicon dictionaries (Liu and Liang, 1986), to path searching from segmentation graphs based on language modeling scores and other statistics (Zhang and Liu, 2002), to character-based sequence labeling (Xue, 2003), to shift-reduce incremental parsing (Zhang and Clark, 2007). Recently, neural network models have also achieved success by effectively learning representation of characters and contexts (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015; Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016). This work proposes and addresses multi-grained WS (MWS). First, we build a large-scale pseudo MWS dataset for model training and tuning by leveraging the annotation heterogeneity of three SWS datasets. Then we manually annotate 1,500 test sentences with true MWS annotations. Finally, we propose three benchmark approaches by casting MWS as constituent parsing and sequence labeling. Experiments and analysis lead to many interesting findings. 1 Introduction To date, all the labeled datasets adopt the single-granularity formalization, an"
D17-1072,D13-1061,0,0.0243576,"3), and Penn Chinese Treebank (CTB) (Xue et al., 2005). Table 1 gives an example sentence segmented in different guidelines. Meanwhile, WS approaches gradually evolve from maximum matching based on lexicon dictionaries (Liu and Liang, 1986), to path searching from segmentation graphs based on language modeling scores and other statistics (Zhang and Liu, 2002), to character-based sequence labeling (Xue, 2003), to shift-reduce incremental parsing (Zhang and Clark, 2007). Recently, neural network models have also achieved success by effectively learning representation of characters and contexts (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015; Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016). This work proposes and addresses multi-grained WS (MWS). First, we build a large-scale pseudo MWS dataset for model training and tuning by leveraging the annotation heterogeneity of three SWS datasets. Then we manually annotate 1,500 test sentences with true MWS annotations. Finally, we propose three benchmark approaches by casting MWS as constituent parsing and sequence labeling. Experiments and analysis lead to many interesting findings. 1 Introduction To date, all the lab"
D17-1149,P17-2021,0,0.00734708,"ts of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc´ıa-Mart´ınez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model. Li et al. (2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016)"
D17-1149,D16-1162,0,0.0486271,"ropose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpo"
D17-1149,J93-2003,0,0.107649,"e, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese→English translation show that the proposed model achieves significant improvements over the baseline on various test sets. 1 Introduction Neural machine translation (NMT) has been receiving increasing attention due to its impressive ∗ Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used. Specifically, the encoder encodes a source sentence into a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine tra"
D17-1149,P05-1033,0,0.0429496,"the vocabulary as the general NMT decoder does. Experiment results on the Chinese→English translation show that the proposed model achieves significant improvements over the baseline on various test sets. 1 Introduction Neural machine translation (NMT) has been receiving increasing attention due to its impressive ∗ Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used. Specifically, the encoder encodes a source sentence into a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavi"
D17-1149,P16-1160,0,0.0905162,"o a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005) as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts. Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units. In contrast, phrases are much better than words as translation units in SMT and have made a significant advance in translation quality. Therefore, a natural question arises: Can we translate phrases in NMT? Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016). However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the wordby-word generation framework"
D17-1149,P16-2058,0,0.0325284,"Missing"
D17-1149,P15-1002,0,0.0266067,"rporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT."
D17-1149,P16-1078,0,0.0231179,"istic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc´ıa-Mart´ınez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model. Li et al. (2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of appro"
D17-1149,D16-1249,0,0.0212281,"ich dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system’s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016). 6 Conclusion In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the encoder-decoder architecture. At decoding, the SMT model dynamically generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory. Then the proposed model reads the phrase memory and uses the balancer to make probability estimations for the phrases in the phrase memory. Finally the NMT decoder selects a phrase from the phrase memory or"
D17-1149,P16-5005,0,0.0185392,"Missing"
D17-1149,C16-1172,0,0.0273232,"Missing"
D17-1149,P16-1154,0,0.0169774,"e log-likelihood: C(θ) = Ty N X X n=1 i=1 n log P (yin |y&lt;i , xn ) (5) given the training data with N bilingual sentences (Cho, 2015). In the testing phase, given a source sentence x, we use beam search strategy to search a target senˆ that approximately maximizes the conditence y tional probability P (y|x) ˆ = argmax P (y|x) y y 3 (6) Approach In this section, we introduce the proposed model which incorporates a phrase memory into the encoder-decoder architecture of NMT. Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT. 1422 The balancing weight λ is produced by the balancer – a multi-layer network. The balancer network takes as input the decoding information, including the context vector ci , the previous decoding state si−1 and the previous generated word yi−1 : λi = σ(fb (si , yi−1 , ci )) (8) where σ(·) is a sigmoid function and fb (·) is the activation function. Intuitively, the weight λ can be treated as the estimated importance of the phrase to be generated. We expect λ to be high if the phra"
D17-1149,P16-1014,0,0.00748876,"model by maximizing the log-likelihood: C(θ) = Ty N X X n=1 i=1 n log P (yin |y&lt;i , xn ) (5) given the training data with N bilingual sentences (Cho, 2015). In the testing phase, given a source sentence x, we use beam search strategy to search a target senˆ that approximately maximizes the conditence y tional probability P (y|x) ˆ = argmax P (y|x) y y 3 (6) Approach In this section, we introduce the proposed model which incorporates a phrase memory into the encoder-decoder architecture of NMT. Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT. 1422 The balancing weight λ is produced by the balancer – a multi-layer network. The balancer network takes as input the decoding information, including the context vector ci , the previous decoding state si−1 and the previous generated word yi−1 : λi = σ(fb (si , yi−1 , ci )) (8) where σ(·) is a sigmoid function and fb (·) is the activation function. Intuitively, the weight λ can be treated as the estimated importance of the phrase to be generated. We expect λ to be"
D17-1149,P15-1001,0,0.0571924,"ral sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT po"
D17-1149,D13-1176,0,0.164447,"Missing"
D17-1149,P03-1021,0,0.0613915,"n source and target words, which is derived from attention distribution produced by the NMT model (Wang et al., 2017). SMT coverage vector in (Wang et al., 2017) is also introduced to avoid repeat phrasal recommendations. In our work, the potential phrase is phrase with high SMT score which is defined as following: SM Tscore (pl |y&lt;t , x) = M X m=1 wm hm (pl , x(pl )) (10) where pl is a target phrase and x(pl ) is its corresponding source span. hm (pl , x(pl )) is a SMT feature function and wm is its weight. The feature weights can be tuned by the minimum error rate training (MERT) algorithm (Och, 2003). This leads to a better interaction between SMT and NMT models. It should be emphasized that our memory is dynamically updated at each decoding step based on the decoding history from both SMT and NMT models. The proposed model is very flexible, where the phrase memory can be either fully dynamically generated by an SMT model or directly extracted from a bilingual dictionary, or any other bilingual resources storing idiomatic translations or bilingual multi-word expressions, which may lead to a further improvement. 2 Reading Phrase Memory When phrases are read from the memory, they are rescor"
D17-1149,W16-2209,0,0.161524,"d model with its selection preference for special target phrases. With these information, we enrich the context vector ci to enable the proposed model to make better decisions, as described below. Following the commonly-used strategy in sequence tagging tasks (Xue and Shen, 2003), we allow the words in a phrase to share the same chunk tag and introduce a special tag for the beginning word. For example, the phrase “ &E S (information security)” is tagged as a noun phrase “NP”, and the tag sequence should be “NP B NP”. Partially motivated by the work on integrating linguistic features into NMT (Sennrich and Haddow, 2016), we represent the encoder input as the combination of word embeddings and chunking tag embeddings, instead of word embeddings alone in the conventional NMT. The new input is formulated as follows: [E w xi , E t ti ] 1 (9) Overlapped phrases may result in a high dimensionality in translation hypothesis representation and make it hard to employ shared fragments for efficient dynamic programming. 1423 NMT |is a word embedding where E w ∈ Rdw×|V matrix and dw is the word embedding dimensionT AG | ality, E t ∈ Rdt×|V is a tag embedding matrix and dt is the tag embedding dimensionality. [·] is the"
D17-1149,P16-1162,0,0.0342203,"ory that stores phrase pairs in symbolic forms for NMT. During decoding, the NMT decoder enquires the phrase memory and properly generates phrase translations. The significant differences between these efforts and ours are 1) that we dynamically generate phrase translations via an SMT model, and 2) that at the same time we modify the encoder to incorporate structural information to enhance the capability of NMT in phrase translation. Incorporating linguistic information into NMT NMT is essentially a sequence to sequence mapping network that treats the input/output units, eg., words, subwords (Sennrich et al., 2016), characters (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016), as non-linguistic symbols. However, linguistic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc´ıa-Mart´ınez et al. (2016) propo"
D17-1149,E17-2058,0,0.0163956,"Missing"
D17-1149,N03-1017,0,0.0969388,"enerates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese→English translation show that the proposed model achieves significant improvements over the baseline on various test sets. 1 Introduction Neural machine translation (NMT) has been receiving increasing attention due to its impressive ∗ Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used. Specifically, the encoder encodes a source sentence into a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al."
D17-1149,P16-2049,0,0.093448,"phrase cannot be always deducible from the meanings of its individual words or parts. Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units. In contrast, phrases are much better than words as translation units in SMT and have made a significant advance in translation quality. Therefore, a natural question arises: Can we translate phrases in NMT? Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016). However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the wordby-word generation framework. To explore the phrase generation in NMT beyond the word-byword generation framework, we propose a novel architecture that integrates a phrase-based SMT 1421 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1421–1431 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics model into NMT. Specifically, we add an auxiliary phrase memory to store target phrases i"
D17-1149,P17-1064,1,0.855535,"this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc´ıa-Mart´ınez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model. Li et al. (2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NM"
D17-1149,C16-1291,0,0.0145267,"ssigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system’s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016). 6 Conclusion In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the encoder-decoder architecture. At decoding, the SMT model dynamically generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory. Then the proposed model reads the phrase memory and uses the balancer to make probability estimations for the phrases in the phrase memory. Finally the NMT decoder selects a phrase from the phrase memory or a word from the voc"
D17-1149,P16-1100,0,0.024674,"uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005) as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts. Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units. In contrast, phrases are much better than words as translation units in SMT and have made a significant advance in translation quality. Therefore, a natural question arises: Can we translate phrases in NMT? Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016). However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the wordby-word generation framework. To explore the phrase generation in NMT beyond the word-by"
D17-1149,Q17-1007,1,0.899484,"Missing"
D17-1149,P16-1008,1,0.871968,"Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations"
D17-1149,1983.tc-1.13,0,0.583759,"Missing"
D17-1149,P16-1007,0,0.0372004,"the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system’s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016). 6 Conclusion In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the encoder-decoder architecture. At decoding, the SMT model dynamically generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory. Then the proposed model reads the phrase memory and"
D17-1149,W03-1728,0,0.0345713,"c programming, we restrict ourselves to non-overlap phrases.1 (2) We explicitly utilize the boundary information of the source-side chunk phrases, to better guide the proposed model to adopt a target phrase at an appropriate decoding step. (3) We enable the model to exploit the syntactic categories of chunk phrases to enhance the proposed model with its selection preference for special target phrases. With these information, we enrich the context vector ci to enable the proposed model to make better decisions, as described below. Following the commonly-used strategy in sequence tagging tasks (Xue and Shen, 2003), we allow the words in a phrase to share the same chunk tag and introduce a special tag for the beginning word. For example, the phrase “ &E S (information security)” is tagged as a noun phrase “NP”, and the tag sequence should be “NP B NP”. Partially motivated by the work on integrating linguistic features into NMT (Sennrich and Haddow, 2016), we represent the encoder input as the combination of word embeddings and chunking tag embeddings, instead of word embeddings alone in the conventional NMT. The new input is formulated as follows: [E w xi , E t ti ] 1 (9) Overlapped phrases may result i"
D17-1149,C16-1170,0,0.0367497,"Missing"
D17-1149,P17-2060,0,0.0224158,"2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system’s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016). 6 Conclusion In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the en"
D17-1149,P15-4025,0,0.0387348,"Missing"
D17-1149,P07-2045,0,\N,Missing
D18-1049,N18-1118,0,0.216407,"ce and Technology # Sogou Inc., Beijing, China § Soochow University, Suzhou, China Abstract pability to minimize the path length between longdistance dependencies in neural networks contributes to its exceptional performance. However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores documentlevel context. Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored to model document-level context for the"
D18-1049,D11-1084,1,0.945014,"ology, Tsinghua University, Beijing, China ‡ Beijing National Research Center for Information Science and Technology # Sogou Inc., Beijing, China § Soochow University, Suzhou, China Abstract pability to minimize the path length between longdistance dependencies in neural networks contributes to its exceptional performance. However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores documentlevel context. Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the"
D18-1049,D12-1108,0,0.535179,"versity, Beijing, China ‡ Beijing National Research Center for Information Science and Technology # Sogou Inc., Beijing, China § Soochow University, Suzhou, China Abstract pability to minimize the path length between longdistance dependencies in neural networks contributes to its exceptional performance. However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores documentlevel context. Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, o"
D18-1049,P15-1001,0,0.0438623,"h et al., 2016) with 32K merges to segment words into sub-word units for all languages. For the original Transformer model and our extended model, the hidden size is set to 512 and the Unfortunately, large-scale document-level parallel corpora are usually unavailable, even for resource-rich languages such as English and Chinese. Under small-data training conditions, document-level NMT is prone to underperform sentence-level NMT because of poor estimates of low-frequency events. To address this problem, we adopt the idea of freezing some parameters while tuning the remaining part of the model (Jean et al., 2015; Zoph et al., 2016). We propose a two-step training strategy that uses an additional sentence-level parallel corpus Ds , which can be larger than Dd . We divide model parameters into two subsets: θ = θs ∪ θd , where θs is a set of original sentencelevel model parameters (highlighted in blue in Figure 1(b)) and θd is a set of newly-introduced document-level model parameters (highlighted in red in Figure 1(b)). In the first step, sentence-level parameters θs are estimated on the combined sentence-level parallel corpus Ds ∪ Dd : 2 X θˆs = argmax log P (y|x; θs ). (24) hx,yi∈Ds ∪Dd Note that the"
D18-1049,D13-1163,1,0.886079,"tlevel context to the encoder and decoder (see Section 2.3). It is clear that integrating document-level context into the encoder (Eq. 12) brings significant improvements (i.e., 45.97 vs. 47.51). Similarly, it is also beneficial to integrate document-level context into the decoder (Eq. 16). Combining both leads to further improvements. This observation suggests that documentlevel context does help to improve Transformer. 3.9 4 Developing document-level models for machine translation has been an important research direction, both for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012; Xiong et al., 2013a,b; Garcia et al., 2014) and NMT (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018). Most existing work on document-level NMT has focused on integrating document-level context into the RNNsearch model (Bahdanau et al., Effect of Context Gating As shown in Table 9, we also validated the effectiveness of context gating (see Section 2.3.3). We find that replacing residual connections with context gating leads to an overall improvement of 0.38 BLEU point. 3.10 Related Work Anal"
D18-1049,P18-1249,0,0.0320133,"nt-level context often includes several sentences, it is important to capture long-range dependencies and identify relevant information. We use multi-head self-attention (Vaswani et al., 2017) to compute the representation of documentlevel context because it is capable of reducing the maximum path length between long-range dependencies to O(1) (Vaswani et al., 2017) and determining the relative importance of different locations in the context (Bahdanau et al., 2015). Because of this property, multi-head self-attention has proven to be effective in other NLP tasks such as constituency parsing (Kitaev and Klein, 2018). where C(1) ∈ RD×M is the annotation of X&lt;k af(1) ter the first layer, A·,m ∈ RD×1 is the column vector for the m-th contextual word, and FNN(·) is a position-wise fully connected feed-forward network (Vaswani et al., 2017). This process iterates Nc times as follows:   A(n) = MultiHead C(n−1) , C(n−1) , C(n−1) , (8) h i (n) (n) C(n) = FNN(A·,1 ); . . . ; FNN(A·,M ) , (9) 535 (k) where A(n) and C(n) (n = 1, . . . , Nc ) are the hidden state and annotation at the n-th layer, respectively. Note that C(0) = Xc . 2.3 where y0 ∈ RD×1 is the vector representation of a begin-of-sentence token and Y"
D18-1049,P17-4012,0,0.171804,"alculated on the development set. filter size is set to 2,048. The multi-head attention has 8 individual attention heads. We set N = Ns = Nt = 6. In training, we use Adam (Kingma and Ba, 2015) for optimization. Each mini-batch contains approximately 24K words. We use the learning rate decay policy described by Vaswani et al. (2017). In decoding, the beam size is set to 4. We use the length penalty (Wu et al., 2016) and set the hyper-parameter α to 0.6. We use four Tesla P40 GPUs for training and one Tesla P40 GPU for decoding. We implement our approach on top of the open-source toolkit THUMT (Zhang et al., 2017). 4 3.2 2. (Kuang et al., 2017): using a cache which stores previous translated words and topical words to incorporate document-level context into the RNNsearch model. They use a document-level parallel corpus containing 2.8M sentence pairs. Table 3 gives the BLEU scores reported in their paper. 3. (Vaswani et al., 2017): the state-of-the-art NMT model that does not exploit documentlevel context. We use the open-source toolkit THUMT (Zhang et al., 2017) to train and evaluate the model. The training dataset is our sentence-level parallel corpus containing 2M sentence pairs. Effect of Context Le"
D18-1049,P18-1118,0,0.40399,"al for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored to model document-level context for the Transformer model (Voita et al., 2018). Previous approaches to document-level NMT have concentrated on the RNNsearch model (Bahdanau et al., 2015). It is challenging to adapt these approaches to Transformer because they are designed specifically for RNNsearch. In this work, we propose to extend the Transformer model to take advantage of documentlevel context. The basic idea is to use multihead self-attention (Vaswani et al., 20"
D18-1049,D16-1163,0,0.0330673,"h 32K merges to segment words into sub-word units for all languages. For the original Transformer model and our extended model, the hidden size is set to 512 and the Unfortunately, large-scale document-level parallel corpora are usually unavailable, even for resource-rich languages such as English and Chinese. Under small-data training conditions, document-level NMT is prone to underperform sentence-level NMT because of poor estimates of low-frequency events. To address this problem, we adopt the idea of freezing some parameters while tuning the remaining part of the model (Jean et al., 2015; Zoph et al., 2016). We propose a two-step training strategy that uses an additional sentence-level parallel corpus Ds , which can be larger than Dd . We divide model parameters into two subsets: θ = θs ∪ θd , where θs is a set of original sentencelevel model parameters (highlighted in blue in Figure 1(b)) and θd is a set of newly-introduced document-level model parameters (highlighted in red in Figure 1(b)). In the first step, sentence-level parameters θs are estimated on the combined sentence-level parallel corpus Ds ∪ Dd : 2 X θˆs = argmax log P (y|x; θs ). (24) hx,yi∈Ds ∪Dd Note that the newly introduced mod"
D18-1049,2012.eamt-1.60,0,0.196011,"2M Chinese-English sentence pairs with 54.8M Chinese words and 60.8M English words. 3 The document-level parallel corpus is a subset of the full training set, including 41K documents with 940K sentence pairs. On average, each document in the training set contains 22.9 sentences. We use the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, 2008 datasets as test sets. The development and test sets contain 588 documents with 5,833 sentences. On average, each document contains 9.9 sentences. In French-English translation task, we use the IWSLT bilingual training data (Mauro et al., 2012) which contains 1,824 documents with 220K sentence pairs as training set. For development and testing, we use the IWSLT 2010 development and test sets, which contains 8 documents with 887 sentence pairs and 11 documents with 1,664 sentence pairs respectively. The evaluation metric for both tasks is case-insensitive BLEU score as calculated by the multi-bleu.perl script. In preprocessing, we use byte pair encoding (Sennrich et al., 2016) with 32K merges to segment words into sub-word units for all languages. For the original Transformer model and our extended model, the hidden size is set to 51"
D18-1049,P16-1162,0,0.260437,"88 documents with 5,833 sentences. On average, each document contains 9.9 sentences. In French-English translation task, we use the IWSLT bilingual training data (Mauro et al., 2012) which contains 1,824 documents with 220K sentence pairs as training set. For development and testing, we use the IWSLT 2010 development and test sets, which contains 8 documents with 887 sentence pairs and 11 documents with 1,664 sentence pairs respectively. The evaluation metric for both tasks is case-insensitive BLEU score as calculated by the multi-bleu.perl script. In preprocessing, we use byte pair encoding (Sennrich et al., 2016) with 32K merges to segment words into sub-word units for all languages. For the original Transformer model and our extended model, the hidden size is set to 512 and the Unfortunately, large-scale document-level parallel corpora are usually unavailable, even for resource-rich languages such as English and Chinese. Under small-data training conditions, document-level NMT is prone to underperform sentence-level NMT because of poor estimates of low-frequency events. To address this problem, we adopt the idea of freezing some parameters while tuning the remaining part of the model (Jean et al., 20"
D18-1049,P16-1159,1,0.842461,"ion. To address this problem, we replace the residual connections after the context attention sub-layer with a position-wise context gating sub-layer: this step. P (y|x; θs ) is identical to the original Transformer model, which is a special case of our model. In the second step, document-level parameters θd are estimated on the document-level parallel corpus Dd only: X log P (Y|X; θˆs , θd ). (25) θˆd = argmax Gating(H) = λH + (1 − λ)SubLayer(H). (21) θd The gating weight is given by λ = σ(Wi H + Ws SubLayer(H)), Our approach is also similar to pre-training which has been widely used in NMT (Shen et al., 2016; Tu et al., 2018). The major difference is that our approach keeps θˆs fixed when estimating θd to prevent the model from overfitting on the relatively smaller document-level parallel corpora. (22) where σ(·) is a sigmoid function, Wi and Ws are model parameters. 2.4 Training Given a document-level parallel corpus Dd , the standard training objective is to maximize the loglikelihood of the training data: ( ) X θˆ = argmax log P (Y|X; θ) . (23) θ 3 Setup We evaluate our approach on Chinese-English and French-English translation tasks. In ChineseEnglish translation task, the training set contai"
D18-1049,W17-4811,0,0.179065,"Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored to model document-level context for the Transformer model (Voita et al., 2018). Previous approaches to document-level NMT have concentrated on the RNNsearch model (Bahdanau et al., 2015). It is challenging to adapt these approaches to Transformer because they are designed specifically for RNNsearch. In this work, we propose to extend the Transformer model to take advantage of documentlevel context. The basic idea is to use"
D18-1049,P18-1117,0,0.365642,"onal SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored to model document-level context for the Transformer model (Voita et al., 2018). Previous approaches to document-level NMT have concentrated on the RNNsearch model (Bahdanau et al., 2015). It is challenging to adapt these approaches to Transformer because they are designed specifically for RNNsearch. In this work, we propose to extend the Transformer model to take advantage of documentlevel context. The basic idea is to use multihead self-attention (Vaswani et al., 2017) to compute the representation of document-level context"
D18-1049,D17-1301,0,0.15683,"Research Center for Information Science and Technology # Sogou Inc., Beijing, China § Soochow University, Suzhou, China Abstract pability to minimize the path length between longdistance dependencies in neural networks contributes to its exceptional performance. However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores documentlevel context. Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored"
D18-1079,W10-4310,0,0.0217568,"was released in 2008 (Prasad et al., 2008), there is a significant amount of research has been carried out on discourse-level relation recognition ∗ Corresponding author 725 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherfor"
D18-1079,C18-1048,0,0.0754989,"Missing"
D18-1079,D16-1020,0,0.0169071,"t relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting from the high-accuracy explicit relation recognition, a simple pattern, such as Argument1+because+Argument2, may enable the acquisition of countless explicitly-related arguments from texts. It makes it p"
D18-1079,W12-1614,0,0.0585359,"8 (Prasad et al., 2008), there is a significant amount of research has been carried out on discourse-level relation recognition ∗ Corresponding author 725 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud"
D18-1079,P09-2004,0,0.0594837,"enn Discourse Treebank of version 2.0 (PDTB) was released in 2008 (Prasad et al., 2008), there is a significant amount of research has been carried out on discourse-level relation recognition ∗ Corresponding author 725 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating i"
D18-1079,P16-1163,0,0.150888,"age Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting from the high-accuracy explicit rela"
D18-1079,prasad-etal-2008-penn,0,0.188193,"Missing"
D18-1079,C18-1046,0,0.586217,"Missing"
D18-1079,D16-1246,0,0.54997,"s in Natural Language Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting from the high-acc"
D18-1079,P17-1093,0,0.11412,"es 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting from the high-accuracy explicit relation recognition,"
D18-1079,D17-1134,0,0.443223,"Missing"
D18-1079,E14-1068,0,0.0416032,", there is a significant amount of research has been carried out on discourse-level relation recognition ∗ Corresponding author 725 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al."
D18-1079,D09-1036,0,0.513605,"Missing"
D18-1079,N15-1081,0,0.376031,"a causal relation. We tackle discourse-level relation recognition, a problem of determining semantic relations between text spans. Implicit relation recognition is challenging due to the lack of explicit relational clues. The increasingly popular neural network techniques have been proven effective for semantic encoding, whereby widely employed to boost semantic relation discrimination. However, learning to predict semantic relations at a deep level heavily relies on a great deal of training data, but the scale of the publicly available data in this field is limited. In this paper, we follow Rutherford and Xue (2015) to expand the training data set using the corpus of explicitly-related arguments, by arbitrarily dropping the overtly presented discourse connectives. On the basis, we carry out an experiment of sampling, in which a simple active learning approach is used, so as to take the informative instances for data expansion. The goal is to verify whether the selective use of external data not only reduces the time consumption of retraining but also ensures a better system performance. Using the expanded training data, we retrain a convolutional neural network (CNN) based classifer which is a simplified"
D18-1079,D16-1130,0,0.126022,"els, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting from the high-accuracy explicit relation recognition, a simple pattern, s"
D18-1079,D16-1253,0,0.135559,"Missing"
D18-1079,P17-2042,0,0.0622622,"to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting from the high-accuracy explicit relation recognition, a simple pattern, such as Argument1+because+Argument2, may enable the acquisition of countless explicitly-related arguments from texts. It makes it possible to cooperate with Rutherford"
D18-1079,D15-1266,0,0.135733,"on Empirical Methods in Natural Language Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting"
D18-1079,C08-1143,0,0.0404349,"n Imp Exp Unlabeled Data Adoption Expansion Experts Exp2Imp General AL workflow Cooperating AL with Exp2Imp Table 1: Hyperparameter settings of CNN data set will be taken, only if their informativeness scores are higher than a constant threshold θ: Figure 2: Workflows of ALs U 0 = {xi |Inf (xi ) &gt; θ, ∀xi ∈ U } where, U is the unlabeled data set while U 0 consists of the potentially informative instances. a series of successive iterations, or termination after a fixed number of iterations. 5 5.2 Cooperating AL with Exp2Imp Informativeness Measurement We employ an uncertainty sampling function (Zhu et al., 2008; Settles, 2010; Yang et al., 2015; Ramirez-Loaiza et al., 2017) to measure the informativeness: X Inf (xi ) = Irj (xi ; M ) (2) 5.3 X P (rj |xi ) log P (rj |xi ) CNN based Classification We follow Qin et al. (2016) to use Siamese CNN for argument modeling and relation classification. The 300-dimensional word embeddings and 50dimensional POS embeddings are used to represent the arguments. We also follow Mikolov et al. (2013) to pretrain the word embeddings and initialize the POS by random sampling in [-1,1]. Table 1 shows the hyperparameter settings. The source codes of AL, Exp2Imp and Siamese"
D18-1401,P07-1056,0,0.0552003,"g by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a"
D18-1401,D15-1007,0,0.0135061,"darker. Is the sun cream really effective? Answer 2: No, just depending on my own experience. Figure 1: Two examples of QA text pairs from “customer questions & answers” section in Amazon. Introduction Sentiment analysis, a.k.a. opinion mining, is a task which aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages. It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015). As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With th"
D18-1401,P17-1055,0,0.0244919,"ed by concatenating the forward and backward hidden states. For simplicity, we note contextual representation of SQi as HQi , and contextual representation of SAj as HAj respectively: HQi = [hi,1 , hi,2 , ..., hi,n , ..., hi,Ni ] (1) HAj = [hj,1 , hj,2 , ..., hj,m , ..., hj,Mj ] (2) where D[i,j] ∈ RNi ×Mj denotes the bidirectional matching matrix for the [SQi , SAj ] unit. Each element in D[i,j] is the score that measures how well the word in SQi semantically matches the word in SAj and vice versa. Given the bidirectional matching matrix D[i,j] , we use attention mechanism (Yang et al., 2016; Cui et al., 2017) to mine the sentiment matching information between question and answer from two directions, which could be seen as an Answerto-Question attention and a Question-to-Answer attention as follows. • Answer-to-Question Attention: We employ row-wise operations to compute the attention r weight vector α[i,j] as follows: r &gt; U[i,j] = tanh(Wr · D[i,j] ) (4) r r α[i,j] = softmax(wr&gt; · U[i,j] ) (5) r where α[i,j] ∈ RNi is the Answer-to-Question attention weight vector regarding the importance degrees of all words in Q-sentence SQi , Wr ∈ 0 0 Rd ×Mj and wr ∈ Rd are weight matrices. After computing the An"
D18-1401,P11-2104,0,0.019958,"ional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level. Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis. Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits. Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995). More recently, some studies aim to learn better word embedding of a word rather than its polarity. Tang et al. (2014) developed three neural networks to learn word em3655 Beauty Shoe Electronic Positive 3,676 4,025 3,807 Negative 981 819 1,017 Conflict 318 412 528 Neutral 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporating sentiment polarities o"
D18-1401,P10-1041,0,0.015063,"eral, the research on traditional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level. Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis. Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits. Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995). More recently, some studies aim to learn better word embedding of a word rather than its polarity. Tang et al. (2014) developed three neural networks to learn word em3655 Beauty Shoe Electronic Positive 3,676 4,025 3,807 Negative 981 819 1,017 Conflict 318 412 528 Neutral 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporati"
D18-1401,P11-1013,0,0.0608791,"Missing"
D18-1401,C04-1200,0,0.274096,"is, a.k.a. opinion mining, is a task which aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages. It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015). As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics c"
D18-1401,D15-1180,0,0.0466625,"Missing"
D18-1401,P10-1043,1,0.831516,"Missing"
D18-1401,P15-2005,1,0.840444,"e annotated data in three domains. bedding by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classificatio"
D18-1401,D17-1048,0,0.011926,"et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance. Wang et al."
D18-1401,P14-5010,0,0.00460503,"Missing"
D18-1401,D15-1298,0,0.0617943,"Missing"
D18-1401,W02-1011,0,0.0273117,"[Q-sentence, A-sentence] unit for exploring sentiment information. Finally, the self-matching attention layer in the model can capture the importance of these [Q-sentence, A-sentence] matching vectors obtained from QA bidirectional matching layer, which could effectively refine the evidence for inferring the sentiment polarity of a QA text pair. Experimental results show that the proposed approach significantly outperforms several strong baselines for QA-style sentiment classification. 2 Related Work Sentiment classification has become a hot research field in NLP since the pioneering work by Pang et al. (2002). In general, the research on traditional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level. Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis. Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the searc"
D18-1401,P13-4009,0,0.17484,"Missing"
D18-1401,W06-1652,0,0.0451964,"l 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel"
D18-1401,P15-1098,0,0.01816,"iment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an a"
D18-1401,C14-1053,0,0.0241938,"6). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wachsmuth et al., 2014; Zhou et al., 2015a). Comparing to the traditional reviews, the QA style reviews can be more informative and convincing. More importantly, because answer providers are randomly picked from the users who already purchased the target item, this new form of review can be more reliable and trustful. Regarding QA-style sentiment analysis, one straightforward method is to directly employ an existing sentiment classification approach that works well on traditional reviews, such as RNN (Nguyen and Shirai, 2015) and LSTM (Chen et al., 2016). However, because of the significant differences between QA-s"
D18-1401,D16-1058,0,0.308112,"assification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance. Wang et al. (2018) proposed a hierarchical attention network to explore both word-level and clause-level sentiment information towards a target aspect. Unlike all the prior studies, this paper focuses on a very different kind of text representation, i.e., QA-style text level, for sentiment classificatio"
D18-1401,P15-1102,0,0.0469393,"Missing"
D18-1401,D16-1172,0,0.173147,"ch aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages. It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015). As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wa"
D18-1401,N16-1174,0,0.313624,"f each word is formed by concatenating the forward and backward hidden states. For simplicity, we note contextual representation of SQi as HQi , and contextual representation of SAj as HAj respectively: HQi = [hi,1 , hi,2 , ..., hi,n , ..., hi,Ni ] (1) HAj = [hj,1 , hj,2 , ..., hj,m , ..., hj,Mj ] (2) where D[i,j] ∈ RNi ×Mj denotes the bidirectional matching matrix for the [SQi , SAj ] unit. Each element in D[i,j] is the score that measures how well the word in SQi semantically matches the word in SAj and vice versa. Given the bidirectional matching matrix D[i,j] , we use attention mechanism (Yang et al., 2016; Cui et al., 2017) to mine the sentiment matching information between question and answer from two directions, which could be seen as an Answerto-Question attention and a Question-to-Answer attention as follows. • Answer-to-Question Attention: We employ row-wise operations to compute the attention r weight vector α[i,j] as follows: r &gt; U[i,j] = tanh(Wr · D[i,j] ) (4) r r α[i,j] = softmax(wr&gt; · U[i,j] ) (5) r where α[i,j] ∈ RNi is the Answer-to-Question attention weight vector regarding the importance degrees of all words in Q-sentence SQi , Wr ∈ 0 0 Rd ×Mj and wr ∈ Rd are weight matrices. Aft"
D18-1401,D16-1021,0,0.0277727,"Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance. Wang et al. (2018) proposed a hierarchical attention network to explore both word-level and clause-level sentiment information towards a target aspect. Unlike all the prior studies, this paper focuses on a very different kind of text representation, i.e., QA-style text level, for sentiment classification. To the best of our knowledge, this is the first attempt to perform sentiment classification on this text level. 3 Data Collection and Annotation We collect QA text pairs from “Asking"
D18-1401,P14-1146,0,0.0370401,"their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits. Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995). More recently, some studies aim to learn better word embedding of a word rather than its polarity. Tang et al. (2014) developed three neural networks to learn word em3655 Beauty Shoe Electronic Positive 3,676 4,025 3,807 Negative 981 819 1,017 Conflict 318 412 528 Neutral 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On o"
D18-1401,P15-1042,0,0.129521,"or Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wachsmuth et al., 2014; Zhou et al., 2015a). Comparing to the traditional reviews, the QA style reviews can be more informative and convincing. More importantly, because answer providers are randomly picked from the users who already purchased the target item, this new form of review can be more reliable and trustful. Regarding QA-style sentiment analysis, one straightforward method is to directly employ an existing sentiment classification approach that works well on traditional reviews, such as RNN (Nguyen and Shirai, 2015) and LSTM (Chen et al., 2016). However, because of the significant differences between QA-style and classical"
D19-1301,P00-1041,0,0.612774,"is more focused on the relevant parts for the summary than the conventional attention mechanism, and greatly advances the stateof-the-art performance on the abstractive sentence summarization task. We release the code at https://github.com/travel-go/ Abstractive-Text-Summarization. 1 Introduction Abstractive sentence summarization aims at generating concise and informative summaries based on the core meaning of source sentences. Previous endeavors tackle the problem through either rule-based methods (Dorr et al., 2003) or statistical models trained on relatively small scale training corpora (Banko et al., 2000). Following its successful applications on machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), the sequence-to-sequence framework is also applied on the abstractive sentence summarization task using large-scale sentence summary corpora (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., ∗ Equal contribution. 2016), obtaining better performance compared to the traditional methods. One central component in state-of-the-art sequence to sequence models is the use of attention for building connections between the source sequence and target words, so that a more informed deci"
D19-1301,N16-1012,0,0.447297,"uction Abstractive sentence summarization aims at generating concise and informative summaries based on the core meaning of source sentences. Previous endeavors tackle the problem through either rule-based methods (Dorr et al., 2003) or statistical models trained on relatively small scale training corpora (Banko et al., 2000). Following its successful applications on machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), the sequence-to-sequence framework is also applied on the abstractive sentence summarization task using large-scale sentence summary corpora (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., ∗ Equal contribution. 2016), obtaining better performance compared to the traditional methods. One central component in state-of-the-art sequence to sequence models is the use of attention for building connections between the source sequence and target words, so that a more informed decision can be made for generating a target word by considering the most relevant parts of the source sequence (Bahdanau et al., 2015; Vaswani et al., 2017). For abstractive sentence summarization, such attention mechanisms can be useful for selecting the most salient words for a short summary,"
D19-1301,C08-1018,0,0.0465846,"2000; Knight and Marcu, 2000; Neto et al., 2002), while the latter grasps the core meaning of the source text and re-state it in short text as abstractive summary (Banko et al., 2000; Rush et al., 2015). In this paper, we focus on abstractive summarization, and especially on abstractive sentence summarization. Previous work deals with the abstractive sentence summarization task by using either rule based methods (Dorr et al., 2003), or statistical methods utilizing a source-summary parallel corpus to train a machine translation model (Banko et al., 2000), or a syntax based transduction model (Cohn and Lapata, 2008; Woodsend et al., 2010). In recent years, sequence-to-sequence neural framework becomes predominant on this task by encoding long source texts and decoding into short summaries together with the attention mechanism. RNN is the most commonly adopted and extensively explored architecture (Chopra et al., 2016; Nallapati et al., 2016; Li et al., 2017). A CNN-based architecture is recently employed by Gehring et al. (2017) using ConvS2S, which applies CNN on both encoder and decoder. Later, Wang et al. (2018) build upon ConvS2S with topic words embedding and encoding, and train the system with rei"
D19-1301,W03-0501,0,0.276518,"ty. Experiments on benchmark datasets show that, the proposed contrastive attention mechanism is more focused on the relevant parts for the summary than the conventional attention mechanism, and greatly advances the stateof-the-art performance on the abstractive sentence summarization task. We release the code at https://github.com/travel-go/ Abstractive-Text-Summarization. 1 Introduction Abstractive sentence summarization aims at generating concise and informative summaries based on the core meaning of source sentences. Previous endeavors tackle the problem through either rule-based methods (Dorr et al., 2003) or statistical models trained on relatively small scale training corpora (Banko et al., 2000). Following its successful applications on machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), the sequence-to-sequence framework is also applied on the abstractive sentence summarization task using large-scale sentence summary corpora (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., ∗ Equal contribution. 2016), obtaining better performance compared to the traditional methods. One central component in state-of-the-art sequence to sequence models is the use of attention for b"
D19-1301,N18-1033,0,0.0551411,"on training. Other explorations with respect to the characteristics of the abstractive summarization task include copying mechanism that copies words from source sequences for composing summaries (Gu et al., 2016; Gulcehre et al., 2016; Song et al., 2018b), the selection mechanism that elaborately selects important parts of source sentences (Zhou et al., 2017; Lin et al., 2018), the distraction mechanism that avoids repeated attention on the same area (Chen et al., 2016), and the sequence level training that avoids exposure bias in teacher forcing methods (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018). Such methods are built on conventional attention, and are orthogonal to our proposed contrastive attention mechanism. 3 Approach We use two categories of attention for summary generation. One is the conventional attention that attends to relevant parts of source sentence, the other is the opponent attention that contrarily attends to irrelevant or less relevant parts. Both categories of attention output probability distributions over summary words, which are jointly optimized by encouraging the contribution from the conventional attention and discouraging the contribution from the opponent a"
D19-1301,P16-1154,0,0.154751,"d so that they can be easily discriminated. In comparison, we apply the contrastive attention mechanism for sentence level summarization by contrastively attending to relevant parts and irrelevant or less relevant parts. Furthermore, we propose a novel softmax softmin functionality to train the attention mechanism, which is different to Song et al. (2018a), who use mean squared error loss for attention training. Other explorations with respect to the characteristics of the abstractive summarization task include copying mechanism that copies words from source sequences for composing summaries (Gu et al., 2016; Gulcehre et al., 2016; Song et al., 2018b), the selection mechanism that elaborately selects important parts of source sentences (Zhou et al., 2017; Lin et al., 2018), the distraction mechanism that avoids repeated attention on the same area (Chen et al., 2016), and the sequence level training that avoids exposure bias in teacher forcing methods (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018). Such methods are built on conventional attention, and are orthogonal to our proposed contrastive attention mechanism. 3 Approach We use two categories of attention for summary generation. On"
D19-1301,P16-1014,0,0.0325471,"n be easily discriminated. In comparison, we apply the contrastive attention mechanism for sentence level summarization by contrastively attending to relevant parts and irrelevant or less relevant parts. Furthermore, we propose a novel softmax softmin functionality to train the attention mechanism, which is different to Song et al. (2018a), who use mean squared error loss for attention training. Other explorations with respect to the characteristics of the abstractive summarization task include copying mechanism that copies words from source sequences for composing summaries (Gu et al., 2016; Gulcehre et al., 2016; Song et al., 2018b), the selection mechanism that elaborately selects important parts of source sentences (Zhou et al., 2017; Lin et al., 2018), the distraction mechanism that avoids repeated attention on the same area (Chen et al., 2016), and the sequence level training that avoids exposure bias in teacher forcing methods (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018). Such methods are built on conventional attention, and are orthogonal to our proposed contrastive attention mechanism. 3 Approach We use two categories of attention for summary generation. One is the conventional a"
D19-1301,D15-1229,0,0.398146,"he annotated Gigaword corpus and preprocess it identically to Rush et al. (2015), which results in around 3.8M training samples, 190K validation samples and 1951 test samples for evaluation. The source-summary pairs are formed through pairing the first sentence of each article with its headline. We use DUC2004 as another English data set only for testing in our experiments. It contains 500 documents, each containing four human-generated reference summaries. The length of the summary is capped at 75 bytes. The last data set we used is a large corpus of Chinese short text summarization (LCSTS) (Hu et al., 2015), which is collected from the Chinese microblogging website Sina Weibo. We follow the data split of the original paper, with 2.4M sourcesummary pairs from the first part of the corpus for training, 725 pairs from the last part with high annotation score for testing. 4.2 Experimental Setup We employ Transformer as our basis architecture4 . Six layers are stacked in both the encoder and decoder, and the dimensions of the embedding vectors and all hidden vectors are set 512. The inner layer of the feed-forward sublayer has the dimensionality of 2048. We set eight heads in the multihead attention."
D19-1301,A00-2024,0,0.265608,"ccuracies compared with RNN and CNN alternatives. When equipped with the proposed contrastive attention mechanism, our Transformer model achieves the best reported results on all data. The visualization of attentions shows that through using the contrastive attention mechanism, our attention is more focused on relevant parts than the baseline. We release our code at XXX. 2 Related Work Automatic summarization has been investigated in two main paradigms: the extractive method and the abstractive method. The former extracts important pieces of source document and concatenates them sequentially (Jing and McKeown, 2000; Knight and Marcu, 2000; Neto et al., 2002), while the latter grasps the core meaning of the source text and re-state it in short text as abstractive summary (Banko et al., 2000; Rush et al., 2015). In this paper, we focus on abstractive summarization, and especially on abstractive sentence summarization. Previous work deals with the abstractive sentence summarization task by using either rule based methods (Dorr et al., 2003), or statistical methods utilizing a source-summary parallel corpus to train a machine translation model (Banko et al., 2000), or a syntax based transduction model (Cohn"
D19-1301,D17-1222,0,0.108139,"Missing"
D19-1301,W04-1013,0,0.0328382,"third layer for deriving the opponent attention in the English experiments, and select the second head of the third layer in the Chinese experiments. All dimensions in the contrastive architecture are set 64. The λ in Equation (9) is tuned on the development set in each experiment. During training, we use the Adam optimizer with β1 = 0.9, β2 = 0.98, ε= 10−9 . The initial learning rate is 0.0005. The inverse square root schedule is applied for initial warm up and annealing (Vaswani et al., 2017). During training, we use a dropout rate of 0.3 on all datasets. During evaluation, we employ ROUGE (Lin, 2004) as our evaluation metric. Since standard Rouge package is used to evaluate the English summarization systems, we also follow the method of Hu et al. (2015) to map Chinese words into numerical IDs in order to evaluate the performance on the Chinese data set. 4.3 4.3.1 Results English Results The experimental results on the English evaluation sets are listed in Table 1. We report the full-length F-1 scores of ROUGE-1 (R-1), ROUGE2 (R-2), and ROUGE-L (R-L) on the evaluation set of the annotated Gigaword, while report the recall-based scores of the R-1, R-2, and R-L on the evaluation set of DUC20"
D19-1301,P18-2027,0,0.499364,"relevant parts and irrelevant or less relevant parts. Furthermore, we propose a novel softmax softmin functionality to train the attention mechanism, which is different to Song et al. (2018a), who use mean squared error loss for attention training. Other explorations with respect to the characteristics of the abstractive summarization task include copying mechanism that copies words from source sequences for composing summaries (Gu et al., 2016; Gulcehre et al., 2016; Song et al., 2018b), the selection mechanism that elaborately selects important parts of source sentences (Zhou et al., 2017; Lin et al., 2018), the distraction mechanism that avoids repeated attention on the same area (Chen et al., 2016), and the sequence level training that avoids exposure bias in teacher forcing methods (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018). Such methods are built on conventional attention, and are orthogonal to our proposed contrastive attention mechanism. 3 Approach We use two categories of attention for summary generation. One is the conventional attention that attends to relevant parts of source sentence, the other is the opponent attention that contrarily attends to irrelevant or less rel"
D19-1301,C18-1146,0,0.0906615,"Missing"
D19-1301,D10-1050,0,0.0175144,"2000; Neto et al., 2002), while the latter grasps the core meaning of the source text and re-state it in short text as abstractive summary (Banko et al., 2000; Rush et al., 2015). In this paper, we focus on abstractive summarization, and especially on abstractive sentence summarization. Previous work deals with the abstractive sentence summarization task by using either rule based methods (Dorr et al., 2003), or statistical methods utilizing a source-summary parallel corpus to train a machine translation model (Banko et al., 2000), or a syntax based transduction model (Cohn and Lapata, 2008; Woodsend et al., 2010). In recent years, sequence-to-sequence neural framework becomes predominant on this task by encoding long source texts and decoding into short summaries together with the attention mechanism. RNN is the most commonly adopted and extensively explored architecture (Chopra et al., 2016; Nallapati et al., 2016; Li et al., 2017). A CNN-based architecture is recently employed by Gehring et al. (2017) using ConvS2S, which applies CNN on both encoder and decoder. Later, Wang et al. (2018) build upon ConvS2S with topic words embedding and encoding, and train the system with reinforcement learning. The"
D19-1301,P17-1101,0,0.35429,"tively attending to relevant parts and irrelevant or less relevant parts. Furthermore, we propose a novel softmax softmin functionality to train the attention mechanism, which is different to Song et al. (2018a), who use mean squared error loss for attention training. Other explorations with respect to the characteristics of the abstractive summarization task include copying mechanism that copies words from source sequences for composing summaries (Gu et al., 2016; Gulcehre et al., 2016; Song et al., 2018b), the selection mechanism that elaborately selects important parts of source sentences (Zhou et al., 2017; Lin et al., 2018), the distraction mechanism that avoids repeated attention on the same area (Chen et al., 2016), and the sequence level training that avoids exposure bias in teacher forcing methods (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018). Such methods are built on conventional attention, and are orthogonal to our proposed contrastive attention mechanism. 3 Approach We use two categories of attention for summary generation. One is the conventional attention that attends to relevant parts of source sentence, the other is the opponent attention that contrarily attends to irr"
D19-1301,K16-1028,0,0.158268,"with the abstractive sentence summarization task by using either rule based methods (Dorr et al., 2003), or statistical methods utilizing a source-summary parallel corpus to train a machine translation model (Banko et al., 2000), or a syntax based transduction model (Cohn and Lapata, 2008; Woodsend et al., 2010). In recent years, sequence-to-sequence neural framework becomes predominant on this task by encoding long source texts and decoding into short summaries together with the attention mechanism. RNN is the most commonly adopted and extensively explored architecture (Chopra et al., 2016; Nallapati et al., 2016; Li et al., 2017). A CNN-based architecture is recently employed by Gehring et al. (2017) using ConvS2S, which applies CNN on both encoder and decoder. Later, Wang et al. (2018) build upon ConvS2S with topic words embedding and encoding, and train the system with reinforcement learning. The most related work to our contrastive attention mechanism is in the field of computer vision. Song et al. (2018a) first propose the contrastive attention mechanism for person re-identification. In their work, based on a pre-provided person and background segmentation, the two regions are contrastively atten"
D19-1301,D15-1044,0,0.701741,"arization. 1 Introduction Abstractive sentence summarization aims at generating concise and informative summaries based on the core meaning of source sentences. Previous endeavors tackle the problem through either rule-based methods (Dorr et al., 2003) or statistical models trained on relatively small scale training corpora (Banko et al., 2000). Following its successful applications on machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), the sequence-to-sequence framework is also applied on the abstractive sentence summarization task using large-scale sentence summary corpora (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., ∗ Equal contribution. 2016), obtaining better performance compared to the traditional methods. One central component in state-of-the-art sequence to sequence models is the use of attention for building connections between the source sequence and target words, so that a more informed decision can be made for generating a target word by considering the most relevant parts of the source sequence (Bahdanau et al., 2015; Vaswani et al., 2017). For abstractive sentence summarization, such attention mechanisms can be useful for selecting the most salient words"
D19-1541,W13-3820,0,0.0306716,"rser as external inputs for the basic SRL model. Experiments on the benchmarks of Chinese Proposition Bank 1.0 and CoNLL2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually an"
D19-1541,C10-3009,0,0.053222,"Missing"
D19-1541,C18-1233,0,0.257748,"Missing"
D19-1541,N19-1423,0,0.212989,"itecture of He et al. (2018a) as our basic SRL model with a modification on the argument representation. The architecture of the basic SRL module is shown in the right part of Figure 2, and we will describe it in the following subsections. 2.1 Input Layer Following He et al. (2018a); Li et al. (2019), we employ CNNs to encode Chinese characters for each word wi into its character representation, denoted as repchar . Then, we concatenate repchar i i with the word embedding embword to represent i the word-level features as our basic model input. In addition, we also employ BERT representations (Devlin et al., 2019) to boost the performance of our baseline model, which we denote as repBERT . Formally, the input representation of i wi is: xi = repchar ⊕ embword ⊕ repBERT i i i (2) , where ⊕ is the concatenation operation. Our basic SRL model and BERT-enhanced baseline depend on whether including the BERT representation repBERT or not. i 2.2 Motivated by the recently presented span-based models (He et al., 2018a; Li et al., 2019) for Y BiLSTM Encoder Over the input layer, we employ the BiLSTMs with highway connections (Srivastava et al., 2015; 5383 Argument Representation MLP Scorer X Predicate Representat"
D19-1541,P11-2051,0,0.0287699,"nchmarks of Chinese Proposition Bank 1.0 and CoNLL2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an example of the two forms in"
D19-1541,P18-2058,0,0.193907,"NLP tasks, such as information exCorresponding author. AM-TMP 买 Tom bought A0 买.01 汤姆 obj Figure 1: Example of span-based (blue blocks) and word-based (red blocks) SRL formulations in a sentence, where the top part is its dependency tree. Introduction ∗ subj punc root adjct Intuitively, syntax and semantics are strongly correlative. For example, the semantic A0 and A1 roles are usually the syntactic subject and object, as shown in Figure 1. Inspired by the correlation, researchers try to improve SRL performance by exploring various ways to integrate syntactic knowledge (Roth and Lapata, 2016; He et al., 2018b; Swayamdipta et al., 2018). In contrast, some recent works (He et al., 2017; Tan et al., 2018; Cai et al., 2018) propose deep neural models for SRL without considering any syntactic in5382 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5382–5392, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics formation, achieving promising results. Most recently, He et al. (2018a); Li et al. (2019) extend the span-based models to jointly tackle the p"
D19-1541,P17-1044,0,0.273409,"买.01 汤姆 obj Figure 1: Example of span-based (blue blocks) and word-based (red blocks) SRL formulations in a sentence, where the top part is its dependency tree. Introduction ∗ subj punc root adjct Intuitively, syntax and semantics are strongly correlative. For example, the semantic A0 and A1 roles are usually the syntactic subject and object, as shown in Figure 1. Inspired by the correlation, researchers try to improve SRL performance by exploring various ways to integrate syntactic knowledge (Roth and Lapata, 2016; He et al., 2018b; Swayamdipta et al., 2018). In contrast, some recent works (He et al., 2017; Tan et al., 2018; Cai et al., 2018) propose deep neural models for SRL without considering any syntactic in5382 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5382–5392, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics formation, achieving promising results. Most recently, He et al. (2018a); Li et al. (2019) extend the span-based models to jointly tackle the predicate and argument identification sub-tasks of SRL. Compared with the larg"
D19-1541,P18-1192,0,0.416408,"NLP tasks, such as information exCorresponding author. AM-TMP 买 Tom bought A0 买.01 汤姆 obj Figure 1: Example of span-based (blue blocks) and word-based (red blocks) SRL formulations in a sentence, where the top part is its dependency tree. Introduction ∗ subj punc root adjct Intuitively, syntax and semantics are strongly correlative. For example, the semantic A0 and A1 roles are usually the syntactic subject and object, as shown in Figure 1. Inspired by the correlation, researchers try to improve SRL performance by exploring various ways to integrate syntactic knowledge (Roth and Lapata, 2016; He et al., 2018b; Swayamdipta et al., 2018). In contrast, some recent works (He et al., 2017; Tan et al., 2018; Cai et al., 2018) propose deep neural models for SRL without considering any syntactic in5382 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5382–5392, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics formation, achieving promising results. Most recently, He et al. (2018a); Li et al. (2019) extend the span-based models to jointly tackle the p"
D19-1541,C16-1038,0,0.0207963,"gs. Xia et al. (2017) propose a progressive model to learn and transfer knowledge from heterogeneous SRL data. The above works are all focus on the span-based Chinese SRL, and we compare with their results in Table 2. Different from them, we propose a MTL framework to integrate implicit syntactic representations into a simple unified model on both span-based and wordbased SRL, achieving substantial improvements. In addition to the hard parameter sharing strategy that we discuss in Section 3.2, partial parameter sharing strategy is also a commonly studied approach in MTL and domain adaptation. Kim et al. (2016) introduce simple neural extensions of feature argumentation by employing a global LSTM used across all domains and independent LSTMs used within individual domains. Peng 5389 et al. (2017) explore a multitask learning approach which shares parameters across formalisms for semantic dependency parsing. In addition, Peng et al. (2018) present a multi-task approach for frame-semantic parsing and semantic dependency parsing with latent structured variables. 7 Conclusion This paper proposes a syntax-aware MTL framework to integrate implicit syntactic representations into a simple unified SRL model."
D19-1541,D17-1018,0,0.0257275,"e development of neuralnetwork-based approaches in the NLP community, much attention has been paid to build more powerful neural model without considering any syntactic information. Zhou and Xu (2015) employ deep stacked BiLSTMs and achieve strong performance for span-based English SRL. He et al. (2017) extend their work (Zhou and Xu, 2015) by employing several advanced practices in recent deep learning literature, leading to significant improvements. Tan et al. (2018) present a strong self-attention based model, achieving significant improvements. Inspired by the span-based model proposed by Lee et al. (2017) for coreference resolution, He et al. (2018a); Ouchi et al. (2018) present similar span-based models for SRL which can exploit span-level features. For word-based SRL, Marcheggiani et al. (2017) propose a simple and fast syntax-agnostic model with rich input representations. Cai et al. (2018) present an endto-end model with BiLSTMs and biaffine scorer Apart from the above syntax-free works, researchers also pay much attention on improving the neural-based SRL approaches by introducing syntactic knowledge. Roth and Lapata (2016) introduce the dependency path embeddings to the neural-based mode"
D19-1541,C10-1081,0,0.0887575,"Experiments on the benchmarks of Chinese Proposition Bank 1.0 and CoNLL2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an exampl"
D19-1541,K17-1041,0,0.331544,"nese Proposition Bank 1.0 (CPB1.0) (span-based) (Xue, 2008) and CoNLL-2009 Chinese (word-based) (Hajiˇc et al., 2009). The CPB1.0 dataset follows the same annotation guideline with the English PropBank benchmark (Palmer et al., 2005). Wu and Palmer (2015) present a top model based selection preference approach to improve Chinese SRL. Since the amount of CPB1.0 dataset is small, Xia et al. (2017) exploit heterogeneous SRL data to improve the performance via a progressive learning approach. The CoNLL-2009 benchmark is released by the CoNLL-2009 shared task (Hajiˇc et al., 2009). Previous works (Marcheggiani et al., 2017; He et al., 2018b; Cai et al., 2018) mainly focus on building more powerful models or exploring the usage of external knowledge on this dataset. Inspired by the development of neural models and exploration of syntactic information, this paper proposes a MTL framework to extract syntactic representations as the external input features for the simple unified SRL model. The contributions of our paper are three-folds: 1. We introduce a simple unified model for span-based and word-based Chinese SRL. 2. We propose a MTL framework to extract implicit syntactic representations for SRL model, which si"
D19-1541,D17-1159,0,0.0627452,"2018a); Ouchi et al. (2018) present similar span-based models for SRL which can exploit span-level features. For word-based SRL, Marcheggiani et al. (2017) propose a simple and fast syntax-agnostic model with rich input representations. Cai et al. (2018) present an endto-end model with BiLSTMs and biaffine scorer Apart from the above syntax-free works, researchers also pay much attention on improving the neural-based SRL approaches by introducing syntactic knowledge. Roth and Lapata (2016) introduce the dependency path embeddings to the neural-based model and achieve substantial improvements. Marcheggiani and Titov (2017) employ the graph convolutional neural networks on top of the BiLSTM encoder to encode syntactic information. He et al. (2018b) propose a k-th order argument pruning algorithm based on systematic dependency trees. Strubell et al. (2018) propose a self-attention based neural MTL model which incorporate dependency parsing as a auxiliary task for SRL. Swayamdipta et al. (2018) propose a MTL framework using hard parameter strategy to incorporate constituent parsing loss into semantic tasks, i.e. SRL and coreference resolution, which outperforms their baseline by +0.8 F1 score. Xia et al. (2019) in"
D19-1541,D09-1153,0,0.17987,"fectively improve the performance (p &lt; 0.0001), no matter whether employ the BERT representations or not. Especially, our proposed framework (IIR) consistently outperforms the hard parameter sharing strategy. So we only report the results of our proposed framework in later experiments. Our final results outperforms the best previous model (Xia et al., 2017) by 7.87 and 4.24 F1 scores with BERT representations or not, respectively. Table 3 shows the results of our framework in the end-to-end setting. To our best knowledge, we are the first to present the results of end-toMethods Previous Works Sun et al. (2009) Wang et al. (2015b) Sha et al. (2016) Xia et al. (2017) Ours Baseline Baseline + Dep (HPS) Baseline + Dep (IIR) Baseline + BERT Baseline + BERT + Dep (HPS) Baseline + BERT + Dep (IIR) F1 74.12 77.59 77.69 79.67 80.48 83.51 83.91 86.62 87.03 87.54 Table 2: Results and comparison with previous works on CPB1.0 test set. end on the CPB1.0 dataset. We achieve the result of 85.57 in F1 score, which is a strong baseline for later works. It is clear that our framework can still achieve better results compared with the strong baseline, which employs BERT representations as the external input. Results"
D19-1541,W04-2705,0,0.0823318,"ucted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an example of the two forms in a sentence, where “bought” is the given predicate. Semantic role labeling (SRL) is a fundamental and important task in natural language processing (NLP), which aims to identify the semantic structure (Who did what to whom, when and where, etc.) of each given predicate in a sentence. Semantic knowledge has been widely exploited in many down-stream NLP tasks, such"
D19-1541,W08-2121,0,0.176317,"Missing"
D19-1541,D18-1191,0,0.0754045,"ity, much attention has been paid to build more powerful neural model without considering any syntactic information. Zhou and Xu (2015) employ deep stacked BiLSTMs and achieve strong performance for span-based English SRL. He et al. (2017) extend their work (Zhou and Xu, 2015) by employing several advanced practices in recent deep learning literature, leading to significant improvements. Tan et al. (2018) present a strong self-attention based model, achieving significant improvements. Inspired by the span-based model proposed by Lee et al. (2017) for coreference resolution, He et al. (2018a); Ouchi et al. (2018) present similar span-based models for SRL which can exploit span-level features. For word-based SRL, Marcheggiani et al. (2017) propose a simple and fast syntax-agnostic model with rich input representations. Cai et al. (2018) present an endto-end model with BiLSTMs and biaffine scorer Apart from the above syntax-free works, researchers also pay much attention on improving the neural-based SRL approaches by introducing syntactic knowledge. Roth and Lapata (2016) introduce the dependency path embeddings to the neural-based model and achieve substantial improvements. Marcheggiani and Titov (201"
D19-1541,J05-1004,0,0.6859,"tively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an example of the two forms in a sentence, where “bought” is the given predicate. Semantic role labeling (SRL) is a fundamental and important task in natural language processing (NLP), which aims to identify the semantic structure (Who did what to whom, when and where, etc.) of each given predicate in a sentence. Semantic knowledge has been widely exploited in"
D19-1541,P17-1186,0,0.0863687,"Missing"
D19-1541,N18-1135,0,0.0174564,"ed model on both span-based and wordbased SRL, achieving substantial improvements. In addition to the hard parameter sharing strategy that we discuss in Section 3.2, partial parameter sharing strategy is also a commonly studied approach in MTL and domain adaptation. Kim et al. (2016) introduce simple neural extensions of feature argumentation by employing a global LSTM used across all domains and independent LSTMs used within individual domains. Peng 5389 et al. (2017) explore a multitask learning approach which shares parameters across formalisms for semantic dependency parsing. In addition, Peng et al. (2018) present a multi-task approach for frame-semantic parsing and semantic dependency parsing with latent structured variables. 7 Conclusion This paper proposes a syntax-aware MTL framework to integrate implicit syntactic representations into a simple unified SRL model. The experimental results show that our proposed framework can effectively improve the basic SRL model, even when the basic model is enhanced with BERT representations. Especially, our proposed framework is more effective at utilizing syntactic information, compared with the hard parameter sharing strategy of MTL. By utilizing BERT"
D19-1541,P16-1113,0,0.558864,"ed in many down-stream NLP tasks, such as information exCorresponding author. AM-TMP 买 Tom bought A0 买.01 汤姆 obj Figure 1: Example of span-based (blue blocks) and word-based (red blocks) SRL formulations in a sentence, where the top part is its dependency tree. Introduction ∗ subj punc root adjct Intuitively, syntax and semantics are strongly correlative. For example, the semantic A0 and A1 roles are usually the syntactic subject and object, as shown in Figure 1. Inspired by the correlation, researchers try to improve SRL performance by exploring various ways to integrate syntactic knowledge (Roth and Lapata, 2016; He et al., 2018b; Swayamdipta et al., 2018). In contrast, some recent works (He et al., 2017; Tan et al., 2018; Cai et al., 2018) propose deep neural models for SRL without considering any syntactic in5382 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5382–5392, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics formation, achieving promising results. Most recently, He et al. (2018a); Li et al. (2019) extend the span-based models to joi"
D19-1541,P06-2104,0,0.102644,"Missing"
D19-1541,D18-1412,0,0.0798863,"information exCorresponding author. AM-TMP 买 Tom bought A0 买.01 汤姆 obj Figure 1: Example of span-based (blue blocks) and word-based (red blocks) SRL formulations in a sentence, where the top part is its dependency tree. Introduction ∗ subj punc root adjct Intuitively, syntax and semantics are strongly correlative. For example, the semantic A0 and A1 roles are usually the syntactic subject and object, as shown in Figure 1. Inspired by the correlation, researchers try to improve SRL performance by exploring various ways to integrate syntactic knowledge (Roth and Lapata, 2016; He et al., 2018b; Swayamdipta et al., 2018). In contrast, some recent works (He et al., 2017; Tan et al., 2018; Cai et al., 2018) propose deep neural models for SRL without considering any syntactic in5382 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5382–5392, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics formation, achieving promising results. Most recently, He et al. (2018a); Li et al. (2019) extend the span-based models to jointly tackle the predicate and argument identi"
D19-1541,P15-2115,0,0.408103,"ets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an example of the two forms in a sentence, where “bought” is the given predicate. Semantic role"
D19-1541,D15-1186,0,0.33176,"ets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an example of the two forms in a sentence, where “bought” is the given predicate. Semantic role"
D19-1541,S15-1027,0,0.0220554,"t al. (2018a); Li et al. (2019) extend the span-based models to jointly tackle the predicate and argument identification sub-tasks of SRL. Compared with the large amount of research for English SRL, Chinese SRL works are rare, mainly because of the limited amount of data and lack of attention of Chinese researchers. For Chinese, the commonly used datasets are Chinese Proposition Bank 1.0 (CPB1.0) (span-based) (Xue, 2008) and CoNLL-2009 Chinese (word-based) (Hajiˇc et al., 2009). The CPB1.0 dataset follows the same annotation guideline with the English PropBank benchmark (Palmer et al., 2005). Wu and Palmer (2015) present a top model based selection preference approach to improve Chinese SRL. Since the amount of CPB1.0 dataset is small, Xia et al. (2017) exploit heterogeneous SRL data to improve the performance via a progressive learning approach. The CoNLL-2009 benchmark is released by the CoNLL-2009 shared task (Hajiˇc et al., 2009). Previous works (Marcheggiani et al., 2017; He et al., 2018b; Cai et al., 2018) mainly focus on building more powerful models or exploring the usage of external knowledge on this dataset. Inspired by the development of neural models and exploration of syntactic informatio"
D19-1541,P17-1189,0,0.453027,"d with the large amount of research for English SRL, Chinese SRL works are rare, mainly because of the limited amount of data and lack of attention of Chinese researchers. For Chinese, the commonly used datasets are Chinese Proposition Bank 1.0 (CPB1.0) (span-based) (Xue, 2008) and CoNLL-2009 Chinese (word-based) (Hajiˇc et al., 2009). The CPB1.0 dataset follows the same annotation guideline with the English PropBank benchmark (Palmer et al., 2005). Wu and Palmer (2015) present a top model based selection preference approach to improve Chinese SRL. Since the amount of CPB1.0 dataset is small, Xia et al. (2017) exploit heterogeneous SRL data to improve the performance via a progressive learning approach. The CoNLL-2009 benchmark is released by the CoNLL-2009 shared task (Hajiˇc et al., 2009). Previous works (Marcheggiani et al., 2017; He et al., 2018b; Cai et al., 2018) mainly focus on building more powerful models or exploring the usage of external knowledge on this dataset. Inspired by the development of neural models and exploration of syntactic information, this paper proposes a MTL framework to extract syntactic representations as the external input features for the simple unified SRL model. Th"
D19-1541,D16-1212,0,0.302848,"Missing"
D19-1541,N19-1075,0,0.327315,"on. We adopt the official scripts pro3 https://github.com/google-research/bert# pre-trained-models 4 https://catalog.ldc.upenn.edu/LDC2003T09 vided by CoNLL-20055 and CoNLL-20096 for span-based and word-based SRL evaluation, respectively. We conduct significant tests using the Dan Bikel’s randomized parsing evaluation comparer. 4.2 Syntax-aware Methods To illustrate the effectiveness and advantage of our proposed framework7 (Integration of Implicit Representations, IIR), we conduct several experiments with the recently employed syntax-aware methods on CPB1.0 dataset for comparison: • Tree-GRU Xia et al. (2019) investigate several syntax-aware methods for the English span-based SRL, showing the effectiveness of introducing syntactic knowledge into the SRL task. We only compare with the TreeGRU method, since the other methods are all predicate-specific and hence not fit into our basic SRL model. • FIR Following Yu et al. (2018) and Zhang et al. (2019), we extract the outputs of BiLSTMs as the fixed implicit representations (FIR) from a pre-trained biaffine parser. In detail, we train the biaffine parser with the same training data used in our framework, and employ the combination of development data"
D19-1541,D07-1002,0,0.0552069,"CoNLL2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an example of the two forms in a sentence, where “bought” is the given predic"
D19-1541,J08-2004,0,0.260693,"l Language Processing, pages 5382–5392, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics formation, achieving promising results. Most recently, He et al. (2018a); Li et al. (2019) extend the span-based models to jointly tackle the predicate and argument identification sub-tasks of SRL. Compared with the large amount of research for English SRL, Chinese SRL works are rare, mainly because of the limited amount of data and lack of attention of Chinese researchers. For Chinese, the commonly used datasets are Chinese Proposition Bank 1.0 (CPB1.0) (span-based) (Xue, 2008) and CoNLL-2009 Chinese (word-based) (Hajiˇc et al., 2009). The CPB1.0 dataset follows the same annotation guideline with the English PropBank benchmark (Palmer et al., 2005). Wu and Palmer (2015) present a top model based selection preference approach to improve Chinese SRL. Since the amount of CPB1.0 dataset is small, Xia et al. (2017) exploit heterogeneous SRL data to improve the performance via a progressive learning approach. The CoNLL-2009 benchmark is released by the CoNLL-2009 shared task (Hajiˇc et al., 2009). Previous works (Marcheggiani et al., 2017; He et al., 2018b; Cai et al., 20"
D19-1541,C18-1047,0,0.102196,"Missing"
D19-1541,D18-1548,0,0.104419,"al. (2018) present an endto-end model with BiLSTMs and biaffine scorer Apart from the above syntax-free works, researchers also pay much attention on improving the neural-based SRL approaches by introducing syntactic knowledge. Roth and Lapata (2016) introduce the dependency path embeddings to the neural-based model and achieve substantial improvements. Marcheggiani and Titov (2017) employ the graph convolutional neural networks on top of the BiLSTM encoder to encode syntactic information. He et al. (2018b) propose a k-th order argument pruning algorithm based on systematic dependency trees. Strubell et al. (2018) propose a self-attention based neural MTL model which incorporate dependency parsing as a auxiliary task for SRL. Swayamdipta et al. (2018) propose a MTL framework using hard parameter strategy to incorporate constituent parsing loss into semantic tasks, i.e. SRL and coreference resolution, which outperforms their baseline by +0.8 F1 score. Xia et al. (2019) investigate and compare several syntax-aware methods on span-based SRL, showing the effectiveness of integrating syntactic information. Compared with the large amount of works on English SRL, Chinese SRL works are rare, mainly because of"
D19-1541,N19-1118,1,0.818401,"Missing"
D19-1541,P16-1040,0,0.0132588,"epend on whether including the BERT representation repBERT or not. i 2.2 Motivated by the recently presented span-based models (He et al., 2018a; Li et al., 2019) for Y BiLSTM Encoder Over the input layer, we employ the BiLSTMs with highway connections (Srivastava et al., 2015; 5383 Argument Representation MLP Scorer X Predicate Representation Biaffine Scorer BiLSTMs Input representation 买 bought 买 bought 了 了 一双 a pair of 鞋 shoes Figure 2: The detailed architecture of our proposed framework, where the left part is the dependency parser and the right part is the basic SRL module, respectively. Zhang et al., 2016b) to encode long-range dependencies and obtain rich representations denoted as hi for time stamp i. The highway connections are used to alleviate the gradient vanishing problem when training deep neural networks. ment can compose a semantic relation. 2.3 3 Predicate and Argument Representations We directly employ the output of the top BiLSTM as the predicate representation at each time stamp. For all the candidate arguments, we simplify the representations by employing the mean operation over the BiLSTM outputs within the corresponding argument spans, which achieves similar results compared w"
D19-1541,D09-1004,0,0.213027,"gure 6: Sentence F1 scores comparison on CoNLL2009 Chinese test data, where the x axis presents the F1 scores of Baseline + BERT and y axis shows the F1 scores of Baseline + BERT + Dep (IIR), respectively. ure 6, we can see that most of the scatter points are off the diagonal line, demonstrating strong differences between the two models. Based on this finding, how to better integrate syntactic knowledge and BERT representations becomes an interesting and meaningful question, and we leave it for future work. 6 Related Work Traditional discrete-feature-based SRL works (Swanson and Gordon, 2006; Zhao et al., 2009) mainly make heavy use of syntactic information. Along with the impressive development of neuralnetwork-based approaches in the NLP community, much attention has been paid to build more powerful neural model without considering any syntactic information. Zhou and Xu (2015) employ deep stacked BiLSTMs and achieve strong performance for span-based English SRL. He et al. (2017) extend their work (Zhou and Xu, 2015) by employing several advanced practices in recent deep learning literature, leading to significant improvements. Tan et al. (2018) present a strong self-attention based model, achievin"
D19-1541,P15-1109,0,0.0289841,"nal line, demonstrating strong differences between the two models. Based on this finding, how to better integrate syntactic knowledge and BERT representations becomes an interesting and meaningful question, and we leave it for future work. 6 Related Work Traditional discrete-feature-based SRL works (Swanson and Gordon, 2006; Zhao et al., 2009) mainly make heavy use of syntactic information. Along with the impressive development of neuralnetwork-based approaches in the NLP community, much attention has been paid to build more powerful neural model without considering any syntactic information. Zhou and Xu (2015) employ deep stacked BiLSTMs and achieve strong performance for span-based English SRL. He et al. (2017) extend their work (Zhou and Xu, 2015) by employing several advanced practices in recent deep learning literature, leading to significant improvements. Tan et al. (2018) present a strong self-attention based model, achieving significant improvements. Inspired by the span-based model proposed by Lee et al. (2017) for coreference resolution, He et al. (2018a); Ouchi et al. (2018) present similar span-based models for SRL which can exploit span-level features. For word-based SRL, Marcheggiani e"
D19-1548,W13-2322,0,0.305832,"en two concepts. Experimental results on English AMR benchmark datasets show that our approach significantly outperforms the state of the art with 29.66 and 31.82 BLEU scores on LDC2015E86 and LDC2017T10, respectively. To the best of our knowledge, these are the best results achieved so far by supervised models on the benchmarks. 1 Introduction AMR-to-text generation is a task of automatically generating a natural language sentence from an Abstract Meaning Representation (AMR) graph. Due to the importance of AMR as a widely adopted semantic formalism in representing the meaning of a sentence (Banarescu et al., 2013), AMR has become popular in semantic representation and AMR-to-text generation has been drawing more and more attention in the last decade. As the example in Figure 1(a) shows, nodes, such as he and convict-01, represent semantic concepts ∗ Corresponding Author: Junhui Li. and edges, such as “:ARG1” and “:quant”, refer to semantic relations between the concepts. Since two concepts close in an AMR graph may map into two segments that are distant in the corresponding sentence, AMR-to-text generation is challenging. For example in Figure 1, the neighboring concepts he and convict-01 correspond to"
D19-1548,W05-0909,0,0.102631,"the filter size dw as 128. We use OpenNMT (Klein et al., 2017) as the implementation of the Transformer seq2seq model.2 In parameter setting, we set the number of layers in both the encoder and decoder to 6. For optimization we use Adam with β1 = 0.1 (Kingma and Ba, 2015). The number of heads is set to 8. In addition, we set the embedding and the hidden sizes to 512 and the batch token-size to 4096. Accordingly, the dx and dz in Section 2 are 64. In all experiments, we train the models for 300K steps on a single K40 GPU. For performance evaluation, we use BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), and CHRF++ (Popovi, 2017) as metrics. We report results of single models that are tuned on the development set. We make our code available at https://github.com/Amazing-J/ structural-transformer. 3.2 2 https://github.com/OpenNMT/OpenNMT-py Meteor 33.20 31.60 31.78 28.04 CHRF++ 60.30 58.09 58.43 51.88 Table 2: Ablation results of our baseline system on the LDC2015E86 development set. line system (an improvement from 18.77 to 24.93 in BLEU), revealing the fact that they are two effective ways to address the issue of data sparseness for AMR-to-text generation. Table"
D19-1548,P18-1026,0,0.261134,"hich locate at the different ends of the sentence. To address the above mentioned challenge, recent studies on AMR-to-text generation regard the task as a sequence-to-sequence (seq2seq) learning problem by properly linearizing an AMR graph into a sequence (Konstas et al., 2017). Such an input representation, however, is apt to lose useful structural information due to the removal of reentrant structures for linearization. To better model graph structures, previous studies propose various graph-based seq2seq models to incorporate graphs as an additional input representation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019). Although such graph-to-sequence models can achieve the state-of-the-art results, they focus on modeling one-hop relations only. That is, they only model concept pairs connected directly by an edge (Song et al., 2018; Beck et al., 2018), and as a result, ignore explicit structural information of indirectly connected concepts in AMR graphs, e.g. the relation between concepts he and possible in Figure 1. To make better use of structural information in an AMR graph, we attempt to model arbitrary concept pairs no matter whether directly connected or not. To this end, we"
D19-1548,N19-1223,0,0.679159,"roposed approach achieves higher performance than the baseline. As representative, we use CNN-based method to obtain structural representation. 5463 LDC2015E86 LDC2017T10 BLEU Meteor CHRF++ #P (M) BLEU Meteor CHRF++ Baseline 25.50 33.16 59.88 49.1 27.43 34.62 61.85 feature-based 27.23 34.53 61.55 49.4 30.18 35.83 63.20 avg-based 28.37 35.10 62.29 49.1 29.56 35.24 62.86 Our Approach sum-based 28.69 34.97 62.05 49.1 29.92 35.68 63.04 SA-based 29.66 35.45 63.00 49.3 31.54 36.02 63.84 CNN-based 29.10 35.00 62.10 49.2 31.82 36.38 64.05 Previous works with single models Konstas et al. (2017)∗ 22.00 Cao and Clark (2019)∗ 23.5 26.8 † Song et al. (2018) 23.30 23.3 50.4 Beck et al. (2018)† Damonte and Cohen (2019)† 24.40 23.60 24.54 24.07 Guo et al. (2019)† 25.7 27.6 57.3 Song et al. (2016)‡ 22.44 Previous works with either ensemble models or unlabelled data, or both Konstas et al. (2017)∗ 33.8 † Song et al. (2018) 33.0 Beck et al. (2018)† 27.5 53.5 Guo et al. (2019)† 35.3 System Table 3: Comparison results of our approaches and related studies on the test sets of LDC2015E86 and LDC2017T10. #P indicates the size of parameters in millions. ∗ indicates seq2seq-based systems while † for graph-based models, and ‡ f"
D19-1548,J07-2003,0,0.0231668,"on (SMT) and neural machine translation (NMT). Flanigan et al. (2016) first transform an AMR graph into a tree, then specify a number of tree-to-string transduction rules based on alignments that are used to drive a tree-based SMT model (Graehl and Knight, 2004). Pourdamghani et al. (2016) develop a method that learns to linearize AMR graphs into AMR strings, and then feed them into a phrase-based SMT model (Koehn et al., 2003). Song et al. (2017) use synchronous node replacement grammar (SNRG) to generate text. Different from synchronous context-free grammar in hierarchical phrase-based SMT (Chiang, 2007), SNRG is a grammar over graphs. Moving to neural seq2seq approaches, Konstas et al. (2017) successfully apply seq2seq model together with large-scale unlabeled data for both text-to-AMR parsing and AMR-to-text generation. With special interest in the target side syn5466 tax, Cao and Clark (2019) use seq2seq models to generate target syntactic structure, and then the surface form. To prevent the information loss in linearizing AMR graphs into sequences, (Song et al., 2018; Beck et al., 2018) propose graphto-sequence models to encode graph structure directly. Focusing on reentrancies, Damonte a"
D19-1548,N19-1366,0,0.508017,"different ends of the sentence. To address the above mentioned challenge, recent studies on AMR-to-text generation regard the task as a sequence-to-sequence (seq2seq) learning problem by properly linearizing an AMR graph into a sequence (Konstas et al., 2017). Such an input representation, however, is apt to lose useful structural information due to the removal of reentrant structures for linearization. To better model graph structures, previous studies propose various graph-based seq2seq models to incorporate graphs as an additional input representation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019). Although such graph-to-sequence models can achieve the state-of-the-art results, they focus on modeling one-hop relations only. That is, they only model concept pairs connected directly by an edge (Song et al., 2018; Beck et al., 2018), and as a result, ignore explicit structural information of indirectly connected concepts in AMR graphs, e.g. the relation between concepts he and possible in Figure 1. To make better use of structural information in an AMR graph, we attempt to model arbitrary concept pairs no matter whether directly connected or not. To this end, we extend the encoder in the"
D19-1548,W14-3348,0,0.0301044,"We use OpenNMT (Klein et al., 2017) as the implementation of the Transformer seq2seq model.2 In parameter setting, we set the number of layers in both the encoder and decoder to 6. For optimization we use Adam with β1 = 0.1 (Kingma and Ba, 2015). The number of heads is set to 8. In addition, we set the embedding and the hidden sizes to 512 and the batch token-size to 4096. Accordingly, the dx and dz in Section 2 are 64. In all experiments, we train the models for 300K steps on a single K40 GPU. For performance evaluation, we use BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), and CHRF++ (Popovi, 2017) as metrics. We report results of single models that are tuned on the development set. We make our code available at https://github.com/Amazing-J/ structural-transformer. 3.2 2 https://github.com/OpenNMT/OpenNMT-py Meteor 33.20 31.60 31.78 28.04 CHRF++ 60.30 58.09 58.43 51.88 Table 2: Ablation results of our baseline system on the LDC2015E86 development set. line system (an improvement from 18.77 to 24.93 in BLEU), revealing the fact that they are two effective ways to address the issue of data sparseness for AMR-to-text generation. Table 3 presents the comparison of"
D19-1548,N16-1087,0,0.303256,"pectively. SEQ2 and GRAPH are the outputs of the seq2seq and the graph models in Damonte and Cohen (2019), respectively. tion in AMR graphs, our model tends to use past tense, as provided and did in Example (1) and (2). Similarly, without information concerning singular form and plural form, our model is more likely to use plural nouns, as centers and lawyers in Example (1) and (5). 5 Related Work Most studies in AMR-to-text generation regard it as a translation problem and are motivated by the recent advances in both statistical machine translation (SMT) and neural machine translation (NMT). Flanigan et al. (2016) first transform an AMR graph into a tree, then specify a number of tree-to-string transduction rules based on alignments that are used to drive a tree-based SMT model (Graehl and Knight, 2004). Pourdamghani et al. (2016) develop a method that learns to linearize AMR graphs into AMR strings, and then feed them into a phrase-based SMT model (Koehn et al., 2003). Song et al. (2017) use synchronous node replacement grammar (SNRG) to generate text. Different from synchronous context-free grammar in hierarchical phrase-based SMT (Chiang, 2007), SNRG is a grammar over graphs. Moving to neural seq2se"
D19-1548,P16-1014,0,0.0281535,"l. (2017) to linearize AMR graphs and to obtain simplified AMRs. We remove variables, wiki links and sense tags before linearization. Figure 1(b) shows an example linearization result for the AMR graph in Figure 1(a). Note that the reentrant concept he in Figure 1 (a) maps to two different tokens in the linearized sequence. Vocabulary: Training AMR-to-text generation systems solely on labeled data may suffer from data sparseness. To attack this problem, previous works adopt techniques like anonymization to remove named entities and rare words (Konstas et al., 2017), or apply a copy mechanism (Gulcehre et al., 2016) such that the models can learn to copy rare words from the input sequence. In this paper we instead use two simple yet effective techniques. One is to apply Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split words into smaller, more frequent sub-word units. The other is to use a shared vocabulary for both source and target sides. Experiments in Section 3.2 demonstrate the necessity of the techniques in building a strong baseline. xi W Q 2.2  Modeling Graph Structures in Transformer Input Representation: We also use the depthfirst traversal strategy to linearize AMR graphs and to obtai"
D19-1548,Q19-1019,0,0.653449,"/OpenNMT-py Meteor 33.20 31.60 31.78 28.04 CHRF++ 60.30 58.09 58.43 51.88 Table 2: Ablation results of our baseline system on the LDC2015E86 development set. line system (an improvement from 18.77 to 24.93 in BLEU), revealing the fact that they are two effective ways to address the issue of data sparseness for AMR-to-text generation. Table 3 presents the comparison of our approach and related works on the test sets of LDC2015E86 and LDC2017T10. From the results we can see that the Transformer-based baseline outperforms most of graph-to-sequence models and is comparable with the latest work by Guo et al. (2019). The strong performance of the baseline is attributed to the capability of the Transformer to encode global and implicit structural information in AMR graphs. By comparing the five methods of learning graph structure representations, we have the following observations. • All of them achieve significant improvements over the baseline: the biggest improvements are 4.16 and 4.39 BLEU scores on LDC2015E86 and LDC2017T10, respectively. • Methods using continuous representations (such as SA-based and CNN-based) outperform the methods using discrete representations (such as feature-based). • Compare"
D19-1548,P14-1062,0,0.0146263,"sum of its hidden states: r= k X αi hi (10) i=1 li (8) i=1 Self-Attention-based (SA-based for short) As shown in Figure 2, given the label sequence s = s1 , · · · , sk , we first obtain the sequence e, whose element is the addition of a word embedding and the corresponding position embedding. Then we use the self-attention, as presented in Eq. 1 to obtain its hidden states h, i.e, h = Attention(e), where hi ∈ Rdz . Our aim is to encode a variable length sentence into a dz sized vector. Motivated by (Lin et al., 2017), we achieve this by choosing a linear combination of CNN-based Motivated by (Kalchbrenner et al., 2014), we use convolutional neural network (CNN) to convolute the label sequence l into a vector r, as follow: 5462 conv = Conv1D(kernel size = (m), strides = 1, f ilters = dz , (11) input shape = dz activation =0 relu0 ) r = conv (l) (12) Model Baseline -BPE -Share Vocab. -Both where kernel size m is set to 4 in our experiments. 3 Experimentation 3.1 Experimental Settings For evaluation of our approach, we use the sentences annotated with AMRs from the LDC release LDC2015E86 and LDC2017T10. The two datasets contain 16,833 and 36,521 training AMRs, respectively, and share 1,368 development AMRs and"
D19-1548,W16-6603,0,0.31594,"). Similarly, without information concerning singular form and plural form, our model is more likely to use plural nouns, as centers and lawyers in Example (1) and (5). 5 Related Work Most studies in AMR-to-text generation regard it as a translation problem and are motivated by the recent advances in both statistical machine translation (SMT) and neural machine translation (NMT). Flanigan et al. (2016) first transform an AMR graph into a tree, then specify a number of tree-to-string transduction rules based on alignments that are used to drive a tree-based SMT model (Graehl and Knight, 2004). Pourdamghani et al. (2016) develop a method that learns to linearize AMR graphs into AMR strings, and then feed them into a phrase-based SMT model (Koehn et al., 2003). Song et al. (2017) use synchronous node replacement grammar (SNRG) to generate text. Different from synchronous context-free grammar in hierarchical phrase-based SMT (Chiang, 2007), SNRG is a grammar over graphs. Moving to neural seq2seq approaches, Konstas et al. (2017) successfully apply seq2seq model together with large-scale unlabeled data for both text-to-AMR parsing and AMR-to-text generation. With special interest in the target side syn5466 tax,"
D19-1548,P16-1162,0,0.563838,"igure 1(a). Note that the reentrant concept he in Figure 1 (a) maps to two different tokens in the linearized sequence. Vocabulary: Training AMR-to-text generation systems solely on labeled data may suffer from data sparseness. To attack this problem, previous works adopt techniques like anonymization to remove named entities and rare words (Konstas et al., 2017), or apply a copy mechanism (Gulcehre et al., 2016) such that the models can learn to copy rare words from the input sequence. In this paper we instead use two simple yet effective techniques. One is to apply Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split words into smaller, more frequent sub-word units. The other is to use a shared vocabulary for both source and target sides. Experiments in Section 3.2 demonstrate the necessity of the techniques in building a strong baseline. xi W Q 2.2  Modeling Graph Structures in Transformer Input Representation: We also use the depthfirst traversal strategy to linearize AMR graphs and to obtain simplified AMRs which only consist of concepts. As shown in Figure 1 (c), the input sequence is much shorter than the input sequence in the baseline. Besides, we also obtain a matrix which records the gra"
D19-1548,N18-2074,0,0.160242,"ut sequence in the baseline. Besides, we also obtain a matrix which records the graph structure between every concept pair, which implies their semantic relationship (Section2.3). Vocabulary: To be compatible with sub-words, we extend the original AMR graph, if necessary, to include the structures of sub-words. As sentence01 in Figure 1(a) is segmented into sent@@ ence01, we split the original node into two connected ones with an edge labeled as the incoming edge of the first unit. Figure 1(d) shows the graph structure for sub-words sent@@ ence-01. Structure-Aware Self-Attention: Motivated by Shaw et al. (2018), we extend the conventional self-attention architecture to explicitly encode the relation between an element pair (xi , xj ) in the alignment model by replacing Equation 4 with Equation 5. Note that the relation rij ∈ Rdz is the vector representation for element pair (xi , xj ), and will be learned in Section 2.3.  T xi W Q xj W K + rij W R √ eij = (5) dz where W R ∈ Rdz ×dz is a parameter matrix. Then, we update Equation 2 accordingly to propagate structure information to the sublayer output by: zi = n X αij xj W V + rij W F  (6) j=1 where W F ∈ Rdz ×dz is a parameter matrix. 2.3 Learning"
D19-1548,P17-2002,0,0.218561,"5 Related Work Most studies in AMR-to-text generation regard it as a translation problem and are motivated by the recent advances in both statistical machine translation (SMT) and neural machine translation (NMT). Flanigan et al. (2016) first transform an AMR graph into a tree, then specify a number of tree-to-string transduction rules based on alignments that are used to drive a tree-based SMT model (Graehl and Knight, 2004). Pourdamghani et al. (2016) develop a method that learns to linearize AMR graphs into AMR strings, and then feed them into a phrase-based SMT model (Koehn et al., 2003). Song et al. (2017) use synchronous node replacement grammar (SNRG) to generate text. Different from synchronous context-free grammar in hierarchical phrase-based SMT (Chiang, 2007), SNRG is a grammar over graphs. Moving to neural seq2seq approaches, Konstas et al. (2017) successfully apply seq2seq model together with large-scale unlabeled data for both text-to-AMR parsing and AMR-to-text generation. With special interest in the target side syn5466 tax, Cao and Clark (2019) use seq2seq models to generate target syntactic structure, and then the surface form. To prevent the information loss in linearizing AMR gra"
D19-1548,D16-1224,0,0.0392513,"Missing"
D19-1548,P18-1150,0,0.704359,"he and convicted which locate at the different ends of the sentence. To address the above mentioned challenge, recent studies on AMR-to-text generation regard the task as a sequence-to-sequence (seq2seq) learning problem by properly linearizing an AMR graph into a sequence (Konstas et al., 2017). Such an input representation, however, is apt to lose useful structural information due to the removal of reentrant structures for linearization. To better model graph structures, previous studies propose various graph-based seq2seq models to incorporate graphs as an additional input representation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019). Although such graph-to-sequence models can achieve the state-of-the-art results, they focus on modeling one-hop relations only. That is, they only model concept pairs connected directly by an edge (Song et al., 2018; Beck et al., 2018), and as a result, ignore explicit structural information of indirectly connected concepts in AMR graphs, e.g. the relation between concepts he and possible in Figure 1. To make better use of structural information in an AMR graph, we attempt to model arbitrary concept pairs no matter whether directly connected or no"
D19-1548,P17-4012,0,0.047258,"tated with AMRs from the LDC release LDC2015E86 and LDC2017T10. The two datasets contain 16,833 and 36,521 training AMRs, respectively, and share 1,368 development AMRs and 1,371 testing AMRs. We segment words into sub-word units by BPE (Sennrich et al., 2016) with 10K operations on LDC2015E86 and 20K operations on LDC2017T10. For efficiently learning graph structure representation for concept pairs (except the featurebased method), we limit the maximum label sequence length to 4 and ignore the labels exceeding the maximum. In SA-based method, we set the filter size dw as 128. We use OpenNMT (Klein et al., 2017) as the implementation of the Transformer seq2seq model.2 In parameter setting, we set the number of layers in both the encoder and decoder to 6. For optimization we use Adam with β1 = 0.1 (Kingma and Ba, 2015). The number of heads is set to 8. In addition, we set the embedding and the hidden sizes to 512 and the batch token-size to 4096. Accordingly, the dx and dz in Section 2 are 64. In all experiments, we train the models for 300K steps on a single K40 GPU. For performance evaluation, we use BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), and CHRF"
D19-1548,P15-1150,0,0.168289,"Missing"
D19-1548,W04-3250,0,0.0272772,"7 Guo et al. (2019)† 25.7 27.6 57.3 Song et al. (2016)‡ 22.44 Previous works with either ensemble models or unlabelled data, or both Konstas et al. (2017)∗ 33.8 † Song et al. (2018) 33.0 Beck et al. (2018)† 27.5 53.5 Guo et al. (2019)† 35.3 System Table 3: Comparison results of our approaches and related studies on the test sets of LDC2015E86 and LDC2017T10. #P indicates the size of parameters in millions. ∗ indicates seq2seq-based systems while † for graph-based models, and ‡ for other models. All our proposed systems are significant over the baseline at 0.01, tested by bootstrap resampling (Koehn, 2004). System Baseline Our approach No indirectly connected concept pairs BLEU 27.43 31.82 29.92 Table 4: Performance on the test set of our approach with or without modeling structural information of indirectly connected concept pairs. shows that by modeling structural information of indirectly connected concept pairs, our approach improves the performance on the test set from 29.92 to 31.82 in BLEU scores. It also shows that even without modeling structural information of indirectly connected concept pairs, our approach achieves better performance than the baseline. 4.2 4.1 Effect of Modeling Str"
D19-1548,N03-1017,0,0.015868,"Example (1) and (5). 5 Related Work Most studies in AMR-to-text generation regard it as a translation problem and are motivated by the recent advances in both statistical machine translation (SMT) and neural machine translation (NMT). Flanigan et al. (2016) first transform an AMR graph into a tree, then specify a number of tree-to-string transduction rules based on alignments that are used to drive a tree-based SMT model (Graehl and Knight, 2004). Pourdamghani et al. (2016) develop a method that learns to linearize AMR graphs into AMR strings, and then feed them into a phrase-based SMT model (Koehn et al., 2003). Song et al. (2017) use synchronous node replacement grammar (SNRG) to generate text. Different from synchronous context-free grammar in hierarchical phrase-based SMT (Chiang, 2007), SNRG is a grammar over graphs. Moving to neural seq2seq approaches, Konstas et al. (2017) successfully apply seq2seq model together with large-scale unlabeled data for both text-to-AMR parsing and AMR-to-text generation. With special interest in the target side syn5466 tax, Cao and Clark (2019) use seq2seq models to generate target syntactic structure, and then the surface form. To prevent the information loss in"
D19-1548,N19-1238,0,0.105079,"Missing"
D19-1548,P17-1014,0,0.400799,"uch as “:ARG1” and “:quant”, refer to semantic relations between the concepts. Since two concepts close in an AMR graph may map into two segments that are distant in the corresponding sentence, AMR-to-text generation is challenging. For example in Figure 1, the neighboring concepts he and convict-01 correspond to the words he and convicted which locate at the different ends of the sentence. To address the above mentioned challenge, recent studies on AMR-to-text generation regard the task as a sequence-to-sequence (seq2seq) learning problem by properly linearizing an AMR graph into a sequence (Konstas et al., 2017). Such an input representation, however, is apt to lose useful structural information due to the removal of reentrant structures for linearization. To better model graph structures, previous studies propose various graph-based seq2seq models to incorporate graphs as an additional input representation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019). Although such graph-to-sequence models can achieve the state-of-the-art results, they focus on modeling one-hop relations only. That is, they only model concept pairs connected directly by an edge (Song et al., 2018; Beck et al., 201"
D19-1548,P02-1040,0,0.107013,"mum. In SA-based method, we set the filter size dw as 128. We use OpenNMT (Klein et al., 2017) as the implementation of the Transformer seq2seq model.2 In parameter setting, we set the number of layers in both the encoder and decoder to 6. For optimization we use Adam with β1 = 0.1 (Kingma and Ba, 2015). The number of heads is set to 8. In addition, we set the embedding and the hidden sizes to 512 and the batch token-size to 4096. Accordingly, the dx and dz in Section 2 are 64. In all experiments, we train the models for 300K steps on a single K40 GPU. For performance evaluation, we use BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), and CHRF++ (Popovi, 2017) as metrics. We report results of single models that are tuned on the development set. We make our code available at https://github.com/Amazing-J/ structural-transformer. 3.2 2 https://github.com/OpenNMT/OpenNMT-py Meteor 33.20 31.60 31.78 28.04 CHRF++ 60.30 58.09 58.43 51.88 Table 2: Ablation results of our baseline system on the LDC2015E86 development set. line system (an improvement from 18.77 to 24.93 in BLEU), revealing the fact that they are two effective ways to address the issue of data sparseness"
D19-1548,W17-4770,0,0.120852,"Missing"
D19-1552,P17-1067,0,0.497484,"tal results show the usefulness of personal attributes, and the effectiveness of our proposed NPD approach in capturing such personal attributes with significant gains over the state-of-the-art models. 1 Introduction The advent of social media and its prosperity enable the creation of massive online user-generated content including opinions and product reviews. Analyzing such user-generated contents allows to detect the users’ emotional states, which are useful for various downstream applications. In the literature, there are a large number of works on emotion detection (Roberts et al., 2012; Abdul-Mageed and Ungar, 2017; Gupta et al., 2017), both discrete and neural models have been ∗ corresponding auther used to predict the emotions of posts in social media. For example, Roberts et al. (2012) used a series of binary SVM classifiers to detect the emotion of a post, while Gupta et al. (2017) used sentiment based semantic embedding and a LSTM model to learn the representation of a post for emotion detection. Different from previous researches, which consider each post individually, we think that posts in social media are much correlated by the authors’ backgrounds. Motivated by the principle of homophily (Laza"
D19-1552,D16-1053,0,0.0134642,"ere,1) SVM is a widely used baseline to predict the emotion of a post in social media (Yang et al., 2007).2) Abdul17 is a standard LSTM model which consist of a LSTM layer and a fully connected layer, and it is modified from the model in Abdul-Mageed and Ungar (2017). The LSTM model yields the stateof-the-art performance on emotion detection in recent researches.3)Vaswani17 is an improved LSTM model with a self-attention mechanism. The self-attention mechanism is used to capture the structural information and has been successfully applied in various natural language processing tasks recently (Cheng et al., 2016; Vaswani et al., 2017) From Table 2, we find that all of the neural models outperform SVM significantly. This indicates that neural models are much more effective than discrete models in emotion detection. In addition, our proposed NPD model outperforms both the standard LSTM model (Abdul17) and the improved LSTM model with self-attention (Vaswani17) significantly. This shows the effectiveness of our proposed NPD model with both adversarial discriminators and attention mechanisms. This also shows the usefulness of personal attributes for emotion detection. Moreover, we find that the performan"
D19-1552,P15-1073,0,0.0289832,"ollobert et al., 2011; Goldberg, 2016). However, few works use neural network models for emotion detection. Abdul-Mageed and Ungar (2017) used a gated recurrent neural network model for emotion detection with a largescale dataset. Zhang et al. (2018) used an auxiliary and attention based LSTM to detect emotion on a cross-lingual dataset. Lexicon and social information are very important for emotion detection, and there are many researches focus on this topic. For example, Strapparava and Mihalcea (2008) used WordNetAffect to compute the sentimental score of a post. More recently, In addition, Hovy (2015) used both the age and gender information of the authors to improve the performance of sentiment analysis. Vosoughi et al. (2016) explored the relationship among locations, date time, authors and sentiments. Different from previous works which consider each post individually, we think that the posts in social media can be connected through the authors’ backgrounds and should be better addressed. On the basis, we propose a neural personal discrimination model to determine the personal background attributes from each post through adversarial discriminators, and aggregate the representation of in"
D19-1552,roberts-etal-2012-empatweet,0,0.18827,"r addressed. Experimental results show the usefulness of personal attributes, and the effectiveness of our proposed NPD approach in capturing such personal attributes with significant gains over the state-of-the-art models. 1 Introduction The advent of social media and its prosperity enable the creation of massive online user-generated content including opinions and product reviews. Analyzing such user-generated contents allows to detect the users’ emotional states, which are useful for various downstream applications. In the literature, there are a large number of works on emotion detection (Roberts et al., 2012; Abdul-Mageed and Ungar, 2017; Gupta et al., 2017), both discrete and neural models have been ∗ corresponding auther used to predict the emotions of posts in social media. For example, Roberts et al. (2012) used a series of binary SVM classifiers to detect the emotion of a post, while Gupta et al. (2017) used sentiment based semantic embedding and a LSTM model to learn the representation of a post for emotion detection. Different from previous researches, which consider each post individually, we think that posts in social media are much correlated by the authors’ backgrounds. Motivated by th"
D19-1552,W15-2904,0,0.0729155,"Missing"
D19-1552,C14-1050,1,0.814011,"eat this in such weather!) However, the personal attributes are not easy to obtain in most social media websites. On one hand, most websites may not contain useful personal information. On the other hand, people are normally not willing to attach their personal information in social media. Besides, integrating personal attributes into emotion detection is challenging, since it is hard to capture attributes-aware words, such as “little brother” and “bashi”(comfortable), to connect the posts with similar backgrounds. Although there are some related works on either personal attribute extraction (Wang et al., 2014) or emotion detection with personal attributes (Li et al., 2016), none of them address both challenges at the same time. In this paper, we propose a Neural Personal Discrimination (NPD) model with both adversarial discriminators and attention mechanisms to tackle above challenges. Here, the Adversarial discriminators (Goodfellow et al., 2014) are used to determine the personal attributes, e.g., gender or location, of a post, providing the inherent correlationship between emotions and personal backgrounds, while the Attention mechanisms (Wang et al., 2016) are utilized to aggregate the represen"
D19-1552,C16-1153,1,0.929679,"either personal attribute extraction (Wang et al., 2014) or emotion detection with personal attributes (Li et al., 2016), none of them address both challenges at the same time. In this paper, we propose a Neural Personal Discrimination (NPD) model with both adversarial discriminators and attention mechanisms to tackle above challenges. Here, the Adversarial discriminators (Goodfellow et al., 2014) are used to determine the personal attributes, e.g., gender or location, of a post, providing the inherent correlationship between emotions and personal backgrounds, while the Attention mechanisms (Wang et al., 2016) are utilized to aggregate the representation of informative attributes-aware words into a vector for emotion prediction, providing insights into which words contribute to a personal background. Experimental results show the usefulness of personal attributes in emotion detection, and the effectiveness of our proposed NPD model with both adversarial discriminators and attention mechanisms over the state-of-the-art discrete and neural models. 2 Related Work Earlier works on emotion detection are based on discrete models. For example, Yang et al. (2007) built a support vector machine (SVM) model"
D19-1560,D16-1011,0,0.0517837,"ground-truth sentiment rating for aspect xaspect . δ The model will assign a reward score to each seis a L2 regularization. quence according to the designed scores function, and then estimates b(τ h ) as the average of those 3 Experimentation rewards. Similarly, the policy gradient w.r.t. θl of 3.1 Experimental Settings low-level policy is given by, ∇θl J(θ ) = Eτ l ∼πl [ l ki  Rl ∇θl log π l (ai,j |sli,j ; θl )] j=1 (8) Data. We conduct our experiments on three public datasets on DASC, i.e., TripUser (Li et al., 2018), TripAdvisor (Wang et al., 2010) and BeerAdvocate (McAuley et al., 2012; Lei et al., 2016). In the experiment, we adopt Discourse Segmentation 5585 TripUser Development Test Acc.↑ MSE↓ Acc.↑ MSE↓ SVM 46.35† 1.025† LSTM 53.23 0.787 52.74 0.794 MAMC 55.49† 0.583† HARN 58.15† 0.528† HUARN 60.70† 0.514† C-HAN 58.49 0.602 57.38 0.543 HS-LSTM 59.75 0.566 59.01 0.524 RL-Word-Selection 60.15 0.475 59.55 0.519 RL-Clause-Selection 61.32 0.433 60.54 0.461 HRL 62.97 0.336 62.84 0.351 Approaches TripAdvisor Development Test Acc.↑ MSE↓ Acc.↑ MSE↓ 34.30‡ 1.982‡ 35.26‡ 1.963‡ 43.85‡ 1.525‡ 44.02‡ 1.470‡ 46.21‡ 1.091‡ 46.56‡ 1.083‡ 48.21‡ 0.923‡ 47.61 0.914 47.08 0.955 48.45 0.947 46.84 1.013 48.55"
D19-1560,D15-1167,0,0.042799,".8; λ1 , λ2 and λ3 are 0.25, 0.25 and 0.5 respectively. λ1 , λ2 are 0.6 and 0.4. Additionally, the batch size is set to be 64, regularization weight is set to be 10−5 and the dropout rate is 0.2. Evaluation Metrics. The performance is evaluated using Accuracy (Acc.) and MSE as Yin et al. (2017). Moreover, t-test is used to evaluate the signiﬁcance of the performance difference between two approaches (Yang and Liu, 1999). Baselines. We compare HRL with the following baselines: 1) SVM (Yin et al., 2017). This approach only adopts unigram, bigram as features to train an SVM classiﬁer. 2) LSTM (Tang et al., 2015). This is a neural network approach to document-level sentiment classiﬁcation which employs gated LSTM to learn text representation. 3) MAMC (Yin et al., 2017). This approach employs hierarchical iterative attention to learn aspect-speciﬁc representation. This is a state-of3 http://alt.qcri.org/tools/discourse-parser/ the-art approach to DASC. 4) HARN (Li et al., 2018). This approach adopts hierarchical attention to incorporate overall rating and aspect information so as to learn aspect-speciﬁc representation. This is another state-of-the-art approach to DASC. 5) HUARN (Li et al., 2018). This"
D19-1560,C18-1079,0,0.214585,"t classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - service: # # # # $ (4) ral models have shown to be effective for performance improvement on DASC. Despite the advantages, these complex neural network approaches often offer little transparency w.r.t. their inner working mechanisms and suffer from the lack of interpretability. However, clearly understanding where and how such a model makes such a decision is rather important for developing real-world applications (Liu et al., 2018; Marcus, 2018). As human beings, if asked to evaluate the sentiment rating for a speciﬁc asp"
D19-1560,D16-1021,0,0.142468,"Missing"
D19-1560,D16-1127,0,0.103263,"Missing"
D19-1560,P10-1043,1,0.876096,"oach to DASC over the state-of-the-art baselines. 1 is a little uncomfortable .]Clause3 [I’m often nitpicking for room decoration.]]Clause4 [ Besides, the price is very expensive ] Clause5 [ although the staff service is professional .]]Clause6 Rating of Each Aspect - location: # # # # # (5) - value: # $ $ $ $ (1) Figure 1: An example of a review document, where clauses and words with different colors refer to different aspects. Introduction Document-level Aspect Sentiment Classiﬁcation (DASC) is a ﬁne-grained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - ser"
D19-1560,P08-1036,0,0.0746941,"assiﬁcation (DASC) is a ﬁne-grained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - service: # # # # $ (4) ral models have shown to be effective for performance improvement on DASC. Despite the advantages, these complex neural network approaches often offer little transparency w.r.t. their inner working mechanisms and suffer from the lack of interpretability. However, clearly understanding where and how such a model makes such a decision is rather important for developing real-world applications (Liu et al., 2018; Marcus, 2018). As human beings, if asked to evaluate th"
D19-1560,D16-1058,0,0.252027,"Missing"
D19-1560,D17-1217,0,0.61734,"e-grained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - service: # # # # $ (4) ral models have shown to be effective for performance improvement on DASC. Despite the advantages, these complex neural network approaches often offer little transparency w.r.t. their inner working mechanisms and suffer from the lack of interpretability. However, clearly understanding where and how such a model makes such a decision is rather important for developing real-world applications (Liu et al., 2018; Marcus, 2018). As human beings, if asked to evaluate the sentiment rating"
D19-1560,C18-1074,0,0.0316687,"ect sentiment-relevant words and discard those irrelevant and noisy words. For instance, for aspect location, words “this”, “is” in Clause1 are noisy words and should be discarded since they make no contribution to implying the sentiment rating. One possible way to alleviate this problem is to also leverage the soft-attention mechanism as proposed in Li et al. (2018). However, this soft-attention mechanism may induce additional noise and lack interpretability because it tends to assign higher weights to some domain-speciﬁc words rather than real sentiment-relevant words (Mudinas et al., 2012; Zou et al., 2018). For instance, this soft-attention mechanism tends to regard the name of a hotel “Hilton” with a good reputation in Clause3 as a positive word which could mislead the model into assigning a higher rating to aspect room. Therefore, a well-behaved approach should highlight sentiment-relevant words and discard noisy words for a speciﬁc aspect during model training. In this paper, we propose a Hierarchical Reinforcement Learning (HRL) approach with a highlevel policy and a low-level policy to address the above two challenges in DASC. First, a highlevel policy is leveraged to select aspect-relevan"
E09-1096,W04-3216,0,0.0547428,"Missing"
E09-1096,P06-1144,0,0.137233,"Missing"
E09-1096,P06-1103,0,0.0559973,"counts the number of word translation found between the two terms, as described in the following. Let and be the term list of and respectively, the similarity score in our model is: , ∑ , , · · 25 , · · , 25 , · (5) Distribution Similarity Measurement using Monolingual Term Finally, we apply the results of time-series research to replace Pearson’s correlation which is used in the baseline model, in our calculation of the similarity score of two frequency distributions. A popular technique for time sequence matching is to use Discrete Fourier Transform ( ) (Agrawal et al, 1993). More recently, Klementiev and Roth (2006) also use F-index (Hetland, 2004), a score using , to calculate the time distribution similarity. In our model, we assume that the frequency chain of a word is a sequence, and calculate score for each chain by the following formula: . (6) In time series research, it is proven that only the first few coefficients of a chain are strong and important for comparison (Agrawal et al, 1993). Our experiments in section 5 show that the best value for is 7 for both language pairs. (7) , The , in equation (5) is replaced by , in equation (8) to calculate the Monolingual Term Distribution ( ) score. , · ,"
E09-1096,C04-1138,0,0.0946157,"ning corpus for a statistical-based summarization system. The research on similarity calculation for multilingual comparable corpora has attracted more attention than monolingual comparable corpora. However, the purpose and scenario of these works are rather varied. Steinberger et al. (2002) represent document contents using descriptor terms of a multilingual thesaurus EUROVOC 1 , and calculate the semantic similarity based on the distance between the two documents’ representations. The assignment of descriptors is trained by log-likelihood test and computed by , Cosine, and Okapi. Similarly, Pouliquen et al. (2004) use a linear combination of three types of knowledge: cognates, geographical place names reference, and map documents based on the EUROVOC. The major limitation of these works is the use of EUROVOC, which is a specific resource workable only for European languages. Aligning documents across parallel corpora is another area of interest. Patry and Langlais (2005) use three similarity scores, Cosine, Normalized Edit Distance, and Sentence Alignment Score, to compute the similarity between two parallel documents. An Adaboost classifier is trained on a list of scored text pairs labeled as parallel"
E09-1096,I08-2084,1,0.716725,"oach, and our studies on the correlations of single, multiple and commonly appearing words, we propose using “term” or “multiword” instead of “single-word” or “word” to calculate the similarity of term frequency distribution between two documents. This presents us with two main advantages. Firstly, the smaller number of terms compared to the number of words present in any document would imply fewer possible document alignment pairs for the system. This increases the computation speed remarkably. To extract automatically the list of terms in each document, we use the term extraction model from Vu et al. (2008). In corpora used in our experiments, the average ratios of word/term per document are 556/37, 410/28 and 384/28 for English, Chinese, and Malay respectively. The other advantage of using terms is that terms are more distinctive than words as they contain less ambiguity, thus enabling high correlation to be observed when compared with single words. 3.3.2 Bilingual Dictionary Incorporation In addition to using terms for the computation, we observed from equation (3) that the only mutual feature relating the two documents is the frequency distribution coefficient , . It is likely that the alignm"
I05-1034,A00-2030,0,0.0584531,"stness in handling large-scale or new domain data due to two reasons. First, rules have to be rewritten for different tasks or when porting to different domains. Second, generating rules manually is quite labor- and time-consuming. 1 GPE is an acronym introduced by the ACE (2004) program to represent a Geo-Political Entity --- an entity with land and a government. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 378 – 389, 2005. © Springer-Verlag Berlin Heidelberg 2005 Discovering Relations Between Named Entities from a Large Raw Corpus 379 Since then, various supervised learning approaches [2,3,4,5] have been explored extensively in relation extraction. These approaches automatically learn relation patterns or models from a large annotated corpus. To decrease the corpus annotation requirement, some researchers turned to weakly supervised learning approaches [6,7], which rely on a small set of initial seeds instead of a large annotated corpus. However, there is no systematic way in selecting initial seeds and deciding an “optimal” number of them. Alternatively, Hasegawa et al. [8] proposed a cosine similarity-based unsupervised learning approach for extracting relations from a large raw c"
I05-1034,P03-1005,0,0.0243155,"Missing"
I05-1034,P04-1043,0,0.255007,"Missing"
I05-1034,sekine-etal-2002-extended,0,0.0148849,"ht, NP], [sold, NP, yesterday]) = 1 + K ( bought, sold ) + K (NP, NP) = 1+0.25+0.25+K c ([a, red, car], [the, flat]) = 1.5 + K (a, the) + K (car, flat ) =2 The above similarity score is more than one. This is because we did not normalize the score using Formula (6). 2.3 Tree Similarity Based Unsupervised Learning Our method consists of five steps: 1) Named Entity (NE) tagging and sentence parsing: Detailed and accurate NE types provide more effective information for relation discovery. Here we use Sekine’s NE tagger [20], where 150 hierarchical types and subtypes of Named Entities are defined [21]. This NE tagger has also been adopted by Hasegawa et al. [8]. Besides, Collin’s parser [18] is adopted to generate shallow parse trees. 2) Similarity calculation: The similarity between two relation instances is defined between two parse trees. However, the state-of-the-art of parser is always error-prone. Therefore, we only use the minimum span parse tree including the NE pairs when calculating the similarity function [4]. Please note that the two entities may not be the leftmost or rightmost node in the sub-tree. 3) NE pairs clustering: Clustering of NE pairs is based on the similarity scor"
I05-1034,J03-4003,0,\N,Missing
I05-1034,P04-1054,0,\N,Missing
I05-1034,P04-1053,0,\N,Missing
I05-1034,P02-1034,0,\N,Missing
I05-1034,P04-1016,0,\N,Missing
I05-1051,J93-2003,0,0.0170524,"ity and the length of the phrase unit. We present Level-Of-Detail (LOD) approach, an agglomerative approach for learning phrase-level alignment. Our experiments show that LOD approach signiﬁcantly improves the performance of the word-based approach. LOD demonstrates a clear advantage that the phrase translation table grows only sub-linearly over the maximum phrase length, while having a performance comparable to those of other phrase-based approaches. 1 Introduction Early approach to statistical machine translation relies on the word-based translation model to describe the translation process [1]. However, the underlying assumption of word-to-word translation often fails to capture all properties of the language, i.e. the existence of the phrase where a group of words often function together as a unit. Many researchers have proposed to move from the word-based to the phrase-based translation model [2] [3] [4]. A phrase-based approach oﬀers many advantages as a phrase translation captures word context and local reordering inherently [3]. It has become popular in statistical machine translation applications. There are typically two groups of approaches to constructing the phrasebased mo"
I05-1051,W99-0604,0,0.500578,"n table grows only sub-linearly over the maximum phrase length, while having a performance comparable to those of other phrase-based approaches. 1 Introduction Early approach to statistical machine translation relies on the word-based translation model to describe the translation process [1]. However, the underlying assumption of word-to-word translation often fails to capture all properties of the language, i.e. the existence of the phrase where a group of words often function together as a unit. Many researchers have proposed to move from the word-based to the phrase-based translation model [2] [3] [4]. A phrase-based approach oﬀers many advantages as a phrase translation captures word context and local reordering inherently [3]. It has become popular in statistical machine translation applications. There are typically two groups of approaches to constructing the phrasebased model. The ﬁrst group learns phrase translation directly from the sentence pair. It learns both word and phrase units simultaneously. Although these approaches appear intuitive, it usually suﬀers from a prohibitive computational cost. It might have to consider all possible multi-word sequences as phrase candidat"
I05-1051,C00-2163,0,0.0202859,"ble grows only sub-linearly over the maximum phrase length, while having a performance comparable to those of other phrase-based approaches. 1 Introduction Early approach to statistical machine translation relies on the word-based translation model to describe the translation process [1]. However, the underlying assumption of word-to-word translation often fails to capture all properties of the language, i.e. the existence of the phrase where a group of words often function together as a unit. Many researchers have proposed to move from the word-based to the phrase-based translation model [2] [3] [4]. A phrase-based approach oﬀers many advantages as a phrase translation captures word context and local reordering inherently [3]. It has become popular in statistical machine translation applications. There are typically two groups of approaches to constructing the phrasebased model. The ﬁrst group learns phrase translation directly from the sentence pair. It learns both word and phrase units simultaneously. Although these approaches appear intuitive, it usually suﬀers from a prohibitive computational cost. It might have to consider all possible multi-word sequences as phrase candidates a"
I05-1051,W02-1018,0,0.1513,"grows only sub-linearly over the maximum phrase length, while having a performance comparable to those of other phrase-based approaches. 1 Introduction Early approach to statistical machine translation relies on the word-based translation model to describe the translation process [1]. However, the underlying assumption of word-to-word translation often fails to capture all properties of the language, i.e. the existence of the phrase where a group of words often function together as a unit. Many researchers have proposed to move from the word-based to the phrase-based translation model [2] [3] [4]. A phrase-based approach oﬀers many advantages as a phrase translation captures word context and local reordering inherently [3]. It has become popular in statistical machine translation applications. There are typically two groups of approaches to constructing the phrasebased model. The ﬁrst group learns phrase translation directly from the sentence pair. It learns both word and phrase units simultaneously. Although these approaches appear intuitive, it usually suﬀers from a prohibitive computational cost. It might have to consider all possible multi-word sequences as phrase candidates and a"
I05-1051,C96-2141,0,0.333138,"Missing"
I05-1051,W03-1001,0,0.486944,"phrase units simultaneously. Although these approaches appear intuitive, it usually suﬀers from a prohibitive computational cost. It might have to consider all possible multi-word sequences as phrase candidates and all possible pairings as phrase translations at the same time. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 576–587, 2005. c Springer-Verlag Berlin Heidelberg 2005  Phrase-Based Statistical Machine Translation: A Level of Detail Approach 577 The second group of approaches learns phrase translations through word-level alignment: alignment template [2] and projection extension [6], just to name a few. In general, these approaches take the word-level alignment, a by-product of the word-based translation model, as their input and then utilize a heuristic measurement to learn the phrase translation. The heuristic measurement contains all possible conﬁgurations of word-level alignment on a phrase translation. It is noted that the underlying word-level alignment is just an approximation to the exact alignment. The approximation is reﬂected by a probability produced by the word-based translation model. The majority of approaches do not make use of this probability, whereas i"
I05-1051,N03-1017,0,0.304925,"slation model, as their input and then utilize a heuristic measurement to learn the phrase translation. The heuristic measurement contains all possible conﬁgurations of word-level alignment on a phrase translation. It is noted that the underlying word-level alignment is just an approximation to the exact alignment. The approximation is reﬂected by a probability produced by the word-based translation model. The majority of approaches do not make use of this probability, whereas it may provide a valuable clue leading to a better phrase translation from a statistical point of view. Koehn, et. al [8] compared the representative of both groups and reported that learning phrase translation using a simple heuristic from word alignment yields a better translation performance than learning phrase translation directly from the sentence pair. Many approaches try to learn all phrase translations in one step, either directly from the sentence pair or through word alignment. As a result, they may encounter a huge amount of phrase translation candidates at once. Usually, they limit the maximum phrase length to reduce the choice of candidates. Although this method is suﬃcient to satisfy the computati"
I05-1051,2001.mtsummit-papers.68,0,0.059186,"Missing"
I05-1051,N04-1033,0,0.0618716,"Missing"
I05-1051,W97-0311,0,0.127525,"th introduce an equal number of additional entries to the phrase translation table. As longer phrases occur less often, there should be fewer entries introduced into the phrase translation table. We propose an agglomerative approach to learn phrase translations. Our approach is motivated by the second group, which is to learn phrase translation through word-alignment, while addressing the common issues: the size of the phrase translation table, the use of underlying translation model probability and the length of the phrase unit. Only a few approaches move away from one-step learning. Melamed [13] presented an agglomerative approach to learn the phrases progressively from a parallel corpus by using sub-phrase bigram statistics. Moore [14] proposed a similar approach which identiﬁes the phrase candidates by parsing the raw training data. Our idea diﬀers from these approaches in that we look into the association of the alignments rather than the association of the words to discover the phrases. In this paper, we propose the Level of Detail (LOD) approach for learning of phrase translations in phrase-based statistical machine translation. Section 2 discusses the background and motivation"
I05-1051,W01-1411,0,0.0154729,"tries introduced into the phrase translation table. We propose an agglomerative approach to learn phrase translations. Our approach is motivated by the second group, which is to learn phrase translation through word-alignment, while addressing the common issues: the size of the phrase translation table, the use of underlying translation model probability and the length of the phrase unit. Only a few approaches move away from one-step learning. Melamed [13] presented an agglomerative approach to learn the phrases progressively from a parallel corpus by using sub-phrase bigram statistics. Moore [14] proposed a similar approach which identiﬁes the phrase candidates by parsing the raw training data. Our idea diﬀers from these approaches in that we look into the association of the alignments rather than the association of the words to discover the phrases. In this paper, we propose the Level of Detail (LOD) approach for learning of phrase translations in phrase-based statistical machine translation. Section 2 discusses the background and motivation and then formulates the LOD approach 578 H. Setiawan et al. while section 3 describes the learning process in details. Section 4 describes the e"
I05-1051,W04-3250,0,0.0819061,"Missing"
I05-1051,C04-1030,0,\N,Missing
I05-1051,P02-1040,0,\N,Missing
I05-1051,P03-1041,0,\N,Missing
I05-1053,W03-1501,0,0.402766,"l at the phrase level. We also present a twostep search to decode the best result from the models. Our proposed model is evaluated on the LDC Chinese-English NE translation corpus. The experiment results show that our proposed model is high effective for NE translation. 1 Introduction A Named Entity (NE) is essentially a proper noun phrase. Automatic NE translation is an indispensable component of cross-lingual applications such as machine translation and cross-lingual information retrieval and extraction. NE is translated by a combination of meaning translation and/or phoneme transliteration [1]. NE transliteration has been given much attention in the literature. Many attempts, including phoneme and grapheme-based methods, various machine learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) [4], have been made recently to tackle the issue of NE transliteration. However, only a few works have been reported in NE translation. Chen et al. [1] proposed a frequency-based approach to learn formulation and transformation rules for multilingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the translation of Arabic NEs to English using monolingual and"
I05-1053,C02-1099,0,0.106662,"experiment results show that our proposed model is high effective for NE translation. 1 Introduction A Named Entity (NE) is essentially a proper noun phrase. Automatic NE translation is an indispensable component of cross-lingual applications such as machine translation and cross-lingual information retrieval and extraction. NE is translated by a combination of meaning translation and/or phoneme transliteration [1]. NE transliteration has been given much attention in the literature. Many attempts, including phoneme and grapheme-based methods, various machine learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) [4], have been made recently to tackle the issue of NE transliteration. However, only a few works have been reported in NE translation. Chen et al. [1] proposed a frequency-based approach to learn formulation and transformation rules for multilingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the translation of Arabic NEs to English using monolingual and bilingual resources. Huang et al. [6] described an approach to translate rarely occurring NEs by combining phonetic and semantic similarities. In this paper, we pay special attention to"
I05-1053,P04-1021,1,0.870234,"del is high effective for NE translation. 1 Introduction A Named Entity (NE) is essentially a proper noun phrase. Automatic NE translation is an indispensable component of cross-lingual applications such as machine translation and cross-lingual information retrieval and extraction. NE is translated by a combination of meaning translation and/or phoneme transliteration [1]. NE transliteration has been given much attention in the literature. Many attempts, including phoneme and grapheme-based methods, various machine learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) [4], have been made recently to tackle the issue of NE transliteration. However, only a few works have been reported in NE translation. Chen et al. [1] proposed a frequency-based approach to learn formulation and transformation rules for multilingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the translation of Arabic NEs to English using monolingual and bilingual resources. Huang et al. [6] described an approach to translate rarely occurring NEs by combining phonetic and semantic similarities. In this paper, we pay special attention to the issue of NE translation. Although NE t"
I05-1053,P02-1051,0,0.181878,"ated by a combination of meaning translation and/or phoneme transliteration [1]. NE transliteration has been given much attention in the literature. Many attempts, including phoneme and grapheme-based methods, various machine learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) [4], have been made recently to tackle the issue of NE transliteration. However, only a few works have been reported in NE translation. Chen et al. [1] proposed a frequency-based approach to learn formulation and transformation rules for multilingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the translation of Arabic NEs to English using monolingual and bilingual resources. Huang et al. [6] described an approach to translate rarely occurring NEs by combining phonetic and semantic similarities. In this paper, we pay special attention to the issue of NE translation. Although NE translation is less sophisticated than machine translation (MT) in general, to some extent, the issues in NE translation are similar to those in MT. Its challenges lie in not only the ambiguity in lexical mapping such as <副(Fu),Deputy> and <副(Fu),Vice> in Fig.1 in the next page, but also the pos"
I05-1053,N04-1036,0,0.0681695,"much attention in the literature. Many attempts, including phoneme and grapheme-based methods, various machine learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) [4], have been made recently to tackle the issue of NE transliteration. However, only a few works have been reported in NE translation. Chen et al. [1] proposed a frequency-based approach to learn formulation and transformation rules for multilingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the translation of Arabic NEs to English using monolingual and bilingual resources. Huang et al. [6] described an approach to translate rarely occurring NEs by combining phonetic and semantic similarities. In this paper, we pay special attention to the issue of NE translation. Although NE translation is less sophisticated than machine translation (MT) in general, to some extent, the issues in NE translation are similar to those in MT. Its challenges lie in not only the ambiguity in lexical mapping such as <副(Fu),Deputy> and <副(Fu),Vice> in Fig.1 in the next page, but also the position permutation and fertility of words. Fig.1 illustrates two excerpts of NE translation from the LDC corpus [7]"
I05-1053,2002.tmi-tutorials.2,0,0.0491198,"our study, we enhance the LMM with the PM to account for the word reordering issue in NE translation, so our model is capable of modeling the non-monotone problem. In contrast, JSCM only models the monotone problem. Both rule-based [1] and statistical model-based [5,6] methods have been proposed to address the NE translation problem. The model-based methods mostly are based on conditional probability under the noisy-channel framework [8]. Now let’s review the different modeling methods: 1) 2) 3) As far as lexical choice issue is concerned, the noisy-channel model, represented by IBM Model 1-5 [8], models lexical dependency using a context-free conditional probability. Marcu and Wong [10] proposed a phrase-based context-free joint probability model for lexical mapping. In contrast, our LMM models lexical dependency using n-order bilingual contextual information. Another characteristic of our method lies in its modeling and search strategy. NE translation and MT are usually viewed as a non-monotone search problem and it is well-known that a non-monotone search is exponentially more complex than a monotone search. Thus, we propose the two separated models and the two-step search, so that"
I05-1053,W02-1018,0,0.0349964,"nslation, so our model is capable of modeling the non-monotone problem. In contrast, JSCM only models the monotone problem. Both rule-based [1] and statistical model-based [5,6] methods have been proposed to address the NE translation problem. The model-based methods mostly are based on conditional probability under the noisy-channel framework [8]. Now let’s review the different modeling methods: 1) 2) 3) As far as lexical choice issue is concerned, the noisy-channel model, represented by IBM Model 1-5 [8], models lexical dependency using a context-free conditional probability. Marcu and Wong [10] proposed a phrase-based context-free joint probability model for lexical mapping. In contrast, our LMM models lexical dependency using n-order bilingual contextual information. Another characteristic of our method lies in its modeling and search strategy. NE translation and MT are usually viewed as a non-monotone search problem and it is well-known that a non-monotone search is exponentially more complex than a monotone search. Thus, we propose the two separated models and the two-step search, so that the lexical mapping issue can be resolved by monotone search. This results in a large improv"
I05-1053,W99-0604,0,0.114464,"Missing"
I05-1053,N03-1017,0,0.0566621,"hrase-based MT research, to carry out the same NE translation experiments as reference cases. All the experiments conducted in this paper are listed as follow: 1) 3 4 IBM method C: word-based IBM Model 4 trained by GIZA++3 [15] and ISI Decoder4 [14,16]; http://www.fjoch.com/ http://www.isi.edu/natural-language/software/decoder/manual.html A Phrase-Based Context-Dependent Joint Probability Model 2) 3) 4) 607 IBM method D: phrase-based IBM Model 4 trained by GIZA++ on phrasealigned corpus and ISI Decoder working on phrase-segmented testing corpus. Koehn method: Koehn et al.’s phrase-based model [12] and PHARAOH5 decoder6; Our method: phrase-based bi-gram LMM and bi-gram PM, and our two-step decoder. To make an accurate comparison, all the above three phrase-based models are trained on the same phrase-segmented and aligned corpus, and tested on the same phrase-segmented corpus. ISI Decoder carries out a greedy search, and PHARAOH is a beam-search stack decoder. To optimize their performances, the two decoders are allowed to do unlimited reordering without penalty. We train trigram language models in the first three experiments and bi-gram models in the forth experiment. 5.2 NE Translation"
I05-1053,P01-1030,0,0.0646095,"Missing"
I05-1053,J03-1002,0,0.00868264,"ntext-dependent joint probability model. 3 Training Following the modeling strategy discussed above, the training process consists of three steps: phrase alignment, reordering of corpus, and learning statistical parameters for lexical mapping and permutation models. 3.1 Acquiring Phrase Pairs To reduce vocabulary size and avoid sparseness, we constrain the phrase length to up to three words and the lower-frequency phrase pairs are pruned out for accurate 604 M. Zhang et al. phrase-alignment1. Given a word alignment corpus which can be obtained by means of the publicly available GIZA++ toolkit [15], it is very straightforward to construct the phrase-alignment corpus by incrementally traversing the word-aligned NE from left to right2. The set of resulting phrase pairs forms a lexical mapping table. 3.2 Reordering Corpus The context-dependent lexical mapping model assumes monotonic alignment in the bilingual training corpus. Thus, the phrase aligned corpus needs to be reordered so that it is in either source-ordered or target-ordered alignment. We choose to reorder the target phrases to follow the source order. Only in this way can we use the lexical mapping model to describe the monotoni"
I05-1053,N03-1010,0,0.0245356,"Missing"
I05-1053,J03-1005,0,0.0554023,"Missing"
I05-1053,2001.mtsummit-papers.68,0,0.0298051,"Missing"
I05-1053,W00-0508,0,0.0570397,"et us discuss the major differences between them: 610 1) 2) M. Zhang et al. Our LMM models the lexical mapping and target word selection using a context-dependent joint probability while IBM Model 1 using a contextindependent conditional probability and a target n-gram language model. Our LMM carries out the target word selection and our PM only models the target word connectivity while the language model in IBM Model 1 performs the function of target word selection. Alternatively, finite-state automata (FSA) for statistical MT were previous suggested for decoding using contextual information [21,22]. Bangalore and Riccardi [21] proposed a phrase-based variable length n-gram model followed by a reordering scheme for spoken language translation. However, their re-ordering scheme was not evaluated by empirical experiments. 7 Conclusions In this paper, we propose a new model for NE translation. We present the training and decoding methods for the proposed model. We also compare the proposed method with related work. Empirical experiments show that our method outperforms the previous methods significantly in all test cases. We conclude that our method works more effectively and efficiently in"
I05-1053,P04-1065,0,0.0254848,"et us discuss the major differences between them: 610 1) 2) M. Zhang et al. Our LMM models the lexical mapping and target word selection using a context-dependent joint probability while IBM Model 1 using a contextindependent conditional probability and a target n-gram language model. Our LMM carries out the target word selection and our PM only models the target word connectivity while the language model in IBM Model 1 performs the function of target word selection. Alternatively, finite-state automata (FSA) for statistical MT were previous suggested for decoding using contextual information [21,22]. Bangalore and Riccardi [21] proposed a phrase-based variable length n-gram model followed by a reordering scheme for spoken language translation. However, their re-ordering scheme was not evaluated by empirical experiments. 7 Conclusions In this paper, we propose a new model for NE translation. We present the training and decoding methods for the proposed model. We also compare the proposed method with related work. Empirical experiments show that our method outperforms the previous methods significantly in all test cases. We conclude that our method works more effectively and efficiently in"
I05-1053,C04-1030,0,\N,Missing
I05-1053,J93-2003,0,\N,Missing
I05-1053,P02-1040,0,\N,Missing
I05-1053,N04-1033,0,\N,Missing
I05-1053,J98-4003,0,\N,Missing
I08-1008,J96-1002,0,0.14677,"Missing"
I08-1008,P04-1024,0,0.691486,"and name entity recognition. Unfortunately, little attention has been given to name origin recognition (NOR) so far in the literature. In this paper, we are interested in two kinds of name origin recognition: the origin of names written in English (ENOR) and the origin of names written in Chinese (CNOR). For ENOR, the origins include English (Eng), Japanese (Jap), Chinese Mandarin Pinyin (Man) and Chinese Cantonese Jyutping (Can). For CNOR, they include three origins: Chinese (Chi, for both Mandarin and Cantonese), Japanese and English (refer to Latinscripted language). Unlike previous work (Qu and Grefenstette, 2004; Li et al., 2006; Li et al., 2007) where NOR was formulated with a generative model, we regard the NOR task as a classification problem. We further propose using a discriminative learning algorithm (Maximum Entropy model: MaxEnt) to solve the problem. To draw direct comparison, we conduct experiments on the same personal name corpora as that in the previous work by Li et al. (2006). We show that the MaxEnt method effectively incorporates diverse features and outperforms previous methods consistently across all test cases. The rest of the paper is organized as follows: in section 2, we review"
I08-1008,P07-1016,1,\N,Missing
I08-1008,Y04-1029,0,\N,Missing
I08-1008,J98-4003,0,\N,Missing
I08-1066,P06-1067,0,0.122811,"Missing"
I08-1066,H05-1098,0,0.184236,"h space using swapping window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a st"
I08-1066,J07-2003,0,0.0945551,"exity of BTG decoding with m-gram language model is O(n3+4(m−1) ). If a 4-gram language model is used (common in many current SMT systems), the time complexity is as high as O(n15 ). Therefore with this time complexity translating long sentences is time-consuming even with highly stringent pruning strategy. To speed up BTG decoding, Huang et al. (2005) adapted the hook trick which changes the time complexity from O(n3+4(m−1) ) to O(n3+3(m−1) ). However, the implementation of the hook trick with pruning is quite complicated. Another method to increase decoding speed is cube pruning proposed by Chiang (2007) which reduces search space significantly. In this paper, we propose two refinements to address the two issues, including (1) reordering heuristics to prevent incorrect swapping and reduce search space using swapping window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT a"
I08-1066,W00-2010,0,0.0338613,"swapping window and punctuation restriction. Swapping Window (SW): It constrains block swapping in the following way ACTIVATE A → hA1 , A2 i IF |A1s |+ |A2s |&lt; sws where |Ais |denotes the number of words on the source side Ais of block Ai , sws is a pre-defined swapping window size. Any inverted reordering beyond the pre-defined swapping window size is prohibited. Punctuation Restriction (PR): If two neighboring blocks include any of the punctuation marks p ∈ {， 、 ： ； 「 」 《 》 （ ） “ ”}, the two blocks will be merged with straight order. Punctuation marks were already used in parsing (Christine Doran, 2000) and statistical machine translation (Och et al., 2003). In (Och et al., 2003), three kinds of features are defined, all related to punctuation marks like quotes, parentheses and commas. Unfortunately, no statistically significant improvement on the BLEU score was reported in (Och et al., 2003). In this paper, we consider this problem from a different perspective. We emphasize that words around punctuation marks are reordered ungrammatically and therefore we positively use punctuation marks as a hard decision to restrict such reordering around punctuations. This is straightforward but yet resu"
I08-1066,W05-1507,0,0.0156052,"of bilingual phrases as features to predict their orders. Xiong et al. (2006) reported significant performance improvement on Chinese-English translation tasks in two different domains when compared with both Pharaoh (Koehn, 2004) and the original BTG using flat reordering. However, error analysis of the translation output of Xiong et al. (2006) reveals that boundary words predict wrong swapping, especially for long phrases although the MaxEnt-based reordering model shows better performance than baseline reordering models. Another big problem with BTG-based SMT is the high computational cost. Huang et al. (2005) reported that the time complexity of BTG decoding with m-gram language model is O(n3+4(m−1) ). If a 4-gram language model is used (common in many current SMT systems), the time complexity is as high as O(n15 ). Therefore with this time complexity translating long sentences is time-consuming even with highly stringent pruning strategy. To speed up BTG decoding, Huang et al. (2005) adapted the hook trick which changes the time complexity from O(n3+4(m−1) ) to O(n3+3(m−1) ). However, the implementation of the hook trick with pruning is quite complicated. Another method to increase decoding speed"
I08-1066,koen-2004-pharaoh,0,0.044683,"ss, BTG restriction is widely used for reordering in SMT (Zens et al., 2004). However, BTG restriction does not provide a mechanism to predict final orders between two neighboring blocks. 505 {htmi, liuqun, sxlin}@ict.ac.cn To solve this problem, Xiong et al. (2006) proposed an enhanced BTG with a maximum entropy (MaxEnt) based reordering model (MEBTG). MEBTG uses boundary words of bilingual phrases as features to predict their orders. Xiong et al. (2006) reported significant performance improvement on Chinese-English translation tasks in two different domains when compared with both Pharaoh (Koehn, 2004) and the original BTG using flat reordering. However, error analysis of the translation output of Xiong et al. (2006) reveals that boundary words predict wrong swapping, especially for long phrases although the MaxEnt-based reordering model shows better performance than baseline reordering models. Another big problem with BTG-based SMT is the high computational cost. Huang et al. (2005) reported that the time complexity of BTG decoding with m-gram language model is O(n3+4(m−1) ). If a 4-gram language model is used (common in many current SMT systems), the time complexity is as high as O(n15 )."
I08-1066,P07-2045,0,0.00543539,"prevent incorrect swapping and reduce search space using swapping window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has"
I08-1066,P06-1077,1,0.851772,"tion restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose rules are weighted"
I08-1066,W02-2018,0,0.00938526,"hrase penalty and word penalty, respectively and λs are weights of features. These features are commonly used in the state-of-the-art systems (Koehn et al., 2005; Chiang et al., 2005). 2.2 MaxEnt-based Reordering Model The MaxEnt-based reordering model is defined on two consecutive blocks A1 and A2 together with their order o ∈ {straight, inverted} according to the maximum entropy framework. P exp( i θi hi (o, A1 , A2 )) P Ω = pθ (o|A , A ) = P 1 2 o exp( i θi hi (o, A , A )) (6) where the functions hi ∈ {0, 1} are model features and θi are weights of the model features trained automatically (Malouf, 2002). There are three steps to train a MaxEnt-based reordering model. First, we need to extract reordering examples from unannotated bilingual data, then generate features from these examples and finally estimate feature weights. 1 2 For extracting reordering examples, there are two points worth mentioning: 1. In the extraction of useful reordering examples, there is no length limitation over blocks compared with extracting bilingual phrases. 2. When enumerating all combinations of neighboring blocks, a good way to keep the number of reordering examples acceptable is to extract smallest blocks wit"
I08-1066,W06-1606,0,0.050471,"g window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose"
I08-1066,P02-1038,0,0.0523918,"ining data, (3) a CKY-style decoder using beam search similar to that of Wu (1996). We describe the first two components briefly below. 2.1 Model The translation process is modeled using BTG rules which are listed as follows A → [A1 , A2 ] (1) A → hA1 , A2 i (2) A → x/y (3) The lexical rule (3) is used to translate source phrase x into target phrase y and generate a block A. The 506 two rules (1) and (2) are used to merge two consecutive blocks into a single larger block in a straight or inverted order. To construct a stochastic BTG, we calculate rule probabilities using the log-linear model (Och and Ney, 2002). For the two merging rules (1) and (2), the assigned probability P rm (A) is defined as follows LM P rm (A) = ΩλΩ · 4λpLM (A1 ,A2 ) (4) where Ω, the reordering score of block A1 and A2 , is calculated using the MaxEnt-based reordering model (Xiong et al., 2006) described in the next section, λΩ is the weight of Ω, and 4pLM (A1 ,A2 ) is the increment of language model score of the two blocks according to their final order, λLM is its weight. For the lexical rule (3), it is applied with a probability P rl (A) P rl (A) = p(x|y)λ1 · p(y|x)λ2 · plex (x|y)λ3 ·plex (y|x)λ4 · exp(1)λ5 · exp(|y|)λ6 LM"
I08-1066,P96-1021,0,0.151694,"valuation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose rules are weighted using different features in log-linear form, (2) a MaxEnt-based reordering model with features automatically learned from bilingual training data, (3) a CKY-style decoder using beam search similar to that of Wu (1996). We describe the first two components briefly below. 2.1 Model The translation process is modeled using BTG rules which are listed as follows A → [A1 , A2 ] (1) A → hA1 , A2 i (2) A → x/y (3) The lexical rule (3) is used to translate source phrase x into target phrase y and generate a block A. The 506 two rules (1) and (2) are used to merge two consecutive blocks into a single larger block in a straight or inverted order. To construct a stochastic BTG, we calculate rule probabilities using the log-linear model (Och and Ney, 2002). For the two merging rules (1) and (2), the assigned probabilit"
I08-1066,P06-1066,1,0.959397,"tegies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose rules are weighted using different features in log-linear form, (2) a MaxEnt-based reordering model with features automatically learned from bilingual training data, (3) a CKY-style decoder using beam search similar to that of Wu (1996). We describe the first two components briefly below. 2.1 Model The translation process is modeled using BTG rules which are listed as follows A → [A1 , A2 ] (1) A → hA1 , A2 i (2) A → x/y (3) The lexical rule (3) is used to translate source ph"
I08-1066,C04-1030,0,0.374843,"Missing"
I08-1066,zhang-etal-2004-interpreting,0,0.0650612,"Missing"
I08-1066,2005.iwslt-1.8,0,\N,Missing
I08-2084,P01-1025,0,0.0754834,"Missing"
I08-2084,W02-1407,0,0.0129571,"tified (e.g. noun-noun, adjective-noun-noun combinations etc.). The second step involves the use of frequency- or statisticalbased evidence measures to compute weights indicating to what degree a candidate qualifies as a terminological unit. There are many methods in literature trying to improve this second step. Some of them borrowed the metrics from Information Retrieval to evaluate how important a term is within a document or a corpus. Those metrics are Term Frequency/Inverse Document Frequency (TF/IDF), Mutual Information, T-Score, Cosine, and Information Gain. There are also other works (Nakagawa and Mori, 2002; Frantzi and Ananiadou, 1998) that introduced better method to weigh the term candidates. Currently, the C/NC method (Frantzi and Ananiadou, 1998) is widely considered as the state-of-the-art model for TE. Although this method was first applied on English, it also performed well on other languages such as Japanese (Hideki Mima and Sophia Ananiadou, 2001), Slovene (Špela Vintar, 2004), and other domains such as medical corpus (Frantzi and Ananiadou, 1998), and computer science (E. Milios et al, 2003). In terminology research, a term is evaluated using two types of feature: termhood1 and unitho"
I08-2109,P06-2010,1,0.900752,"hods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate matching mechanisms over substructures and nodes. Experimental results show that the GTK significant"
I08-2109,P02-1031,0,0.0363188,"verbs or nouns and some constituents of the sentence. In previous work, data-driven techniques, including feature-based and kernel-based learning methods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure"
I08-2109,P04-1043,0,0.078076,"ce. In previous work, data-driven techniques, including feature-based and kernel-based learning methods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate"
I08-2109,N06-2025,0,0.0686209,"= opt([d]) = 1, p = 3. Then according to Eq (14), ∆p (cn1 , cn2 ) can be calculated recursively as Eq. (15) (Please refer to the next page). Finally, we have ∆p (cn1 , cn2 ) = λ1 × ∆0 (a, a) × 0 ∆ (b, b) × ∆0 (c, c) By means of the above algorithm, we can compute the ∆0 (n1 , n2 ) in O(p|cn1 |· |cn2 |2 ) (Lodhi et al., 2002). This means that the worst case complexity of the FGTK-I is O(pρ3 |N1 |· |N2 |2 ), where ρ is the maximum branching factor of the two trees. 3.2 Fast Grammar-driven Convolution Tree Kernel II (FGTK-II) Our FGTK-II algorithm is motivated by the partial trees (PTs) kernel (Moschitti, 2006). The PT kernel algorithm uses the following recursive formulas to evaluate ∆p (cn1 , cn2 ): |cn1 ||cn2 | ∆p (cn1 , cn2 ) = X X ∆0p (cn1 [1 : i], cn2 [1 : j]) (16) i=1 j=1 where cn1 [1 : i] and cn2 [1 : j] are the child subsequences of cn1 and cn2 from 1 to i and from 1 to j, respectively. Given two child node sequences s1 a = cn1 [1 : i] and s2 b = cn2 [1 : j] (a and b are the last children), the PT kernel computes ∆0p (·, ·) as follows:  0 ∆p (s1 a, s2 b) = µ2 ∆0 (a, b)Dp (|s1 |, |s2 |) 0 if a = b else (17) where ∆0 (a, b) is defined in Eq. (7) and Dp is recursively defined as follows: Dp ("
I08-2109,W05-0639,0,0.0418752,"Missing"
I08-2109,P07-1026,1,0.892395,"in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate matching mechanisms over substructures and nodes. Experimental results show that the GTK significantly outperforms the TK (Zhang et al., 2007). Theoretically, the GTK method is applicable to any problem that uses syntax structure features and can be solved by the TK methods, such as parsing, relation extraction, and so on. In this paper, we use SRL as an application to test our proposed algorithms. Although the GTK shows promising results"
I08-2109,W05-0620,0,\N,Missing
I11-1065,J90-1003,0,0.09213,"cks cross-lingual information. Various measures for cross-lingual word semantic similarity have been proposed to explore statistical techniques and semantic network. Research works propose to use WordNet by Resnik (1999) to measure similarity between English words. Liu and Li (2002) adopt HowNet calculate word similarity in machine translation. Xia et al. (2011) propose to explore cross-lingual word similarity by observing concept definition provided by HowNet. Corpus-based measures for semantic similarity are found more interesting. The classical method is Pointwise Mutual Information (PMI) (Church and Hanks, 1990). Many researches are based on PMI, such as PMI-IR (Turney, 2001) and Second Order Co-occurrence PMI (SOCPMI) (Islam and Inkpen, 2006). SOCPMI is proved better than PMI-IR and some other similarity measures (Islam and Inkpen, 2006). In this work, we implement three representative measures: HowNet-based measure (Xia et al., 2011), SOCPMI measure (Islam and Inkpen, 2006) and COV measure (Farahat and Kamel, 2010). 3 3.1 Cross-Lingual Generalized VSM Generalized VSM be a set of docuLet ments which contain M terms, be a matrix whose element represents the weight of term in document . GVSM (Wang et"
I11-1065,W05-1203,0,0.0537671,"Missing"
I11-1065,islam-inkpen-2006-second,0,0.106272,"echniques and semantic network. Research works propose to use WordNet by Resnik (1999) to measure similarity between English words. Liu and Li (2002) adopt HowNet calculate word similarity in machine translation. Xia et al. (2011) propose to explore cross-lingual word similarity by observing concept definition provided by HowNet. Corpus-based measures for semantic similarity are found more interesting. The classical method is Pointwise Mutual Information (PMI) (Church and Hanks, 1990). Many researches are based on PMI, such as PMI-IR (Turney, 2001) and Second Order Co-occurrence PMI (SOCPMI) (Islam and Inkpen, 2006). SOCPMI is proved better than PMI-IR and some other similarity measures (Islam and Inkpen, 2006). In this work, we implement three representative measures: HowNet-based measure (Xia et al., 2011), SOCPMI measure (Islam and Inkpen, 2006) and COV measure (Farahat and Kamel, 2010). 3 3.1 Cross-Lingual Generalized VSM Generalized VSM be a set of docuLet ments which contain M terms, be a matrix whose element represents the weight of term in document . GVSM (Wang et al, , where every row and column represents a term, respectively. In tradition GVSM (Wang et al, 1985), terms are represented as vecto"
I11-1065,P00-1056,0,0.0754142,"Missing"
I11-1065,C04-1138,0,0.0689709,"Missing"
I11-1065,D09-1091,0,0.0300142,"Missing"
I11-1135,P02-1051,0,0.0471523,"Missing"
I11-1135,J96-1002,0,0.0468198,"Missing"
I11-1135,C10-2165,1,0.138036,"romising results have been reported, one of major issues is that the current transliteration methods rely heavily on significant amount of source-target parallel data to learn transliteration model. However, such corpora are not always available and the amounts of the currently available corpora, even for language pairs with English involved, are far from enough for training, letting alone many low-density language pairs. Indeed, transliteration corpora for most language pairs without English involved are unavailable and are usually rather expensive to manually construct (Khapra et al., 2010; Zhang et al., 2010). To date, only two previous works (Khapra et al., 2010; Zhang et al., 2010) touch this issue of transliterating names across low-density language pairs. Both of them resort to pivot language-based approaches to address this issue. 1207 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1207–1215, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Khapra et al. (2010) proposes the system-based pivot strategy for machine transliteration, which learns a source-pivot model from source-pivot data and a pivot-target model from pivot-target data, respe"
I11-1135,2008.iwslt-papers.1,0,\N,Missing
I11-1135,N10-1065,0,\N,Missing
I11-1135,C02-1099,0,\N,Missing
I11-1135,C04-1103,1,\N,Missing
I11-1135,P06-1103,0,\N,Missing
I11-1135,N07-1061,0,\N,Missing
I11-1135,P06-1010,0,\N,Missing
I11-1135,W06-1672,0,\N,Missing
I11-1135,D08-1037,0,\N,Missing
I11-1135,P08-1045,0,\N,Missing
I11-1135,N03-1017,0,\N,Missing
I11-1135,J03-1002,0,\N,Missing
I11-1135,P09-1018,0,\N,Missing
I11-1135,P07-1092,0,\N,Missing
I11-1135,W09-3502,1,\N,Missing
I11-1135,P09-1016,1,\N,Missing
I13-1035,P11-1040,0,0.0835156,"The feature-rich event filter led to significantly higher precision and doubled recall when compared to the state-of-the-art baseline system. In our experiments we observed that a news event can be detected more than once in one time window, which each appearance representing one aspects of the event. Building these sub-events into a hierarchy will be explored in the future. In addition to the above group of work, which represents events with a few messages or features showing the topic information, some researchers try to extract structured information for events. Given a set of seed events, Benson et al. (2011) use a factor graph to extract artist and venue information of a concert event. Popescu et al. (2011) extract main entities, actions and audience opinions. Data from social medias like Twitter are very sparse in presenting thousands of events, while some researchers mainly focus on specific types of events. Sakaki et al. (2010) detected disaster events like earthquakes and typhoons from Twitter. Pohl et al. (2012) tried to detect subevent to assist disaster management with Flickr and YouTube data. Agarwal et al. (2012) analyzed tweets containing specific keywords and report Fire-in Factory and"
I13-1035,N09-1025,0,0.0346455,"horoscope topic heterogeneous collection Table 3: Example Events. have fixed query words and search for related messages from social media websites for data. The query words are challenges to define as they are vital to the quality of dataset, which will greatly influence the results. Becker et al. (2012) tried to generate queries for a planned event to relax the limitation. Our work mainly focus on news event detection problem on Twitter. Rich features have been used in other tasks in NLP, such as POS-tagging (Toutanova et al., 2003), parsing (Zhang and Nivre, 2011) and machine translation (Chiang et al., 2009). Our work is in line with these. t. Tweet segmentation is firstly proposed by Li et al. (2012b) for an named entity recognition system on Twitter. They claim that segments are much more meaningful and easier to read than words. Twevent is the most related work to this paper. We adopt tweet segmentation, and segment tweets into non-overlapping segments that are regarded as bursty feature candidates, and utilize a feature-pivot clustering method to group bursty segments into clusters as events. The difference between this paper and Twevent is that they use a simple measurement (newsworthiness)"
I13-1035,N03-1033,0,0.0216373,"ce voting related to e4 photographer died when chasing justin bieber related to e6 horoscope topic heterogeneous collection Table 3: Example Events. have fixed query words and search for related messages from social media websites for data. The query words are challenges to define as they are vital to the quality of dataset, which will greatly influence the results. Becker et al. (2012) tried to generate queries for a planned event to relax the limitation. Our work mainly focus on news event detection problem on Twitter. Rich features have been used in other tasks in NLP, such as POS-tagging (Toutanova et al., 2003), parsing (Zhang and Nivre, 2011) and machine translation (Chiang et al., 2009). Our work is in line with these. t. Tweet segmentation is firstly proposed by Li et al. (2012b) for an named entity recognition system on Twitter. They claim that segments are much more meaningful and easier to read than words. Twevent is the most related work to this paper. We adopt tweet segmentation, and segment tweets into non-overlapping segments that are regarded as bursty feature candidates, and utilize a feature-pivot clustering method to group bursty segments into clusters as events. The difference between"
I13-1035,P12-1056,0,0.0249372,"d increased precision. 1 Introduction We study news event detection from Twitter messages (tweets). Generally, tweets can be classified into three groups: 1) news events, or breaking news such as “Manchester united Vs Athletic in Jan. 1st”; 2) hot topics that spread among a large amount of Twitter users, such as horoscope topics (e.g. “You have recently experienced a phase of expansionism and it’s... More for Sagittarius”); and 3) heterogeneous collections or, meaningless non-event tweets, such as “Need buddy wanna chat”. Some previous work (Cataldi et al., 2010; Kasiviswanathan et al., 2011; Diao et al., 2012) regards both news events and hot topics as subjects of detection, while other work (Jackoway et al., 2011; Sakaki et al., 2010; Becker et al., 2012) 302 International Joint Conference on Natural Language Processing, pages 302–310, Nagoya, Japan, 14-18 October 2013. The objective function is defined as: news from some topics, includes horoscope topics (“sagittarius; approach; big trouble”) and topics such as “hitler; fox; megan fox; rip; megan; selena gomez”, which contain segments that can also frequently occur in Wikipedia; 2) as a single measure, newsworthiness is subject to a tradeoff betw"
I13-1035,P11-2033,1,0.791098,"er died when chasing justin bieber related to e6 horoscope topic heterogeneous collection Table 3: Example Events. have fixed query words and search for related messages from social media websites for data. The query words are challenges to define as they are vital to the quality of dataset, which will greatly influence the results. Becker et al. (2012) tried to generate queries for a planned event to relax the limitation. Our work mainly focus on news event detection problem on Twitter. Rich features have been used in other tasks in NLP, such as POS-tagging (Toutanova et al., 2003), parsing (Zhang and Nivre, 2011) and machine translation (Chiang et al., 2009). Our work is in line with these. t. Tweet segmentation is firstly proposed by Li et al. (2012b) for an named entity recognition system on Twitter. They claim that segments are much more meaningful and easier to read than words. Twevent is the most related work to this paper. We adopt tweet segmentation, and segment tweets into non-overlapping segments that are regarded as bursty feature candidates, and utilize a feature-pivot clustering method to group bursty segments into clusters as events. The difference between this paper and Twevent is that t"
I17-1006,C10-1011,0,0.0510393,"Missing"
I17-1006,P16-1231,0,0.0713207,"dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objective, showing promising results. Meanwhile, it is still unclear how PAs can be used by other main-stream dependency parsers, such as the traditional linear graph-based parser (LGPar) and transition-based parser (LTPar), and the newly proposed biaffine neural network graph-based parser (Biaffine) (Dozat and Manning, 2017) and globally normalized neural network transition-based parser (GN3Par) (Andor et al., 2016). Introduction Traditional supervised approaches for structural classification assume full annotation (FA), meaning that the training instances have complete manually-labeled structures. In the case of dependency parsing, FA means a complete parse tree is provided for each training sentence. However, recent studies suggest that it is more economic and effective to construct labeled data with partial annotation (PA). A lot of research effort has been attracted to obtain partially-labeled data for different ∗ This paper aims to thoroughly study this issue and make systematic comparison on differ"
I17-1006,W02-1001,0,0.150548,"d we only need to disable some illegal combination operations during dynamic programming. LTPar can also directly learn from PA in a similar way, as shown in Algorithm 1. Constrained decoding is performed to find a pseudo gold-standard reference (line 8). It is more complicate to design constrained decoding for transition-based parsing Directly training parsers with PA As described in Li et al. (2014), CRF parsers such as LLGPar and Biaffine can naturally learn 51 train-1K train-39K #Sentence #Token 1,000 24,358 dev the beam size is 64 and the standard early update is adopted during training (Collins, 2002). For LGPar and LTPar, averaged perceptron is adopted (Collins, 2002). For Biaffine, we directly adopt most hyperparameters of the released code of Dozat and Manning (2017), only removing the components related with dependency labels, since we focus on unlabeled dependency parsing in this work. The LSTM (two forward plus two backward) layers all use 300-dimension hidden cells. Dropout with ratio of 0.75 is applied to most layers before output. The two MLPs both have 100-dimension outputs without hidden layer. Adam optimization is adopted with α1 = α2 = 0.9. For GN3Par, we follow Daniel et al."
I17-1006,P08-1109,0,0.105003,"Missing"
I17-1006,W15-2202,0,0.019127,"conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert pa"
I17-1006,P09-1042,0,0.0314207,", most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar)"
I17-1006,P10-1002,0,0.0598798,"Missing"
I17-1006,P13-1075,0,0.0606609,"Missing"
I17-1006,P10-1001,0,0.026383,"h effort has been attracted to obtain partially-labeled data for different ∗ This paper aims to thoroughly study this issue and make systematic comparison on different approaches for dependency parsing with PA. In sumCorrespondence author 49 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 49–58, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP mary, we make the following contributions. Dynamic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). Biaffine belongs to the first-order model and only incorporates scores of single dependencies. In contrast, for LLGPar and LGPar, we follow Li et al. (2014) and adopt the second-order model of McDonald and Pereira (2006) considering scores of single dependencies and adjacent siblings. Biaffine and LLGPar both belong to CRF parser. Please note that the original Biaffine is locally trained on each word. In this work, we follow Ma and Hovy (2017) and add a global CRF loss in the projective case, in order to directly use the proposed approach of Li et al. (2014). In other words, we extend the or"
I17-1006,P92-1017,0,0.548641,"Missing"
I17-1006,C12-2067,0,0.0303579,"oding is also used for completing partial trees. We conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency"
I17-1006,N07-1051,0,0.0542854,"Missing"
I17-1006,C14-1075,1,0.641626,"om the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objec"
I17-1006,P02-1035,0,0.274531,"w) // Unconstrained decoding: LGPar 6: a− = arg maxa→d∈Y(xj ) Score(xj , a → d; w) // Unconstrained decoding: LTPar 7: d+ = arg maxd∈Y(xj ,dp ) Score(xj , d; w) // Constrained decoding: LGPar 8: j a+ = arg maxa→d∈Y(xj ,dp ) Score(xj , a → d; w) // Constrained decoding: LTPar j 9: wk+1 = wk + f (x, d+ ) − f (x, d− ) // Update: LGPar 10: wk+1 = wk + f (x, a+ ) − f (x, a− ) // Update: LTPar 11: k =k+1 12: end for 13: end for 2.2 Transition-based Approach from PA based on the idea of ambiguous labeling, which allows a sentence to have multiple parse trees (forest) as its gold-standard reference (Riezler et al., 2002; Dredze et al., 2009; T¨ackstr¨om et al., 2013). First, a partial tree dp is converted into a forest by adding all possible dependencies pointing to remaining words without heads, with the constraint that a newly added dependency does not violate existing ones in dp . The forest can be formally defined as F(x, dp ) = {d : d ∈ Y(x), dp ⊆ d}, whose conditional probability is the sum of probabilities of all trees that it contains: The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each actio"
I17-1006,P16-1033,1,0.853955,"enn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into f"
I17-1006,P10-1037,0,0.22981,"-based parser (LTPar). For the test phase, constrained decoding is also used for completing partial trees. We conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc"
I17-1006,D14-1093,1,0.936317,"d other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objective, showing promising results. Meanwhile, it is still unclear how PAs can be used by other main-st"
I17-1006,W09-1104,0,0.197058,"ulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased depe"
I17-1006,I17-1007,0,0.0144737,"ic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). Biaffine belongs to the first-order model and only incorporates scores of single dependencies. In contrast, for LLGPar and LGPar, we follow Li et al. (2014) and adopt the second-order model of McDonald and Pereira (2006) considering scores of single dependencies and adjacent siblings. Biaffine and LLGPar both belong to CRF parser. Please note that the original Biaffine is locally trained on each word. In this work, we follow Ma and Hovy (2017) and add a global CRF loss in the projective case, in order to directly use the proposed approach of Li et al. (2014). In other words, we extend the original Biaffine Parser described in Dozat and Manning (2017) by adding a CRF layer. Under the CRF model, the conditional probability of d given x is: • We present a general framework for directly training GN3Par, LGPar and LTPar with PA based on constrained decoding. The basic idea is to use the current feature weights to parse the sentence under the PA-constrained search space, and use the best parse as a pseudo gold-standard reference for feat"
I17-1006,W13-5711,0,0.0354672,"Missing"
I17-1006,N13-1126,0,0.0556355,"Missing"
I17-1006,D14-1097,0,0.0698373,"Missing"
I17-1006,W03-3023,0,0.070973,"et al., 2009; T¨ackstr¨om et al., 2013). First, a partial tree dp is converted into a forest by adding all possible dependencies pointing to remaining words without heads, with the constraint that a newly added dependency does not violate existing ones in dp . The forest can be formally defined as F(x, dp ) = {d : d ∈ Y(x), dp ⊆ d}, whose conditional probability is the sum of probabilities of all trees that it contains: The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each action in a (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011): Score(x, d; w) = Score(x, a → d; w) ∑|a| = Score(x, ci , ai ; w) i=1 (6) where ai is the action taken at step i and ci is the configuration status after taking action a1 ...ai−1 . Transition-based methods use inexact beam search to find a highest-scoring action sequence. GN3Par uses a neural network to predict scores of different actions given a state (Chen and Manning, 2014; Andor et al., 2016). First, 48 atomic features are embeded and concatenated as the input layer. Then, two hidden layers are applied to get the scores of all feasible actions. Unlike"
I17-1006,P13-2109,0,0.0494961,"Missing"
I17-1006,D14-1010,0,0.149707,"rmance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objective, showing promising results. Meanwhile, it is still unclear how PAs can be used by other main-stream dependency parsers, such as the traditi"
I17-1006,P05-1012,0,0.118936,"nstruct labeled data with partial annotation (PA). A lot of research effort has been attracted to obtain partially-labeled data for different ∗ This paper aims to thoroughly study this issue and make systematic comparison on different approaches for dependency parsing with PA. In sumCorrespondence author 49 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 49–58, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP mary, we make the following contributions. Dynamic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). Biaffine belongs to the first-order model and only incorporates scores of single dependencies. In contrast, for LLGPar and LGPar, we follow Li et al. (2014) and adopt the second-order model of McDonald and Pereira (2006) considering scores of single dependencies and adjacent siblings. Biaffine and LLGPar both belong to CRF parser. Please note that the original Biaffine is locally trained on each word. In this work, we follow Ma and Hovy (2017) and add a global CRF loss in the projective case, in order to directly use the pro"
I17-1006,J11-1005,1,0.858367,"Missing"
I17-1006,E06-1011,0,0.0639672,"th partial annotation (PA). A lot of research effort has been attracted to obtain partially-labeled data for different ∗ This paper aims to thoroughly study this issue and make systematic comparison on different approaches for dependency parsing with PA. In sumCorrespondence author 49 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 49–58, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP mary, we make the following contributions. Dynamic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). Biaffine belongs to the first-order model and only incorporates scores of single dependencies. In contrast, for LLGPar and LGPar, we follow Li et al. (2014) and adopt the second-order model of McDonald and Pereira (2006) considering scores of single dependencies and adjacent siblings. Biaffine and LLGPar both belong to CRF parser. Please note that the original Biaffine is locally trained on each word. In this work, we follow Ma and Hovy (2017) and add a global CRF loss in the projective case, in order to directly use the proposed approach of Li et al."
I17-1006,P11-2033,1,0.820633,"First, a partial tree dp is converted into a forest by adding all possible dependencies pointing to remaining words without heads, with the constraint that a newly added dependency does not violate existing ones in dp . The forest can be formally defined as F(x, dp ) = {d : d ∈ Y(x), dp ⊆ d}, whose conditional probability is the sum of probabilities of all trees that it contains: The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each action in a (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011): Score(x, d; w) = Score(x, a → d; w) ∑|a| = Score(x, ci , ai ; w) i=1 (6) where ai is the action taken at step i and ci is the configuration status after taking action a1 ...ai−1 . Transition-based methods use inexact beam search to find a highest-scoring action sequence. GN3Par uses a neural network to predict scores of different actions given a state (Chen and Manning, 2014; Andor et al., 2016). First, 48 atomic features are embeded and concatenated as the input layer. Then, two hidden layers are applied to get the scores of all feasible actions. Unlike the traditional perceptron-like train"
I17-1006,P15-1134,0,0.0363309,"Missing"
I17-1006,W11-2917,0,0.165393,"e test phase, constrained decoding is also used for completing partial trees. We conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only"
I17-1006,W03-3017,0,0.110861,"t al., 2013). First, a partial tree dp is converted into a forest by adding all possible dependencies pointing to remaining words without heads, with the constraint that a newly added dependency does not violate existing ones in dp . The forest can be formally defined as F(x, dp ) = {d : d ∈ Y(x), dp ⊆ d}, whose conditional probability is the sum of probabilities of all trees that it contains: The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each action in a (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011): Score(x, d; w) = Score(x, a → d; w) ∑|a| = Score(x, ci , ai ; w) i=1 (6) where ai is the action taken at step i and ci is the configuration status after taking action a1 ...ai−1 . Transition-based methods use inexact beam search to find a highest-scoring action sequence. GN3Par uses a neural network to predict scores of different actions given a state (Chen and Manning, 2014; Andor et al., 2016). First, 48 atomic features are embeded and concatenated as the input layer. Then, two hidden layers are applied to get the scores of all feasible actions. Unlike the tradition"
I17-1006,J14-2001,0,0.0967859,"n achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objective, showing promising results. Meanwhile, it is still unclear how PAs can be used by other main-stream dependency pars"
I17-1006,P99-1010,0,\N,Missing
I17-1006,D14-1082,0,\N,Missing
I17-1006,D07-1101,0,\N,Missing
J10-3009,P06-1067,0,0.0764773,"ith Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the ﬂat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more ﬂexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous g"
J10-3009,1992.tmi-1.8,0,0.355669,"he test corpus when we consider the gap in syntactic reordering patterns. BWR (gap) BWR+LAR (gap) 562 Precision Recall F1 46.28 48.80 44.91 50 45.58 49.39 Xiong et al. Linguistically Annotated Reordering approach, reordering knowledge is included in synchronous rules. The last two categories reorder the source sentence during decoding, which distinguishes them from the ﬁrst approach. Note that some researchers integrate multiple reordering approaches in one decoder (Lin 2004; Quirk, Menezes, and Cherry 2005; Ge, Ittycheriah, and Papineni 2008). 9.1.1 The Preprocessing Approach. In early work, Brown et al. (1992) describe an approach to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a preprocessing approach which automatically learns reordering patterns based on CFG productions. Since then, the preprocessing approach seems to have been more popular. Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees using ﬁne-grained human-written rules, mostly concentrating on VP and NP structures. Li et al. (2007) improve the preprocessing approach by generating"
J10-3009,W07-0718,0,0.0483372,"em outputs in a ﬁne-grained manner with regard to reordering. In their method, common word n-grams occurring in both reference translations and system translations are extracted and generalized to part-ofspeech tag sequences. A recall is calculated for each certain tag sequence to indicate the ability of reordering models to capture this tag sequence in system translations. Popovic et al. (2006) use the relative difference between WER (word error rate) and PER (position independent word error rate) to indicate reordering errors. The larger the difference, the more reordering errors there are. Callison-Burch et al. (2007) propose a constituent-based evaluation that is very similar to our method in Steps (1)–(3). They also parse the source sentence and automatically align the parse tree with the reference/system translations. The difference is that they highlight constituents from the parse tree to enable human evaluation of the translations of these constituents, rather than automatically analyzing constituent movement. They use this method for human evaluation in the shared translation task of the 2007 and 2008 ACL Workshop on Statistical Machine Translation. Fox (2002) systematically studies syntactic cohesi"
J10-3009,P08-1009,0,0.059842,"rst combined with d d d d, which is a sub-phrase of PP preceding VP in an inverted order. The remaining part of the VP phrase is then merged. This merging process continues regardless of the source parse tree. The comparison of BTG trees of BWR+LAR and BWR in the two examples suggests that reordering models should respect syntactic structures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita"
J10-3009,P05-1033,0,0.753435,"tricted by the ITG constraint (Wu 1997). Although it only allows two orders (straight or inverted) of nodes in any binary branching structure, it is broadly veriﬁed that the ITG constraint has good coverage of word reorderings on various language pairs (Wu, Carpuat, and Shen 2006). This makes phrase reordering in phrasal SMT a more tractable task. After enhancing phrasal SMT with a hard hierarchical skeleton, we further inject soft linguistic information into the nodes of the skeleton. We annotate each BTG node 1 In this article, we use Penn Chinese Treebank phrase labels (Xue et al. 2000). 2 Chiang (2005) also generates hierarchical structures in phrasal SMT. One difference is that Chiang’s hierarchical grammar is lexicon-sensitive because the model requires at least one pair of aligned words in each rule except for the “glue rule.” The other difference is that his grammar allows multiple nonterminals. These two differences make Chiang’s grammar more expressive than the BTG but at the cost of learning a larger model. 536 Xiong et al. Linguistically Annotated Reordering with syntactic and lexical elements by projecting the source parse tree onto the BTG binary tree. The challenge, of course, is"
J10-3009,H05-1098,0,0.0193236,"cluded in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax-based translation rules. 9.2 Automatic Analysis of Reordering Although there is a variety of work on phrase reordering, automatic analysis of phrase reordering is not widely explored in the SMT literature. Chiang et al. (2005) propose 563 Computational Linguistics Volume 36, Number 3 an automatic method to compare different system outputs in a ﬁne-grained manner with regard to reordering. In their method, common word n-grams occurring in both reference translations and system translations are extracted and generalized to part-ofspeech tag sequences. A recall is calculated for each certain tag sequence to indicate the ability of reordering models to capture this tag sequence in system translations. Popovic et al. (2006) use the relative difference between WER (word error rate) and PER (position independent word erro"
J10-3009,P05-1066,0,0.0849743,"Missing"
J10-3009,P03-2041,0,0.0375795,"; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more ﬂexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax"
J10-3009,W02-1039,0,0.156118,"rdering when combined with BWR. We want to further study what happens when we combine BWR with LAR. In particular, we want to investigate to what extent the integrated linguistic knowledge (from LAR) changes phrase movement in an actual SMT system, and in what direction the change takes place. The investigations will enable us to have a better understanding of the relationship between phrase movement and linguistic context, and therefore to explore linguistic knowledge more effectively in phrasal SMT. Because syntactic constituents are often moved together across languages during translation (Fox 2002), we particularly study how linguistic knowledge affects syntactic constituent movement. To that end, we introduce a syntax-based analysis method. We parse source sentences, and align the parse trees with reference translations as well as system translations. We then summarize syntactic reordering patterns using contextfree grammar (CFG) rules from the obtained tree-to-string alignments. The extracted reordering patterns clearly show the trace of syntactic constituent movement in both reference translations and system translations. With the proposed analysis method, we analyze the combination"
J10-3009,N04-1035,0,0.0231289,"ctures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a hard constraint on the ITG constraint to allow reorderings which respect the source parse tree. They all report signiﬁcant improvements on different language pairs, which indicates that syntactic cohesion is very useful for phrasal SMT. Our analysis demonstrates that linguistically annotated reordering provides an alte"
J10-3009,W08-0408,0,0.0311117,"Missing"
J10-3009,2006.amta-papers.8,0,0.0332198,"Missing"
J10-3009,W04-3250,0,0.161864,"Missing"
J10-3009,2005.iwslt-1.8,0,0.263134,"submission received: 12 March 2010; accepted for publication: 21 April 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 local word reorderings within phrases. Unfortunately, reordering at the phrase level is still problematic for phrasal SMT. The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases. In order to utilize lexical information for phrase reordering, Tillman (2004) and Koehn et al. (2005) propose lexicalized reordering models which directly condition phrase movement on phrases themselves. One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data. To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering evidence. Although these lexicalized reordering models signiﬁcantly outperform the distortion-based reordering model as reported, only using lexical information (e.g., boundary words) is"
J10-3009,N03-1017,0,0.0454533,"Missing"
J10-3009,H05-1021,0,0.0226242,"ting Phrase Movement with Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the ﬂat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more ﬂexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and"
J10-3009,P07-1091,0,0.066262,"precision/recall for this structure is c/b and c/a, respectively. We can further calculate the F1 -score as 2 × c/(a + b). These syntax-based metrics intuitively show how well the reordering model can reorder this structure. By summarizing all reordering patterns of all constituents, we can obtain an overall precision, recall, and F1 -score for the tested reordering model. This new syntax-based analysis for reordering is motivated in part by recent work which transforms the order of nodes in the source-side parse tree before translation (Xia and McCord 2004; Collins, Koehn, and Kucerova 2005; Li et al. 2007; Wang, Collins, and Koehn 2007). Here we focus on the order transformation of syntactic constituents performed by reordering models during translation. In addition to aligning parse trees with reference translations, we also align parse trees with system translations so that we can learn the movement of syntactic constituents carried out by the reordering models and investigate the performance of the reordering models by comparing both alignments. For notational convenience, we denote syntactic reordering patterns that are extracted from the alignments between source parse trees and reference"
J10-3009,C04-1090,0,0.0184945,"ities of movement with linguistic information. In the third Table 12 Revised overall precision and recall of BWR+LAR vs. BWR on the test corpus when we consider the gap in syntactic reordering patterns. BWR (gap) BWR+LAR (gap) 562 Precision Recall F1 46.28 48.80 44.91 50 45.58 49.39 Xiong et al. Linguistically Annotated Reordering approach, reordering knowledge is included in synchronous rules. The last two categories reorder the source sentence during decoding, which distinguishes them from the ﬁrst approach. Note that some researchers integrate multiple reordering approaches in one decoder (Lin 2004; Quirk, Menezes, and Cherry 2005; Ge, Ittycheriah, and Papineni 2008). 9.1.1 The Preprocessing Approach. In early work, Brown et al. (1992) describe an approach to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a preprocessing approach which automatically learns reordering patterns based on CFG productions. Since then, the preprocessing approach seems to have been more popular. Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees using ﬁne-g"
J10-3009,P06-1077,0,0.0481188,"Missing"
J10-3009,W06-1606,0,0.143376,"n ﬂuent translations for these constituents. However, the allowance of interruptions is sometimes beyond the representability of BTG rules. For example, to solve the lexical divergence problem, bilingual rules with aligned lexicons have to be introduced. To capture reorderings of these constituents, we propose to integrate special reordering rules with richer contextual information into BTG to extend BTG’s ability to deal with interruptions. Completely replacing BTG with richer formalisms, such as hierarchical phrase (Chiang 2005) and tree-to-string (Liu, Liu, and Lin 2006) or string-to-tree (Marcu et al. 2006), introduces a huge extra cost. Instead, integrating a small number of reordering rules into BTG to model reorderings of non-reorderable constituents would be more desirable. 8.5 Discussion In the deﬁnition of syntactic reordering patterns, we only consider the relative order of individual constituents on the target side. We do not consider whether or not they remain contiguous on the target side. It is possible that other words are inserted between spans of two contiguous constituents. We use the term gap to refer to when this happens. The absence of a gap in the deﬁnition of syntactic reorde"
J10-3009,P08-1114,0,0.0620184,"with d d d d, which is a sub-phrase of PP preceding VP in an inverted order. The remaining part of the VP phrase is then merged. This merging process continues regardless of the source parse tree. The comparison of BTG trees of BWR+LAR and BWR in the two examples suggests that reordering models should respect syntactic structures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a"
J10-3009,2001.mtsummit-papers.45,0,0.0347596,"lauses with six types of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees using ﬁne-grained human-written rules, mostly concentrating on VP and NP structures. Li et al. (2007) improve the preprocessing approach by generating n-best reordered source sentences with reordering knowledge automatically learned from the alignments between source parse trees and target translations. The approach proposed in Li et al. also enhances the connection between the preprocessing and decoding by adding a source reordering probability feature. Other approaches introduced in Nießn and Ney (2001), Popovi´c and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological, POS, and chunk knowledge in the preprocessing approach, respectively. 9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the ﬂat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved."
J10-3009,P03-1021,0,0.0120847,"Missing"
J10-3009,P00-1056,0,0.0905788,"TG-based phrasal SMT system, developed following Section 2. We integrate the boundary word–based reordering model and the linguistically annotated reordering model into our system according to our reordering conﬁguration. We carried out various experiments to evaluate the reordering example extraction algorithms of Section 3, the linguistically annotated reordering model vs. boundary word–based reordering model, and the effects of linguistically annotated features on the Chinese-toEnglish translation task of the NIST MT-05 using large scale training data. 7.1 Experimental Setup We ran GIZA++ (Och and Ney 2000) on the parallel corpora (consisting of 101.93M Chinese words and 112.78M English words) listed in Table 2 in both directions and then applied the “grow-diag-ﬁnal” reﬁnement rule (Koehn, Och, and Marcu 2003) to 550 Xiong et al. Linguistically Annotated Reordering Table 2 Corpora used. Corpus LDC catalog Chinese words English words United Nations Hong Kong News Sinorama Magazine FBIS Xinhua Chinese News Translation Chinese Treebank Multiple Translation Chinese LDC2004E12 LDC2004T08 LDC2005T10 LDC2003E14 LDC2002E18 LDC2005T06 LDC2003E07 LDC2004T07 68.63M 15.07M 10.26M 7.09M 0.40M 0.28M 0.10M 0.1"
J10-3009,P02-1040,0,0.0831864,"ITG constraint. We call this two-step phrase reordering strategy linguistically annotated reordering (LAR) (Xiong et al. 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reordering strategy based on BTG. However, they use boundary words as reordering features at the second step. To distinguish this from our work, we call their approach boundary word–based reordering (BWR). LAR and BWR can be considered as two reordering variants for BTG-based phrasal SMT, which have similar training procedures. Furthermore, they can be combined. We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al. 2002). The BLEU scores show that LAR is comparable to BWR and signiﬁcantly improves phrase reordering when combined with BWR. We want to further study what happens when we combine BWR with LAR. In particular, we want to investigate to what extent the integrated linguistic knowledge (from LAR) changes phrase movement in an actual SMT system, and in what direction the change takes place. The investigations will enable us to have a better understanding of the relationship between phrase movement and linguistic context, and therefore to explore linguistic knowledge more effectively in phrasal SMT. Beca"
J10-3009,W06-3101,0,0.344324,"Missing"
J10-3009,popovic-ney-2006-pos,0,0.273344,"The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases. In order to utilize lexical information for phrase reordering, Tillman (2004) and Koehn et al. (2005) propose lexicalized reordering models which directly condition phrase movement on phrases themselves. One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data. To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering evidence. Although these lexicalized reordering models signiﬁcantly outperform the distortion-based reordering model as reported, only using lexical information (e.g., boundary words) is not adequate to move phrases to appropriate positions. Consider the following Chinese example with its English translation: [VP [PP d(while) dd(develop) dd(related) dd(legislation) d] [VP [VV d d(consider)] [NP [DNP [NP dd(this) dddd(referendum)] [DEG d(of)]] [NP d d(results)]]]]1 consider th"
J10-3009,P05-1034,0,0.0719418,"Missing"
J10-3009,N04-4026,0,0.654536,"eived: 24 October 2008; revised submission received: 12 March 2010; accepted for publication: 21 April 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 local word reorderings within phrases. Unfortunately, reordering at the phrase level is still problematic for phrasal SMT. The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases. In order to utilize lexical information for phrase reordering, Tillman (2004) and Koehn et al. (2005) propose lexicalized reordering models which directly condition phrase movement on phrases themselves. One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data. To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering evidence. Although these lexicalized reordering models signiﬁcantly outperform the distortion-based reordering model as reported, only using lexical information ("
J10-3009,D07-1077,0,0.0334249,"Missing"
J10-3009,P96-1021,0,0.353417,"important and challenging tasks in building a BTG-based phrasal SMT system is to deﬁne P(r m ). 2.2 Reordering Under the ITG Constraint Under the ITG constraint, three nodes {Al , Ar , Ap } are involved when we consider the order o between the two children {Al , Ar } in any binary subtrees. Therefore it is natural to deﬁne the ITG reordering P(r m ) as a function as follows: P(rm ) = f (Al , Ar , Ap , o) (5) where o ∈ {straight, inverted}. Based on this function, various reordering models are built according to different assumptions. For example, the ﬂat reordering model in the original BTG (Wu 1996) assigns prior probabilities for the straight and inverted order assuming the order is highly related to the properties of language pairs. It is formulated as  m P(r ) = ps , o = straight 1 − ps , o = inverted (6) Supposing French and English are the source and target language, respectively, the value of ps can be set as high as 0.8 to prefer monotone orientations because the two languages have similar word orders in most cases. The main problem of the ﬂat reordering model is also the problem of the standard distortion model (Koehn, Och, and Marcu 2003): Neither model considers linguistic con"
J10-3009,J97-3002,0,0.787827,"ch succeeding phrase should be translated ﬁrst. If high-level linguistic knowledge, such as the syntactic context VP→PP VP, is given, the position of the PP phrase can be easily determined since the pre-verbal modiﬁer PP in Chinese is frequently translated into a post-verbal counterpart in English. In this article, we focus on linguistically motivated phrase reordering, which integrates high-level linguistic knowledge in phrase reordering. We adopt a two-step strategy. In the ﬁrst step, we establish a hierarchical skeleton in phrasal SMT by incorporating Bracketing Transduction Grammar (BTG) (Wu 1997) into phrasal SMT. In the second step, we inject soft linguistic information into nodes of the skeleton. There are two signiﬁcant advantages to using BTG in phrasal SMT. First, BTG is able to generate hierarchical structures.2 This not only enhances phrasal SMT’s capability for hierarchical and long-distance reordering but also establishes a platform for phrasal SMT to incorporate knowledge from linguistic structure. Second, phrase reordering is restricted by the ITG constraint (Wu 1997). Although it only allows two orders (straight or inverted) of nodes in any binary branching structure, it i"
J10-3009,C04-1073,0,0.197301,"4 October 2008; revised submission received: 12 March 2010; accepted for publication: 21 April 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 local word reorderings within phrases. Unfortunately, reordering at the phrase level is still problematic for phrasal SMT. The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases. In order to utilize lexical information for phrase reordering, Tillman (2004) and Koehn et al. (2005) propose lexicalized reordering models which directly condition phrase movement on phrases themselves. One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data. To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering evidence. Although these lexicalized reordering models signiﬁcantly outperform the distortion-based reordering model as reported, only using lexical information ("
J10-3009,I05-1007,1,0.889647,"Missing"
J10-3009,P06-1066,1,0.924664,"Missing"
J10-3009,P08-2038,1,0.858008,". The challenge, of course, is that BTG hierarchical structures are not always aligned with the linguistic structures in the source language parse tree. To address this issue, we propose an annotation algorithm. The algorithm is able to label any BTG nodes during decoding with very little overhead, regardless of whether the BTG nodes are aligned with syntactic constituent nodes in the source parse tree. The annotated linguistic elements are then used to guide phrase reordering under the ITG constraint. We call this two-step phrase reordering strategy linguistically annotated reordering (LAR) (Xiong et al. 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reordering strategy based on BTG. However, they use boundary words as reordering features at the second step. To distinguish this from our work, we call their approach boundary word–based reordering (BWR). LAR and BWR can be considered as two reordering variants for BTG-based phrasal SMT, which have similar training procedures. Furthermore, they can be combined. We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al. 2002). The BLEU scores show that LAR is comparable to BWR and signiﬁcantly improves phrase reordering when co"
J10-3009,I08-1066,1,0.939151,". The challenge, of course, is that BTG hierarchical structures are not always aligned with the linguistic structures in the source language parse tree. To address this issue, we propose an annotation algorithm. The algorithm is able to label any BTG nodes during decoding with very little overhead, regardless of whether the BTG nodes are aligned with syntactic constituent nodes in the source parse tree. The annotated linguistic elements are then used to guide phrase reordering under the ITG constraint. We call this two-step phrase reordering strategy linguistically annotated reordering (LAR) (Xiong et al. 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reordering strategy based on BTG. However, they use boundary words as reordering features at the second step. To distinguish this from our work, we call their approach boundary word–based reordering (BWR). LAR and BWR can be considered as two reordering variants for BTG-based phrasal SMT, which have similar training procedures. Furthermore, they can be combined. We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al. 2002). The BLEU scores show that LAR is comparable to BWR and signiﬁcantly improves phrase reordering when co"
J10-3009,P01-1067,0,0.0235166,"and BWR in the two examples suggests that reordering models should respect syntactic structures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a hard constraint on the ITG constraint to allow reorderings which respect the source parse tree. They all report signiﬁcant improvements on different language pairs, which indicates that syntactic cohesion is very useful for phrasal SMT."
J10-3009,W08-0401,0,0.0240288,"Missing"
J10-3009,P03-1019,0,0.0283626,"n-best reordered source sentences with reordering knowledge automatically learned from the alignments between source parse trees and target translations. The approach proposed in Li et al. also enhances the connection between the preprocessing and decoding by adding a source reordering probability feature. Other approaches introduced in Nießn and Ney (2001), Popovi´c and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological, POS, and chunk knowledge in the preprocessing approach, respectively. 9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the ﬂat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more ﬂ"
J10-3009,D07-1056,0,0.0189614,"ITG constraint, the corresponding model is the ﬂat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more ﬂexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in sy"
J10-3009,P05-1059,0,0.0165509,"caused by the use of heuristic selection rules: keeping some block pairs as reordering examples while abandoning other block pairs. The kept block pairs are not necessarily the best training instances for tuning an ITG order predictor. To avoid this problem we can extract reordering examples from the BTG trees of sentence pairs. Reordering examples extracted in this way are naturally suitable for BTG order prediction. There are various ways to build BTG trees over sentence pairs. One can use BTG to produce bilingual parses of sentence pairs, similar to the approaches proposed by Wu (1997) and Zhang and Gildea (2005) but using the more sophisticated reordering models BWR or LAR. After parsing, reordering examples can be extracted from bilingual parse trees and a better reordering model is therefore induced from the extracted reordering examples. Using the better reordering model, the bilingual sentences are parsed again. This procedure is run iteratively until no performance gain is obtained in terms of translation or parsing accuracy. Formally, we can use expectation-maximization (EM) training in this procedure. In the expectation step, we ﬁrst estimate the likelihood of all BTG trees of sentence pairs w"
J10-3009,C08-1136,0,0.013257,"tactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax-based translation rules. 9.2 Automatic Analysis of Reordering Although there is a variety of work on phrase reordering, automatic analysis of phrase reordering is not widely explored in the SMT literature. Chiang et al. (2005) propose 563 Computational Linguistics Volume 36, Number 3 an automatic method to compare different system outputs in a ﬁne-grained manner with regard to reordering. In their method, common word n-grams occurring in both reference translations and"
J10-3009,P08-1064,1,0.846608,"tactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax-based translation rules. 9.2 Automatic Analysis of Reordering Although there is a variety of work on phrase reordering, automatic analysis of phrase reordering is not widely explored in the SMT literature. Chiang et al. (2005) propose 563 Computational Linguistics Volume 36, Number 3 an automatic method to compare different system outputs in a ﬁne-grained manner with regard to reordering. In their method, common word n-grams occurring in both reference translations and"
J10-3009,W07-0401,0,0.0144484,"ITG constraint, the corresponding model is the ﬂat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more ﬂexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in sy"
J10-3009,2008.iwslt-evaluation.2,0,0.0426999,"Missing"
J10-4011,D09-1051,0,0.0118921,"carefully tailored rules to identify collocations of different types, which may potentially achieve better results, but the method turns out to be less generalizable. This language-speciﬁc approach corresponds to the characteristics of Chinese collocations. The chapter would have been more helpful if it had included the work on collocativity measures based on limited modiﬁability with only shallow parsing (Wermter and Hahn 2004), statistical methods using accurate collocational information based on full parsing (Seretan and Wehrli 2006), and the more recent monolingual word alignment method (Liu et al. 2009). The appendix provides a comprehensive and categorized list of linguistic resources. The references are generally complete and helpful to readers interested in further studies. In summary, the book is clear and self-contained, discussing fundamental and critical issues in Chinese NLP. As the foundation of any NLP technology, morphological analysis (where Chinese morphology differs most apparently from other languages) is the ﬁrst crucial procedure prior to syntactic and semantic processing. Although it would be more integrated to include syntactic and semantic analysis, I do believe the autho"
J10-4011,P06-1120,0,0.012878,"curacies caused by insufﬁcient statistics; a categorical approach adopts a set of carefully tailored rules to identify collocations of different types, which may potentially achieve better results, but the method turns out to be less generalizable. This language-speciﬁc approach corresponds to the characteristics of Chinese collocations. The chapter would have been more helpful if it had included the work on collocativity measures based on limited modiﬁability with only shallow parsing (Wermter and Hahn 2004), statistical methods using accurate collocational information based on full parsing (Seretan and Wehrli 2006), and the more recent monolingual word alignment method (Liu et al. 2009). The appendix provides a comprehensive and categorized list of linguistic resources. The references are generally complete and helpful to readers interested in further studies. In summary, the book is clear and self-contained, discussing fundamental and critical issues in Chinese NLP. As the foundation of any NLP technology, morphological analysis (where Chinese morphology differs most apparently from other languages) is the ﬁrst crucial procedure prior to syntactic and semantic processing. Although it would be more inte"
J10-4011,C04-1141,0,0.0321556,"a semantic approach exploits semantic relations such as synonyms obtained from WordNet to overcome the inaccuracies caused by insufﬁcient statistics; a categorical approach adopts a set of carefully tailored rules to identify collocations of different types, which may potentially achieve better results, but the method turns out to be less generalizable. This language-speciﬁc approach corresponds to the characteristics of Chinese collocations. The chapter would have been more helpful if it had included the work on collocativity measures based on limited modiﬁability with only shallow parsing (Wermter and Hahn 2004), statistical methods using accurate collocational information based on full parsing (Seretan and Wehrli 2006), and the more recent monolingual word alignment method (Liu et al. 2009). The appendix provides a comprehensive and categorized list of linguistic resources. The references are generally complete and helpful to readers interested in further studies. In summary, the book is clear and self-contained, discussing fundamental and critical issues in Chinese NLP. As the foundation of any NLP technology, morphological analysis (where Chinese morphology differs most apparently from other langu"
J10-4011,I05-1047,0,0.0605814,"Missing"
K16-2021,P09-2004,0,0.065353,"re “0” means that Arg1 locates at the same sentence containing CP, “1” means that Arg1 is in the previous sentence of the sentence containing CP, and so on. We throw instances in which Arg1 or Arg2 locates at multiple sentences. 4 Train 14231 (0) 364 (44) 70 (18) 57(22) and perform dynamic programming based search from right to left. For simplicity, we set the window size to 6, meaning that the model considers at most six sentences, from the 0th sentence containing CP, to the 5th sentence in front. For the features, we directly adopt those described in Lin et al. (2014), Pitler et al. (2009), Pitler and Nenkova (2009), and Knott (1996). Especially, we design a three-tag label set in order to enforce the model to return exactly one sentence with Arg1. Explicit-Arg1 Sentence Locator: Sequence Labeling As far as we know, most previous participating systems last year assume that Arg1 lies in the same sentence or the previous sentence of CP. However, we find that there exist many cases that Arg1 locates at longer-distance sentences from the CP. Table 3 shows data statistics regarding the sentencelevel distance of Arg1 and CP. We also find that there are cases that Arg1 locates at more than one sentences, and th"
K16-2021,P09-1077,0,0.0629195,"nce containing CP, where “0” means that Arg1 locates at the same sentence containing CP, “1” means that Arg1 is in the previous sentence of the sentence containing CP, and so on. We throw instances in which Arg1 or Arg2 locates at multiple sentences. 4 Train 14231 (0) 364 (44) 70 (18) 57(22) and perform dynamic programming based search from right to left. For simplicity, we set the window size to 6, meaning that the model considers at most six sentences, from the 0th sentence containing CP, to the 5th sentence in front. For the features, we directly adopt those described in Lin et al. (2014), Pitler et al. (2009), Pitler and Nenkova (2009), and Knott (1996). Especially, we design a three-tag label set in order to enforce the model to return exactly one sentence with Arg1. Explicit-Arg1 Sentence Locator: Sequence Labeling As far as we know, most previous participating systems last year assume that Arg1 lies in the same sentence or the previous sentence of CP. However, we find that there exist many cases that Arg1 locates at longer-distance sentences from the CP. Table 3 shows data statistics regarding the sentencelevel distance of Arg1 and CP. We also find that there are cases that Arg1 locates at more"
K16-2021,W02-1001,0,0.233326,"lows previous practice and employs a cas∗ Train 2000 17619 38967 14722 17813 Dev 100 783 1675 680 756 Table 1: Data statistics of English. caded framework and comprises 9 components, as shown in Figure 2. In the following, we will introduce each component in detail. The codes are released at http://hlt.suda.edu.cn/ ˜zhli for future research study. 2 Classification and Sequence Labeling Based on Linear Model In this work, we implement our classification and sequence labeling models based on linear model due to its simplicity and good performance on variety of natural language processing tasks (Collins, 2002). Given an input instance x and a label y, a linear model defines the score of labeling x as y: Score(x, y) = w · f (x, y) Correspondence author. 150 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 150–157, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Figure 3: Example of a parse tree from which we extract features. Figure 2: Framework of our system. 3 where f (.) is a feature vector constructed according to a hand-crafted feature template list and w is the corresponding feature weight vector. The decoding t"
K16-2021,prasad-etal-2008-penn,0,0.218986,"s paper descirbes our participating system for CoNLL-2016 discourse parsing shared task (Xue et al., 2016). We participate in the closed track, and due to the time limitation, we focus on English. Given an document, which contains several paragraphs and each paragraph is composed of a few sentences, discourse parsing aims to identify explicit and non-explict discourse relations, including explicit connnective phrases (CP), explicit/non-explicit arguments and senses. Figure 1 presents a graphical illustration of the task. Following the official requirement, we use Section 2-21 of the PDTB 2.0 (Prasad et al., 2008; Prasad et al., 2014) as the training data, Section 22 as the development data, and Section 23 as the test data. A blind test is also used for evaluation. Table 1 presents the data statistics. Due to the complexity of the task, our system follows previous practice and employs a cas∗ Train 2000 17619 38967 14722 17813 Dev 100 783 1675 680 756 Table 1: Data statistics of English. caded framework and comprises 9 components, as shown in Figure 2. In the following, we will introduce each component in detail. The codes are released at http://hlt.suda.edu.cn/ ˜zhli for future research study. 2 Class"
K16-2021,J14-4007,0,0.0532068,"participating system for CoNLL-2016 discourse parsing shared task (Xue et al., 2016). We participate in the closed track, and due to the time limitation, we focus on English. Given an document, which contains several paragraphs and each paragraph is composed of a few sentences, discourse parsing aims to identify explicit and non-explict discourse relations, including explicit connnective phrases (CP), explicit/non-explicit arguments and senses. Figure 1 presents a graphical illustration of the task. Following the official requirement, we use Section 2-21 of the PDTB 2.0 (Prasad et al., 2008; Prasad et al., 2014) as the training data, Section 22 as the development data, and Section 23 as the test data. A blind test is also used for evaluation. Table 1 presents the data statistics. Due to the complexity of the task, our system follows previous practice and employs a cas∗ Train 2000 17619 38967 14722 17813 Dev 100 783 1675 680 756 Table 1: Data statistics of English. caded framework and comprises 9 components, as shown in Figure 2. In the following, we will introduce each component in detail. The codes are released at http://hlt.suda.edu.cn/ ˜zhli for future research study. 2 Classification and Sequence"
K16-2021,E14-1068,0,0.100065,"Missing"
K16-2021,K15-2004,0,0.12151,"utational Linguistics Figure 3: Example of a parse tree from which we extract features. Figure 2: Framework of our system. 3 where f (.) is a feature vector constructed according to a hand-crafted feature template list and w is the corresponding feature weight vector. The decoding task in the linear model is to find the maximum-scoring label: CP Identification Given an input document, the first task is to extract all connective phrases (CPs) (e.g., “so that”) in the document,1 which we refer to as CP identification. We directly adopt the method described in previous works (Wang and Lan, 2015; Kong et al., 2015), and take two steps for this task. yˆ = arg max Score(x, y) y 1. Candidate CP extraction. We extract all candidate CPs in the input document by exact matching with a phrase dictionary. If a string in a sentence exactly matchs a phrase in the dictionary, it then is considered as a candidate CP and will be verified in the second step. The dictionary is provided by the official organizer and contains 100 phrases. To learn w, we use the standard online training procedure, which use one instance for feature weight update at a time: w(t+1) = w(t) + f (x, y ∗ ) − f (x, yˆ) where t is the global time"
K16-2021,K15-2003,0,0.0145208,"Arg2 locates at the the same sentence with CP. Therefore, based on the results of Arg1 sentence locator, we have two cases to handle: Arg1 and Arg2 locate at the same sentence with CP (SS), or Arg1 locates at a previous sentence of CP (PS). Then, we use three sequence labeling models to locate the exact words of Arg1/2. All three models perform at the level of words, and each time assign a “Arg1/Arg2/None” tag to a word. Many systems in CoNLL-2015 (Xue et al., 2015) evaluation also treat Arg1/2 word location as a sequence labeling problem, and uses conditional random filed (CRF) based models (Stepanov et al., 2015; Nguyen et al., 2015; Lalitha Devi et al., 2015) or recurrent neural networks (RNN) (Wang et al., 2015). 5.1 True Positive 16940 718 7 Non-explicit Sense Classification After processing the explicit relations, we then turn to the problem of non-explicit relation parsing. As suggested by the official organizer, if two adjacent sentences do not have explicit relation after previous processing, we consider them as a candidate sentence pair having non-explicit relation. Please note that we only consider sentence pairs that are in the same paragraph. As far as we know, most previous work directly"
K16-2021,K15-2007,0,0.0348038,"Missing"
K16-2021,K15-2002,0,0.500051,"Association for Computational Linguistics Figure 3: Example of a parse tree from which we extract features. Figure 2: Framework of our system. 3 where f (.) is a feature vector constructed according to a hand-crafted feature template list and w is the corresponding feature weight vector. The decoding task in the linear model is to find the maximum-scoring label: CP Identification Given an input document, the first task is to extract all connective phrases (CPs) (e.g., “so that”) in the document,1 which we refer to as CP identification. We directly adopt the method described in previous works (Wang and Lan, 2015; Kong et al., 2015), and take two steps for this task. yˆ = arg max Score(x, y) y 1. Candidate CP extraction. We extract all candidate CPs in the input document by exact matching with a phrase dictionary. If a string in a sentence exactly matchs a phrase in the dictionary, it then is considered as a candidate CP and will be verified in the second step. The dictionary is provided by the official organizer and contains 100 phrases. To learn w, we use the standard online training procedure, which use one instance for feature weight update at a time: w(t+1) = w(t) + f (x, y ∗ ) − f (x, yˆ) where"
K16-2021,K15-2014,0,0.149924,"Missing"
K16-2021,K15-2001,0,0.134588,"ve 4850 200 Table 7: Distribution of adjacent sentences having non-explicit relation. Data statistics show that for explicit relations, nearly all Arg2 locates at the the same sentence with CP. Therefore, based on the results of Arg1 sentence locator, we have two cases to handle: Arg1 and Arg2 locate at the same sentence with CP (SS), or Arg1 locates at a previous sentence of CP (PS). Then, we use three sequence labeling models to locate the exact words of Arg1/2. All three models perform at the level of words, and each time assign a “Arg1/Arg2/None” tag to a word. Many systems in CoNLL-2015 (Xue et al., 2015) evaluation also treat Arg1/2 word location as a sequence labeling problem, and uses conditional random filed (CRF) based models (Stepanov et al., 2015; Nguyen et al., 2015; Lalitha Devi et al., 2015) or recurrent neural networks (RNN) (Wang et al., 2015). 5.1 True Positive 16940 718 7 Non-explicit Sense Classification After processing the explicit relations, we then turn to the problem of non-explicit relation parsing. As suggested by the official organizer, if two adjacent sentences do not have explicit relation after previous processing, we consider them as a candidate sentence pair having"
K16-2021,K16-2001,0,0.0403783,"ee are treated as classification problems. All our sequence labeling and classification models are implemented based on linear models with averaged perceptron training. Our feature sets are mostly borrowed from previous works. The main focus of our effort is to recall cases when Arg1 locates at sentences far before the connective phrase, with some yet limited success. 1 Figure 1: Illustration of discourse parsing. Document Paragraph Sentence Explicit relations Non-explicit relations General Description This paper descirbes our participating system for CoNLL-2016 discourse parsing shared task (Xue et al., 2016). We participate in the closed track, and due to the time limitation, we focus on English. Given an document, which contains several paragraphs and each paragraph is composed of a few sentences, discourse parsing aims to identify explicit and non-explict discourse relations, including explicit connnective phrases (CP), explicit/non-explicit arguments and senses. Figure 1 presents a graphical illustration of the task. Following the official requirement, we use Section 2-21 of the PDTB 2.0 (Prasad et al., 2008; Prasad et al., 2014) as the training data, Section 22 as the development data, and Se"
K16-2021,miltsakaki-etal-2004-penn,0,\N,Missing
K16-2021,K15-2010,0,\N,Missing
K19-2014,hajic-etal-2012-announcing,0,0.40014,"Missing"
K19-2014,W12-3602,0,0.222709,"a Li2∗, Min Zhang2 1 Alibaba Group, China 2 School of Computer Science and Technology, Soochow University, China {shiyu.zy, junjie.junjiecao, masi.wr}@alibaba-inc.com wjiang0501@stu.suda.edu.cn, kirosummer.nlp@gmail.com {zhli13, minzhang}@suda.edu.cn Abstract Semantic Dependency Parsing (SDP) aims to parse the predicate-argument relationships for all words in the input sentence, leading to bilexical semantic dependency graphs (Oepen et al., 2014, 2015, 2016). This shared task focuses on two different formal types of SDP representations, i.e., DELPH-IN MRS Bi-Lexical Dependencies (abbr. as DM, Ivanova et al., 2012) and Prague Semantic Dependencies (abbr. as PSD, Hajiˇc et al., 2012; Miyao et al., 2014). They are both classified as Flavor 0 representations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the"
K19-2014,S19-2002,1,0.758372,"istic framework (Flavor 1) firstly proposed by Abend and Rappoport (2013). In UCCA graphs, input words are leaf (or terminal) nodes. One non-terminal node governs one or more nodes, which may be discontinuous; and one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and other remote edges. Relationships between nodes are given by edge labels. The primary edges form a tree structure, whereas the remote edges introduce reentrancy, forming directed acyclic graphs (DAGs).2 We directly adopt the previous graph-based UCCA parser proposed by Jiang et al. (2019), treating UCCA graph parsing as constituent parsing and remote edge recovery under the MTL framework. In this paper, we describe our participating systems in the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2019 Conference for Computational Language Learning (CoNLL). The task includes five frameworks for graph-based meaning representations, i.e., DM, PSD, EDS, UCCA, and AMR. One common characteristic of our systems is that we employ graph-based methods instead of transition-based methods when predicting edges between nodes. For SDP, we jointly perform edge predict"
K19-2014,N16-1030,0,0.0442481,"ds SDP We construct our SDP parser based on the ideas of Dozat and Manning (2017) and Dozat and Manning (2018). Note that lemmas, POS tags and frames are also included in the MRP evaluation metrics, so our method is a bit different from Dozat and Manning (2018). Edge Prediction. Our basic edge prediction model is similar to the Dozat and Manning (2017) and Dozat and Manning (2018). The input words are first mapped into a dense vector composed by pretrained word embeddings and character-level features. xi = eword ⊕ echar i i where echar is extracted by the bidirectional i character-level LSTM (Lample et al., 2016). They are then fed into a multilayer bidirectional wordlevel LSTM to get contextualized representations. Finally, two modules are applied to predict edges. One is to predict whether or not a directed edge exists between two words (keeping the edges between pairs of words with positive scores); and the other is to predict the most probable label for each potential edge (choosing the label with maximum score). Each of them has two seperate MLPs for head and dependent representations and a biaffine layer for scoring. The training loss is the sum of sigmoid cross-entropy loss for edges and softma"
K19-2014,P13-1023,0,0.187952,"entations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the MTL framework. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework (Flavor 1) firstly proposed by Abend and Rappoport (2013). In UCCA graphs, input words are leaf (or terminal) nodes. One non-terminal node governs one or more nodes, which may be discontinuous; and one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and other remote edges. Relationships between nodes are given by edge labels. The primary edges form a tree structure, whereas the remote edges introduce reentrancy, forming directed acyclic graphs (DAGs).2 We directly adopt the previous graph-based UCCA parser proposed by Jiang et al. (2019), treating UCCA graph parsing as constituent parsing a"
K19-2014,P18-1037,0,0.437775,"ency SRL (semantic role labeling) to produce nodes. For the edge prediction, the widely-used Biaffine model is used. Abstract meaning representation (AMR), proposed by Banarescu et al. (2013), is a broadcoverage sentence-level semantic formalism (Flavor 2) to encode the meaning of natural language sentences. AMR can be regarded as a rooted labeled directed acyclic graph. Nodes in AMR graphs represent concepts, and labeled directed edges are relations between the concepts. Due to the time limitation and the complexity of the AMR parsing problem, we directly employ the stateof-the-art parser of Lyu and Titov (2018), which treats AMR parsing as a graph prediction problem. Methodology Summarization. Our participating systems can be characterized in the following aspects: under the MTL framework and it is adopted by the DM, PSD, and UCCA models. For both EDS and AMR, we first produce nodes and then predict edges in a pipeline architecture. We have not attempted to jointly solve multiple semantic frameworks via MTL yet. • BERT. We observe that using BERT as our extra inputs is effective for all the models, except AMR. It is also interesting that BERTlarge does not produce more improvements over BERT-base ba"
K19-2014,W13-2322,0,0.464322,"rings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence labeling. To tackle with the explicit, many-to-many relationship between nodes and sub-strings of the underlying sentence (via anchoring), we introduce a similar method used in dependency SRL (semantic role labeling) to produce nodes. For the edge prediction, the widely-used Biaffine model is used. Abstract meaning representation (AMR), proposed by Banarescu et al. (2013), is a broadcoverage sentence-level semantic formalism (Flavor 2) to encode the meaning of natural language sentences. AMR can be regarded as a rooted labeled directed acyclic graph. Nodes in AMR graphs represent concepts, and labeled directed edges are relations between the concepts. Due to the time limitation and the complexity of the AMR parsing problem, we directly employ the stateof-the-art parser of Lyu and Titov (2018), which treats AMR parsing as a graph prediction problem. Methodology Summarization. Our participating systems can be characterized in the following aspects: under the MTL"
K19-2014,S14-2056,0,0.253914,"how University, China {shiyu.zy, junjie.junjiecao, masi.wr}@alibaba-inc.com wjiang0501@stu.suda.edu.cn, kirosummer.nlp@gmail.com {zhli13, minzhang}@suda.edu.cn Abstract Semantic Dependency Parsing (SDP) aims to parse the predicate-argument relationships for all words in the input sentence, leading to bilexical semantic dependency graphs (Oepen et al., 2014, 2015, 2016). This shared task focuses on two different formal types of SDP representations, i.e., DELPH-IN MRS Bi-Lexical Dependencies (abbr. as DM, Ivanova et al., 2012) and Prague Semantic Dependencies (abbr. as PSD, Hajiˇc et al., 2012; Miyao et al., 2014). They are both classified as Flavor 0 representations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the MTL framework. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguis"
K19-2014,P17-1112,0,0.0478012,"amework and followed by our corresponding approaches. ∗ 1 2 The full UCCA scheme also has implicit nodes and linkage relations, which are excluded in the shared task. Corresponding author http://mrp.nlpl.eu/index.php?page=1 149 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 149–157 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2014 Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor 1) proposed by Oepen and Lønning (2006). Buys and Blunsom (2017) introduce a neural encoder-decoder transition-based model to obtain the EDS graph. They use external knowledge to generate nodes3 . Chen et al. (2018) introduce a novel SHRG (Synchronous Hyperedge Replacement Grammar) extraction algorithm which requires a syntactic tree and alignments between conceptual edges and surface strings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence labeling. To tackle with"
K19-2014,K19-2001,0,0.157991,"Missing"
K19-2014,P18-1038,0,0.0536348,"ared task. Corresponding author http://mrp.nlpl.eu/index.php?page=1 149 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 149–157 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2014 Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor 1) proposed by Oepen and Lønning (2006). Buys and Blunsom (2017) introduce a neural encoder-decoder transition-based model to obtain the EDS graph. They use external knowledge to generate nodes3 . Chen et al. (2018) introduce a novel SHRG (Synchronous Hyperedge Replacement Grammar) extraction algorithm which requires a syntactic tree and alignments between conceptual edges and surface strings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence labeling. To tackle with the explicit, many-to-many relationship between nodes and sub-strings of the underlying sentence (via anchoring), we introduce a similar method used in"
K19-2014,L16-1630,0,0.0887716,"Missing"
K19-2014,S15-2153,0,0.651634,"Missing"
K19-2014,P18-2077,0,0.232099,"ency graphs (Oepen et al., 2014, 2015, 2016). This shared task focuses on two different formal types of SDP representations, i.e., DELPH-IN MRS Bi-Lexical Dependencies (abbr. as DM, Ivanova et al., 2012) and Prague Semantic Dependencies (abbr. as PSD, Hajiˇc et al., 2012; Miyao et al., 2014). They are both classified as Flavor 0 representations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the MTL framework. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework (Flavor 1) firstly proposed by Abend and Rappoport (2013). In UCCA graphs, input words are leaf (or terminal) nodes. One non-terminal node governs one or more nodes, which may be discontinuous; and one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and othe"
K19-2014,S14-2008,0,0.350225,"Missing"
K19-2014,oepen-lonning-2006-discriminant,0,0.184117,"ef introduction of each framework and followed by our corresponding approaches. ∗ 1 2 The full UCCA scheme also has implicit nodes and linkage relations, which are excluded in the shared task. Corresponding author http://mrp.nlpl.eu/index.php?page=1 149 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 149–157 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2014 Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor 1) proposed by Oepen and Lønning (2006). Buys and Blunsom (2017) introduce a neural encoder-decoder transition-based model to obtain the EDS graph. They use external knowledge to generate nodes3 . Chen et al. (2018) introduce a novel SHRG (Synchronous Hyperedge Replacement Grammar) extraction algorithm which requires a syntactic tree and alignments between conceptual edges and surface strings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence"
K19-2014,D14-1162,0,0.096733,"rs. Concept Identification Model. The concept identification model chooses a concept c conditioned on the aligned word k based on the BiLSTM state hk , which is defined as Pθ (c|hk , wk ). For more details about the re-categorization and candidate concept, please refer to Lyu and Titov (2018). Relation Identification Model. The relation identification model is arc-factored as: Pφ (R|a, c, w) = m Y 3 Experiments This section describes model parameters used in our models, and the overall results of all the five tasks. 3.1 Model Parameters In both SDP and UCCA tasks, we use 100dimensional GloVe (Pennington et al., 2014) as pretrained embedding and random initialized 50dimensional char embedding. The char lstm output is 100-dimensional. We also utilize the BERT embeddings extracted from the last four transformer layers. The final BERT representation is their normalized weighted sum, which is concatenated with the word embeddings. The other parameters are the same with the previous works (Dozat and Manning, 2018; Jiang et al., 2019). In EDS task, external resources we use are: 1) word embeddings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese; and 2) BERT 10 (Devlin et al.,"
K19-2014,P17-1076,0,0.0287443,"MLP MLP Shared BiLSTMs ... xi ... Figure 2: An example of newest version of UCCA. Figure 1: The framework of our SDP Parser. To handle discontinuous node, we trace bottomup from a discontinuous leaf node until we find the specific node whose parent is the lowest common ancestor (LCA) of the discontinuous node and leaf node. Finally we move the edge to make the specific node become the child of the discontinuous, with “-ancestor” added behind the edge label. Please refer to Jiang et al. (2019) for more conversion details. Constituent Parsing. We directly adopt the minimal span-based parser of Stern et al. (2017). Given an input sentence X = {x0 , x1 , · · · , xn }, each word xi is mapped into a dense vector xi and fed into bidirectional LSTM layers. The top-layer output of each position are used to represent the span as cross-entropy loss for labels. 0 ` = `label + `edge Lexical Taggers. This SDP task is more difficult than the ealier 2014 and 2015 SDP tasks (Oepen et al., 2014, 2015), since the gold tokenization result, lemmas, and POS tags are not available in the parser input data and the predictions are parts of the MRP evaluation metrics. We use automatic tokenization result and lemmas provided"
L18-1706,H05-1091,0,0.308427,"Missing"
L18-1706,J81-4005,0,0.735931,"Missing"
L18-1706,P82-1020,0,0.832646,"Missing"
L18-1706,D15-1064,0,0.0503344,"Missing"
L18-1706,P15-1033,0,0.0548731,"ime t. ? , ? , ? , ? denote the weight matrices of different gates for input ? , and ? , ? , ? , ? are the weight matrices for hidden state ℎ . ? , ? , ? , ? denote the bias vectors. It should be noted that we do not include peephole connections (Gers et al., 2003) in the our LSTM formulation. 3.2.2 Bi-LSTM For many sequence labeling tasks it is beneficial to have access to both past (left) and future (right) contexts. However, the LSTM’s hidden state ℎ takes information only from past, knowing nothing about the future. An elegant solution whose effectiveness has been proven by previous work (Dyer et al., 2015) is bi-directional LSTM (Bi-LSTM). The basic idea is to present each sequence forwards and backwards to two separate hidden states to capture past and future information, respectively. Then the two hidden states are concatenated to form the final output. We treat NER as a classification problem in the final stage. 3.3 Bi-LSTM-CRF Then, we can add a CRF layer to the Bi-LSTM model as shown in Figure 1. That is Bi-LSTM-CRFs (Huang, Xu, and Kai 2015) which are well-suited for sequence labeling. Bi-LSTM-CRF can be regarded as a combination of bidirectional LSTM and CRF. By contrast to the local cla"
L18-1706,I08-4017,0,0.10243,"Missing"
N06-1037,P01-1017,0,0.057162,"Missing"
N06-1037,A00-2030,0,\N,Missing
N06-1037,P05-1053,1,\N,Missing
N06-1037,P04-1043,0,\N,Missing
N06-1037,P04-1054,0,\N,Missing
N06-1037,P03-1005,0,\N,Missing
N06-1037,H05-1091,0,\N,Missing
N06-1037,P05-1052,0,\N,Missing
N10-1016,J96-1002,0,0.00599551,". In addition, hard linguistic constraints are also explored. (Wu and Ng, 1995) employs syntactic bracketing information to constrain search in order to improve speed and accuracy. (Collins et al., 2005) and (Wang et al., 2007) use hard syntactic constraints to perform reorderings according to source-side parse trees. (Xiong et al., 2008) prohibit any swappings which violate punctuation based constraints. Non-linguistic constraints are also widely used in phrase-based decoding. The IBM and ITG constraints (Zens et al., 2004) are used to restrict reorderings in practical phrase-based systems. (Berger et al., 1996) introduces the concept of rift into a machine translation system, which is similar to our definition of translation boundary. They also use a maximum entropy model to predict whether a source position is a rift based on features only from source sentences. Our work differs from (Berger et al., 1996) in three major respects. 1) We distinguish a segment boundary into two categories: beginning and ending boundary due to their different distributions (see Table 1). However, Berger et al. ignore this difference. 2) We train two classifiers to predict beginning and ending boundary respectively whil"
N10-1016,H92-1053,0,0.182045,"Missing"
N10-1016,P08-1009,0,0.236099,"ins. 1 Introduction It has been known that phrase-based decoding (phrase segmentation/translation/reordering (Chiang, 2005)) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. (Xiong et al., 2009) further presents a bracketing model to include thousands of context-sensitive syntactic constraints. All of these approaches achieve their improvements by guiding the phrase-based decoder to prefer translations which respect source-side parse trees. One major probl"
N10-1016,P05-1033,0,0.123433,"ies for any source sentences. The classifiers are trained directly on word-aligned corpus without using any additional resources. We report the accuracy of our translation boundary classifiers. We show that using constraints based on translation boundaries predicted by our classifiers achieves significant improvements over the baseline on large-scale Chinese-toEnglish translation experiments. The new constraints also significantly outperform constituent boundary based syntactic constrains. 1 Introduction It has been known that phrase-based decoding (phrase segmentation/translation/reordering (Chiang, 2005)) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn"
N10-1016,P05-1066,0,0.00867273,"ide parse tree boundary violation counting feature to build soft constraints for phrase-based decoding, and (Xiong et al., 2009), which calculates a score to indicate to what extent a source phrase can be translated as a unit using a bracketing model with richer syntactic features. More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. In addition, hard linguistic constraints are also explored. (Wu and Ng, 1995) employs syntactic bracketing information to constrain search in order to improve speed and accuracy. (Collins et al., 2005) and (Wang et al., 2007) use hard syntactic constraints to perform reorderings according to source-side parse trees. (Xiong et al., 2008) prohibit any swappings which violate punctuation based constraints. Non-linguistic constraints are also widely used in phrase-based decoding. The IBM and ITG constraints (Zens et al., 2004) are used to restrict reorderings in practical phrase-based systems. (Berger et al., 1996) introduces the concept of rift into a machine translation system, which is similar to our definition of translation boundary. They also use a maximum entropy model to predict whether"
N10-1016,J99-4005,0,0.0418273,"sing any additional resources. We report the accuracy of our translation boundary classifiers. We show that using constraints based on translation boundaries predicted by our classifiers achieves significant improvements over the baseline on large-scale Chinese-toEnglish translation experiments. The new constraints also significantly outperform constituent boundary based syntactic constrains. 1 Introduction It has been known that phrase-based decoding (phrase segmentation/translation/reordering (Chiang, 2005)) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a countin"
N10-1016,N03-1017,0,0.0561793,"2005)) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. (Xiong et al., 2009) further presents a bracketing model to include thousands of context-sensitive syntactic constraints. All of these approaches achieve their improvements by guiding the phrase-based decoder to prefer translations which respect source-side parse trees. One major problem with such constituent boundary based constraints is that syntactic structures of the source language do not necessarily"
N10-1016,W04-3250,0,0.181357,"Missing"
N10-1016,P08-1114,0,0.295696,"It has been known that phrase-based decoding (phrase segmentation/translation/reordering (Chiang, 2005)) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. (Xiong et al., 2009) further presents a bracketing model to include thousands of context-sensitive syntactic constraints. All of these approaches achieve their improvements by guiding the phrase-based decoder to prefer translations which respect source-side parse trees. One major problem with such constituent bound"
N10-1016,P00-1056,0,0.0697293,"Missing"
N10-1016,P03-1021,0,0.0107032,"add a new feature to the decoder’s loglinear model: translation boundary violation counting feature. This counting feature accumulates whenever hypotheses have a partial translation spanning ci ...cj (j > i) where ci ∈ / By or cj ∈ / Ey . The 140 LDC ID LDC2004E12 LDC2004T08 LDC2005T10 LDC2003E14 LDC2002E18 LDC2005T06 LDC2003E07 LDC2004T07 Description United Nations Hong Kong News Sinorama Magazine FBIS Xinhua News V1 beta Chinese News Translation Chinese Treebank Multiple Translation Chinese Table 5: Training corpora. weight λv of this feature is tuned via minimal error rate training (MERT) (Och, 2003) with other feature weights. Unlike hard constraints, which simply prevent any hypotheses from violating translation boundaries, soft constraints allow violations of translation boundaries but with a penalty of exp(−λv Cv ) where Cv is the violation count. By using soft constraints, we can enable the model to prefer hypotheses which are consistent with translation boundaries. 5 Experiment Our baseline system is a phrase-based system using BTGs (Wu, 1997), which includes a contentdependent reordering model discriminatively trained using reordering examples (Xiong et al., 2006). We carried out v"
N10-1016,P02-1040,0,0.104176,"Missing"
N10-1016,C08-1094,0,0.0139392,"age do not necessarily reflect translation structures where the source and target language correspond to each other. In this paper, we investigate building classifiers that directly address the problem of translation boundary, rather than extracting constituent boundary from sourceside parsers built for a different purpose. A translation boundary is a position in the source sequence which begins or ends a translation zone 1 spanning multiple source words. In a translation zone, the source phrase is translated as a unit. Reorderings which cross translation zones are not desirable. Inspired by (Roark and Hollingshead, 2008) which introduces classifiers to decide if a word can begin/end a multi-word constituent, we build two discriminative classifiers to tag each word in the source sequence with a binary class label. The first classifier decides if a word can begin a multi-sourceword translation zone; the second classifier decides if a word can end a multi-source-word translation 1 We will give a formal definition of translation zone in Section 2. 136 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 136–144, c Los Angeles, California, June 2010. 2010 Associat"
N10-1016,D07-1077,0,0.00652594,"lation counting feature to build soft constraints for phrase-based decoding, and (Xiong et al., 2009), which calculates a score to indicate to what extent a source phrase can be translated as a unit using a bracketing model with richer syntactic features. More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. In addition, hard linguistic constraints are also explored. (Wu and Ng, 1995) employs syntactic bracketing information to constrain search in order to improve speed and accuracy. (Collins et al., 2005) and (Wang et al., 2007) use hard syntactic constraints to perform reorderings according to source-side parse trees. (Xiong et al., 2008) prohibit any swappings which violate punctuation based constraints. Non-linguistic constraints are also widely used in phrase-based decoding. The IBM and ITG constraints (Zens et al., 2004) are used to restrict reorderings in practical phrase-based systems. (Berger et al., 1996) introduces the concept of rift into a machine translation system, which is similar to our definition of translation boundary. They also use a maximum entropy model to predict whether a source position is a"
N10-1016,Y95-1025,0,0.0786103,"er. Our introduction has already briefly mentioned (Cherry, 2008) and (Marton and Resnik, 2008), which utilize source-side parse tree boundary violation counting feature to build soft constraints for phrase-based decoding, and (Xiong et al., 2009), which calculates a score to indicate to what extent a source phrase can be translated as a unit using a bracketing model with richer syntactic features. More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. In addition, hard linguistic constraints are also explored. (Wu and Ng, 1995) employs syntactic bracketing information to constrain search in order to improve speed and accuracy. (Collins et al., 2005) and (Wang et al., 2007) use hard syntactic constraints to perform reorderings according to source-side parse trees. (Xiong et al., 2008) prohibit any swappings which violate punctuation based constraints. Non-linguistic constraints are also widely used in phrase-based decoding. The IBM and ITG constraints (Zens et al., 2004) are used to restrict reorderings in practical phrase-based systems. (Berger et al., 1996) introduces the concept of rift into a machine translation"
N10-1016,J97-3002,0,0.0255434,"Treebank Multiple Translation Chinese Table 5: Training corpora. weight λv of this feature is tuned via minimal error rate training (MERT) (Och, 2003) with other feature weights. Unlike hard constraints, which simply prevent any hypotheses from violating translation boundaries, soft constraints allow violations of translation boundaries but with a penalty of exp(−λv Cv ) where Cv is the violation count. By using soft constraints, we can enable the model to prefer hypotheses which are consistent with translation boundaries. 5 Experiment Our baseline system is a phrase-based system using BTGs (Wu, 1997), which includes a contentdependent reordering model discriminatively trained using reordering examples (Xiong et al., 2006). We carried out various experiments to evaluate the impact of integrating translation boundary based soft constraints into decoding on the system performance on the Chinese-to-English translation task of the NIST MT-05 using large scale training data. 5.1 Experimental Setup Our training corpora are listed in Table 5. The whole corpora consist of 96.9M Chinese words and 109.5M English words in 3.8M sentence pairs. We ran GIZA++ (Och and Ney, 2000) on the parallel corpora"
N10-1016,I05-1007,1,0.393667,"Missing"
N10-1016,P06-1066,1,0.524346,"rror rate training (MERT) (Och, 2003) with other feature weights. Unlike hard constraints, which simply prevent any hypotheses from violating translation boundaries, soft constraints allow violations of translation boundaries but with a penalty of exp(−λv Cv ) where Cv is the violation count. By using soft constraints, we can enable the model to prefer hypotheses which are consistent with translation boundaries. 5 Experiment Our baseline system is a phrase-based system using BTGs (Wu, 1997), which includes a contentdependent reordering model discriminatively trained using reordering examples (Xiong et al., 2006). We carried out various experiments to evaluate the impact of integrating translation boundary based soft constraints into decoding on the system performance on the Chinese-to-English translation task of the NIST MT-05 using large scale training data. 5.1 Experimental Setup Our training corpora are listed in Table 5. The whole corpora consist of 96.9M Chinese words and 109.5M English words in 3.8M sentence pairs. We ran GIZA++ (Och and Ney, 2000) on the parallel corpora in both directions and then applied the “grow-diag-final” refinement rule (Koehn et al., 2005) to obtain many-to-many word a"
N10-1016,P09-1036,1,0.852114,"008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. (Xiong et al., 2009) further presents a bracketing model to include thousands of context-sensitive syntactic constraints. All of these approaches achieve their improvements by guiding the phrase-based decoder to prefer translations which respect source-side parse trees. One major problem with such constituent boundary based constraints is that syntactic structures of the source language do not necessarily reflect translation structures where the source and target language correspond to each other. In this paper, we investigate building classifiers that directly address the problem of translation boundary, rather"
N10-1016,C08-1136,0,0.145511,"straints based on translation boundaries predicted by our classifiers achieves significant improvements over the baseline on large-scale Chinese-toEnglish translation experiments. The new constraints also significantly outperform constituent boundary based syntactic constrains. 1 Introduction It has been known that phrase-based decoding (phrase segmentation/translation/reordering (Chiang, 2005)) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. (Xion"
N10-1016,C04-1030,0,\N,Missing
N10-1016,I08-1066,1,\N,Missing
N10-1016,2005.iwslt-1.8,0,\N,Missing
N19-1044,P16-5005,0,0.38354,"anslations and their corresponding source words during decoding, and thus can hurt translation fidelity (Hasler et al., 2018). There is not a mechanism that allows the model to learn constraint translations during training, which the placeholder method allows. We investigate a novel method based on data augmentation, which combines the advantages of both methods above. The idea is to construct synthetic parallel sentences from the original paralIntroduction One important research question in domainspecific machine translation (Luong and Manning, 2015) is how to impose translation constraints (Crego et al., 2016; Hokamp and Liu, 2017; Post and Vilar, 2018). As shown in Figure 1 (a), the word “breadboard” can be translated into “切面 包板 (a wooden board that is used to cut bread on)” in the food domain, but “电 路 板 (a construction base for prototyping of electronics)” in the electronic domain. To enhance translation quality, a lexicon can be leveraged for domainspecific or user-provided words (Arthur et al., 2016; Hasler et al., 2018). We investigate the method of leveraging pre-specified translation for NMT using such a lexicon. For leveraging pre-specified translation, one existing approach uses placeho"
N19-1044,W17-4715,0,0.0270094,"modify NMT models by integrating translation lexicons. In addition, our data augmentation method is more flexible, because it is model-free. Alkhouli et al. (2018) simulate a dictionaryguided translation task to evaluate NMT’s alignment extraction. A one-to-one word translation dictionary is used to guide NMT decoding. In their method, a dictionary entry is limited to only one word on both the source and target sides. In addition, a pre-specified translation can come into effect only if the corresponding source-side word is successfully aligned during decoding. On translating named entities, Currey et al. (2017) augment the training data by copying target-side sentences to the source-side, resulting in augmented training corpora where the source and the target sides contain identical sentences. The augmented data is shown to improve translation performance, especially for proper nouns and other words that are identical in the source and target languages. 3 Given a bilingual training corpus, we sample augmented sentence pairs by leveraging a SMT phrase table, which can be trained over the same bilingual corpus or a different large corpus. We extract source-target phrase pairs2 from the phrase table, r"
N19-1044,J82-2005,0,0.667768,"Missing"
N19-1044,2015.iwslt-evaluation.11,0,0.0200356,"oes not explicitly explore the correlation between pre-specified translations and their corresponding source words during decoding, and thus can hurt translation fidelity (Hasler et al., 2018). There is not a mechanism that allows the model to learn constraint translations during training, which the placeholder method allows. We investigate a novel method based on data augmentation, which combines the advantages of both methods above. The idea is to construct synthetic parallel sentences from the original paralIntroduction One important research question in domainspecific machine translation (Luong and Manning, 2015) is how to impose translation constraints (Crego et al., 2016; Hokamp and Liu, 2017; Post and Vilar, 2018). As shown in Figure 1 (a), the word “breadboard” can be translated into “切面 包板 (a wooden board that is used to cut bread on)” in the food domain, but “电 路 板 (a construction base for prototyping of electronics)” in the electronic domain. To enhance translation quality, a lexicon can be leveraged for domainspecific or user-provided words (Arthur et al., 2016; Hasler et al., 2018). We investigate the method of leveraging pre-specified translation for NMT using such a lexicon. For leveraging"
N19-1044,N13-1073,0,0.19482,"Missing"
N19-1044,P02-1040,0,0.103818,"e systems, the vocabulary size is set to 50K on both sides. For “Data augmentation”, to allow the source-side dictionary to cover target-side words, the target- and source-side vocabularies are merged for a new source vocabulary. For “Shared embeddings”, the source vocabulary remains the same as the baselines, where the source-side target words use embeddings from target-side vocabulary. Experiments We compare our method with strong baselines on large-scale En-Ru and Ch-En tasks on various test sets across different domains, using a strongly optimized Transformer (Vaswani et al., 2017). BLEU (Papineni et al., 2002) is used for evaluation. 5.1 Data Our training corpora are taken from the WMT2018 news translation task. En-Ru. We use 13.88M sentences as baseline training data, containing both a real bilingual corpus and a synthetic back-translation corpus (Sennrich et al., 2015a). The synthetic corpus is translated from “NewsCommonCrawl”, which can be obtained from the WMT task. The news domain contains four different test sets published by WMT2018 over the recent years, namely “news2015”, “news2016”, “news2017”, and “news2018”, respectively, each having one reference. The e-commerce domain contains four f"
N19-1044,P16-1154,0,0.0234543,"is learned because such information is available both in training and decoding. As a data augmentation method, it can be used on any NMT architecture. In addition, our method enables the model to translate code-switched source sentences, and preserve its strength in translating un-replaced sentences. To further strengthen copying, we propose two model-level adjustments: First, we share targetside embeddings with source-side target words, so that target vocabulary words have a unique embedding in the NMT system. Second, we integrate pointer network (Vinyals et al., 2015; Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017) into the decoder. The copy mechanism was firstly proposed to copy source words. In our method, it is further used to copy source-side target words. Results on large scale English-to-Russian (EnRu) and Chinese-to-English (Ch-En) tasks show that our method outperforms both placeholder and lexical constraint methods over a state-of-the-art Transformer (Vaswani et al., 2017) model on various test sets across different domains. We also show that shared embedding and pointer network can lead to more successful applications of the copying mechanism. We release four high-quality En"
N19-1044,N18-1119,0,0.462311,"ides during training, so that a model can translate such words by learning to translate placeholder tags. For example, the i-th named entity in the source sentence is replaced with “tagi ”, as well as its corresponding translation in the target side. Placeholder tags in the output are replaced with pre-specified translation as a post-processing step. One disadvantage of this approach, however, is that the meaning of the original words in the pre-specified translation is not fully retained, which can be harmful to both adequacy and fluency of the output. Another approach (Hokamp and Liu, 2017; Post and Vilar, 2018) imposes pre-specified translation via lexical constraints, making sure such constraints are satisfied by modifying NMT decoding. This method ensures that pre-specified translations appear in the output. A problem of this method is that it does not explicitly explore the correlation between pre-specified translations and their corresponding source words during decoding, and thus can hurt translation fidelity (Hasler et al., 2018). There is not a mechanism that allows the model to learn constraint translations during training, which the placeholder method allows. We investigate a novel method b"
N19-1044,P16-1014,0,0.135791,"especified translation is learned because such information is available both in training and decoding. As a data augmentation method, it can be used on any NMT architecture. In addition, our method enables the model to translate code-switched source sentences, and preserve its strength in translating un-replaced sentences. To further strengthen copying, we propose two model-level adjustments: First, we share targetside embeddings with source-side target words, so that target vocabulary words have a unique embedding in the NMT system. Second, we integrate pointer network (Vinyals et al., 2015; Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017) into the decoder. The copy mechanism was firstly proposed to copy source words. In our method, it is further used to copy source-side target words. Results on large scale English-to-Russian (EnRu) and Chinese-to-English (Ch-En) tasks show that our method outperforms both placeholder and lexical constraint methods over a state-of-the-art Transformer (Vaswani et al., 2017) model on various test sets across different domains. We also show that shared embedding and pointer network can lead to more successful applications of the copying mechanism. We release fou"
N19-1044,N18-2081,0,0.0896831,"Missing"
N19-1044,P17-1099,0,0.0240029,"e such information is available both in training and decoding. As a data augmentation method, it can be used on any NMT architecture. In addition, our method enables the model to translate code-switched source sentences, and preserve its strength in translating un-replaced sentences. To further strengthen copying, we propose two model-level adjustments: First, we share targetside embeddings with source-side target words, so that target vocabulary words have a unique embedding in the NMT system. Second, we integrate pointer network (Vinyals et al., 2015; Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017) into the decoder. The copy mechanism was firstly proposed to copy source words. In our method, it is further used to copy source-side target words. Results on large scale English-to-Russian (EnRu) and Chinese-to-English (Ch-En) tasks show that our method outperforms both placeholder and lexical constraint methods over a state-of-the-art Transformer (Vaswani et al., 2017) model on various test sets across different domains. We also show that shared embedding and pointer network can lead to more successful applications of the copying mechanism. We release four high-quality En-Ru e-commerce test"
N19-1044,P17-1141,0,0.344153,"he source and target sides during training, so that a model can translate such words by learning to translate placeholder tags. For example, the i-th named entity in the source sentence is replaced with “tagi ”, as well as its corresponding translation in the target side. Placeholder tags in the output are replaced with pre-specified translation as a post-processing step. One disadvantage of this approach, however, is that the meaning of the original words in the pre-specified translation is not fully retained, which can be harmful to both adequacy and fluency of the output. Another approach (Hokamp and Liu, 2017; Post and Vilar, 2018) imposes pre-specified translation via lexical constraints, making sure such constraints are satisfied by modifying NMT decoding. This method ensures that pre-specified translations appear in the output. A problem of this method is that it does not explicitly explore the correlation between pre-specified translations and their corresponding source words during decoding, and thus can hurt translation fidelity (Hasler et al., 2018). There is not a mechanism that allows the model to learn constraint translations during training, which the placeholder method allows. We inves"
N19-1044,W16-2316,0,0.0204654,"ted as ct = i=1 αt,i ∗ hi,n , where αt,i is attention weight mentioned earlier. {h1,n , h2,n , ..., hm,n } are the source-side hidden states of the encoder’s last layer. 5 5.2 Experimental Settings We use six self-attention layers for both the encoder and the decoder. The embedding size and the hidden size are set to 512. Eight heads are used for self-attention. A feed-forward layer with 2048 cells and Swish (Ramachandran et al., 2018) is used as the activation function. Adam (Kingma and Ba, 2014) is used for training; warmup step is 16000; the learning rate is 0.0003. We use label smoothing (Junczys-Dowmunt et al., 2016) with a confidence score of 0.9, and all the drop-out (Gal and Ghahramani, 2016) probabilities are set to 0.1. We extract a SMT phrase table on the bilingual training corpus by using moses (Koehn et al., 2007) with default setting, which is used for matching sentence pairs to generate augmented training data. We apply count-based pruning (Zens et al., 2012) to the phrase table, the threshold is set to 10. During decoding, similar to Hasler et al. (2018), Alkhouli et al. (2018) and Post and Vilar (2018), we make use of references to obtain gold constraints. Following previous work, prespecified"
N19-1044,P18-4020,0,0.0242973,"Missing"
N19-1044,W17-4742,0,0.0670706,"Missing"
N19-1044,D12-1089,0,0.0182688,"d-forward layer with 2048 cells and Swish (Ramachandran et al., 2018) is used as the activation function. Adam (Kingma and Ba, 2014) is used for training; warmup step is 16000; the learning rate is 0.0003. We use label smoothing (Junczys-Dowmunt et al., 2016) with a confidence score of 0.9, and all the drop-out (Gal and Ghahramani, 2016) probabilities are set to 0.1. We extract a SMT phrase table on the bilingual training corpus by using moses (Koehn et al., 2007) with default setting, which is used for matching sentence pairs to generate augmented training data. We apply count-based pruning (Zens et al., 2012) to the phrase table, the threshold is set to 10. During decoding, similar to Hasler et al. (2018), Alkhouli et al. (2018) and Post and Vilar (2018), we make use of references to obtain gold constraints. Following previous work, prespecified translations for each source sentence are sampled from references and used by all systems for fair comparison. In all the baseline systems, the vocabulary size is set to 50K on both sides. For “Data augmentation”, to allow the source-side dictionary to cover target-side words, the target- and source-side vocabularies are merged for a new source vocabulary."
N19-1044,J93-2003,0,\N,Missing
N19-1044,P07-2045,0,\N,Missing
N19-1044,D16-1162,0,\N,Missing
N19-1044,P16-1162,0,\N,Missing
N19-1044,D17-1098,0,\N,Missing
N19-1044,W18-6318,0,\N,Missing
N19-1118,P17-2021,0,0.0240852,"e structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its b"
N19-1118,D17-1209,0,0.232385,"mework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential"
N19-1118,P17-1177,0,0.407881,"propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of Neubig et al. (2017), our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integrated into a classica"
N19-1118,D17-1304,0,0.0354014,"Missing"
N19-1118,P18-1163,0,0.07328,"drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019"
N19-1118,W14-4012,0,0.16658,"Missing"
N19-1118,D14-1179,0,0.0643719,"Missing"
N19-1118,W06-1628,0,0.1279,"Missing"
N19-1118,W17-3203,0,0.04525,"Missing"
N19-1118,P81-1022,0,0.687283,"Missing"
N19-1118,P16-1078,0,0.150303,"al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of Neubig et al. (2017), our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integra"
N19-1118,D17-1012,0,0.247924,"e source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate ba"
N19-1118,W15-3014,0,0.0253779,"ne Input Encoder Decoder 教育 o1 o2 o3 o4 o5 o6 head=1, top 是 现代 文明 的 基石 head=0, root head=4, amod head=6, assmod head=4, assm head=2, attr • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. Figure 1: An example to illustrate our method of encoding source dependency syntax, where the English translation is “Education is the cornerstone of modern civilization” for the source Chinese input. In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence"
N19-1118,W04-3250,0,0.132416,"h et al., 2016) with 32K merges to obtain subword units, and construct the target vocabularies by the most frequent 32K subwords. During training, we use only the sentence pairs whose source and target lengths both are no longer than 50 and 150 for Chinese-English and English-Vietnamese translations, respectively. Evaluation. We use the case insensitive 4gram BLEU score as the main evaluation metrics (Papineni et al., 2002), and adopt the script multi-bleu.perl in the Mose toolkit.3 Significance tests are conducted based on the bestBLEU results for each approach by using bootstrap resampling (Koehn, 2004). Alternatively, in order to compare the effectiveness of our model with other syntax integration methods, we implement a Tree-RNN approach and a Tree-Linearization approach, respectively: • Tree-RNN: We build a one-layer bidirectional Tree-RNN with GRU over input word embeddings, producing syntaxenhanced word representations, which are then fed into the encoder of NMT as basic inputs. The method is similar to the model proposed by Chen et al. (2017a). • Tree-Linearization: We first convert dependency trees into constituent trees (Sun and Wan, 2013), and then feed it into the NMT model propose"
N19-1118,W17-3204,0,0.0210465,"ore overlapping with other network components. This further demonstrates that pretrained syntax-aware word representations are helpful for NMT. 4.4.2 Alignment Study Alignment quality is an important metric to illustrate and evaluate machine translation outputs. Here we study how syntax features influence the alignment results for NMT. We approximate the alignment scores by the attention probabilities as shown in Equation 4.8 For better understanding 8 We aim to offer an intuitive interpretation by a carefullyselected example. In fact, the alignment computation method here may be problematic (Koehn and Knowles, 2017). 1156 System Baseline×3 SAWR×3 Tree-RNN×3 Tree-Linearization×3 Hybrid MT03 40.90 41.94 42.03 41.74 42.72 MT04 43.25 44.59 44.15 44.23 45.14 MT05 40.64 41.91 41.50 41.32 42.38 MT06 40.16 41.97 41.41 41.44 42.15 Average/∆ 41.24 42.60/+1.36 42.27/+1.03 42.18/+0.94 43.10/+1.86 Table 4: Ensemble performances, where the Hybrid model denotes SAWR + Tree-RNN + Tree-Linearization. System ... 现代 (modern) ... 的 (’s) ... Baseline Tree-RNN Baseline SAWR Tree-Linearization 42 BLEU SAWR Tree-RNN 38 Tree-Linearization 34 Figure 3: Alignments for the baseline and syntaxintegrated systems, where the same examp"
N19-1118,E17-2093,0,0.0238477,"ecent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested (Chen et al., 2017a; Yang et al., 2017). Since Tree-RNN suffers serious inefficiency problem, Li et al. (2017) suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbols mixed with words and syn1158 tactic tags. The method is as effective as TreeRNN approaches yet"
N19-1118,D15-1278,0,0.0190868,"network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested (Chen et al., 2017a; Yang et al., 2017). Since Tree-RNN suffers serious inefficiency problem, Li et al. (2017) suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbo"
N19-1118,N13-1060,0,0.0581759,"Missing"
N19-1118,P17-1064,1,0.696123,"of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of Neubig et al. (2017), our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integrated into a classical Seq2Seq system. To solve the problem, Tree-Linearization is a good alternative for syntax encoding. The main idea is to linearize syntax trees into sequential symbols, and then exploit the resulting sequences as inputs for NMT. Li et al. (2017) propose a depth-first method to traverse a constituent tree, converting it into a sequence of symbols mixed with sentential words and syntax labels. Similarly, Wu et al. (2017b) combine several strategies of tree traversing for dependency syntax integration. In this work, we present an implicit syntax encoding method for NMT, enhancing NMT models by syntax-aware word representations (SAWRs). Figure 1 illustrates the basic idea, where trees are modeled indirectly by sequential vectors extracted from an encoder-decoder dependency parser. On the one hand, the method avoids the structural heterog"
N19-1118,P02-1040,0,0.105406,"t datasets, respectively. For the source side sentences, we construct vocabularies of the most frequent 50K words, while for the target side sentences, we apply byte-pair encodings (BPE) (Sennrich et al., 2016) with 32K merges to obtain subword units, and construct the target vocabularies by the most frequent 32K subwords. During training, we use only the sentence pairs whose source and target lengths both are no longer than 50 and 150 for Chinese-English and English-Vietnamese translations, respectively. Evaluation. We use the case insensitive 4gram BLEU score as the main evaluation metrics (Papineni et al., 2002), and adopt the script multi-bleu.perl in the Mose toolkit.3 Significance tests are conducted based on the bestBLEU results for each approach by using bootstrap resampling (Koehn, 2004). Alternatively, in order to compare the effectiveness of our model with other syntax integration methods, we implement a Tree-RNN approach and a Tree-Linearization approach, respectively: • Tree-RNN: We build a one-layer bidirectional Tree-RNN with GRU over input word embeddings, producing syntaxenhanced word representations, which are then fed into the encoder of NMT as basic inputs. The method is similar to t"
N19-1118,N18-1202,0,0.0504328,"rocess can be formalized as follows:  h = Bi-RNN ex1 ⊕ s1 , · · · , exn ⊕ sn . (6) Noticeably, the SAWR method can be regarded as an adaption of joint learning as well. We can train both dependency parsing and machine translation model parameters concurrently. In this work, we focus on the machine translation task and do not involve the training objective of dependency parsing. However, we can still finetune model parameters of the encoder part of dependency parsing by back-propagating the training losses of NMT into this part as well. Actually, SAWRs are also similar to the ELMO embeddings (Peters et al., 2018). ELMO learns context word representations by using language model as objective, while SAWRs learn syntaxaware word representations by using dependency parsing as objective. On the other hand, compared with the Tree-RNN and Tree-Linearization methods which encode syntax trees by neural networks directly, SAWRs are less sensitive to the output syntax trees. Thus the SAWR method can alleviate the error propagation problem. 4 the TED tst2012 and tst2013 as the development and test datasets, respectively. For the source side sentences, we construct vocabularies of the most frequent 50K words, whil"
N19-1118,W16-2209,0,0.0414485,"see that syntax information can still give Related Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present th"
N19-1118,P16-1162,0,0.0740184,"objective, while SAWRs learn syntaxaware word representations by using dependency parsing as objective. On the other hand, compared with the Tree-RNN and Tree-Linearization methods which encode syntax trees by neural networks directly, SAWRs are less sensitive to the output syntax trees. Thus the SAWR method can alleviate the error propagation problem. 4 the TED tst2012 and tst2013 as the development and test datasets, respectively. For the source side sentences, we construct vocabularies of the most frequent 50K words, while for the target side sentences, we apply byte-pair encodings (BPE) (Sennrich et al., 2016) with 32K merges to obtain subword units, and construct the target vocabularies by the most frequent 32K subwords. During training, we use only the sentence pairs whose source and target lengths both are no longer than 50 and 150 for Chinese-English and English-Vietnamese translations, respectively. Evaluation. We use the case insensitive 4gram BLEU score as the main evaluation metrics (Papineni et al., 2002), and adopt the script multi-bleu.perl in the Mose toolkit.3 Significance tests are conducted based on the bestBLEU results for each approach by using bootstrap resampling (Koehn, 2004). A"
N19-1118,P16-1159,0,0.0239934,"明 的 基石 head=0, root head=4, amod head=6, assmod head=4, assm head=2, attr • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. Figure 1: An example to illustrate our method of encoding source dependency syntax, where the English translation is “Education is the cornerstone of modern civilization” for the source Chinese input. In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predic"
N19-1118,D16-1159,0,0.226869,"the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151"
N19-1118,Q13-1025,0,0.0621683,"Missing"
N19-1118,P06-1077,0,0.132793,"on. We implement Tree-RNN and TreeLinearization for Transformer in a similar way, only adapting the source input word representing. We adopt a widely-used setting with 8 heads, 6 layers and the hidden dimension size of 512. Table 5 shows the results. As shown, the transformer results are indeed much better than RNNbased baseline. The BLEU scores show an average increase of 40.74 − 37.09 = 3.65. In addition, we can see that syntax information can still give Related Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because"
N19-1118,2015.iwslt-evaluation.11,0,0.117502,"Missing"
N19-1118,P15-1150,0,0.104527,"1 o2 o3 o4 o5 o6 head=1, top 是 现代 文明 的 基石 head=0, root head=4, amod head=6, assmod head=4, assm head=2, attr • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. Figure 1: An example to illustrate our method of encoding source dependency syntax, where the English translation is “Education is the cornerstone of modern civilization” for the source Chinese input. In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden"
N19-1118,D15-1166,0,0.266251,"6 head=1, top 是 现代 文明 的 基石 head=0, root head=4, amod head=6, assmod head=4, assm head=2, attr • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. Figure 1: An example to illustrate our method of encoding source dependency syntax, where the English translation is “Education is the cornerstone of modern civilization” for the source Chinese input. In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then i"
N19-1118,P08-1114,0,0.0513682,"arization for Transformer in a similar way, only adapting the source input word representing. We adopt a widely-used setting with 8 heads, 6 layers and the hidden dimension size of 512. Table 5 shows the results. As shown, the transformer results are indeed much better than RNNbased baseline. The BLEU scores show an average increase of 40.74 − 37.09 = 3.65. In addition, we can see that syntax information can still give Related Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network"
N19-1118,D16-1096,0,0.0201671,", neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapol"
N19-1118,P16-1105,0,0.0360613,"14; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested (Chen et al., 2017a; Yang et al., 2017). Since Tree-RNN suffers serious inefficiency problem, Li et al. (2017) suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbols mixed with words and syn1158 tactic tags. The method is as eff"
N19-1118,P17-1065,0,0.296249,"ariety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that i"
N19-1118,1983.tc-1.13,0,0.712779,"Missing"
N19-1118,D11-1020,0,0.0301346,"in a similar way, only adapting the source input word representing. We adopt a widely-used setting with 8 heads, 6 layers and the hidden dimension size of 512. Table 5 shows the results. As shown, the transformer results are indeed much better than RNNbased baseline. The BLEU scores show an average increase of 40.74 − 37.09 = 3.65. In addition, we can see that syntax information can still give Related Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing a"
N19-1118,D17-1150,0,0.0770327,"esting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of Neubig et al. (2017), our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integrated into a classical Seq2Seq system. To solv"
N19-1118,N16-1035,0,0.02831,"es such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested (Chen et al., 2017a; Yang et al., 2017). Since Tree-RNN suffers serious inefficiency problem, Li et al. (2017) suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbols mixed with words"
N19-1118,Q18-1011,0,0.0337858,"Missing"
N19-1118,P17-2092,0,0.0126484,"lated Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tre"
N19-1118,P17-2060,0,0.0194939,"lated Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tre"
P02-1023,P00-1073,1,0.803588,"criterion, which is used to estimate the performance loss of the pruned model. Given the pruning criterion, a simple thresholding algorithm for pruning bigram models can be described as follows: 1. Select a threshold θ. 2. Compute the performance loss due to pruning each bigram individually using the pruning criterion. 3. Remove all bigrams with performance loss less than θ. 4. Re-compute backoff weights. Figure 1: Thresholding algorithm for bigram pruning The algorithm in Figure 1 together with several pruning criteria has been studied previously (Seymore and Rosenfeld, 1996; Stolcke, 1998; Gao and Lee, 2000; etc). A comparative study of these techniques is presented in (Goodman and Gao, 2000). In this paper, three pruning criteria will be studied: probability, rank, and entropy. Probability serves as the baseline pruning criterion. It is derived from perplexity which has been widely used as a LM evaluation measure. Rank and entropy have been previously used as a metric for LM evaluation in (Clarkson and Robinson, 2001). In the current paper, these two measures will be studied for the purpose of backoff n-gram model pruning. In the next section, we will describe how pruning criteria are developed"
P04-1021,W03-1508,0,0.797183,"Missing"
P04-1021,C00-1056,0,\N,Missing
P05-1053,P04-1054,0,0.650545,"Missing"
P05-1053,A00-2030,0,0.0346955,"tion on the 5 ACE relation types. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 and Section 4 describe our approach and various features employed respectively. Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. 2 Related Work The relation extraction task was formulated at the 7th Message Understanding Conference (MUC-7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived fr"
P05-1053,C02-1151,0,\N,Missing
P05-1053,J03-4003,0,\N,Missing
P06-1016,P04-1054,0,0.168458,"he previous best-reported system. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typic"
P06-1016,A00-2030,0,0.0727802,"rchical strategy outperforms the previous best-reported system. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome t"
P06-1016,I05-1034,1,0.93572,"tion 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typically applied some smoothing techniques to integrate different scales of cont"
P06-1016,P05-1052,0,0.412762,"ve steady performance given the current corpus size. Given the relative large size of this corpus, it will be time-consuming and very expensive to further expand the corpus with a reasonable gain in performance. Even if we can somehow expend the corpus and achieve steady performance on major relation subtypes, it will be still far beyond practice for those minor subtypes given the much unevenly distribution among different relation subtypes. While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations. This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem by modeling the commonality among related classes. Through organizing various classes hierarchically, a linear discriminative function is determined for each class in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-lev"
P06-1016,P02-1060,1,0.825946,"Missing"
P06-1016,P05-1053,1,0.864337,"Missing"
P06-1016,C02-1151,0,\N,Missing
P06-1016,J03-4003,0,\N,Missing
P06-1016,P04-1053,0,\N,Missing
P06-1016,H05-1091,0,\N,Missing
P06-1104,P01-1017,0,0.0536781,"Missing"
P06-1104,A00-2030,0,\N,Missing
P06-1104,P05-1053,1,\N,Missing
P06-1104,P04-1043,0,\N,Missing
P06-1104,P04-1054,0,\N,Missing
P06-1104,P03-1005,0,\N,Missing
P06-1104,H05-1091,0,\N,Missing
P06-1104,P05-1052,0,\N,Missing
P06-2005,P00-1037,0,0.383464,"ese SMS translation system using a wordgroup model. In addition, in most of the commercial SMS translation applications 2 , SMS lingo (i.e., SMS short form) dictionary is provided to replace SMS short-forms with normal English words. Most of the systems do not handle OOV (out-of-vocabulary) items and ambiguous inputs. Following compares SMS text normalization with other similar or related applications. 2.1 Gale, 1991) that mostly model the edit operations using distance measures (Damerau 1964; Levenshtein 1966), specific word set confusions (Golding and Roth, 1999) and pronunciation modeling (Brill and Moore, 2000; Toutanova and Moore, 2002). These models are mostly character-based or string-based without considering the context. In addition, the author might not be aware of the errors in the word introduced during the edit operations, as most errors are due to mistype of characters near to each other on the keyboard or homophones, such as “poor” or “pour”. In SMS, errors are not isolated within word and are usually not surrounded by clean context. Words are altered deliberately to reflect sender’s distinct creation and idiosyncrasies. A character can be deleted on purpose, such as “wat” (what) and “hv"
P06-2005,J93-2003,0,0.0118108,"≈ max {P(T |e1N )i P( s1K |e1K )} (1) = arg max {P( s1M |e1N )i P( e1N )} T e1N This is the basic function of the channel model for the phrase-based SMS normalization model, where we used the maximum approximation for the sum over all segmentations. Then we further decompose the probability P( s1K |e1K ) using a Assuming that one SMS word is mapped exactly to one English word in the channel model P ( s |e) under an alignment A , we need to consider only two types of probabilities: the alignment probabilities denoted by P(m |am ) and the lexicon mapping probabilities denoted by P ( sm |eam ) (Brown et al. 1993). The channel phrase alignment A as done in the previous word-based model. P( s1K |e1K ) = ∑ P( s1K , A |e1K ) model can be written as in the following equation where m is the position of a word in s and am its alignment in e . A = ∑{P( A |e1K )i P( s1K |A, e1K )} A P( s1M |e1N ) = ∑ P( s1M , A |e1N )  K  (4) = ∑  ∏ P( k |ak )i P( sk |s1k −1 , e aa1k )  A  k =1  { A = ∑ P( A |e1N )i P( s1M |A, e1N ) (2) }  K  ≈ ∑  ∏ P( k |ak )i P( sk |eak )   A  k =1 { A  M  ≈ ∑  ∏ P( m |am )i P( sm |eam )  A  m =1  { (3) = ∑ P(T |e1N )i P( s1K |e1K ) = arg max {P( e |s )} N 1 } } We are now"
P06-2005,C02-1134,0,0.127259,"tem using a wordgroup model. In addition, in most of the commercial SMS translation applications 2 , SMS lingo (i.e., SMS short form) dictionary is provided to replace SMS short-forms with normal English words. Most of the systems do not handle OOV (out-of-vocabulary) items and ambiguous inputs. Following compares SMS text normalization with other similar or related applications. 2.1 Gale, 1991) that mostly model the edit operations using distance measures (Damerau 1964; Levenshtein 1966), specific word set confusions (Golding and Roth, 1999) and pronunciation modeling (Brill and Moore, 2000; Toutanova and Moore, 2002). These models are mostly character-based or string-based without considering the context. In addition, the author might not be aware of the errors in the word introduced during the edit operations, as most errors are due to mistype of characters near to each other on the keyboard or homophones, such as “poor” or “pour”. In SMS, errors are not isolated within word and are usually not surrounded by clean context. Words are altered deliberately to reflect sender’s distinct creation and idiosyncrasies. A character can be deleted on purpose, such as “wat” (what) and “hv” (have). It also consists o"
P06-2005,C90-2036,0,\N,Missing
P06-2005,P02-1040,0,\N,Missing
P06-2005,P01-1008,0,\N,Missing
P06-2005,P02-1019,0,\N,Missing
P06-2005,N03-1017,0,\N,Missing
P06-2005,shimohata-sumita-2002-automatic,0,\N,Missing
P06-2010,W05-0627,1,0.831541,"based method use a large number of hand-craft diverse features, from word, POS, syntax and semantics, NER, etc. The standard features with polynomial kernel gets the best performance. The reason is that the arbitrary binary combination among features implicated by the polynomial kernel is useful to SRL. We believe that combining the two methods can perform better. In order to make full use of the syntactic information and the standard flat features, we present a composite kernel between hybrid kernel (Khybrid ) and standard features with polynomial Stage 4: A rule-based post-processing stage (Liu et al., 2005) is used to handle some unmatched arguments with constituents, such as AM-MOD, AM-NEG. 5.1.4 Classifier We use the Voted Perceptron (Freund and Schapire, 1998) algorithm as the kernel machine. The performance of the Voted Perceptron is close to, but not as good as, the performance of SVM on the same problem, while saving computation time and programming effort significantly. SVM is too slow to finish our experiments for tuning parameters. The Voted Perceptron is a binary classifier. In order to handle multi-classification problems, we adopt the one vs. others strategy and select the one with t"
P06-2010,P98-1013,0,0.0108697,"l for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model th"
P06-2010,W04-2412,0,0.0431802,"Missing"
P06-2010,J93-2004,0,0.0306892,"hybrid convolution tree kernel, Khybrid . The aim of our experiments is to verify the effectiveness of our hybrid convolution tree kernel and and its combination with the standard flat features. Since the size of a parse tree is not constant, we normalize K(T1 , T2 ) by dividing it by p K(T1 , T1 ) · K(T2 , T2 ) 5.1 Experimental Setting 5.1.1 Corpus We use the benchmark corpus provided by CoNLL-2005 SRL shared task (Carreras and M`arquez, 2005) provided corpus as our training, development, and test sets. The data consist of sections of the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al., 2005). We followed the standard partition used in syntactic parsing: sections 02-21 for training, section 24 for development, and section 23 for test. In addition, the test set of the shared task includes three sections of the Brown corpus. Table 2 provides counts of sentences, tokens, annotated propositions, and arguments in the four data sets. 4.3 Comparison with Previous Work It would be interesting to investigate the differences between our method and the feature-based methods. The basic"
P06-2010,W05-0620,0,0.262008,"Missing"
P06-2010,P04-1043,0,0.365021,"and Duffy, 2001) provide an elegant kernel-based solution to implicitly explore tree structure features by directly computing the similarity between two trees. In addition, some machine learning algorithms with dual form, such as Perceptron and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000), which do not need know the exact presentation of objects and only need compute their kernel functions during the process of learning and prediction. They can be well used as learning algorithms in the kernel-based methods. They are named kernel machines. In this paper, we decompose the Moschitti (2004)’s predicate-argument feature (PAF) kernel into a Path kernel and a Constituent Structure kerIntroduction In the last few years there has been increasing interest in Semantic Role Labeling (SRL). It is currently a well defined task with a substantial body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows"
P06-2010,A00-2018,0,0.0820962,"Missing"
P06-2010,W03-0423,0,0.026918,"sentation in our feature space is more robust than the Parse Tree Path feature in the flat feature set since the Path feature is sensitive to small changes of the parse trees and it also does not maintain the hierarchical information of a parse tree. Sentences Tokens Propositions Arguments Train 39,832 950,028 90,750 239,858 Devel 1,346 32,853 3,248 8,346 tWSJ 2,416 56,684 5,267 14,077 tBrown 426 7,159 804 2,177 Table 2: Counts on the data set The preprocessing modules used in CONLL2005 include an SVM based POS tagger (Gim´enez and M`arquez, 2003), Charniak (2000)’s full syntactic parser, and Chieu and Ng (2003)’s Named Entity recognizer. 5.1.2 Evaluation The system is evaluated with respect to precision, recall, and Fβ=1 of the predicted arguments. P recision (p) is the proportion of arguments predicted by a system which are correct. Recall (r) is the proportion of correct arguments which are predicted by a system. Fβ=1 computes the harmonic mean of precision and recall, which is the final measure to evaluate the performances of systems. It is formulated as: Fβ=1 = 2pr/(p + r). srl-eval.pl2 is the official program of the CoNLL-2005 SRL shared task to evaluate a system performance. It is also worth c"
P06-2010,J05-1004,0,0.316121,"d a Constituent Structure kerIntroduction In the last few years there has been increasing interest in Semantic Role Labeling (SRL). It is currently a well defined task with a substantial body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods 1 http://www.cs.unt.edu/∼rada/senseval/senseval3/ 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, c Sydney, July 2006. 2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in"
P06-2010,P05-1072,0,0.24175,"tive evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods 1 http://www.cs.unt.edu/∼rada/senseval/senseval3/ 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, c Sydney, July 2006. 2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in the development sets of CoNLL-2005 SRL shared task. In addition, the final composing kernel between hybrid convolution tree kernel and standard features’ polynomial kernel outperforms each of them individually."
P06-2010,P04-1054,0,0.121072,"hod. Section 5 shows the experimental results. We conclude our work in Section 6. 2 Many kernel functions have been proposed in machine learning community and have been applied to NLP study. In particular, Haussler (1999) and Watkins (1999) proposed the best-known convolution kernels for a discrete structure. In the context of convolution kernels, more and more kernels for restricted syntaxes or specific domains, such as string kernel for text categorization (Lodhi et al., 2002), tree kernel for syntactic parsing (Collins and Duffy, 2001), kernel for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features"
P06-2010,C04-1197,0,0.0455194,"ing was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorithms from generalizing unseen data well. As an alternative to the standard feature-based methods, kernel-based methods have been proposed to implicitly explore features in a highdimension space by directly calculating the simi"
P06-2010,W05-0639,0,0.113832,"Missing"
P06-2010,J02-3001,0,0.676474,"al body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods 1 http://www.cs.unt.edu/∼rada/senseval/senseval3/ 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, c Sydney, July 2006. 2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in the development sets of CoNLL-2005 SRL shared task. In addition, the final composing kernel between hybrid convolution tree kernel and standard features’ polynomial kernel outperforms each"
P06-2010,P02-1031,0,0.0644078,"cate-Constituent related features parse tree path from the predicate to the constituent the relative position of the constituent and the predicate, before or after the nodes number on the parse tree path some part on the parse tree path the clause layers from the constituent to the predicate Table 1: Standard flat features However, to find relevant features is, as usual, a complex task. In addition, according to the description of the standard features, we can see that the syntactic features, such as Path, Path Length, bulk large among all features. On the other hand, the previous researches (Gildea and Palmer, 2002; Punyakanok et al., 2005) have also recognized the 74 Firstly, a parse tree T can be represented by a vector of integer counts of each sub-tree type (regardless of its ancestors): Φ(T ) = (# of sub-trees of type 1, . . . , # of sub-trees of type i, . . . , # of sub-trees of type n) This results in a very high dimension since the number of different subtrees is exponential to the tree’s size. Thus it is computationally infeasible to use the feature vector Φ(T ) directly. To solve this problem, we introduce the tree kernel function which is able to calculate the dot product between the above hi"
P06-2010,W04-3212,0,0.270004,"ent Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorit"
P06-2010,W04-3211,0,\N,Missing
P06-2010,C98-1013,0,\N,Missing
P07-1026,P98-1013,0,0.0102809,"nvolution tree kernel on the data set of the CoNLL-2005 SRL shared task. The remainder of the paper is organized as follows: Section 2 reviews the previous work and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treeba"
P07-1026,W05-0620,0,0.371187,"classification are regarded as two key steps in semantic role labeling. Semantic role identification involves classifying each syntactic element in a sentence into either a semantic argument or a non-argument while semantic role classification involves classifying each semantic argument identified into a specific semantic role. This paper focuses on semantic role classification task with the assumption that the semantic arguments have been identified correctly. Both feature-based and kernel-based learning methods have been studied for semantic role classification (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). In feature-based methods, a flat feature vector is used to represent a predicateargument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument structures. As we know, kernel methods are more effective in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity between two predicate-arguments. Th"
P07-1026,A00-2018,0,0.0600695,"Missing"
P07-1026,J05-1004,0,0.0606769,"Missing"
P07-1026,W04-3212,0,0.110658,"ork and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “N"
P07-1026,P06-1104,1,0.842473,"hods for SRL: as an alternative, kernel methods are more effective in modeling structured objects. This is because a kernel can measure the similarity between two structured objects using the original representation of the objects instead of explicitly enumerating their features. Many kernels have been proposed and applied to the NLP study. In particular, Haussler (1999) proposed the well-known convolution kernels for a discrete structure. In the context of it, more and more kernels for restricted syntaxes or specific domains (Collins and Duffy, 2001; Lodhi et al., 2002; Zelenko et al., 2003; Zhang et al., 2006) are proposed and explored in the NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. He selected portions of syntactic parse trees as predicateargument feature spaces, which include salient substructures of predicate-arguments, to define convolution kernels for the task of semantic role classification. Under the same framework, Che et al. (2006) proposed a hybrid convolution tree kernel, which consists of two individual convolution kernels: a Path kernel and a Constituent Structure kern"
P07-1026,W04-3211,0,\N,Missing
P07-1026,J93-2004,0,\N,Missing
P07-1026,N06-2025,0,\N,Missing
P07-1026,W03-1012,0,\N,Missing
P07-1026,C04-1197,0,\N,Missing
P07-1026,P05-1072,0,\N,Missing
P07-1026,C98-1013,0,\N,Missing
P07-1026,P04-1043,0,\N,Missing
P07-1026,J02-3001,0,\N,Missing
P07-1026,P04-1016,0,\N,Missing
P07-1026,P06-2010,1,\N,Missing
P07-1026,W04-2412,0,\N,Missing
P08-1064,2007.mtsummit-papers.8,0,0.196123,"rase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed mod"
P08-1064,P05-1067,0,0.429194,"od statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine th"
P08-1064,P03-2041,0,0.821253,"that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is"
P08-1064,N04-1035,0,0.617925,"Missing"
P08-1064,P06-1121,0,0.542205,"ive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and syntax-based methods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the target side of translation model and language mod560 el under the phrase-based translation framework, resulting in good performance improvement. However, neither source side syntactic knowledge nor reordering model is further explored. 2) Galley et al. (2006) handle non-syntactic phrasal translations by traversing the tree upwards until a node that subsumes the phrase is reached. This solution requires larger applicability contexts (Marcu et al., 2006). However, phrases are utilized independently in the phrase-based method without depending on any contexts. 3) Addressing the issues in Galley et al. (2006), Marcu et al. (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be comb"
P08-1064,N04-1014,0,0.102142,"Missing"
P08-1064,2003.mtsummit-papers.22,0,0.0978139,"Missing"
P08-1064,N03-1017,0,0.108044,"igned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al."
P08-1064,koen-2004-pharaoh,0,0.260163,"he proposed model works. First, the source sentence is parsed into a source parse tree. Next, the source parse tree is detached into two source tree sequences (the left hand side of rules in Fig. 3). Then the two rules in Fig. 3 are used to map the two source tree sequences to two target tree sequences, which are then combined to generate a target parse tree. Finally, a target translation is yielded from the target tree. Our model is implemented under log-linear framework (Och and Ney, 2002). We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2004): 1) bidirectional rule mapping probabilities; 2) bidirectional lexical rule translation probabilities; 3) the target language model; 4) the number of rules used and 5) the number of target words. In addition, we define two new features: 1) the number of lexical words in a rule to control the model’s preference for lexicalized rules over un-lexicalized 562 rules and 2) the average tree depth in a rule to balance the usage of hierarchical rules and flat rules. Note that we do not distinguish between larger (taller) and shorter source side tree sequences, i.e. we let these rules compete directly"
P08-1064,P06-1077,0,0.798378,"al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence 1 as the basic tran"
P08-1064,P07-1089,0,0.865288,"d Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence 1 as the basic translation unit and u"
P08-1064,W06-1606,0,0.778456,"hods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the target side of translation model and language mod560 el under the phrase-based translation framework, resulting in good performance improvement. However, neither source side syntactic knowledge nor reordering model is further explored. 2) Galley et al. (2006) handle non-syntactic phrasal translations by traversing the tree upwards until a node that subsumes the phrase is reached. This solution requires larger applicability contexts (Marcu et al., 2006). However, phrases are utilized independently in the phrase-based method without depending on any contexts. 3) Addressing the issues in Galley et al. (2006), Marcu et al. (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be combined with other genuine non-terminals for acquiring the genuine parse trees. The name of the pseudo non-terminal is designed to reflect the full realization of the corresponding rule. The problem i"
P08-1064,P04-1083,0,0.0266683,"Missing"
P08-1064,P02-1038,0,0.183089,"Missing"
P08-1064,P03-1021,0,0.0106146,"s (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h ( d = 1 and h = 2 for the SCFG; d = 1 and h = 6 for the STSG; d = 4 and h = 6 for our model). We optimized these parameters on the training and develo"
P08-1064,J04-4002,0,0.538035,"pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al.,"
P08-1064,P02-1040,0,0.104502,"trained the translation model on the FBIS corpus (7.2M+9.2M words) and trained a 4gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h ( d = 1"
P08-1064,P05-1034,0,0.739531,"icantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phra"
P08-1064,P07-1090,1,0.843636,"07b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003). The formally syntax-based model for SMT was first advocated by Wu (1997). Xiong et al. (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997) while Setiawan et al. (2007) propose a function word-based reordering model for BTG. Chiang (2005)’s hierarchal phrase-based model achieves significant performance improvement. However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and syntax-based methods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the targe"
P08-1064,J97-3002,0,0.763933,"sh translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tr"
P08-1064,P01-1067,0,0.813115,"modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence 1"
P08-1064,zhang-etal-2004-interpreting,0,0.20435,"Missing"
P08-1064,2006.amta-papers.8,0,\N,Missing
P08-1064,C00-2092,0,\N,Missing
P08-1064,P03-1054,0,\N,Missing
P08-1064,P06-1123,0,\N,Missing
P08-1064,P06-1066,0,\N,Missing
P08-1064,P03-1011,0,\N,Missing
P08-1064,P07-2045,0,\N,Missing
P08-1064,N06-1032,0,\N,Missing
P08-1064,W06-1628,0,\N,Missing
P08-1064,J08-3004,0,\N,Missing
P08-2038,P03-1021,0,0.0408658,"Missing"
P08-2038,D07-1077,0,0.060863,"ng Transduction Grammar (BTG) proposed by (Wu, 1997) has been widely used in statistical machine translation (SMT). However, the original BTG does not provide an effective mechanism to predict the most appropriate orders between two neighboring phrases. To address this problem, Xiong et al. (2006) enhance the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual phrases as features. Although this model outperforms previous unlexicalized models, it does not utilize any linguistically syntactic features, which have proven useful for phrase reordering (Wang et al., 2007). Zhang et al. (2007) integrates source-side syntactic knowledge into a phrase reordering model based on BTG-style rules. However, one limitation of this method is that it only reorders syntactic phrases because linguistic knowledge from parse trees is only carried by syntactic phrases as far as reordering is concerned, while non-syntactic phrases are combined monotonously with a flat reordering score. In this paper, we propose a linguistically annotated reordering model for BTG-based SMT, which is a significant extension to the work mentioned above. The new model annotates each BTG node with"
P08-2038,J97-3002,0,0.403967,"Missing"
P08-2038,I05-1007,1,0.90995,"Missing"
P08-2038,P06-1066,1,0.929388,"nclude in Section 5. 2 Baseline SMT System The baseline system is a phrase-based system which uses the BTG lexical rules (A → x/y) to translate source phrase x into target phrase y and the BTG merging rules (A → [A, A]|hA, Ai) to combine two neighboring phrases with a straight or inverted order. The BTG lexical rules are weighted with several features, such as phrase translation, word penalty and language models, in a log-linear form. For the merging rules, a MaxEnt-based reordering model using boundary words of neighboring phrases as features is used to predict the merging order, similar to (Xiong et al., 2006). We call this reordering model 149 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 149–152, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics boundary words based reordering model (BWR). In this paper, we propose to incorporate a linguistically annotated reordering model into the log-linear translation model, so as to strengthen the BWR’s phrase reordering ability. We train all the model scaling factors on the development set to maximize the BLEU score. A CKY-style decoder is developed to generate the best BTG binary tree for each input senten"
P08-2038,D07-1056,0,0.0632565,"posed by (Wu, 1997) has been widely used in statistical machine translation (SMT). However, the original BTG does not provide an effective mechanism to predict the most appropriate orders between two neighboring phrases. To address this problem, Xiong et al. (2006) enhance the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual phrases as features. Although this model outperforms previous unlexicalized models, it does not utilize any linguistically syntactic features, which have proven useful for phrase reordering (Wang et al., 2007). Zhang et al. (2007) integrates source-side syntactic knowledge into a phrase reordering model based on BTG-style rules. However, one limitation of this method is that it only reorders syntactic phrases because linguistic knowledge from parse trees is only carried by syntactic phrases as far as reordering is concerned, while non-syntactic phrases are combined monotonously with a flat reordering score. In this paper, we propose a linguistically annotated reordering model for BTG-based SMT, which is a significant extension to the work mentioned above. The new model annotates each BTG node with linguistic knowledge"
P08-2040,P07-2045,0,0.0115781,"ta again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this paper, we further exploit the potential of the N-best hypotheses and propose several schemes to derive the posterior knowledge from the N-best hypotheses, in an effort to enhance the language model, translation model, and source word reordering under a re-decoding framework of any phrase-based SMT system. 2 Self-Enhancement Knowledge with Posterior The self-enhancement system structure is shown in Figure 1. Our baseline system is set up using Moses (Koehn et al., 2007), a state-of-the-art phrase-base SMT open source package. In the followings, we detail the approaches to exploiting the three different kinds of posterior knowledge, namely, language model, translation model and word reordering. 157 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 157–160, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 4. Repeat step 1-3 for a fixed number of iterations. 2.2 Figure 1: Self-enhancement system structure, where TM is translation model, LM is language model, and RM is reordering model. 2.1 Language Model We consi"
P08-2040,P07-1091,0,0.0782534,"hence improve the translation model. The procedure for translation model selfenhancement can be summarized as follows. 1. Run decoding and extract N-best hypotheses. 2. Extract “good phrase-pairs” according to the hypotheses’ phrase-alignment information and append them to the original phrase table to generate a new phrase table. 3. Score the new phrase table to create a new translation model. 4. Optimize the weights of the decoder with the above new translation model. 5. Repeat step 1-4 for a fixed number of iterations. 2.3 Word Reordering Some previous work (Costa-jussà and Fonollosa, 2006; Li et al., 2007) have shown that reordering a source sentence to match the word order in its corresponding target sentence can produce better translations for a phrase-based SMT system. We bring this idea forward to our word reordering selfenhancement framework, which similarly translates a source sentence (S) to target sentence (T) in two stages: S → S ′ → T , where S ′ is the reordered source sentence. The phrase-alignment information in each hypothesis indicates the word reordering for source sentence. We select the word reordering with the highest posterior probability as the best word reordering for a gi"
P08-2040,C02-1164,0,0.139094,"approaches to exploiting the three different kinds of posterior knowledge, namely, language model, translation model and word reordering. 157 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 157–160, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 4. Repeat step 1-3 for a fixed number of iterations. 2.2 Figure 1: Self-enhancement system structure, where TM is translation model, LM is language model, and RM is reordering model. 2.1 Language Model We consider self-enhancement of language model as a language model adaptation problem similar to (Nakajima et al., 2002). The original monolingual target training data is regarded as general-domain data while the test data as a domain-specific data. Obviously, the real domain-specific target data (test data) is unavailable for training. In this work, the N-best hypotheses of the test set are used as a quasi-corpus to train a language model. This new language model trained on the quasi-corpus is then used together with the language model trained on the general-domain data (original training data) to produce a new list of N-best hypotheses under our self-enhancement framework. The feature function of the language"
P08-2040,P02-1040,0,0.0731461,"Missing"
P08-2040,2006.iwslt-papers.3,0,0.553265,"erence between the two models. Nakajima et al. used only 1-best hypothesis, while we use N-best hypotheses of test set as the quasicorpus to train the language model. In the work of (Costa-jussà and Fonollosa, 2006; Li et al., 2007) which similarly translates a source sentence (S) to target sentence (T) in two stages: S → S ′ → T , they derive S ′ from training data; while we obtain S ′ based on the occurrence frequency, i.e. posterior probability of each source word reordering in the N-best hypotheses list. An alternative solution for enhancing the translation model is through self-training (Ueffing, 2006; Ueffing et al., 2007) which re-trains the source-target N-best hypotheses together with the original training data, and thus differs from ours in the way of new phrase pairs extraction. We only supplement those phrase-pairs appeared in the Nbest hypotheses to the original phrase table. Further experiment showed that improvement obtained by self-training method is not as consistent on both development and test sets as that by our method. One possible reason is that in self-training, the entire translation model is adjusted with the addition of new phrase-pairs extracted from the source-target"
P08-2040,2003.mtsummit-papers.52,0,0.0248546,"he MT system as the first decoding has discarded many undesirable translation candidates. Thus, the knowledge captured in the N-best hypotheses, such as posterior probabilities for words, n-grams, phrase-pairs, and source word reorderings, etc. is more compatible with the source sentences and thus could potentially be used to improve the translation performance. Word posterior probabilities estimated from the N-best hypotheses have been widely used for confidence measure in automatic speech recognition (Wessel, 2002) and have also been adopted into machine translation. Blatz et al. (2003) and Ueffing et al. (2003) used word posterior probabilities to estimate the confidence of machine translation. Chen et al. (2005), Zens and Ney (2006) reported performance improvements by computing target ngrams posterior probabilities estimated on the Nbest hypotheses in a rescoring framework. Transductive learning method (Ueffing et al., 2007) which repeatedly re-trains the generated sourcetarget N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this paper, we further exploit the"
P08-2040,P07-1004,0,0.750203,"ed to improve the translation performance. Word posterior probabilities estimated from the N-best hypotheses have been widely used for confidence measure in automatic speech recognition (Wessel, 2002) and have also been adopted into machine translation. Blatz et al. (2003) and Ueffing et al. (2003) used word posterior probabilities to estimate the confidence of machine translation. Chen et al. (2005), Zens and Ney (2006) reported performance improvements by computing target ngrams posterior probabilities estimated on the Nbest hypotheses in a rescoring framework. Transductive learning method (Ueffing et al., 2007) which repeatedly re-trains the generated sourcetarget N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this paper, we further exploit the potential of the N-best hypotheses and propose several schemes to derive the posterior knowledge from the N-best hypotheses, in an effort to enhance the language model, translation model, and source word reordering under a re-decoding framework of any phrase-based SMT system. 2 Self-Enhancement Knowledge with Posterior T"
P08-2040,W06-3110,0,0.0123789,"-best hypotheses, such as posterior probabilities for words, n-grams, phrase-pairs, and source word reorderings, etc. is more compatible with the source sentences and thus could potentially be used to improve the translation performance. Word posterior probabilities estimated from the N-best hypotheses have been widely used for confidence measure in automatic speech recognition (Wessel, 2002) and have also been adopted into machine translation. Blatz et al. (2003) and Ueffing et al. (2003) used word posterior probabilities to estimate the confidence of machine translation. Chen et al. (2005), Zens and Ney (2006) reported performance improvements by computing target ngrams posterior probabilities estimated on the Nbest hypotheses in a rescoring framework. Transductive learning method (Ueffing et al., 2007) which repeatedly re-trains the generated sourcetarget N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this paper, we further exploit the potential of the N-best hypotheses and propose several schemes to derive the posterior knowledge from the N-best hypotheses,"
P08-2040,C04-1046,0,\N,Missing
P08-2040,W06-1609,0,\N,Missing
P08-2040,2005.iwslt-1.11,1,\N,Missing
P09-1020,P03-2041,0,0.260919,"work while section 3 defines our translation model. In section 4 and section 5, the key rule extraction and decoding algorithms are elaborated. Experimental results are reported in section 6 and the paper is concluded in section 7. 2 Related work As discussed in section 1, two of the major challenges to syntax-based SMT are structure divergence and parse errors. Many techniques have been proposed to address the structure divergence issue while only fewer studies are reported in addressing the parse errors in the SMT research community. To address structure divergence issue, many researchers (Eisner, 2003; Zhang et al., 2007) propose using the Synchronous Tree Substitution Grammar (STSG) grammar in syntax-based SMT since the STSG uses larger tree fragment as translation unit. Although promising results have been reported, STSG only uses one single subtree as translation unit which is still committed to the syntax strictly. Motivated by the fact that non-syntactic phrases make non-trivial contribution to phrase-based SMT, the tree sequencebased translation model is proposed (Liu et al., 2007; Zhang et al., 2008a) that uses tree sequence as the basic translation unit, rather than using single su"
P09-1020,N04-1035,0,0.445227,"as translation input, where a forest is a compact representation of exponentially number of n-best parse trees. Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. By using forest in rule extraction and decoding, their methods are able to well address the parse error issue. From the above discussion, we can see that traditional tree sequence-based method uses single tree as translation input while the forestbased model uses single sub-tree as the basic translation unit that can only learn tree-to-string (Galley et al. 2004; Liu et al., 2006) rules. Therefore, the two methods display different strengths, and which would be complementary to each other. To integrate their strengths, in this paper, we propose a forest-based tree sequence to string translation model. 3 Forest-based tree sequence to string model In this section, we first explain what a packed forest is and then define the concept of the tree sequence in the context of forest followed by the discussion on our proposed model. 3.1 Packed Forest A packed forest (forest in short) is a special kind of hyper-graph (Klein and Manning, 2001; Huang and Chiang,"
P09-1020,P08-1067,0,0.479802,"ms. 1 Introduction Recently syntax-based statistical machine translation (SMT) methods have achieved very promising results and attracted more and more interests in the SMT research community. Fundamentally, syntax-based SMT views translation as a structural transformation process. Therefore, structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT (Zhang et al., 2008a; Mi et al., 2008). Many solutions have been proposed to address the above two issues. Among these advances, forest-based modeling (Mi et al., 2008; Mi and Huang, 2008) and tree sequence-based modeling (Liu et al., 2007; Zhang et al., 2008a) are two interesting modeling methods with promising results reported. Forest-based modeling aims to improve translation accuracy through digging the potential better parses from n-bests (i.e. forest) while tree sequence-based modeling aims to model non-syntactic translations with structured syntactic knowledge. In nature, the two methods would be complementary to each other since they manage to solve the negative impacts of monolingual parse errors and cross-lingual structure divergence on translation results from differ"
P09-1020,P07-1019,0,0.172606,"vel process in a small span. Finally, we re-build the NSS of current span for upper level NSS combination use (line 20-22). In Fig. 8, the hyper-edge “IP=&gt;NP VV+VV NP” is an auxiliary hyper-edge introduced by Algorithm 2. By Algorithm 2, we convert the translation forest into a complete translation forest. We then use a bottom-up node-based search 4 The concept of translation forest is proposed in Mi et al. (2008). It is a forest that consists of only the hyperedges induced from translation rules. algorithm to do decoding on the complete translation forest. We also use Cube Pruning algorithm (Huang and Chiang 2007) to speed up the translation process. Figure 8. Auxiliary hyper-edge in a translation forest Algorithm 2. add auxiliary hyper-edges into mt forest F Input: mt forest F Output: complete forest F with auxiliary hyper-edges 1. for i := 1 to L do 2. for each node n of span [i, i] do 3. add n into NSS(i, i) 4. for length := 1 to L - 1 do 5. for start := 1 to L - length do 6. stop := start + length 7. for pivot := start to stop-1 do 8. for each ns1 in NSS (start, pivot) do for each ns2 in NSS (pivot+1,stop) do 9. 10. create 1 2 11. if ns is not in NSS(start, stop) then 12. add ns into NSS (start, st"
P09-1020,W01-1812,0,0.155074,"y learn tree-to-string (Galley et al. 2004; Liu et al., 2006) rules. Therefore, the two methods display different strengths, and which would be complementary to each other. To integrate their strengths, in this paper, we propose a forest-based tree sequence to string translation model. 3 Forest-based tree sequence to string model In this section, we first explain what a packed forest is and then define the concept of the tree sequence in the context of forest followed by the discussion on our proposed model. 3.1 Packed Forest A packed forest (forest in short) is a special kind of hyper-graph (Klein and Manning, 2001; Huang and Chiang, 2005), which is used to represent all derivations (i.e. parse trees) for a given sentence under a context free grammar (CFG). A forest F is defined as a triple , , , where is non-terminal node set, is hyper-edge set and is leaf node set (i.e. all sentence words). A forest F satisfies the following two conditions: 1) Each node in should cover a phrase, which is a continuous word sub-sequence in . 2) Each hyper-edge in is defined as … … , , , where … … covers a sequence of contiis the father nuous and non-overlap phrases, node of the children sequence … … . The is just the su"
P09-1020,N03-1017,0,0.0217877,"Missing"
P09-1020,P07-2045,0,0.0395597,"Missing"
P09-1020,P06-1077,0,0.317991,"t, where a forest is a compact representation of exponentially number of n-best parse trees. Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. By using forest in rule extraction and decoding, their methods are able to well address the parse error issue. From the above discussion, we can see that traditional tree sequence-based method uses single tree as translation input while the forestbased model uses single sub-tree as the basic translation unit that can only learn tree-to-string (Galley et al. 2004; Liu et al., 2006) rules. Therefore, the two methods display different strengths, and which would be complementary to each other. To integrate their strengths, in this paper, we propose a forest-based tree sequence to string translation model. 3 Forest-based tree sequence to string model In this section, we first explain what a packed forest is and then define the concept of the tree sequence in the context of forest followed by the discussion on our proposed model. 3.1 Packed Forest A packed forest (forest in short) is a special kind of hyper-graph (Klein and Manning, 2001; Huang and Chiang, 2005), which is us"
P09-1020,P07-1089,0,0.738499,"tical machine translation (SMT) methods have achieved very promising results and attracted more and more interests in the SMT research community. Fundamentally, syntax-based SMT views translation as a structural transformation process. Therefore, structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT (Zhang et al., 2008a; Mi et al., 2008). Many solutions have been proposed to address the above two issues. Among these advances, forest-based modeling (Mi et al., 2008; Mi and Huang, 2008) and tree sequence-based modeling (Liu et al., 2007; Zhang et al., 2008a) are two interesting modeling methods with promising results reported. Forest-based modeling aims to improve translation accuracy through digging the potential better parses from n-bests (i.e. forest) while tree sequence-based modeling aims to model non-syntactic translations with structured syntactic knowledge. In nature, the two methods would be complementary to each other since they manage to solve the negative impacts of monolingual parse errors and cross-lingual structure divergence on translation results from different viewpoints. Therefore, one natural way is to co"
P09-1020,P08-1023,0,0.650639,"rimental results on the NIST MT-2003 Chinese-English translation task show that our method statistically significantly outperforms the four baseline systems. 1 Introduction Recently syntax-based statistical machine translation (SMT) methods have achieved very promising results and attracted more and more interests in the SMT research community. Fundamentally, syntax-based SMT views translation as a structural transformation process. Therefore, structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT (Zhang et al., 2008a; Mi et al., 2008). Many solutions have been proposed to address the above two issues. Among these advances, forest-based modeling (Mi et al., 2008; Mi and Huang, 2008) and tree sequence-based modeling (Liu et al., 2007; Zhang et al., 2008a) are two interesting modeling methods with promising results reported. Forest-based modeling aims to improve translation accuracy through digging the potential better parses from n-bests (i.e. forest) while tree sequence-based modeling aims to model non-syntactic translations with structured syntactic knowledge. In nature, the two methods would be complementary to each other"
P09-1020,D08-1022,0,0.756548,"e systems. 1 Introduction Recently syntax-based statistical machine translation (SMT) methods have achieved very promising results and attracted more and more interests in the SMT research community. Fundamentally, syntax-based SMT views translation as a structural transformation process. Therefore, structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT (Zhang et al., 2008a; Mi et al., 2008). Many solutions have been proposed to address the above two issues. Among these advances, forest-based modeling (Mi et al., 2008; Mi and Huang, 2008) and tree sequence-based modeling (Liu et al., 2007; Zhang et al., 2008a) are two interesting modeling methods with promising results reported. Forest-based modeling aims to improve translation accuracy through digging the potential better parses from n-bests (i.e. forest) while tree sequence-based modeling aims to model non-syntactic translations with structured syntactic knowledge. In nature, the two methods would be complementary to each other since they manage to solve the negative impacts of monolingual parse errors and cross-lingual structure divergence on translation results from differ"
P09-1020,P02-1038,0,0.0183765,"n model is formulated as: Pr , , ∑ , , , ∏ By the above Eq., translation becomes a tree sequence structure to string mapping issue. Given the F, TS and A, there are multiple derivations that could map F to TS under the constraint A. in our The mapping probability Pr , , study is obtained by summing over the probabilities of all derivations Θ. The probability of each derivation is given as the product of the probabilities of all the rules p (ri ) used in the derivation (here we assume that each rule is applied independently in a derivation). Our model is implemented under log-linear framework (Och and Ney, 2002). We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2003): 1) bidirectional rule mapping probabilities, 2) bidirectional lexical rule translation probabilities, 3) target language model, 4) number of rules used and 5) number of target words. In addition, we define two new features: 1) number of leaf nodes in auxiliary rules (the auxiliary rule will be explained later in this paper) and 2) product of the probabilities of all hyper-edges of the tree sequences in forest. 4 Training This section discusses how to extract our transla. As we"
P09-1020,P03-1021,0,0.400511,"Missing"
P09-1020,J03-1002,0,0.00270206,"Missing"
P09-1020,P02-1040,0,0.105492,"Missing"
P09-1020,zhang-etal-2004-interpreting,0,0.0526669,"Missing"
P09-1020,2006.amta-papers.8,0,\N,Missing
P09-1020,A00-2018,0,\N,Missing
P09-1020,C08-1138,1,\N,Missing
P09-1020,W05-1506,0,\N,Missing
P09-1020,P08-1064,1,\N,Missing
P09-1036,P08-1009,0,0.602914,"uent translations and reorderings. This, unfortunately, significantly jeopardizes performance (Koehn et al., 2003; Xiong et al., 2008) because by integrating syntactic constraint into decoding as a hard constraint, it simply prohibits any other useful non-syntactic translations which violate constituent boundaries. To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. These constituent matching/violation counts are used as a feature in the decoder’s log-linear model and their weights are tuned via minimal error rate training (MERT) (Och, 2003). In this way, syntactic constraint is integrated into decoding as a soft constraint to enable the decoder to reward hypotheses that respect syntactic analyses or to peSyntactic analysis influences the way in which the source sentence is translated. Previous efforts add syntactic constraints to phrase-based translation by directly rewarding/punishi"
P09-1036,P05-1033,0,0.731805,"nly on phrase movement but also on the lexical selection for the multi-meaning word “节”1 . To avert such errors, the decoder can fully respect linguistic structures by only allowing syntactic constituent translations and reorderings. This, unfortunately, significantly jeopardizes performance (Koehn et al., 2003; Xiong et al., 2008) because by integrating syntactic constraint into decoding as a hard constraint, it simply prohibits any other useful non-syntactic translations which violate constituent boundaries. To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. These constituent matching/violation counts are used as a feature in the decoder’s log-linear model and their weights are tuned via minimal error rate training (MERT) (Och, 2003). In this way, syntactic constraint is integrated into decoding as a soft constraint to enable the decoder to reward hypotheses that respect syntactic"
P09-1036,D08-1024,0,0.0439355,"Missing"
P09-1036,N03-1017,0,0.34237,"r inadequately breaks up the second NP phrase and translates the two words “航海” and “节” separately. However, the parse tree of the source fragment constrains the phrase “航海 节” to be translated as a unit. Without considering syntactic constraints from the parse tree, the decoder makes wrong decisions not only on phrase movement but also on the lexical selection for the multi-meaning word “节”1 . To avert such errors, the decoder can fully respect linguistic structures by only allowing syntactic constituent translations and reorderings. This, unfortunately, significantly jeopardizes performance (Koehn et al., 2003; Xiong et al., 2008) because by integrating syntactic constraint into decoding as a hard constraint, it simply prohibits any other useful non-syntactic translations which violate constituent boundaries. To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. These constituent matching/v"
P09-1036,W04-3250,0,0.433632,"Missing"
P09-1036,C02-1003,0,0.0654943,"Missing"
P09-1036,P08-1114,0,0.39465,"ly allowing syntactic constituent translations and reorderings. This, unfortunately, significantly jeopardizes performance (Koehn et al., 2003; Xiong et al., 2008) because by integrating syntactic constraint into decoding as a hard constraint, it simply prohibits any other useful non-syntactic translations which violate constituent boundaries. To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. These constituent matching/violation counts are used as a feature in the decoder’s log-linear model and their weights are tuned via minimal error rate training (MERT) (Och, 2003). In this way, syntactic constraint is integrated into decoding as a soft constraint to enable the decoder to reward hypotheses that respect syntactic analyses or to peSyntactic analysis influences the way in which the source sentence is translated. Previous efforts add syntactic constraints to phrase-based translation by directly"
P09-1036,P00-1056,0,0.707896,"Missing"
P09-1036,P03-1021,0,0.160074,"on-syntactic translations which violate constituent boundaries. To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. These constituent matching/violation counts are used as a feature in the decoder’s log-linear model and their weights are tuned via minimal error rate training (MERT) (Och, 2003). In this way, syntactic constraint is integrated into decoding as a soft constraint to enable the decoder to reward hypotheses that respect syntactic analyses or to peSyntactic analysis influences the way in which the source sentence is translated. Previous efforts add syntactic constraints to phrase-based translation by directly rewarding/punishing a hypothesis whenever it matches/violates source-side constituents. We present a new model that automatically learns syntactic constraints, including but not limited to constituent matching/violation, from training corpus. The model brackets a sou"
P09-1036,P02-1040,0,0.0758415,"ot use any syntactic constraints on Chinese-to-English translation. To compare with the CMVC, we also conduct experiments using (Marton and Resnik, 2008)’s XP+. The XP+ accumulates a count for each hypothesis whenever it violates the boundaries of a constituent with a label from {NP, VP, CP, IP, PP, ADVP, QP, LCP, DNP}. The XP+ is the best feature among all features that Marton and Resnik use for Chinese-toEnglish translation. Our experimental results display that our SDB model achieves a substantial improvement over the baseline and significantly outperforms XP+ according to the BLEU metric (Papineni et al., 2002). In addition, our analysis shows further evidences of the performance gain from a different perspective than that of BLEU. The paper proceeds as follows. In section 2 we describe how to learn bracketing instances from a training corpus. In section 3 we elaborate the syntax-driven bracketing model, including feature generation and the integration of the SDB model into phrase-based SMT. In section 4 and 5, we present our experiments and analysis. And we finally conclude in section 6. 2 The Acquisition of Bracketing Instances In this section, we formally define the bracketing instance, comprisin"
P09-1036,J97-3002,0,0.783539,"Missing"
P09-1036,I05-1007,1,0.887578,"Missing"
P09-1036,P06-1066,1,0.890811,"ndaries of a constituent. Otherwise, a lower probability is given. Through this additional feature, we want the decoder to prefer hypotheses that translate source spans which can be translated as a unit, and avoids translating those which are discontinuous after translation. The weight of this new feature is tuned via MERT, which measures the extent to which this feature should be trusted. In this paper, we implement the SDB model in a state-of-the-art phrase-based system which adapts a binary bracketing transduction grammar (BTG) (Wu, 1997) to phrase translation and reordering, described in (Xiong et al., 2006). Whenever a BTG merging rule (s → [s1 s2 ] or s → hs1 s2 i) is used, the SDB model gives a probability to the span s covered by the rule, which estimates the extent to which the span is bracketable. For the unary SDB model, we only consider the features from τ (s). For the binary SDB model, we use all features from τ (s1 ), τ (s2 ) and τ (s) since the binary SDB model is naturally suitable to the binary BTG rules. The SDB model, however, is not only limited to phrase-based SMT using BTG rules. Since it is applied on a source span each time, any other hierarchical phrase-based or syntax-based"
P09-1036,C08-1127,1,0.822854,"s up the second NP phrase and translates the two words “航海” and “节” separately. However, the parse tree of the source fragment constrains the phrase “航海 节” to be translated as a unit. Without considering syntactic constraints from the parse tree, the decoder makes wrong decisions not only on phrase movement but also on the lexical selection for the multi-meaning word “节”1 . To avert such errors, the decoder can fully respect linguistic structures by only allowing syntactic constituent translations and reorderings. This, unfortunately, significantly jeopardizes performance (Koehn et al., 2003; Xiong et al., 2008) because by integrating syntactic constraint into decoding as a hard constraint, it simply prohibits any other useful non-syntactic translations which violate constituent boundaries. To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. These constituent matching/violation counts are u"
P09-1036,W02-1039,0,\N,Missing
P09-1036,2005.iwslt-1.8,0,\N,Missing
P09-1103,W04-3312,0,0.181174,"Missing"
P09-1103,P01-1067,0,0.0458264,"5 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et a"
P09-1103,P03-1011,0,0.0228204,"that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them,"
P09-1103,P08-1064,1,0.555183,"; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs1, and find them useless via real syntax-based translation systems. However, Wellington et al. (2006) statistically report that discontinuities are very useful for translational equivalence analysis using binary branching structures under word alignment and parse tree constraints. Bod (2007) also finds that discontinues phrasal rules make significant improvement in linguistically motivated STSG-based translation model. The above observations are conflicting to each other. In our opinion, the non-contiguous phras"
P09-1103,2003.mtsummit-papers.22,0,0.0301668,"ch in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs1, and find them useless"
P09-1103,P07-2045,0,0.00914277,"Missing"
P09-1103,P06-1077,0,0.044472,"oduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence"
P09-1103,W02-1018,0,0.0451173,"contiguous tree sequencebased model, the proposed model can well handle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment. An algorithm targeting the noncontiguous constituent decoding is also proposed. Experimental results on the NIST MT-05 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic"
P09-1103,J04-4002,0,0.0988341,"proposed model can well handle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment. An algorithm targeting the noncontiguous constituent decoding is also proposed. Experimental results on the NIST MT-05 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equival"
P09-1103,P05-1034,0,0.0295013,"y significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-conti"
P09-1103,2007.mtsummit-papers.8,0,\N,Missing
P09-1103,C08-1138,1,\N,Missing
P09-1103,P03-1054,0,\N,Missing
P09-1103,P02-1040,0,\N,Missing
P09-1103,P06-1123,0,\N,Missing
P09-1103,P07-1089,0,\N,Missing
P09-1103,P05-1067,0,\N,Missing
P09-1103,P03-2041,0,\N,Missing
P09-1103,W06-1606,0,\N,Missing
P09-1103,N03-1017,0,\N,Missing
P09-1103,P06-1121,0,\N,Missing
P09-1103,zhang-etal-2004-interpreting,0,\N,Missing
P09-1106,C08-1005,0,0.185663,"Missing"
P09-1106,C08-1014,1,0.832383,"e (2005) proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alignment of hypotheses using synonyms as found in WordNet (Fellbaum, 1998) and a two-pass alignment strategy based on TER word alignment approach. He et al. (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. Although many methods have been attempted, no systematic comparison among them has been reported. A through and fair comparison among them would be of great meaning to t"
P09-1106,J07-2003,0,0.0421201,"the training, dev and test data for IWSLT and NIST tasks. task data Sent. Words Dev Sent. IWSLT Words Test Sent. Words Add. Words Train Sent. Words Dev Sent. NIST 2002 Words Test Sent. 2005 Words Add. Words Train Ch En 406K 4.4M 4.6M 489 489 × 7 5,896 45,449 500 500 × 7 6,296 51,227 1.7M 238K 7.0M 8.9M 878 878 × 4 23,248 108,616 1,082 1,082 × 4 30,544 141,915 61.5M Table 1: Statistics of training, dev and test data for IWSLT and NIST tasks. In both experiments, we used four systems, as listed in Table 2, they are phrase-based system Moses (Koehn et al., 2007), hierarchical phrasebased system (Chiang, 2007), BTG-based lexicalized reordering phrase-based system (Xiong et al., 2006) and a tree sequence alignment-based tree-to-tree translation system (Zhang et al., 2008). Each system for the same task is trained on the same data set. 4.2 Experiments setting For each system, we used the top 10 scored hypotheses to build the confusion network. Similar to (Rosti et al., 2007a), each word in the hypothesis is assigned with a rank-based score of 1/ (1 + r ) , where r is the rank of the hypothesis. And we assign the same weights to each system. For selecting the backbone, only the top hypothesis from eac"
P09-1106,D08-1011,0,0.634478,"um Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alignment of hypotheses using synonyms as found in WordNet (Fellbaum, 1998) and a two-pass alignment strategy based on TER word alignment approach. He et al. (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. Although many methods have been attempted, no systematic comparison among them has been reported. A through and fair comparison among them would be of great meaning to the MT sys941 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 941–948, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP tem combination research. In this paper, we implement a confusion network-based decoder. Based on this decoder, we compare four commonly used wo"
P09-1106,D07-1029,0,0.129059,"Missing"
P09-1106,P05-3026,0,0.0260536,"is. 3) Confusion network construction: to build a confusion network based on hypothesis alignments. 4) Confusion network decoding: to decode the best translation from a confusion network. Among the four steps, the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems (Rosti et al, 2007). Many techniques have been studied to address this issue. Bangalore et al. (2001) used the edit distance alignment algorithm which is extended to multiple strings to build confusion network, it only allows monotonic alignment. Jayaraman and Lavie (2005) proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et"
P09-1106,P08-2021,0,0.037903,"fusion network, it only allows monotonic alignment. Jayaraman and Lavie (2005) proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alignment of hypotheses using synonyms as found in WordNet (Fellbaum, 1998) and a two-pass alignment strategy based on TER word alignment approach. He et al. (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. Although many methods have been attempted, no systematic comparison among them has been reported. A th"
P09-1106,C04-1183,1,0.869719,"Missing"
P09-1106,P07-2045,0,0.00783503,"Missing"
P09-1106,N04-1022,0,0.147586,"cludes the paper. 2 Confusion network combination based system In order to compare different hypothesis alignment methods, we implement a confusion network decoding system as follows: Backbone selection: in the previous work, Matusov et al. (2006, 2008) let every hypothesis play the role of the backbone (also called “skeleton” or “alignment reference”) once. We follow the work of (Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; He et al., 2008) and choose the hypothesis that best agrees with other hypotheses on average as the backbone by applying Minimum Bayes Risk (MBR) decoding (Kumar and Byrne, 2004). TER score (Snover et al, 2006) is used as the loss function in MBR decoding. Given a hypothesis set H, the backbone can be computed using the following equation, where TER(•, •) returns the TER score of two hypotheses. Eb = arg min ∑ TER ( Eˆ , E ) Eˆ ∈H (1) E∈H Hypothesis alignment: all hypotheses are word-aligned to the corresponding backbone in a many-to-one manner. We apply four word alignment methods: GIZA++-based, TER-based, CLA-based, and IHMM-based word alignment algorithm. For each method, we will give details in the next section. Confusion network construction: confusion network is"
P09-1106,E06-1031,0,0.0608531,"Missing"
P09-1106,E06-1005,0,0.490885,"confusion network. Among the four steps, the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems (Rosti et al, 2007). Many techniques have been studied to address this issue. Bangalore et al. (2001) used the edit distance alignment algorithm which is extended to multiple strings to build confusion network, it only allows monotonic alignment. Jayaraman and Lavie (2005) proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alig"
P09-1106,J00-2004,0,0.132613,"allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alignment of hypotheses using synonyms as found in WordNet (Fellbaum, 1998) and a two-pass alignment strategy based on TER word alignment approach. He et al. (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. Although many methods have been attempted, no systematic comparison among them has been reported. A through and fair comparison among them would be of great meaning to the MT sys941 Proceedings of the 47th Annual Meeting of th"
P09-1106,P03-1021,0,0.0185165,"probabilities (arc scores of the confusion network), • N-gram frequencies (Chen et al., 2005), • N-gram posterior probabilities (Zens and Ney, 2006). Word alignment algorithms We compare four word alignment methods which are widely used in confusion network based system combination or bilingual parallel corpora word alignment. bigram ei′ei′+1 observed in the hypothesis list; e2 Word penalty, The n-grams used in the last two feature functions are collected from the original hypotheses list from each single system. The weights of feature functions are optimized to maximize the scoring measure (Och, 2003). where p (ei′ei′+1 ) is the occurrence probability of e1 • Hypothesis-to-backbone ment word alignGIZA++: Matusov et al. (2006, 2008) proposed using GIZA++ (Och and Ney, 2003) to align words between the backbone and hypothesis. This method uses enhanced HMM model bootstrapped from IBM Model-1 to estimate the alignment model. All hypotheses of the whole test set are collected to create sentence pairs for GIZA++ training. GIZA++ produces hypothesisbackbone many-to-1 word alignments. TER-based: TER-based word alignment method (Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b) is an exte"
P09-1106,J03-1002,0,0.00892965,"lgorithms We compare four word alignment methods which are widely used in confusion network based system combination or bilingual parallel corpora word alignment. bigram ei′ei′+1 observed in the hypothesis list; e2 Word penalty, The n-grams used in the last two feature functions are collected from the original hypotheses list from each single system. The weights of feature functions are optimized to maximize the scoring measure (Och, 2003). where p (ei′ei′+1 ) is the occurrence probability of e1 • Hypothesis-to-backbone ment word alignGIZA++: Matusov et al. (2006, 2008) proposed using GIZA++ (Och and Ney, 2003) to align words between the backbone and hypothesis. This method uses enhanced HMM model bootstrapped from IBM Model-1 to estimate the alignment model. All hypotheses of the whole test set are collected to create sentence pairs for GIZA++ training. GIZA++ produces hypothesisbackbone many-to-1 word alignments. TER-based: TER-based word alignment method (Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b) is an extension of multiple string matching algorithm based on Levenshtein edit distance (Bangalore et al., 2001). The TER (translation error rate) score (Snover et al., 2006) measures"
P09-1106,P02-1040,0,0.0768188,"Missing"
P09-1106,N07-1029,0,0.405741,"ur word alignment methods on both Chinese-to-English spoken and written language tasks. 1 Introduction Machine translation (MT) system combination technique leverages on multiple MT systems to achieve better performance by combining their outputs. Confusion network based system combination for machine translation has shown promising advantage compared with other techniques based system combination, such as sentence level hypothesis selection by voting and source sentence re-decoding using the phrases or translation models that are learned from the source sentences and target hypotheses pairs (Rosti et al., 2007a; Huang and Papineni, 2007). In general, the confusion network based system combination method for MT consists of four steps: 1) Backbone selection: to select a backbone (also called “skeleton”) from all hypotheses. The backbone defines the word orders of the final translation. 2) Hypothesis alignment: to build word-alignment between backbone and each hypothesis. 3) Confusion network construction: to build a confusion network based on hypothesis alignments. 4) Confusion network decoding: to decode the best translation from a confusion network. Among the four steps, the hypothesis alignment pr"
P09-1106,P07-1040,0,0.145783,"Missing"
P09-1106,W08-0329,0,0.0334511,"e. Bangalore et al. (2001) used the edit distance alignment algorithm which is extended to multiple strings to build confusion network, it only allows monotonic alignment. Jayaraman and Lavie (2005) proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alignment of hypotheses using synonyms as found in WordNet (Fellbaum, 1998) and a two-pass alignment strategy based on TER word alignment approach. He et al. (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a"
P09-1106,2006.amta-papers.25,0,0.473331,", 2007). Many techniques have been studied to address this issue. Bangalore et al. (2001) used the edit distance alignment algorithm which is extended to multiple strings to build confusion network, it only allows monotonic alignment. Jayaraman and Lavie (2005) proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alignment of hypotheses using synonyms as found in WordNet (Fellbaum, 1998) and a two-pass alignment strategy based on TER word alignment approach. He et al. (2008) proposed an IHMM-based word alignmen"
P09-1106,takezawa-etal-2002-toward,0,0.0321791,"two links share a same hypothesis or backbone word and also satisfy the constraints, we choose the link that with the highest similarity score. For example, in Figure 2, since MCS-based similarity scores S ( shot , shoot ) > S ( shot , the) , we choose alignment (a). 4 4.1 Experiments and results Tasks and single systems Experiments are carried out in two domains. One is in spoken language domain while the other is on newswire corpus. Both experiments are on Chinese-to-English translation. Experiments on spoken language domain were carried out on the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2002) Chinese- to-English data augmented with HITcorpus1 . BTEC is a multilingual speech corpus which contains sentences spoken by tourists. 40K sentence-pairs are used in our experiment. HIT-corpus is a balanced corpus and has 500K sentence-pairs in total. We selected 360K sentence-pairs that are more similar to BTEC data according to its sub-topic. Additionally, the English sentences of Tanaka corpus2 were also used to train our language model. We ran experiments on an IWSLT challenge task which uses IWSLT20063 DEV clean text set as development set and IWSLT-2006 TEST clean text as test set. b Fi"
P09-1106,P06-1066,0,0.0199915,"ent. Words Dev Sent. IWSLT Words Test Sent. Words Add. Words Train Sent. Words Dev Sent. NIST 2002 Words Test Sent. 2005 Words Add. Words Train Ch En 406K 4.4M 4.6M 489 489 × 7 5,896 45,449 500 500 × 7 6,296 51,227 1.7M 238K 7.0M 8.9M 878 878 × 4 23,248 108,616 1,082 1,082 × 4 30,544 141,915 61.5M Table 1: Statistics of training, dev and test data for IWSLT and NIST tasks. In both experiments, we used four systems, as listed in Table 2, they are phrase-based system Moses (Koehn et al., 2007), hierarchical phrasebased system (Chiang, 2007), BTG-based lexicalized reordering phrase-based system (Xiong et al., 2006) and a tree sequence alignment-based tree-to-tree translation system (Zhang et al., 2008). Each system for the same task is trained on the same data set. 4.2 Experiments setting For each system, we used the top 10 scored hypotheses to build the confusion network. Similar to (Rosti et al., 2007a), each word in the hypothesis is assigned with a rank-based score of 1/ (1 + r ) , where r is the rank of the hypothesis. And we assign the same weights to each system. For selecting the backbone, only the top hypothesis from each system is considered as a candidate for the backbone. Concerning the four"
P09-1106,W06-3110,0,0.0565219,"Missing"
P09-1106,P08-1064,1,0.843256,"NIST 2002 Words Test Sent. 2005 Words Add. Words Train Ch En 406K 4.4M 4.6M 489 489 × 7 5,896 45,449 500 500 × 7 6,296 51,227 1.7M 238K 7.0M 8.9M 878 878 × 4 23,248 108,616 1,082 1,082 × 4 30,544 141,915 61.5M Table 1: Statistics of training, dev and test data for IWSLT and NIST tasks. In both experiments, we used four systems, as listed in Table 2, they are phrase-based system Moses (Koehn et al., 2007), hierarchical phrasebased system (Chiang, 2007), BTG-based lexicalized reordering phrase-based system (Xiong et al., 2006) and a tree sequence alignment-based tree-to-tree translation system (Zhang et al., 2008). Each system for the same task is trained on the same data set. 4.2 Experiments setting For each system, we used the top 10 scored hypotheses to build the confusion network. Similar to (Rosti et al., 2007a), each word in the hypothesis is assigned with a rank-based score of 1/ (1 + r ) , where r is the rank of the hypothesis. And we assign the same weights to each system. For selecting the backbone, only the top hypothesis from each system is considered as a candidate for the backbone. Concerning the four alignment methods, we use the default setting for GIZA++; and use toolkit TERCOM (Snover"
P09-1106,zhang-etal-2004-interpreting,0,0.0683296,"Missing"
P09-1106,2005.eamt-1.20,0,\N,Missing
P09-1106,2005.iwslt-1.11,1,\N,Missing
P09-1106,2006.iwslt-evaluation.15,0,\N,Missing
P09-4006,I08-2084,1,0.829486,"pulate the corpora in three levels of abstraction – clusters, documents and terms. And our key task over here is to find the underlying associations of documents or terminologies in each level across different languages. First, monolingual documents are grouped into clusters by k-means algorithm using simple word vectors. Then, monolingual noun terms are extracted from each cluster using linguistic patterns and filtered by occurrence statistics globally (within cluster) and locally (within document), so that they are good representatives for cluster as a whole as well as individual documents (Vu et al., 2008). The extracted terms are then used in document clustering in a new cycle and the whole process is repeated until the result converges. Next, cluster alignment is carried out between the pivot language (English) and the other languages (Chinese, Malay). Clusters can be conceptualized as the collection of documents with the same themes (e.g. finance, politics or sports) and their alignments as the correspondents in the other languages. Since there may be overlaps among themes, e.g. finance and economy, each cluster is allowed to align to more than one cluster with varying degree of alignment sc"
P09-4006,W03-1108,0,\N,Missing
P09-4006,E09-1096,1,\N,Missing
P10-1016,P09-1088,0,0.0329656,"Missing"
P10-1016,J93-2003,0,0.118946,"a kind of basic multi-word expression that characterizes minimal sequence of consecutive words in sense of translation. By casting pseudo-word searching problem into a parsing framework, we search for pseudo-words in a monolingual way and a bilingual synchronous way. Experiments show that pseudo-word significantly outperforms word for PB-SMT model in both travel translation domain and news translation domain. 1 Introduction The pipeline of most Phrase-Based Statistical Machine Translation (PB-SMT) systems starts from automatically word aligned parallel corpus generated from word-based models (Brown et al., 1993), proceeds with step of induction of phrase table (Koehn et al., 2003) or synchronous grammar (Chiang, 2007) and with model weights tuning step. Words are taken as inputs to PB-SMT at the very beginning of the pipeline. But there is a deficiency in such manner that word is too finegrained in some cases such as non-compositional phrasal equivalences, where clear word alignments do not exist. For example in Chinese-toEnglish translation, “ 想 ” and “would like to” constitute a 1-to-n phrasal equivalence, “多 少 钱” and “how much is it” constitute a m-to-n phrasal equivalence. No clear word alignment"
P10-1016,W08-0336,0,0.0431284,"tion word segmentation on languages where word boundaries are not orthographically marked. In Chineseto-English translation task where Chinese word boundaries are not marked, Xu et al. (2004) used word aligner to build a Chinese dictionary to resegment Chinese sentence. Xu et al. (2008) used a Bayesian semi-supervised method that combines Chinese word segmentation model and Chinese-to-English translation model to derive a Chinese segmentation suitable for machine translation. There are also researches focusing on the impact of various segmentation tools on machine translation (Ma et al. 2007; Chang et al. 2008; Zhang et al. 2008). Since there are many 1-to-n phrasal equivalences in Chinese-to-English translation (Ma and Way. 2009), only focusing on Chinese word as basic translational unit is not adequate to model 1-to-n translations. Ma and Way (2009) tackle this problem by using word aligner to bootstrap bilingual segmentation suitable for machine translation. Lambert and Banchs (2005) detect bilingual multi-word ex148 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 148–156, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Lingui"
P10-1016,W07-0403,0,0.0132778,"equivalence, “多 少 钱” and “how much is it” constitute a m-to-n phrasal equivalence. No clear word alignments are there in such phrasal equivalences. Moreover, should basic translational unit be word or coarsegrained multi-word is an open problem for optimizing SMT models. Some researchers have explored coarsegrained translational unit for machine translation. Marcu and Wong (2002) attempted to directly learn phrasal alignments instead of word alignments. But computational complexity is prohibitively high for the exponentially large number of decompositions of a sentence pair into phrase pairs. Cherry and Lin (2007) and Zhang et al. (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. Blunsom et al. (2008; 2009) induced phrasal synchronous grammar, which aimed at finding hierarchical phrasal equivalences. Another direction of questioning word as basic translational unit is to directly question word segmentation on languages where word boundaries are not orthographically marked. In Chineseto-English translation task where Chinese word boundaries are not marked, Xu et al. (2004) used word aligner to bui"
P10-1016,H05-1022,0,0.146358,"it is not adequate to model 1-to-n translations. Ma and Way (2009) tackle this problem by using word aligner to bootstrap bilingual segmentation suitable for machine translation. Lambert and Banchs (2005) detect bilingual multi-word ex148 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 148–156, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics pressions by monotonically segmenting a given Spanish-English sentence pair into bilingual units, where word aligner is also used. IBM model 3, 4, 5 (Brown et al., 1993) and Deng and Byrne (2005) are another kind of related works that allow 1-to-n alignments, but they rarely questioned if such alignments exist in word units level, that is, they rarely questioned word as basic translational unit. Moreover, m-ton alignments were not modeled. This paper focuses on determining the basic translational units on both language sides without using word aligner before feeding them into PBSMT pipeline. We call such basic translational unit as pseudo-word to differentiate with word. Pseudo-word is a kind of multi-word expression (includes both unary word and multi-word). Pseudo-word searching pro"
P10-1016,P07-2045,0,0.013411,"Missing"
P10-1016,J97-3002,0,0.185169,"l equivalence. No clear word alignments are there in such phrasal equivalences. Moreover, should basic translational unit be word or coarsegrained multi-word is an open problem for optimizing SMT models. Some researchers have explored coarsegrained translational unit for machine translation. Marcu and Wong (2002) attempted to directly learn phrasal alignments instead of word alignments. But computational complexity is prohibitively high for the exponentially large number of decompositions of a sentence pair into phrase pairs. Cherry and Lin (2007) and Zhang et al. (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. Blunsom et al. (2008; 2009) induced phrasal synchronous grammar, which aimed at finding hierarchical phrasal equivalences. Another direction of questioning word as basic translational unit is to directly question word segmentation on languages where word boundaries are not orthographically marked. In Chineseto-English translation task where Chinese word boundaries are not marked, Xu et al. (2004) used word aligner to build a Chinese dictionary to resegment Chinese sentence. X"
P10-1016,W04-3250,0,0.0430311,"into phrase pairs. Cherry and Lin (2007) and Zhang et al. (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. Blunsom et al. (2008; 2009) induced phrasal synchronous grammar, which aimed at finding hierarchical phrasal equivalences. Another direction of questioning word as basic translational unit is to directly question word segmentation on languages where word boundaries are not orthographically marked. In Chineseto-English translation task where Chinese word boundaries are not marked, Xu et al. (2004) used word aligner to build a Chinese dictionary to resegment Chinese sentence. Xu et al. (2008) used a Bayesian semi-supervised method that combines Chinese word segmentation model and Chinese-to-English translation model to derive a Chinese segmentation suitable for machine translation. There are also researches focusing on the impact of various segmentation tools on machine translation (Ma et al. 2007; Chang et al. 2008; Zhang et al. 2008). Since there are many 1-to-n phrasal equivalences in Chinese-to-English translation (Ma and Way. 2009), only focusing on Chinese word as basic translatio"
P10-1016,P07-1039,0,0.491414,"to directly question word segmentation on languages where word boundaries are not orthographically marked. In Chineseto-English translation task where Chinese word boundaries are not marked, Xu et al. (2004) used word aligner to build a Chinese dictionary to resegment Chinese sentence. Xu et al. (2008) used a Bayesian semi-supervised method that combines Chinese word segmentation model and Chinese-to-English translation model to derive a Chinese segmentation suitable for machine translation. There are also researches focusing on the impact of various segmentation tools on machine translation (Ma et al. 2007; Chang et al. 2008; Zhang et al. 2008). Since there are many 1-to-n phrasal equivalences in Chinese-to-English translation (Ma and Way. 2009), only focusing on Chinese word as basic translational unit is not adequate to model 1-to-n translations. Ma and Way (2009) tackle this problem by using word aligner to bootstrap bilingual segmentation suitable for machine translation. Lambert and Banchs (2005) detect bilingual multi-word ex148 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 148–156, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for C"
P10-1016,E09-1063,0,0.622793,"egment Chinese sentence. Xu et al. (2008) used a Bayesian semi-supervised method that combines Chinese word segmentation model and Chinese-to-English translation model to derive a Chinese segmentation suitable for machine translation. There are also researches focusing on the impact of various segmentation tools on machine translation (Ma et al. 2007; Chang et al. 2008; Zhang et al. 2008). Since there are many 1-to-n phrasal equivalences in Chinese-to-English translation (Ma and Way. 2009), only focusing on Chinese word as basic translational unit is not adequate to model 1-to-n translations. Ma and Way (2009) tackle this problem by using word aligner to bootstrap bilingual segmentation suitable for machine translation. Lambert and Banchs (2005) detect bilingual multi-word ex148 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 148–156, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics pressions by monotonically segmenting a given Spanish-English sentence pair into bilingual units, where word aligner is also used. IBM model 3, 4, 5 (Brown et al., 1993) and Deng and Byrne (2005) are another kind of related works that allow"
P10-1016,2002.tmi-tutorials.2,0,0.0319943,"anner that word is too finegrained in some cases such as non-compositional phrasal equivalences, where clear word alignments do not exist. For example in Chinese-toEnglish translation, “ 想 ” and “would like to” constitute a 1-to-n phrasal equivalence, “多 少 钱” and “how much is it” constitute a m-to-n phrasal equivalence. No clear word alignments are there in such phrasal equivalences. Moreover, should basic translational unit be word or coarsegrained multi-word is an open problem for optimizing SMT models. Some researchers have explored coarsegrained translational unit for machine translation. Marcu and Wong (2002) attempted to directly learn phrasal alignments instead of word alignments. But computational complexity is prohibitively high for the exponentially large number of decompositions of a sentence pair into phrase pairs. Cherry and Lin (2007) and Zhang et al. (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. Blunsom et al. (2008; 2009) induced phrasal synchronous grammar, which aimed at finding hierarchical phrasal equivalences. Another direction of questioning word as basic translational u"
P10-1016,P03-1021,0,0.0199466,"e is computed only on pairs of blank boxes, solid boxes are excluded in this computation to represent NULL alignment cases. is ks1 ks2 js it kt1 kt2 jt a) non-reversed is ks1 ks2 js it kt1 kt2 jt Experiments and Results In our experiments, pseudo-words are fed into PB-SMT pipeline. The pipeline uses GIZA++ model 4 (Brown et al., 1993; Och and Ney, 2003) for pseudo-word alignment, uses Moses (Koehn et al., 2007) as phrase-based decoder, uses the SRI Language Modeling Toolkit to train language model with modified Kneser-Ney smoothing (Kneser and Ney 1995; Chen and Goodman 1998). Note that MERT (Och, 2003) is still on original words of target language. In our experiments, pseudo-word length is limited to no more than six unary words on both sides of the language pair. We conduct experiments on Chinese-toEnglish machine translation. Two data sets are adopted, one is small corpus of IWSLT-2008 BTEC task of spoken language translation in travel domain (Paul, 2008), the other is large corpus in news domain, which consists Hong Kong News (LDC2004T08), Sinorama Magazine (LDC2005T10), FBIS (LDC2003E14), Xinhua (LDC2002E18), Chinese News Translation (LDC2005T06), Chinese Treebank (LDC2003E07), Multiple"
P10-1016,J03-1002,0,0.016175,"ottom-up way, and the optimal decomposition of the sentence pair is obtained correspondingly. z Algorithm of Excluded Synchronous Searching for Pseudo-words (ESSP) The algorithm of SSP in Figure 2 explores all span-pairs, but it neglects NULL alignments, where words and “empty” word are aligned. In fact, SSP requires that all parts of a sentence pair should be aligned. This requirement is too strong because NULL alignments are very common in many language pairs. In SSP, words that should be aligned to “empty” word are programmed to be aligned to real words. Unlike most word alignment methods (Och and Ney, 2003) that add “empty” word to account for NULL alignment entries, we propose a method to naturally exclude such NULL alignments. We call this method as Excluded Synchronous Searching for Pseudo-words (ESSP). The main difference between ESSP and SSP is in steps 3-6 in Figure 3. We illustrate Figure 3’s span-pair configuration in Figure 4. 151 Initialization: if is = js or it = jt then W i s , j s , i t , j t = Sig i s , j s ,it , jt ; else W i s , j s , it , jt = 0 ; 1: for ds = 2 … ns, dt = 2 … nt do 2: for all is, js, it, jt s.t. js-is=ds-1 and jt-it=dt-1 do 3: for ks1=is+1 … js, ks2=ks1-1 … js-1"
P10-1016,C08-1128,0,\N,Missing
P10-1016,P02-1040,0,\N,Missing
P10-1016,W05-0909,0,\N,Missing
P10-1016,W07-0734,0,\N,Missing
P10-1016,P08-1012,0,\N,Missing
P10-1016,N03-1017,0,\N,Missing
P10-1016,W08-0335,0,\N,Missing
P10-1016,J07-2003,0,\N,Missing
P10-1016,2005.mtsummit-posters.11,0,\N,Missing
P10-1016,W04-1118,0,\N,Missing
P10-1016,2008.iwslt-evaluation.1,0,\N,Missing
P10-1032,D08-1092,0,0.0612318,"Missing"
P10-1032,P06-1104,1,0.833494,"ndering the syntactic rich task of sub-tree alignment less convincing and attractive. This may be due to the fact that the syntactic structures in a parse tree pair are hard to describe using plain features. In addition, explicitly utilizing syntactic tree fragments results in exponentially high dimensional feature vectors, which is hard to compute. Alternatively, convolution parse tree kernels (Collins and Duffy, 2001), which implicitly explore the tree structure information, have been successfully applied in many NLP tasks, such as Semantic parsing (Moschitti, 2004) and Relation Extraction (Zhang et al. 2006). However, all those studies are carried out in monolingual tasks. In multilingual tasks such as machine translation, tree kernels are seldom applied. In this paper, we propose Bilingual Tree Kernels (BTKs) to model the bilingual translational equivalences, in our case, to conduct sub-tree alignment. This is motivated by the decent effectiveness of tree kernels in expressing the similarity between tree structures. We propose two kinds of BTKs named dependent Bilingual Tree Kernel (dBTK), which takes the sub-tree pair as a whole and independent Bilingual Tree Kernel (iBTK), which individually m"
P10-1032,2007.mtsummit-papers.71,1,0.677302,"the development and test set. The evaluation metric is case-sensitive BLEU-4. 313 System Model BLEU Moses BP* DirC EWoS STSG DirC EWoS STSSG DirC EWoS 23.86 23.98 24.48 24.71 25.16 25.38 25.92 25.95 26.45 Syntax STSG Syntax STSSG Table 5. MT evaluation on various systems *BP denotes bilingual phrases For the phrase based system, we use Moses (Koehn et al., 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree3 as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement the STSG/STSSG based model in the Pisces decoder with the identical features and settings in Sun et al. (2009). In the Pisces decoder, the STSSG based decoder translates each span iteratively in a bottom up manner which guarantees that when translatin"
P10-1032,C04-1154,0,0.297672,"sub-tree alignment, translational equivalent sub-tree pairs are coupled as aligned counterparts. Each pair consists of both the lexical constituents and their maximum tree structures generated over the lexical sequences in the original parse trees. Due to the 1-to-1 mapping between sub-trees and tree nodes, sub-tree alignment can also be considered as node alignment by conducting multiple links across the internal nodes as shown in Fig. 1. Previous studies conduct sub-tree alignments by either using a rule based method or conducting some similarity measurement only based on lexical features. Groves et al. (2004) conduct sub-tree alignment by using some heuristic rules, lack of extensibility and generality. Tinsley et al. (2007) 306 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 306–315, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics and Imamura (2001) propose some score functions based on the lexical similarity and co-occurrence. These works fail to utilize the structural features, rendering the syntactic rich task of sub-tree alignment less convincing and attractive. This may be due to the fact that the syntactic str"
P10-1032,P08-1064,1,0.836887,"use Moses (Koehn et al., 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree3 as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement the STSG/STSSG based model in the Pisces decoder with the identical features and settings in Sun et al. (2009). In the Pisces decoder, the STSSG based decoder translates each span iteratively in a bottom up manner which guarantees that when translating a source span, any of its subspans is already translated. The STSG based decoding can be easily performed with the STSSG decoder by restricting the translation rule set to be elementary tree pairs only. As for the alignment setting, we use the word alignment trained on the entire FBIS (240k) corpus by GIZA++ with heuristic grow-di"
P10-1032,P03-1054,0,0.0682289,"corpus with parsing errors. In addition, HIT corpus is not applicable for MT experiment due to the problems of domain divergence, annotation discrepancy (Chinese parse tree employs a different grammar from Penn Treebank annotations) and degree of tolerance for parsing errors. Due to the above issues, we annotate a new data set to apply the sub-tree alignment in machine translation. We randomly select 300 bilingual sentence pairs from the Chinese-English FBIS corpus with the length 30 in both the source and target sides. The selected plain sentence pairs are further parsed by Stanford parser (Klein and Manning, 2003) on both the English and Chinese sides. We manually annotate the sub-tree alignment for the automatically parsed tree pairs according to the definition in Section 1. To be fully consistent with the definition, we strictly reserve the semantic equivalence for the aligned sub-trees to keep a high precision. In other words, we do not conduct any doubtful links. The corpus is further divided into 200 aligned tree pairs for training and 100 for testing as shown in Table 2. # of Sentence pair Avg. Sentence Length Avg. # of sub-tree Avg. # of alignment Chinese English 300 16.94 20.81 28.97 34.39 17.0"
P10-1032,N03-1017,0,0.0113571,"stic grow-diag-final for both Moses and the syntax system. For sub-treealignment, we use the above word alignment to learn lexical/word alignment feature, and train with the FBIS training corpus (200) using the composite kernel of Plain+dBTK-Root+iBTKRdSTT. 7.2 Experimental results Compared with the adoption of word alignment, translational equivalences generated from structural alignment tend to be more grammatically aware and syntactically meaningful. However, utilizing syntactic translational equivalences alone for machine translation loses the capability of modeling non-syntactic phrases (Koehn et al., 2003). Consequently, instead of using phrases constraint by sub-tree alignment alone, we attempt to combine word alignment and sub-tree alignment and deploy the capability of both with two methods. • Directly Concatenate (DirC) is operated by directly concatenating the rule set genereted from sub-tree alignment and the original rule set generated from word alignment (Tinsley et al., 2009). As shown in Table 5, we gain minor improvement in the Bleu score for all configurations. • Alternatively, we proposed a new approach to generate the rule set from the scratch. We constrain the bilingual phrases t"
P10-1032,P07-2045,0,0.00705698,"002 test set as the development set (to speed up tuning for syntax based system) and the NIST MT-2005 test set as our test set. We use the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is case-sensitive BLEU-4. 313 System Model BLEU Moses BP* DirC EWoS STSG DirC EWoS STSSG DirC EWoS 23.86 23.98 24.48 24.71 25.16 25.38 25.92 25.95 26.45 Syntax STSG Syntax STSSG Table 5. MT evaluation on various systems *BP denotes bilingual phrases For the phrase based system, we use Moses (Koehn et al., 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree3 as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implem"
P10-1032,J03-1002,0,0.00680383,"kipped over, the hypothesis is chosen as a sure link. Heuristic span1 postpones the selection of the hypotheses on the POS level. Since the highest-scoring hypotheses tend to appear on the leaf nodes, it may introduce ambiguity when conducting the alignment for a POS node whose child word appears twice in a sentence. The baseline method proposes two score functions based on the lexical translation probability. They also compute the score function by splitting the tree into the internal and external components. Tinsley et al. (2007) adopt the lexical translation probabilities dumped by GIZA++ (Och and Ney, 2003) to compute the span based scores for each pair of sub-trees. Although all of their heuristics combinations are re-implemented in our study, we only present the best result among them with the highest Recall and F-value as our baseline, denoted as skip2_s1_span12. 2 s1 denotes score function 1 in Tinsley et al. (2007), skip2_s1_span1 denotes the utilization of heuristics skip2 and span1 while using score function 1 Feature Space Lex Lex +Online Str Plain +dBTK-STT Plain +dBTK-RdSTT Plain +dBTK-RgSTT Plain +dBTK-Root Plain +iBTK-STT Plain +iBTK-RdSTT Plain +iBTK-RgSTT Plain +iBTK-Root Plain +dB"
P10-1032,P04-1043,0,0.202504,"ail to utilize the structural features, rendering the syntactic rich task of sub-tree alignment less convincing and attractive. This may be due to the fact that the syntactic structures in a parse tree pair are hard to describe using plain features. In addition, explicitly utilizing syntactic tree fragments results in exponentially high dimensional feature vectors, which is hard to compute. Alternatively, convolution parse tree kernels (Collins and Duffy, 2001), which implicitly explore the tree structure information, have been successfully applied in many NLP tasks, such as Semantic parsing (Moschitti, 2004) and Relation Extraction (Zhang et al. 2006). However, all those studies are carried out in monolingual tasks. In multilingual tasks such as machine translation, tree kernels are seldom applied. In this paper, we propose Bilingual Tree Kernels (BTKs) to model the bilingual translational equivalences, in our case, to conduct sub-tree alignment. This is motivated by the decent effectiveness of tree kernels in expressing the similarity between tree structures. We propose two kinds of BTKs named dependent Bilingual Tree Kernel (dBTK), which takes the sub-tree pair as a whole and independent Biling"
P10-1032,P09-1103,1,0.845996,"nefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree3 as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement the STSG/STSSG based model in the Pisces decoder with the identical features and settings in Sun et al. (2009). In the Pisces decoder, the STSSG based decoder translates each span iteratively in a bottom up manner which guarantees that when translating a source span, any of its subspans is already translated. The STSG based decoding can be easily performed with the STSSG decoder by restricting the translation rule set to be elementary tree pairs only. As for the alignment setting, we use the word alignment trained on the entire FBIS (240k) corpus by GIZA++ with heuristic grow-diag-final for both Moses and the syntax system. For sub-treealignment, we use the above word alignment to learn lexical/word a"
P10-1032,2007.mtsummit-papers.62,0,0.47504,"two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment. 1 Introduction Syntax based Statistical Machine Translation (SMT) systems allow the translation process to be more grammatically performed, which provides decent reordering capability. However, most of the syntax based systems construct the syntactic translation rules based on word alignment, which not only suffers from the pipeline errors, but also fails to effectively utilize the syntactic structural features. To address those deficiencies, Tinsley et al. (2007) attempt to directly capture the syntactic translational equivalences by automatically conducting sub-tree alignment, which can be defined as follows: A sub-tree alignment process pairs up sub-tree pairs across bilingual parse trees whose contexts are semantically translational equivalent. According to Tinsley et al. (2007), a sub-tree aligned parse tree pair follows the following criteria: (i) a node can only be linked once; Figure 1: Sub-tree alignment as referred to Node alignment (ii) descendants of a source linked node may only link to descendants of its target linked counterpart; (iii) a"
P10-1062,P03-1021,0,0.003576,"Missing"
P10-1062,P02-1040,0,0.101194,"Missing"
P10-1062,2009.eamt-1.15,0,0.0390746,"Missing"
P10-1062,2007.mtsummit-papers.54,0,0.830199,"tic features, according to their final results. Ueffing and Ney (2007) exhaustively explore various word-level confidence measures to label each word in a generated translation hypothesis as correct or incorrect. All their measures are based on word posterior probabilities, which are estimated from 1) system output, such as word lattices or N -best lists and 2) word or phrase translation table. Their experimental results show that word posterior probabilities directly estimated from phrase translation table are better than those from system output except for the Chinese-English language pair. Sanchis et al. (2007) adopt a smoothed naive Bayes model to combine different word posterior probability based confidence features which are estimated from N -best lists, similar to (Ueffing and Ney, 2007). Raybaud et al. (2009) study several confidence features based on mutual information between words and n-gram and backward n-gram language model for word-level and sentence-level CE. They also explore linguistic features using information from syntactic category, tense, gender and so on. Unfortunately, such linguistic features neither improve performance at the word level nor at the sentence level. Our work depa"
P10-1062,1993.iwpt-1.22,0,0.00848038,"o be incorrect. The challenge of using syntactic knowledge for error detection is that machinegenerated hypotheses are rarely fully grammatical. They are mixed with grammatical and ungrammatical parts, which hence are not friendly to traditional parsers trained on grammatical sentences because ungrammatical parts of a machinegenerated sentence could lead to a parsing failure. To overcome this challenge, we select the Link Grammar (LG) parser 3 as our syntactic parser to generate syntactic features. The LG parser produces a set of labeled links which connect pairs of words with a link grammar (Sleator and Temperley, 1993). The main reason why we choose the LG parser is that it provides a robustness feature: null-link scheme. The null-link scheme allows the parser to parse a sentence even when the parser can not fully interpret the entire sentence (e.g. including ungrammatical parts). When the parser fail to parse the entire sentence, it ignores one word each time until it finds linkages for remaining words. After parsing, those ignored words are not connected to any other words. We call them null-linked words. Our hypothesis is that null-linked words are prone to be syntactically incorrect. We hence straightfo"
P10-1062,C04-1047,0,0.0535615,"Missing"
P10-1062,H05-1006,0,0.0326993,"development set. Sometimes the step 2) is not necessary if only one effective feature is used (Ueffing and Ney, 2007); and sometimes the step 2) and 3) can be merged into a single step if we directly output predicting results from binary classifiers instead of making thresholding decision. Various features from different SMT models and system outputs are investigated (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007; Raybaud et al., 2009). Experimental results show that they are useful for error detection. However, it is not adequate to just use these features as discussed in (Shi and Zhou, 2005) because the information that they carry is either from the inner components of SMT systems or from system outputs. To some extent, it has already been considered by SMT systems. Hence finding external information Introduction Translation hypotheses generated by a statistical machine translation (SMT) system always contain both correct parts (e.g. words, n-grams, phrases matched with reference translations) and incorrect parts. Automatically distinguishing incorrect parts from correct parts is therefore very desirable not only for post-editing and interactive machine translation (Ueffing and N"
P10-1062,J96-1002,0,0.0522358,"Missing"
P10-1062,2003.mtsummit-papers.52,0,0.344912,"likely to be incorrect than words in frequently occurring patterns. To some extent, these two features have similar function to a target language model or pos-based target language model. yes, w has links no, otherwise In Figure 1 we show an example of a generated translation hypothesis with its link parse. Here links are denoted with dotted lines which are annotated with link types (e.g., Jp, Op). Bracketed words, namely “,” and “including”, are null-linked words. 3.3 Word Posterior Probability Features Our word posterior probability is calculated on N best list, which is first proposed by (Ueffing et al., 2003) and widely used in (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007). Given a source sentence f , let {en }N 1 be the N best list generated by an SMT system, and let ein is the i-th word in en . The major work of calculating word posterior probabilities is to find the Levenshtein alignment (Levenshtein, 1966) between the best hypothesis e1 and its competing hypothesis 3.2 Syntactic Features High-level linguistic knowledge such as syntactic information about a word is a very natural and promising indicator to decide whether this word is syntactically correct or not. Words occur"
P10-1062,W06-3110,0,0.0456356,"sidered by SMT systems. Hence finding external information Introduction Translation hypotheses generated by a statistical machine translation (SMT) system always contain both correct parts (e.g. words, n-grams, phrases matched with reference translations) and incorrect parts. Automatically distinguishing incorrect parts from correct parts is therefore very desirable not only for post-editing and interactive machine translation (Ueffing and Ney, 2007) but also for SMT itself: either by rescoring hypotheses in the N -best list using the probability of correctness calculated for each hypothesis (Zens and Ney, 2006) or by generating new hypotheses using N best lists from one SMT system or multiple sys604 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 604–611, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics confidence estimation at the word level as well as at the sentence level. The features they use for word level CE include word posterior probabilities estimated from N -best lists, features based on SMT models, semantic features extracted from WordNet as well as simple syntactic features, i.e. parentheses and quotation m"
P10-1062,W03-0413,0,0.0622143,"Missing"
P10-1062,P05-3026,0,0.0200006,"Missing"
P10-1062,N03-1017,0,0.00275441,"variable c to indicate whether this word is correct or not. In the feature vector, we look at 2 words before and 2 words after the current word position (w−2 , w−1 , w, w1 , w2 ). We collect features {wd, pos, link, dwpp} for each word among these words and combine them into the feature vector ψ for w. As such, we want the feature vector to capture the contextual environment, e.g., pos sequence pattern, syntactic pattern, where the word w occurs. To obtain machine-generated translation hypotheses for our error detection, we use a state-of-the-art phrase-based machine translation system MOSES (Koehn et al., 2003; Koehn et al., 2007). The translation task is on the official NIST Chineseto-English evaluation data. The training data consists of more than 4 million pairs of sentences (including 101.93M Chinese words and 112.78M English words) from LDC distributed corpora. Table 2 shows the corpora that we use for the translation task. We build a four-gram language model using the SRILM toolkit (Stolcke, 2002), which is trained 607 Feature wd pos link dwpp Example { 1, f (c, ψ) = { 0, 1, f (c, ψ) = { 0, 1, f (c, ψ) = { 0, 1, f (c, ψ) = 0, ψ.w.wd = ”.”, c = correct otherwise ψ.w2 .pos = ”N N ”, c = incorre"
P10-1062,J07-1003,0,\N,Missing
P10-1062,P07-2045,0,\N,Missing
P10-1062,C04-1046,0,\N,Missing
P10-1062,2005.eamt-1.20,0,\N,Missing
P10-1090,W06-1673,0,0.0167499,"(10) reviewer of the paper, we can consider the forest ?? ???? ?1 ∙ ?? ???? ?2 kernel as an alternative solution proposed for the general problem of noisy inference pipelines (eg. Finally, since the size of input forests is not speech translation by composition of FSTs, maconstant, the forest kernel value is normalized chine translation by translating over 'lattices' of using the following equation. segmentations (Dyer et al., 2008) or using parse ?? ?1 , ?2 tree info for downstream applications in our cas?? ?1 , ?2 = (11) es) . Following this line, Bunescu (2008) and ?? ?1 , ?1 ∙ ?? ?2 , ?2 Finkel et al. (2006) are two typical related works done in reducing cascading noisy. However, our From the above discussion, we can see that the works are not overlapped with each other as proposed forest kernel is defined together by eqs. there are two totally different solutions for the (11), (10), (9) and (8). Thanks to the compact same general problem. In addition, the main morepresentation of trees in forest and the recursive tivation of this paper is also different from theirs. nature of the kernel function, the introduction of fractional counts and normalization do not 4 Experiments change the convolution"
P10-1090,P06-1104,1,0.917321,"product in a high-dimensional space over the original representations of objects, has made kernel methods an effective solution to modeling structured objects in NLP. In the context of parse tree, convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. The tree kernel has shown much success in many NLP applications like parsing (Collins and Duffy, 2002), semantic role labeling (Moschitti, 2004; Zhang et al., 2007), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006), question classification (Zhang and Lee, 2003) and machine translation (Zhang and Li, 2009), where the tree kernel is used to compute the similarity between two NLP application instances that are usually represented by parse trees. However, in those studies, the tree kernel only covers the features derived from single 1875 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 875–885, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics PP IN DT NN in the bank PP IN DT NN in the bank"
P10-1090,P08-1064,1,0.833375,"illustrate the concept of packed forest and then give a detailed discussion on the covered feature space, fractional count, feature value and the forest kernel function itself. 3.1 Packed forest of parse trees Informally, a packed parse forest, or (packed) forest in short, is a compact representation of all the derivations (i.e. parse trees) for a given sentence under context-free grammar (Tomita, 1987; Billot and Lang, 1989; Klein and Manning, 2001). It is the core data structure used in natural language parsing and other downstream NLP applications, such as syntax-based machine translation (Zhang et al., 2008; Zhang et al., 2009a). In parsing, a sentence corresponds to exponential number of parse trees with different tree probabilities, where a forest can compact all the parse trees by sharing their common subtrees in a bottom-up manner. Formally, a packed forest ? can be described as a triple: ? = &lt; ?, ?, ? &gt; where ?is the set of non-terminal nodes, ? is the set of hyper-edges and ? is a sentence 877 represented as an ordered word sequence. A hyper-edge ? is a group of edges in a parse tree which connects a father node and its all child nodes, representing a CFG rule. A non-terminal node in a for"
P10-1090,D08-1022,0,\N,Missing
P10-1090,N06-2025,0,\N,Missing
P10-1090,C02-1132,0,\N,Missing
P10-1090,D08-1070,0,\N,Missing
P10-1090,W04-3212,0,\N,Missing
P10-1090,D09-1108,1,\N,Missing
P10-1090,J03-4003,0,\N,Missing
P10-1090,P09-1020,1,\N,Missing
P10-1090,J87-1004,0,\N,Missing
P10-1090,D09-1073,1,\N,Missing
P10-1090,P04-1043,0,\N,Missing
P10-1090,P06-1006,0,\N,Missing
P10-1090,P05-1022,0,\N,Missing
P10-1090,W05-0620,0,\N,Missing
P10-1090,P08-1067,0,\N,Missing
P10-1090,P01-1017,0,\N,Missing
P10-1090,P08-1115,0,\N,Missing
P10-1090,J05-1004,0,\N,Missing
P10-1090,P06-2010,1,\N,Missing
P10-1090,P07-1026,1,\N,Missing
P10-1090,P89-1018,0,\N,Missing
P10-1090,W01-1812,0,\N,Missing
P11-1129,D07-1090,0,0.234408,"the efforts that advance translation models from word-based paradigm to syntax-based philosophy, in recent years we have also witnessed increasing efforts dedicated to extend standard n-gram language models for SMT. We roughly categorize these efforts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed manner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation. The second direction digs deeply into monolingual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine translation which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing long-distance dependenci"
P11-1129,J93-2003,0,0.016013,"information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline. 1 Introduction Language model is one of the most important knowledge sources for statistical machine translation (SMT) (Brown et al., 1993). The standard n-gram language model (Goodman, 2001) assigns probabilities to hypotheses in the target language conditioning on a context history of the preceding n − 1 words. Along with the efforts that advance translation models from word-based paradigm to syntax-based philosophy, in recent years we have also witnessed increasing efforts dedicated to extend standard n-gram language models for SMT. We roughly categorize these efforts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC"
P11-1129,2003.mtsummit-papers.6,0,0.593667,"rts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed manner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation. The second direction digs deeply into monolingual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine translation which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing long-distance dependencies. In this paper, we pursue the second direction without resorting to any linguistic resources such as a syntactic parser. With a belief that a language model that embraces a larger context provides better prediction ability, we learn additional inf"
P11-1129,J07-2003,0,0.832723,"language model assigns a probability Pb (w1m ) to w1m by looking at the succeeding context according to Pb (w1m ) = m ∏ i=1 m )≈ P (wi |wi+1 m ∏ i+n−1 P (wi |wi+1 ) (2) i=1 3.1 Training et al., 2006) and 2) a standard phrase-based decoder (Koehn et al., 2003). Both decoders translate source sentences from the beginning of a sentence to the ending. Wu (1996) introduce a dynamic programming algorithm to integrate a forward bigram language model with inversion transduction grammar. His algorithm is then adapted and extended for integrating forward n-gram language models into synchronous CFGs by Chiang (2007). Our algorithms are different from theirs in two major aspects 1. The string input to the algorithms is in a reverse order. 2. We adopt a different way to calculate language model probabilities for partial hypotheses so that we can utilize incomplete n-grams. Before we introduce the integration algorithms, we define three functions P, L, and R on strings (in a reverse order) over the English terminal alphabet T . The function P is defined as follows. P(wk ...w1 ) = P (wk )...P (wk−n+2 |wk ...wk−n+3 ) {z } | a ∏ P (wi |wi+n−1 ...wi+1 ) × 1≤i≤k−n+1 | For the convenience of training, we invert t"
P11-1129,J90-1003,0,0.0548025,"n that long-distance dependencies between words are very important for statistical language modeling. However, n-gram language models can only capture short-distance dependencies within an n-word window. In order to model long-distance dependencies, previous work such as (Rosenfeld et al., 1994) and (Zhou, 2004) exploit trigger pairs. A trigger pair is defined as an ordered 2-tuple (x, y) where word x occurs in the preceding context of word y. It can also be denoted in a more visual manner as x → y with x being the trigger and y the triggered word5 . We use pointwise mutual information (PMI) (Church and Hanks, 1990) to measure the strength of the association between x and y, which is defined as follows P M I(x, y) = log( P (x, y) ) P (x)P (y) (12) 5 In this paper, we require that word x and y occur in the same sentence. 1292 i−1 P (wi |wi−n+1 )) m i−n ∏ ∏ exp(P M I(wk , wi , i − k − 1)) i=n+1 k=1 (13) There are two components in his model. The first component is still the standard n-gram language model. The second one is the MI trigger model which multiples all exponential PMI values for trigger pairs where the current word is the triggered word and all preceding words outside the n-gram window of the cu"
P11-1129,D09-1117,0,0.277157,"build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a scenario where sentences are proceeded from the ending to the beginning. Duchateau et al. (2002) use the BLM score as a confidence measure to detect wrongly recognized words in speech recognition. Finch and Sumita (2009) use the BLM in their reverse translation decoder where source sentences are proceeded from the ending to the beginning. Our BLM is different from theirs in that we access the BLM during decoding (rather than after decoding) where source sentences are still proceeded from the beginning to the ending. Rosenfeld et al. (1994) introduce trigger pairs into a maximum entropy based language model as features. The trigger pairs are selected according to their mutual information. Zhou (2004) also propose an enhanced language model (MI-Ngram) which consists of a standard forward n-gram language model a"
P11-1129,N03-1017,0,0.482736,"= m ∏ P (wi |w1i−1 ) ≈ m ∏ i−1 ) (1) P (wi |wi−n+1 i=1 i=1 where the approximation is based on the nth order Markov assumption. In other words, when we predict the current word wi , we only consider the preceding n − 1 words wi−n+1 ...wi−1 instead of the whole context history w1 ...wi−1 . Different from the forward n-gram language model, the backward n-gram language model assigns a probability Pb (w1m ) to w1m by looking at the succeeding context according to Pb (w1m ) = m ∏ i=1 m )≈ P (wi |wi+1 m ∏ i+n−1 P (wi |wi+1 ) (2) i=1 3.1 Training et al., 2006) and 2) a standard phrase-based decoder (Koehn et al., 2003). Both decoders translate source sentences from the beginning of a sentence to the ending. Wu (1996) introduce a dynamic programming algorithm to integrate a forward bigram language model with inversion transduction grammar. His algorithm is then adapted and extended for integrating forward n-gram language models into synchronous CFGs by Chiang (2007). Our algorithms are different from theirs in two major aspects 1. The string input to the algorithms is in a reverse order. 2. We adopt a different way to calculate language model probabilities for partial hypotheses so that we can utilize incomp"
P11-1129,W04-3250,0,0.221159,"Missing"
P11-1129,D09-1022,0,0.0230483,"guage model and an MI trigger model. The latter model measures the mutual information of distancedependent trigger pairs. Our MI trigger model is mostly inspired by the work of these two papers, especially by Zhou’s MI-Ngram model (2004). The difference is that our model is distance-independent and, of course, we are interested in an SMT problem rather than a speech recognition one. Raybaud et al. (2009) use MI triggers in their confidence measures to assess the quality of translation results after decoding. Our method is different from theirs in the MI calculation and trigger pair selection. Mauser et al. (2009) propose bilingual triggers where two source words trigger one target word to 1 Language model adaptation is not very related to our work so we ignore it. improve lexical choice of target words. Our analysis (Section 6) show that our monolingual triggers can also help in the selection of target words. 3 Backward Language Model Given a sequence of words w1m = (w1 ...wm ), a standard forward n-gram language model assigns a probability Pf (w1m ) to w1m as follows. Pf (w1m ) = m ∏ P (wi |w1i−1 ) ≈ m ∏ i−1 ) (1) P (wi |wi−n+1 i=1 i=1 where the approximation is based on the nth order Markov assumpti"
P11-1129,P03-1021,0,0.0184105,"istance-dependent since trigger pairs (wk , wi ) are sensitive to their distance i − k − 1 (zero distance for adjacent words). Therefore the distance between word x and word y should be taken into account when calculating their PMI. In this paper, for simplicity, we adopt a distanceindependent MI trigger model as follows M I(w1m ) = m i−n ∏ ∏ exp(P M I(wk , wi )) (14) i=n+1 k=1 We integrate the MI trigger model into the loglinear model of machine translation as an additional knowledge source which complements the standard n-gram language model in capturing long-distance dependencies. By MERT (Och, 2003), we are even able to tune the weight of the MI trigger model against the weight of the standard n-gram language model while Zhou (2004) sets equal weights for both models. 4.1 Training We can use the maximum likelihood estimation method to calculate PMI for each trigger pair by taking counts from training data. Let C(x, y) be the co-occurrence count of the trigger pair (x, y) in the training data. The joint probability of (x, y) is calculated as C(x, y) x,y C(x, y) P (x, y) = ∑ (15) phrase-based decoder. But we still can handle it by dynamic programming as follows M I(e1 e2 ) = M I(e1 )M I(e2"
P11-1129,P02-1040,0,0.0811835,"English Gigaword corpus (306 million words). Firstly, we built a forward 5-gram language model using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. Then we trained a backward 5-gram language model on the same monolingual corpus in the way described in Section 3.1. Finally, we trained our MI trigger model still on this corpus according to the method in Section 4.1. The trained MI trigger model consists of 2.88M trigger pairs. We used the NIST MT03 evaluation test data as the development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU4 (Papineni et al., 2002) as the evaluation metric, which uses the shortest reference sentence length for the brevity penalty. Statistical significance in BLEU differences is tested by paired bootstrap re-sampling (Koehn, 2004). 5.3 Experimental Results The experimental results on the two NIST test sets are shown in Table 2. When we combine the backward language model with the forward language 7 LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07 and LDC2004T07. 1294 Model Forward (Baseline) Forward+Backward Forward+MI Forward+Backward+MI MT-04 35.67 36.16+ 36.00+ 36.76+ MT-05 34.41 34.9"
P11-1129,2008.amta-papers.16,0,0.0143574,"in more detail, describe the training procedures and explain how the models are integrated into the phrase-based decoder. Section 5 will empirically evaluate the effectiveness of these two models. Section 6 will conduct an indepth analysis. In the end, we conclude in Section 7. 2 Related Work Previous work devoted to improving language models in SMT mostly focus on two categories as we 1289 mentioned before1 : large language models (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007; Talbot and Osborne, 2007) and syntax-based language models (Charniak et al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philosophy is fundamentally different from them in that we build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a scenario where sentences are proceeded from the ending to the beginning. Duchateau et al. (2002) use the BLM score as a confide"
P11-1129,P08-1066,0,0.2483,"from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed manner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation. The second direction digs deeply into monolingual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine translation which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing long-distance dependencies. In this paper, we pursue the second direction without resorting to any linguistic resources such as a syntactic parser. With a belief that a language model that embraces a larger context provides better prediction ability, we learn additional information from training data to enhance conventional n-gram language models and extend their ability to capture richer contexts and"
P11-1129,P07-1065,0,0.060148,"ork. Section 3 and 4 will elaborate the backward language model and the MI trigger model respectively in more detail, describe the training procedures and explain how the models are integrated into the phrase-based decoder. Section 5 will empirically evaluate the effectiveness of these two models. Section 6 will conduct an indepth analysis. In the end, we conclude in Section 7. 2 Related Work Previous work devoted to improving language models in SMT mostly focus on two categories as we 1289 mentioned before1 : large language models (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007; Talbot and Osborne, 2007) and syntax-based language models (Charniak et al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philosophy is fundamentally different from them in that we build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a scenario where sentences are"
P11-1129,P96-1021,0,0.127851,"arkov assumption. In other words, when we predict the current word wi , we only consider the preceding n − 1 words wi−n+1 ...wi−1 instead of the whole context history w1 ...wi−1 . Different from the forward n-gram language model, the backward n-gram language model assigns a probability Pb (w1m ) to w1m by looking at the succeeding context according to Pb (w1m ) = m ∏ i=1 m )≈ P (wi |wi+1 m ∏ i+n−1 P (wi |wi+1 ) (2) i=1 3.1 Training et al., 2006) and 2) a standard phrase-based decoder (Koehn et al., 2003). Both decoders translate source sentences from the beginning of a sentence to the ending. Wu (1996) introduce a dynamic programming algorithm to integrate a forward bigram language model with inversion transduction grammar. His algorithm is then adapted and extended for integrating forward n-gram language models into synchronous CFGs by Chiang (2007). Our algorithms are different from theirs in two major aspects 1. The string input to the algorithms is in a reverse order. 2. We adopt a different way to calculate language model probabilities for partial hypotheses so that we can utilize incomplete n-grams. Before we introduce the integration algorithms, we define three functions P, L, and R"
P11-1129,J97-3002,0,0.0282538,"age model without any other changes. To be consistent with training, we also need to reverse the order of translation hypotheses when we access the trained backward language model2 . Note that the Markov context history of Eq. (2) is wi+n−1 ...wi+1 instead of wi+1 ...wi+n−1 after we invert the order. The words are the same but the order is completely reversed. 3.2 Decoding In this section, we will present two algorithms to integrate the backward n-gram language model into two kinds of phrase-based decoders respectively: 1) a CKY-style decoder that adopts bracketing transduction grammar (BTG) (Wu, 1997; Xiong 2 This is different from the reverse decoding in (Finch and Sumita, 2009) where source sentences are reversed in the order. 1290 {z } b (3) This function consists of two parts: • The first part (a) calculates incomplete n-gram language model probabilities for word wk to wk−n+2 . That means, we calculate the unigram probability for wk (P (wk )), bigram probability for wk−1 (P (wk−1 |wk )) and so on until we take n − 1-gram probability for wk−n+2 (P (wk−n+2 |wk ...wk−n+3 )). This resembles the way in which the forward language model probability in the future cost is computed in the stand"
P11-1129,P06-1066,1,0.940398,"2 )P (b1 |b2 ) P (a3 )P (a2 |a3 ) P (a3 )P (a2 |a3 )P (a1 |a3 a2 ) P (b3 )P (b2 |b3 )P (b1 |b3 b2 ) P (b2 )P (b1 |b2 ) P (a3 |b2 b1 )P (a2 |b1 a3 ) P (b3 )P (b2 |b3 )P (b1 |b3 b2 ) P (a3 |b2 b1 )P (a2 |b1 a3 )P (a1 |a3 a2 ) Table 1: Values of P, L, and R in a 3-gram example . (4) P(e2 e1 ) = P(e1 )P(e2 ) (5) The L and R function return the leftmost and rightmost n − 1 words from a string in a reverse order respectively. Following Chiang (2007), we describe our algorithms in a deductive system. We firstly show the algorithm3 that integrates the backward language model into a BTG-style decoder (Xiong et al., 2006) in Figure 1. The item [A, i, j; l|r] indicates that a BTG node A has been constructed spanning from i to j on the source side with the leftmost|rightmost n − 1 words l|r on the target side. As mentioned before, all target strings assessed by the defined functions (P, L, and R) are in an inverted order (denoted by e). We only display the backward language model probability for each item, ignoring all other scores such as phrase translation probabilities. The Eq. (8) in Figure 1 shows how we calculate the backward language model probability for the axiom which applies a BTG lexicon rule to tran"
P11-1129,W06-1626,0,0.307465,"1 words. Along with the efforts that advance translation models from word-based paradigm to syntax-based philosophy, in recent years we have also witnessed increasing efforts dedicated to extend standard n-gram language models for SMT. We roughly categorize these efforts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed manner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation. The second direction digs deeply into monolingual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine translation which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing lo"
P11-1129,C04-1014,0,0.354464,"use the BLM score as a confidence measure to detect wrongly recognized words in speech recognition. Finch and Sumita (2009) use the BLM in their reverse translation decoder where source sentences are proceeded from the ending to the beginning. Our BLM is different from theirs in that we access the BLM during decoding (rather than after decoding) where source sentences are still proceeded from the beginning to the ending. Rosenfeld et al. (1994) introduce trigger pairs into a maximum entropy based language model as features. The trigger pairs are selected according to their mutual information. Zhou (2004) also propose an enhanced language model (MI-Ngram) which consists of a standard forward n-gram language model and an MI trigger model. The latter model measures the mutual information of distancedependent trigger pairs. Our MI trigger model is mostly inspired by the work of these two papers, especially by Zhou’s MI-Ngram model (2004). The difference is that our model is distance-independent and, of course, we are interested in an SMT problem rather than a speech recognition one. Raybaud et al. (2009) use MI triggers in their confidence measures to assess the quality of translation results aft"
P11-2094,D08-1023,0,0.0252377,"s zero probability. For efficiency reason, we choose the probabilistic SCFG as the proposal distribution. We pre-parse the training instances3 before inference and save the structure of synchronous parsing forests. During the inference, we only change rule probabilities in parsing forests without changing the forest structures. The probability of rule r ∈ RA in Q is estimated by relative frequency θr = P ′ [fr ]−i [f ′ ]−i , where RA is the set of rules r r ∈RA rooted at A, and [fr ]−i is the number of times that rule r is used in the tree set T−i . We use the sampling algorithm described in (Blunsom and Osborne, 2008) to draw a synchronous tree from the parsing forest according to the proposal Q. Following (Johnson and Goldwater, 2009), we put an uninformative Beta(1,1) prior on a and a “vague” Gamma(10, 0.1) prior on b to model the uncertainty of hyperparameters. 3 Machine Transliteration 3.1 Grammars For machine transliteration, we design the following grammar to learn syllable mappings4 : Name Syl Syl Syl NECs SECs TECs NEC SEC TEC → → → → → → → → → → hSyl / Syli+ hNECs / NECsi hNECs SECs / NECs SECsi hNECs TECs / NECs TECsi hNEC / NECi+ hSEC / SECi+ hTEC / TECi+ hsi / tj i hε / tj i hsi / εi 3.2 We imp"
P11-2094,P09-1088,0,0.030397,"f Computer Science National University of Singapore 13 Computing Drive, Singapore single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heuristics are introduced to obtain many-to-many alignments by combining two directional one-to-many alignments (Rama and Gali, 2009). Compared to maximum likelihood approaches, Bayesian models provide a systemic way to encode knowledges and infer compact structures. They have been successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and G"
P11-2094,J07-2003,0,0.139092,"Missing"
P11-2094,W09-3510,0,0.0230283,"a general framework without heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a Department of Computer Science National Universi"
P11-2094,2010.iwslt-papers.7,0,0.0321043,"hout heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a Department of Computer Science National University of Singapore 13 Comput"
P11-2094,D10-1028,0,0.0471331,"learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534 Proceedings of the 49th Annual Meeting of the"
P11-2094,N09-1036,0,0.152951,"et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 534–539, c"
P11-2094,P08-1046,0,0.356951,"dels, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 534–539, c Portland, Oregon"
P11-2094,P10-1117,0,0.012015,"n successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534"
P11-2094,P04-1021,1,0.799914,"ble equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a Department of Computer Science National University of Singapore 13 Computing Drive, Singapore single character in one langu"
P11-2094,W09-3501,1,0.873539,"r of flatten rules. One may verify that the rule probabilities are well normalized. Based on this merged grammar G′ , we parse the training string pairs, then encode the parsed forest into the lattice. Figure 2 show a lattice example for the string pair ha a l t o / C[a] [er] ÷[tuo]i. The transition probabilities in the lattice are the “inside” 537 4.1 Data and Settings We conduct experiments on the English-Chinese data in the ACL Named Entities Workshop (NEWS 2009) 6 . Table 1 gives some statistics of the data. For evaluation, we report the word accuracy and mean F-score metrics defined in (Li et al., 2009). # Entry # En Char # Ch Char # Ch Type Train 31,961 218,073 101,205 370 Dev 2,896 19,755 9,160 275 Test 2,896 19,864 9,246 283 Table 1: Transliteration data statistics In the inference step, we first run sampler through the whole training corpus for 10 iterations, then collect adapted subtree statistics for every 10 iterations, and finally stop after 20 collections. After each iteration, we resample each of hyperparameters from the posterior distribution of hyperparameters using a slice sampler (Neal, 2003). 4.2 Results We implement the joint source-channel model (Li et al., 2004) as the base"
P11-2094,D09-1136,0,0.0225751,"me approaches restrict that a Department of Computer Science National University of Singapore 13 Computing Drive, Singapore single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heuristics are introduced to obtain many-to-many alignments by combining two directional one-to-many alignments (Rama and Gali, 2009). Compared to maximum likelihood approaches, Bayesian models provide a systemic way to encode knowledges and infer compact structures. They have been successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology an"
P11-2094,W09-3528,0,0.0296661,". This model provides a general framework without heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a Department of Computer S"
P11-2094,W09-3520,0,0.0574031,"Missing"
P11-2094,J97-3002,0,0.10341,"to the proposal Q. Following (Johnson and Goldwater, 2009), we put an uninformative Beta(1,1) prior on a and a “vague” Gamma(10, 0.1) prior on b to model the uncertainty of hyperparameters. 3 Machine Transliteration 3.1 Grammars For machine transliteration, we design the following grammar to learn syllable mappings4 : Name Syl Syl Syl NECs SECs TECs NEC SEC TEC → → → → → → → → → → hSyl / Syli+ hNECs / NECsi hNECs SECs / NECs SECsi hNECs TECs / NECs TECsi hNEC / NECi+ hSEC / SECi+ hTEC / TECi+ hsi / tj i hε / tj i hsi / εi 3.2 We implement the CKY-like bottom up parsing algorithm described in (Wu, 1997). The complexity is O(|s|3 |t|3 ). 4 Similar to (Johnson, 2008), the adapted nonterminal are underlined. Similarly, we also use rules in the regular expression style X → hA / Ai+ to denote the following three rules: → → → Name → hWord / Wordi+ Word → hSyl / Syli+ We might further add a new adapted nonterminal Col to learn the word collocations. The following rules appear in the collocation grammar: Name → hCol / Coli+ Col → hWord / Wordi+ Word → hSyl / Syli+ Figure 1 gives one synchronous parsing trees under the collocation grammar of the example hm a x / ð[mai] [ke] d[si]i. 3 X As As where t"
P11-2094,W09-3515,0,0.0492781,"Missing"
P11-2094,P08-1012,0,0.0262763,"that a Department of Computer Science National University of Singapore 13 Computing Drive, Singapore single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heuristics are introduced to obtain many-to-many alignments by combining two directional one-to-many alignments (Rama and Gali, 2009). Compared to maximum likelihood approaches, Bayesian models provide a systemic way to encode knowledges and infer compact structures. They have been successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segm"
P11-2094,W10-2417,0,\N,Missing
P12-1023,W06-2920,0,0.149607,"Missing"
P12-1023,D07-1101,0,0.447259,"irstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data. McDonald et al. (2005) and Covington (2001) develop models that represent first-order features over a single arc in graphs. By extending the firstorder model, McDonald and Pereira (2006) and Carreras (2007) exploit second-order features over two adjacent arcs in second-order models. Koo and Collins (2010) further propose a third-order model that uses third-order features. These models utilize higher-order feature representations and achieve better performance than the first-order models. But this achievement is at the cost of the higher decoding complexity, from O(n2 ) to O(n4 ), where n is the length of the input sentence. Thus, it is very hard to develop higher-order models further in this way. 1 Introduction In recent years, there are many data-driven models that have been proposed for depend"
P12-1023,I08-1012,1,0.920814,"on words of WSJ text.3 We used the MXPOST tagger trained on training data to assign part-of-speech tags and used the Baseline parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008; Chen et al., 2009). For the unannotated data, we used the XIN CMN portion of Chinese Gigaword5 Version 2.0 (LDC2009T14) (Huang, 2009), 2 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 4 http://www.cis.upenn.edu/˜chinese/. 5 We excluded the sentences of the CTB data from the Gigaword data 3 which has approximately 311 million words whose segmentation and POS tags are given. We discarded the annotations due to the differences in annotation policy between CTB and this"
P12-1023,D09-1060,1,0.857031,"XPOST tagger trained on training data to assign part-of-speech tags and used the Baseline parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008; Chen et al., 2009). For the unannotated data, we used the XIN CMN portion of Chinese Gigaword5 Version 2.0 (LDC2009T14) (Huang, 2009), 2 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 4 http://www.cis.upenn.edu/˜chinese/. 5 We excluded the sentences of the CTB data from the Gigaword data 3 which has approximately 311 million words whose segmentation and POS tags are given. We discarded the annotations due to the differences in annotation policy between CTB and this corpus. We used the MMA system (Kruen"
P12-1023,C96-1058,0,0.393535,"n et al., 2008). We can use an original parser to produce the K-best list. This method has the potential to be very fast. However, because the performance of this method is restricted to the K-best list, we may have to set K to a high number in order to find the best parsing tree (with DLM) or a tree acceptably close to the best (Shen et al., 2008). 4.2 created from pairs of smaller ones in a bottom-up style. In the following figures, complete items are represented by triangles and incomplete items are represented by trapezoids. Figure 2 illustrates the cubic parsing actions of the algorithm (Eisner, 1996) in the right direction, where s, r, and t refer to the start and end indices of the chart items. In Figure 2-(a), all the items on the left side are complete and the algorithm creates the incomplete item (trapezoid on the right side) of s – t. This action builds a dependency relation from s to t. In Figure 2-(b), the item of s – r is incomplete and the item of r – t is complete. Then the algorithm creates the complete item of s – t. In this action, all the children of r are generated. In Figure 2, the longer vertical edge in a triangle or a trapezoid corresponds to the subroot of the structur"
P12-1023,P10-1001,0,0.261279,"large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data. McDonald et al. (2005) and Covington (2001) develop models that represent first-order features over a single arc in graphs. By extending the firstorder model, McDonald and Pereira (2006) and Carreras (2007) exploit second-order features over two adjacent arcs in second-order models. Koo and Collins (2010) further propose a third-order model that uses third-order features. These models utilize higher-order feature representations and achieve better performance than the first-order models. But this achievement is at the cost of the higher decoding complexity, from O(n2 ) to O(n4 ), where n is the length of the input sentence. Thus, it is very hard to develop higher-order models further in this way. 1 Introduction In recent years, there are many data-driven models that have been proposed for dependency parsing (McDonald and Nivre, 2007). Among them, graphbased dependency parsing models have achie"
P12-1023,P08-1068,0,0.18586,"mpared, where McDonald06 refers to the second-order parser of McDonald 219 System McDonald06 Koo08-standard Koo10-model1 Koo08-dep2c Suzuki09 Chen09-ord2s Zhou11 MSTB-DLM2 UAS 91.5 92.02 93.04 93.16 93.79 92.51 92.64 92.76 Cost O(n3 ) O(n4 ) O(n4 ) O(n4 ) O(n4 ) O(n3 ) O(n4 ) O(Kn3 ) Table 7: Relevant results for English. G denotes the supervised graph-based parsers, S denotes the graph-based parsers with semi-supervised methods, D denotes our new parsers 6.7 Table 6: Main results for Chinese 6.6 and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al. (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al., 2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009), and Zhou11 refers to the second-order parser with web-derived selectional preference features of Zhou et al. (2011). The results showed that our MSTB-DLM2 obtained the comparable accuracy with the previous state-of-the-art systems. Koo10-model1 (Koo and Collins,"
P12-1023,P09-1058,0,0.0270839,"2009). For the unannotated data, we used the XIN CMN portion of Chinese Gigaword5 Version 2.0 (LDC2009T14) (Huang, 2009), 2 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 4 http://www.cis.upenn.edu/˜chinese/. 5 We excluded the sentences of the CTB data from the Gigaword data 3 which has approximately 311 million words whose segmentation and POS tags are given. We discarded the annotations due to the differences in annotation policy between CTB and this corpus. We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. 6.2 set for English. We added the DLM-based features to MST1. Figure 4 shows the UAS curves on the development set, where K is beam size for Intersect and K-best for Rescoring, the X-axis represents K, and the Y-axis represents the UAS scores. The parsing performance generally increased as the K increased. The parser with Intersect always outperformed the one with Rescoring. Features for basic and enhanced parsers 0.928 0.926 System MST1 MSTB1 MST2 MST"
P12-1023,J93-2004,0,0.0440292,"Missing"
P12-1023,P06-1043,0,0.0373156,"s from a large amount of data and used them as the additional features to improve dependency parsing. They approaches were still restricted in a small number of arcs in the graphs. Suzuki et al. (2009) presented a semisupervised learning approach. They extended a Semi-supervised Structured Conditional Model (SSSCM)(Suzuki and Isozaki, 2008) to the dependency parsing problem and combined their method with the approach of Koo et al. (2008). In future work, we may consider apply their methods on our parsers to improve further. Another group of methods are the cotraining/self-training techniques. McClosky et al. (2006) presented a self-training approach for phrase structure parsing. Sagae and Tsujii (2007) used the co-training technique to improve performance. First, two parsers were used to parse the sentences in unannotated data. Then they selected some sentences which have the same trees produced by those two parsers. They retrained a parser on newly parsed sentences and the original labeled data. We are able to use the output of our systems for co-training/self-training techniques. 9 Conclusion We have presented an approach to utilizing the dependency language model to improve graph-based dependency par"
P12-1023,D07-1013,0,0.302751,"d-order features over two adjacent arcs in second-order models. Koo and Collins (2010) further propose a third-order model that uses third-order features. These models utilize higher-order feature representations and achieve better performance than the first-order models. But this achievement is at the cost of the higher decoding complexity, from O(n2 ) to O(n4 ), where n is the length of the input sentence. Thus, it is very hard to develop higher-order models further in this way. 1 Introduction In recent years, there are many data-driven models that have been proposed for dependency parsing (McDonald and Nivre, 2007). Among them, graphbased dependency parsing models have achieved state-of-the-art performance for a wide range of languages as shown in recent CoNLL shared tasks ∗ Corresponding author How to enrich high-order feature representations without increasing the decoding complexity for graph-based models becomes a very challenging problem in the dependency parsing task. In this paper, we solve this issue by enriching the feature representations for a graph-based model using a dependency language model (DLM) (Shen et al., 2008). The N-gram DLM has the ability to predict the next child based on the N-"
P12-1023,E06-1011,0,0.437851,"r approach has two advantages. Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data. McDonald et al. (2005) and Covington (2001) develop models that represent first-order features over a single arc in graphs. By extending the firstorder model, McDonald and Pereira (2006) and Carreras (2007) exploit second-order features over two adjacent arcs in second-order models. Koo and Collins (2010) further propose a third-order model that uses third-order features. These models utilize higher-order feature representations and achieve better performance than the first-order models. But this achievement is at the cost of the higher decoding complexity, from O(n2 ) to O(n4 ), where n is the length of the input sentence. Thus, it is very hard to develop higher-order models further in this way. 1 Introduction In recent years, there are many data-driven models that have been"
P12-1023,P05-1012,0,0.412625,"epresent a set of features for the parsing model. Finally, the features are efficiently integrated into the parsing model during decoding using beam search. Our approach has two advantages. Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data. McDonald et al. (2005) and Covington (2001) develop models that represent first-order features over a single arc in graphs. By extending the firstorder model, McDonald and Pereira (2006) and Carreras (2007) exploit second-order features over two adjacent arcs in second-order models. Koo and Collins (2010) further propose a third-order model that uses third-order features. These models utilize higher-order feature representations and achieve better performance than the first-order models. But this achievement is at the cost of the higher decoding complexity, from O(n2 ) to O(n4 ), where n is the length of the input"
P12-1023,W04-2407,0,0.0332134,"Missing"
P12-1023,W96-0213,0,0.364741,"Missing"
P12-1023,D07-1111,0,0.0671497,"ency parsing. They approaches were still restricted in a small number of arcs in the graphs. Suzuki et al. (2009) presented a semisupervised learning approach. They extended a Semi-supervised Structured Conditional Model (SSSCM)(Suzuki and Isozaki, 2008) to the dependency parsing problem and combined their method with the approach of Koo et al. (2008). In future work, we may consider apply their methods on our parsers to improve further. Another group of methods are the cotraining/self-training techniques. McClosky et al. (2006) presented a self-training approach for phrase structure parsing. Sagae and Tsujii (2007) used the co-training technique to improve performance. First, two parsers were used to parse the sentences in unannotated data. Then they selected some sentences which have the same trees produced by those two parsers. They retrained a parser on newly parsed sentences and the original labeled data. We are able to use the output of our systems for co-training/self-training techniques. 9 Conclusion We have presented an approach to utilizing the dependency language model to improve graph-based dependency parsing. We represent new features based on the dependency language model and integrate them"
P12-1023,P08-1066,0,0.757145,"data-driven models that have been proposed for dependency parsing (McDonald and Nivre, 2007). Among them, graphbased dependency parsing models have achieved state-of-the-art performance for a wide range of languages as shown in recent CoNLL shared tasks ∗ Corresponding author How to enrich high-order feature representations without increasing the decoding complexity for graph-based models becomes a very challenging problem in the dependency parsing task. In this paper, we solve this issue by enriching the feature representations for a graph-based model using a dependency language model (DLM) (Shen et al., 2008). The N-gram DLM has the ability to predict the next child based on the N-1 immediate previous children and their head (Shen et al., 2008). The basic idea behind is that we use the DLM to evaluate whether a valid dependency tree (McDonald and Nivre, 2007) 213 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 213–222, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics is well-formed from a view of large scope. The parsing model searches for the final dependency trees by considering the original scores and the sc"
P12-1023,P08-1076,0,0.265603,"(2009), Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009), and Zhou11 refers to the second-order parser with web-derived selectional preference features of Zhou et al. (2011). The results showed that our MSTB-DLM2 obtained the comparable accuracy with the previous state-of-the-art systems. Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers. Suzuki2009 (Suzuki et al., 2009) reported the best reported result by combining a Semisupervised Structured Conditional Model (Suzuki and Isozaki, 2008) with the method of (Koo et al., 2008). However, their decoding complexities were higher than ours and we believe that the performance of our parser can be further enhanced by integrating their methods with our parser. Compare with previous work on Chinese Table 8 shows the comparative results, where Chen08 refers to the parser of (Chen et al., 2008), Yu08 refers to the parser of (Yu et al., 2008), Zhao09 refers to the parser of (Zhao et al., 2009), and Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009). The results showed that our score for this d"
P12-1023,D09-1058,0,0.366989,"O(n4 ) O(n4 ) O(n4 ) O(n4 ) O(n3 ) O(n4 ) O(Kn3 ) Table 7: Relevant results for English. G denotes the supervised graph-based parsers, S denotes the graph-based parsers with semi-supervised methods, D denotes our new parsers 6.7 Table 6: Main results for Chinese 6.6 and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al. (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al., 2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009), and Zhou11 refers to the second-order parser with web-derived selectional preference features of Zhou et al. (2011). The results showed that our MSTB-DLM2 obtained the comparable accuracy with the previous state-of-the-art systems. Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers. Suzuki2009 (Suzuki et al., 2009) reported the best reported result by combining a Semisupervised Structured Conditional Model (Suzuki"
P12-1023,W03-3023,0,0.67974,"Missing"
P12-1023,C08-1132,0,0.307062,"t.3 We used the MXPOST tagger trained on training data to assign part-of-speech tags and used the Baseline parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008; Chen et al., 2009). For the unannotated data, we used the XIN CMN portion of Chinese Gigaword5 Version 2.0 (LDC2009T14) (Huang, 2009), 2 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 4 http://www.cis.upenn.edu/˜chinese/. 5 We excluded the sentences of the CTB data from the Gigaword data 3 which has approximately 311 million words whose segmentation and POS tags are given. We discarded the annotations due to the differences in annotation policy between CTB and this corpus. We used t"
P12-1023,D08-1059,0,0.094736,"algorithm is in the bottom-up style, the nearer children are generated earlier than the farther ones of the same head. Thus, we calculate the left or right side probability for a new child when a new dependency relation is built. For Figure 2-(a), we add the features of PRc (xt |HIS). Figure 3 shows the structure, where cRs refers to the current children (nearer than xt ) of xs . In the figure, HIS includes cRs and xs . Figure 3: Add DLM-based features in cubic parsing We use beam search to choose the one having the overall best score as the final parse, where K spans are built at each step (Zhang and Clark, 2008). At each step, we perform the parsing actions in the current beam and then choose the best K resulting spans for the next step. The time complexity of the new decoding algorithm is O(Kn3 ) while the original one is O(n3 ), where n is the length of the input sentence. With the rich feature set in Table 1, the running time of Intersect is longer than the time of Rescoring. But Intersect considers more combination of spans with the DLM-based features than Rescoring that is only given a K-best list. 5 Implementation Details 5.1 Baseline parser We implement our parsers based on the MSTParser1 , a"
P12-1023,P09-1007,0,0.0234185,"ised parsers. Suzuki2009 (Suzuki et al., 2009) reported the best reported result by combining a Semisupervised Structured Conditional Model (Suzuki and Isozaki, 2008) with the method of (Koo et al., 2008). However, their decoding complexities were higher than ours and we believe that the performance of our parser can be further enhanced by integrating their methods with our parser. Compare with previous work on Chinese Table 8 shows the comparative results, where Chen08 refers to the parser of (Chen et al., 2008), Yu08 refers to the parser of (Yu et al., 2008), Zhao09 refers to the parser of (Zhao et al., 2009), and Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009). The results showed that our score for this data was the best reported so far and significantly higher than the previous scores. System Chen08 Yu08 Zhao09 Chen09-ord2s MSTB-DLM2 UAS 86.52 87.26 87.0 89.43 91.59 Table 8: Relevant results for Chinese 7 Analysis Dependency parsers tend to perform worse on heads which have many children. Here, we studied the effect of DLM-based features for this structure. We calculated the number of children for each head and listed the accuracy changes for diff"
P12-1023,P11-1156,0,0.277527,"Missing"
P12-1023,D07-1096,0,\N,Missing
P12-1079,W09-0432,0,0.0127262,"y rule (c). To address such issue of the topic similarity model, we further introduce a topic sensitivity model to describe the topic sensitivity of a rule using entropy as a metric: Sensitivity(P (z|r)) (1) =− k=1 Hellinger function is used to calculate distribution distance and is popular in topic model (Blei and Lafferty, 2007).1 By topic similarity, we aim to encourage or penalize the application of a rule for a given document according to their topic distributions, which then helps the SMT system make better translation decisions. 3.2 Topic Sensitivity Domain adaptation (Wu et al., 2008; Bertoldi and Federico, 2009) often distinguishes general-domain data from in-domain data. Similarly, we divide the rules into topic-insensitive rules and topic-sensitive 1 We also try other distance functions, including Euclidean distance, Kullback-Leibler divergence and cosine function. They produce similar results in our preliminary experiments. 752 K ∑ P (z = k|r) × log (P (z = k|r)) (2) k=1 According to the Eq. (2), a topic-insensitive rule has a large entropy, while a topic-sensitive rule has a smaller entropy. By incorporating the topic sensitivity model with the topic similarity model, we enable our SMT system to"
P12-1079,2007.mtsummit-papers.11,0,0.0504094,"cific topic. A phrase pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and Future Work We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system. Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topicspecific lexicon translation. This verifies the advantage of exploiting topic model at t"
P12-1079,D08-1024,0,0.0143161,"one-to-one mapping between source-side and target-side topics. 6.5 Effect on Various Types of Rules To get a more detailed analysis of the result, we further compare the effect of our method on different types of rules. We divide the rules into three types: phrase rules, which only contain terminals and are the same as the phrase pairs in phrasebased system; monotone rules, which contain nonterminals and produce monotone translations; reordering rules, which also contain non-terminals but change the order of translations. We define the monotone and reordering rules according to Chiang et al., (2008). Table 5 show the results. We can see that our method achieves improvements on all the three types of rules. Our topic similarity method on monotone rule achieves the most improvement which is 0.6 B LEU points, while the improvement on reordering rules is the smallest among the three types. This shows that topic information also helps the selections of rules with non-terminals. 7 Related Work In addition to the topic-specific lexicon translation method mentioned in the previous sections, researchers also explore topic model for machine translation in other ways. Foster and Kunh (2007) describ"
P12-1079,J07-2003,0,0.909756,"nstitute of Computing Technology Institute for Infocomm Research Chinese Academy of Sciences {xiaoxinyan, liuqun, sxlin}@ict.ac.cn Abstract by these probabilities. However, the state-of-theart SMT systems translate sentences by using sequences of synchronous rules or phrases, instead of translating word by word. Since a synchronous rule is rarely factorized into individual words, we believe that it is more reasonable to incorporate the topic model directly at the rule level rather than the word level. Consequently, we propose a topic similarity model for hierarchical phrase-based translation (Chiang, 2007), where each synchronous rule is associated with a topic distribution. In particular, Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given doc"
P12-1079,W07-0717,0,0.166869,"Missing"
P12-1079,C08-1041,1,0.127263,"pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and Future Work We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system. Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topicspecific lexicon translation. This verifies the advantage of exploiting topic model at the rule level ove"
P12-1079,N03-1017,0,0.025585,"which comes from the FBIS portion of LDC data. There are 10,947 documents in the FBIS corpus. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST B LEU (Papineni et al., 2002) was used to mea755 sure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are est"
P12-1079,W04-3250,0,0.229373,"2.29 22.69 22.39 22.69 22.92 Avg 26.07 26.47 26.55 26.45 26.71 26.94 Speed 12.6 3.3 11.5 11.7 11.2 10.2 Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the traditional hierarchical system (“Baseline”) and the topic-specific lexicon translation method (“TopicLex”). “SimSrc” and “SimTgt” denote similarity by source-side and target-side rule-distribution respectively, while “Sim+Sen” activates the two similarity and two sensitivity features. “Avg” is the average B LEU score on the two test sets. Scores marked in bold mean significantly (Koehn, 2004) better than Baseline (p &lt; 0.01). 2. Is it helpful to introduce the topic sensitivity model to distinguish topic-insensitive and topic-sensitive rules? 3. Is it necessary to project topics by one-to-many correspondence instead of one-to-one correspondence? 4. What is the effect of our method on various types of rules, such as phrase rules and rules with non-terminals? 6.1 Data We present our experiments on the NIST ChineseEnglish translation tasks. The bilingual training data contains 239K sentence pairs with 6.9M Chinese words and 9.14M English words, which comes from the FBIS portion of LDC"
P12-1079,D09-1092,0,0.0451605,"Missing"
P12-1079,P02-1038,0,0.0955267,"Missing"
P12-1079,J03-1002,0,0.00274223,"al training data contains 239K sentence pairs with 6.9M Chinese words and 9.14M English words, which comes from the FBIS portion of LDC data. There are 10,947 documents in the FBIS corpus. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST B LEU (Papineni et al., 2002) was used to mea755 sure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampl"
P12-1079,P03-1021,0,0.0245589,"6/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST B LEU (Papineni et al., 2002) was used to mea755 sure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are estimated from the Chinese part and English part of FBIS corpus respectively. We set the number of topic K = 30 for both source-side and target-side, and use the default setting of the tool for training and inference.4 During decoding, we first infer the topic distribution of given documents before translation accord"
P12-1079,P02-1040,0,0.0984212,"ns 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST B LEU (Papineni et al., 2002) was used to mea755 sure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are estimated from the Chinese part and English part of FBIS corpus respectively. We set the number of topic K = 30 for both source-side and target-side, and use the default setting of the tool for training and inference.4 Du"
P12-1079,W11-2133,0,0.168652,". Finally, they 757 combine a specific domain translation model with a general domain translation model depending on various text distances. One way to calculate the distance is using topic model. Gong et al. (2010) introduce topic model for filtering topic-mismatched phrase pairs. They first assign a specific topic for the document to be translated. Similarly, each phrase pair is also assigned with one specific topic. A phrase pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and"
P12-1079,D09-1008,0,0.0603759,"arded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and Future Work We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system. Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topicspecific lexicon translation. This verifies the advantage of exploiting topic model at the rule level over the word level. Fu"
P12-1079,C08-1125,0,0.0128983,"one-to-one mapping between source-side and target-side topics. 6.5 Effect on Various Types of Rules To get a more detailed analysis of the result, we further compare the effect of our method on different types of rules. We divide the rules into three types: phrase rules, which only contain terminals and are the same as the phrase pairs in phrasebased system; monotone rules, which contain nonterminals and produce monotone translations; reordering rules, which also contain non-terminals but change the order of translations. We define the monotone and reordering rules according to Chiang et al., (2008). Table 5 show the results. We can see that our method achieves improvements on all the three types of rules. Our topic similarity method on monotone rule achieves the most improvement which is 0.6 B LEU points, while the improvement on reordering rules is the smallest among the three types. This shows that topic information also helps the selections of rules with non-terminals. 7 Related Work In addition to the topic-specific lexicon translation method mentioned in the previous sections, researchers also explore topic model for machine translation in other ways. Foster and Kunh (2007) describ"
P12-1079,P06-2124,0,0.674686,"aches that work at the word level. 1 • Given a document to be translated, we calculate the topic similarity between a rule and the document based on their topic distributions. We augment the hierarchical phrase-based system by integrating the proposed topic similarity model as a new feature (Section 3.1). Introduction Topic model (Hofmann, 1999; Blei et al., 2003) is a popular technique for discovering the underlying topic structure of documents. To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality. Topic-specific lexicon translation models focus on word-level translations. Such models first estimate word translation probabilities conditioned on topics, and then adapt lexical weights of phrases ∗ {dyxiong, mzhang∗ }@i2r.a-star.edu.sg Corresponding author • As we will discuss in Section 3.2, the similarity between a generic rule and a given source document computed by our topic similarity model is often very low. We don’t want to penalize these generic rules. Therefore we further propose a topic sensitivity model whic"
P12-1095,W11-2136,0,0.123696,"rectly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). There are other previous studies that explore only source side predicate-argument structures. Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. Wu et al. (2011) automate this procedure by automatically extracting reordering rules from predicate-argument structures and applying these rules to reorder source language sentences. Aziz et al. (2011) incorporate source language semantic role labels into a tree-to-string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argument movement (rel903 ative to its predicate) prediction problem and use a discriminatively trained classifier for such predictions. Our predicate translation model is also related to prev"
P12-1095,J96-1002,0,0.0224461,"s into a tree-to-string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argument movement (rel903 ative to its predicate) prediction problem and use a discriminatively trained classifier for such predictions. Our predicate translation model is also related to previous discriminative lexicon translation models (Berger et al., 1996; Venkatapathy and Bangalore, 2007; Mauser et al., 2009). While previous models predict translations for all words in vocabulary, we only focus on verbal predicates. This will tremendously reduce the amount of training data required, which usually is a problem in discriminative lexicon translation models (Mauser et al., 2009). Furthermore, the proposed translation model also differs from previous lexicon translation models in that we use both lexical and semantic features. Our experimental results show that semantic features are able to further improve translation accuracy. 3 Predicate Transla"
P12-1095,J07-2003,0,0.609026,"tion 3.3 to train the maximum entropy classifier as formulated in Eq. (4). We perform 100 iterations of L-BFGS. 5 Integrating the Two Models into SMT In this section, we elaborate how to integrate the two models into phrase-based SMT. In particular, we integrate the models into a phrase-based system which uses bracketing transduction grammars (BTG) (Wu, 1997) for phrasal translation (Xiong et al., 2006). Since the system is based on a CKY-style decoder, the integration algorithms introduced here can be easily adapted to other CKY-based decoding systems such as the hierarchical phrasal system (Chiang, 2007). 5.1 Integrating the Predicate Translation Model It is straightforward to integrate the predicate translation model into phrase-based SMT (Koehn et al., 906 2003; Xiong et al., 2006). We maintain word alignments for each phrase pair in the phrase table. Given a source sentence with its predicateargument structure, we detect all verbal predicates and load trained predicate translation classifiers for these verbs. Whenever a hypothesis covers a new verbal predicate v, we find the target translation e for v through word alignments and then calculate its translation probability pt (e|C(v)) accord"
P12-1095,N03-1017,0,0.107241,"(v).Ah1 = d e = adjourn and C(v).Ar2 = null e = adjourn and C(v).Ah3 = null This will increase the number of classes to be predicted by the maximum entropy classifier. But according to our observation, it is still computationally tractable (see Section 3.3). If a verbal predicate is not translated, we set e = NULL so that we can also capture null translations for verbal predicates. Table 1: Semantic feature examples. 3.2 Features The apparent advantage of discriminative lexicon translation models over generative translation models (e.g., conventional lexical translation model as described in (Koehn et al., 2003)) is that discriminative models allow us to integrate richer contexts (lexical, syntactic or semantic) into target translation prediction. We use two kinds of features to predict translations for verbal predicates: 1) lexical features and 2) semantic features. All features are in the following binary form. f (e, C(v)) =  1, if e = ♣ and C(v).♥ = ♠ 0, else (3) where the symbol ♣ is a placeholder for a possible target translation (up to 4 words), the symbol ♥ indicates a contextual (lexical or semantic) element for the verbal predicate v, and the symbol ♠ represents the value of ♥. Lexical Feat"
P12-1095,W04-3250,0,0.219224,"ntic role labeler6 (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbal predicates. After we obtained semantic roles on the source side, we extracted features as described in Section 3.2 and 4.2 and used these features to train our two models as described in Section 3.3 and 4.3. We used the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as the evaluation metric. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). 6.2 Results Our first group of experiments is to investigate whether the predicate translation model is able to improve translation accuracy in terms of BLEU and whether semantic features are useful. The experimental results are shown in Table 4. From the table, we have the following two observations. • The proposed predicate translation models achieve an average improvement of 0.57 BLEU points across the two NIST test sets when all features (lex+sem) are used. Such an improvement is statistically significant (p < 0.01). According to our statistics, there are 5.07 verbal predicates per sente"
P12-1095,2006.iwslt-evaluation.11,0,0.121648,"ork. As PAS analysis widely employs global and sentence-wide features, it is computationally expensive to integrate target side predicateargument structures into the dynamic programming style of SMT decoding (Wu and Fung, 2009b). Therefore they either postpone the integration of target side PASs until the whole decoding procedure is completed (Wu and Fung, 2009b), or directly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). There are other previous studies that explore only source side predicate-argument structures. Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. Wu et al. (2011) automate this procedure by automatically extracting reordering rules from predicate-argument structures and applying these rules to reorder source language sentences. Aziz et al. (2011) incorporate source language semantic role labels into a tree-to-string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separa"
P12-1095,P10-1113,0,0.215021,"Missing"
P12-1095,C10-1081,0,0.636321,"nt reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English translation tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy. 1 Introduction Recent years have witnessed increasing efforts towards integrating predicate-argument structures into statistical machine translation (SMT) (Wu and Fung, 2009b; Liu and Gildea, 2010). In this paper, we take a step forward by introducing a novel approach to incorporate such semantic structures into SMT. Given a source side predicate-argument structure, we attempt to translate each semantic frame (predicate and its associated arguments) into an appropriate target string. We believe that the translation of predicates and reordering of arguments are the two central ∗ issues concerning the transfer of predicate-argument structure across languages. Predicates1 are essential elements in sentences. Unfortunately they are usually neither correctly translated nor translated at all"
P12-1095,D09-1022,0,0.0179658,"cus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argument movement (rel903 ative to its predicate) prediction problem and use a discriminatively trained classifier for such predictions. Our predicate translation model is also related to previous discriminative lexicon translation models (Berger et al., 1996; Venkatapathy and Bangalore, 2007; Mauser et al., 2009). While previous models predict translations for all words in vocabulary, we only focus on verbal predicates. This will tremendously reduce the amount of training data required, which usually is a problem in discriminative lexicon translation models (Mauser et al., 2009). Furthermore, the proposed translation model also differs from previous lexicon translation models in that we use both lexical and semantic features. Our experimental results show that semantic features are able to further improve translation accuracy. 3 Predicate Translation Model In this section, we present the features and"
P12-1095,P02-1040,0,0.0970476,"ordering model, we first parsed all source sentences using the Berkeley Chinese parser (Petrov et al., 2006) and then ran the Chinese semantic role labeler6 (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbal predicates. After we obtained semantic roles on the source side, we extracted features as described in Section 3.2 and 4.2 and used these features to train our two models as described in Section 3.3 and 4.3. We used the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as the evaluation metric. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). 6.2 Results Our first group of experiments is to investigate whether the predicate translation model is able to improve translation accuracy in terms of BLEU and whether semantic features are useful. The experimental results are shown in Table 4. From the table, we have the following two observations. • The proposed predicate translation models achieve an average improvement of 0.57 BLEU points across the two NIST test sets when all features (lex+sem) are used. Such"
P12-1095,P06-1055,0,0.00611792,"rs with 96.9M Chinese words and 109.5M English words. We ran GIZA++ on these corpora in both directions and then applied the “grow-diag-final” refinement rule to obtain word alignments. We then used all these word-aligned corpora to generate our phrase table. Our 5-gram language model was trained on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. To train the proposed predicate translation model and argument reordering model, we first parsed all source sentences using the Berkeley Chinese parser (Petrov et al., 2006) and then ran the Chinese semantic role labeler6 (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbal predicates. After we obtained semantic roles on the source side, we extracted features as described in Section 3.2 and 4.2 and used these features to train our two models as described in Section 3.3 and 4.3. We used the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as the evaluation metric. Statistical significance in BLEU differences was tested by"
P12-1095,W07-0413,0,0.0437434,"Missing"
P12-1095,2009.eamt-1.30,0,0.497454,"redicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English translation tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy. 1 Introduction Recent years have witnessed increasing efforts towards integrating predicate-argument structures into statistical machine translation (SMT) (Wu and Fung, 2009b; Liu and Gildea, 2010). In this paper, we take a step forward by introducing a novel approach to incorporate such semantic structures into SMT. Given a source side predicate-argument structure, we attempt to translate each semantic frame (predicate and its associated arguments) into an appropriate target string. We believe that the translation of predicates and reordering of arguments are the two central ∗ issues concerning the transfer of predicate-argument structure across languages. Predicates1 are essential elements in sentences. Unfortunately they are usually neither correctly translate"
P12-1095,N09-2004,0,0.55119,"redicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English translation tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy. 1 Introduction Recent years have witnessed increasing efforts towards integrating predicate-argument structures into statistical machine translation (SMT) (Wu and Fung, 2009b; Liu and Gildea, 2010). In this paper, we take a step forward by introducing a novel approach to incorporate such semantic structures into SMT. Given a source side predicate-argument structure, we attempt to translate each semantic frame (predicate and its associated arguments) into an appropriate target string. We believe that the translation of predicates and reordering of arguments are the two central ∗ issues concerning the transfer of predicate-argument structure across languages. Predicates1 are essential elements in sentences. Unfortunately they are usually neither correctly translate"
P12-1095,I11-1004,0,0.200715,"yle of SMT decoding (Wu and Fung, 2009b). Therefore they either postpone the integration of target side PASs until the whole decoding procedure is completed (Wu and Fung, 2009b), or directly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). There are other previous studies that explore only source side predicate-argument structures. Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. Wu et al. (2011) automate this procedure by automatically extracting reordering rules from predicate-argument structures and applying these rules to reorder source language sentences. Aziz et al. (2011) incorporate source language semantic role labels into a tree-to-string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argume"
P12-1095,J97-3002,0,0.28733,"Missing"
P12-1095,P06-1066,1,0.600139,"): the function finds all predicateargument pairs that cross the two neighboring spans (i, k) and (k + 1, j). S It can be formulated as A(i, j, τ ) − (A(i, k, τ ) A(k + 1, j, τ )). We then define another function Pr to calculate the argument reordering model probability on all arguments which are found by the previous two functions A and N as follows. Y Pr (B) = pr (mA |C(A)) (6) A∈B where B denotes either A or N . Following (Chiang, 2007), we describe the algorithm in a deductive system. It is shown in Figure 2. The algorithm integrates the argument reordering model into a CKY-style decoder (Xiong et al., 2006). The item [X, i, j] denotes a BTG node X spanning from i to j on the source side. For notational convenience, we only show the argument reordering model probability for each item, ignoring all other sub-model probabilities such as the language model probability. The Eq. (7) shows how we calculate the argument reordering model probability when a lexical rule is applied to translate a source phrase c to a target phrase e. The Eq. (8) shows how we compute the argument reordering model probability for a span (i, j) in a dynamic programming manner when a merging rule is applied to combine its two"
P12-1095,J08-2004,0,0.0947935,"tly translated nor translated at all in many SMT systems according to the error study by Wu and Fung (2009a). This suggests that conventional lexical and phrasal translation models adopted in those SMT systems are not sufficient to correctly translate predicates in source sentences. Thus we propose a discriminative, feature-based predicate translation model that captures not only lexical information (i.e., surrounding words) but also high-level semantic contexts to correctly translate predicates. Arguments contain information for questions of who, what, when, where, why, and how in sentences (Xue, 2008). One common error in translating arguments is about their reorderings: arguments are placed at incorrect positions after translation. In order to reduce such errors, we introduce a discriminative argument reordering model that uses the position of a predicate as the reference axis to estimate positions of its associated arguments on the target side. In this way, the model predicts moving directions of arguments relative to their predicates with semantic features. We integrate these two discriminative models into a state-of-the-art phrase-based system. Experimental results on large-scale Chine"
P13-1043,P05-1022,0,0.0285913,"Missing"
P13-1043,A00-2018,0,0.0580873,"oduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the parsing accuracies. Our method is conceptually simple, requiring only one additional transition action to eliminate size differences between different candidate outp"
P13-1043,D09-1060,1,0.318907,"ed, and output the best state item in the agenda. With this new transition process, we experimented with several extended features,and found that the templates in Table 2 are useful to improve the accuracies further. Here si ll denotes the left child of si ’s left child. Other notations can be explained in a similar way. 4.2 Dependency Relations: Lexical Dependencies Lexical dependencies represent linguistic relations between words: whether a word modifies another word. The idea of exploiting lexical dependency information from auto-parsed data has been explored before for dependency parsing (Chen et al., 2009) and constituent parsing (Zhu et al., 2012). To extract lexical dependencies, we first run the baseline parser on unlabeled data. To simplify the extraction process, we can convert auto-parsed constituency trees into dependency trees by using Penn2Malt. 2 From the dependency trees, we extract bigram lexical dependencies hw1 , w2 , L/Ri where the symbol L (R) means that w1 (w2 ) is the head of w2 (w1 ). We also extract trigram lexical 4 Semi-supervised Parsing with Large Data This section discusses how to extract information from unlabeled data or auto-parsed data to further improve shift-reduc"
P13-1043,P12-1023,1,0.715706,"Missing"
P13-1043,P04-1015,0,0.364497,"cq0 w, s0 cq0 t, q0 wq1 w, q0 wq1 t, q0 tq1 w, q0 tq1 t, s1 wq0 w, s1 wq0 t, s1 cq0 w, s1 cq0 t s0 cs1 cs2 c, s0 ws1 cs2 c, s0 cs1 wq0 t s0 cs1 cs2 w, s0 cs1 cq0 t, s0 ws1 cq0 t s0 cs1 wq0 t, s0 cs1 cq0 w NN address i=1 Φ(ai ) · θ~ Here Φ(ai ) represents the feature vector for the ith action ai in state item α. It is computed by applying the feature templates in Table 1 to the context of α. N is the total number of actions in α. The model parameter ~ θ is trained with the averaged perceptron algorithm, applied to state items (sequence of actions) globally. We apply the early update strategy (Collins and Roark, 2004), stopping parsing for parameter updates when the goldstandard state item falls off the agenda. 2.3 NP address NNS issues issues items for the same sentence can have different numbers of unary actions. Take the phrase “address issues” for example, two possible parses are shown in Figure 2 (a) and (b), respectively. The first parse corresponds to the action sequence [SHIFT, SHIFT, REDUCE-R-NP, FINISH], while the second parse corresponds to the action sequence [SHIFT, SHIFT, UNARY-NP, REDUCE-LVP, FINISH], which consists of one more action than the first case. In practice, variances between state"
P13-1043,P97-1003,0,0.0366356,"d as the 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the parsing accuracies. Our method is conceptually simple, requiring only one additional transition action to eliminate size differences between differen"
P13-1043,D12-1133,0,0.0157449,"2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In addition, processing tens of sentences per second (Zhang and Nivre, 2011), these transition-based parsers can be a favorable choice for dependency parsing. Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global trai"
P13-1043,W08-2102,0,0.0934831,"Missing"
P13-1043,D09-1087,0,0.0306782,"Missing"
P13-1043,P10-1110,0,0.023533,"ang Chen∗ , Min Zhang∗ and Jingbo Zhu† † Natural Language Processing Lab., Northeastern University, China ‡ Singapore University of Technology and Design, Singapore ∗ Soochow University, China and Institute for Infocomm Research, Singapore zhumuhua@gmail.com yue zhang@sutd.edu.sg chenwenliang@gmail.com mzhang@i2r.a-star.edu.sg zhujingbo@mail.neu.edu.cn Abstract Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In"
P13-1043,D10-1002,0,0.0607489,"Missing"
P13-1043,P08-1067,0,0.0401316,"Missing"
P13-1043,N06-2033,0,0.0133712,"eam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. The above global-learning and beam-search framework can be applied to transition-based phrase-structure (constituent) parsing also (Zhang and Clark, 2009), maintaining all the aforementioned benefits. However, the effects were not as significant as for transition-based dependency parsing. The best reported accuracies of transition-based constituent parsers still lag behind the state-of-the-art (Sagae and Lavie, 2006; Zhang and Clark, 2009). One difference between phrasestructure parsing and dependency parsing is that for the former, parse trees with different numbers of unary rules require different numbers of actions to build. Hence the scoring model needs to disambiguate between transitions sequences with different sizes. For the same sentence, the largest output can take twice as many as actions to build as the 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local d"
P13-1043,P08-1066,0,0.0170814,"Missing"
P13-1043,P12-1026,0,0.0106504,"tions are captured by word clustering, lexical dependencies, and a dependency language model, respectively. Based on the information, we propose a set of novel features specifically designed for shift-reduce constituent parsing. [S|s0 , i, false, k, c] [S|X, i, false, k + 1, c + cu ] [S, n, false, k, c] [S, n, true, k + 1, c + cf ] [S, n, true, k, c] [S, n, true, k + 1, c + ci ] 4.1 Paradigmatic Relations: Word Clustering Figure 3: Deductive system of the extended transition system. Word clusters are regarded as lexical intermediaries for dependency parsing (Koo et al., 2008) and POS tagging (Sun and Uszkoreit, 2012). We employ the Brown clustering algorithm (Liang, 2005) on unannotated data (word segmentation is performed if necessary). In the initial state of clustering, each word in the input corpus is regarded as a cluster, then the algorithm repeatedly merges pairs of clusters that cause the least decrease in the likelihood of the input corpus. The clustering results are a binary tree with words appearing as leaves. Each cluster is represented as a bit-string from the root to the tree node that represents the cluster. We define a function CLU(w) to return the cluster ID (a bit string) of an input wor"
P13-1043,P08-1068,0,0.00967274,"nd structural relations. These relations are captured by word clustering, lexical dependencies, and a dependency language model, respectively. Based on the information, we propose a set of novel features specifically designed for shift-reduce constituent parsing. [S|s0 , i, false, k, c] [S|X, i, false, k + 1, c + cu ] [S, n, false, k, c] [S, n, true, k + 1, c + cf ] [S, n, true, k, c] [S, n, true, k + 1, c + ci ] 4.1 Paradigmatic Relations: Word Clustering Figure 3: Deductive system of the extended transition system. Word clusters are regarded as lexical intermediaries for dependency parsing (Koo et al., 2008) and POS tagging (Sun and Uszkoreit, 2012). We employ the Brown clustering algorithm (Liang, 2005) on unannotated data (word segmentation is performed if necessary). In the initial state of clustering, each word in the input corpus is regarded as a cluster, then the algorithm repeatedly merges pairs of clusters that cause the least decrease in the likelihood of the input corpus. The clustering results are a binary tree with words appearing as leaves. Each cluster is represented as a bit-string from the root to the tree node that represents the cluster. We define a function CLU(w) to return the"
P13-1043,W03-3023,0,0.0272721,"y rules require different numbers of actions to build. Hence the scoring model needs to disambiguate between transitions sequences with different sizes. For the same sentence, the largest output can take twice as many as actions to build as the 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce pro"
P13-1043,P08-1101,1,0.41006,"u† , Yue Zhang‡ , Wenliang Chen∗ , Min Zhang∗ and Jingbo Zhu† † Natural Language Processing Lab., Northeastern University, China ‡ Singapore University of Technology and Design, Singapore ∗ Soochow University, China and Institute for Infocomm Research, Singapore zhumuhua@gmail.com yue zhang@sutd.edu.sg chenwenliang@gmail.com mzhang@i2r.a-star.edu.sg zhujingbo@mail.neu.edu.cn Abstract Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohn"
P13-1043,W09-3825,1,0.736189,"outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. The above global-learning and beam-search framework can be applied to transition-based phrase-structure (constituent) parsing also (Zhang and Clark, 2009), maintaining all the aforementioned benefits. However, the effects were not as significant as for transition-based dependency parsing. The best reported accuracies of transition-based constituent parsers still lag behind the state-of-the-art (Sagae and Lavie, 2006; Zhang and Clark, 2009). One difference between phrasestructure parsing and dependency parsing is that for the former, parse trees with different numbers of unary rules require different numbers of actions to build. Hence the scoring model needs to disambiguate between transitions sequences with different sizes. For the same sentenc"
P13-1043,J93-2004,0,0.0616461,"Missing"
P13-1043,N06-1020,0,0.0167397,"Missing"
P13-1043,P05-1012,0,0.0147001,"ion-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the parsing accuracies. Our method is conceptually simple, requiring only one additional transition action to eliminate size differences between different candidate outputs. On standard evaluat"
P13-1043,nivre-etal-2006-maltparser,0,0.0175741,"Missing"
P13-1043,N07-1051,0,0.0287651,"Missing"
P13-1043,W97-0301,0,0.268373,"Missing"
P13-1043,W05-1513,0,0.352838,"needs to disambiguate between transitions sequences with different sizes. For the same sentence, the largest output can take twice as many as actions to build as the 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the p"
P13-1043,P11-2033,1,0.692073,"sing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In addition, processing tens of sentences per second (Zhang and Nivre, 2011), these transition-based parsers can be a favorable choice for dependency parsing. Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the"
P13-1043,C12-1194,1,0.77715,"enda. With this new transition process, we experimented with several extended features,and found that the templates in Table 2 are useful to improve the accuracies further. Here si ll denotes the left child of si ’s left child. Other notations can be explained in a similar way. 4.2 Dependency Relations: Lexical Dependencies Lexical dependencies represent linguistic relations between words: whether a word modifies another word. The idea of exploiting lexical dependency information from auto-parsed data has been explored before for dependency parsing (Chen et al., 2009) and constituent parsing (Zhu et al., 2012). To extract lexical dependencies, we first run the baseline parser on unlabeled data. To simplify the extraction process, we can convert auto-parsed constituency trees into dependency trees by using Penn2Malt. 2 From the dependency trees, we extract bigram lexical dependencies hw1 , w2 , L/Ri where the symbol L (R) means that w1 (w2 ) is the head of w2 (w1 ). We also extract trigram lexical 4 Semi-supervised Parsing with Large Data This section discusses how to extract information from unlabeled data or auto-parsed data to further improve shift-reduce parsing accuracies. We consider three typ"
P13-1043,J03-4003,0,\N,Missing
P14-1043,D09-1087,0,0.0224704,"results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two par"
P14-1043,P11-1070,0,0.0123477,"chen}@suda.edu.cn Abstract of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated f"
P14-1043,D12-1133,0,0.025955,"Missing"
P14-1043,C10-1011,0,0.0365972,"al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 h s m (b) adjacent sibling {(h,m)}⊆d + X wsib · fsib (x, h, s, m) {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. 2. We summarize the atomic features used in each feature category in Table 1. These atomic features are concatenated in different combinations to compose rich feature sets. Please refer to Table 4 of Bohnet (2010) for the complete feature list. Graph-based Dependency Parser (GParser) In this work, we adopt the graph-based paradigm because it allows us to naturally derive conditional probability of a dependency tree d given a sentence x, which is required to compute likelihood of both labeled and unlabeled"
P14-1043,D07-1101,0,0.552209,"of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 < m ≤ n}, where (h, m) indicates a directed arc from the head word wh to the modifier wm , and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 h s m (b) adjacent sibling {(h,m)}⊆d + X wsib · fsib (x, h, s, m) {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (2010) which include two categories correspondi"
P14-1043,P10-1001,0,0.198994,"arsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 < m ≤ n}, where (h, m) indicates a directed arc from the head word wh to the modifier wm , and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 h s m (b) adjacent sibling {(h,m)}⊆d + X wsib · fsib (x, h, s, m) {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of s"
P14-1043,P08-1068,0,0.409193,"g∗, Wenliang Chen Provincial Key Laboratory for Computer Information Processing Technology Soochow University {zhli13,minzhang,wlchen}@suda.edu.cn Abstract of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse tr"
P14-1043,C12-1103,1,0.917666,"Missing"
P14-1043,P05-1022,0,0.0577924,"iveness of our approach. Specifically, we find that our approach is very effective when using divergent parsers such as the generative parser, and it is also helpful to properly balance the size and oracle accuracy of the parse forest of the unlabeled data. For future work, among other possible extensions, we would like to see how our approach performs when employing more diverse parsers to compose the parse forest of higher quality for the unlabeled data, such as the easyfirst non-directional dependency parser (Goldberg and Elhadad, 2010) and other constituent parsers (Collins and Koo, 2005; Charniak and Johnson, 2005; Finkel et al., 2008). 93.2 93.1 93 UAS 92.9 92.8 92.7 92.6 92.5 92.4 B+Z Parser 92.3 0 50K 100K 200K 500K 1M 1.7M Unlabeled Data Size Figure 3: Performance of GParser with different sizes of “Unlabeled ← B+Z” on English test set. 5 Related Work Our work is originally inspired by the work of T¨ackstr¨om et al. (2013). They first apply the idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for a resourcepoor target language by making use of sourcelanguage treebanks. Different from their work, we explore the ide"
P14-1043,N06-1020,0,0.116258,"w parser. Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best p"
P14-1043,E06-1011,0,0.470919,"h , ws , wm , th , tm , ts , th±1 , tm±1 , ts±1 dir(h, m), dist(h, m) 3. We build the first state-of-the-art CRF-based dependency parser. Using the probabilistic parser, we benchmark and conduct systematic comparisons among ours and all previous bootstrapping methods, including self/co/tritraining. Table 1: Brief illustration of the syntactic features. ti denotes the POS tag of wi . b is an index between h and m. dir(i, j) and dist(i, j) denote the direction and distance of the dependency (i, j). 2 Supervised Dependency Parsing We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. 2.1 Then the score of a dependency tree is: X Score(x, d; w) = wdep · fdep (x, h, m) Given an input sentence x = w0 w1 ...wn , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 < m ≤ n}, where (h, m) indicates a directed arc from the head word wh to the modifier wm , and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspe"
P14-1043,D09-1060,1,0.844222,"ory for Computer Information Processing Technology Soochow University {zhli13,minzhang,wlchen}@suda.edu.cn Abstract of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is t"
P14-1043,D13-1129,1,0.895096,"Missing"
P14-1043,J05-1003,0,0.0347821,"demonstrates the effectiveness of our approach. Specifically, we find that our approach is very effective when using divergent parsers such as the generative parser, and it is also helpful to properly balance the size and oracle accuracy of the parse forest of the unlabeled data. For future work, among other possible extensions, we would like to see how our approach performs when employing more diverse parsers to compose the parse forest of higher quality for the unlabeled data, such as the easyfirst non-directional dependency parser (Goldberg and Elhadad, 2010) and other constituent parsers (Collins and Koo, 2005; Charniak and Johnson, 2005; Finkel et al., 2008). 93.2 93.1 93 UAS 92.9 92.8 92.7 92.6 92.5 92.4 B+Z Parser 92.3 0 50K 100K 200K 500K 1M 1.7M Unlabeled Data Size Figure 3: Performance of GParser with different sizes of “Unlabeled ← B+Z” on English test set. 5 Related Work Our work is originally inspired by the work of T¨ackstr¨om et al. (2013). They first apply the idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for a resourcepoor target language by making use of sourcelanguage treebanks. Different from th"
P14-1043,P05-1012,0,0.196144,"Given an input sentence x = w0 w1 ...wn , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 < m ≤ n}, where (h, m) indicates a directed arc from the head word wh to the modifier wm , and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 h s m (b) adjacent sibling {(h,m)}⊆d + X wsib · fsib (x, h, s, m) {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (20"
P14-1043,P08-1108,0,0.0613393,"ws produce divergent structures. Impact of unlabeled data size: To understand how our approach performs with regards to the unlabeled data size, we train semi-supervised GParser with different sizes of unlabeled data. Fig. 3 shows the accuracy curve on the test set. We can see that the parser consistently achieves higher accuracy with more unlabeled data, demonstrating the effectiveness of our approach. We expect that our approach has potential to achieve higher accuracy with more additional data. uses one parser’s outputs as guide features for another parser, leading to improved performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree (Sagae and Lavie, 2006; Surdeanu and Manning, 2010). One possible drawback of parser ensemble is that several parsers are required to parse the same sentence during the test phase. Moreover, our approach can benefit from these methods in that we can get parse forests of higher quality on unlabeled data (Zhou, 2009). 6 Conclusions This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguou"
P14-1043,W03-3017,0,0.0361248,"modifier wm , and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 h s m (b) adjacent sibling {(h,m)}⊆d + X wsib · fsib (x, h, s, m) {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. 2. We summarize the atomic features used in each feature category in Table 1. These atomic features are concatenated in different combinations"
P14-1043,N07-1051,0,0.170069,"Figure 1 contains four parse trees after combination of the two different choices. Second, the parser is able to learn useful features from the unambiguous parts of the parse forest. Finally, with sufficient unlabeled data, it is possible that the parser can learn to resolve such uncertainty by biasing to more reasonable parse trees. To construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser (Zhang and Nivre, 2011), and a generative constituent parser (Petrov and Klein, 2007). The 1-best parse trees of these three parsers are aggregated in different ways. Evaluation on labeled data shows the oracle accuracy of parse forest is much higher than that of 1-best outputs of single parsers (see Table 3). Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. Experimental results on both English and Chinese datasets demonstrate that the proposed ambiguity-aware ensemble training outperforms other entire-tree based methods suc"
P14-1043,N06-2033,0,0.0303478,"fferent sizes of unlabeled data. Fig. 3 shows the accuracy curve on the test set. We can see that the parser consistently achieves higher accuracy with more unlabeled data, demonstrating the effectiveness of our approach. We expect that our approach has potential to achieve higher accuracy with more additional data. uses one parser’s outputs as guide features for another parser, leading to improved performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree (Sagae and Lavie, 2006; Surdeanu and Manning, 2010). One possible drawback of parser ensemble is that several parsers are required to parse the same sentence during the test phase. Moreover, our approach can benefit from these methods in that we can get parse forests of higher quality on unlabeled data (Zhou, 2009). 6 Conclusions This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings. For each unlabeled sentence, we combine the 1-best parse trees of several diverse parsers to compose ambiguous labelings, represented by a parse forest. The training obj"
P14-1043,P08-1109,0,0.0368128,"Missing"
P14-1043,D07-1111,0,0.0936021,"up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical. In this way, the autoparsed unlabeled data becomes more reliable. 1 Introduction Supervised dependency parsing has made great progress during the past decade. However, it is very difficult to further improve performance ∗ Cor"
P14-1043,N10-1115,0,0.0132809,"parsers, than traditional tri-training. Detailed analysis demonstrates the effectiveness of our approach. Specifically, we find that our approach is very effective when using divergent parsers such as the generative parser, and it is also helpful to properly balance the size and oracle accuracy of the parse forest of the unlabeled data. For future work, among other possible extensions, we would like to see how our approach performs when employing more diverse parsers to compose the parse forest of higher quality for the unlabeled data, such as the easyfirst non-directional dependency parser (Goldberg and Elhadad, 2010) and other constituent parsers (Collins and Koo, 2005; Charniak and Johnson, 2005; Finkel et al., 2008). 93.2 93.1 93 UAS 92.9 92.8 92.7 92.6 92.5 92.4 B+Z Parser 92.3 0 50K 100K 200K 500K 1M 1.7M Unlabeled Data Size Figure 3: Performance of GParser with different sizes of “Unlabeled ← B+Z” on English test set. 5 Related Work Our work is originally inspired by the work of T¨ackstr¨om et al. (2013). They first apply the idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for a resourcepoor target language by maki"
P14-1043,D07-1070,0,0.0173488,"eatures into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed"
P14-1043,C10-1120,0,0.166841,"owsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical. In this way, the autoparsed unlabeled data becomes more reliable. 1 Introduction Supervised dependency parsing has made great progress during the past decade. However, it is very difficult to further improve performance ∗ Correspondence author 457 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 457–467, c B"
P14-1043,W09-1104,0,0.135652,"supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical. In this way, the autoparsed unlabeled data becomes more reliable."
P14-1043,N10-1091,0,0.0241289,"led data. Fig. 3 shows the accuracy curve on the test set. We can see that the parser consistently achieves higher accuracy with more unlabeled data, demonstrating the effectiveness of our approach. We expect that our approach has potential to achieve higher accuracy with more additional data. uses one parser’s outputs as guide features for another parser, leading to improved performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree (Sagae and Lavie, 2006; Surdeanu and Manning, 2010). One possible drawback of parser ensemble is that several parsers are required to parse the same sentence during the test phase. Moreover, our approach can benefit from these methods in that we can get parse forests of higher quality on unlabeled data (Zhou, 2009). 6 Conclusions This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings. For each unlabeled sentence, we combine the 1-best parse trees of several diverse parsers to compose ambiguous labelings, represented by a parse forest. The training objective is to maximize the mix"
P14-1043,D09-1058,0,0.150625,"s to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labeled data and auto-pars"
P14-1043,N13-1126,0,0.0453798,"Missing"
P14-1043,D08-1017,0,0.0995804,"Missing"
P14-1043,P08-1061,0,0.01868,"ed parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labe"
P14-1043,W03-3023,0,0.134172,"rom the head word wh to the modifier wm , and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 h s m (b) adjacent sibling {(h,m)}⊆d + X wsib · fsib (x, h, s, m) {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. 2. We summarize the atomic features used in each feature category in Table 1. These atomic features are concatenated in different"
P14-1043,P95-1026,0,0.203753,"referred parse trees as long as the likelihood improves. 2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser. Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and"
P14-1043,D12-1030,0,0.024625,"hey adopt the higher-order model of Carreras (2007). Again, our method may be combined with their work to achieve higher performance. Comparison with Previous Work We adopt the best settings on development data for semi-supervised GParser with our proposed approach, and make comparison with previous results on test data. Table 4 shows the results. The first major row lists several state-of-theart supervised methods. McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. Koo and Collins (2010) propose a third-order graphbased parser. Zhang and McDonald (2012) explore higher-order features for graph-based dependency parsing, and adopt beam search for fast decoding. Zhang and Nivre (2011) propose a feature-rich transition-based parser. All work in the second major row adopts semi-supervised methods. The results show that our approach achieves comparable accuracy with most previous semi-supervised methods. Both Suzuki et al. (2009) and Chen et al. (2013) adopt the higherorder parsing model of Carreras (2007), and Suzuki et al. (2009) also incorporate word cluster features proposed by Koo et al. (2008) in their system. We expect our approach may achie"
P14-1043,P11-2033,0,0.406452,"with high oracle score. Please note that the parse forest in Figure 1 contains four parse trees after combination of the two different choices. Second, the parser is able to learn useful features from the unambiguous parts of the parse forest. Finally, with sufficient unlabeled data, it is possible that the parser can learn to resolve such uncertainty by biasing to more reasonable parse trees. To construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser (Zhang and Nivre, 2011), and a generative constituent parser (Petrov and Klein, 2007). The 1-best parse trees of these three parsers are aggregated in different ways. Evaluation on labeled data shows the oracle accuracy of parse forest is much higher than that of 1-best outputs of single parsers (see Table 3). Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. Experimental results on both English and Chinese datasets demonstrate that the proposed ambiguity-aware ens"
P14-1043,P11-1156,0,0.0131227,"{zhli13,minzhang,wlchen}@suda.edu.cn Abstract of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-bes"
P14-1137,P07-1020,0,0.0342617,"l for rule selection in hierarchical phrasebased translation. Xiong and Zhang (2013) employ a sentence-level topic model to capture coherence for document-level machine translation. The difference between our work and these previous studies on topic model for SMT lies in that we adopt topic-based WSI to obtain word senses rather than generic topics and integrate induced word senses into machine translation. Lexical selection Our work is also related to lexical selection in SMT where appropriate target lexical items for source words are selected by a statistical model with context information (Bangalore et al., 2007; Mauser et al., 2009). The reformulated WSD discussed before can also be considered as a lexical selection model. The significant difference from these studies is that we perform lexical selection using automatically induced word senses by the HDP on the source side. 7 Conclusion We have presented a sense-based translation model that integrates word senses into machine translation. We capitalize on the broad-coverage word sense induction system that is built on the nonparametric Bayesian HDP to learn sense clusters for words in the source language. We generate pseudo documents for word tokens"
P14-1137,J96-1002,0,0.0724325,"xtract both the lexicon and sense features from a ±k-word window centered on the word c. The lexicon features are defined as the preceding k words, the succeeding k words and the word c itself: {c−k , ..., c−1 , c, c1 , ..., ck }. The sense features are defined as the predicted senses for these words: {sc−k , ..., sc−1 , sc , sc1 , ..., sck }. As we also use these neighboring words to predict word senses in the HDP-based WSI, the information provided by the lexicon and sense features may overlap. This is not a issue for the MaxEnt classifier as it can deal with arbitrary overlapping features (Berger et al., 1996). One may also wonder whether the sense features can contribute to SMT new information that can NOT be obtained from the lexicon features. First, we believe that the senses induced by the HDP-based WSI provide a different view of data than that of the lexicon features. Second, the sense features contain semantic distributional information learned by the HDP across contexts where lexical words occur. Third, we empirically investigate this doubt by comparing two MaxEnt-based translation models 1462 in Section 5. One model only uses the lexicon features while the other integrates both the lexicon"
P14-1137,W07-0717,0,0.0559068,"Missing"
P14-1137,E09-1013,0,0.559708,"word tokens occur. The biggest difference from word sense disambiguation lies in that WSI does not rely on a predefined sense inventory. Such a prespecified list of senses is normally assumed by WSD which predicts senses of word tokens using this given inventory. From this perspective, WSI can be treated as a clustering problem while WSD a classification one. Various clustering algorithms, such as k-means, have been previously used for WSI. Recently, we have also witnessed that WSI is cast as a topic modeling problem where the sense clusters of a word type are considered as underlying topics (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). We follow this line to tailor a topic modeling framework to induce word senses for our large-scale training data. In the topic-based WSI, surrounding context of a word token is considered as a pseudo document of the corresponding word type. A pseudo document is composed of either a bag of neighboring words of a word token, or the Part-to-Speech tags of neighboring words, or other contextual information elements. In this paper, we define a pseudo 1460 document as ±N neighboring words centered on a given word token. Table 1 shows examples of pseudo docum"
P14-1137,N03-1017,0,0.0180357,"ated WSD for SMT? 5.1 Setup Our baseline system is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (Xiong et al., 2006). We used LDC corpora LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07, LDC2004T07 as our bilingual training data which consists of 3.84M bilingual sentences, 109.5M English word tokens and 96.9M Chinese word tokens. We ran Giza++ on the training data in two directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. From the word-aligned data, we extracted weighted phrase pairs to generate our phrase table. We trained a 5-gram language model on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit (Stolcke, 2002) with the modified Kneser-Ney smoothing (Chen and Goodman, 1996). We trained our HDP-based WSI models via the C++ HDP toolkit3 (Wang and Blei, 2012). We set the hyperparameters γ = 0.1 and α0 = 1.0 following Lau et al. (2012).We extracted pseudo documents from a ±10-word window centered on the corresponding word token for each wor"
P14-1137,P05-1048,0,0.774466,"anslation task. Results show that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation. 1 Introduction One of very common phenomena in language is that a plenty of words have multiple meanings. In the context of machine translation, such different meanings normally produce different target translations. Therefore a natural assumption is that word sense disambiguation (WSD) may contribute to statistical machine translation (SMT) by providing appropriate word senses for target translation selection with context features (Carpuat and Wu, 2005). ∗ Corresponding author This assumption, however, has not been empirically verified in the early days. Carpuat and Wu (2005) adopt a standard formulation of WSD: predicting word senses that are defined on an ontology for ambiguous words. As they apply WSD to Chinese-to-English translation, they predict word senses from a Chinese ontology HowNet and project the predicted senses to English glosses provided by HowNet. These glosses, used as the sense predictions of their WSD system, are integrated into a word-based SMT system either to substitute for translation candidates of their translation m"
P14-1137,E12-1060,0,0.390364,"m word sense disambiguation lies in that WSI does not rely on a predefined sense inventory. Such a prespecified list of senses is normally assumed by WSD which predicts senses of word tokens using this given inventory. From this perspective, WSI can be treated as a clustering problem while WSD a classification one. Various clustering algorithms, such as k-means, have been previously used for WSI. Recently, we have also witnessed that WSI is cast as a topic modeling problem where the sense clusters of a word type are considered as underlying topics (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). We follow this line to tailor a topic modeling framework to induce word senses for our large-scale training data. In the topic-based WSI, surrounding context of a word token is considered as a pseudo document of the corresponding word type. A pseudo document is composed of either a bag of neighboring words of a word token, or the Part-to-Speech tags of neighboring words, or other contextual information elements. In this paper, we define a pseudo 1460 document as ±N neighboring words centered on a given word token. Table 1 shows examples of pseudo documents for a Chinese word “wǎngluò” (netwo"
P14-1137,D07-1007,0,0.816903,"didates of their translation model or to postedit the output of their SMT system. They report that WSD degenerates the translation quality of SMT. In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulate the task of WSD for SMT as predicting possible target translations rather than senses for ambiguous source words. They show that such a reformulated WSD can improve the accuracy of a simplified word translation task. Following this WSD reformulation for SMT, Chan et al. (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). Carpuat and Wu (2007) also use this reformulated WSD and further adapt it to multi-word phrasal disambiguation. They both report that the redefined WSD can significantly improve SMT. Although this reformulated WSD has proved helpful for SMT, one question is not answered yet: are pure word senses useful for SMT? The early WSD for SMT (Carpuat and Wu, 2005) uses projected word senses while the reformulated WSD sidesteps word senses. In this paper we would like to re-investigate this question by resorting to word sense induction (WSI) that is related to but different from WSD.1 We use 1 We will discuss the relation a"
P14-1137,D09-1022,0,0.148492,"Missing"
P14-1137,P07-1005,0,0.849062,"edictions of their WSD system, are integrated into a word-based SMT system either to substitute for translation candidates of their translation model or to postedit the output of their SMT system. They report that WSD degenerates the translation quality of SMT. In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulate the task of WSD for SMT as predicting possible target translations rather than senses for ambiguous source words. They show that such a reformulated WSD can improve the accuracy of a simplified word translation task. Following this WSD reformulation for SMT, Chan et al. (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). Carpuat and Wu (2007) also use this reformulated WSD and further adapt it to multi-word phrasal disambiguation. They both report that the redefined WSD can significantly improve SMT. Although this reformulated WSD has proved helpful for SMT, one question is not answered yet: are pure word senses useful for SMT? The early WSD for SMT (Carpuat and Wu, 2005) uses projected word senses while the reformulated WSD sidesteps word senses. In this paper we would like to re-investigate this question by resor"
P14-1137,P96-1041,0,0.142489,"DC2005T06, LDC2003E07, LDC2004T07 as our bilingual training data which consists of 3.84M bilingual sentences, 109.5M English word tokens and 96.9M Chinese word tokens. We ran Giza++ on the training data in two directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. From the word-aligned data, we extracted weighted phrase pairs to generate our phrase table. We trained a 5-gram language model on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit (Stolcke, 2002) with the modified Kneser-Ney smoothing (Chen and Goodman, 1996). We trained our HDP-based WSI models via the C++ HDP toolkit3 (Wang and Blei, 2012). We set the hyperparameters γ = 0.1 and α0 = 1.0 following Lau et al. (2012).We extracted pseudo documents from a ±10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). As described in Section 2.2, we preprocessed the source part of our bilingual training data by removing stop words and infrequent words that occurs less than 3 http://www.cs.cmu.edu/˜chongw/ resource.html # Word Types # Total Pseudo Documents # Avg Pseudo Documents # Total Senses # Avg Sen"
P14-1137,P05-1033,0,0.0277485,"translation candidates of their translation model or to postedit the output of their SMT system. They report that WSD degenerates the translation quality of SMT. In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulate the task of WSD for SMT as predicting possible target translations rather than senses for ambiguous source words. They show that such a reformulated WSD can improve the accuracy of a simplified word translation task. Following this WSD reformulation for SMT, Chan et al. (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). Carpuat and Wu (2007) also use this reformulated WSD and further adapt it to multi-word phrasal disambiguation. They both report that the redefined WSD can significantly improve SMT. Although this reformulated WSD has proved helpful for SMT, one question is not answered yet: are pure word senses useful for SMT? The early WSD for SMT (Carpuat and Wu, 2005) uses projected word senses while the reformulated WSD sidesteps word senses. In this paper we would like to re-investigate this question by resorting to word sense induction (WSI) that is related to but different from WSD.1 We use 1 We will"
P14-1137,P11-2031,0,0.00652729,"ta as described in Section 3.2. We set the Gaussian prior to 1 to avoid overfitting. On average, we obtained 346 classes (target translations) per source word type with the maximum number of classes being 256,243. It took an average of 57.5 seconds for training a Maxent classifier. We used the NIST MT03 evaluation test data as our development set, and the NIST MT05 as the test set. We evaluated translation quality with the case-insensitive BLEU-4 (Papineni et al., 2002) and NIST (Doddington, 2002). In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al. (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments. 5.2 Statistics and Examples of Word Senses Before we present our experiment results of the sense-based translation model, we study some statistics of the HDP-based WSI on the training and test data. We show these statistics in Table 2. There are 67,723 and 4,348 unique word types in the training and test data after the preprocessing step. For these word types, we extract 27.73M and 11,777 pseudo documents from the training and test set respectively. On average, there are 427.79 4 http://ho"
P14-1137,P02-1040,0,0.101344,"We performed 100 iterations of the L-BFGS algorithm implemented in the training toolkit on the collected training events from the sense-annotated data as described in Section 3.2. We set the Gaussian prior to 1 to avoid overfitting. On average, we obtained 346 classes (target translations) per source word type with the maximum number of classes being 256,243. It took an average of 57.5 seconds for training a Maxent classifier. We used the NIST MT03 evaluation test data as our development set, and the NIST MT05 as the test set. We evaluated translation quality with the case-insensitive BLEU-4 (Papineni et al., 2002) and NIST (Doddington, 2002). In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al. (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments. 5.2 Statistics and Examples of Word Senses Before we present our experiment results of the sense-based translation model, we study some statistics of the HDP-based WSI on the training and test data. We show these statistics in Table 2. There are 67,723 and 4,348 unique word types in the training and test data after the preprocessing step. For thes"
P14-1137,J97-3002,0,0.0186056,"n using large-scale bilingual training data. In order to build the proposed sense-based translation model, we annotated the source part of the bilingual training data with word senses induced by the HDPbased WSI. With the trained sense-based translation model, we would like to investigate the following two questions: • Do word senses automatically induced by the HDP-based WSI improve translation quality? • Does the sense-based translation model outperform the reformulated WSD for SMT? 5.1 Setup Our baseline system is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (Xiong et al., 2006). We used LDC corpora LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07, LDC2004T07 as our bilingual training data which consists of 3.84M bilingual sentences, 109.5M English word tokens and 96.9M Chinese word tokens. We ran Giza++ on the training data in two directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. From the word-aligned data, we extracted weighted phrase pairs to generate our phrase tabl"
P14-1137,P12-1079,1,0.588216,"ction and empirically show that the HDP-based WSI is better than the LDA-based WSI. We follow them to set the hyperparameters of HDP for training and incorporate automatically induced word senses into SMT in our work. Topic model for SMT Generic topic models are also explored for SMT. Zhao and Xing (2007) propose a bilingual topic model and integrate a topic-specific lexicon translation model into SMT. Tam et al. (2007) also explore a bilingual topic model for translation and language model adaptation. Foster and Kunh (2007) introduce a mixture model approach for translation model adaptation. Xiao et al. (2012) propose a topic-based similarity model for rule selection in hierarchical phrasebased translation. Xiong and Zhang (2013) employ a sentence-level topic model to capture coherence for document-level machine translation. The difference between our work and these previous studies on topic model for SMT lies in that we adopt topic-based WSI to obtain word senses rather than generic topics and integrate induced word senses into machine translation. Lexical selection Our work is also related to lexical selection in SMT where appropriate target lexical items for source words are selected by a statis"
P14-1137,P06-1066,1,0.659265,"n model, we annotated the source part of the bilingual training data with word senses induced by the HDPbased WSI. With the trained sense-based translation model, we would like to investigate the following two questions: • Do word senses automatically induced by the HDP-based WSI improve translation quality? • Does the sense-based translation model outperform the reformulated WSD for SMT? 5.1 Setup Our baseline system is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (Xiong et al., 2006). We used LDC corpora LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07, LDC2004T07 as our bilingual training data which consists of 3.84M bilingual sentences, 109.5M English word tokens and 96.9M Chinese word tokens. We ran Giza++ on the training data in two directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. From the word-aligned data, we extracted weighted phrase pairs to generate our phrase table. We trained a 5-gram language model on the Xinhua section of the English Gigaword corpus (306 million word"
P14-1137,W11-1102,0,0.106183,"Missing"
P14-1137,H05-1097,0,\N,Missing
P14-1137,P03-1021,0,\N,Missing
P15-1023,W11-1014,0,0.0265826,"Missing"
P15-1023,D07-1007,0,0.24854,"ish translation example to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish transl"
P15-1023,P07-1005,0,0.0775738,"Missing"
P15-1023,P11-2031,0,0.106229,"Missing"
P15-1023,P12-2023,0,0.214218,"r a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual Meeting of the Associati"
P15-1023,P06-1121,0,0.055962,"a translation system for lexical selection. Experiment results on NIST ChineseEnglish test sets demonstrate that 1) our model significantly outperforms previous lexical selection methods and 2) modeling correlations between local words and global topics can further improve translation quality. 1 Introduction Lexical selection is a very important task in statistical machine translation (SMT). Given a sentence in the source language, lexical selection statistically predicts translations for source words, based on various translation knowledge. Most conventional SMT systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007) exploit very limited context information contained in bilingual rules for lexical selection. ∗ Corresponding author. duì gāi wèntí zhōngguó bǎochí zhōnglì lìchǎng {problem, issue ...}wèntí {stance, attitude ...}lìchǎng [Economy topic, Politics topic ...] Figure 1: A Chinese-English translation example to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local"
P15-1023,D12-1010,1,0.890599,"Missing"
P15-1023,D08-1039,0,0.104532,"e to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presen"
P15-1023,E14-1035,0,0.15347,"anslations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conf"
P15-1023,W14-3358,0,0.182188,"anslations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conf"
P15-1023,C08-1041,0,0.0994588,"xts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if l"
P15-1023,N03-1017,0,0.0655323,"l is integrated into a translation system for lexical selection. Experiment results on NIST ChineseEnglish test sets demonstrate that 1) our model significantly outperforms previous lexical selection methods and 2) modeling correlations between local words and global topics can further improve translation quality. 1 Introduction Lexical selection is a very important task in statistical machine translation (SMT). Given a sentence in the source language, lexical selection statistically predicts translations for source words, based on various translation knowledge. Most conventional SMT systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007) exploit very limited context information contained in bilingual rules for lexical selection. ∗ Corresponding author. duì gāi wèntí zhōngguó bǎochí zhōnglì lìchǎng {problem, issue ...}wèntí {stance, attitude ...}lìchǎng [Economy topic, Politics topic ...] Figure 1: A Chinese-English translation example to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that"
P15-1023,W04-3250,0,0.175547,"baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the toolkit4 developed by Zhang (2004) to train the reordering model with the following parameters: iteration number iter=200 and Gaussian prior g=1.0. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002) metric. Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 2 http://people.sutd.edu.sg/∼yue zhang/doc/index.html http://nlp.stanford.edu/software 4 http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html 3 (6) 233 Model CATM (± 6w) CATM (± 8w) CATM (± 10w) CATM (± 12w) CATM (± 14w) MT05 33.35 33.43 33.42 33.49 33.30 Table 2: Experiment results on the development set using different window sizes ws . To train CATM, we set the topic number Nz as 25.5 For hyperparameters α and β, we empirically set α=50/Nz and β=0.1, as implemented in (Griffiths and Steyvers, 2004). Following Han et al. (2012), we se"
P15-1023,D08-1010,1,0.79717,"inspired by (Han and Sun, 2012), where an entity-topic model is presented for entity linking. We successfully adapt this work to lexical selection in SMT. The related work mainly includes the following two strands. (1) Lexical Selection in SMT. In order to explore rich context information for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Along this line, Shen et al. (2009) introduce four new linguistic and contextual features for translation selection in SMT. Recently, we have witnessed an increasing efforts in exploiting document-level context information to improve lexical selection. Xiao et al. (2011) impose a hard constraint to guarantee 235 Target-side Topical Items UNHCR republic refugee refugee Kosovo federal military missile military United States system war country development economy international economic trade Taiwan China relation cross-strait cross-strait relation issue Topic Source-side Contextual Words J¬(ref"
P15-1023,D09-1022,0,0.124816,"effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On t"
P15-1023,P02-1038,0,0.0449124,"topical items. Formally, for the position i in the document corresponding to the content word f , we collect the sampled count that translation e˜ generates f , denoted by Csam (˜ e, f ). This count can be normalized to form a new translation probability in the following way: Csam (˜ e, f ) + k p(˜ e|f ) = Csam + k · Ne˜,f where Csam is the total number of samples during inference and Ne˜,f is the number of candidate translations of f . Here we apply add-k smoothing to refine this translation probability, where k is a tunable global smoothing constant. Under the framework of log-linear model (Och and Ney, 2002), we use this translation probability as a new feature to improve lexical selection in SMT. 4 Experiments In order to examine the effectiveness of our model, we carried out several groups of experiments on Chinese-to-English translation. 4.1 Setup Our bilingual training corpus is from the FBIS corpus and the Hansards part of LDC2004T07 corpus (1M parallel sentences, 54.6K documents, with 25.2M Chinese words and 29M English words). We first used ZPar toolkit2 and Stanford toolkit3 to preprocess (i.e., word segmenting, PoS tagging) the Chinese and English parts of training corpus, and then word-"
P15-1023,J03-1002,0,0.00668871,"robability as a new feature to improve lexical selection in SMT. 4 Experiments In order to examine the effectiveness of our model, we carried out several groups of experiments on Chinese-to-English translation. 4.1 Setup Our bilingual training corpus is from the FBIS corpus and the Hansards part of LDC2004T07 corpus (1M parallel sentences, 54.6K documents, with 25.2M Chinese words and 29M English words). We first used ZPar toolkit2 and Stanford toolkit3 to preprocess (i.e., word segmenting, PoS tagging) the Chinese and English parts of training corpus, and then word-aligned them using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of MT05 as the development set, and the sets of MT06/MT08 as test sets. On average, these three sets contain 17.2, 13.9 and 14.1 content words per sentence, respectively. We trained a 5-gram language model on the Xinhua portion of Gigaword corpus using the SRILM Toolkit (Stolcke, 2002). Our baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the"
P15-1023,P03-1021,0,0.0641451,"Missing"
P15-1023,J04-4002,0,0.305645,"Missing"
P15-1023,D09-1008,0,0.126118,"pics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts sugges"
P15-1023,P12-1048,1,0.783622,"or missile and war, respectively; “ü” and “W” together means cross-starit. the document-level translation consistency. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Also relevant is the work of Xiong et al.(2013), who use three different models to capture lexical cohesion for document-level SMT. (2) SMT with Topic Models. In this strand, Zhao and Xing (2006, 2007) first present a bilingual topical admixture formalism for word alignment in SMT. Tam et al. (2007) and Ruiz et al. (2012) apply topic model into language model adaptation. Su et al. (2012) conduct translation model adaptation with monolingual topic information. Gong et al. (2010) and Xiao et al. (2012) introduce topic-based similarity models to improve SMT system. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Eidelman et al. (2012) incorporate topic-specific lexical weights into translation model. Hewavitharana et al. (2013) propose an incremental topic based translation model adaptation approach that satisfies the causality constraint imposed by spoken conversations. Hasl"
P15-1023,N12-1046,0,0.0474752,"Missing"
P15-1023,J97-3002,0,0.0509132,"ord segmenting, PoS tagging) the Chinese and English parts of training corpus, and then word-aligned them using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of MT05 as the development set, and the sets of MT06/MT08 as test sets. On average, these three sets contain 17.2, 13.9 and 14.1 content words per sentence, respectively. We trained a 5-gram language model on the Xinhua portion of Gigaword corpus using the SRILM Toolkit (Stolcke, 2002). Our baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the toolkit4 developed by Zhang (2004) to train the reordering model with the following parameters: iteration number iter=200 and Gaussian prior g=1.0. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002) metric. Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 2 http://people.sutd.edu.sg/∼yue zhang/do"
P15-1023,2011.mtsummit-papers.13,0,0.169815,"h black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated"
P15-1023,P12-1079,1,0.93568,"ation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual M"
P15-1023,P06-1066,1,0.778544,"using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of MT05 as the development set, and the sets of MT06/MT08 as test sets. On average, these three sets contain 17.2, 13.9 and 14.1 content words per sentence, respectively. We trained a 5-gram language model on the Xinhua portion of Gigaword corpus using the SRILM Toolkit (Stolcke, 2002). Our baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the toolkit4 developed by Zhang (2004) to train the reordering model with the following parameters: iteration number iter=200 and Gaussian prior g=1.0. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002) metric. Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 2 http://people.sutd.edu.sg/∼yue zhang/doc/index.html http://nlp.stanford.edu/software 4 http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html 3 (6) 233"
P15-1023,P14-1137,1,0.852105,"Missing"
P15-1023,P06-2124,0,0.0377211,"topical items and contextual words learned by CATM with Nz =25 and Ws =12. Chinese words that do not have direct English translations are denoted with ”*”. Here “q” and “|” are Chinese quantifiers for missile and war, respectively; “ü” and “W” together means cross-starit. the document-level translation consistency. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Also relevant is the work of Xiong et al.(2013), who use three different models to capture lexical cohesion for document-level SMT. (2) SMT with Topic Models. In this strand, Zhao and Xing (2006, 2007) first present a bilingual topical admixture formalism for word alignment in SMT. Tam et al. (2007) and Ruiz et al. (2012) apply topic model into language model adaptation. Su et al. (2012) conduct translation model adaptation with monolingual topic information. Gong et al. (2010) and Xiao et al. (2012) introduce topic-based similarity models to improve SMT system. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Eidelman et al. (2012) incorporate topic-specific lexical weights into t"
P15-1023,P02-1040,0,\N,Missing
P15-1023,W11-2133,0,\N,Missing
P15-1023,P13-2122,0,\N,Missing
P15-1023,J07-2003,0,\N,Missing
P15-1172,N09-2054,0,0.0755111,"Missing"
P15-1172,P09-1059,0,0.320863,"aseline model. Particularly, when M ′ = 1K, the model converges very slowly. However, from the trend of the curves, we expect that the accuracy gap between our coupled model with M ′ = 5K/20K and the baseline model should be much smaller when reaching convergence. Based on the above observation, we adopt N ′ = 5K and M ′ = 5K in the following experiments. Moreover, we select the best iteration on the development data, and use the corresponding model to parse the test data. 5.2 Final Results Table 3 shows the final results on the CTB test data. We re-implement the guide-feature based method of Jiang et al. (2009), referred to as twostage CRF. Li et al. (2012) jointly models Chinese POS tagging and dependency parsing, and report the best tagging accuracy on CTB. The results show that our coupled model outperforms the baseline model by large margin, and also achieves slightly higher accuracy than the guide-feature based method. 5.3 Accuracy 94.10 94.81 (+0.71) † 95.00 (+0.90) †‡ 94.60 Table 3: Final results on CTB test data. † means the corresponding approach significantly outperforms the baseline at confidence level of p < 10−5 ; whereas ‡ means the accuracy difference between the two-stage CRF and the"
P15-1172,P13-1075,0,0.0367226,"Missing"
P15-1172,C12-1103,1,0.827616,"Missing"
P15-1172,P14-1043,1,0.750353,"this work, our coupled CRF jointly models two same tasks which have different annotation schemes. Moreover, this work provides a natural way to 1790 learn from incomplete annotations where one sentence only contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). Acknowledgments The authors would like to thank the undergraduate students Fangli Lu and Xiaojing Wang for building our annotation system, and Le Lu, Die Hu, Yue Zhang, Jian Zhang, Qiuyi Yan, Xinzhou Jiang for data annotation. We are also grateful that Yu Ding kindly shared her earlier codes on which our annotation system was built. We also thank the helpful comm"
P15-1172,C14-1075,1,0.842938,"this work, our coupled CRF jointly models two same tasks which have different annotation schemes. Moreover, this work provides a natural way to 1790 learn from incomplete annotations where one sentence only contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). Acknowledgments The authors would like to thank the undergraduate students Fangli Lu and Xiaojing Wang for building our annotation system, and Le Lu, Die Hu, Yue Zhang, Jian Zhang, Qiuyi Yan, Xinzhou Jiang for data annotation. We are also grateful that Yu Ding kindly shared her earlier codes on which our annotation system was built. We also thank the helpful comm"
P15-1172,D14-1093,0,0.0197553,"contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). Acknowledgments The authors would like to thank the undergraduate students Fangli Lu and Xiaojing Wang for building our annotation system, and Le Lu, Die Hu, Yue Zhang, Jian Zhang, Qiuyi Yan, Xinzhou Jiang for data annotation. We are also grateful that Yu Ding kindly shared her earlier codes on which our annotation system was built. We also thank the helpful comments from our anonymous reviewers. This work was supported by National Natural Science Foundation of China (Grant No. 61432013, 61203314) and Jiangsu Planned Projects for Postdoctoral Research Funds (No. 14010"
P15-1172,P08-1108,0,0.285245,"Missing"
P15-1172,C14-1145,0,0.0497809,"Missing"
P15-1172,C12-2093,0,0.0181146,"otations. They adopt an approximate decoding algorithm which tries to find the best single-side tag sequence with reference to tags at the other side. In contrast, our approach is a direct extension of traditional CRF, and is more theoretically simple from the perspective of modelling. The use of both joint and separate features is proven to be crucial for the success of our coupled model. In addition, their work indicates that their model relies on a hand-crafted loose mapping between annotations, which is opposite to our findings. The naming of the “coupled” CRF is borrowed from the work of Qiu et al. (2012), which treats the joint task of Chinese word segmentation and POS tagging as two coupled sequence labeling problems. Zhang et al. (2014) propose a shift-reduce dependency parsing model which can simultaneously learn and produce two heterogeneous parse trees. However, their approach assumes the existence of data with annotations at both sides, which is obtained by converting phrase-structure trees into dependency trees with different heuristic rules. This work is also closely related with multitask learning, which aims to jointly learn multiple related tasks with the benefit of using interacti"
P15-1172,D13-1062,0,0.545921,"ed tag set. In practice, we usually only need results following one annotation style. Therefore, we employ our coupled model to convert PD into the style of CTB, and train our baseline model with two training data with homogeneous annotations. Again, Algorithm 1 is used to merge the two data with N ′ = 5K and M ′ = 5K. The results are shown in the bottom row in Table 6. We can see that with the extra converted data, the baseline model can achieve slightly lower accuracy with the coupled model and avoid the inefficiency problem at the meantime. 6 Related Work This work is partially inspired by Qiu et al. (2013), who propose a model that performs heterogeneous Chinese word segmentation and POS tagging and produces two sets of results following CTB and PD styles respectively. Different from our CRFbased coupled model, their approach adopts a linear model, which directly combines two separate sets of features based on single-side tags, without considering the interacting joint features between the two annotations. They adopt an approximate decoding algorithm which tries to find the best single-side tag sequence with reference to tags at the other side. In contrast, our approach is a direct extension of"
P15-1172,N13-1126,0,0.0283562,"Missing"
P15-1172,D14-1010,0,0.0482597,"labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). Acknowledgments The authors would like to thank the undergraduate students Fangli Lu and Xiaojing Wang for building our annotation system, and Le Lu, Die Hu, Yue Zhang, Jian Zhang, Qiuyi Yan, Xinzhou Jiang for data annotation. We are also grateful that Yu Ding kindly shared her earlier codes on which our annotation system was built. We also thank the helpful comments from our anonymous reviewers. This work was supported by National Natural Science Foundation of China (Grant No. 61432013, 61203314) and Jiangsu Planned Projects for Postdoctoral Research Funds (No. 1401075B), and was also parti"
P15-1172,P08-1101,0,0.0356683,"are based on single-side tags. The advantages of our coupled model over the traditional model are to provide us with the flexibility of using both kinds of features, which significantly contributes to the accuracy improvement as shown in the following experiments. 3.1 Mapping Functions defines the probability of a tag sequence as: exp(Score(x, t; θ)) P (t|x; θ) = P ′ t′ exp(Score(x, t ; θ)) X (1) Score(x, t; θ) = θ · f (x, i, ti−1 , ti ) 1≤i≤n where f (x, i, ti−1 , ti ) is the feature vector at the ith word and θ is the weight vector. We adopt the state-of-the-art tagging features in Table 1 (Zhang and Clark, 2008). 3 Coupled POS Tagging (TaggerCTB&PD ) In this section, we introduce our coupled model, which is able to learn and predict two heterogeneous annotations simultaneously. The idea is to bundle two sets of POS tags together and let the CRF-based model work in the enlarged tag space. For example, a CTB tag “NN” and a PD tag “n” would be bundled into “[NN,n]”. Figure 2 shows the graphical structure of our model. Different from the traditional model in Eq. (1), our coupled model defines the score of a bundled tag sequence as follows: Score(x, [ta , tb ]; θ) =   f (x, i, [tai−1 , tbi−1 ], [tai , t"
P15-1172,C14-1051,0,0.143791,"the other side. In contrast, our approach is a direct extension of traditional CRF, and is more theoretically simple from the perspective of modelling. The use of both joint and separate features is proven to be crucial for the success of our coupled model. In addition, their work indicates that their model relies on a hand-crafted loose mapping between annotations, which is opposite to our findings. The naming of the “coupled” CRF is borrowed from the work of Qiu et al. (2012), which treats the joint task of Chinese word segmentation and POS tagging as two coupled sequence labeling problems. Zhang et al. (2014) propose a shift-reduce dependency parsing model which can simultaneously learn and produce two heterogeneous parse trees. However, their approach assumes the existence of data with annotations at both sides, which is obtained by converting phrase-structure trees into dependency trees with different heuristic rules. This work is also closely related with multitask learning, which aims to jointly learn multiple related tasks with the benefit of using interactive features under a share representation (BenDavid and Schuller, 2003; Ando and Zhang, 2005; Parameswaran and Weinberger, 2010). However,"
P15-1172,P02-1035,0,0.0333588,"ling tasks, such as POS tagging and chunking. In this work, our coupled CRF jointly models two same tasks which have different annotation schemes. Moreover, this work provides a natural way to 1790 learn from incomplete annotations where one sentence only contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). Acknowledgments The authors would like to thank the undergraduate students Fangli Lu and Xiaojing Wang for building our annotation system, and Le Lu, Die Hu, Yue Zhang, Jian Zhang, Qiuyi Yan, Xinzhou Jiang for data annotation. We are also grateful that Yu Ding kindly shared her earlier codes on which our annotation syste"
P15-1172,P11-2009,0,0.0609344,"Missing"
P15-1172,P12-1026,0,0.0513514,"Missing"
P15-1172,P12-1025,0,0.592973,"Missing"
P16-1033,D14-1108,0,0.0232974,"nnotated sentence, where only the heads of “saw” and “with” are decided. upsurge of web data (e.g., tweets, blogs, and product comments) imposes great challenges to existing parsing techniques. Meanwhile, previous research on out-of-domain dependency parsing gains little success (Dredze et al., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial"
P16-1033,C10-1011,0,0.131693,"Missing"
P16-1033,W09-2307,0,0.0617144,"Missing"
P16-1033,P10-1001,0,0.0312588,"lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based pa"
P16-1033,D07-1015,0,0.0416854,"Missing"
P16-1033,N06-1019,0,0.0701527,"Missing"
P16-1033,W08-1301,0,0.158099,"Missing"
P16-1033,C12-2067,0,0.0659251,"Missing"
P16-1033,C14-1075,1,0.946592,"certain metric for AL for sequence labeling problems. In the case of dependency parsing, the marginal probability of a dependency is the sum of probabilities of all legal trees that contain the dependency. ∑ p(h ↷ m|x; w) = p(d|x; w) (4) d∈Y(x):h↷m∈d Intuitively, marginal probability is a more principled metric for measuring reliability of a dependency since it considers all legal parses in the search space, compared to previous methods based on scores of local classifiers (Sassano and Kurohashi, 2010; Flannery and Mori, 2015) or votes of n-best parses (Mirroshandel and Nasr, 2011). Moreover, Li et al. (2014) find strong correlation between marginal probability and correctness of a dependency in cross-lingual syntax projection. Score(x, d∗ ) (5) n1.5 Normalized tree probability. The CRF-based parser allows us, for the first time in AL for dependency parsing, to directly use tree probabilities for uncertainty measurement. Unlike previous approximate methods based on k-best parses (Mirroshandel and Nasr, 2011), tree probabilities globally consider all parse trees in the search space, and thus are intuitively more consistent and proper for measuring the reliability of a tree. Our initial assumption i"
P16-1033,U12-1005,0,0.0219821,"2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They f"
P16-1033,P14-1126,0,0.0479024,"Missing"
P16-1033,W15-2202,0,0.24441,"notation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting"
P16-1033,I11-1087,0,0.0377813,"Missing"
P16-1033,W13-5711,0,0.102764,"arsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences pro"
P16-1033,D14-1097,0,0.0830651,"Missing"
P16-1033,I11-1100,0,0.0281049,"Missing"
P16-1033,P15-1119,1,0.88885,"Missing"
P16-1033,E06-1011,0,0.0810264,"4th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based parsing model to AL for dependency parsing. We adopt the second-order graphbased model of McDonald and Pereira (2006), which casts the problem as finding an optimal tree from a fully-connect directed graph and factors the score of a dependency tree into scores of pairs of sibling dependencies. (1) This is the first work that applies a stateof-the-art probabilistic parsing model to AL for dependency parsing. The CRF-based dependency parser on the one hand allows us to use probabilities of trees or marginal probabilities of single dependencies for uncertainty measurement, and on the other hand can directly learn parameters from partially annotated trees. Using probabilistic models may be ubiquitous in AL for r"
P16-1033,P99-1010,0,0.894454,"Missing"
P16-1033,W07-2216,0,0.0829738,"Missing"
P16-1033,J04-3001,0,0.119634,"d McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and"
P16-1033,N12-1053,0,0.0241059,"omain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing perf"
P16-1033,C08-1113,0,0.0347094,"Missing"
P16-1033,P15-1134,0,0.075254,"Missing"
P16-1033,D14-1122,0,0.0228308,"where only the heads of “saw” and “with” are decided. upsurge of web data (e.g., tweets, blogs, and product comments) imposes great challenges to existing parsing techniques. Meanwhile, previous research on out-of-domain dependency parsing gains little success (Dredze et al., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which"
P16-1033,W11-2917,0,0.448477,"es full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units r"
P16-1033,P11-2033,1,0.841112,"ng trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based parsing model to AL for de"
P16-1033,P92-1017,0,0.8556,"Missing"
P16-1033,D15-1039,0,0.0453421,"Missing"
P16-1033,P02-1035,0,0.254628,"e, and may be asked to annotate another selected word in the same sentence in next AL iteration. Obviously, frequently switching sentences incurs great waste of cognitive effort, 3.4 Learning from PA A major challenge for AL with PA is how to learn from partially labeled sentences, as depicted in Figure 1. Li et al. (2014) show that a probabilistic CRF-based parser can naturally and effectively learn from PA. The basic idea is converting a partial tree into a forest as shown in Figure 2, 347 and using the forest as the gold-standard reference during training, also known as ambiguous labeling (Riezler et al., 2002; T¨ackstr¨om et al., 2013). For each remaining word without head, we add all dependencies linking to it as long as the new dependency does not violate the existing dependencies. We denote the resulting forest as Fj, whose probability is naturally the sum of probabilities of each tree d in F. ∑ p(F|x; w) = p(d|x; w) d∈F ∑ eScore(x,d;w) = ∑ d∈F Score(x,d′ ;w) d′ ∈Y(x) e Train Chinese English ∑N i=1 log p(Fi |xi ; w) #Sentences 14,304 803 1,910 #Tokens 318,408 20,454 50,319 #Sentences 39,115 1,700 2,416 #Tokens 908,154 40,117 56,684 are selected and annotated at each iteration. In the case of si"
P16-1033,P10-1037,0,0.419473,"09). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Lin"
P16-1033,D08-1112,0,0.139903,"Missing"
P16-1033,D07-1014,0,0.220985,"Missing"
P16-1033,N13-1126,0,0.0792451,"Missing"
P16-1033,P02-1016,0,0.0777853,"l., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; F"
P17-1064,J07-2003,0,0.0490256,"Figure 5 suggest that the Mixed RNN encoder is the simplest. Moreover, comparing to conventional NMT encoders, the difference lies only in the length of the input sequence. Statistics on our training data reveal that the Mixed RNN encoder approximately triples the input sequence length compared to conventional NMT encoders. 4 Experimentation We have presented our approaches to incorporating the source syntax into NMT encoders. In this section, we evaluate their effectiveness on Chinese-to-English translation. 4.1 • cdec (Dyer et al., 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.7 Experimental Settings Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively.4 We choose NIST MT 06 dataset (1664 sentence pairs) as our development set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respectively) as our test sets.5 To get the source syntax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser 6"
P17-1064,W14-4012,0,0.0168517,"Missing"
P17-1064,D14-1179,0,0.0247785,"Missing"
P17-1064,D16-1257,0,0.0120836,"any two different words. However, considering the lack of efficient way to directly model structural information, an alternative way is to linearize the phrase parse tree into a sequence of structural labels and learn the structural context through the sequence. For example, Figure 3(c) shows the structural label sequence of Figure 3(b) in a simple way following a depth-first traversal order. Note that linearizing a parse tree in a depth-first traversal order into a sequence of structural labels has also been widely adopted in recent advances in neural syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), suggesting that the linearized sequence can be viewed as an alternative to its tree structure.2 There is no doubt that the structural label sequence is much longer than its word sequence. In order to obtain the structural label annotation vector for wi in word sequence, we simply look for wi ’s part-of-speech (POS) tag in the label sequence and view the tag’s annotation vector as wi ’s label annotation vector. This is because wi ’s POS tag location can also represent wi ’s location in the parse tree. For example, in Figure 3, word w1 in (a) maps to l3 in (c) since l3 is the POS tag of w1 . L"
P17-1064,P10-4002,0,0.0137269,"ur method with two state-of-theart models of SMT and NMT: • Figure 4 and Figure 5 suggest that the Mixed RNN encoder is the simplest. Moreover, comparing to conventional NMT encoders, the difference lies only in the length of the input sequence. Statistics on our training data reveal that the Mixed RNN encoder approximately triples the input sequence length compared to conventional NMT encoders. 4 Experimentation We have presented our approaches to incorporating the source syntax into NMT encoders. In this section, we evaluate their effectiveness on Chinese-to-English translation. 4.1 • cdec (Dyer et al., 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.7 Experimental Settings Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively.4 We choose NIST MT 06 dataset (1664 sentence pairs) as our development set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respectively) as our test sets.5 To get the source syntax for sentences on th"
P17-1064,P16-1078,0,0.160511,"Missing"
P17-1064,P16-1162,0,0.0130766,"Missing"
P17-1064,W15-3014,0,0.0140583,"n-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT exp"
P17-1064,P08-1066,0,0.0134567,"improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008; Introduction Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) ∗ VV Work done at Huawei Noah’s Ark Lab, HongKong. 688 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1064 Shen et al., 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT effectively, we find that translations of NMT sometimes fail to well respect source syntax. Figure 1 (a) shows a Chinese-to-English translation example of NMT. In this example, the NMT seq2seq model incorrectly translates the Chinese noun phrase (i.e., 新 生/xinsheng 银 行/yinhang) into a discontinuous phrase in English (i.e., new ... bank) due to the failure of capturing the internal syntactic structure in the input Chinese sentence. Statistics on our development set show that one forth of Chinese noun phrases are translate"
P17-1064,W04-3250,0,0.104681,"Missing"
P17-1064,D16-1159,0,0.168192,"e syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translati"
P17-1064,N13-1060,1,0.934388,"Missing"
P17-1064,Q17-1007,1,0.0517642,"Missing"
P17-1064,P06-1077,0,0.0689507,"training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy. In principle, syntax is a promising avenue for translation modeling. This has been verified by tremendous encouraging studies on syntaxbased SMT that substantially improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008; Introduction Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) ∗ VV Work done at Huawei Noah’s Ark Lab, HongKong. 688 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1064 Shen et al., 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT"
P17-1064,P16-1008,1,0.860335,"we group sentences of similar lengths together and compute BLEU scores. Figure 6 presents the BLEU scores over different lengths of input sentences. It shows that Mixed RNN system outperforms RNNSearch over sentences with all different lengths. It also shows that the performance drops substantially 693 System RNNSearch Mixed RNN AER 50.1 47.9 System RNNSearch Table 2: Evaluation of alignment quality. The lower the score, the better the alignment quality. when the length of input sentences increases. This performance trend over the length is consistent with the findings in (Cho et al., 2014a; Tu et al., 2016, 2017a). We also observe that the NMT systems perform surprisingly bad on sentences over 50 in length, especially compared to the performance of SMT system (i.e., cdec). We think that the bad behavior of NMT systems towards long sentences (e.g., length of 50) is due to the following two reasons: (1) the maximum source sentence length limit is set as 50 in training, 9 making the learned models not ready to translate sentences over the maximum length limit; (2) NMT systems tend to stop early for long input sentences. 5.2 Mixed RNN Cont. 57.3 59.8 47.3 54.0 58.1 63.3 63.1 54.5 56.2 60.4 Dis. 33."
P17-1064,2015.iwslt-evaluation.11,0,0.0637674,"ves is provided to reveal how source syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syn"
P17-1064,D15-1166,0,0.0233336,"vided to reveal how source syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syn"
P17-1064,P08-1114,0,0.0438962,"m to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy. In principle, syntax is a promising avenue for translation modeling. This has been verified by tremendous encouraging studies on syntaxbased SMT that substantially improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008; Introduction Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) ∗ VV Work done at Huawei Noah’s Ark Lab, HongKong. 688 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1064 Shen et al., 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT effectively, we find tha"
P17-1064,D16-1249,0,0.0283303,"Missing"
P17-1064,J03-1002,0,0.00476306,"Word Alignment Due to the capability of carrying syntactic information in source annotation vectors, we conjecture that our model with source syntax is also beneficial for alignment. To test this hypothesis, we carry out experiments of the word alignment task on the evaluation dataset from Liu and Sun (2015), which contains 900 manually aligned Chinese-English sentence pairs. We force the decoder to output reference translations, as to get automatic alignments between input sentences and their reference translations. To evaluate alignment performance, we report the alignment error rate (AER) (Och and Ney, 2003) in Table 2. Table 2 shows that source syntax information improves the attention model as expected by maintaining an annotation vector summarizing structural information on each source word. 5.3 XP PP NP CP QP ALL PP NP CP QP ALL 5.4 Analysis on Over Translation To estimate the over translation generated by NMT, we propose ratio of over translation (ROT): Analysis on Phrase Alignment ROT = The above subsection examines the alignment performance at the word level. In this subsection, we turn to phrase alignment analysis by moving from word unit to phrase unit. Given a source phrase XP, we use w"
P17-1064,P02-1040,0,0.13215,"Missing"
P17-1064,N07-1051,0,0.0913188,"Missing"
P17-1064,W16-2209,0,0.284718,"tion, over translation usually happens along with the disrespect of syntax which results in the repeated translation of the same source words in multiple positions of the target sentence. In this paper we are not aiming at solving any particular issue, either the discontinuous translation or the over translation. Alternatively, we address how to incorporate explicitly the source syntax to improve the NMT translation accuracy with the expectation of alleviating the issues above in general. Specifically, rather than directly assigning each source word with manually designed syntactic labels, as Sennrich and Haddow (2016) do, we linearize a phrase parse tree into a structural label sequence and let the model automatically learn useful syntactic information. On the basis, we systematically propose and compare several different approaches to incorporating the label sequence into the seq2seq NMT model. Experimentation on Chinese-to-English translation demonstrates that all proposed approaches are able to improve the translation accuracy. 2 h h1 h1 yi hm h hm si-1 h1 h1 x1 x2 ….. xm (a) encoder Atten ci MLP si RNN yi-1 (b) decoder Figure 2: Attention-based NMT model. mulated using a pair of neural networks, i.e.,"
P18-1252,P16-1231,0,0.0506344,"Missing"
P18-1252,D12-1133,0,0.0569982,"Missing"
P18-1252,P12-1071,1,0.896274,"or boosting parsing performance. Though under different linguistic theories or annotation guidelines, the treebanks are painstakingly developed to capture the syntactic structures of the same language, thereby having a great deal of common grounds. Previous researchers have proposed two approaches for multi-treebank exploitation. On the one hand, the guiding-feature method projects the knowledge of the source-side treebank into the target-side treebank, and utilizes extra pattern-based features as guidance for the target-side parsing, mainly for the traditional discrete-feature based parsing (Li et al., 2012). On the other hand, the multi-task learning method simultaneously trains two parsers on two treebanks and uses shared neural network parameters for representing common-ground syntactic knowledge (Guo et al., 2016).2 Regardless of their effectiveness, while the guiding-feature method fails to directly use the source-side treebank as extra training data, the multi-task learning method is incapable of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches. Compared with the indirect approaches, treeb"
P18-1252,D14-1082,0,0.0998588,"we propose two simple yet effective treebank conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multiple treebank exploitation and leads to significantly higher parsing accuracy. 1 Introduction During the past few years, neural network based dependency parsing has achieved significant progress and outperformed the traditional discrete-feature based parsing (Chen and Manning, 2014; Dyer et al., 2015; Zhou ∗ #Tok Grammar 0.36M Case grammar 1.62M Phrase structure 1.00M Phrase structure 0.90M Phrase structure 0.90M Dependency structure 1.40M Dependency structure The first two (student) authors make equal contributions to this work. Zhenghua is the correspondence author. et al., 2015; Andor et al., 2016). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep biaffine parser that further advances the state-of-the-art accuracy by large margin. As reported, their parser outperforms the state-of-the-art discrete-feature based parser of Bohnet and Nivre"
P18-1252,P16-1033,1,0.654377,"iSeqLSTM at wk , denoted as hseq k , is fed into two separate MLPs to get two lowerdimensional representation vectors. ( seq ) H rH hk k = MLP (1) ( seq ) D D rk = MLP hk where rH k is the representation vector of wk as a head word, and rD k as a dependent. Finally, the score of the dependency i ← j is computed via a biaffine operation. [ score(i ← j) = rD i 1 ]T Wb rH j (2) During training, the original biaffine parser uses the local softmax loss. For each wi and its head wj , its loss is defined as score(i←j) − log ∑e escore(i←k) . Since our training data is k partially annotated, we follow Li et al. (2016) and employ the global CRF loss (Ma and Hovy, 2017) for better utilization of the data, leading to consistent accuracy gain. Multi-task learning aims to incorporate labeled data of multiple related tasks for improving performance (Collobert and Weston, 2008). Guo et al. (2016) apply multi-task learning to multi-treebank exploitation based on the neural transition-based parser of Dyer et al. (2015), and achieve higher improvement than the guiding-feature approach of Li et al. (2012). Based on the state-of-the-art biaffine parser, this work makes a straightforward extension to realize multi-task"
P18-1252,P07-1033,0,0.462705,"Missing"
P18-1252,P15-1033,0,0.104829,"t effective treebank conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multiple treebank exploitation and leads to significantly higher parsing accuracy. 1 Introduction During the past few years, neural network based dependency parsing has achieved significant progress and outperformed the traditional discrete-feature based parsing (Chen and Manning, 2014; Dyer et al., 2015; Zhou ∗ #Tok Grammar 0.36M Case grammar 1.62M Phrase structure 1.00M Phrase structure 0.90M Phrase structure 0.90M Dependency structure 1.40M Dependency structure The first two (student) authors make equal contributions to this work. Zhenghua is the correspondence author. et al., 2015; Andor et al., 2016). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep biaffine parser that further advances the state-of-the-art accuracy by large margin. As reported, their parser outperforms the state-of-the-art discrete-feature based parser of Bohnet and Nivre (2012) by 0.97 (93."
P18-1252,C16-1002,0,0.486022,"ing a great deal of common grounds. Previous researchers have proposed two approaches for multi-treebank exploitation. On the one hand, the guiding-feature method projects the knowledge of the source-side treebank into the target-side treebank, and utilizes extra pattern-based features as guidance for the target-side parsing, mainly for the traditional discrete-feature based parsing (Li et al., 2012). On the other hand, the multi-task learning method simultaneously trains two parsers on two treebanks and uses shared neural network parameters for representing common-ground syntactic knowledge (Guo et al., 2016).2 Regardless of their effectiveness, while the guiding-feature method fails to directly use the source-side treebank as extra training data, the multi-task learning method is incapable of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches. Compared with the indirect approaches, treebank conversion aims to directly convert a source-side treebank into the target-side guideline, and uses the converted treebank as extra labeled data for training the targetside model. Taking the example in Figure 1"
P18-1252,N13-1013,0,0.0187158,"of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches. Compared with the indirect approaches, treebank conversion aims to directly convert a source-side treebank into the target-side guideline, and uses the converted treebank as extra labeled data for training the targetside model. Taking the example in Figure 1, the goal of this work is to convert the under tree that follows the HIT-CDT guideline (Che et al., 2012) into the upper one that follows our new guideline. However, due to the lack 2 Johansson (2013) applies the feature-sharing approach of Daumé III (2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion"
P18-1252,P13-2105,0,0.0200075,"treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion (Niu et al., 2009) or pseudo bi-tree aligned data (Zhu et al., 2011; Li et al., 2013), making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion. The key motivation is to better utilize a largescale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions. (1) We have manually annotated a highquality bi-tree aligned data containing over ten thousand sentences, by reannotating the HIT-CDT treebank according to a new guideline. (2) We propose a pattern embedding conversion approach by retrofitting the indirect guiding-feature method of Li et al. (2012) to th"
P18-1252,I17-1007,0,0.0141555,"two separate MLPs to get two lowerdimensional representation vectors. ( seq ) H rH hk k = MLP (1) ( seq ) D D rk = MLP hk where rH k is the representation vector of wk as a head word, and rD k as a dependent. Finally, the score of the dependency i ← j is computed via a biaffine operation. [ score(i ← j) = rD i 1 ]T Wb rH j (2) During training, the original biaffine parser uses the local softmax loss. For each wi and its head wj , its loss is defined as score(i←j) − log ∑e escore(i←k) . Since our training data is k partially annotated, we follow Li et al. (2016) and employ the global CRF loss (Ma and Hovy, 2017) for better utilization of the data, leading to consistent accuracy gain. Multi-task learning aims to incorporate labeled data of multiple related tasks for improving performance (Collobert and Weston, 2008). Guo et al. (2016) apply multi-task learning to multi-treebank exploitation based on the neural transition-based parser of Dyer et al. (2015), and achieve higher improvement than the guiding-feature approach of Li et al. (2012). Based on the state-of-the-art biaffine parser, this work makes a straightforward extension to realize multi-task learning. We treat the source-side and target-side"
P18-1252,P16-1105,0,0.141086,"is H unchanged. The extended rD i,i←j and rj,i←j are fed into the biaffine layer to compute a more reliable score of the dependency i ← j, with the help of the guidance of dsrc . 4.2 The TreeLSTM Approach Compared with the pattern embedding approach, our second conversion approach employs treeLSTM to obtain a deeper representation of i ← j in the source-side tree dsrc . Tai et al. (2015) first propose treeLSTM as a generalization of seqLSTM for encoding treestructured inputs, and show that treeLSTM is more effective than seqLSTM on the semantic relatedness and sentiment classification tasks. Miwa and Bansal (2016) compare three treeLSTM variants on the relation extraction task and show that the SP-tree (shortest path) treeLSTM is superior to the full-tree and subtree treeLSTMs. In this work, we employ the SP-tree treeLSTM of Miwa and Bansal (2016) for our treebank conversion task. Our preliminary experiments also show the SP-tree treeLSTM outperforms the full-tree treeLSTM, which is consistent with Miwa and Bansal. We did not implement the in-between subtree treeLSTM. 2710 score(i ← j) consistent: i ← j h↑a wa grand: i ← k ← j sibling: i ← k → j reverse: i → j Biaffine wi h↓i reverse grand: i → k → j e"
P18-1252,P09-1006,0,0.036991,"pplies the feature-sharing approach of Daumé III (2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion (Niu et al., 2009) or pseudo bi-tree aligned data (Zhu et al., 2011; Li et al., 2013), making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion. The key motivation is to better utilize a largescale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions. (1) We have manually annotated a highquality bi-tree aligned data containing over ten thousand sentences, by reannotating the HIT-CDT treebank according to a new guideline. (2) We propose a pattern embedding conversion approach by retrofi"
P18-1252,D14-1162,0,0.079859,"Missing"
P18-1252,C14-1026,0,0.0706607,"Missing"
P18-1252,P15-1150,0,0.0608474,"←j rpat ⊕ eli ⊕ elj ⊕ ela i←j = e (3) Through rpat i←j , the extended word representaH tions, i.e., rD i,i←j and rj,i←j , now contain the structural information of wi and wj in dsrc . The remaining parts of the biaffine parser is H unchanged. The extended rD i,i←j and rj,i←j are fed into the biaffine layer to compute a more reliable score of the dependency i ← j, with the help of the guidance of dsrc . 4.2 The TreeLSTM Approach Compared with the pattern embedding approach, our second conversion approach employs treeLSTM to obtain a deeper representation of i ← j in the source-side tree dsrc . Tai et al. (2015) first propose treeLSTM as a generalization of seqLSTM for encoding treestructured inputs, and show that treeLSTM is more effective than seqLSTM on the semantic relatedness and sentiment classification tasks. Miwa and Bansal (2016) compare three treeLSTM variants on the relation extraction task and show that the SP-tree (shortest path) treeLSTM is superior to the full-tree and subtree treeLSTMs. In this work, we employ the SP-tree treeLSTM of Miwa and Bansal (2016) for our treebank conversion task. Our preliminary experiments also show the SP-tree treeLSTM outperforms the full-tree treeLSTM, w"
P18-1252,telljohann-etal-2004-tuba,0,0.122222,"Missing"
P18-1252,L16-1034,0,0.0229828,"Missing"
P18-1252,P15-1117,0,0.081965,"Missing"
P18-1252,P11-2126,0,0.0209418,"2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion (Niu et al., 2009) or pseudo bi-tree aligned data (Zhu et al., 2011; Li et al., 2013), making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion. The key motivation is to better utilize a largescale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions. (1) We have manually annotated a highquality bi-tree aligned data containing over ten thousand sentences, by reannotating the HIT-CDT treebank according to a new guideline. (2) We propose a pattern embedding conversion approach by retrofitting the indirect guiding-feature method of Li e"
P19-1229,P16-1231,0,0.0859338,"Missing"
P19-1229,K18-2005,0,0.0589861,"1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the major challenge for applying syntactic analysis in realistic NLP systems. To meet this challenge, the community has organized several shared tasks to attract more research attention (Nivre et al., 2007; Hajiˇc et al., 2009; Petrov and McDonald, 2012). 2386 Proceedings of the 57th Annual Meeting of"
P19-1229,D14-1082,0,0.145105,"ency from the head wh to the modifier wm ∗ ł this Introduction Corresponding author The two domain-specific datasets, plus another one for product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the"
P19-1229,D18-1217,0,0.105638,"Missing"
P19-1229,P07-1033,0,0.607207,"Missing"
P19-1229,N19-1423,0,0.066859,"Missing"
P19-1229,P15-1033,0,0.0218671,"the modifier wm ∗ ł this Introduction Corresponding author The two domain-specific datasets, plus another one for product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation"
P19-1229,N09-1068,0,0.0306646,"meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain. Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labeled target-domain data, which can be used to directly learn the domain-specific distributions or features. Daum´e III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labeling tasks. The idea is to distinguish domain-specific and general features by making a copy of each feature for each domain plus a shared (general) pseudo domain. Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea. As pointed by Finkel and Manning (2009), those two works can be understood as MTL under the traditional discrete-feature ML framework. Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging. Different from the unsupervised method of Guo et al. (2018), they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances. In the parsing community, Flannery and Mori (2015) propose to annotate partially lab"
P19-1229,W15-2202,0,0.0307514,"shared (general) pseudo domain. Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea. As pointed by Finkel and Manning (2009), those two works can be understood as MTL under the traditional discrete-feature ML framework. Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging. Different from the unsupervised method of Guo et al. (2018), they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances. In the parsing community, Flannery and Mori (2015) propose to annotate partially labeled target-domain data with active learning for cross-domain Japanese dependency parsing. Similarly, Joshi et al. (2018) annotate a few dozen partially labeled target-domain sentences with a few brackets for cross-domain constituent parsing. Both results report large improvement and show the usefulness of even small amount of target-domain annotation, showing the great potential of semi-supervised domain adaptation for parsing. 6 Conclusions This work addresses the task of semi-supervised domain adaptation for Chinese dependency parsing, based on our two newl"
P19-1229,I11-1100,0,0.0717653,"Missing"
P19-1229,C16-1002,0,0.0777974,"erse annotation guideline) for a language. Inspired by their work, we propose to concatenate each word position with an extra domain embedding to indicate which domain this training sentence comes from, as illustrated in Figure 3. In this way, we expect the model can fully utilize both training datasets, since most parameters are shared except the two domain embedding vectors, and learn to distinguish the domain-specific and general features as well. (3) Multi-task learning (MTL) aims to incorporate labeled data of multiple related tasks for improving performance (Collobert and Weston, 2008). Guo et al. (2016) first employ MTL to improve parsing performance by utilizing multiple heterogeneous treebanks and treating each treebank as a separate task. As shown in Figure 4, we make a straightforward extension to the biaffine parser to realize multi-task learning. The sourcedomain and target-domain parsing are treated as xi ... Figure 4: The framework of MTL. two individual tasks with shared parameters for word/tag embeddings and BiLSTMs. The main weakness of MTL is that the model cannot make full use of the source-domain labeled data, since the source-domain training data only contributes to the traini"
P19-1229,D18-1498,0,0.0496972,"ubset from the source-domain training data to train the parsing model, instead of using all the labeled data (Plank and van Noord, 2011; Khan et al., 2013). The multi-source domain adaptation problem assumes there are labeled datasets for multiple source domains. Given a target domain, the challenge is how to effectively combine knowledge in the source domains. McClosky et al. (2010) first raise this scenario for constituent parsing. They employ a regression model to predict crossdomain performance, and then use the values to combine parsing models independently trained on each source domain. Guo et al. (2018) employ a similar idea of mixture of experts under the neural MTL framework, and conduct experiments on sentiment classification and POS tagging tasks. They employ meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain. Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labeled target-domain data, which can be used to directly learn the domain-specific distributions or features. Daum´e III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labe"
P19-1229,P18-1252,1,0.921905,"this work, we choose two typical domain-aware web texts for annotation, i.e., product blogs and web fictions. This section introduces the details about the data annotation procedure. Data selection. The product blog (PB) texts are crawled from the Taobao headline website, which contains articles written by users mainly on description and comparison of different commercial products. After data cleaning and automatic word segmentation, we have collected about 340K sentences. Then, we select 10 thousand sentences with [5, 25] words for manual annotation following the active learning workflow of Jiang et al. (2018). The remaining sentences are used as unlabeled data. For web fictions, we follow the work on cross-domain word segmentation of Zhang et al. (2014), and adopt the popular novel named as “Zhuxian” (ZX, also known as “Jade dynasty”). Among their annotated 4,555 sentences, we select about 3,400 sentences with [5, 45] words for annotation. The remaining 32K sentences of ZX are used as unlabeled data in this work. Annotation guideline. After comparing several publicly available guidelines for dependency parsing including the universal dependencies (UD) (McDonald et al., 2013), we adopt the guidelin"
P19-1229,P18-1110,0,0.0770222,"Missing"
P19-1229,R13-1046,0,0.0528327,"Missing"
P19-1229,P17-1060,0,0.0488226,", which can be used to directly learn the domain-specific distributions or features. Daum´e III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labeling tasks. The idea is to distinguish domain-specific and general features by making a copy of each feature for each domain plus a shared (general) pseudo domain. Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea. As pointed by Finkel and Manning (2009), those two works can be understood as MTL under the traditional discrete-feature ML framework. Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging. Different from the unsupervised method of Guo et al. (2018), they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances. In the parsing community, Flannery and Mori (2015) propose to annotate partially labeled target-domain data with active learning for cross-domain Japanese dependency parsing. Similarly, Joshi et al. (2018) annotate a few dozen partially labeled target-domain sentences with a few brackets for cross-do"
P19-1229,Q16-1023,0,0.122391,"Missing"
P19-1229,P18-1249,0,0.0262471,"g has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the major challenge for applying syntactic analysis in realistic NLP systems. To meet this challenge, the community has organized several shared tasks to attract more research attention (Nivre et al., 2007; Hajiˇc et al., 2009; Petrov and McDonald, 2012). 2386 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
P19-1229,D14-1108,0,0.0607182,"Missing"
P19-1229,P14-1043,1,0.843831,"e 4: The framework of MTL. two individual tasks with shared parameters for word/tag embeddings and BiLSTMs. The main weakness of MTL is that the model cannot make full use of the source-domain labeled data, since the source-domain training data only contributes to the training of the shared parameters. The corpus weighting strategy. For all above three approaches, the target-domain labeled data would be overwhelmed by the source-domain data during training if directly combined, since there usually exists a very big gap in their scale. Therefore, we employ the simple corpus weighting strategy (Li et al., 2014) as a useful trick. Before each iteration, we randomly sample training sentences separately from the target- and sourcedomain training data in the proportion of 1 : M . Then we merge and randomly shuffle the sampled data for one-iteration training. We treat M ≥ 1 as a hyper-parameter tuned on the dev data. 3.3 Utilizing Unlabeled Data Besides labeled data, how to exploit unlabeled data, both target- and source-domain, has been an interesting and important direction for crossdomain parsing for a long time, as discussed in Section 5. Recently, Peters et al. (2018) introduce embeddings from langu"
P19-1229,P18-1130,0,0.0228167,"product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the majo"
P19-1229,P06-1043,0,0.131945,"arch, we try to give a brief (and far from complete) review on some representative approaches of high relevance with syntactic parsing. Unsupervised domain adaptation. Due to the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta2392 tion, assuming there is only labeled data for the source domain. Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect. As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015). There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing. Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007). In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting researc"
P19-1229,N10-1004,0,0.035199,"s from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting research direction. Given a target domain, the idea is to automatically select a most relevant subset from the source-domain training data to train the parsing model, instead of using all the labeled data (Plank and van Noord, 2011; Khan et al., 2013). The multi-source domain adaptation problem assumes there are labeled datasets for multiple source domains. Given a target domain, the challenge is how to effectively combine knowledge in the source domains. McClosky et al. (2010) first raise this scenario for constituent parsing. They employ a regression model to predict crossdomain performance, and then use the values to combine parsing models independently trained on each source domain. Guo et al. (2018) employ a similar idea of mixture of experts under the neural MTL framework, and conduct experiments on sentiment classification and POS tagging tasks. They employ meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain. Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labele"
P19-1229,D07-1111,0,0.070654,"the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta2392 tion, assuming there is only labeled data for the source domain. Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect. As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015). There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing. Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007). In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting research direction. Given a target domain, the idea is to automatically select a most relevant subset from the source-domain training data to train the parsing model, instead of usin"
P19-1229,D14-1122,0,0.0233107,"sume there is no labeled target-domain training data and thus focus on unsupervised domain adaptation. So far, approaches in this direction have made limited progress, due to the intrinsic difficulty of both domain adaptation and parsing (see discussions in Section 5). On the other hand, due to the extreme complexity and heavy cost, progress on syntactic data annotation on new-domain texts has been very slow, and only several small-scale datasets on web texts have been built, mostly as evaluation data for cross-domain parsing (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). To meet the above challenges, this paper presents two newly-annotated large-scale domainaware datasets (over 12K sentences), and try to tackle the task of semi-supervised domain adaptation for Chinese dependency parsing. With the access of both labeled and unlabeled targetdomain data, we propose and evaluate several simple approaches and conduct error analysis in order to investigate the following three questions: Q1: How to effectively combine the source- and target-domain labeled training data? Q2: How to utilize the target-domain unlabeled data for further improvements? Q3: Given a certai"
P19-1229,W15-2201,0,0.286589,"lete) review on some representative approaches of high relevance with syntactic parsing. Unsupervised domain adaptation. Due to the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta2392 tion, assuming there is only labeled data for the source domain. Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect. As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015). There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing. Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007). In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting research direction. Given a target domain, the i"
P19-1229,E14-1062,0,0.0554062,"about the data annotation procedure. Data selection. The product blog (PB) texts are crawled from the Taobao headline website, which contains articles written by users mainly on description and comparison of different commercial products. After data cleaning and automatic word segmentation, we have collected about 340K sentences. Then, we select 10 thousand sentences with [5, 25] words for manual annotation following the active learning workflow of Jiang et al. (2018). The remaining sentences are used as unlabeled data. For web fictions, we follow the work on cross-domain word segmentation of Zhang et al. (2014), and adopt the popular novel named as “Zhuxian” (ZX, also known as “Jade dynasty”). Among their annotated 4,555 sentences, we select about 3,400 sentences with [5, 45] words for annotation. The remaining 32K sentences of ZX are used as unlabeled data in this work. Annotation guideline. After comparing several publicly available guidelines for dependency parsing including the universal dependencies (UD) (McDonald et al., 2013), we adopt the guideline released by Jiang et al. (2018) based on three considerations. First, their guideline contains 20 relations specifically designed to capture Chin"
P19-1229,P15-1117,0,0.0182641,"ł this Introduction Corresponding author The two domain-specific datasets, plus another one for product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, w"
P19-1229,N18-1202,0,0.26759,"inese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the major challenge for applying syntactic analysis in realistic NLP systems. To meet this challenge, the community has organized several shared tasks to a"
P19-1229,P11-1157,0,0.0874358,"Missing"
P19-1296,D17-1304,1,0.807793,"Missing"
P19-1296,I17-1002,1,0.860783,"Missing"
P19-1296,P18-1192,0,0.0281199,"mize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. 1 Introduction Neural network based methods have been applied to several natural language processing tasks (Zhang et al., 2016; Li et al., 2018; Chen et al., 2018; Li et al., 2019; He et al., 2018). In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. ∗ Mingming Yang was an internship research fellow at NICT when co"
P19-1296,P18-1164,0,0.250005,"conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. ∗ Mingming Yang was an internship research fellow at NICT when conducting this work. Based on this hypothesis, Kuang et al. (2018) proposed a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation. Tu et al. (2017) incorporated a reconstructor module into NMT, which reconstructs the input source sentence from the hidden layer of the output target sentence to enhance source representation. However, in previous studies, the training objective function was usually based on word-level and lacked explicit sentencelevel relationships (Zhang and Zhao, 2019). Although Transformer model (Vaswani et al., 2017) has archived state-of-the-art performance of NMT,"
P19-1296,C18-1271,0,0.0150768,"ose a sentencelevel agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. 1 Introduction Neural network based methods have been applied to several natural language processing tasks (Zhang et al., 2016; Li et al., 2018; Chen et al., 2018; Li et al., 2019; He et al., 2018). In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. ∗ Mingming"
P19-1296,D15-1166,0,0.122472,"Missing"
P19-1296,P02-1040,0,0.103188,"Missing"
P19-1296,W18-6319,0,0.0337367,"Missing"
P19-1296,W16-0533,0,0.0307462,"Although Transformer model (Vaswani et al., 2017) has archived state-of-the-art performance of NMT, more attention is paid to the words-level relationship via self-attention networks. Sentence-level agreement method has been applied to many natural language processing tasks. Aliguliyev (2009) used sentence similarity measure technique for automatic text summarization. Liang et al. (2010) have shown that the sentence similarity algorithm based on VSM is beneficial to address the FAQ problem. Su et al. (2016) presented a sentence similarity method for spoken dialogue system to improve accuracy. Rei and Cummins (2016) proposed sentence similarity measures to improve the estimation of topical relevance. Wang et al. (2017b; 2018) used sentence similarity to select sentences with the similar domains. The above methods only considered monolingual sentence-level agreement. In human translation, a translator’s primary concern is to translate a sentence through its entire meaning rather than word-by-word meaning. Therefore, in early machine translation studies, such as example-based machine translation (Nagao, 1984; Nio et al., 2013), use the sentence similarity matching between the sentences to be translated and"
P19-1296,W16-2323,0,0.0605544,"Missing"
P19-1296,P16-1162,0,0.158734,"Missing"
P19-1296,P16-1131,0,0.017386,"this paper, we propose a sentencelevel agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. 1 Introduction Neural network based methods have been applied to several natural language processing tasks (Zhang et al., 2016; Li et al., 2018; Chen et al., 2018; Li et al., 2019; He et al., 2018). In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target w"
P19-1296,P16-1008,0,0.0212731,"nction in between: FFN(x) = max(0, xW1 + b1 )W2 + b2 , (2) where W1 and W2 are both linear transformation networks, b1 and b2 are both bias. We define Henc as the sentence representation of X via the self-attention layers in encoder, and Hdec as the sentence representation of words Y via embedding layers in decoder. The parameters of Transformer are trained to minimize the following objective function on a set of training examples {(X n , Y n )}N n=1 : Lmle = − 3 N Iy 1 XX n logP (yin |y<i , Henc , Hdec ). N n=1 i=1 (3) Agreement on Source and Target Sentence Some studies (Luong et al., 2015; Tu et al., 2016; Chen et al., 2017a,b; Kuang et al., 2018) showed that improving word alignment is beneficial to machine translation. Their idea is based on word-level agreement and make the embeddings of source words and corresponding target words similar. In this paper, we investigate the sentence-level relationship between the source and target sentences. We propose a sentence-level agreement method which can make the sentencelevel semantics of the source and target closer. The entire architecture of the proposed method is illustrated in Figure 1. 3.1 Sentence-Level Agreement First, we need to get the sen"
P19-1296,P17-2089,1,0.919928,"Missing"
P19-1296,C18-1269,0,0.0400623,"Missing"
P19-1296,D17-1155,1,0.822564,"ntion is paid to the words-level relationship via self-attention networks. Sentence-level agreement method has been applied to many natural language processing tasks. Aliguliyev (2009) used sentence similarity measure technique for automatic text summarization. Liang et al. (2010) have shown that the sentence similarity algorithm based on VSM is beneficial to address the FAQ problem. Su et al. (2016) presented a sentence similarity method for spoken dialogue system to improve accuracy. Rei and Cummins (2016) proposed sentence similarity measures to improve the estimation of topical relevance. Wang et al. (2017b; 2018) used sentence similarity to select sentences with the similar domains. The above methods only considered monolingual sentence-level agreement. In human translation, a translator’s primary concern is to translate a sentence through its entire meaning rather than word-by-word meaning. Therefore, in early machine translation studies, such as example-based machine translation (Nagao, 1984; Nio et al., 2013), use the sentence similarity matching between the sentences to be translated and the sentences in the 3076 Proceedings of the 57th Annual Meeting of the Association for Computational L"
P19-1305,P00-1041,0,0.241717,"sentences paired with the true summaries to build a training corpus for the cross-lingual ASSUM. This alleviates the data scarcity that no cross-lingual ASSUM corpus is available. • Extensive experimental results on two benchmark datasets show that our proposed method is able to perform better than several baselines and related works, and significantly reduce the performance gap between the crosslingual ASSUM and the monolingual ASSUM. 2 2.1 Related Work Monolingual ASSUM There are various methods exploring the effective way to model the monolingual ASSUM process including statistical models (Banko et al., 2000; Cohn and Lapata, 2008) or neural models (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). Neural models become dominant in this task since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). On the basis of the sequence-tosequence neural architecture, there are many further explorations such as using rich linguistic features and large vocabulary set (Nallapati et al., 2016), global training procedures on the sentence level (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018; Wang et al., 2018), topic enhancement in"
P19-1305,P17-1176,0,0.0183466,"018). 2.2 Zero Resource Neural Machine Translation Current state-of-the-art NMT models are effective in modeling the translation process, but they are highly dependent on the large-scale parallel corpus. When applied on zero resource language pairs such as the two languages that do not have direct parallel corpus, the NMT systems perform well below the satisfactory level. To address such problem, three NMT paradigms are explored. The first is the triangular NMT systems that add one additional resource rich language to the zero resource language pair to build a triangular translation scenario (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017), the second is the multilingual translation system that concatenates parallel corpora of different language pairs and builds one NMT model for all (Johnson et al., 2017), the third is the unsupervised NMT systems that do not use any parallel data resources (Artetxe et al., 2018; Lample et al., 2018). Our work is closely related to the first paradigm in which source language, pivot language, and target language form a triangular translation scenario. In our setting, the target language {sentence, summary} pair and the source language sentence form the t"
P19-1305,D18-1399,0,0.0519969,"Missing"
P19-1305,P18-1163,0,0.0181086,"he NMT system involved in all our experiments 4 https://github.com/pytorch/fairseq is Transformer, with the same parameter setup to those of ASSUM systems. It is trained on 1.25M sentence pairs extracted from LDC corpora5 , and is evaluated on NIST sets using multibleu.perl. Chinese-to-English results of caseinsensitive BLEU and English-to-Chinese results of character-based BLEU are reported in Table 1. Since there are four English references for one Chinese sentence in NIST evaluation sets, we report averaged BLEU of four English input sentences in English-to-Chinese translation. Compared to Cheng et al. (2018) on Chineseto-English translation, which targets at robust machine translation and uses the same data to ours, our Transformer significantly outperforms their work, indicating that we build a solid system for machine translation. 4.3 Experimental Results Monolingual ASSUM Performance We build a strong monolingual ASSUM system as shown in Table 2. The comparison is made between our basis architecture Transformer and previous works including state-of-the-art monolingual ASSUM systems. The work of ABS+ (Rush et al., 2015) is the pioneer work of using neural models for monolingual ASSUM. The works"
P19-1305,N16-1012,0,0.0342663,"s-lingual ASSUM. This alleviates the data scarcity that no cross-lingual ASSUM corpus is available. • Extensive experimental results on two benchmark datasets show that our proposed method is able to perform better than several baselines and related works, and significantly reduce the performance gap between the crosslingual ASSUM and the monolingual ASSUM. 2 2.1 Related Work Monolingual ASSUM There are various methods exploring the effective way to model the monolingual ASSUM process including statistical models (Banko et al., 2000; Cohn and Lapata, 2008) or neural models (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). Neural models become dominant in this task since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). On the basis of the sequence-tosequence neural architecture, there are many further explorations such as using rich linguistic features and large vocabulary set (Nallapati et al., 2016), global training procedures on the sentence level (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018; Wang et al., 2018), topic enhancement in the summaries (Wang et al., 2018), additional selective gate networks in the enco"
P19-1305,C08-1018,0,0.0490986,"h the true summaries to build a training corpus for the cross-lingual ASSUM. This alleviates the data scarcity that no cross-lingual ASSUM corpus is available. • Extensive experimental results on two benchmark datasets show that our proposed method is able to perform better than several baselines and related works, and significantly reduce the performance gap between the crosslingual ASSUM and the monolingual ASSUM. 2 2.1 Related Work Monolingual ASSUM There are various methods exploring the effective way to model the monolingual ASSUM process including statistical models (Banko et al., 2000; Cohn and Lapata, 2008) or neural models (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). Neural models become dominant in this task since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). On the basis of the sequence-tosequence neural architecture, there are many further explorations such as using rich linguistic features and large vocabulary set (Nallapati et al., 2016), global training procedures on the sentence level (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018; Wang et al., 2018), topic enhancement in the summaries (Wang et a"
P19-1305,N18-1033,0,0.152927,"ASSUM process including statistical models (Banko et al., 2000; Cohn and Lapata, 2008) or neural models (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). Neural models become dominant in this task since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). On the basis of the sequence-tosequence neural architecture, there are many further explorations such as using rich linguistic features and large vocabulary set (Nallapati et al., 2016), global training procedures on the sentence level (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018; Wang et al., 2018), topic enhancement in the summaries (Wang et al., 2018), additional selective gate networks in the encoder (Zhou et al., 2017), and facts fusion measures (Cao et al., 2018). 2.2 Zero Resource Neural Machine Translation Current state-of-the-art NMT models are effective in modeling the translation process, but they are highly dependent on the large-scale parallel corpus. When applied on zero resource language pairs such as the two languages that do not have direct parallel corpus, the NMT systems perform well below the satisfactory level. To address such problem, three NMT p"
P19-1305,D15-1229,0,0.715344,"qual contribution. monolingual ASSUM corpus, the cross-lingual ASSUM is seldom explored due to the lack of training corpus. This zero-shot challenge drives the cross-lingual ASSUM to resort to two existing independent techniques, i.e., the monolingual ASSUM and the bilingual translation. The both techniques should be leveraged together to overcome the difficulty of data scarcity in the cross-lingual ASSUM. Regarding the techniques of the monolingual ASSUM, neural methods become dominant in this area since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). The corpus consists of huge number of source-summary pairs, and neural methods model these pairs as as a sequence-to-sequence task by encoding the source sentence into vectorized information and decoding it into the abstractive summary. Regarding the techniques of the bilingual translation, recent years witnessed the method transition from statistical machine translation (SMT) (Koehn et al., 2003) to neural machine translation (NMT). NMT employs the sequence-to-sequence architecture with various implementations such as RNN-based (Sutskever et al., 2014; Bahdanau et al., 2015), CNN-based (Geh"
P19-1305,Q17-1024,0,0.037043,"n applied on zero resource language pairs such as the two languages that do not have direct parallel corpus, the NMT systems perform well below the satisfactory level. To address such problem, three NMT paradigms are explored. The first is the triangular NMT systems that add one additional resource rich language to the zero resource language pair to build a triangular translation scenario (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017), the second is the multilingual translation system that concatenates parallel corpora of different language pairs and builds one NMT model for all (Johnson et al., 2017), the third is the unsupervised NMT systems that do not use any parallel data resources (Artetxe et al., 2018; Lample et al., 2018). Our work is closely related to the first paradigm in which source language, pivot language, and target language form a triangular translation scenario. In our setting, the target language {sentence, summary} pair and the source language sentence form the triangle in which the target language sentence functions as the pivot. We adopt the teacher-student framework that is also applied 3163 teacher in Chen et al. (2017), but we have significant difference to them in"
P19-1305,N03-1017,0,0.062611,"Missing"
P19-1305,J82-2005,0,0.347435,"Missing"
P19-1305,W04-1013,0,0.0537104,"luation Metric Transformer is employed as our basis architecture4 (Vaswani et al., 2017). Six layers are stacked in both the encoder and decoder, and the dimensions of the embedding vectors and all hidden vectors are set 512. We set eight heads in the multi-head attention. The source embedding, the target embedding and the linear sublayer are shared in the teacher networks, while are not shared in the student networks. Byte-pair encoding is employed with a vocabulary of about 32k tokens on English side and Chinese side respectively (Sennrich et al., 2016b). During evaluation, we employ ROUGE (Lin, 2004) as our evaluation metric. On Gigaword, the full-length F-1 based ROUGE scores are reported. On DUC2004, the recall based ROUGE scores are reported to be consistent with previous works. NMT Performance The NMT system involved in all our experiments 4 https://github.com/pytorch/fairseq is Transformer, with the same parameter setup to those of ASSUM systems. It is trained on 1.25M sentence pairs extracted from LDC corpora5 , and is evaluated on NIST sets using multibleu.perl. Chinese-to-English results of caseinsensitive BLEU and English-to-Chinese results of character-based BLEU are reported in"
P19-1305,K16-1028,0,0.133639,"d on the large-scale ∗ Equal contribution. monolingual ASSUM corpus, the cross-lingual ASSUM is seldom explored due to the lack of training corpus. This zero-shot challenge drives the cross-lingual ASSUM to resort to two existing independent techniques, i.e., the monolingual ASSUM and the bilingual translation. The both techniques should be leveraged together to overcome the difficulty of data scarcity in the cross-lingual ASSUM. Regarding the techniques of the monolingual ASSUM, neural methods become dominant in this area since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). The corpus consists of huge number of source-summary pairs, and neural methods model these pairs as as a sequence-to-sequence task by encoding the source sentence into vectorized information and decoding it into the abstractive summary. Regarding the techniques of the bilingual translation, recent years witnessed the method transition from statistical machine translation (SMT) (Koehn et al., 2003) to neural machine translation (NMT). NMT employs the sequence-to-sequence architecture with various implementations such as RNN-based (Sutskever et al., 2014; Bahdanau et al., 201"
P19-1305,D15-1044,0,0.593029,"udies that are based on the large-scale ∗ Equal contribution. monolingual ASSUM corpus, the cross-lingual ASSUM is seldom explored due to the lack of training corpus. This zero-shot challenge drives the cross-lingual ASSUM to resort to two existing independent techniques, i.e., the monolingual ASSUM and the bilingual translation. The both techniques should be leveraged together to overcome the difficulty of data scarcity in the cross-lingual ASSUM. Regarding the techniques of the monolingual ASSUM, neural methods become dominant in this area since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). The corpus consists of huge number of source-summary pairs, and neural methods model these pairs as as a sequence-to-sequence task by encoding the source sentence into vectorized information and decoding it into the abstractive summary. Regarding the techniques of the bilingual translation, recent years witnessed the method transition from statistical machine translation (SMT) (Koehn et al., 2003) to neural machine translation (NMT). NMT employs the sequence-to-sequence architecture with various implementations such as RNN-based (Sutskever et al., 20"
P19-1305,P16-1009,0,0.549035,"stem. We use the teacher-student framework in which the monolingual ASSUM system is taken as the teacher and the cross-lingual ASSUM system is the student. The teacher let the student to simulate both the summary word distribution and attention weights according to those of the teacher networks. In comparison to the pseudo summaries used in the work of Ayana et al. (2018), we generate pseudo sources instead and use true summaries to constitute source-summary pairs. This is motivated by the successful application of backtranslation which generates pseudo-source paired with true-target for NMT (Sennrich et al., 2016a; Lample et al., 2018). The main contributions of this paper include: • We propose teaching both summary word generation distribution and attention weights in the cross-lingual ASSUM networks by using the monolingual ASSUM networks. The distribution teacher is directly from the monolingual ASSUM, while the attention weights teacher is obtained by an attention relay mechanism. • We use a back-translation procedure that generates pseudo source sentences paired with the true summaries to build a training corpus for the cross-lingual ASSUM. This alleviates the data scarcity that no cross-lingual"
P19-1305,P16-1162,0,0.763181,"stem. We use the teacher-student framework in which the monolingual ASSUM system is taken as the teacher and the cross-lingual ASSUM system is the student. The teacher let the student to simulate both the summary word distribution and attention weights according to those of the teacher networks. In comparison to the pseudo summaries used in the work of Ayana et al. (2018), we generate pseudo sources instead and use true summaries to constitute source-summary pairs. This is motivated by the successful application of backtranslation which generates pseudo-source paired with true-target for NMT (Sennrich et al., 2016a; Lample et al., 2018). The main contributions of this paper include: • We propose teaching both summary word generation distribution and attention weights in the cross-lingual ASSUM networks by using the monolingual ASSUM networks. The distribution teacher is directly from the monolingual ASSUM, while the attention weights teacher is obtained by an attention relay mechanism. • We use a back-translation procedure that generates pseudo source sentences paired with the true summaries to build a training corpus for the cross-lingual ASSUM. This alleviates the data scarcity that no cross-lingual"
P19-1305,P11-1155,0,0.51672,"t al., 2014; Bahdanau et al., 2015), CNN-based (Gehring et al., 2017), and Transformer (Vaswani et al., 2017). Early works on the cross-lingual ASSUM leverage the above two techniques through using bilingual features to cooperate with the monolingual ASSUM based on the data condition that largescale monolingual ASSUM corpus is not available while large-scale translation corpora are easy to obtain. They utilize bilingual features such as phrase pairs or predicate-argument parallel structures, which are obtained from SMT systems, to achieve extractive or abstractive cross-lingual summarization (Wan, 2011; Yao et al., 2015; Zhang et al., 2016). 3162 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3162–3172 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Recently, Ayana et al. (2018) propose the first large-scale corpus-based cross-lingual ASSUM system in which the ASSUM corpus is monolingual. They generate summaries using the monolingual ASSUM system, and train the cross-lingual ASSUM based on these pseudo summaries. On the contrary, we propose in this paper to use genuine summaries paired with the gener"
P19-1305,P10-1094,0,0.467142,"opt the teacher-student framework that is also applied 3163 teacher in Chen et al. (2017), but we have significant difference to them in that we generate pseudo source while they generate pseudo target, which results in different teacher-student networks. 2.3 3.1 NMT tgt lang sentence tgt lang summary student Cross-lingual Summarization Early explorations on cross-lingual summarization mainly depend on the traditional monolingual summarization methods, and integrate bilingual parallel informations into the monolingual methods through sentence selection based on translation quality estimation (Wan et al., 2010), sentence ranking based on cross-lingual sentence similarity (Wan, 2011), or abstractive summarization based on phrase pair (Yao et al., 2015) and predicate-argument structure fusing (Zhang et al., 2016). The first cross-lingual ASSUM system based on the large-scale monolingual ASSUM corpus is proposed by Ayana et al. (2018), which is most related to our work. It is motivated by the triangular NMT systems with pseudo target in the teacher-student networks. In contrast, we use pseudo source and apply different teacher-student networks. 3 src lang sentence Our Approach Overall Framework To over"
P19-1305,D15-1012,0,0.31399,"Missing"
P19-1305,P17-1101,0,0.0238251,"lapati et al., 2016). Neural models become dominant in this task since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). On the basis of the sequence-tosequence neural architecture, there are many further explorations such as using rich linguistic features and large vocabulary set (Nallapati et al., 2016), global training procedures on the sentence level (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018; Wang et al., 2018), topic enhancement in the summaries (Wang et al., 2018), additional selective gate networks in the encoder (Zhou et al., 2017), and facts fusion measures (Cao et al., 2018). 2.2 Zero Resource Neural Machine Translation Current state-of-the-art NMT models are effective in modeling the translation process, but they are highly dependent on the large-scale parallel corpus. When applied on zero resource language pairs such as the two languages that do not have direct parallel corpus, the NMT systems perform well below the satisfactory level. To address such problem, three NMT paradigms are explored. The first is the triangular NMT systems that add one additional resource rich language to the zero resource language pair to"
P19-1345,D17-1047,0,0.161136,"e-ofthe-art approaches to ASC as baselines. Since the input of all these approaches should be a single sequence, we concatenate question and answer text to generate a single sequence. Besides, we employ some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-theart deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-ofthe-art approach to ASC which combines CNN and gating mechanisms to learn text representation. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opinion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehension. We substitute its decoding layer with softmax decoder to perform ASC-QA. 6) HMN (Shen et al., 2018a). This is a QA matching approach to coarse-grained sentimen"
P19-1345,P14-2009,0,0.0298397,"QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has become increasingly popular and a large-scale of such QA style reviews (as shown in Figure 1) could be found in several famous e-commerce platforms (e.g., Amazon and Taobao). Compared to traditional non-interactive cust"
P19-1345,P11-1016,0,0.0880512,"ent Classiﬁcation Towards QA - Input: QA text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Cust"
P19-1345,D16-1011,0,0.0706496,"Missing"
P19-1345,C18-1079,0,0.0249668,"Missing"
P19-1345,D13-1171,0,0.358181,"Missing"
P19-1345,S15-2082,0,0.190919,"Missing"
P19-1345,S14-2004,0,0.52776,"oposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has become increasingly popular and a large-scale of such QA style reviews (as shown in Figure 1) could be found in several famous e-commerce platfor"
P19-1345,P13-4009,0,0.0428117,"Missing"
P19-1345,D18-1401,1,0.853282,"Missing"
P19-1345,D16-1021,0,0.411895,"wards QA - Input: QA text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answe"
P19-1345,P08-1036,0,0.0510394,"entence-level text classiﬁcation which aims to incorporate aspect information into a model. Recently, Wang et al. (2016); Ma et al. (2017) propose an attention based LSTM to ASC by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016b), Chen et al. (2017) and Wang et al. (2018b) employ memory networks to model the context and aspect. Wang and Lu (2018) propose a segmentation attention to capture structural dependency between target and opinion terms. Document-level ASC aims to predict sentiment ratings for aspects inside a long text. Traditional studies (Titov and McDonald, 2008; Wang et al., 2010; Pontiki et al., 2016) solve document-level ASC as a sub-problem by utilizing heuristic based methods or topic models. Recently, Lei et al. (2016) focus on extracting rationales for aspects in a document. Li et al. (2018) propose an useraware attention approach to document-level ASC. Yin et al. (2017) model document-level ASC as a machine comprehension problem, of which the input is also a parallel unit, i.e., question and answer. However, their question texts are pseudo and artiﬁcially constructed. This disaccords with the fact that real-world question texts also possibly"
P19-1345,P18-1088,0,0.34373,"text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has beco"
P19-1345,D16-1058,0,0.427641,"determine the polarity towards each aspect category discussed in a QA text pair. 4.2 Baselines For comparison, we implement several state-ofthe-art approaches to ASC as baselines. Since the input of all these approaches should be a single sequence, we concatenate question and answer text to generate a single sequence. Besides, we employ some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-theart deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-ofthe-art approach to ASC which combines CNN and gating mechanisms to learn text representation. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opinion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehension. We substitute its decod"
P19-1345,P18-1234,0,0.0124307,"e a single sequence, we concatenate question and answer text to generate a single sequence. Besides, we employ some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-theart deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-ofthe-art approach to ASC which combines CNN and gating mechanisms to learn text representation. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opinion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehension. We substitute its decoding layer with softmax decoder to perform ASC-QA. 6) HMN (Shen et al., 2018a). This is a QA matching approach to coarse-grained sentiment classiﬁcation towards QA style reviews. 7) MAMC (Yin et al., 2017). This is a QA matching"
P19-1345,D17-1217,0,0.0355952,"Missing"
S19-2002,P05-1013,0,0.438798,"Missing"
S19-2002,N18-1202,0,0.049253,"he embeddings of the named entity tags and the dependency labels, but find limited performance gains. Then, the parser employs two cascaded bidirectional LSTM layers as the encoder, and use the top-layer outputs as the word representations. Afterwards, the parser represents each span wi ...wj as 2.4 Use of BERT For the open tracks, we use the contextualized word representations produced by BERT (Devlin et al., 2018) as extra input features.2 Following previous works, we use the weighted summation of the last four transformer layers and then multiply a task-specific weight parameter following (Peters et al., 2018). 2 We use the multilingual cased BERT from https:// github.com/google-research/bert. ri,j = (fj − fi ) ⊕ (bi − bj ) 13 3 Cross-lingual Parsing Because of little training data for French, we borrow the treebank embedding approach of Stymne et al. (2018) for exploiting multiple heterogeneous treebanks for the same language, and propose a language embedding approach to utilize English and German training data. The training datasets of the three languages are merged to train a single UCCA parsing model. The only modification is to concatenate each word position with an extra language embedding (o"
S19-2002,P18-2098,0,0.0642438,"Missing"
S19-2002,P13-1023,0,0.359727,"t al. (2017) first propose a transition-based UCCA Parser, which is used as the baseline in the closed tracks of this shared task. Based on the recent progress on transitionbased parsing techniques, they propose a novel set of transition actions to handle both discontinuous and remote nodes and design useful features based on bidirectional LSTMs. Hershcovich et al. (2018) then extend their previous approach and propose to utilize the annotated data with other Introduction Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework for semantic annotation proposed by Abend and Rappoport (2013). Figure 1 shows an example sentence and its UCCA graph. Words are represented as terminal nodes. Circles denote non-terminal nodes, and the semantic relation 1 The full UCCA scheme also has implicit and linkage relations, which are overlooked in the community so far. ∗ Corresponding author, hlt.suda.edu.cn/zhenghua 11 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 11–15 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics ROOT semantic formalisms such as abstract meaning representation (AMR), universal depend"
S19-2002,N19-1423,0,0.0680309,"Missing"
S19-2002,P17-1104,0,0.0904906,"lustration. between two non-terminal nodes is represented by the label on the edge. One node may have multiple parents, among which one is annotated as the primary parent, marked by solid line edges, and others as remote parents, marked by dashed line edges. The primary edges form a tree structure, whereas the remote edges enable reentrancy, forming directed acyclic graphs (DAGs).1 The second feature of UCCA is the existence of nodes with discontinuous leaves, known as discontinuity. For example, node 3 in Figure 1 is discontinuous because some terminal nodes it spans are not its descendants. Hershcovich et al. (2017) first propose a transition-based UCCA Parser, which is used as the baseline in the closed tracks of this shared task. Based on the recent progress on transitionbased parsing techniques, they propose a novel set of transition actions to handle both discontinuous and remote nodes and design useful features based on bidirectional LSTMs. Hershcovich et al. (2018) then extend their previous approach and propose to utilize the annotated data with other Introduction Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework for semantic annotation proposed by Abend and Ra"
S19-2002,P18-1035,0,0.105592,"cyclic graphs (DAGs).1 The second feature of UCCA is the existence of nodes with discontinuous leaves, known as discontinuity. For example, node 3 in Figure 1 is discontinuous because some terminal nodes it spans are not its descendants. Hershcovich et al. (2017) first propose a transition-based UCCA Parser, which is used as the baseline in the closed tracks of this shared task. Based on the recent progress on transitionbased parsing techniques, they propose a novel set of transition actions to handle both discontinuous and remote nodes and design useful features based on bidirectional LSTMs. Hershcovich et al. (2018) then extend their previous approach and propose to utilize the annotated data with other Introduction Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework for semantic annotation proposed by Abend and Rappoport (2013). Figure 1 shows an example sentence and its UCCA graph. Words are represented as terminal nodes. Circles denote non-terminal nodes, and the semantic relation 1 The full UCCA scheme also has implicit and linkage relations, which are overlooked in the community so far. ∗ Corresponding author, hlt.suda.edu.cn/zhenghua 11 Proceedings of the 13th Int"
S19-2002,P17-1076,0,\N,Missing
W06-0125,C04-1004,1,0.905303,"Missing"
W06-0125,I05-1047,1,0.897369,"Missing"
W06-0125,P02-1060,1,\N,Missing
W09-3501,W09-3517,0,0.0374294,"ementations are based on approaches that are language-independent. Indeed, many of the participants fielded their systems on multiple languages, as can be seen from Table 3. We also note that combination of several different models via re-ranking of their outputs (CRF, Maximum Entropy Model, Margin Infused Relaxed Algorithm) proves to be very successful (Oh et al., 2009); their system (reported as Team ID 6) produced the best or second-best transliteration performance consistently across all metrics, in all tasks, except Japanese back-transliteration. Examples of other model combinations are (Das et al., 2009). At least two teams (reported as Team IDs 14 and 27) incorporate language origin detection in their system (Bose and Sarkar, 2009; Khapra and Bhattacharyya, 2009). The Indian language corpora contains names of both English and Indic origin. Khapra and Bhattacharyya (2009) demonstrate how much the transliteration performance can be improved when language of origin detection is employed, followed by a language-specific transliteration model for decoding. Some systems merit specific mention as they adopt are rather unique approaches. Jiampojamarn et al. (2009) propose DirectTL discriminative seq"
W09-3501,N03-1017,0,0.00241786,"umber of runs submitted for each task. Number of participants coincides with the number of standard runs submitted. evaluation results published for this edition of the transliteration shared task. Note that two teams have updated their results (after fixing bugs in their systems) after the deadline; their results are identified specifically. We find that two approaches to transliteration are most popular in the shared task submissions. One of these approaches is Phrase-based statistical machine transliteration (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al., 2003). Systems that adopted this approach are (Song, 2009; Haque et al., 2009; Noeman, 2009; Rama and Gali, 2009; Chinnakotla and Damani, 2009).1 The other is Conditional Random Fields(Lafferty et al., 2001) (CRF), adopted by (Aramaki and Abekawa, 2009; Shishtla et al., 2009). With only a few exceptions, most implementations are based on approaches that are language-independent. Indeed, many of the participants fielded their systems on multiple languages, as can be seen from Table 3. We also note that combination of several different models via re-ranking of their outputs (CRF, Maximum Entropy Mode"
W09-3501,I08-8003,0,0.0610299,"h to Russian English to Chinese EnKa EnRu 14 5 13 16 English to Kannada Table 2: Number of runs submitted for each task. Number of participants coincides with the number of standard runs submitted. evaluation results published for this edition of the transliteration shared task. Note that two teams have updated their results (after fixing bugs in their systems) after the deadline; their results are identified specifically. We find that two approaches to transliteration are most popular in the shared task submissions. One of these approaches is Phrase-based statistical machine transliteration (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al., 2003). Systems that adopted this approach are (Song, 2009; Haque et al., 2009; Noeman, 2009; Rama and Gali, 2009; Chinnakotla and Damani, 2009).1 The other is Conditional Random Fields(Lafferty et al., 2001) (CRF), adopted by (Aramaki and Abekawa, 2009; Shishtla et al., 2009). With only a few exceptions, most implementations are based on approaches that are language-independent. Indeed, many of the participants fielded their systems on multiple languages, as can be seen from Table 3. We also note that combination of sever"
W09-3501,W09-3506,0,0.0583655,"2009; Noeman, 2009; Rama and Gali, 2009; Chinnakotla and Damani, 2009).1 The other is Conditional Random Fields(Lafferty et al., 2001) (CRF), adopted by (Aramaki and Abekawa, 2009; Shishtla et al., 2009). With only a few exceptions, most implementations are based on approaches that are language-independent. Indeed, many of the participants fielded their systems on multiple languages, as can be seen from Table 3. We also note that combination of several different models via re-ranking of their outputs (CRF, Maximum Entropy Model, Margin Infused Relaxed Algorithm) proves to be very successful (Oh et al., 2009); their system (reported as Team ID 6) produced the best or second-best transliteration performance consistently across all metrics, in all tasks, except Japanese back-transliteration. Examples of other model combinations are (Das et al., 2009). At least two teams (reported as Team IDs 14 and 27) incorporate language origin detection in their system (Bose and Sarkar, 2009; Khapra and Bhattacharyya, 2009). The Indian language corpora contains names of both English and Indic origin. Khapra and Bhattacharyya (2009) demonstrate how much the transliteration performance can be improved when language"
W09-3501,P08-1045,0,0.0839769,"Missing"
W09-3501,W09-3528,0,0.020518,"bmitted. evaluation results published for this edition of the transliteration shared task. Note that two teams have updated their results (after fixing bugs in their systems) after the deadline; their results are identified specifically. We find that two approaches to transliteration are most popular in the shared task submissions. One of these approaches is Phrase-based statistical machine transliteration (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al., 2003). Systems that adopted this approach are (Song, 2009; Haque et al., 2009; Noeman, 2009; Rama and Gali, 2009; Chinnakotla and Damani, 2009).1 The other is Conditional Random Fields(Lafferty et al., 2001) (CRF), adopted by (Aramaki and Abekawa, 2009; Shishtla et al., 2009). With only a few exceptions, most implementations are based on approaches that are language-independent. Indeed, many of the participants fielded their systems on multiple languages, as can be seen from Table 3. We also note that combination of several different models via re-ranking of their outputs (CRF, Maximum Entropy Model, Margin Infused Relaxed Algorithm) proves to be very successful (Oh et al., 2009); their system (reported"
W09-3501,P04-1021,1,\N,Missing
W09-3501,W09-3512,0,\N,Missing
W09-3501,W09-3526,0,\N,Missing
W09-3501,P06-1103,0,\N,Missing
W09-3501,P07-1119,0,\N,Missing
W09-3501,P06-1010,0,\N,Missing
W09-3501,W09-3508,0,\N,Missing
W09-3501,W09-3514,0,\N,Missing
W09-3501,W06-1672,0,\N,Missing
W09-3501,D08-1037,0,\N,Missing
W09-3501,W09-3525,0,\N,Missing
W09-3501,W09-3530,0,\N,Missing
W09-3501,W09-3513,0,\N,Missing
W09-3501,W09-3523,0,\N,Missing
W09-3501,J98-4003,0,\N,Missing
W09-3501,W09-3518,0,\N,Missing
W09-3501,W09-3507,0,\N,Missing
W09-3501,W09-3504,0,\N,Missing
W09-3502,P04-1021,1,\N,Missing
W10-2401,P06-1103,0,0.0614407,"hou Li† , A Kumaran‡ , Min Zhang† and Vladimir Pervouchine† † Institute for Infocomm Research, A*STAR, Singapore 138632 {hli,mzhang,vpervouchine}@i2r.a-star.edu.sg ‡ Multilingual Systems Research, Microsoft Research India A.Kumaran@microsoft.com Abstract tems. Much research effort has been made to address the transliteration issue in the research community (Knight and Graehl, 1998; Meng et al., 2001; Li et al., 2004; Zelenko and Aone, 2006; Sproat et al., 2006; Sherif and Kondrak, 2007; Hermjakob et al., 2008; Al-Onaizan and Knight, 2002; Goldwasser and Roth, 2008; Goldberg and Elhadad, 2008; Klementiev and Roth, 2006; Oh and Choi, 2002; Virga and Khudanpur, 2003; Wan and Verspoor, 1998; Kang and Choi, 2000; Gao et al., 2004; Zelenko and Aone, 2006; Li et al., 2009b; Li et al., 2009a). These previous work fall into three categories, i.e., grapheme-based, phoneme-based and hybrid methods. Graphemebased method (Li et al., 2004) treats transliteration as a direct orthographic mapping and only uses orthography-related features while phonemebased method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration. Hybrid method refers to the combination of several different mod"
W10-2401,I08-8003,0,0.0263392,"ageindependent approach but added languagespecific pre- or post-processing (Jiampojamarn et al., 2010; Das et al., 2010; Song et al., 2010), including name origin recognition for English to Hindi task (Jiampojamarn et al., 2010). 5.1 Standard runs All the results are presented numerically in Tables 4–15, for all evaluation metrics. These are the official evaluation results published for this edition of the transliteration shared task. Among the four submitted system papers1 , Song et al. (2010) and Finch and Sumita (2010) adopt the approach of phrase-based statistical machine transliteration (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al., 2003) while Das et al. (2010) adopts the approach of Conditional Random Fields (CRF) (Lafferty et al., 2001). Jiampojamarn et al. (2010) further develop DirectTL approach presented at the previous NEWS workshop (Jiampojamarn et al., 2009), achieving very good performance in the NEWS 2010. An example of a completely languageCombination of different models via re-ranking of their outputs has been used in most of the systems (Das et al., 2010; Song et al., 2010; Finch and Sumita, 2010). In fact, one system (Song et al., 2010"
W10-2401,W10-2406,0,0.156882,"nKa EnJa EnKo JnJk ArAe EnBa Standard runs Non-standard runs 3 1 2 0 1 0 1 0 2 0 3 2 Table 2: Number of runs submitted for each task. Number of participants coincides with the number of standard runs submitted. Team Organisation ID 1∗ 2 3 4 5 6 7 IIT, Bombay University of Alberta City University Hong Kong NICT of EnCh ChEn EnTh ThEn EnHi EnTa EnKa EnJa EnKo JnJk ArAe EnBa x x x x x x x x x x x x x x x x x x x x x x x x x x x x x Jadavpur University x Table 3: Participation of teams in different tasks. ∗ Participation without a system paper. 5 Task Results and Analysis independent approach is (Finch and Sumita, 2010). Other participants used languageindependent approach but added languagespecific pre- or post-processing (Jiampojamarn et al., 2010; Das et al., 2010; Song et al., 2010), including name origin recognition for English to Hindi task (Jiampojamarn et al., 2010). 5.1 Standard runs All the results are presented numerically in Tables 4–15, for all evaluation metrics. These are the official evaluation results published for this edition of the transliteration shared task. Among the four submitted system papers1 , Song et al. (2010) and Finch and Sumita (2010) adopt the approach of phrase-based statis"
W10-2401,W10-2405,0,\N,Missing
W10-2401,C02-1099,0,\N,Missing
W10-2401,P04-1021,1,\N,Missing
W10-2401,W03-1508,0,\N,Missing
W10-2401,W02-0505,0,\N,Missing
W10-2401,P07-1119,0,\N,Missing
W10-2401,W09-3511,0,\N,Missing
W10-2401,P06-1010,0,\N,Missing
W10-2401,W09-3501,1,\N,Missing
W10-2401,P98-2220,0,\N,Missing
W10-2401,C98-2215,0,\N,Missing
W10-2401,W06-1672,0,\N,Missing
W10-2401,D08-1037,0,\N,Missing
W10-2401,P08-1045,0,\N,Missing
W10-2401,N03-1017,0,\N,Missing
W10-2401,W10-2409,0,\N,Missing
W10-2401,W10-2411,0,\N,Missing
W10-2401,J98-4003,0,\N,Missing
W10-2401,W09-3504,0,\N,Missing
W10-2402,P04-1021,1,0.854044,"Missing"
W11-3201,P08-1045,0,0.144492,"Missing"
W11-3201,W10-2401,1,0.257521,"ndence to generate the transliteration. Hybrid method refers to the combination of several different models or knowledge sources to support the transliteration generation. The first machine transliteration shared task (Li et al., 2009b; Li et al., 2009a) was held in NEWS 2009 at ACL-IJCNLP 2009. It was the first time to provide common benchmarking data in diverse language pairs for evaluation of state-of-the-art techniques. While the focus of the 2009 shared task was on establishing the quality metrics and on baselining the transliteration quality based on those metrics, the 2010 shared task (Li et al., 2010a; Li et al., 2010b) expanded the scope of the transliteration generation task to about a dozen languages, and explored the quality depending on the direction of transliteration, between the languages. NEWS 2011 was a continued effort of NEWS 2010 and NEWS 2009. The rest of the report is organised as follows. Section 2 outlines the machine transliteration task and the corpora used and Section 3 discusses the metrics chosen for evaluation, along with the ratioThis report documents the Machine Transliteration Shared Task conducted as a part of the Named Entities Workshop (NEWS 2011), an IJCNLP 2"
W11-3201,W11-3213,0,0.120956,"ery good performance. This system is based on phrase-based statistical machine transliteration (SMT) (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al., 2003), where the SMT system’s log-linear model is augmented with a set of features specifically suited to the task of transliteration. In particular, the model utilizes a feature based on a joint source-channel model, and a feature based on a maximum entropy model that predicts target grapheme sequences using the local context of graphemes and grapheme sequences in both source and target languages. Jiang et al. (2011) extensively explore the use of accessor variety (a similarity measure) of the source graphemes as a feature under CRF framework for machine transliteration and report promising results. Kruengkrai et al. (2011) study discriminative training based on the Margin Infused Relaxed Algorithm with simple character alignments under SMT framework for machine transliteration. They report very impressive results. Bhargava et al. (2011) attemp to improve transliteration performance by leveraging transliterations from multiple languages. Dasigi and Diab (2011) adopt the approach of phrase-based statistica"
W11-3201,W03-1508,0,0.23567,"maran‡ and Ming Liu † † Institute for Infocomm Research, A*STAR, Singapore 138632 {mzhang,hli,mliu}@i2r.a-star.edu.sg ‡ Multilingual Systems Research, Microsoft Research India A.Kumaran@microsoft.com Abstract tems. Much research effort has been made to address the transliteration issue in the research community (Knight and Graehl, 1998; Meng et al., 2001; Li et al., 2004; Zelenko and Aone, 2006; Sproat et al., 2006; Sherif and Kondrak, 2007; Hermjakob et al., 2008; Al-Onaizan and Knight, 2002; Goldwasser and Roth, 2008; Goldberg and Elhadad, 2008; Klementiev and Roth, 2006; Oh and Choi, 2002; Virga and Khudanpur, 2003; Wan and Verspoor, 1998; Kang and Choi, 2000; Gao et al., 2004; Zelenko and Aone, 2006; Li et al., 2009b; Li et al., 2009a). These previous work fall into three categories, i.e., grapheme-based, phoneme-based and hybrid methods. Graphemebased method (Li et al., 2004) treats transliteration as a direct orthographic mapping and only uses orthography-related features while phonemebased method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration. Hybrid method refers to the combination of several different models or knowledge sources to support the transl"
W11-3201,W11-3217,0,\N,Missing
W11-3201,W10-2402,1,\N,Missing
W11-3201,C02-1099,0,\N,Missing
W11-3201,P04-1021,1,\N,Missing
W11-3201,W02-0505,0,\N,Missing
W11-3201,P06-1103,0,\N,Missing
W11-3201,P07-1119,0,\N,Missing
W11-3201,P06-1010,0,\N,Missing
W11-3201,W09-3501,1,\N,Missing
W11-3201,P98-2220,0,\N,Missing
W11-3201,C98-2215,0,\N,Missing
W11-3201,W06-1672,0,\N,Missing
W11-3201,D08-1037,0,\N,Missing
W11-3201,N03-1017,0,\N,Missing
W11-3201,W11-3206,0,\N,Missing
W11-3201,W11-3205,0,\N,Missing
W11-3201,W11-3215,0,\N,Missing
W11-3201,W10-2411,0,\N,Missing
W11-3201,W11-3212,0,\N,Missing
W11-3201,W11-3216,0,\N,Missing
W11-3201,I08-8003,0,\N,Missing
W11-3201,J98-4003,0,\N,Missing
W11-3201,W11-3204,0,\N,Missing
W11-3201,W11-3203,0,\N,Missing
W11-3202,P04-1021,1,0.839488,"Missing"
W12-4401,P04-1021,1,\N,Missing
W12-4402,P06-1103,0,0.0603668,"source for name equivalence, the bilingual dictionaries — whether handcrafted or statistical — offer only limited support because new names always emerge. All of the above point to the critical need for robust Machine Transliteration technology and sys10 tems. Much research effort has been made to address the transliteration issue in the research community (Knight and Graehl, 1998; Meng et al., 2001; Li et al., 2004; Zelenko and Aone, 2006; Sproat et al., 2006; Sherif and Kondrak, 2007; Hermjakob et al., 2008; Al-Onaizan and Knight, 2002; Goldwasser and Roth, 2008; Goldberg and Elhadad, 2008; Klementiev and Roth, 2006; Oh and Choi, 2002; Virga and Khudanpur, 2003; Wan and Verspoor, 1998; Kang and Choi, 2000; Gao et al., 2004; Zelenko and Aone, 2006; Li et al., 2009b; Li et al., 2009a). These previous work fall into three categories, i.e., grapheme-based, phoneme-based and hybrid methods. Graphemebased method (Li et al., 2004) treats transliteration as a direct orthographic mapping and only uses orthography-related features while phonemebased method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration. Hybrid method refers to the combination of several different mod"
W12-4402,N03-1017,0,0.038172,"esented numerically in Tables 4–17, for all evaluation metrics. These are the official evaluation results published for this edition of the transliteration shared task. The methodologies used in the ten submitted system papers are summarized as follows. Similar to their NEWS 2011 system, Finch et al. (2012) employ non-Parametric Bayesian method to cosegment bilingual named entities for model training and report very good performance. This system is based on phrase-based statistical machine transliteration (SMT) (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al., 2003), where the SMT system’s log-linear model is augmented with a set of features specifically suited to the task of transliteration. In particular, the model utilizes a fea14 ture based on a joint source-channel model, and a feature based on a maximum entropy model that predicts target grapheme sequences using the local context of graphemes and grapheme sequences in both source and target languages. Different from their NEWS 2011 system, in order to solve the data sparseness issue, they use two RNN-based LM to project the grapheme set onto a smaller hidden representation: one for the target graph"
W12-4402,W12-4411,0,0.107555,"performance at English-Korean tasks. Okuno (2012) studies the mpaligner (an improvement of m2m-aligner) and shows that mpaligner is more effective than m2maligner. They also find that de-romanization is crucial to JnJk task and mora is the best alignment unit for EnJa task. Ammar et al. (2012) use CRF as the basic model but with two innovations: a training objective that optimizes toward any of a set of possible correct labels (i.e., multiple references) and a k-best reranking with non-local features. Their results on ArEn show that the two features are very effective in accuracy improvement. Kondrak et al. (2012) study the languagespecific adaptations in the context of two language pairs: English to Chinese (Pinyin representation) and Arabic to English (letter mapping). They conclude that Pinyin representation is useful while letter mapping is less effective. Kuo et al. (2012) explore two-stage CRF for Enligsh-to-Chinese task and show that the two-stage CRF outperform traditional one-stage CRF. 5.2 Non-standard runs For the non-standard runs, we pose no restrictions on the use of data or other linguistic resources. The purpose of non-standard runs is to see how best personal name transliteration can b"
W12-4402,W12-4412,0,0.233735,"tion Yuan Ze University CMU EnCh ChEn EnTh ThEn EnHi EnTa EnKa EnJa EnKo JnJk ArEn EnBa EnPe EnHe x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x Table 3: Participation of teams in different tasks. 5 5.1 Task Results and Analysis Standard runs All the results are presented numerically in Tables 4–17, for all evaluation metrics. These are the official evaluation results published for this edition of the transliteration shared task. The methodologies used in the ten submitted system papers are summarized as follows. Similar to their NEWS 2011 system, Finch et al. (2012) employ non-Parametric Bayesian method to cosegment bilingual named entities for model training and report very good performance. This system is based on phrase-based statistical machine transliteration (SMT) (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al., 2003), where the SMT system’s log-linear model is augmented with a set of features specifically suited to the task of transliteration. In particular, the model utilizes a fea14 ture based on a joint source-channel model, and a feature based on a maximum entropy model that predicts target graph"
W12-4402,P04-1021,1,0.956117,"he correct conversion of names between the languages in several studies (Demner-Fushman and Oard, 2002; Mandl and Womser-Hacker, 2005; Hermjakob et al., 2008; Udupa et al., 2009). The traditional source for name equivalence, the bilingual dictionaries — whether handcrafted or statistical — offer only limited support because new names always emerge. All of the above point to the critical need for robust Machine Transliteration technology and sys10 tems. Much research effort has been made to address the transliteration issue in the research community (Knight and Graehl, 1998; Meng et al., 2001; Li et al., 2004; Zelenko and Aone, 2006; Sproat et al., 2006; Sherif and Kondrak, 2007; Hermjakob et al., 2008; Al-Onaizan and Knight, 2002; Goldwasser and Roth, 2008; Goldberg and Elhadad, 2008; Klementiev and Roth, 2006; Oh and Choi, 2002; Virga and Khudanpur, 2003; Wan and Verspoor, 1998; Kang and Choi, 2000; Gao et al., 2004; Zelenko and Aone, 2006; Li et al., 2009b; Li et al., 2009a). These previous work fall into three categories, i.e., grapheme-based, phoneme-based and hybrid methods. Graphemebased method (Li et al., 2004) treats transliteration as a direct orthographic mapping and only uses orthograp"
W12-4402,D08-1037,0,0.0370038,"kob et al., 2008; Udupa et al., 2009). The traditional source for name equivalence, the bilingual dictionaries — whether handcrafted or statistical — offer only limited support because new names always emerge. All of the above point to the critical need for robust Machine Transliteration technology and sys10 tems. Much research effort has been made to address the transliteration issue in the research community (Knight and Graehl, 1998; Meng et al., 2001; Li et al., 2004; Zelenko and Aone, 2006; Sproat et al., 2006; Sherif and Kondrak, 2007; Hermjakob et al., 2008; Al-Onaizan and Knight, 2002; Goldwasser and Roth, 2008; Goldberg and Elhadad, 2008; Klementiev and Roth, 2006; Oh and Choi, 2002; Virga and Khudanpur, 2003; Wan and Verspoor, 1998; Kang and Choi, 2000; Gao et al., 2004; Zelenko and Aone, 2006; Li et al., 2009b; Li et al., 2009a). These previous work fall into three categories, i.e., grapheme-based, phoneme-based and hybrid methods. Graphemebased method (Li et al., 2004) treats transliteration as a direct orthographic mapping and only uses orthography-related features while phonemebased method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration. Hybrid me"
W12-4402,W10-2401,1,0.455916,"ndence to generate the transliteration. Hybrid method refers to the combination of several different models or knowledge sources to support the transliteration generation. The first machine transliteration shared task (Li et al., 2009b; Li et al., 2009a) was held in NEWS 2009 at ACL-IJCNLP 2009. It was the first time to provide common benchmarking data in diverse language pairs for evaluation of state-of-the-art techniques. While the focus of the 2009 shared task was on establishing the quality metrics and on baselining the transliteration quality based on those metrics, the 2010 shared task (Li et al., 2010a; Li et al., 2010b) expanded the scope of the transliteration generation task to about a dozen languages, and explored the quality depending on the direction of transliteration, between the languages. In NEWS 2011 (Zhang et al., 2011a; Zhang et al., 2011b), we significantly increased the hand-crafted parallel named entities corpora to include 14 different language pairs from 11 language families, and made them available as the common dataset for the shared task. NEWS 2012 was a continued effort of NEWS 2011, NEWS Proceedings of the 50th Annual Meeting of the Association for Computational Ling"
W12-4402,W11-3201,1,0.256256,"009b; Li et al., 2009a) was held in NEWS 2009 at ACL-IJCNLP 2009. It was the first time to provide common benchmarking data in diverse language pairs for evaluation of state-of-the-art techniques. While the focus of the 2009 shared task was on establishing the quality metrics and on baselining the transliteration quality based on those metrics, the 2010 shared task (Li et al., 2010a; Li et al., 2010b) expanded the scope of the transliteration generation task to about a dozen languages, and explored the quality depending on the direction of transliteration, between the languages. In NEWS 2011 (Zhang et al., 2011a; Zhang et al., 2011b), we significantly increased the hand-crafted parallel named entities corpora to include 14 different language pairs from 11 language families, and made them available as the common dataset for the shared task. NEWS 2012 was a continued effort of NEWS 2011, NEWS Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 10–20, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics 2010 and NEWS 2009. The rest of the report is organised as follows. Section 2 outlines the machine transliteration task an"
W12-4402,W12-4410,0,\N,Missing
W12-4402,W10-2402,1,\N,Missing
W12-4402,C02-1099,0,\N,Missing
W12-4402,W03-1508,0,\N,Missing
W12-4402,W02-0505,0,\N,Missing
W12-4402,P07-1119,0,\N,Missing
W12-4402,P06-1010,0,\N,Missing
W12-4402,W09-3501,1,\N,Missing
W12-4402,P98-2220,0,\N,Missing
W12-4402,C98-2215,0,\N,Missing
W12-4402,W06-1672,0,\N,Missing
W12-4402,P08-1045,0,\N,Missing
W12-4402,W12-4408,0,\N,Missing
W12-4402,W11-3202,1,\N,Missing
W12-4402,W12-4409,0,\N,Missing
W12-4402,W11-3216,0,\N,Missing
W12-4402,I08-8003,0,\N,Missing
W12-4402,W12-4407,0,\N,Missing
W12-4402,J98-4003,0,\N,Missing
W12-4402,W12-4406,0,\N,Missing
W12-4402,W11-3203,0,\N,Missing
W15-2504,W12-3156,0,0.0920358,".arbylon.net/projects/ 34 (2) 3 Cohesion Score based on Simplified Lexical Chain Text adequacy is the most important standard for the purpose of successful communication. According to the work of Wong and Kit (2012), cohesion is another important element to organize text. They found: SMT systems tend to use less lexical cohesion devices than those of human translators. Here lexical cohesion devices mainly refer to content words reiterating once or more times in a document. They propose to build document-level MT metrics by integrating cohesion score based on lexical cohesion devices. However, Carpuat and Simard (2012) draw a different conclusion: MT output tend to have more incorrect repetition than human translation when the MT model is especially trained on smaller corpora. Suppose these incorrect repetition as “false” cohesion, metrics in (Wong and Kit, 2012) will fail to distinguish such ”false” cohesion devices. In our opinion, the lack of Wong’s work is completely ignoring text cohesion of references, and they only model the cohesion score of MT output. In this study, we assume the correct cohesion of MT output should be consistent with the one of references. Reference is the equivalent of its source"
W15-2504,N07-1006,0,0.0212351,"This method is also adopted by famous MetricsMaTr (the NIST Metrics for Machine Translation Challenge) and approximated in Gimenez et al. (2010) and Wong and Kit (2012). (6) Finally, the METEOR score is obtained as follows: score = (1 − pen)Fmean . (8) where Gmdoc refers to document-level BLEU or METEOR score (one score per document), Smdoc to gist consistency score(Stopic ) or text cohesion score(Doccs ) proposed in this paper. α and β are weights which are tuned on MTC2 evaluation dataset (see Section 5.1) by a gradient ascending algorithm with the optimum goal of maximum correlation value (Liu and Gildea, 2007). where pn is the precision of n-gram and BP is a penalty factor, preventing BLEU from favoring short segments due to the lack of direct consideration of recall. It is obvious that, although BLEU takes all n-grams into consideration, the importance of different n-grams is ignored except their lengths. METEOR is based on unigram alignment of references and MT output. Each unigram in one system translation is at most mapped to one unigram in the references first and then three successive stages of “exact”, “porter stem” and “WN synonymy” are used to create alignment in turn. Once the final align"
W15-2504,J91-1002,0,0.749156,"Missing"
W15-2504,P02-1040,0,0.0961081,"Missing"
W15-2504,W12-3117,0,0.0348898,"Missing"
W15-2504,D11-1084,1,0.93525,"Missing"
W15-2504,2013.mtsummit-posters.13,0,0.0424254,"Missing"
W15-2504,2006.amta-papers.20,0,0.0353304,"can be tuned with the minimal perplexity (Blei et al., 2003). 2.2 After constructing a trained topic model, the “document-topic” distribution of MT output and reference on evaluation dataset (see Section 5.1) can be respectively inferred. We use Kullback-Leibler divergence to measure topic consistency between MT output and reference with the basic unit of document. Denote the “document-topic” distribution of one reference (dr ) as P (Z|dr ), and the one of its MT output (dt ) as Q(Z|dt ), the KL divergence of Q from P is defined to be: Gist Consistency Score based on Topic Model DKL (P ||Q) = Reeder (2006) proposes to measure MT adequacy at the document level with Latent Semantic Analysis (LSA) (Landauer et al., 1998). However, Reeder only uses a set of complex configuration to show the close correlation between LSA model and human assessments and does not suggest how to use it to design an evaluation metric. Raphael et al. (2012; 2013) exploit bilingual topic models to do quality estimation (without references) for machine translation. In this study, since each evaluation document has 4 references, we show a simple way to design document-level metrics with monolingual topic model. 2.1 G X P (z"
W15-2504,W10-2602,0,0.448563,"Missing"
W15-2504,2011.mtsummit-papers.13,0,0.085941,"Missing"
W15-2504,P12-1079,1,0.906029,"Missing"
W15-2504,D13-1163,1,0.898249,"Missing"
W15-2504,D12-1097,0,0.663858,"vements by using system-level metrics, such as BLEU (Papineni et al., 2002). Whether improvements in performance at system level are really able to reflect the change of text-level translation quality is still to doubt. Nowadays, the study of real document-level MT metrics has been drawing more and more attention. Based on Discourse Representation Theory (Kamp and Reyle, 1993), Gimenez et al. (2010) propose to use co-reference and discourse relations to build evaluation metrics. The metrics by extending traditional metrics with lexical cohesion devices show some positive experimental results (Wong and Kit, 2012). Bilingual topic model (Blei et al., 2003) is applied to do MT quality estimation(Raphael et al., 2012; Raphael et al, 2013). Guzman et al. (2014) use two discourse-aware similarity measures based on discourse structure to improve existing MT evaluation metrics. According to the afore-mentioned definition of text, the most important standard of evaluating translation quality for one document should be to what degree the MT output correctly communicates the main idea of origin text. From this regard, this paper first proposes to measure gist consistency of text via topic model. Topic model is"
W15-2504,W10-1750,0,\N,Missing
W15-2504,N12-1046,0,\N,Missing
W15-2504,W05-0909,0,\N,Missing
W15-2504,C04-1046,0,\N,Missing
W15-2504,P14-1065,0,\N,Missing
W15-2504,D12-1108,0,\N,Missing
W15-3901,P04-1021,1,\N,Missing
W15-3902,J98-4003,0,0.37313,"2015 Machine Transliteration Shared Task Rafael E. Banchs1, Min Zhang2, Xiangyu Duan2, Haizhou Li1, A. Kumaran3 1 Institute for Infocomm Research, A*STAR, Singapore 138632 {rembanchs,hli}@i2r.a-star.edu.sg 2 Soochow University, China 215006 {minzhang,xiangyuduan}@suda.edu.cn 3 Multilingual Systems Research, Microsoft Research India a.kumaran@microsoft.com All of the above points to the critical need for robust Machine Transliteration methods and systems. During the last decade, significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, Al-Onaizan and Knight 2002, Goldwasser and Roth 2008, Goldberg and Elhadad 2008, Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004, Li et al. 2009a, Li et al. 2009b). These previous works fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Grapheme based methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthography-re"
W15-3902,P07-2045,0,0.00686568,"Missing"
W15-3902,P07-1119,0,0.0443311,"A. Kumaran3 1 Institute for Infocomm Research, A*STAR, Singapore 138632 {rembanchs,hli}@i2r.a-star.edu.sg 2 Soochow University, China 215006 {minzhang,xiangyuduan}@suda.edu.cn 3 Multilingual Systems Research, Microsoft Research India a.kumaran@microsoft.com All of the above points to the critical need for robust Machine Transliteration methods and systems. During the last decade, significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, Al-Onaizan and Knight 2002, Goldwasser and Roth 2008, Goldberg and Elhadad 2008, Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004, Li et al. 2009a, Li et al. 2009b). These previous works fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Grapheme based methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthography-related features while phoneme-based methods (Knight and Graehl 1998) make use of phonetic correspondenc"
W15-3902,P06-1010,0,0.0359544,"Duan2, Haizhou Li1, A. Kumaran3 1 Institute for Infocomm Research, A*STAR, Singapore 138632 {rembanchs,hli}@i2r.a-star.edu.sg 2 Soochow University, China 215006 {minzhang,xiangyuduan}@suda.edu.cn 3 Multilingual Systems Research, Microsoft Research India a.kumaran@microsoft.com All of the above points to the critical need for robust Machine Transliteration methods and systems. During the last decade, significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, Al-Onaizan and Knight 2002, Goldwasser and Roth 2008, Goldberg and Elhadad 2008, Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004, Li et al. 2009a, Li et al. 2009b). These previous works fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Grapheme based methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthography-related features while phoneme-based methods (Knight and Graehl 1998) make use"
W15-3902,W15-3912,0,0.127502,"Missing"
W15-3902,W15-3911,0,0.0752468,"Missing"
W15-3902,C02-1099,0,0.0658677,"gyuduan}@suda.edu.cn 3 Multilingual Systems Research, Microsoft Research India a.kumaran@microsoft.com All of the above points to the critical need for robust Machine Transliteration methods and systems. During the last decade, significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, Al-Onaizan and Knight 2002, Goldwasser and Roth 2008, Goldberg and Elhadad 2008, Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004, Li et al. 2009a, Li et al. 2009b). These previous works fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Grapheme based methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthography-related features while phoneme-based methods (Knight and Graehl 1998) make use of phonetic correspondences to generate the transliteration. The hybrid approach refers to the combination of several different models or knowledge sources to support the tr"
W15-3902,N10-1103,0,\N,Missing
W15-3902,W10-2402,1,\N,Missing
W15-3902,P04-1021,1,\N,Missing
W15-3902,W03-1508,0,\N,Missing
W15-3902,W02-0505,0,\N,Missing
W15-3902,N07-1047,0,\N,Missing
W15-3902,P06-1103,0,\N,Missing
W15-3902,W10-2401,1,\N,Missing
W15-3902,P98-2220,0,\N,Missing
W15-3902,C98-2215,0,\N,Missing
W15-3902,W06-1672,0,\N,Missing
W15-3902,D08-1037,0,\N,Missing
W15-3902,P08-1045,0,\N,Missing
W15-3902,W11-3202,1,\N,Missing
W15-3902,W15-3913,0,\N,Missing
W15-3902,W15-3909,0,\N,Missing
W15-3902,W11-3201,1,\N,Missing
W15-3902,W09-3504,0,\N,Missing
W16-2708,P04-1021,1,0.668693,"Missing"
W16-2709,W16-2713,0,0.0529993,"Missing"
W16-2709,W15-3912,0,0.0431505,"Missing"
W16-2709,W16-2711,0,0.147552,"Missing"
W16-2709,P04-1021,1,0.691542,"Xiangyu Duan2,Rafael E. Banchs1, Min Zhang2, Haizhou Li1, A. Kumaran3 1 Institute for Infocomm Research, A*STAR, Singapore 138632 {rembanchs,hli}@i2r.a-star.edu.sg 2 Soochow University, China 215006 {xiangyuduan,minzhang}@suda.edu.cn 3 Multilingual Systems Research, Microsoft Research India a.kumaran@microsoft.com All of the above points to the critical need for robust Machine Transliteration methods and systems. During the last decade, significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, Al-Onaizan and Knight 2002,Goldwasser and Roth 2008, Goldberg and Elhadad 2008,Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004,Li et al. 2009a, Li et al. 2009b). These previous works fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Graphemebased methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthography-related features while phoneme-based met"
W16-2709,D08-1037,0,0.0235887,"mbanchs,hli}@i2r.a-star.edu.sg 2 Soochow University, China 215006 {xiangyuduan,minzhang}@suda.edu.cn 3 Multilingual Systems Research, Microsoft Research India a.kumaran@microsoft.com All of the above points to the critical need for robust Machine Transliteration methods and systems. During the last decade, significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, Al-Onaizan and Knight 2002,Goldwasser and Roth 2008, Goldberg and Elhadad 2008,Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004,Li et al. 2009a, Li et al. 2009b). These previous works fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Graphemebased methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthography-related features while phoneme-based methods (Knight and Graehl 1998) make use of phonetic correspondences to generate the transliteration. The hybrid approach refers to the combinatio"
W16-2709,W10-2401,1,0.722231,"rs to the combination of several different models or knowledge sources to support the transliteration generation process. The first machine transliteration shared task (Li et al. 2009b, Li et al. 2009a) was organized and conducted aspart of NEWS 2009 at ACLIJCNLP 2009. It was the first time that common benchmarking data in diverse language pairs was provided for evaluating state-of-the-art machine transliteration. While the focus of the 2009 shared task was on establishing the quality metrics and on setting up a baselinefor transliteration quality based on those metrics, the 2010 shared task (Li et al. 2010a, Li et al. 2010b) focused on expanding the scope of the transliteration generation task to about a dozen languages and on exploring the quality of the task depending on the direction of transliteration. In NEWS 2011 (Zhang et al. 2011a, Zhang et al. 2011b), Abstract This report presents the results from the Machine Transliteration Shared Task conducted as part of The Sixth Named Entities Workshop (NEWS 2016) held at ACL 2016in Berlin, Germany. Similar to previous editions of NEWS Workshop, the Shared Task featured machine transliteration of proper names over 14 different language pairs, incl"
W16-2709,W09-3504,0,0.0634137,"Missing"
W16-2709,W15-3911,0,0.117942,"Missing"
W16-2709,N10-1103,0,0.0421265,"Missing"
W16-2709,C02-1099,0,0.0830346,",minzhang}@suda.edu.cn 3 Multilingual Systems Research, Microsoft Research India a.kumaran@microsoft.com All of the above points to the critical need for robust Machine Transliteration methods and systems. During the last decade, significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, Al-Onaizan and Knight 2002,Goldwasser and Roth 2008, Goldberg and Elhadad 2008,Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004,Li et al. 2009a, Li et al. 2009b). These previous works fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Graphemebased methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthography-related features while phoneme-based methods (Knight and Graehl 1998) make use of phonetic correspondences to generate the transliteration. The hybrid approach refers to the combination of several different models or knowledge sources to support the tran"
W16-2709,W16-2710,0,0.20966,"Missing"
W16-2709,P06-1103,0,0.0441788,"China 215006 {xiangyuduan,minzhang}@suda.edu.cn 3 Multilingual Systems Research, Microsoft Research India a.kumaran@microsoft.com All of the above points to the critical need for robust Machine Transliteration methods and systems. During the last decade, significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, Al-Onaizan and Knight 2002,Goldwasser and Roth 2008, Goldberg and Elhadad 2008,Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004,Li et al. 2009a, Li et al. 2009b). These previous works fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Graphemebased methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthography-related features while phoneme-based methods (Knight and Graehl 1998) make use of phonetic correspondences to generate the transliteration. The hybrid approach refers to the combination of several different models or knowledge sources t"
W16-2709,P07-1119,0,0.0158206,"A. Kumaran3 1 Institute for Infocomm Research, A*STAR, Singapore 138632 {rembanchs,hli}@i2r.a-star.edu.sg 2 Soochow University, China 215006 {xiangyuduan,minzhang}@suda.edu.cn 3 Multilingual Systems Research, Microsoft Research India a.kumaran@microsoft.com All of the above points to the critical need for robust Machine Transliteration methods and systems. During the last decade, significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, Al-Onaizan and Knight 2002,Goldwasser and Roth 2008, Goldberg and Elhadad 2008,Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004,Li et al. 2009a, Li et al. 2009b). These previous works fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Graphemebased methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthography-related features while phoneme-based methods (Knight and Graehl 1998) make use of phonetic correspondences t"
W16-2709,P06-1010,0,0.0387549,"hang2, Haizhou Li1, A. Kumaran3 1 Institute for Infocomm Research, A*STAR, Singapore 138632 {rembanchs,hli}@i2r.a-star.edu.sg 2 Soochow University, China 215006 {xiangyuduan,minzhang}@suda.edu.cn 3 Multilingual Systems Research, Microsoft Research India a.kumaran@microsoft.com All of the above points to the critical need for robust Machine Transliteration methods and systems. During the last decade, significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, Al-Onaizan and Knight 2002,Goldwasser and Roth 2008, Goldberg and Elhadad 2008,Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004,Li et al. 2009a, Li et al. 2009b). These previous works fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Graphemebased methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthography-related features while phoneme-based methods (Knight and Graehl 1998) make use of p"
W16-2709,P98-2220,0,0.225327,"s Research, Microsoft Research India a.kumaran@microsoft.com All of the above points to the critical need for robust Machine Transliteration methods and systems. During the last decade, significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, Al-Onaizan and Knight 2002,Goldwasser and Roth 2008, Goldberg and Elhadad 2008,Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004,Li et al. 2009a, Li et al. 2009b). These previous works fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Graphemebased methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthography-related features while phoneme-based methods (Knight and Graehl 1998) make use of phonetic correspondences to generate the transliteration. The hybrid approach refers to the combination of several different models or knowledge sources to support the transliteration generation process. The first machine"
W16-2709,W15-3910,0,0.0327487,"Missing"
W16-2709,W15-3913,0,0.0273111,"Missing"
W16-2709,W06-1672,0,0.0300352,"afael E. Banchs1, Min Zhang2, Haizhou Li1, A. Kumaran3 1 Institute for Infocomm Research, A*STAR, Singapore 138632 {rembanchs,hli}@i2r.a-star.edu.sg 2 Soochow University, China 215006 {xiangyuduan,minzhang}@suda.edu.cn 3 Multilingual Systems Research, Microsoft Research India a.kumaran@microsoft.com All of the above points to the critical need for robust Machine Transliteration methods and systems. During the last decade, significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, Al-Onaizan and Knight 2002,Goldwasser and Roth 2008, Goldberg and Elhadad 2008,Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004,Li et al. 2009a, Li et al. 2009b). These previous works fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Graphemebased methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthography-related features while phoneme-based methods (Knight and Graehl"
W16-2709,W03-1508,0,\N,Missing
W16-2709,W02-0505,0,\N,Missing
W16-2709,N07-1047,0,\N,Missing
W16-2709,P07-2045,0,\N,Missing
W16-2709,C98-2215,0,\N,Missing
W16-2709,W11-3202,1,\N,Missing
W16-2709,W12-4402,1,\N,Missing
W16-2709,J98-4003,0,\N,Missing
W16-2709,W11-3201,1,\N,Missing
W18-2408,P04-1021,1,0.453791,"Missing"
W18-2409,W18-2414,0,0.0493369,"Missing"
W18-2409,P04-1021,1,0.614661,"en1, Rafael E. Banchs2, Min Zhang3, Xiangyu Duan3, Haizhou Li4 1 Singapore University of Technology and Design, Singapore nancychen@alum.mit.edu 2 Nanyang Technological University, Singapore rbanchs@ntu.edu.sg 3 Soochow University, China {minzhang,xiangyuduan}@suda.edu.cn 4 National University of Singapore, Singapore haizhou.li@nus.edu.sg All of the above points to the critical need for robust machine transliteration methods and systems. Significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, AlOnaizan and Knight 2002, Goldwasser and Roth 2008, Goldberg and Elhadad 2008, Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004, Li et al. 2009a, Li et al. 2009b). These efforts fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Grapheme based methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthographyrelated features while phoneme-based methods"
W18-2409,W10-2401,1,0.712599,"ently, neural network approaches have been explored with varying successes, depending on the size of the training data. The first machine transliteration shared task (Li et al. 2009a, Li et al. 2009b) was organized and conducted as part of NEWS 2009 at ACLIJCNLP 2009. It was the first time that common benchmarking data in diverse language pairs was provided for evaluating state-of-the-art machine transliteration. While the focus of the 2009 shared task was on establishing the quality metrics and on setting up a baseline for transliteration quality based on those metrics, the 2010 shared task (Li et al. 2010a, Li et al. 2010b) foAbstract This report presents the results from the Named Entity Transliteration Shared Task conducted as part of The Seventh Named Entities Workshop (NEWS 2018) held at ACL 2018 in Melbourne, Australia. Similar to previous editions of NEWS, the Shared Task featured 19 tasks on proper name transliteration, including 13 different languages and two different Japanese scripts. A total of 6 teams from 8 different institutions participated in the evaluation, submitting 424 runs, involving different transliteration methodologies. Four performance metrics were used to report the"
W18-2409,D08-1037,0,0.0247158,"um.mit.edu 2 Nanyang Technological University, Singapore rbanchs@ntu.edu.sg 3 Soochow University, China {minzhang,xiangyuduan}@suda.edu.cn 4 National University of Singapore, Singapore haizhou.li@nus.edu.sg All of the above points to the critical need for robust machine transliteration methods and systems. Significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, AlOnaizan and Knight 2002, Goldwasser and Roth 2008, Goldberg and Elhadad 2008, Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004, Li et al. 2009a, Li et al. 2009b). These efforts fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Grapheme based methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthographyrelated features while phoneme-based methods (Knight and Graehl 1998) make use of phonetic correspondences to generate the transliteration. The hybrid approach refers to the combination of"
W18-2409,P17-4012,0,0.0145929,"e-based machine translation). All other systems used some version of neural modeling. It is interesting to note that non-neural systems by SINGA, while not the highest in performance, are generally comparable to neural systems or system combinations which include neural models. Regarding the systems participating in this year evaluation, the UALB’s system (Najafi et al. 2018) was based on multiple system combinations. They presented experimental results involving five different well-known transliteration approaches: DirecTL+ (Jiampojamarn et al. 2009), Sequitur (Bisani and Ney 2008), OpenNMT (Klein et al. 2017), BaseNMT (Sutskever et al. 2014), and RL-NMT (Najafi et al., 2018). They T)EnPe 0 WIPO 0.2 0.4 SINGA UQAM 0.6 UJUS 0.8 EDI 1 UALB Figure 1: Mean F-scores (Top-1) on the evaluation set for all primary submissions and tasks. The UJUS system (Kundu et al. 2018) used an RNN-based NMT framework and a CNN-based NMT framework, where both byte-pair encoding and character-based segmentation were employed for both cases. They also adopted an ensemble method to choose the hypothesis that has the highest frequency of occurrence to further improve accuracy. The EDI system (Grundkiewicz et al. 2018) system"
W18-2409,C02-1099,0,0.129101,"oochow University, China {minzhang,xiangyuduan}@suda.edu.cn 4 National University of Singapore, Singapore haizhou.li@nus.edu.sg All of the above points to the critical need for robust machine transliteration methods and systems. Significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, AlOnaizan and Knight 2002, Goldwasser and Roth 2008, Goldberg and Elhadad 2008, Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004, Li et al. 2009a, Li et al. 2009b). These efforts fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Grapheme based methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthographyrelated features while phoneme-based methods (Knight and Graehl 1998) make use of phonetic correspondences to generate the transliteration. The hybrid approach refers to the combination of several different models or knowledge sources to support the transliter"
W18-2409,P06-1103,0,0.0560026,"ore rbanchs@ntu.edu.sg 3 Soochow University, China {minzhang,xiangyuduan}@suda.edu.cn 4 National University of Singapore, Singapore haizhou.li@nus.edu.sg All of the above points to the critical need for robust machine transliteration methods and systems. Significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, AlOnaizan and Knight 2002, Goldwasser and Roth 2008, Goldberg and Elhadad 2008, Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004, Li et al. 2009a, Li et al. 2009b). These efforts fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Grapheme based methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthographyrelated features while phoneme-based methods (Knight and Graehl 1998) make use of phonetic correspondences to generate the transliteration. The hybrid approach refers to the combination of several different models or knowledge sources to supp"
W18-2409,P07-1119,0,0.0417746,"Li4 1 Singapore University of Technology and Design, Singapore nancychen@alum.mit.edu 2 Nanyang Technological University, Singapore rbanchs@ntu.edu.sg 3 Soochow University, China {minzhang,xiangyuduan}@suda.edu.cn 4 National University of Singapore, Singapore haizhou.li@nus.edu.sg All of the above points to the critical need for robust machine transliteration methods and systems. Significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, AlOnaizan and Knight 2002, Goldwasser and Roth 2008, Goldberg and Elhadad 2008, Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004, Li et al. 2009a, Li et al. 2009b). These efforts fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Grapheme based methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthographyrelated features while phoneme-based methods (Knight and Graehl 1998) make use of phonetic correspondences to gen"
W18-2409,P07-2045,0,0.0067284,"Missing"
W18-2409,P06-1010,0,0.0603142,"ngyu Duan3, Haizhou Li4 1 Singapore University of Technology and Design, Singapore nancychen@alum.mit.edu 2 Nanyang Technological University, Singapore rbanchs@ntu.edu.sg 3 Soochow University, China {minzhang,xiangyuduan}@suda.edu.cn 4 National University of Singapore, Singapore haizhou.li@nus.edu.sg All of the above points to the critical need for robust machine transliteration methods and systems. Significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, Meng et al. 2001, Li et al. 2004, Zelenko and Aone 2006, Sproat et al. 2006, Sherif and Kondrak 2007, Hermjakob et al. 2008, AlOnaizan and Knight 2002, Goldwasser and Roth 2008, Goldberg and Elhadad 2008, Klementiev and Roth 2006, Oh and Choi 2002, Virga and Khudanpur 2003, Wan and Verspoor 1998, Kang and Choi 2000, Gao et al. 2004, Li et al. 2009a, Li et al. 2009b). These efforts fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Grapheme based methods (Li et al. 2004) treat transliteration as a direct orthographic mapping and only uses orthographyrelated features while phoneme-based methods (Knight and Graehl 1998) make use of phonet"
Y12-1061,N10-1083,0,0.328638,"ined: 3 Feature-based CCM where K(S|w) is independent of B and the following production is taken over tree spans only. One advantage of the locally normalized model is that the EM algorithm could be still used to estimate parameters, which will be described in the next subsection. If we define the same factors of CCM (sequence and context for constituent and distituent) and set weights properly, then the probability of featurebased model is degenerated to the original CCM model. So the original CCM can be treated as a special case of the feature-based model. 3.1 Model Definition Motivated by (Berg-Kirkpatrick et al., 2010), we define factors in the log-linear form with local normalization. Let F1,...,K be K different factors. Each factor Fk corresponds to a nk -dimensional feature vector fk and a nk -dimensional weight vector wk . For the kth factor Fk , the corresponding multinomial parameter in original CCM is now treated as a function of weights wk . Define the factor category function δk to be +1 if Fk is constituent factor, and −1 otherwise. In detail, for span hi, ji in some bracketing B for sentence S, define Fk (Shi,ji |wk ) = Pk (Shi,ji |Bhi,ji = δk , wk ) exp(wk · fk (Shi,ji )) (2) =P v exp(wk · fk (v"
Y12-1061,D10-1117,0,0.540973,"long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentences, its performance drops for longer sentences. There are two possible reasons: (1) CCM models all"
Y12-1061,P06-1109,0,0.13074,"significant improvement on longer sentences. 1 Introduction Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to est"
Y12-1061,A00-2018,0,0.0251888,"e also used for the proposed FCCM, which we plan to do in future work. Klein and Manning (2004) demonstrate the joint model of constituency and dependency could improve unsupervised grammar inference. Some other approaches also consider to use additional information such as the words (Headden III et al., 2009), the automatic induced tags (Headden III et al., 2008), or the parent information (Mirroshandel and Ghassem-Sani, 2008). These information could be easily incorporated to the proposed model as features. Feature-based models have been widely used in many supervised tasks such as parsing (Charniak, 2000), word alignment (Moore, 2005; Liu et al., 2006), machine translation (Koehn et al., 2003), etc. For the unsupervised learning tasks, the calculation of normalization part is usually time-consuming or even impossible. Existing approaches are mainly based on the contrastive estimation (Smith and Eisner, 2005; Smith and Eisner, 2005; Dyer et al., 2011) to learn parameters. The local normalized featurebased model has been proposed in (Berg-Kirkpatrick et al., 2010), in which features are defined over generative rules and the normalization is done locally. They use the ℓ2 regularization, while we"
Y12-1061,N09-1009,0,0.0632485,"strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentences, its performance drops for longer sentences. T"
Y12-1061,N09-1062,0,0.0965438,"n longer sentences. 1 Introduction Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the d"
Y12-1061,D11-1018,0,0.0137879,"amework, we could automatically choose suitable model parameters rather than setting them empirically. Experiments on the English treebank demonstrate that the feature-based model achieves comparable performance on short sentences but significant improvement on longer sentences. 1 Introduction Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency g"
Y12-1061,P11-1042,0,0.0215671,"III et al., 2008), or the parent information (Mirroshandel and Ghassem-Sani, 2008). These information could be easily incorporated to the proposed model as features. Feature-based models have been widely used in many supervised tasks such as parsing (Charniak, 2000), word alignment (Moore, 2005; Liu et al., 2006), machine translation (Koehn et al., 2003), etc. For the unsupervised learning tasks, the calculation of normalization part is usually time-consuming or even impossible. Existing approaches are mainly based on the contrastive estimation (Smith and Eisner, 2005; Smith and Eisner, 2005; Dyer et al., 2011) to learn parameters. The local normalized featurebased model has been proposed in (Berg-Kirkpatrick et al., 2010), in which features are defined over generative rules and the normalization is done locally. They use the ℓ2 regularization, while we apply the local-normalization model to CCM with ℓ1 regularization and show improved performance could be achieved with sparse solution. Many unsupervised approaches aim to learn compact and sparse grammar, including the Bayesian models (Johnson et al., 2007; Cohn et al., 2010; Blunsom and Cohn, 2010) and posterior regularization (Ganchev et al., 2010"
Y12-1061,P12-2004,0,0.315104,"stimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentences, its performance drops for longer sentences. There are two possible reasons: (1) CCM models all constituents under only single multinomial distributions, which can not capture the detailed information of span contents; and (2) long sequences only occur a few times in the training corpus, so the probability estimation highly depends on smoothing. Another problem of original CCM and following improved unsupervised models (Smith and Eisner, 2004; Mirroshandel and Ghassem-Sani, 2008; Golland et al., 2012) is the problematic evaluation framework. The previous approaches train and evaluate models on the same dataset, so there is no reasonable way to choose model parameters unless setting them empirically. In this paper, we focus on CCM and present a general feature-based framework in which various overlapping features could be easily added. Previous dependency induction approach (Cohen and Copyright 2012 by Yun Huang, Min Zhang, and Chew Lim Tan 26th Pacific Asia Conference on Language,Information and Computation pages 564–573 Smith, 2009) demonstrates enabling factored covariance between the pr"
Y12-1061,C08-1042,0,0.0316883,"Missing"
Y12-1061,N09-1012,0,0.401925,"Missing"
Y12-1061,N10-1074,0,0.120137,"est set to evaluate the performance. Under this framework, we could automatically choose suitable model parameters rather than setting them empirically. Experiments on the English treebank demonstrate that the feature-based model achieves comparable performance on short sentences but significant improvement on longer sentences. 1 Introduction Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but"
Y12-1061,P02-1017,0,0.731217,"ments on the English treebank demonstrate that the feature-based model achieves comparable performance on short sentences but significant improvement on longer sentences. 1 Introduction Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the precedin"
Y12-1061,P04-1061,0,0.791288,"task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentenc"
Y12-1061,N03-1017,0,0.0065762,"Missing"
Y12-1061,P06-1077,0,0.010649,"an to do in future work. Klein and Manning (2004) demonstrate the joint model of constituency and dependency could improve unsupervised grammar inference. Some other approaches also consider to use additional information such as the words (Headden III et al., 2009), the automatic induced tags (Headden III et al., 2008), or the parent information (Mirroshandel and Ghassem-Sani, 2008). These information could be easily incorporated to the proposed model as features. Feature-based models have been widely used in many supervised tasks such as parsing (Charniak, 2000), word alignment (Moore, 2005; Liu et al., 2006), machine translation (Koehn et al., 2003), etc. For the unsupervised learning tasks, the calculation of normalization part is usually time-consuming or even impossible. Existing approaches are mainly based on the contrastive estimation (Smith and Eisner, 2005; Smith and Eisner, 2005; Dyer et al., 2011) to learn parameters. The local normalized featurebased model has been proposed in (Berg-Kirkpatrick et al., 2010), in which features are defined over generative rules and the normalization is done locally. They use the ℓ2 regularization, while we apply the local-normalization model to CCM with"
Y12-1061,J93-2004,0,0.0397779,"res and constant feature. 5 Experiments 5.1 5.2 Induction Results EM algorithm is sensitive to the initial condition. We adopt the same uniform-split initialization and the same smoothing values (2 for constituents and 8 for distituents) as described in (Klein, 2005). For feature-based model (F-CCM), we still use uniformsplit strategy to initialize probabilities in the first Estep, and set all weights to zero as the initial point of the gradient-based search algorithm in the M-step. Datasets and Settings We carry out experiments on the Wall Street Journal portion of the Penn English Treebank (Marcus et al., 1993). We report the unlabeled F1 score (the harmonic mean of precision and recall) as evaluation metric. Constituents which could not be gotten wrong (single words and entire sentences) are discarded. These are standard settings used in previous work (Klein, 2005). To perform model selection and parameter tunning, we split the treebank into three parts: section 02-21 as training set, section 00 as development set, and section 23 as test set. As standard machine learning pipeline, we perform EM on training set, tune parameters on development set, and report the result of selected model on test set."
Y12-1061,H05-1011,0,0.0281801,", which we plan to do in future work. Klein and Manning (2004) demonstrate the joint model of constituency and dependency could improve unsupervised grammar inference. Some other approaches also consider to use additional information such as the words (Headden III et al., 2009), the automatic induced tags (Headden III et al., 2008), or the parent information (Mirroshandel and Ghassem-Sani, 2008). These information could be easily incorporated to the proposed model as features. Feature-based models have been widely used in many supervised tasks such as parsing (Charniak, 2000), word alignment (Moore, 2005; Liu et al., 2006), machine translation (Koehn et al., 2003), etc. For the unsupervised learning tasks, the calculation of normalization part is usually time-consuming or even impossible. Existing approaches are mainly based on the contrastive estimation (Smith and Eisner, 2005; Smith and Eisner, 2005; Dyer et al., 2011) to learn parameters. The local normalized featurebased model has been proposed in (Berg-Kirkpatrick et al., 2010), in which features are defined over generative rules and the normalization is done locally. They use the ℓ2 regularization, while we apply the local-normalization"
Y12-1061,P11-1108,0,0.155025,"Missing"
Y12-1061,P07-1049,0,0.292535,"t improvement on longer sentences. 1 Introduction Unsupervised grammar induction, the task to induce hierarchical structures from plain strings, has attracted research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameter"
Y12-1061,P04-1062,0,0.0785454,"CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentences, its performance drops for longer sentences. There are two possible reasons: (1) CCM models all constituents under only single multinomial distributions, which can not capture the detailed information of span contents; and (2) long sequences only occur a few times in the training corpus, so the probability estimation highly depends on smoothing. Another problem of original CCM and following improved unsupervised models (Smith and Eisner, 2004; Mirroshandel and Ghassem-Sani, 2008; Golland et al., 2012) is the problematic evaluation framework. The previous approaches train and evaluate models on the same dataset, so there is no reasonable way to choose model parameters unless setting them empirically. In this paper, we focus on CCM and present a general feature-based framework in which various overlapping features could be easily added. Previous dependency induction approach (Cohen and Copyright 2012 by Yun Huang, Min Zhang, and Chew Lim Tan 26th Pacific Asia Conference on Language,Information and Computation pages 564–573 Smith, 20"
Y12-1061,P05-1044,0,0.0353815,"al., 2009), the automatic induced tags (Headden III et al., 2008), or the parent information (Mirroshandel and Ghassem-Sani, 2008). These information could be easily incorporated to the proposed model as features. Feature-based models have been widely used in many supervised tasks such as parsing (Charniak, 2000), word alignment (Moore, 2005; Liu et al., 2006), machine translation (Koehn et al., 2003), etc. For the unsupervised learning tasks, the calculation of normalization part is usually time-consuming or even impossible. Existing approaches are mainly based on the contrastive estimation (Smith and Eisner, 2005; Smith and Eisner, 2005; Dyer et al., 2011) to learn parameters. The local normalized featurebased model has been proposed in (Berg-Kirkpatrick et al., 2010), in which features are defined over generative rules and the normalization is done locally. They use the ℓ2 regularization, while we apply the local-normalization model to CCM with ℓ1 regularization and show improved performance could be achieved with sparse solution. Many unsupervised approaches aim to learn compact and sparse grammar, including the Bayesian models (Johnson et al., 2007; Cohn et al., 2010; Blunsom and Cohn, 2010) and po"
Y12-1061,N10-1116,0,0.0125173,"research interests for a long time. The induced grammars can be used to construct large treebanks (van Zaanen, 2000), study language acquisition (Jones et al., 2010), improve machine translation (DeNero and Uszkoreit, 2011), and so on. In general, most approaches either induce the constituency grammars (Klein and Manning, 2002; 564 2 Department of Computer Science National University of Singapore 13 Computing Drive, Singapore Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Specifically, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentences, its performance drops for longer sentences. There are two possible rea"
Y12-1061,D11-1117,0,0.176292,"re first increases and then decreases with the increase of λd:s . In contrast, with fixed λd:s , the performance varies little for different λc:s . These results somehow demonstrate the distituents modelled in CCM play a more important role than the constituents. 5.4 Discussion Experiments show that we achieve better performance than original CCM while using compact grammars. There are some issues we want to discuss here. 1. We only test a few feature templates. Other features such as words, stems may improve the results. Moreover, punctuations contain useful information in grammar induction (Spitkovsky et al., 2011b; Ponvert et al., 2011), while currently punctuations are ignored in our model. 2. In previous unsupervised constituency grammar induction, how to choose parameters is an art. While in the proposed model, we use development set to perform model selection. 3. EM algorithm could only find sub-optima. One possible solution is the Lateen EM (Spitkovsky et al., 2011a), in which multiple objective functions are an alternative optimized. Another method is the annealing technique during probability estimation process. We will investigate these in future work. 4. ℓ1 -norm regularization is used to lea"
Y12-1061,W11-0303,0,0.100745,"re first increases and then decreases with the increase of λd:s . In contrast, with fixed λd:s , the performance varies little for different λc:s . These results somehow demonstrate the distituents modelled in CCM play a more important role than the constituents. 5.4 Discussion Experiments show that we achieve better performance than original CCM while using compact grammars. There are some issues we want to discuss here. 1. We only test a few feature templates. Other features such as words, stems may improve the results. Moreover, punctuations contain useful information in grammar induction (Spitkovsky et al., 2011b; Ponvert et al., 2011), while currently punctuations are ignored in our model. 2. In previous unsupervised constituency grammar induction, how to choose parameters is an art. While in the proposed model, we use development set to perform model selection. 3. EM algorithm could only find sub-optima. One possible solution is the Lateen EM (Spitkovsky et al., 2011a), in which multiple objective functions are an alternative optimized. Another method is the annealing technique during probability estimation process. We will investigate these in future work. 4. ℓ1 -norm regularization is used to lea"
Y12-1061,C00-2139,0,0.355606,"Missing"
Y12-1061,N10-1016,1,0.794326,"combinations that we can not try each of them in experiments. In experiments, we use following sets of features. The first feature set includes the sequences with length up to 5: {seq1, seq2, seq3, seq4, seq5}. Note that sequences with arbitrary lengths are modelled in the original CCM, while we restrict the maximal sequence length to be 5. Since most of the longer sequences occurs only once or twice in the training corpus, we discard them to speed up training procedure and reduce memory usage. Boundary words have been proven useful for detecting phrase boundaries in supervised learning task (Xiong et al., 2010). We introduce this idea to unsupervised grammar induction. The features used in experiments are combinations of left boundary and right boundary words with lengths up to 2: {lb1, lb2, rb1, rb2, lb1.rb1, lb1.rb2, lb2.rb1, lb2.rb2}. The original CCM also considers the pair of preceding one word and following one word as contexts. We consider combinations of left context and right context words with lengths up to 2: {lx1, lx2, rx1, rx2, lx1.rx1, lx1.rx2, lx2.rx1, lx2.rx2}. The special token ⋄ is introduced to represent sentence boundaries. The last feature used is the constant feature {const}. T"
