2006.amta-papers.8,J93-2003,0,0.017409,"Missing"
2006.amta-papers.8,N03-1017,0,0.0147799,"bei r4 ⇓ [jingfang]2 [jibi]1 Figure 2: A synatx-directed translation process for Example (1). VP VBD (r3 ) was VP-C x1 :VBN IN PP → bei x2 x1 x2 :NP-C by which captures the fact that the agent (NP-C, “the police”) and the verb (VBN, “killed”) are always inverted between English and Chinese in a passive voice. Finally, we apply rules r4 and r5 which perform phrasal translations for the two remaining sub-trees in (d), respectively, and get the completed Chinese string in (e). 67 ◦ 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Kni"
2006.amta-papers.8,koen-2004-pharaoh,0,0.0531818,"end up with 140 English sentences to translate in both dev and test sets. Note that this arrangement makes sure the test set is blind. 6.2 Systems We implemented our system as follows: for each input sentence, we first run Algorithm 1, which returns the 1-best translation and also builds the derivation forest of all translations for this sentence. Then we extract the top-k non-duplicate translated strings from this forest using the algorithm in Section 5.2 and rescore them with the trigram model and the length penalty. We compared our system with a state-of-theart phrase-based system Pharaoh (Koehn, 2004) on the evaluation data. Since the target language is Chinese, we will report character-based BLEU scores instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based). Feature weights of both systems are tuned for BLEU-4 (up to 4-grams) on the dev set. For Pharaoh, we use the standard minimum error-rate training (Och, 2003) (David Chiang’s implementation); and for our system, since there are only two independent features (as we always fix α = 1), we use a simple grid-based line-optimization along the language-model weight ax"
2006.amta-papers.8,N03-1019,0,0.00473659,"ranslations for the two remaining sub-trees in (d), respectively, and get the completed Chinese string in (e). 67 ◦ 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model and do parsing and transformation in a joint search, essentially over a packed forest of parse-trees. To this end, their methods are not directed by a syntactic tree. Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts t"
2006.amta-papers.8,A00-2018,0,0.040742,"-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level context-free rule, while our approach decouples the source-language analyzer and the recursive converter, so that the latter can have an extended domain of locality. In addition, our model also enjoys a speed-up by this decoupling, with each of the two stages having a smaller search space. In fact, the recursive transfer step can be done by a linear-time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000). In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable. There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work). Our model, being linguistically motivated, is also more expressive than the formally syntaxbased models of Chiang (2005) and Wu (1997). Consider, again, the passive example in rule r3 . In Chiang’s SCFG, there is only one nonterminal X, so a corresponding rule would be I was [aslee"
2006.amta-papers.8,P05-1033,0,0.810076,"2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model and do parsing and transformation in a joint search, essentially over a packed forest of parse-trees. To this end, their methods are not directed by a syntactic tree. Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level context-free rule, while our approach decouples the source-language analyzer and the recursive converter, so that the latter can have an extended domain of locality. In addition, our model also enjoys a speed-"
2006.amta-papers.8,C04-1090,0,0.561123,"a corresponding rule would be I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). Recent works on dependency-based MT (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) are closest to this work in the sense that their translations are also based on source-language parse trees. The difference is that they use dependency trees instead of constituent trees. Although they share with this work the basic motivations and similar speed-up, it is difficult to specify reordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two works). Our approach, in contrast, explicitly models the re-ordering of sub-trees within indi"
2006.amta-papers.8,P05-1066,0,0.184638,"Missing"
2006.amta-papers.8,N06-1045,1,0.682453,"hs. It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, . . . , k th derivations. Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules. In practice, this results in a very small ratio of unique strings among top-k derivations, while the rescoring approach prefers diversity within the k-best list. To alleviate this problem, determinization techniques have been proposed by Mohri and Riley (2002) for finite-state automata and extended to tree automata by May and Knight (2006). These methods eliminate spurious ambiguity by effectively transforming the grammar into an equivalent deterministic form. However, this transformation often leads to a blow-up in forest size, which is exponential in the original size in the worst-case. So instead of determinization, here we present a simple-yet-effective extension to the Algorithm 3 of Huang and Chiang (2005) that guarantees to output unique translated strings: • keep a hash-table of unique strings at each vertex in the hypergraph • when asking for the next-best derivation of a vertex, keep asking until we get a new string,"
2006.amta-papers.8,P02-1038,0,0.170149,"τ ∗ ) = {d |E(d) = τ ∗ } that translates English tree τ into some Chinese string and apply the Viterbi approximation again to search for the best derivation d∗ : c∗ = C(d∗ ) = C(argmax Pr(d)) (6) d∈D(τ ∗ ) Assuming different rules in a derivation are applied independently, we approximate Pr(d) as Y Pr(r) (7) Pr(d) = r∈d where the probability Pr(r) of the rule r is estimated by conditioning on the root symbol ρ(t(r)): Pr(r) = Pr(t(r), s(r) |ρ(t(r))) c(r) = P 0 r 0 :ρ(t(r 0 ))=ρ(t(r)) c(r ) (8) where c(r) is the count (or frequency) of rule r in the training data. 4.2 Log-Linear Model Following Och and Ney (2002), we extend the direct model into a general log-linear framework in order to incorporate other features: c∗ = argmax Pr(c |e)α · Pr(c)β · e−λ|c| (9) c where Pr(c) is the language model and e−λ|c |is the length penalty term based on |c|, the length of the translation. Parameters α, β, and λ are the weights of relevant features. Note that positive λ prefers longer translations, thus we call λ the length-bonus parameter. We use a standard trigram model for Pr(c). 5 Search Algorithms We first present a linear-time algorithm for searching the best derivation under the direct model, and then extend"
2006.amta-papers.8,P05-1067,0,0.754859,"nding rule would be I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). Recent works on dependency-based MT (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) are closest to this work in the sense that their translations are also based on source-language parse trees. The difference is that they use dependency trees instead of constituent trees. Although they share with this work the basic motivations and similar speed-up, it is difficult to specify reordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two works). Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules."
2006.amta-papers.8,J04-4002,0,0.149326,"2 [jibi]1 Figure 2: A synatx-directed translation process for Example (1). VP VBD (r3 ) was VP-C x1 :VBN IN PP → bei x2 x1 x2 :NP-C by which captures the fact that the agent (NP-C, “the police”) and the verb (VBN, “killed”) are always inverted between English and Chinese in a passive voice. Finally, we apply rules r4 and r5 which perform phrasal translations for the two remaining sub-trees in (d), respectively, and get the completed Chinese string in (e). 67 ◦ 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chian"
2006.amta-papers.8,P03-2041,0,0.850735,"rget-language string with the highest probability. However, the structural divergence across languages often results in non-isomorphic parse-trees that is beyond the power of SCFGs. For example, the S(VO) structure in English is translated into a VSO wordorder in Arabic, an instance of complex reordering not captured by any SCFG (Fig. 1). To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree. For example, Shieber and Schabes (1990) introduce synchronous treeadjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-"
2006.amta-papers.8,P03-1021,0,0.0783409,"ted strings from this forest using the algorithm in Section 5.2 and rescore them with the trigram model and the length penalty. We compared our system with a state-of-theart phrase-based system Pharaoh (Koehn, 2004) on the evaluation data. Since the target language is Chinese, we will report character-based BLEU scores instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based). Feature weights of both systems are tuned for BLEU-4 (up to 4-grams) on the dev set. For Pharaoh, we use the standard minimum error-rate training (Och, 2003) (David Chiang’s implementation); and for our system, since there are only two independent features (as we always fix α = 1), we use a simple grid-based line-optimization along the language-model weight axis. For a given language-model weight β, we use binary search to find the best length bonus parameter λ that leads to a length-ratio closest to 1 against the reference. dev set BLEU-4 25.96 ±2.8 22.10 ±2.6 test set (140 sentences) BLEU-4 BLEU-8 23.54 ±1.9 6.739 ±1.2 24.53 ±2.2 7.309 ±1.9 26.01 ±2.7 26.95 ±2.8 25.74 ±2.3 26.69 ±2.4 8.489 ±2.1 9.323 ±2.2 6.3 Results and Statistical Significance"
2006.amta-papers.8,W02-1039,0,0.210419,"nchronous treeadjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since syntax-directed translation models sep66 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 66-73, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas arate the source-language analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFG-based Treeb"
2006.amta-papers.8,N04-1035,1,0.81719,"reeadjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since syntax-directed translation models sep66 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 66-73, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas arate the source-language analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFG-based Treebank parser but focus o"
2006.amta-papers.8,N04-1014,1,0.849397,"-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since syntax-directed translation models sep66 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 66-73, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas arate the source-language analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFG-based Treebank parser but focus on the extended domain in the recursive converter. Following Galley et al."
2006.amta-papers.8,W05-1506,1,0.28971,"caching the best solution for future use 16: return cache[η] . returns the best string with its prob. However, integrating the n-gram model Pr(C(d)) with the translation model in the search is computationally very expensive. As a standard alternative, rather than aiming at the exact best derivation, we search for top-k derivations under the direct model using Algorithm 1, and then rerank the k-best list with the language model and length penalty. Like other instances of dynamic programming, Algorithm 1 can be viewed as a hypergraph search problem. To this end, we use an efficient algorithm by Huang and Chiang (2005, Algorithm 3) that solves the general k-best derivations problem in monotonic hypergraphs. It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, . . . , k th derivations. Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules. In practice, this results in a very small ratio of unique strings among top-k derivations, while the rescoring approach prefers diversity within the k-best list. To alleviate this problem, determinization techniques have been propos"
2006.amta-papers.8,W06-1608,0,0.0648092,"Missing"
2006.amta-papers.8,P05-1034,0,0.570793,"as [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). Recent works on dependency-based MT (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) are closest to this work in the sense that their translations are also based on source-language parse trees. The difference is that they use dependency trees instead of constituent trees. Although they share with this work the basic motivations and similar speed-up, it is difficult to specify reordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two works). Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules. In addition, it is mo"
2006.amta-papers.8,C90-3045,0,0.426197,"vation (a sequence of translation steps) that converts the whole tree into some target-language string with the highest probability. However, the structural divergence across languages often results in non-isomorphic parse-trees that is beyond the power of SCFGs. For example, the S(VO) structure in English is translated into a VSO wordorder in Arabic, an instance of complex reordering not captured by any SCFG (Fig. 1). To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree. For example, Shieber and Schabes (1990) introduce synchronous treeadjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often"
2006.amta-papers.8,J97-3002,0,0.123979,"by a linear-time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000). In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable. There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work). Our model, being linguistically motivated, is also more expressive than the formally syntaxbased models of Chiang (2005) and Wu (1997). Consider, again, the passive example in rule r3 . In Chiang’s SCFG, there is only one nonterminal X, so a corresponding rule would be I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common"
2006.amta-papers.8,P01-1067,1,0.929016,"n et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model and do parsing and transformation in a joint search, essentially over a packed forest of parse-trees. To this end, their methods are not directed by a syntactic tree. Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level context-free rule, while our approach decouples the source-language analyzer and the recursive converter, so that the latter can have an extended domain of locality. In addition, our model al"
2006.amta-papers.8,J03-4003,0,\N,Missing
2006.amta-papers.8,J08-3004,1,\N,Missing
2006.amta-papers.8,W90-0102,0,\N,Missing
2015.mtsummit-papers.5,2007.mtsummit-papers.3,0,0.0283474,"ven’t used the lexicalized reordering model here. 6 Related Work On a high level, this work is a combination of two different research directions. One direction is to induce translation rules directly from bitext, rather than using word alignment (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011; Levenberg et al., 2012; Xiao and Xiong, 2013). They can learn better translation rules, but don’t care about parameter tuning of SMT. Another direction is discriminative training for MT parameter tuning (Liang et al., 2006; Arun and Koehn, 2007; Blunsom et al., 2008; Flanigan et al., 2013; Green et al., 2013; Yu et al., 2013; Zhao et al., 2014). Both the two directions have achieved promising results. We differ from these works in that we make efforts to combine their spirit together, and try to do rule induction and parameter tuning in one step. 7 Experiments To evaluate our method, we conduct experiments on Chinese-to-English transaltion. Since we need to do forced decoding and real decoding on training corpus each iteration, to guarantee the training efficiency, here we use a small scale data. It includes about 100K sentence pair"
2015.mtsummit-papers.5,P09-1088,0,0.0170033,"directional lexical translation probabilities (estimated based on (Koehn et al., 2003) by word translation probabilities from Moses (Koehn et al., 2007) based on GIZA++), language model, rule penalty, length penalty, and distortion cost. To simplify the system, we haven’t used the lexicalized reordering model here. 6 Related Work On a high level, this work is a combination of two different research directions. One direction is to induce translation rules directly from bitext, rather than using word alignment (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011; Levenberg et al., 2012; Xiao and Xiong, 2013). They can learn better translation rules, but don’t care about parameter tuning of SMT. Another direction is discriminative training for MT parameter tuning (Liang et al., 2006; Arun and Koehn, 2007; Blunsom et al., 2008; Flanigan et al., 2013; Green et al., 2013; Yu et al., 2013; Zhao et al., 2014). Both the two directions have achieved promising results. We differ from these works in that we make efforts to combine their spirit together, and try to do rule induction and parameter tuning in one step. 7 Experiments To evaluat"
2015.mtsummit-papers.5,P08-1024,0,0.0198137,"ized reordering model here. 6 Related Work On a high level, this work is a combination of two different research directions. One direction is to induce translation rules directly from bitext, rather than using word alignment (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011; Levenberg et al., 2012; Xiao and Xiong, 2013). They can learn better translation rules, but don’t care about parameter tuning of SMT. Another direction is discriminative training for MT parameter tuning (Liang et al., 2006; Arun and Koehn, 2007; Blunsom et al., 2008; Flanigan et al., 2013; Green et al., 2013; Yu et al., 2013; Zhao et al., 2014). Both the two directions have achieved promising results. We differ from these works in that we make efforts to combine their spirit together, and try to do rule induction and parameter tuning in one step. 7 Experiments To evaluate our method, we conduct experiments on Chinese-to-English transaltion. Since we need to do forced decoding and real decoding on training corpus each iteration, to guarantee the training efficiency, here we use a small scale data. It includes about 100K sentence pairs from FBIS, where the"
2015.mtsummit-papers.5,W07-0403,0,0.0272182,"anslation probabilities (computed by Formula (1) in § 4) 4 , bidirectional lexical translation probabilities (estimated based on (Koehn et al., 2003) by word translation probabilities from Moses (Koehn et al., 2007) based on GIZA++), language model, rule penalty, length penalty, and distortion cost. To simplify the system, we haven’t used the lexicalized reordering model here. 6 Related Work On a high level, this work is a combination of two different research directions. One direction is to induce translation rules directly from bitext, rather than using word alignment (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011; Levenberg et al., 2012; Xiao and Xiong, 2013). They can learn better translation rules, but don’t care about parameter tuning of SMT. Another direction is discriminative training for MT parameter tuning (Liang et al., 2006; Arun and Koehn, 2007; Blunsom et al., 2008; Flanigan et al., 2013; Green et al., 2013; Yu et al., 2013; Zhao et al., 2014). Both the two directions have achieved promising results. We differ from these works in that we make efforts to combine their spirit together, and try to do rule induc"
2015.mtsummit-papers.5,D08-1024,0,0.0272445,"combines rule induction and parameter tuning in one single module. Preliminary experiments show that our learned model achieves comparable translation quality to the traditional MT training pipeline. 1 Introduction bitext (c) GIZA bitext (b) GIZA (a) bitext Typically, as shown in Figure 1(a), traditional machine translation (MT) training involves a long pipeline of several stages, including word alignment (by GIZA++), rule extraction, translation model (TM) estimation (by max-likelihood), and parameter tuning by M ERT (Och, 2003), P RO (Hopkins and May, 2011), or M IRA (Watanabe et al., 2007; Chiang et al., 2008). This cascaded procedure inevitably propagates errors downstream, while at the same time making MT training overly complicated. hard word alignment rule rule TM rule full tune extr. set est. probs model ideal framework for rule induction & parameter tuning full model joint framework for rule soft align probs induction & parameter tuning full model Figure 1: Three approaches to MT training. (a) the standard pipeline; (b) ideal end-to-end MT training; (c) our work, in between (a) and (b), combines rule induction, TM estimation, and parameter tuning in one module. In this paper, instead of follo"
2015.mtsummit-papers.5,D08-1033,0,0.051627,"Missing"
2015.mtsummit-papers.5,W06-3105,0,0.0233304,"le combinations of these phrases as rules for forced decoding. In addition, since training sentence pairs do not always contain equal information on both sides, we introduce two null rules hf, nulli and hnull, ei to capture the redundant information for forced decoding. hf, nulli deletes a source word, and hnull, ei inserts a target word to the translation. After forced decoding, we collect the rules used in forced derivations and add them to the standing rule set R (line 6 in Algorithm 1). 3 Based on the rule counts in R, we recalculate the rule conditional probabilities by the formula from (DeNero et al., 2006): φ(e|f ) = c(f, e) c(f ) + k l−1 (1) where f and e are the source and target phrase, c(·) is the count of phrase or phrase pair, l is the length of phrase f , and k is a tuning parameter. The formula boosts the probability of short phrases, and results in better translation quality. After some validation experiments, we set k = 4.0 finally. Similar to our rule generation process, Wuebker and Ney (2013) has proposed a lengthbased training method to do rule induction by EM algorithm. The major difference between our framework and theirs is that their algorithm only relates to rule induction, an"
2015.mtsummit-papers.5,N13-1025,0,0.0124035,"here. 6 Related Work On a high level, this work is a combination of two different research directions. One direction is to induce translation rules directly from bitext, rather than using word alignment (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011; Levenberg et al., 2012; Xiao and Xiong, 2013). They can learn better translation rules, but don’t care about parameter tuning of SMT. Another direction is discriminative training for MT parameter tuning (Liang et al., 2006; Arun and Koehn, 2007; Blunsom et al., 2008; Flanigan et al., 2013; Green et al., 2013; Yu et al., 2013; Zhao et al., 2014). Both the two directions have achieved promising results. We differ from these works in that we make efforts to combine their spirit together, and try to do rule induction and parameter tuning in one step. 7 Experiments To evaluate our method, we conduct experiments on Chinese-to-English transaltion. Since we need to do forced decoding and real decoding on training corpus each iteration, to guarantee the training efficiency, here we use a small scale data. It includes about 100K sentence pairs from FBIS, where the length of each sentenc"
2015.mtsummit-papers.5,D11-1125,0,0.0222478,"of end-to-end MT training, and propose a framework which combines rule induction and parameter tuning in one single module. Preliminary experiments show that our learned model achieves comparable translation quality to the traditional MT training pipeline. 1 Introduction bitext (c) GIZA bitext (b) GIZA (a) bitext Typically, as shown in Figure 1(a), traditional machine translation (MT) training involves a long pipeline of several stages, including word alignment (by GIZA++), rule extraction, translation model (TM) estimation (by max-likelihood), and parameter tuning by M ERT (Och, 2003), P RO (Hopkins and May, 2011), or M IRA (Watanabe et al., 2007; Chiang et al., 2008). This cascaded procedure inevitably propagates errors downstream, while at the same time making MT training overly complicated. hard word alignment rule rule TM rule full tune extr. set est. probs model ideal framework for rule induction & parameter tuning full model joint framework for rule soft align probs induction & parameter tuning full model Figure 1: Three approaches to MT training. (a) the standard pipeline; (b) ideal end-to-end MT training; (c) our work, in between (a) and (b), combines rule induction, TM estimation, and paramete"
2015.mtsummit-papers.5,P07-1019,1,0.642521,"in one step. 7 Experiments To evaluate our method, we conduct experiments on Chinese-to-English transaltion. Since we need to do forced decoding and real decoding on training corpus each iteration, to guarantee the training efficiency, here we use a small scale data. It includes about 100K sentence pairs from FBIS, where the length of each sentence is less than 30 words. We use GIZA++ and grow-diagfinal-and strategy to create symmetric word alignment. We train a trigram language model on 1.5M English sentences. We base our experiments on Cubit, a state-of-the-art phrase-based system in Python Huang and Chiang (2007). For the joint learning method, we set the beam size for forced decoding as 10, real decoding as 30. A maximum phrase length of three was used for both baseline and our joint system. The beam size for final test decoding is set to 50. We take the newswire portion of NIST MT 2006 data as our dev set, and the NIST MT 03-05 data and the newswire portion of 2008 data as the test set. For baseline, we use M ERT Och (2003) to tune weights. 4 For the newly generated rules which do not have any counts, we assign a very small probability for them. Proceedings of MT Summit XV, vol.1: MT Researchers' Tr"
2015.mtsummit-papers.5,N12-1015,1,0.829678,"the best translation (line 9) 1 . 1 At first, since all the weights are set to 0, all derivations have the same score 0. Both the best-scoring forced derivation and real decoding derivation are selected randomly Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 57 If this translation e(d0 ) is different from the reference translation y, then an update is needed to reward the highest-scoring forced (or “gold”) derivation d∗ and to penalize the highest-scoring non-gold (Viterbi) derivation d0 (line 12). We apply a latent-variable max-violation perceptron (Huang et al., 2012), which has been successfully used in MT training (Yu et al., 2013; Zhao et al., 2014), to do update. Technically, this framework is similar to (Xiao and Xiong, 2013). The main difference is that we try to do end-to-end MT training, and combine rule induction, TM estimation, and parameter tuning together, while they only focus on rule induction. Moreover, to accommodate the vast amount of search errors in decoding, we update weights by max-violation perceptron, performing prefix instead of full-sequence updates, whereas the updates in Xiao and Xiong (2013) are still full-sequence updates which"
2015.mtsummit-papers.5,P07-2045,0,0.00564639,"io, we find it is very easy to get overfitting with sparse features. We conjecture that this is because sparse features have a tight connection to rules. Once some bad rules are introduced, it is difficult for the learner to correct them. We will make more effort on this in future. Here, we only use dense features, which are the same as the ones for phrase-based translation, including bidirectional translation probabilities (computed by Formula (1) in § 4) 4 , bidirectional lexical translation probabilities (estimated based on (Koehn et al., 2003) by word translation probabilities from Moses (Koehn et al., 2007) based on GIZA++), language model, rule penalty, length penalty, and distortion cost. To simplify the system, we haven’t used the lexicalized reordering model here. 6 Related Work On a high level, this work is a combination of two different research directions. One direction is to induce translation rules directly from bitext, rather than using word alignment (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011; Levenberg et al., 2012; Xiao and Xiong, 2013). They can learn better translation rules, but don’t care about"
2015.mtsummit-papers.5,N03-1017,0,0.0355557,"n allows arbitrary features. However, in our joint learning scenario, we find it is very easy to get overfitting with sparse features. We conjecture that this is because sparse features have a tight connection to rules. Once some bad rules are introduced, it is difficult for the learner to correct them. We will make more effort on this in future. Here, we only use dense features, which are the same as the ones for phrase-based translation, including bidirectional translation probabilities (computed by Formula (1) in § 4) 4 , bidirectional lexical translation probabilities (estimated based on (Koehn et al., 2003) by word translation probabilities from Moses (Koehn et al., 2007) based on GIZA++), language model, rule penalty, length penalty, and distortion cost. To simplify the system, we haven’t used the lexicalized reordering model here. 6 Related Work On a high level, this work is a combination of two different research directions. One direction is to induce translation rules directly from bitext, rather than using word alignment (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011; Levenberg et al., 2012; Xiao and Xiong, 201"
2015.mtsummit-papers.5,D12-1021,0,0.0137744,"es (estimated based on (Koehn et al., 2003) by word translation probabilities from Moses (Koehn et al., 2007) based on GIZA++), language model, rule penalty, length penalty, and distortion cost. To simplify the system, we haven’t used the lexicalized reordering model here. 6 Related Work On a high level, this work is a combination of two different research directions. One direction is to induce translation rules directly from bitext, rather than using word alignment (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011; Levenberg et al., 2012; Xiao and Xiong, 2013). They can learn better translation rules, but don’t care about parameter tuning of SMT. Another direction is discriminative training for MT parameter tuning (Liang et al., 2006; Arun and Koehn, 2007; Blunsom et al., 2008; Flanigan et al., 2013; Green et al., 2013; Yu et al., 2013; Zhao et al., 2014). Both the two directions have achieved promising results. We differ from these works in that we make efforts to combine their spirit together, and try to do rule induction and parameter tuning in one step. 7 Experiments To evaluate our method, we conduct experiments on Chine"
2015.mtsummit-papers.5,P06-1096,0,0.111241,"Missing"
2015.mtsummit-papers.5,W02-1018,0,0.0490467,"uding bidirectional translation probabilities (computed by Formula (1) in § 4) 4 , bidirectional lexical translation probabilities (estimated based on (Koehn et al., 2003) by word translation probabilities from Moses (Koehn et al., 2007) based on GIZA++), language model, rule penalty, length penalty, and distortion cost. To simplify the system, we haven’t used the lexicalized reordering model here. 6 Related Work On a high level, this work is a combination of two different research directions. One direction is to induce translation rules directly from bitext, rather than using word alignment (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011; Levenberg et al., 2012; Xiao and Xiong, 2013). They can learn better translation rules, but don’t care about parameter tuning of SMT. Another direction is discriminative training for MT parameter tuning (Liang et al., 2006; Arun and Koehn, 2007; Blunsom et al., 2008; Flanigan et al., 2013; Green et al., 2013; Yu et al., 2013; Zhao et al., 2014). Both the two directions have achieved promising results. We differ from these works in that we make efforts to combine their spirit together, an"
2015.mtsummit-papers.5,P11-1064,0,0.0147647,"anslation probabilities (estimated based on (Koehn et al., 2003) by word translation probabilities from Moses (Koehn et al., 2007) based on GIZA++), language model, rule penalty, length penalty, and distortion cost. To simplify the system, we haven’t used the lexicalized reordering model here. 6 Related Work On a high level, this work is a combination of two different research directions. One direction is to induce translation rules directly from bitext, rather than using word alignment (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011; Levenberg et al., 2012; Xiao and Xiong, 2013). They can learn better translation rules, but don’t care about parameter tuning of SMT. Another direction is discriminative training for MT parameter tuning (Liang et al., 2006; Arun and Koehn, 2007; Blunsom et al., 2008; Flanigan et al., 2013; Green et al., 2013; Yu et al., 2013; Zhao et al., 2014). Both the two directions have achieved promising results. We differ from these works in that we make efforts to combine their spirit together, and try to do rule induction and parameter tuning in one step. 7 Experiments To evaluate our method, we cond"
2015.mtsummit-papers.5,P03-1021,0,0.0756994,"e the possibility of end-to-end MT training, and propose a framework which combines rule induction and parameter tuning in one single module. Preliminary experiments show that our learned model achieves comparable translation quality to the traditional MT training pipeline. 1 Introduction bitext (c) GIZA bitext (b) GIZA (a) bitext Typically, as shown in Figure 1(a), traditional machine translation (MT) training involves a long pipeline of several stages, including word alignment (by GIZA++), rule extraction, translation model (TM) estimation (by max-likelihood), and parameter tuning by M ERT (Och, 2003), P RO (Hopkins and May, 2011), or M IRA (Watanabe et al., 2007; Chiang et al., 2008). This cascaded procedure inevitably propagates errors downstream, while at the same time making MT training overly complicated. hard word alignment rule rule TM rule full tune extr. set est. probs model ideal framework for rule induction & parameter tuning full model joint framework for rule soft align probs induction & parameter tuning full model Figure 1: Three approaches to MT training. (a) the standard pipeline; (b) ideal end-to-end MT training; (c) our work, in between (a) and (b), combines rule inductio"
2015.mtsummit-papers.5,D07-1080,0,0.0191854,"pose a framework which combines rule induction and parameter tuning in one single module. Preliminary experiments show that our learned model achieves comparable translation quality to the traditional MT training pipeline. 1 Introduction bitext (c) GIZA bitext (b) GIZA (a) bitext Typically, as shown in Figure 1(a), traditional machine translation (MT) training involves a long pipeline of several stages, including word alignment (by GIZA++), rule extraction, translation model (TM) estimation (by max-likelihood), and parameter tuning by M ERT (Och, 2003), P RO (Hopkins and May, 2011), or M IRA (Watanabe et al., 2007; Chiang et al., 2008). This cascaded procedure inevitably propagates errors downstream, while at the same time making MT training overly complicated. hard word alignment rule rule TM rule full tune extr. set est. probs model ideal framework for rule induction & parameter tuning full model joint framework for rule soft align probs induction & parameter tuning full model Figure 1: Three approaches to MT training. (a) the standard pipeline; (b) ideal end-to-end MT training; (c) our work, in between (a) and (b), combines rule induction, TM estimation, and parameter tuning in one module. In this p"
2015.mtsummit-papers.5,W13-2238,0,0.0172832,"used in forced derivations and add them to the standing rule set R (line 6 in Algorithm 1). 3 Based on the rule counts in R, we recalculate the rule conditional probabilities by the formula from (DeNero et al., 2006): φ(e|f ) = c(f, e) c(f ) + k l−1 (1) where f and e are the source and target phrase, c(·) is the count of phrase or phrase pair, l is the length of phrase f , and k is a tuning parameter. The formula boosts the probability of short phrases, and results in better translation quality. After some validation experiments, we set k = 4.0 finally. Similar to our rule generation process, Wuebker and Ney (2013) has proposed a lengthbased training method to do rule induction by EM algorithm. The major difference between our framework and theirs is that their algorithm only relates to rule induction, and still need M ERT to do parameter tuning, while we combine rule induction and parameter tuning together. In this way, the two step in our framework can help each other, but the two step in (Wuebker and Ney, 2 We have also tried longer length limit, but the performance becomes worse, because with longer limit, the learner greatly prefers longer phrases, which are not good at generalization. 3 We count a"
2015.mtsummit-papers.5,D13-1026,0,0.0494335,"real decoding derivation are selected randomly Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 57 If this translation e(d0 ) is different from the reference translation y, then an update is needed to reward the highest-scoring forced (or “gold”) derivation d∗ and to penalize the highest-scoring non-gold (Viterbi) derivation d0 (line 12). We apply a latent-variable max-violation perceptron (Huang et al., 2012), which has been successfully used in MT training (Yu et al., 2013; Zhao et al., 2014), to do update. Technically, this framework is similar to (Xiao and Xiong, 2013). The main difference is that we try to do end-to-end MT training, and combine rule induction, TM estimation, and parameter tuning together, while they only focus on rule induction. Moreover, to accommodate the vast amount of search errors in decoding, we update weights by max-violation perceptron, performing prefix instead of full-sequence updates, whereas the updates in Xiao and Xiong (2013) are still full-sequence updates which are insensitive to search errors. For simplicity reasons we do not make this difference explicit in line 12. 3 Phrase-based Forced Decoding Forced decoding generates"
2015.mtsummit-papers.5,2005.eamt-1.37,0,0.0718234,"Missing"
2015.mtsummit-papers.5,D13-1112,1,0.923185,"e set to 0, all derivations have the same score 0. Both the best-scoring forced derivation and real decoding derivation are selected randomly Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 57 If this translation e(d0 ) is different from the reference translation y, then an update is needed to reward the highest-scoring forced (or “gold”) derivation d∗ and to penalize the highest-scoring non-gold (Viterbi) derivation d0 (line 12). We apply a latent-variable max-violation perceptron (Huang et al., 2012), which has been successfully used in MT training (Yu et al., 2013; Zhao et al., 2014), to do update. Technically, this framework is similar to (Xiao and Xiong, 2013). The main difference is that we try to do end-to-end MT training, and combine rule induction, TM estimation, and parameter tuning together, while they only focus on rule induction. Moreover, to accommodate the vast amount of search errors in decoding, we update weights by max-violation perceptron, performing prefix instead of full-sequence updates, whereas the updates in Xiao and Xiong (2013) are still full-sequence updates which are insensitive to search errors. For simplicity reasons we do no"
2015.mtsummit-papers.5,C04-1030,0,0.0338622,"his position is already “covered (or translated)”), and (s, p) is the score and partial translation of each state. Generally, we can employ a traditional phrase-based decoder to do forced decoding. However, the distortion limit in the decoder will prohibit long-distance reorderings, and exclude many sentence pairs from getting forced derivations, especially for language pairs with very different word orders, such as Chinese and English. Hence, in order to do better forced decoding, we use a more flexible limit to constrain the number of gaps during decoding (also known as “IBM constraint” in (Zens et al., 2004)), rather than distortion limit. Here, a gap refers to a consecutive of positions that are not covered in the coverage vector. For example, consider the third hypothesis of the above derivation, (• •••6 ) : (s2 , “Bush held a talk”), its coverage vector has one gap, i.e., the two untranslated words. Also, we don’t want the decoding process to be too flexible on reordering, so we demand that there are at most two gaps in a specific coverage vector (See Figure 2). gap gap gap gap Figure 2: Two possible scenarios in gap-based decoding. The gray boxes denote covered segments. Proceedings of MT Sum"
2015.mtsummit-papers.5,P08-1012,0,0.0245572,"s (computed by Formula (1) in § 4) 4 , bidirectional lexical translation probabilities (estimated based on (Koehn et al., 2003) by word translation probabilities from Moses (Koehn et al., 2007) based on GIZA++), language model, rule penalty, length penalty, and distortion cost. To simplify the system, we haven’t used the lexicalized reordering model here. 6 Related Work On a high level, this work is a combination of two different research directions. One direction is to induce translation rules directly from bitext, rather than using word alignment (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011; Levenberg et al., 2012; Xiao and Xiong, 2013). They can learn better translation rules, but don’t care about parameter tuning of SMT. Another direction is discriminative training for MT parameter tuning (Liang et al., 2006; Arun and Koehn, 2007; Blunsom et al., 2008; Flanigan et al., 2013; Green et al., 2013; Yu et al., 2013; Zhao et al., 2014). Both the two directions have achieved promising results. We differ from these works in that we make efforts to combine their spirit together, and try to do rule induction and parameter t"
2015.mtsummit-papers.5,P14-2127,1,0.849139,"erivations have the same score 0. Both the best-scoring forced derivation and real decoding derivation are selected randomly Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 57 If this translation e(d0 ) is different from the reference translation y, then an update is needed to reward the highest-scoring forced (or “gold”) derivation d∗ and to penalize the highest-scoring non-gold (Viterbi) derivation d0 (line 12). We apply a latent-variable max-violation perceptron (Huang et al., 2012), which has been successfully used in MT training (Yu et al., 2013; Zhao et al., 2014), to do update. Technically, this framework is similar to (Xiao and Xiong, 2013). The main difference is that we try to do end-to-end MT training, and combine rule induction, TM estimation, and parameter tuning together, while they only focus on rule induction. Moreover, to accommodate the vast amount of search errors in decoding, we update weights by max-violation perceptron, performing prefix instead of full-sequence updates, whereas the updates in Xiao and Xiong (2013) are still full-sequence updates which are insensitive to search errors. For simplicity reasons we do not make this differen"
2020.acl-main.254,D14-1140,0,0.557362,"Missing"
2020.acl-main.254,E17-1099,0,0.648601,"rpreters usually start translation before the source sentence ends. However, this makes the translation process much more challenging than the full-sentence translation, because to balance the translation quality and latency, interpreters need to make decisions on when to continue translation and when to stop temporarily to wait for more source side information, which are difficult, especially for syntactically divergent language pairs, such as German and English. The above decisions can be considered as two actions: READ (wait for a new source word) and WRITE (emit a translated target word) (Gu et al., 2017). Then we only need to decide which action to choose at each step, and the solution can be represented by a policy. Earlier works (Yarmohammadi et al., 2013; Bangalore et al., 2012; F¨ugen et al., 2007; Sridhar et al., 2013; Jaitly et al., 2016) study policies as a part of speech-to-speech ST system, where the policies usually try to separate the source sentence into several chunks that can be translated safely. Recent works focus on obtaining policies for text-to-text ST, which can be generally divided into two categories: fixed and adaptive. Fixed policies (Ma et al., 2019; Dalvi et al., 201"
2020.acl-main.254,P02-1040,0,0.11586,"dev set, and NIST 2008 as test set. For DE→EN, we use WMT15 parallel corpus for training, newstest-2013 for validation and newstest-2015 for testing. All datasets are tokenized and segmented into sub-word units with byte-pair encoding (Sennrich et al., 2016). We take Transformer-base (Vaswani et al., 2017) as our model architecture, and follow Ma et al. (2019) to train our model with wait-k policies for integer 1 ≤ k ≤ 10. In the following experiments, we only use catchup (Ma et al., 2019) for DE→EN translation, where we read one additional source token after every 6 predictions. We use BLEU (Papineni et al., 2002) as the translation quality metric, and Average Lagging (AL) (Ma et al., 2019) as the latency metric, which measures the lag behind source in terms of the number of source tokens. Performance with different policies. We first evaluate the performance of each model with different policies, which helps us to choose models for different policies. Specifically, we apply each model with ten different wait-k policies on dev set to compare the performance. Fig. 3 shows the results of five models. We find the best model for one policy may not be the one trained with that policy. For example, on ZH→EN"
2020.acl-main.254,P16-1162,0,0.0745238,"propose to apply ensemble of the top-3 models for each policy. That is, we first generate distribution with the top-3 models independently with the same policy, Experiments Datasets and models. We conduct experiments on Chinese→English (ZH→EN) and German→English (DE→EN) translation. For ZH→EN, we use NIST corpus (2M sentence pairs) as training set, NIST 2006 as dev set, and NIST 2008 as test set. For DE→EN, we use WMT15 parallel corpus for training, newstest-2013 for validation and newstest-2015 for testing. All datasets are tokenized and segmented into sub-word units with byte-pair encoding (Sennrich et al., 2016). We take Transformer-base (Vaswani et al., 2017) as our model architecture, and follow Ma et al. (2019) to train our model with wait-k policies for integer 1 ≤ k ≤ 10. In the following experiments, we only use catchup (Ma et al., 2019) for DE→EN translation, where we read one additional source token after every 6 predictions. We use BLEU (Papineni et al., 2002) as the translation quality metric, and Average Lagging (AL) (Ma et al., 2019) as the latency metric, which measures the lag behind source in terms of the number of source tokens. Performance with different policies. We first evaluate t"
2020.acl-main.254,I13-1141,0,\N,Missing
2020.acl-main.254,D17-1208,0,\N,Missing
2020.acl-main.254,D18-1337,0,\N,Missing
2020.acl-main.254,N18-2079,0,\N,Missing
2020.acl-main.254,D19-1137,1,\N,Missing
2020.acl-main.254,D19-1144,1,\N,Missing
2020.acl-main.254,N13-1023,0,\N,Missing
2020.acl-main.42,D18-1337,0,0.0154142,"ow latency, it is often inevitable to generate target words with insufficient source information, which makes this task extremely challenging. Recently, there are many efforts towards balancing the translation latency and quality with mainly two types of approaches. On one hand, Ma et al. (2019a) propose very simple frameworks that decode following a fixed-latency policy such as waitk. On the other hand, there are many attempts to learn an adaptive policy which enables the model to decide READ or WRITE action on the fly using various techniques such as reinforcement learning (Gu et al., 2017; Alinejad et al., 2018; Grissom II ∗ … ˆ t6w y &gt; &gt; : 1 t &lt;latexit sha1_base64=&quot;t6XaytdIsHwdU4AeCNDSjPNP5sM=&quot;&gt;AAAB6HicbVDLSgNBEOz1GeMr6tHLYBA8hV0f6DHoxWMC5gHJEmYns8mY2dllplcIS77AiwdFvPpJ3vwbJ8keNLGgoajqprsrSKQw6Lrfzsrq2vrGZmGruL2zu7dfOjhsmjjVjDdYLGPdDqjhUijeQIGStxPNaRRI3gpGd1O/9cS1EbF6wHHC/YgOlAgFo2ilOvZKZbfizkCWiZeTMuSo9Upf3X7M0ogrZJIa0/HcBP2MahRM8kmxmxqeUDaiA96xVNGIGz+bHTohp1bpkzDWthSSmfp7IqORMeMosJ0RxaFZ9Kbif14nxfDGz4RKUuSKzReFqSQYk+nXpC80ZyjHllCmhb2VsCHVlKHNpmhD8BZfXibN84p3UbmqX5art3kcBTiGEzgDD66hCvdQgwYw4PAMr/DmPDovzrvzMW9dcfKZI/gD5/MH4m2M/w==&lt;/latexit&gt; &lt;latexit sha1_base64=&quot;SeGBdmF0K269BzW9NTI6ylilwtw=&quot;&gt;AAAB6n"
2020.acl-main.42,P19-1126,0,0.172879,"ented as yˆt6w . The timely correction only revises this part in future steps. Different shapes denote different words. In this example, from step t to t + 1, all previously opportunistically decoded words are revised, and an extra triangle word is generated in opportunistic window. From step t + 1 to t + 2, two words from previous opportunistic window are kept and only the triangle word is revised. et al., 2014), supervised learning over pseudooracles (Zheng et al., 2019a), imitation learning (Zheng et al., 2019b), model ensemble (Zheng et al., 2020) or monotonic attention (Ma et al., 2019d; Arivazhagan et al., 2019). Introduction Simultaneous translation, which starts translation before the speaker finishes, is extremely useful in many scenarios, such as international conferences, travels, and so on. In order to achieve low latency, it is often inevitable to generate target words with insufficient source information, which makes this task extremely challenging. Recently, there are many efforts towards balancing the translation latency and quality with mainly two types of approaches. On one hand, Ma et al. (2019a) propose very simple frameworks that decode following a fixed-latency policy such as waitk. O"
2020.acl-main.42,W18-6322,0,0.0287497,"be easily applied to adaptive policies. Simultaneous Translation. Without loss of generality, regardless the actual design of policy, simultaneous translation is represented as: Q|y| pg (y |x) = t=1 p(yt |x6g(t) , y&lt;t ) (2) where g(t) can be used to represent any arbitrary fixed or adaptive policy. For simplicity, we assume the policy is given and does not distinguish the difference between two types of policies. 3 Correction with Beam Search. When the model is committing more than one word at a time, we can use beam search to further improve the translation quality and reduce revision rate (Murray and Chiang, 2018; Ma et al., 2019c). The decoder maintains a beam Btk of size b at step t, which is ordered list of pairs Opportunistic Decoding with Timely Correction and Beam Search Opportunistic Decoding. For simplicity, we first apply this method to fixed policies. We de438 1 2 3 4 5 6 7 8 9 Jiāng Zémín dùi bùshí zǒngtǒng de fāyán biăoshì zàntóng 10 bìngqiě 11 Jiang Zemin to Bush President of speech express agreement and … t=4 decoding time Jiang Zemin expressed his welcome to t=5 Jiang Zemin expressed his agreement to President t=6 Jiang Zemin expressed his agreement to President Bush … Figure 2: The dec"
2020.acl-main.42,P14-2090,0,0.102518,", which represents the word that is decoded in time step t with original model. We denote the additional decoded words at time step t as yˆt6w = (yt1 , ..., ytw ), where w denote the number of extra decoded words. In our setting, the decoding process is as follows: reduce the latency. At the same time, it also employs a timely correction mechanism to review the extra outputs from previous steps with more source context, and revises these outputs with current preference when there is a disagreement. Our algorithm can be used in both speech-to-text and speech-to-speech simultaneous translation (Oda et al., 2014; Bangalore et al., 2012; Yarmohammadi et al., 2013). In the former case, the audience will not be overwhelmed by the modifications since we only review and modify the last few output words with a relatively low revision rate. In the later case, the revisable extra words can be used in look-ahead window in incremental TTS (Ma et al., 2019b). By contrast, the alternative retranslation strategy (Arivazhagan et al., 2020) will cause non-local revisions which makes it impossible to be used in incremental TTS. We also define, for the first time, two metrics for revision-enabled simultaneous transla"
2020.acl-main.42,2020.iwslt-1.27,0,0.0851605,"e context, and revises these outputs with current preference when there is a disagreement. Our algorithm can be used in both speech-to-text and speech-to-speech simultaneous translation (Oda et al., 2014; Bangalore et al., 2012; Yarmohammadi et al., 2013). In the former case, the audience will not be overwhelmed by the modifications since we only review and modify the last few output words with a relatively low revision rate. In the later case, the revisable extra words can be used in look-ahead window in incremental TTS (Ma et al., 2019b). By contrast, the alternative retranslation strategy (Arivazhagan et al., 2020) will cause non-local revisions which makes it impossible to be used in incremental TTS. We also define, for the first time, two metrics for revision-enabled simultaneous translation: a more general latency metric Revision-aware Average Lagging (RAL) as well as the revision rate. We demonstrate the effectiveness of our proposed technique using fixed (Ma et al., 2019a) and adaptive (Zheng et al., 2019a) policies in both Chineseto-English and English-to-Chinese translation. 2 pg (yt ◦ yˆt6w |x6g(t) ) = Q pg (yt |x6g(t) ) w yti |x6g(t) , yt ◦ yˆt&lt;i ) i=1 pg (ˆ (3) where ◦ is the string concatenat"
2020.acl-main.42,D14-1140,0,0.420248,"Missing"
2020.acl-main.42,2020.acl-main.254,1,0.80324,"ding continues to generate additional w words which are represented as yˆt6w . The timely correction only revises this part in future steps. Different shapes denote different words. In this example, from step t to t + 1, all previously opportunistically decoded words are revised, and an extra triangle word is generated in opportunistic window. From step t + 1 to t + 2, two words from previous opportunistic window are kept and only the triangle word is revised. et al., 2014), supervised learning over pseudooracles (Zheng et al., 2019a), imitation learning (Zheng et al., 2019b), model ensemble (Zheng et al., 2020) or monotonic attention (Ma et al., 2019d; Arivazhagan et al., 2019). Introduction Simultaneous translation, which starts translation before the speaker finishes, is extremely useful in many scenarios, such as international conferences, travels, and so on. In order to achieve low latency, it is often inevitable to generate target words with insufficient source information, which makes this task extremely challenging. Recently, there are many efforts towards balancing the translation latency and quality with mainly two types of approaches. On one hand, Ma et al. (2019a) propose very simple fram"
2020.acl-main.42,E17-1099,0,0.127212,"rder to achieve low latency, it is often inevitable to generate target words with insufficient source information, which makes this task extremely challenging. Recently, there are many efforts towards balancing the translation latency and quality with mainly two types of approaches. On one hand, Ma et al. (2019a) propose very simple frameworks that decode following a fixed-latency policy such as waitk. On the other hand, there are many attempts to learn an adaptive policy which enables the model to decide READ or WRITE action on the fly using various techniques such as reinforcement learning (Gu et al., 2017; Alinejad et al., 2018; Grissom II ∗ … ˆ t6w y &gt; &gt; : 1 t &lt;latexit sha1_base64=&quot;t6XaytdIsHwdU4AeCNDSjPNP5sM=&quot;&gt;AAAB6HicbVDLSgNBEOz1GeMr6tHLYBA8hV0f6DHoxWMC5gHJEmYns8mY2dllplcIS77AiwdFvPpJ3vwbJ8keNLGgoajqprsrSKQw6Lrfzsrq2vrGZmGruL2zu7dfOjhsmjjVjDdYLGPdDqjhUijeQIGStxPNaRRI3gpGd1O/9cS1EbF6wHHC/YgOlAgFo2ilOvZKZbfizkCWiZeTMuSo9Upf3X7M0ogrZJIa0/HcBP2MahRM8kmxmxqeUDaiA96xVNGIGz+bHTohp1bpkzDWthSSmfp7IqORMeMosJ0RxaFZ9Kbif14nxfDGz4RKUuSKzReFqSQYk+nXpC80ZyjHllCmhb2VsCHVlKHNpmhD8BZfXibN84p3UbmqX5art3kcBTiGEzgDD66hCvdQgwYw4PAMr/DmPDovzrvzMW9dcfKZI/gD5/MH4m2M/w==&lt;/latexit&gt; &lt;latexit sha1_base64=&quot;SeGBdmF0K269B"
2020.acl-main.42,D19-1137,1,0.938179,"/latexit&gt; irreversible revision window Figure 1: Besides yt , opportunistic decoding continues to generate additional w words which are represented as yˆt6w . The timely correction only revises this part in future steps. Different shapes denote different words. In this example, from step t to t + 1, all previously opportunistically decoded words are revised, and an extra triangle word is generated in opportunistic window. From step t + 1 to t + 2, two words from previous opportunistic window are kept and only the triangle word is revised. et al., 2014), supervised learning over pseudooracles (Zheng et al., 2019a), imitation learning (Zheng et al., 2019b), model ensemble (Zheng et al., 2020) or monotonic attention (Ma et al., 2019d; Arivazhagan et al., 2019). Introduction Simultaneous translation, which starts translation before the speaker finishes, is extremely useful in many scenarios, such as international conferences, travels, and so on. In order to achieve low latency, it is often inevitable to generate target words with insufficient source information, which makes this task extremely challenging. Recently, there are many efforts towards balancing the translation latency and quality with mainly"
2020.acl-main.42,P19-1582,1,0.798856,"/latexit&gt; irreversible revision window Figure 1: Besides yt , opportunistic decoding continues to generate additional w words which are represented as yˆt6w . The timely correction only revises this part in future steps. Different shapes denote different words. In this example, from step t to t + 1, all previously opportunistically decoded words are revised, and an extra triangle word is generated in opportunistic window. From step t + 1 to t + 2, two words from previous opportunistic window are kept and only the triangle word is revised. et al., 2014), supervised learning over pseudooracles (Zheng et al., 2019a), imitation learning (Zheng et al., 2019b), model ensemble (Zheng et al., 2020) or monotonic attention (Ma et al., 2019d; Arivazhagan et al., 2019). Introduction Simultaneous translation, which starts translation before the speaker finishes, is extremely useful in many scenarios, such as international conferences, travels, and so on. In order to achieve low latency, it is often inevitable to generate target words with insufficient source information, which makes this task extremely challenging. Recently, there are many efforts towards balancing the translation latency and quality with mainly"
2020.acl-main.42,D19-1144,1,0.922737,"/latexit&gt; irreversible revision window Figure 1: Besides yt , opportunistic decoding continues to generate additional w words which are represented as yˆt6w . The timely correction only revises this part in future steps. Different shapes denote different words. In this example, from step t to t + 1, all previously opportunistically decoded words are revised, and an extra triangle word is generated in opportunistic window. From step t + 1 to t + 2, two words from previous opportunistic window are kept and only the triangle word is revised. et al., 2014), supervised learning over pseudooracles (Zheng et al., 2019a), imitation learning (Zheng et al., 2019b), model ensemble (Zheng et al., 2020) or monotonic attention (Ma et al., 2019d; Arivazhagan et al., 2019). Introduction Simultaneous translation, which starts translation before the speaker finishes, is extremely useful in many scenarios, such as international conferences, travels, and so on. In order to achieve low latency, it is often inevitable to generate target words with insufficient source information, which makes this task extremely challenging. Recently, there are many efforts towards balancing the translation latency and quality with mainly"
2020.acl-main.42,N19-1187,1,0.893357,"Missing"
2020.acl-main.42,I13-1141,0,\N,Missing
2020.findings-emnlp.346,P12-3018,0,0.312669,"the waveform for the first word, which is also played immediately (see Fig. 2). This results in an O(1) rather than O(n) latency. Experiments on English and Chinese TTS show that our approach achieves similar speech naturalness compared to full sentence methods, but only with a constant (1–2 words) latency.1 This paper makes following contributions: • From the model point of view, with monotonic attention in TTS, we don’t need to retrain the model, and only need to adapt the inference. This is different from all other pre1 There also exist incremental TTS efforts using non-neural techniques (Baumann and Schlangen, 2012c,b; Baumann, 2014b; Pouget et al., 2015; Yanagita et al., 2018) which are fundamentally different from our work. See also Sec. 5. b731afe6-X4evPTjAzgj7EjZJZDKPMUXKnhBxOXbUikI2Rw==-500ddfd78926 vious incremental adaptations in simultaneous translation, ASR and TTS (Ma et al., 2019; Novitasari et al., 2019; Yanagita et al., 2019) which rely on new training algorithms and/or different training data preprocessing. • From a practical point of view, our adaptation reduces the TTS latency from O(n) to O(1), which reduces the TTS response time significantly. We also demonstrate that our neural increm"
2020.findings-emnlp.346,W12-1814,0,0.404688,"the waveform for the first word, which is also played immediately (see Fig. 2). This results in an O(1) rather than O(n) latency. Experiments on English and Chinese TTS show that our approach achieves similar speech naturalness compared to full sentence methods, but only with a constant (1–2 words) latency.1 This paper makes following contributions: • From the model point of view, with monotonic attention in TTS, we don’t need to retrain the model, and only need to adapt the inference. This is different from all other pre1 There also exist incremental TTS efforts using non-neural techniques (Baumann and Schlangen, 2012c,b; Baumann, 2014b; Pouget et al., 2015; Yanagita et al., 2018) which are fundamentally different from our work. See also Sec. 5. b731afe6-X4evPTjAzgj7EjZJZDKPMUXKnhBxOXbUikI2Rw==-500ddfd78926 vious incremental adaptations in simultaneous translation, ASR and TTS (Ma et al., 2019; Novitasari et al., 2019; Yanagita et al., 2019) which rely on new training algorithms and/or different training data preprocessing. • From a practical point of view, our adaptation reduces the TTS latency from O(n) to O(1), which reduces the TTS response time significantly. We also demonstrate that our neural increm"
2020.findings-emnlp.346,W12-1641,0,0.0223187,"econd stage, being much slower, is more commonly parallel (Oord et al., 2018; Prenger et al., 2019). Despite these successes, standard full-sentence neural TTS systems still suffer from two types of latencies: (a) the computational latency (synthesizing time), which still grows linearly with the sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sent"
2020.findings-emnlp.346,W10-4301,0,0.0437413,"Peng et al., 2019), while the second stage, being much slower, is more commonly parallel (Oord et al., 2018; Prenger et al., 2019). Despite these successes, standard full-sentence neural TTS systems still suffer from two types of latencies: (a) the computational latency (synthesizing time), which still grows linearly with the sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computati"
2020.findings-emnlp.346,2020.acl-main.42,1,0.801949,"standard full-sentence neural TTS systems still suffer from two types of latencies: (a) the computational latency (synthesizing time), which still grows linearly with the sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sentence text/phonemes spectrogram wave audio play input latency time computational latency incremental with lookahead-1 tex"
2020.findings-emnlp.346,2020.acl-main.254,1,0.830514,"standard full-sentence neural TTS systems still suffer from two types of latencies: (a) the computational latency (synthesizing time), which still grows linearly with the sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sentence text/phonemes spectrogram wave audio play input latency time computational latency incremental with lookahead-1 tex"
2020.findings-emnlp.346,D19-1137,1,0.844534,"e sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sentence text/phonemes spectrogram wave audio play input latency time computational latency incremental with lookahead-1 text/phonemes k1 = 1 k2 = 0 spectrogram wave audio play time input latency comput. latency time saved Figure 2: Full-sentence TTS vs. our proposed incremental TTS with pref"
2020.findings-emnlp.346,P19-1582,1,0.827152,"e sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sentence text/phonemes spectrogram wave audio play input latency time computational latency incremental with lookahead-1 text/phonemes k1 = 1 k2 = 0 spectrogram wave audio play time input latency comput. latency time saved Figure 2: Full-sentence TTS vs. our proposed incremental TTS with pref"
2020.findings-emnlp.346,D19-1144,1,0.839715,"e sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sentence text/phonemes spectrogram wave audio play input latency time computational latency incremental with lookahead-1 text/phonemes k1 = 1 k2 = 0 spectrogram wave audio play time input latency comput. latency time saved Figure 2: Full-sentence TTS vs. our proposed incremental TTS with pref"
2020.findings-emnlp.346,2020.findings-emnlp.349,1,0.692511,"standard full-sentence neural TTS systems still suffer from two types of latencies: (a) the computational latency (synthesizing time), which still grows linearly with the sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sentence text/phonemes spectrogram wave audio play input latency time computational latency incremental with lookahead-1 tex"
2020.findings-emnlp.349,P19-1126,0,0.139367,"other hand, simultaneous speech-to-speech translation escalates the challenge by considering the smooth cooperation between the modules of speech recognition, translation and speech synthesis. In order to achieve simultaneous speech-tospeech translation (SSST), to the best of our knowledge, most recent approaches (Oda et al., 2014; Xiong et al., 2019) dismantle the entire system into a three-step pipelines, streaming Automatic Speech Recognition (ASR) (Sainath et al., 2020; Inaguma et al., 2020; Li et al., 2020), simultaneous Text-to-Text translation (sT2T) (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020a, 2019b,a), new decoding algorithms (Zheng"
2020.findings-emnlp.349,E17-1099,0,0.425458,"s (English, Chinese, etc.); on the other hand, simultaneous speech-to-speech translation escalates the challenge by considering the smooth cooperation between the modules of speech recognition, translation and speech synthesis. In order to achieve simultaneous speech-tospeech translation (SSST), to the best of our knowledge, most recent approaches (Oda et al., 2014; Xiong et al., 2019) dismantle the entire system into a three-step pipelines, streaming Automatic Speech Recognition (ASR) (Sainath et al., 2020; Inaguma et al., 2020; Li et al., 2020), simultaneous Text-to-Text translation (sT2T) (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020"
2020.findings-emnlp.349,P14-2090,0,0.338171,"s://sat-demo.github.io. Equal contribution Work done at Baidu Research. Current address: Kwai Inc., Seattle, WA, USA. ‡ … tgt speech unnatural pauses Sent. #1 Introduction † Sent. #2 src speech SOV languages (German, Japanese, etc.) and SVO languages (English, Chinese, etc.); on the other hand, simultaneous speech-to-speech translation escalates the challenge by considering the smooth cooperation between the modules of speech recognition, translation and speech synthesis. In order to achieve simultaneous speech-tospeech translation (SSST), to the best of our knowledge, most recent approaches (Oda et al., 2014; Xiong et al., 2019) dismantle the entire system into a three-step pipelines, streaming Automatic Speech Recognition (ASR) (Sainath et al., 2020; Inaguma et al., 2020; Li et al., 2020), simultaneous Text-to-Text translation (sT2T) (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve b"
2020.findings-emnlp.349,2020.acl-main.254,1,0.726839,") (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020a, 2019b,a), new decoding algorithms (Zheng et al., 2019c, 2020b), or multimodal 3928 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3928–3937 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Avg. latency (seconds) Avg. # of unnatural pauses 8 25 7 6 20 5 4 15 3 10 2 1 5 0 1.0 1.5 2.0 2.5 3.0 Source speech rate (words per second) Figure 2: Average Chinese and English speed rate distribution for different speakers. information (Imankulova et al., 2019). However, is it sufficient to only consider the effectiveness of sT2T and ignore the inter"
2020.findings-emnlp.349,D19-1137,1,0.780204,"2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020a, 2019b,a), new decoding algorithms (Zheng et al., 2019c, 2020b), or multimodal 3928 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3928–3937 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Avg. latency (seconds) Avg. # of unnatural pauses 8 25 7 6 20 5 4 15 3 10 2 1 5 0 1.0 1.5 2.0 2.5 3.0 Source speech rate (words per second) Figure 2: Average Chinese and English speed rate distribution for different speakers. information (Imankulova et al., 2019). However, is it sufficient to only consider the effectiveness of sT2T and ignore the interactions between other different components? Furthermore,"
2020.findings-emnlp.349,P19-1582,1,0.742053,"2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020a, 2019b,a), new decoding algorithms (Zheng et al., 2019c, 2020b), or multimodal 3928 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3928–3937 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Avg. latency (seconds) Avg. # of unnatural pauses 8 25 7 6 20 5 4 15 3 10 2 1 5 0 1.0 1.5 2.0 2.5 3.0 Source speech rate (words per second) Figure 2: Average Chinese and English speed rate distribution for different speakers. information (Imankulova et al., 2019). However, is it sufficient to only consider the effectiveness of sT2T and ignore the interactions between other different components? Furthermore,"
2020.findings-emnlp.349,D19-1144,1,0.852399,"2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020a, 2019b,a), new decoding algorithms (Zheng et al., 2019c, 2020b), or multimodal 3928 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3928–3937 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Avg. latency (seconds) Avg. # of unnatural pauses 8 25 7 6 20 5 4 15 3 10 2 1 5 0 1.0 1.5 2.0 2.5 3.0 Source speech rate (words per second) Figure 2: Average Chinese and English speed rate distribution for different speakers. information (Imankulova et al., 2019). However, is it sufficient to only consider the effectiveness of sT2T and ignore the interactions between other different components? Furthermore,"
2020.findings-emnlp.349,2020.acl-main.42,1,0.718068,") (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020a, 2019b,a), new decoding algorithms (Zheng et al., 2019c, 2020b), or multimodal 3928 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3928–3937 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Avg. latency (seconds) Avg. # of unnatural pauses 8 25 7 6 20 5 4 15 3 10 2 1 5 0 1.0 1.5 2.0 2.5 3.0 Source speech rate (words per second) Figure 2: Average Chinese and English speed rate distribution for different speakers. information (Imankulova et al., 2019). However, is it sufficient to only consider the effectiveness of sT2T and ignore the inter"
2020.findings-emnlp.37,D15-1051,0,0.0445046,"Missing"
2020.findings-emnlp.37,N18-1055,0,0.0524309,"Missing"
2020.findings-emnlp.37,N18-1202,0,0.279938,"al. (2019) proposed MUDE, which uses a transformer-encoder (Vaswani et al., 2017) to encode character sequences as word representations and used an LSTM (Hochreiter and Schmidhuber, 1997) for the correction of each word. They also used a Gated Recurrent Units (GRU) (Cho et al., 2014) to perform the character-level correction as an auxiliary task during training. We train ScRNN (Sakaguchi et al., 2017) and MUDE (Wang et al., 2019), both of which are stand-alone neural spelling correctors, on our dataset as baselines. 409 Overview. As row 11 of Table 1 shows, finetuning the Subword (WordPiece (Peters et al., 2018)) encoder model with LM initialization (ERNIE 2.0 (Sun et al., 2019)) on the augmented dataset with synthetic character-level misspellings yields the best performance. Without leveraging a pre-trained LM, the Word+Char Encoder model trained on the augmented dataset with synthetic character-level misspellings performs the best (row 6). In fact, the differences between these approaches are small. In Table 2, we calculate real-word and non-word correction performance to explain the effect of each training technique applied. Note that as shown in Figure 1, because non-word misspellings are preproc"
2020.findings-emnlp.37,E99-1023,0,0.157919,"Missing"
2020.findings-emnlp.37,D09-1093,0,0.0758269,"Missing"
2020.findings-emnlp.37,W14-1713,0,0.0281633,"019), and only focus on denoising the input texts from orthographic perspective without leveraging the retained semantics of the noisy input. On the other hand, Tal Weiss proposed Deep Spelling (Weiss), which uses the sequence-tosequence architecture (Sutskever et al., 2014; Bahdanau et al., 2014) to generate corrected sentences. Note that Deep Spelling is essentially not a spelling corrector since spelling correction must focus only on the misspelled words, not on transforming the whole sentences. For similar reasons, spelling correction is also different from GEC (Grammar Error Correction) (Zhang and Wang, 2014; JunczysDowmunt et al., 2018). As a background, recently pre-trained neural LMs (Peters et al., 2018; Devlin et al., 2018; Yang et al., 2019; Radford et al., 2019; Sun et al., 2019) trained on large corpus on various pre-training tasks have made an enormous success on various benchmarks. These LMs captures the probability of a word or a sentence given their context, which plays a crucial role in correcting real-word misspellings. However, all of the LMs mentioned are based on subword embeddings, such as WordPiece (Peters et al., 2018) or Byte Pair Encoding (Gage, 1994) to avoid OOV words. 5 C"
2020.findings-emnlp.37,Y06-1009,0,0.160274,"Missing"
2021.emnlp-main.473,W09-0434,0,0.0493509,"ce). (mtns: mountains) Introduction Simultaneous translation, which starts translation before the source sentence ends, is substantially more challenging than full-sentence translation due to partial observation of the (incrementally revealed) source sentence. Recently, it has witnessed great progress thanks to fixed-latency policies (such as wait-k) (Ma et al., 2019) and adaptive policies (Gu et al., 2017; Arivazhagan et al., 2019). However, all state-of-the-art simultaneous translation models are trained on conventional parallel text which involve many unnecessary long-distance reorderings (Birch et al., 2009; Braune et al., 2012); see Fig. 1 for an example. The simultaneous translation models trained using these parallel sentences will learn to either make bold hallucinations (for fixed-latency policies) or introduce long delays (for adaptive ones). Alternatively, one may want to use transcribed corpora from professional simultaneous interpretation (Matsubara et al., 2002; Bendazzoli et al., 2005; Neubig et al., 2018). These data are more monotonic in word-order, but they are all very ⇤ zh¯onggu´o de small in size due to the high cost of data collection (e.g., the NAIST one (Neubig et al., 2018)"
2021.emnlp-main.473,2012.eamt-1.42,0,0.0287714,"s) Introduction Simultaneous translation, which starts translation before the source sentence ends, is substantially more challenging than full-sentence translation due to partial observation of the (incrementally revealed) source sentence. Recently, it has witnessed great progress thanks to fixed-latency policies (such as wait-k) (Ma et al., 2019) and adaptive policies (Gu et al., 2017; Arivazhagan et al., 2019). However, all state-of-the-art simultaneous translation models are trained on conventional parallel text which involve many unnecessary long-distance reorderings (Birch et al., 2009; Braune et al., 2012); see Fig. 1 for an example. The simultaneous translation models trained using these parallel sentences will learn to either make bold hallucinations (for fixed-latency policies) or introduce long delays (for adaptive ones). Alternatively, one may want to use transcribed corpora from professional simultaneous interpretation (Matsubara et al., 2002; Bendazzoli et al., 2005; Neubig et al., 2018). These data are more monotonic in word-order, but they are all very ⇤ zh¯onggu´o de small in size due to the high cost of data collection (e.g., the NAIST one (Neubig et al., 2018) has only 387k target w"
2021.emnlp-main.473,J93-2003,0,0.208354,"Missing"
2021.emnlp-main.473,P05-1066,0,0.443793,"Missing"
2021.emnlp-main.473,N13-1073,0,0.0992923,"Missing"
2021.emnlp-main.473,D08-1089,0,0.0613208,"cific syntactic transformations rules to rewrite the original reference into a more monotonic one. By comparison, our work is much more general in the following aspects: (a) it is not restricted to any language pairs; (b) it does not require language-specific grammar rules or syntactic processing tools; and (c) it can generate pseudo-references with a specific policy according to the requirement of latency. 7 6 Related Work In the pre-neural statistical MT era, there exist several efforts using source-side reordering as a preprocessing step for full-sentence translation (Collins et al., 2005; Galley and Manning, 2008; Xu et al., 2009). Unlike this work, they rewrite the source sentences. But in the simultaneous translated scenario, the source input is incrementally revealed Conclusions We have proposed a simple but effective method to generate more monotonic pseudo references for simultaneous translation. These pseudo references cause fewer anticipations and can substantially improve simultaneous translation quality. Acknowledgements This work is supported in part by NSF IIS-1817231 and IIS-2009071 (L.H.). 5861 References Naveen Arivazhagan, Colin Cherry, Wolfgang Macherey, Chung-Cheng Chiu, Semih Yavuz,"
2021.emnlp-main.473,E17-1099,0,0.162918,"o anticipate aggressively, along with the ideal pseudo-references with different wait-k policies. Larger k improves fluency but sacrifices latency (pseudo-refs with k 4 are identical to the original reference). (mtns: mountains) Introduction Simultaneous translation, which starts translation before the source sentence ends, is substantially more challenging than full-sentence translation due to partial observation of the (incrementally revealed) source sentence. Recently, it has witnessed great progress thanks to fixed-latency policies (such as wait-k) (Ma et al., 2019) and adaptive policies (Gu et al., 2017; Arivazhagan et al., 2019). However, all state-of-the-art simultaneous translation models are trained on conventional parallel text which involve many unnecessary long-distance reorderings (Birch et al., 2009; Braune et al., 2012); see Fig. 1 for an example. The simultaneous translation models trained using these parallel sentences will learn to either make bold hallucinations (for fixed-latency policies) or introduce long delays (for adaptive ones). Alternatively, one may want to use transcribed corpora from professional simultaneous interpretation (Matsubara et al., 2002; Bendazzoli et al.,"
2021.emnlp-main.473,D15-1006,0,0.0415993,"Missing"
2021.emnlp-main.473,W04-3230,0,0.177014,"Missing"
2021.emnlp-main.473,matsubara-etal-2002-bilingual,0,0.163615,"19) and adaptive policies (Gu et al., 2017; Arivazhagan et al., 2019). However, all state-of-the-art simultaneous translation models are trained on conventional parallel text which involve many unnecessary long-distance reorderings (Birch et al., 2009; Braune et al., 2012); see Fig. 1 for an example. The simultaneous translation models trained using these parallel sentences will learn to either make bold hallucinations (for fixed-latency policies) or introduce long delays (for adaptive ones). Alternatively, one may want to use transcribed corpora from professional simultaneous interpretation (Matsubara et al., 2002; Bendazzoli et al., 2005; Neubig et al., 2018). These data are more monotonic in word-order, but they are all very ⇤ zh¯onggu´o de small in size due to the high cost of data collection (e.g., the NAIST one (Neubig et al., 2018) has only 387k target words). More importantly, simultaneous interpreters tend to summarize and inevitably make many mistakes (Shimizu et al., 2014; Xiong et al., 2019; Zheng et al., 2020) due to the high cognitive load and intense time pressure during interpretation (Camayd-Freixas, 2011). How can we combine the merits of both types of data, and obtain a large-scale, m"
2021.emnlp-main.473,D19-5211,0,0.0582191,"Missing"
2021.emnlp-main.473,P16-1162,0,0.151925,"Missing"
2021.emnlp-main.473,shimizu-etal-2014-collection,0,0.0180508,"will learn to either make bold hallucinations (for fixed-latency policies) or introduce long delays (for adaptive ones). Alternatively, one may want to use transcribed corpora from professional simultaneous interpretation (Matsubara et al., 2002; Bendazzoli et al., 2005; Neubig et al., 2018). These data are more monotonic in word-order, but they are all very ⇤ zh¯onggu´o de small in size due to the high cost of data collection (e.g., the NAIST one (Neubig et al., 2018) has only 387k target words). More importantly, simultaneous interpreters tend to summarize and inevitably make many mistakes (Shimizu et al., 2014; Xiong et al., 2019; Zheng et al., 2020) due to the high cognitive load and intense time pressure during interpretation (Camayd-Freixas, 2011). How can we combine the merits of both types of data, and obtain a large-scale, more monotonic parallel corpora for simultaneous translation? We propose a simple and effective technique to generate pseudo-references with fewer reorderings; see the “Pseudo-Refs” in Fig. 1. While previous work (He et al., 2015) addresses this problem via languagespecific hand-written rules, our technique can be easily adopted to any language pairs without using extra dat"
2021.emnlp-main.473,D18-1357,1,0.871497,"Missing"
2021.emnlp-main.473,N09-1028,0,0.0356102,"tions rules to rewrite the original reference into a more monotonic one. By comparison, our work is much more general in the following aspects: (a) it is not restricted to any language pairs; (b) it does not require language-specific grammar rules or syntactic processing tools; and (c) it can generate pseudo-references with a specific policy according to the requirement of latency. 7 6 Related Work In the pre-neural statistical MT era, there exist several efforts using source-side reordering as a preprocessing step for full-sentence translation (Collins et al., 2005; Galley and Manning, 2008; Xu et al., 2009). Unlike this work, they rewrite the source sentences. But in the simultaneous translated scenario, the source input is incrementally revealed Conclusions We have proposed a simple but effective method to generate more monotonic pseudo references for simultaneous translation. These pseudo references cause fewer anticipations and can substantially improve simultaneous translation quality. Acknowledgements This work is supported in part by NSF IIS-1817231 and IIS-2009071 (L.H.). 5861 References Naveen Arivazhagan, Colin Cherry, Wolfgang Macherey, Chung-Cheng Chiu, Semih Yavuz, Ruoming Pang, Wei"
2021.emnlp-main.473,D19-1144,1,0.771848,"eration Since the wait-k models are trained on conventional full-sentence bitexts, their performance is hurt by unnecessary long-distance reorderings between the source and target sentences. For example, the training sentence pair in Fig. 2, a wait-2 model learns to output y1 =“there” after observing x1 x2 =“-˝ Ñ” (china ’s) which seems to induce a good anticipation (“-˝ Ñ...” $ “There ...”), but it could be a wrong hallucination in many other contexts (e.g., “-˝ Ñ WS à $” $ “Chinese streets are crowded”, not “There ...”). Even for adaptive policies (Gu et al., 2017; Arivazhagan et al., 2019; Zheng et al., 2019a), the model only learns a higher latency policy (wait till x4 =“ ”) by training on the example in Fig. 2. As a result, training-time wait-k models tend to do wild hallucinations (Ma et al., 2019). To solve this problem, we propose to generate pseudo-references which are non-anticipatory under a specific simultaneous translation policy by the method introduced in Section 3.1. Meanwhile, we also propose to use BLEU score to filter the generated pseudo-references to guarantee that they are semantic preserving in Section 3.2. 3.1 Generating Pseudo-References with Test-time Wait-k (x,y⇤ )2D To ge"
2021.emnlp-main.473,2020.findings-emnlp.349,1,0.789265,"ons (for fixed-latency policies) or introduce long delays (for adaptive ones). Alternatively, one may want to use transcribed corpora from professional simultaneous interpretation (Matsubara et al., 2002; Bendazzoli et al., 2005; Neubig et al., 2018). These data are more monotonic in word-order, but they are all very ⇤ zh¯onggu´o de small in size due to the high cost of data collection (e.g., the NAIST one (Neubig et al., 2018) has only 387k target words). More importantly, simultaneous interpreters tend to summarize and inevitably make many mistakes (Shimizu et al., 2014; Xiong et al., 2019; Zheng et al., 2020) due to the high cognitive load and intense time pressure during interpretation (Camayd-Freixas, 2011). How can we combine the merits of both types of data, and obtain a large-scale, more monotonic parallel corpora for simultaneous translation? We propose a simple and effective technique to generate pseudo-references with fewer reorderings; see the “Pseudo-Refs” in Fig. 1. While previous work (He et al., 2015) addresses this problem via languagespecific hand-written rules, our technique can be easily adopted to any language pairs without using extra data or expert linguistic knowledge. Trainin"
2021.emnlp-main.473,D19-1137,1,0.848198,"eration Since the wait-k models are trained on conventional full-sentence bitexts, their performance is hurt by unnecessary long-distance reorderings between the source and target sentences. For example, the training sentence pair in Fig. 2, a wait-2 model learns to output y1 =“there” after observing x1 x2 =“-˝ Ñ” (china ’s) which seems to induce a good anticipation (“-˝ Ñ...” $ “There ...”), but it could be a wrong hallucination in many other contexts (e.g., “-˝ Ñ WS à $” $ “Chinese streets are crowded”, not “There ...”). Even for adaptive policies (Gu et al., 2017; Arivazhagan et al., 2019; Zheng et al., 2019a), the model only learns a higher latency policy (wait till x4 =“ ”) by training on the example in Fig. 2. As a result, training-time wait-k models tend to do wild hallucinations (Ma et al., 2019). To solve this problem, we propose to generate pseudo-references which are non-anticipatory under a specific simultaneous translation policy by the method introduced in Section 3.1. Meanwhile, we also propose to use BLEU score to filter the generated pseudo-references to guarantee that they are semantic preserving in Section 3.2. 3.1 Generating Pseudo-References with Test-time Wait-k (x,y⇤ )2D To ge"
2021.findings-acl.406,N18-2079,0,0.0520152,"Missing"
2021.findings-acl.406,N19-1202,0,0.0500223,"Missing"
2021.findings-acl.406,D18-2012,0,0.0310561,"riments We conduct experiments on English-to-German (En→De) and English-Spanish (En→Es) translation on MuST-C (Di Gangi et al., 2019). We employ Transformer (Vaswani et al., 2017) as the basic architecture and LSTM (Hochreiter and Schmidhuber, 1997) for LM. For streaming ASR decoding we use a beam size of 5. Translation decoding is greedy due to incremental commitment. Raw audios are processed with Kaldi (Povey et al., 2011) to extract 80-dimensional log-Mel filterbanks stacked with 3-dimensional pitch features using a 10ms step size and a 25ms window size. Text is processed by SentencePiece (Kudo and Richardson, 2018) with a joint vocabulary size of 8K. We take Transformer (Vaswani et al., 2017) as our base architecture, which follows 2 layers of 2D convolution of size 3 with stride size of 2. The Transformer model has 12 encoder layers and 5s k = inf 15 Ren et al.(2020) test-k Ren et al.(2020) LCP SH 10 ST ,θ ASR θfull full (s,y∗ ,z∗ )∈D 4s 20 Joint Training between ST and ASR Different from existing simultaneous translation solutions from (Ren et al., 2020; Ma et al., 2020b,a), which make adaptations over vanilla E2E-ST architecture as shown in gray line of Fig. 4, we instead use simple MTL architecture"
2021.findings-acl.406,2020.aacl-main.58,0,0.260383,"/AEQYo2n&lt;/latexit&gt; 2018; Ma et al., 2019; Zheng et al., 2019a,b, 2020a; Arivazhagan et al., 2019). However, the cascaded approach inevitably suffers from two limitations: (a) error propagation, where streaming ASR’s mistakes confuse the translation module (which are trained on clean text), and this problem worsens with noisy environments and accented speech; and (b) extra latency, where the translation module has to wait until streaming ASR’s output stabilizes, as ASR by default can repeatedly revise its output (see Fig. 1). To overcome the above issues, some recent efforts (Ren et al., 2020; Ma et al., 2020b,a) attempt to directly translate the source speech into target text simultaneously by adapting text-based wait-k strategy (Ma et al., 2019). However, unlike simultaneous translation whose input is already segmented into words or subwords, in speech translation, the key challenge is to figure out the number of valid tokens within a given source speech segment in or4618 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4618–4624 August 1–6, 2021. ©2021 Association for Computational Linguistics der to apply the wait-k policy. Ma et al. (2020b,a) simply assume a f"
2021.findings-acl.406,P14-2090,0,0.0223802,"e-language speech into targetlanguage text, and is widely useful in many crosslingual communication scenarios such as international travels and multinational conferences. The conventional approach to this problem is a cascaded one (Arivazhagan et al., 2020; Xiong et al., 2019; Zheng et al., 2020b), involving a pipeline of two steps. First, the streaming automatic speech recognition (ASR) module transcribes the input speech on the fly (Moritz et al., 2020; Wang et al., 2020), and then a simultaneous text-to-text translation module translates the partial transcription into target-language text (Oda et al., 2014; Dalvi et al., ∗ &lt;latexit sha1_base64=""ZHnLwo37xRG7ecG/MDNfJXlsmbk=""&gt;AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rGi/YA2lM120i7dbMLuRqihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEsG1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6mfqtR1Sax/LBjBP0IzqQPOSMGivdP/WqvVLZrbgzkGXi5aQMOeq90le3H7M0QmmYoFp3PDcxfkaV4UzgpNhNNSaUjegAO5ZKGqH2s9mpE3JqlT4JY2VLGjJTf09kNNJ6HAW2M6JmqBe9qfif10lNeOVnXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtO0YbgLb68TJrnFa9ace8uyrXrPI4CHMMJnIEHl1CDW6hDAxgM4Ble4c0Rzovz7nzMW1ecfOYI/sD5/AEX9o2s&lt;/latexit&gt; &lt;latexit sha1_base64=""GDL7BDHOdr32z1dkxvdmjx3Yb5E=""&gt;AAAB6nicbVDLSgNBEOyNrxh"
2021.findings-acl.406,2020.acl-main.350,0,0.287403,"fn3fmYtxacfOYQ/sD5/AEQYo2n&lt;/latexit&gt; 2018; Ma et al., 2019; Zheng et al., 2019a,b, 2020a; Arivazhagan et al., 2019). However, the cascaded approach inevitably suffers from two limitations: (a) error propagation, where streaming ASR’s mistakes confuse the translation module (which are trained on clean text), and this problem worsens with noisy environments and accented speech; and (b) extra latency, where the translation module has to wait until streaming ASR’s output stabilizes, as ASR by default can repeatedly revise its output (see Fig. 1). To overcome the above issues, some recent efforts (Ren et al., 2020; Ma et al., 2020b,a) attempt to directly translate the source speech into target text simultaneously by adapting text-based wait-k strategy (Ma et al., 2019). However, unlike simultaneous translation whose input is already segmented into words or subwords, in speech translation, the key challenge is to figure out the number of valid tokens within a given source speech segment in or4618 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4618–4624 August 1–6, 2021. ©2021 Association for Computational Linguistics der to apply the wait-k policy. Ma et al. (2020b,a)"
2021.findings-acl.406,2020.acl-main.254,1,0.845587,"sing ASR. Our work (c) uses the intermediate results of the streaming ASR module to guide the decoding policy of (but not feed as input to) the speech translation module. Extra delays between ASR and MT are reduced in direct translation systems (b–c). Simultaneous speech-to-text translation incrementally translates source-language speech into targetlanguage text, and is widely useful in many crosslingual communication scenarios such as international travels and multinational conferences. The conventional approach to this problem is a cascaded one (Arivazhagan et al., 2020; Xiong et al., 2019; Zheng et al., 2020b), involving a pipeline of two steps. First, the streaming automatic speech recognition (ASR) module transcribes the input speech on the fly (Moritz et al., 2020; Wang et al., 2020), and then a simultaneous text-to-text translation module translates the partial transcription into target-language text (Oda et al., 2014; Dalvi et al., ∗ &lt;latexit sha1_base64=""ZHnLwo37xRG7ecG/MDNfJXlsmbk=""&gt;AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rGi/YA2lM120i7dbMLuRqihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEsG1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6mfqtR1Sax/LBjBP0IzqQPOSMGivdP/WqvVLZrbgzkGXi5a"
2021.findings-acl.406,D19-1137,1,0.851583,"I4CHMMJnIEHl1CDW6hDAxgM4Ble4c2Rzovz7nzMW1ecfOYI/sD5/AER5o2o&lt;/latexit&gt; &lt;latexit sha1_base64=""KvmtD2Qzol9BIWTg7UGnEKQGWco=""&gt;AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK9gPaUDbbTbt0swm7E6GG/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWjm6nfeuTaiFg94DjhfkQHSoSCUbTS/VPP65UrbtWdgSwTLycVyFHvlb+6/ZilEVfIJDWm47kJ+hnVKJjkk1I3NTyhbEQHvGOpohE3fjY7dUJOrNInYaxtKSQz9fdERiNjxlFgOyOKQ7PoTcX/vE6K4ZWfCZWkyBWbLwpTSTAm079JX2jOUI4toUwLeythQ6opQ5tOyYbgLb68TJpnVe+i6t6dV2rXeRxFOIJjOAUPLqEGt1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AEQYo2n&lt;/latexit&gt; 2018; Ma et al., 2019; Zheng et al., 2019a,b, 2020a; Arivazhagan et al., 2019). However, the cascaded approach inevitably suffers from two limitations: (a) error propagation, where streaming ASR’s mistakes confuse the translation module (which are trained on clean text), and this problem worsens with noisy environments and accented speech; and (b) extra latency, where the translation module has to wait until streaming ASR’s output stabilizes, as ASR by default can repeatedly revise its output (see Fig. 1). To overcome the above issues, some recent efforts (Ren et al., 2020; Ma et al., 2020b,a) attempt to directly translate the source"
2021.findings-acl.406,P19-1582,1,0.796432,"I4CHMMJnIEHl1CDW6hDAxgM4Ble4c2Rzovz7nzMW1ecfOYI/sD5/AER5o2o&lt;/latexit&gt; &lt;latexit sha1_base64=""KvmtD2Qzol9BIWTg7UGnEKQGWco=""&gt;AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK9gPaUDbbTbt0swm7E6GG/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWjm6nfeuTaiFg94DjhfkQHSoSCUbTS/VPP65UrbtWdgSwTLycVyFHvlb+6/ZilEVfIJDWm47kJ+hnVKJjkk1I3NTyhbEQHvGOpohE3fjY7dUJOrNInYaxtKSQz9fdERiNjxlFgOyOKQ7PoTcX/vE6K4ZWfCZWkyBWbLwpTSTAm079JX2jOUI4toUwLeythQ6opQ5tOyYbgLb68TJpnVe+i6t6dV2rXeRxFOIJjOAUPLqEGt1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AEQYo2n&lt;/latexit&gt; 2018; Ma et al., 2019; Zheng et al., 2019a,b, 2020a; Arivazhagan et al., 2019). However, the cascaded approach inevitably suffers from two limitations: (a) error propagation, where streaming ASR’s mistakes confuse the translation module (which are trained on clean text), and this problem worsens with noisy environments and accented speech; and (b) extra latency, where the translation module has to wait until streaming ASR’s output stabilizes, as ASR by default can repeatedly revise its output (see Fig. 1). To overcome the above issues, some recent efforts (Ren et al., 2020; Ma et al., 2020b,a) attempt to directly translate the source"
2021.findings-acl.406,2020.findings-emnlp.349,1,0.697747,"sing ASR. Our work (c) uses the intermediate results of the streaming ASR module to guide the decoding policy of (but not feed as input to) the speech translation module. Extra delays between ASR and MT are reduced in direct translation systems (b–c). Simultaneous speech-to-text translation incrementally translates source-language speech into targetlanguage text, and is widely useful in many crosslingual communication scenarios such as international travels and multinational conferences. The conventional approach to this problem is a cascaded one (Arivazhagan et al., 2020; Xiong et al., 2019; Zheng et al., 2020b), involving a pipeline of two steps. First, the streaming automatic speech recognition (ASR) module transcribes the input speech on the fly (Moritz et al., 2020; Wang et al., 2020), and then a simultaneous text-to-text translation module translates the partial transcription into target-language text (Oda et al., 2014; Dalvi et al., ∗ &lt;latexit sha1_base64=""ZHnLwo37xRG7ecG/MDNfJXlsmbk=""&gt;AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rGi/YA2lM120i7dbMLuRqihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEsG1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6mfqtR1Sax/LBjBP0IzqQPOSMGivdP/WqvVLZrbgzkGXi5a"
C08-5001,H05-1036,0,0.0149676,"eration Algorithm (REA) (Jim´enez and Marzal, 1999) uses a lazy computation method on top of the Viterbi algorithm to eﬃciently compute the ith-best solution based on the 1st, 2nd, ..., (i − 1)th solutions. A simple k-best Dijkstra algorithm is described in (Mohri and Riley, 2002). For the hypergraph case, the REA algorithm has been adapted for k-best derivations (Jim´enez and Marzal, 2000; Huang and Chiang, 2005). Applications of this algorithm include k-best parsing (McDonald et al., 2005; Mohri and Roark, 2006) and machine translation (Chiang, 2007). It is also implemented as part of Dyna (Eisner et al., 2005), a generic langauge for dynamic programming. The k-best extension of the Knuth Algorithm is studied by Huang (2005). A separate problem, k-shortest hyperpaths, has been studied by Nielsen et al. (2005). Eppstein (2001) compiles an annotated bibliography for k-shortest-path and other related k-best problems. 7 Conclusion This report surveys two frameworks for formalizing dynamic programming and presents two important classes of DP algorithms under these frameworks. We focused on 1-best optimization problems but also discussed other scenarios like non-optimization problems and k-best solutions."
C08-5001,J02-3001,0,0.00703412,"th-best solutions. Both extensions have many applications in NLP. For the former, algorithms based on the Inside semiring (Table 1), including the forward-backward algorithm (Baum, 1972) and Inside-Outside algorithm (Baker, 1979; Lari and Young, 1990) are widely used for unsupervised training with the EM algorithm (Dempster et al., 1977). For the latter, since NLP is often a pipeline of several modules, where the 1-best solution from one module might not be the best input for the next module, and one prefers to postpone disambiguation by propogating a k-best list of candidates (Collins, 2000; Gildea and Jurafsky, 2002; Charniak and Johnson, 2005; Huang and Chiang, 2005). The k-best list is also frequently used in discriminative learning to approximate the whole set of candidates which is usually exponentially large (Och, 2003; McDonald et al., 2005). 6.1 Beyond Optimization Problems We know that in optimization problems, the criteria for using dynamic programming is monotonicity (deﬁnitions 6 and 16). But in non-optimization problems, since there is no comparison, this criteria is no longer applicable. Then when can we apply dynamic programming to a non-optimization problem? Cormen et al. (1990) develop a"
C08-5001,N04-1014,0,0.0144823,"n X → Y Z. The weight function f is simply f (a, b) = a ⊗ b ⊗ w(X → Y Z). The Chomsky Normal Form ensures acyclicity of the hypergraph but there are multiple topological orderings which result in diﬀerent variants of the CKY algorithm, e.g., bottom-up CKY, left-to-right CKY, and right-toleft CKY, etc. 5.2 Knuth Algorithm Knuth (1977) generalizes the Dijkstra algorithm to what he calls the grammar problem, which essentially corresponds to the search problem in a monotonic superior hypergraph (see Table 3). However, he does not provide 13 13 an eﬃcient implementation nor analysis of complexity. Graehl and Knight (2004) present an implementation that runs in time O(V log V + E) using the method described in Algorithm 5 to ensure that every hyperedge is visited only once (assuming the priority queue is implemented as a Fibonaaci heap; for binary heap, it runs in O((V + E) log V )). Algorithm 6 Knuth Algorithm. 1: procedure Knuth(H) 2: Initialize(H) 3: Q ← V [H]  prioritized by d-values 4: for each hyperedge e do 5: r[e] ← |e| 6: while Q = Ø do 7: v ← Extract-Min(Q) 8: for each edge e in FS (v) do 9: e is ({u1 , u2 , · · · , u|e |}, h(e), fe ) 10: r[e] ← r[e] − 1 11: if r[e] == 0 then 12: d(h(e))⊕ = fe (d(u1"
C08-5001,W05-1506,1,0.944941,"rogramming (Cormen et al., 2001). Deﬁnition 18. A function f : Rm → R is superior if the result of function application is worse than each of its argument: ∀i ∈ 1..m, ai  f (a1 , · · · , ai , · · · , am ). A hypergraph H is superior if every weight function f in H is superior. 10 10 4.2 Derivations To do optimization we need to extend the notion of paths in graphs to hypergraphs. This is, however, not straightforward due to the assymmetry of the head and the tail in a hyperedge and there have been multiple proposals in the literature. Here we follow the recursive deﬁnition of derivations in (Huang and Chiang, 2005). See Section 6 for the alternative notion of hyperpaths. Deﬁnition 19. A derivation D of a vertex v in a hypergraph H, its size |D |and its weight w(D) are recursively deﬁned as follows: • If e ∈ BS(v) with |e |= 0, then D = e,  is a derivation of v, its size |D |= 1, and its weight w(D) = fe (). • If e ∈ BS(v) where |e |> 0 and Di is a derivation of Ti (e) for 1 ≤ i ≤ |e|, then D = e, D1 · · · D|e | is a derivation of v, its size |D |= |e| 1 + i=1 |Di |and its weight w(D) = fe (w(D1 ), . . . , w(D|e |)). The ordering on weights in R induces an ordering on derivations: D  D iﬀ w(D)"
C08-5001,N03-1016,0,0.0522323,"binary heap, it runs in O((V + E) log V )). Algorithm 6 Knuth Algorithm. 1: procedure Knuth(H) 2: Initialize(H) 3: Q ← V [H]  prioritized by d-values 4: for each hyperedge e do 5: r[e] ← |e| 6: while Q = Ø do 7: v ← Extract-Min(Q) 8: for each edge e in FS (v) do 9: e is ({u1 , u2 , · · · , u|e |}, h(e), fe ) 10: r[e] ← r[e] − 1 11: if r[e] == 0 then 12: d(h(e))⊕ = fe (d(u1 ), d(u2 ), · · · , d(u|e |)) 13: Decrease-Key(Q, h(e)) 5.2.1 A* Algorithm on Hypergraphs We can also extend the A* idea to hypergraphs to speed up the Knuth Algorithm. A speciﬁc case of this algorithm is the A* parsing of Klein and Manning (2003) where they achieve signiﬁcant speed up using carefully designed heuristic functions. More formally, we ﬁrst need to extend the concept of (exact) outside cost from Eq. 5:  1 v=t α(v) =  (7) D∈D(v,t) w(D) v = t where D(v, t) is the set of (partial) derivations using v as a leaf node. This outside cost can be computed from top-down following the inverse topological order: for each vertex v, for each incoming hyperedge e = ({u1 , . . . , u|e |}, v, fe ) ∈ BS (v), we update α(ui ) ⊕ = fe (d(u1 ) . . . d(ui−1 ), α(v), d(ui+1 ) . . . d(u|e |)) for each i. Basically we replace d(ui ) by α(v) for"
C08-5001,P05-1012,0,0.287342,"Young, 1990) are widely used for unsupervised training with the EM algorithm (Dempster et al., 1977). For the latter, since NLP is often a pipeline of several modules, where the 1-best solution from one module might not be the best input for the next module, and one prefers to postpone disambiguation by propogating a k-best list of candidates (Collins, 2000; Gildea and Jurafsky, 2002; Charniak and Johnson, 2005; Huang and Chiang, 2005). The k-best list is also frequently used in discriminative learning to approximate the whole set of candidates which is usually exponentially large (Och, 2003; McDonald et al., 2005). 6.1 Beyond Optimization Problems We know that in optimization problems, the criteria for using dynamic programming is monotonicity (deﬁnitions 6 and 16). But in non-optimization problems, since there is no comparison, this criteria is no longer applicable. Then when can we apply dynamic programming to a non-optimization problem? Cormen et al. (1990) develop a more general criteria of closed semiring where ⊕ is idempotent and inﬁnite sums are well-deﬁned and present a more sophisticated algorithm that can be proved to work for all closed semirings. This deﬁnition is still not general enough s"
C08-5001,N06-1040,0,0.0130375,"s the modularity of semirings results in much faster k-best algorithms. For example, the Recursive Enumeration Algorithm (REA) (Jim´enez and Marzal, 1999) uses a lazy computation method on top of the Viterbi algorithm to eﬃciently compute the ith-best solution based on the 1st, 2nd, ..., (i − 1)th solutions. A simple k-best Dijkstra algorithm is described in (Mohri and Riley, 2002). For the hypergraph case, the REA algorithm has been adapted for k-best derivations (Jim´enez and Marzal, 2000; Huang and Chiang, 2005). Applications of this algorithm include k-best parsing (McDonald et al., 2005; Mohri and Roark, 2006) and machine translation (Chiang, 2007). It is also implemented as part of Dyna (Eisner et al., 2005), a generic langauge for dynamic programming. The k-best extension of the Knuth Algorithm is studied by Huang (2005). A separate problem, k-shortest hyperpaths, has been studied by Nielsen et al. (2005). Eppstein (2001) compiles an annotated bibliography for k-shortest-path and other related k-best problems. 7 Conclusion This report surveys two frameworks for formalizing dynamic programming and presents two important classes of DP algorithms under these frameworks. We focused on 1-best optimiza"
C08-5001,J03-1006,0,0.193887,"Extract-Min(Q) 6: for each edge e = (v, u) in FS (v) do 7: d(u)⊕ = d(v) ⊗ w(e) 8: Decrease-Key(Q, u)  prioritized by d-values The time complexity of Dijkstra Algorithm is O((E + V ) log V ) with a binary heap, or O(E +V log V ) with a Fibonacci heap (Cormen et al., 2001). 7 7 Since Fibonacci heap has an excessively high constant overhead, it is rarely used in real applications and we will focus on the more popular binary heap case below. For problems that satisfy both acyclicity and superiority, which include many applications in NLP such as HMM tagging, both Dijkstra and Viterbi can apply (Nederhof, 2003). So which one is better in this case? From the above analysis, the complexity O((V + E) log V ) of Dijkstra look inferior to Viterbi’s O(V + E) (due to the overhead for maintaining the priority queue), but keep in mind that we can quit as long as the solution for the target vertex t is found, at which time we can ensure the current solution for the target vertex is already optimal. So the real running time of Dijkstra depends on how early the target vertex is popped from the queue, or how good is the solution of the target vertex compared to those of other vertices, and whether this early ter"
C08-5001,P03-1021,0,0.0164725,"; Lari and Young, 1990) are widely used for unsupervised training with the EM algorithm (Dempster et al., 1977). For the latter, since NLP is often a pipeline of several modules, where the 1-best solution from one module might not be the best input for the next module, and one prefers to postpone disambiguation by propogating a k-best list of candidates (Collins, 2000; Gildea and Jurafsky, 2002; Charniak and Johnson, 2005; Huang and Chiang, 2005). The k-best list is also frequently used in discriminative learning to approximate the whole set of candidates which is usually exponentially large (Och, 2003; McDonald et al., 2005). 6.1 Beyond Optimization Problems We know that in optimization problems, the criteria for using dynamic programming is monotonicity (deﬁnitions 6 and 16). But in non-optimization problems, since there is no comparison, this criteria is no longer applicable. Then when can we apply dynamic programming to a non-optimization problem? Cormen et al. (1990) develop a more general criteria of closed semiring where ⊕ is idempotent and inﬁnite sums are well-deﬁned and present a more sophisticated algorithm that can be proved to work for all closed semirings. This deﬁnition is st"
C08-5001,P05-1022,0,\N,Missing
C08-5001,J08-3004,0,\N,Missing
C08-5001,J07-2003,0,\N,Missing
C10-2096,J07-2003,0,0.0215215,"s: P (r) = P (r|lhs(r))λ4 · P (r|rhs(r))λ5 · P (r|root(lhs(r))λ6 (7) · Plex (lhs(r)|rhs(r))λ7 · Plex (rhs(r)|lhs(r))λ8 , where the last two are the lexical probabilities between the terminals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003). Following Mi et al. (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al. (2008), and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives. For more detail, we refer to the algorithms of Mi et al. (2008). 6 Experiments 6.1 Data Preparation Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram l"
C10-2096,P05-1066,0,0.0822847,"Missing"
C10-2096,P09-1063,1,0.884078,"Missing"
C10-2096,P09-1064,0,0.0274916,"ove 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattice parsing, rule extraction and decoding. Ou"
C10-2096,D08-1076,0,0.0294601,"09) use forest in treeto-tree model, and improve 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattic"
C10-2096,P08-1115,0,0.33427,"alignments and parses in the training phase. However, with limited scope and too many redundancies, it is inefficient to search separately on each of these similar lists (Huang, 2008). Another efficient method is to use compact data structures instead of k-best lists. A lattice or forest, compactly encoded exponentially many derivations, have proven to be a promising technique. For example, Mi and Huang (2008), Mi et al. (2008), Liu et al. (2009) and Zhang et al. (2009) use forests in rule extraction and decoding phases to extract more general rules and weaken the influence of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and 837 Coling 2010: Poster Volume, pages 837–845, Beijing, August 2010 (0, 2, NR) 0 c0 :B`u 1 c1 :sh´ı (2, 3, CC) 2 c2 :yˇu (3, 5, NR) 3 c3 :Sh¯a (5, 6, VV) 4 c4 :l´ong 5 (6, 8, NN) c5 :jˇu 6 c6 :x´ıng 7 (5, 7, VV) (2, 3, P) (8, 9, NN) c7 :tˇao 8 c8 :l`un (7, 9, NN) 9 Figure 2: The lattice of the example:“ B`u sh´ı yˇu Sh¯a l´ong jˇu x´ıng tˇao l`un.” The solid lines show the 1-best result, which is wrong. Jiang et al. (2008b) stress the problem"
C10-2096,D08-1022,1,0.953642,"oxes. As a result, the search space is reduced without losing any derivations. Theoretically, the parsing over fs will speed up in O((n/nr )3 ). And our experiments in Section 6 show the efficiency of our new approach. It turns out that the parsing algorithm developed in lattice-parsing Section 3.2 can be used here without any change. The non-terminals inducted are also shown in Figure 3(c) in italic style. 4 Rule Extraction with Lattice & Forest We now explore the extraction algorithm from aligned source lattice-forest and target string2 , which is a tuple F, τ, a in Figure 5(b). Following Mi and Huang (2008), we extract minimal rules from a lattice-forest also in two steps: (1) frontier set computation (2) fragmentation Following the algorithms developed by Mi and Huang (2008) in Algorithm 1, all the nodes in frontier set (fs) are highlighted with gray in Figure 5(b). Our process of fragmentation (lines 1- 13) is to visit each frontier node v and initial a queue (open) of growing fragments with a pair of empty fragment and node v (line 3). Each fragment is associated with a list of expansion sites (front) being 2 For simplicity and consistency, we use character-based lattice-forest for the runnin"
C10-2096,N09-1046,0,0.0192013,"evious works on SMT. To alleviate the problem of parsing error in 1-best tree-to-string translation model, Mi et al. (2008) first use forest to direct translation. Then Mi and Huang (2008) use forest in rule extraction step. Following the same direction, Liu et al. (2009) use forest in treeto-tree model, and improve 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate th"
C10-2096,P06-1121,0,0.0508816,".pop()  expand frontier 11: for each e ∈ IN (u) do 12: f ← front ∪ (tails(e)  fs) 13: open.append(frag ∪ {e}, f ) (line 7) . Otherwise we pop one expansion node u to grow and spin-off new fragments by IN (u), adding new expansion sites (lines 11- 13), until all active fragments are complete and open queue is empty. The extra minimal rules extracted on latticeforest are listed at the right bottom of Figure 5(c). Compared with the forest-only approach, we can extract smaller and more general rules. After we get all the minimal rules, we compose two or more minimal rules into composed rules (Galley et al., 2006), which will be used in our experiments. For each rule r extracted, we also assign a fractional count which is computed by using insideoutside probabilities: c(r) = α(root(r)) · P (lhs(r)) · Q v∈yield(root(r)) β(TOP) β(v) , (1) where root(r) is the root of the rule, lhs(r) is the left-hand-side of rule, rhs(r) is the righthand-side of rule, P (lhs(r)) is the product of all probabilities of hyperedges involved in lhs(r), yield(root(r)) is the leave nodes, TOP is the root node of the forest, α(v) and β(v) are outside and inside probabilities, respectively. Then we compute three conditional proba"
C10-2096,W05-1506,1,0.746831,"nals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003). Following Mi et al. (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al. (2008), and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives. For more detail, we refer to the algorithms of Mi et al. (2008). 6 Experiments 6.1 Data Preparation Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. We use the 2002 NIST MT Evaluation test set as development set and the 2005 NIST MT Evaluation test"
C10-2096,P07-1019,1,0.838862,"r|lhs(r))λ4 · P (r|rhs(r))λ5 · P (r|root(lhs(r))λ6 (7) · Plex (lhs(r)|rhs(r))λ7 · Plex (rhs(r)|lhs(r))λ8 , where the last two are the lexical probabilities between the terminals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003). Following Mi et al. (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al. (2008), and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives. For more detail, we refer to the algorithms of Mi et al. (2008). 6 Experiments 6.1 Data Preparation Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser"
C10-2096,P08-1067,1,0.865734,"s a result, the search space is reduced without losing any derivations. Theoretically, the parsing over fs will speed up in O((n/nr )3 ). And our experiments in Section 6 show the efficiency of our new approach. It turns out that the parsing algorithm developed in lattice-parsing Section 3.2 can be used here without any change. The non-terminals inducted are also shown in Figure 3(c) in italic style. 4 Rule Extraction with Lattice & Forest We now explore the extraction algorithm from aligned source lattice-forest and target string2 , which is a tuple F, τ, a in Figure 5(b). Following Mi and Huang (2008), we extract minimal rules from a lattice-forest also in two steps: (1) frontier set computation (2) fragmentation Following the algorithms developed by Mi and Huang (2008) in Algorithm 1, all the nodes in frontier set (fs) are highlighted with gray in Figure 5(b). Our process of fragmentation (lines 1- 13) is to visit each frontier node v and initial a queue (open) of growing fragments with a pair of empty fragment and node v (line 3). Each fragment is associated with a list of expansion sites (front) being 2 For simplicity and consistency, we use character-based lattice-forest for the runnin"
C10-2096,P08-1102,1,0.871066,"Missing"
C10-2096,C08-1049,1,0.920892,"e of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and 837 Coling 2010: Poster Volume, pages 837–845, Beijing, August 2010 (0, 2, NR) 0 c0 :B`u 1 c1 :sh´ı (2, 3, CC) 2 c2 :yˇu (3, 5, NR) 3 c3 :Sh¯a (5, 6, VV) 4 c4 :l´ong 5 (6, 8, NN) c5 :jˇu 6 c6 :x´ıng 7 (5, 7, VV) (2, 3, P) (8, 9, NN) c7 :tˇao 8 c8 :l`un (7, 9, NN) 9 Figure 2: The lattice of the example:“ B`u sh´ı yˇu Sh¯a l´ong jˇu x´ıng tˇao l`un.” The solid lines show the 1-best result, which is wrong. Jiang et al. (2008b) stress the problems in reranking phase. Both lattices and forests have become popular in machine translation literature. However, to the best of our knowledge, previous work only focused on one module at a time. In this paper, we investigate the combination of lattice and forest (Section 2), as shown in Figure 1(b). We explore the algorithms of lattice parsing (Section 3.2), rule extraction (Section 4) and decoding (Section 5). More importantly, in the decoding step, our model can search among not only more parse-trees but also more segmentations encoded in the lattice-forests and can take"
C10-2096,P08-1023,1,0.95054,"ee separate phases. To alleviate this problem, an obvious solution is to widen the pipeline with k-best lists rather than 1-best results. For example Venugopal et al. (2008) use k-best alignments and parses in the training phase. However, with limited scope and too many redundancies, it is inefficient to search separately on each of these similar lists (Huang, 2008). Another efficient method is to use compact data structures instead of k-best lists. A lattice or forest, compactly encoded exponentially many derivations, have proven to be a promising technique. For example, Mi and Huang (2008), Mi et al. (2008), Liu et al. (2009) and Zhang et al. (2009) use forests in rule extraction and decoding phases to extract more general rules and weaken the influence of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and 837 Coling 2010: Poster Volume, pages 837–845, Beijing, August 2010 (0, 2, NR) 0 c0 :B`u 1 c1 :sh´ı (2, 3, CC) 2 c2 :yˇu (3, 5, NR) 3 c3 :Sh¯a (5, 6, VV) 4 c4 :l´ong 5 (6, 8, NN) c5 :jˇu 6 c6 :x´ıng 7 (5, 7, VV) (2, 3, P) (8, 9, NN) c7 :tˇao 8 c8 :l`un (7, 9,"
C10-2096,P00-1056,0,0.0903879,"take as input the POS tagging results. Then we parse the segmentation results into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. Forest is pruned by using a marginal probabilitybased pruning algorithm similar to Huang (2008). The pruning threshold are pf = 5 and pf = 10 at rule extraction and decoding steps respectively. We word-align the strings of 1-best segmentations and target strings with GIZA++ (Och and Ney, 2000) and apply the refinement method “grow-diag-final-and” (Koehn et al., 2003) to get the final alignments. Following Mi and Huang 843 (2008) and Mi et al. (2008), we also extract rules from forest-string pairs and translate forest to string. 6.1.2 Lattice-forest System We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al., 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm. Then, as current GIZA++ (Och and Ney, 2000) can only handle alignment between string-string pairs, and word-alingme"
C10-2096,P03-1021,0,0.0918259,"guage model and eλ3 |τ (d) |is the length penalty term on target translation. The P (d|T ) decomposes into the product of rule probabilities P (r), each of which is decomposed further into P (d|T ) =  P (r). (6) r∈d Each P (r) in Equation 6 is decomposed further into the production of five probabilities: P (r) = P (r|lhs(r))λ4 · P (r|rhs(r))λ5 · P (r|root(lhs(r))λ6 (7) · Plex (lhs(r)|rhs(r))λ7 · Plex (rhs(r)|lhs(r))λ8 , where the last two are the lexical probabilities between the terminals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003). Following Mi et al. (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al. (2008), and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives. For more detail,"
C10-2096,P02-1040,0,0.0803448,"we refer to the algorithms of Mi et al. (2008). 6 Experiments 6.1 Data Preparation Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. We use the 2002 NIST MT Evaluation test set as development set and the 2005 NIST MT Evaluation test set as test set. We evaluate the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). We use the standard MERT (Och, 2003) to tune the weights. 6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al., 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results. Then we parse the segmentation results into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. For"
C10-2096,D08-1065,0,0.0217086,"o-tree model, and improve 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattice parsing, rule extrac"
C10-2096,2008.amta-papers.18,0,0.0310293,"Missing"
C10-2096,N03-1017,0,0.00419903,"ults into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. Forest is pruned by using a marginal probabilitybased pruning algorithm similar to Huang (2008). The pruning threshold are pf = 5 and pf = 10 at rule extraction and decoding steps respectively. We word-align the strings of 1-best segmentations and target strings with GIZA++ (Och and Ney, 2000) and apply the refinement method “grow-diag-final-and” (Koehn et al., 2003) to get the final alignments. Following Mi and Huang 843 (2008) and Mi et al. (2008), we also extract rules from forest-string pairs and translate forest to string. 6.1.2 Lattice-forest System We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al., 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm. Then, as current GIZA++ (Och and Ney, 2000) can only handle alignment between string-string pairs, and word-alingment with the pairs of Chinese characters and target-string will obviously re"
C10-2096,I05-1007,1,0.88287,"se the 2002 NIST MT Evaluation test set as development set and the 2005 NIST MT Evaluation test set as test set. We evaluate the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). We use the standard MERT (Och, 2003) to tune the weights. 6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al., 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results. Then we parse the segmentation results into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. Forest is pruned by using a marginal probabilitybased pruning algorithm similar to Huang (2008). The pruning threshold are pf = 5 and pf = 10 at rule extraction and decoding steps respectively. We word-align the strings of 1-best segmentations and target strings with GIZA++ (Och and Ney, 2000) and apply the refinement method “grow-diag-final-and” (Koehn et al., 2003) to get the final alignments. Following"
C10-2096,P09-1019,0,0.0266168,"3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattice parsing, rule extraction and decoding. Our model postpones th"
C10-2096,P09-1020,0,0.0592981,"oblem, an obvious solution is to widen the pipeline with k-best lists rather than 1-best results. For example Venugopal et al. (2008) use k-best alignments and parses in the training phase. However, with limited scope and too many redundancies, it is inefficient to search separately on each of these similar lists (Huang, 2008). Another efficient method is to use compact data structures instead of k-best lists. A lattice or forest, compactly encoded exponentially many derivations, have proven to be a promising technique. For example, Mi and Huang (2008), Mi et al. (2008), Liu et al. (2009) and Zhang et al. (2009) use forests in rule extraction and decoding phases to extract more general rules and weaken the influence of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and 837 Coling 2010: Poster Volume, pages 837–845, Beijing, August 2010 (0, 2, NR) 0 c0 :B`u 1 c1 :sh´ı (2, 3, CC) 2 c2 :yˇu (3, 5, NR) 3 c3 :Sh¯a (5, 6, VV) 4 c4 :l´ong 5 (6, 8, NN) c5 :jˇu 6 c6 :x´ıng 7 (5, 7, VV) (2, 3, P) (8, 9, NN) c7 :tˇao 8 c8 :l`un (7, 9, NN) 9 Figure 2: The lattice of the example"
C10-2096,D09-1005,0,\N,Missing
C10-2096,P08-1000,0,\N,Missing
C14-1107,P81-1022,0,0.553007,"meeting S 4 : Bush held a with held Bush meeting Sharon S 8 : Bush held a meeting with meeting sh with Sharon S 9 : Bush held a meeting with Sharon rey held with S 10 : Bush held a meeting with Sharon Bush meeting Sharon rey S 11 : Bush held a meeting with Sharon held Bush meeting with Figure 1: Linear-time left-to-right dependency parsing. A shift-reduce parser performs a left-to-right scan of the input sentence, and at each parsing step, chooses one of two parsing actions: either shift (sh) the current word onto the stack, or reduce (re) the top two (or more) items at the end of the stack (Aho and Ullman, 1972). In the dependency parsing scenario, the reduce action is further divided into two cases: left-reduce (rex ) and right-reduce (rey ), depending on which one of the two items becomes the head after reduction. Each parsing derivation can be represented by a sequence of parsing actions. 1134 2.1 Shift-reduce Dependency Parsing We will use the following sentence as the running example: Bush held a meeting with Sharon Given an input sentence e, where ei is the ith token, ei ...e j is the substring of e from i to j, a shift-reduce parser searches for a dependency tree with a sequence of shift-reduc"
C14-1107,2003.mtsummit-papers.6,0,0.067579,"rget syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al., 2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010). Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Charniak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is independent of any translation model. Thus, integrating a Slm into a tree-to-string model will not face the problems that tree-to-tree models have. However, integration is not easy, as the following two questions arise. First, the search space grows significantly, as a partial translation has a lot of syntax structures. Second, hypotheses in the same bin may not be comparable, since their syntactic structures may no"
C14-1107,P04-1015,0,0.0412563,"window, consisting of current partial tree and first word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic 1135 IP NP VP VP PP B`ush´ı NP P VV yˇu Sh¯al´ong jˇux´ıng AS NP le hu`ıt´an Figure 2: A parse tree features in Table 1(a). To decide which action is the best of the current structure, we perform a three-way classification based on f, and conjoin these feature instances with each action: [f ◦ (action=sh /rex /rey )] We extract all the feature templates from training data, and use the average perceptron algorithm and early-update strategy (Collins and Roark, 2004; Huang et al., 2012) to train the model. 3 Incremental Tree-to-string Translation with Slm The incremental tree-to-string decoding (Huang and Mi, 2010) performs translation in two separate steps: parsing and decoding. A parser first parses the source language input into a 1-best tree in Figure 2, and the linear incremental decoder then searches for the best derivation that generates a target-language string in strictly left-to-right manner. Figure 3 works out the full running example, and we describe it in the following section. 3.1 Decoding with Slm Since the incremental tree-to-string model"
C14-1107,P09-1087,0,0.0145008,"ontributes more, the larger beam size or the Slm. In contrast, our approach gains significant improvements over a state-of-theart tree-to-string baseline at a reasonable speed, about 6 times slower. And we answer some questions beyond their work. Hassan et al. (2009) incorporate a linear-time CCG parser into a DTM system, and achieve a significant improvement. Different from their work, we pay more attention to the dependency parser, and we also test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they are suffering from the local parsing errors. Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005) to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental parser remains in quadratic time. Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008) and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in translation rules. Both papers integrate dependency structures into translation mod"
C14-1107,N04-1035,0,0.0601723,"parser, w s is the weight of Slm(e), fi are the features in the baseline model and wi are the weights. 1139 4 Experiments 4.1 Data Preparation The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08 (newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement option “grow-diag-and” (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate training (Och, 2003) to maxim"
C14-1107,P06-1121,0,0.0289536,"to-right shift-reduce parser in specific, into an incremental tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system. 1 Introduction Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). Ho"
C14-1107,D09-1123,0,0.0497972,"Missing"
C14-1107,D10-1027,1,0.850995,"tring model, and introduce an efficient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system. 1 Introduction Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches still significantly under-perform than"
C14-1107,P10-1110,1,0.918487,"ad. (a) (b) unigram bigram trigram feature templates s0 .w s0 .t s1 .w s1 .t q0 .w q0 .t s0 .w ◦ s1 .w s0 .t ◦ q0 .t s0 .w ◦ s1 .w ◦ s1 .t s0 .w ◦ s0 .t ◦ s1 .w s0 .t ◦ s1 .t ◦ q0 .t s1 .t ◦ s0 .t ◦ q0 .t (c) ... atomic features s0 .w s0 .t s1 .w s1 .t s0 .lc.t s0 .rc.t q0 .w q0 .t s1 s0 .w ◦ s0 .t s1 .w ◦ s1 .t q0 .w ◦ q0 .t s0 .t ◦ s1 .t s0 .w ◦ s0 .t ◦ s1 .t s0 .t ◦ s1 .w ◦ s1 .t s1 .t ◦ s0 .t ◦ s0 .lc.t s1 .t ◦ s0 .t ◦ s0 .rc.t ←− parsing stack s0 s0 .lc · · · parsing queue −→ q0 s0 .rc Table 1: (a) atomic features, used for parsing signatures. (b): parsing feature templates, adapted from Huang and Sagae (2010). x.w and x.t denotes the root word and POS tag of the partial dependency tree, x.lc and x.rc denote x’s leftmost and rightmost child respectively. (c) the feature window. 2.2 Features We view features as “abstractions” or (partial) observations of the current structure. Feature templates f are functions that draw information from the feature window, consisting of current partial tree and first word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic 1135 IP NP VP VP PP B`ush´ı NP P VV yˇu Sh¯al´ong jˇux´ıng AS NP le hu`ıt´an Figure 2: A parse tree"
C14-1107,2006.amta-papers.8,1,0.780582,"he target syntax to guarantee the grammaticality of the output. Instead of using complex tree-to-tree models, we integrate a structured language model, a left-to-right shift-reduce parser in specific, into an incremental tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system. 1 Introduction Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approac"
C14-1107,N12-1015,1,0.848658,"rrent partial tree and first word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic 1135 IP NP VP VP PP B`ush´ı NP P VV yˇu Sh¯al´ong jˇux´ıng AS NP le hu`ıt´an Figure 2: A parse tree features in Table 1(a). To decide which action is the best of the current structure, we perform a three-way classification based on f, and conjoin these feature instances with each action: [f ◦ (action=sh /rex /rey )] We extract all the feature templates from training data, and use the average perceptron algorithm and early-update strategy (Collins and Roark, 2004; Huang et al., 2012) to train the model. 3 Incremental Tree-to-string Translation with Slm The incremental tree-to-string decoding (Huang and Mi, 2010) performs translation in two separate steps: parsing and decoding. A parser first parses the source language input into a 1-best tree in Figure 2, and the linear incremental decoder then searches for the best derivation that generates a target-language string in strictly left-to-right manner. Figure 3 works out the full running example, and we describe it in the following section. 3.1 Decoding with Slm Since the incremental tree-to-string model generates translatio"
C14-1107,N03-1017,0,0.05875,"he following equation: X fi · wi ) (1) e∗ = argmax exp(Slm(e) · w s + e∈E i where Slm(e) is the dependency parsing score calculated by our parser, w s is the weight of Slm(e), fi are the features in the baseline model and wi are the weights. 1139 4 Experiments 4.1 Data Preparation The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08 (newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement option “grow-diag-and” (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010"
C14-1107,W04-3250,0,0.0210447,"m baseline +Slm MT03 Bleu (T-B)/2 19.94 10.73 21.49 9.44 MT04 Bleu (T-B)/2 22.03 18.63 22.33 18.38 MT05 Bleu (T-B)/2 19.92 11.45 20.51 10.71 MT08 Bleu (T-B)/2 21.06 10.37 21.64 9.88 Avg. (T-B)/2 12.80 12.10 Table 3: Results on all test sets. Bold scores are significantly better than the baseline system (p &lt; 0.5). 4.3 Final Results on All Test Sets Table 3 shows our main results on all test sets. Our method gains an average improvement of 0.7 points in terms of (T-B)/2. Results on NIST MT 03, 05, and 08 are statistically significant with p &lt; 0.05, using bootstrap re-sampling with 1000 samples (Koehn, 2004). The average decoding speed is about 10 times slower than the baseline. 5 Related Work The work of Schwartz et al. (2011) is similar in spirit to ours. We are different in the following ways. First, they integrate an Slm into a phrase-based system (Koehn et al., 2003), we pay more attention to a syntax-based system. Second, their approach slowdowns the speed at near 2000 times, thus, they can only tune their system on short sentences less than 20 words. Furthermore, their results are from a much bigger beam (10 times larger than their baseline), so it is not clear which factor contributes mor"
C14-1107,P06-1077,1,0.793255,"but they lack in the target syntax to guarantee the grammaticality of the output. Instead of using complex tree-to-tree models, we integrate a structured language model, a left-to-right shift-reduce parser in specific, into an incremental tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system. 1 Introduction Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target sid"
C14-1107,P09-1063,1,0.891121,"Missing"
C14-1107,H05-1066,0,0.0363915,"h gains significant improvements over a state-of-theart tree-to-string baseline at a reasonable speed, about 6 times slower. And we answer some questions beyond their work. Hassan et al. (2009) incorporate a linear-time CCG parser into a DTM system, and achieve a significant improvement. Different from their work, we pay more attention to the dependency parser, and we also test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they are suffering from the local parsing errors. Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005) to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental parser remains in quadratic time. Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008) and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in translation rules. Both papers integrate dependency structures into translation model, we instead model the dependency structures with a monolingual parsi"
C14-1107,P10-1145,1,0.907751,"(Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al., 2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010). Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Charniak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is independent of any translation model. Thus, integrating a Slm into a tree-to-string model wi"
C14-1107,P08-1023,1,0.824443,"rd-aligned the training data using GIZA++ with refinement option “grow-diag-and” (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. Our dependency parser is an implementation of the “arc-standard” shift-reduce parser (Nivre, 2004), and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and Sagae (2010), our depe"
C14-1107,W04-0308,0,0.0529218,"us using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. Our dependency parser is an implementation of the “arc-standard” shift-reduce parser (Nivre, 2004), and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and Sagae (2010), our dependency parser achieves a similar performance as Huang and Sagae (2010). We add the structured language model as an additional feature into the baseline system. We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the script mteval-v13a.pl. We also report the Ter scores. 4.2 Complete Comparisons on MT08 To explore the soundness of our ap"
C14-1107,P03-1021,0,0.0375581,"alley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. Our dependency parser is an implementation of the “arc-standard” shift-reduce parser (Nivre, 2004), and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and Sagae (2010), our dependency parser achieves a similar performance as Huang and Sagae (2010). We add the structured language model as an additional feature into the baseline system. We evaluate translation quality using case-insensitive I"
C14-1107,N07-1051,0,0.0353772,"e) is the dependency parsing score calculated by our parser, w s is the weight of Slm(e), fi are the features in the baseline model and wi are the weights. 1139 4 Experiments 4.1 Data Preparation The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08 (newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement option “grow-diag-and” (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights usin"
C14-1107,P05-1034,0,0.0501425,"than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al., 2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010). Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Charniak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is independent of any translation model. Thus, integrating"
C14-1107,P11-1063,0,0.0169543,"B)/2 19.92 11.45 20.51 10.71 MT08 Bleu (T-B)/2 21.06 10.37 21.64 9.88 Avg. (T-B)/2 12.80 12.10 Table 3: Results on all test sets. Bold scores are significantly better than the baseline system (p &lt; 0.5). 4.3 Final Results on All Test Sets Table 3 shows our main results on all test sets. Our method gains an average improvement of 0.7 points in terms of (T-B)/2. Results on NIST MT 03, 05, and 08 are statistically significant with p &lt; 0.05, using bootstrap re-sampling with 1000 samples (Koehn, 2004). The average decoding speed is about 10 times slower than the baseline. 5 Related Work The work of Schwartz et al. (2011) is similar in spirit to ours. We are different in the following ways. First, they integrate an Slm into a phrase-based system (Koehn et al., 2003), we pay more attention to a syntax-based system. Second, their approach slowdowns the speed at near 2000 times, thus, they can only tune their system on short sentences less than 20 words. Furthermore, their results are from a much bigger beam (10 times larger than their baseline), so it is not clear which factor contributes more, the larger beam size or the Slm. In contrast, our approach gains significant improvements over a state-of-theart tree-t"
C14-1107,P08-1066,0,0.101418,"tem, and achieve a significant improvement. Different from their work, we pay more attention to the dependency parser, and we also test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they are suffering from the local parsing errors. Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005) to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental parser remains in quadratic time. Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008) and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in translation rules. Both papers integrate dependency structures into translation model, we instead model the dependency structures with a monolingual parsing model over translation strings. 6 Conclusion In this paper, we presented an efficient algorithm to integrate a structured language model (an incremental shift-reduce parser in specific) into an incremental tree-to-string system. We calcul"
C16-1212,D15-1075,0,0.433454,"s long been believed to be an ideal testbed for discrete approaches using alignments and rigid logic inferences (Zanzotto et al., 2009; MacCartney and Manning, 2009; Wang and Manning, 2010; Watanabe et al., 2012; Tian et al., 2014; Filice et al., 2015). All of these methods are based on sparse features, making them brittle for unseen phrases and sentences. Recent advances in deep learning reveal another promising direction to solve this problem. Instead of discrete features and logics, continuous representation of the sentence is more robust to unseen features without sacrificing performance (Bowman et al., 2015). In particular, the attention model based on LSTM can successfully identify the word-by-word correspondences between the two sentences that lead to entailment or contradiction, which makes the entailment relation inference more focused on local information and less vulnerable to misleading information from other parts of the sentence (Rockt¨aschel et al., 2015; Wang and Jiang, 2015). However, conventional neural attention models for entailment recognition problem treat sentences as sequences, ignoring the fact that sentences are formed from the bottom up with syntactic tree structures, which"
C16-1212,D16-1053,0,0.0992726,"LSTM model (Hochreiter and Schmidhuber, 1997), in the binary Tree-LSTM model of Tai et al. (2015), each tree node has a state represented by a pair of vectors: the output vector h ∈ R1×k , and the memory cell c ∈ R1×k , where k is the length of the Tree-LSTM output representation. We use h as the meaning 2 ˜ ∗ at each row to make each row a probability distribution. We need to normalize A 2252 Method LSTM sent. embedding (Bowman et al., 2015) Sparse Features + Classifier (Bowman et al., 2015) LSTM + word-by-word attention (Rockt¨aschel et al., 2015) mLSTM (Wang and Jiang, 2015) LSTM-network (Cheng et al., 2016) LSTM sent. embedding (our implement. of Bowman et al. (2015)) Binary Tree-LSTM (our implementation of Tai et al. (2015)) Binary Tree-LSTM + simple RNN w/ attention Binary Tree-LSTM + Structured Attention & Composition + dual-attention k 100 100 300 450 100 100 150 150 150 |θ|M 221k 252k 1.9m 3.4m 241k 211k 220k 0.9m 0.9m Train 84.8 99.7 85.3 92.0 88.5 79.0 82.4 82.4 87.0 87.7 Test 77.6 78.2 83.5 86.1 86.3 78.4 79.9 81.8 86.4 87.2 Table 1: Comparison between our structured model with other existing methods. Column k specifies the length of the meaning representations. |θ|M is the number of par"
C16-1212,P15-1097,0,0.0422401,"n of the Natural Logic framework (MacCartney and Manning, 2009). Experiments show that our structured attention and entailment composition model can correctly identify and infer entailment relations from the bottom up, and bring significant improvements in accuracy. 1 Introduction Automatically recognizing sentence entailment relations between a pair of sentences has long been believed to be an ideal testbed for discrete approaches using alignments and rigid logic inferences (Zanzotto et al., 2009; MacCartney and Manning, 2009; Wang and Manning, 2010; Watanabe et al., 2012; Tian et al., 2014; Filice et al., 2015). All of these methods are based on sparse features, making them brittle for unseen phrases and sentences. Recent advances in deep learning reveal another promising direction to solve this problem. Instead of discrete features and logics, continuous representation of the sentence is more robust to unseen features without sacrificing performance (Bowman et al., 2015). In particular, the attention model based on LSTM can successfully identify the word-by-word correspondences between the two sentences that lead to entailment or contradiction, which makes the entailment relation inference more foc"
C16-1212,W09-3714,0,0.538343,"ate-of-the-art accuracy by computing soft word alignments between the premise and hypothesis sentences. However, there remains a major limitation: this line of work completely ignores syntax and recursion, which is helpful in many traditional efforts. We show that it is beneficial to extend the attention model to tree nodes between premise and hypothesis. More importantly, this subtree-level attention reveals information about entailment relation. We study the recursive composition of this subtree-level entailment relation, which can be viewed as a soft version of the Natural Logic framework (MacCartney and Manning, 2009). Experiments show that our structured attention and entailment composition model can correctly identify and infer entailment relations from the bottom up, and bring significant improvements in accuracy. 1 Introduction Automatically recognizing sentence entailment relations between a pair of sentences has long been believed to be an ideal testbed for discrete approaches using alignments and rigid logic inferences (Zanzotto et al., 2009; MacCartney and Manning, 2009; Wang and Manning, 2010; Watanabe et al., 2012; Tian et al., 2014; Filice et al., 2015). All of these methods are based on sparse"
C16-1212,D14-1162,0,0.0898204,"valuate the performances of our structured attention model and structured entailment model on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). The SNLI dataset contains ∼ 570k sentence pairs. We use the binarized trees in SNLI dataset in our experiments. 4.1 Experiment Settings Network Architecture The general structure of our model is illustrated in Figure 2b. We omitted a dropout layer between the word embedding layers and the tree LSTM layers in Figure 2b. We use cross-entropy as the training objective.3 Parameter Initialization & Hyper-parameters We use GloVe (Pennington et al., 2014) to initialize the word embedding layer. In the training we do not change the embeddings, except for the OOV words in the training set. For the parameters of the rest layers, we use a uniform distribution between −0.05 and 0.05 as initialization. Our model is trained in an end-to-end manner with adam (Kingma and Ba, 2014) as the optimizer. We set the learning rate to 0.001, β1 to 0.9, and β2 to 0.999. We use minibatch of size 32 in the training. The dropout rate is 0.2. The length for the Tree-LSTM meaning representation k = 150. The length of the entailment relation vector r = 150. 3 Our code"
C16-1212,D13-1170,0,0.0105348,"otherwise hi should be calculated based on the word x ∈ Rd in the leaf. hi = fMR (xi , hi,1 , hi,2 ). (5) Similar is Equation 4, where the relation ei is recursively calculated from the relation of its two children, as well as the meaning hi comparing with the meaning of the premise tree: ∑ ˜ i,j hj ], ei,1 , ei,2 ). ei = frel ([hi ; A (6) j∈P Note the resemblance between these two equations, which indicates that we can handle them similarly with the same form of composition function f (·). We have various choices for composition function f . For example, we can use simple RNN functions as in Socher et al. (2013). Alternatively, we can use a convolutional layer to extract features from xi , hi,1 , hi,2 and use pooling as aggregation to form hi . In this paper we choose Tree-LSTM model (Tai et al., 2015). Our model is independent to this composition function and any high-quality composition function is sufficient for us to infer the meaning representations and entailments. Here we use Equation 5 as an example. Equation 6 can be handled similarly. Similar to the classical LSTM model (Hochreiter and Schmidhuber, 1997), in the binary Tree-LSTM model of Tai et al. (2015), each tree node has a state represe"
C16-1212,P15-1150,0,0.365429,"its two children, as well as the meaning hi comparing with the meaning of the premise tree: ∑ ˜ i,j hj ], ei,1 , ei,2 ). ei = frel ([hi ; A (6) j∈P Note the resemblance between these two equations, which indicates that we can handle them similarly with the same form of composition function f (·). We have various choices for composition function f . For example, we can use simple RNN functions as in Socher et al. (2013). Alternatively, we can use a convolutional layer to extract features from xi , hi,1 , hi,2 and use pooling as aggregation to form hi . In this paper we choose Tree-LSTM model (Tai et al., 2015). Our model is independent to this composition function and any high-quality composition function is sufficient for us to infer the meaning representations and entailments. Here we use Equation 5 as an example. Equation 6 can be handled similarly. Similar to the classical LSTM model (Hochreiter and Schmidhuber, 1997), in the binary Tree-LSTM model of Tai et al. (2015), each tree node has a state represented by a pair of vectors: the output vector h ∈ R1×k , and the memory cell c ∈ R1×k , where k is the length of the Tree-LSTM output representation. We use h as the meaning 2 ˜ ∗ at each row to"
C16-1212,P14-1008,0,0.0211347,"ed as a soft version of the Natural Logic framework (MacCartney and Manning, 2009). Experiments show that our structured attention and entailment composition model can correctly identify and infer entailment relations from the bottom up, and bring significant improvements in accuracy. 1 Introduction Automatically recognizing sentence entailment relations between a pair of sentences has long been believed to be an ideal testbed for discrete approaches using alignments and rigid logic inferences (Zanzotto et al., 2009; MacCartney and Manning, 2009; Wang and Manning, 2010; Watanabe et al., 2012; Tian et al., 2014; Filice et al., 2015). All of these methods are based on sparse features, making them brittle for unseen phrases and sentences. Recent advances in deep learning reveal another promising direction to solve this problem. Instead of discrete features and logics, continuous representation of the sentence is more robust to unseen features without sacrificing performance (Bowman et al., 2015). In particular, the attention model based on LSTM can successfully identify the word-by-word correspondences between the two sentences that lead to entailment or contradiction, which makes the entailment relat"
C16-1212,C10-1131,0,0.0276328,"ee-level entailment relation, which can be viewed as a soft version of the Natural Logic framework (MacCartney and Manning, 2009). Experiments show that our structured attention and entailment composition model can correctly identify and infer entailment relations from the bottom up, and bring significant improvements in accuracy. 1 Introduction Automatically recognizing sentence entailment relations between a pair of sentences has long been believed to be an ideal testbed for discrete approaches using alignments and rigid logic inferences (Zanzotto et al., 2009; MacCartney and Manning, 2009; Wang and Manning, 2010; Watanabe et al., 2012; Tian et al., 2014; Filice et al., 2015). All of these methods are based on sparse features, making them brittle for unseen phrases and sentences. Recent advances in deep learning reveal another promising direction to solve this problem. Instead of discrete features and logics, continuous representation of the sentence is more robust to unseen features without sacrificing performance (Bowman et al., 2015). In particular, the attention model based on LSTM can successfully identify the word-by-word correspondences between the two sentences that lead to entailment or contr"
C16-1212,C12-1171,0,0.33007,"tion, which can be viewed as a soft version of the Natural Logic framework (MacCartney and Manning, 2009). Experiments show that our structured attention and entailment composition model can correctly identify and infer entailment relations from the bottom up, and bring significant improvements in accuracy. 1 Introduction Automatically recognizing sentence entailment relations between a pair of sentences has long been believed to be an ideal testbed for discrete approaches using alignments and rigid logic inferences (Zanzotto et al., 2009; MacCartney and Manning, 2009; Wang and Manning, 2010; Watanabe et al., 2012; Tian et al., 2014; Filice et al., 2015). All of these methods are based on sparse features, making them brittle for unseen phrases and sentences. Recent advances in deep learning reveal another promising direction to solve this problem. Instead of discrete features and logics, continuous representation of the sentence is more robust to unseen features without sacrificing performance (Bowman et al., 2015). In particular, the attention model based on LSTM can successfully identify the word-by-word correspondences between the two sentences that lead to entailment or contradiction, which makes t"
D08-1022,P89-1018,0,0.35909,"e right. These extracted rules are called minimal rules, which can be glued together to form composed rules with larger tree fragments (e.g. r1 in Fig. 1) (Galley et al., 2006). Our experiments use composed rules. 3 Forest-based Rule Extraction We now extend tree-based extraction algorithm from the previous section to work with a packed forest representing exponentially many parse trees. 3.1 Packed Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Earley, 1970; Billot and Lang, 1989). For example, consider again the Chinese sentence in Example (1) above, which has (at least) two readings depending on the part-ofspeech of the word yˇu: it can be either a conjunction (CC “and”) as shown in Figure 3, or a preposition (P “with”) as shown in Figure 5, with only PP and VPB swapped from the English word order. 3 Admissible set (Wang et al., 2007) is also known as “frontier set” (Galley et al., 2004). For simplicity of presentation, we assume every target word is aligned to at least one source word; see Galley et al. (2006) for handling unaligned target words. These two parse tre"
D08-1022,P05-1033,0,0.674171,"-tree models (see Table 1). Leveraging from structural and linguistic information from parse trees, these models are believed to be better than their phrase-based counterparts in handling non-local reorderings, and have achieved promising translation results.1 However, these systems suffer from a major limitation, that the rule extractor only uses 1-best parse tree(s), which adversely affects the rule set quality due to parsing errors. To make things worse, modern statistical parsers are often trained on domains quite different from those used in MT. By contrast, formally syntax-based models (Chiang, 2005) do not rely on parse trees, yet usually perform better than these linguistically sophisticated counterparts. To alleviate this problem, an obvious idea is to extract rules from k-best parses instead. However, a k-best list, with its limited scope, has too few variations and too many redundancies (Huang, 2008). This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space. In addition, many subtrees are repeated across different parses, so it is 1 For example, in"
D08-1022,P05-1066,0,0.156363,"Missing"
D08-1022,P05-1067,0,0.157243,"Missing"
D08-1022,P81-1022,0,0.805013,"o a rule on the right. These extracted rules are called minimal rules, which can be glued together to form composed rules with larger tree fragments (e.g. r1 in Fig. 1) (Galley et al., 2006). Our experiments use composed rules. 3 Forest-based Rule Extraction We now extend tree-based extraction algorithm from the previous section to work with a packed forest representing exponentially many parse trees. 3.1 Packed Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Earley, 1970; Billot and Lang, 1989). For example, consider again the Chinese sentence in Example (1) above, which has (at least) two readings depending on the part-ofspeech of the word yˇu: it can be either a conjunction (CC “and”) as shown in Figure 3, or a preposition (P “with”) as shown in Figure 5, with only PP and VPB swapped from the English word order. 3 Admissible set (Wang et al., 2007) is also known as “frontier set” (Galley et al., 2004). For simplicity of presentation, we assume every target word is aligned to at least one source word; see Galley et al. (2006) for handling unaligned target wo"
D08-1022,N04-1035,0,0.758849,"ld r4 ⇓ a meeting with NPB Sh¯al´ong r5 ⇓ with Sharon NPB(B`ush´ı) → Bush VPB(VV(jˇux´ıng) AS(le) x1 :NPB) → held x1 NPB(Sh¯al´ong) → Sharon NPB(hu`ıt´an) → a meeting Figure 2: Example derivation of tree-to-string translation, with rules used. Each shaded region denotes a tree fragment that is pattern-matched with the rule being applied. hu`ıt´an (1) B`ush´ı yˇu Sh¯al´ong jˇux´ıng le Bush and/with Sharon1 hold past. meeting2 Tree-based Translation We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006). 2.1 Tree-to-String System Current tree-based systems perform translation in two separate steps: parsing and decoding. The input string is first parsed by a parser into a 1-best tree, which will then be converted to a target language string by applying a set of tree-to-string transformation rules. For example, consider the following example translating from Chinese to English: 207 “Bush held a meeting2 with Sharon1 ” Figure 2 shows how this process works. The Chinese sentence (a) is first parsed into a parse tree (b), which will be converted into an English string in 5 s"
D08-1022,P06-1121,0,0.430054,"tion quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses. When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points. 1 Liang Huang2,1 2 Dept. of Computer & Information Science University of Pennsylvania 3330 Walnut St., Levine Hall Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu examples (partial) Ding and Palmer (2005) Liu et al. (2006); Huang et al. (2006) Galley et al. (2006) Chiang (2005) Table 1: A classification of syntax-based MT. The first three use linguistic syntax, while the last one only formal syntax. Our experiments cover the second type using a packed forest in place of the tree for rule-extraction. Introduction Automatic extraction of translation rules is a fundamental problem in statistical machine translation, especially for many syntax-based models where translation rules directly encode linguistic knowledge. Typically, these models extract rules using parse trees from both or either side(s) of the bitext. The former case, with trees on both sides,"
D08-1022,W05-1506,1,0.423446,"ther a conjunction (CC “and”) as shown in Figure 3, or a preposition (P “with”) as shown in Figure 5, with only PP and VPB swapped from the English word order. 3 Admissible set (Wang et al., 2007) is also known as “frontier set” (Galley et al., 2004). For simplicity of presentation, we assume every target word is aligned to at least one source word; see Galley et al. (2006) for handling unaligned target words. These two parse trees can be represented as a single forest by sharing common subtrees such as NPB0, 1 and VPB3, 6 , as shown in Figure 4. Such a forest has a structure of a hypergraph (Huang and Chiang, 2005), where items like NP0, 3 are called nodes, whose indices denote the source span, and combinations like e1 : IP0, 6 → NPB0, 3 VP3, 6 we call hyperedges. We denote head (e) and tails(e) to be the consequent and antecedant items of hyperedge e, respectively. For example, head (e1 ) = IP0, 6 , tails(e1 ) = {NPB0, 3 , VP3, 6 }. We also denote BS (v) to be the set of incoming hyperedges of node v, being different ways of deriving it. For example, in Figure 4, BS (IP0, 6 ) = {e1 , e2 }. 3.2 Forest-based Rule Extraction Algorithm Like in tree-based extraction, we extract rules from a packed forest F"
D08-1022,P07-1019,1,0.350269,"traction, for example in rule composition (Galley et al., 2006) and tree binarization (Wang et al., 2007). However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest. Nevertheless we suspect that their extraction algorithm is in principle similar to ours, although they do not provide details of forest-based fragmentation (Algorithm 1) which we think is non-trivial. The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007). The first direct application of parse forest in translation is our previous work (Mi et al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below). This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding. 5 0.254 pe=8 0.252 BLEU score Our experiments will use both default 1-best decoding and forest-based decoding. As we will see in the next section, the best result comes when we combine the merits of both, i.e., using forests in both rule extraction and decoding. Th"
D08-1022,2006.amta-papers.8,1,0.839789,") Bush r2 r3 r4 r5 NPB Sh¯al´ong VV r2 ⇓ (d) with VPB held r4 ⇓ a meeting with NPB Sh¯al´ong r5 ⇓ with Sharon NPB(B`ush´ı) → Bush VPB(VV(jˇux´ıng) AS(le) x1 :NPB) → held x1 NPB(Sh¯al´ong) → Sharon NPB(hu`ıt´an) → a meeting Figure 2: Example derivation of tree-to-string translation, with rules used. Each shaded region denotes a tree fragment that is pattern-matched with the rule being applied. hu`ıt´an (1) B`ush´ı yˇu Sh¯al´ong jˇux´ıng le Bush and/with Sharon1 hold past. meeting2 Tree-based Translation We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006). 2.1 Tree-to-String System Current tree-based systems perform translation in two separate steps: parsing and decoding. The input string is first parsed by a parser into a 1-best tree, which will then be converted to a target language string by applying a set of tree-to-string transformation rules. For example, consider the following example translating from Chinese to English: 207 “Bush held a meeting2 with Sharon1 ” Figure 2 shows how this process works. The Chinese sentence (a) is first parsed into a parse tree (b"
D08-1022,P08-1067,1,0.796502,"on, that the rule extractor only uses 1-best parse tree(s), which adversely affects the rule set quality due to parsing errors. To make things worse, modern statistical parsers are often trained on domains quite different from those used in MT. By contrast, formally syntax-based models (Chiang, 2005) do not rely on parse trees, yet usually perform better than these linguistically sophisticated counterparts. To alleviate this problem, an obvious idea is to extract rules from k-best parses instead. However, a k-best list, with its limited scope, has too few variations and too many redundancies (Huang, 2008). This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space. In addition, many subtrees are repeated across different parses, so it is 1 For example, in recent NIST Evaluations, some of these models (Galley et al., 2006; Quirk et al., 2005; Liu et al., 2006) ranked among top 10. See http://www.nist.gov/speech/tests/mt/. 206 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214, c Honolulu, October 2008. 2008 Asso"
D08-1022,P06-1077,0,0.88243,"ticated counterparts. To alleviate this problem, an obvious idea is to extract rules from k-best parses instead. However, a k-best list, with its limited scope, has too few variations and too many redundancies (Huang, 2008). This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space. In addition, many subtrees are repeated across different parses, so it is 1 For example, in recent NIST Evaluations, some of these models (Galley et al., 2006; Quirk et al., 2005; Liu et al., 2006) ranked among top 10. See http://www.nist.gov/speech/tests/mt/. 206 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214, c Honolulu, October 2008. 2008 Association for Computational Linguistics IP NP x3 :VPB → x1 x3 with x2 (a) B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an (b) ⇓ 1-best parser IP x1 :NPB CC x2 :NPB NP yˇu Figure 1: Example translation rule r1 . The Chinese conjunction yˇu “and” is translated into English prep. “with”. VPB NPB CC NPB VV AS NPB B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an r1 ⇓ also inefficient to extract rules separate"
D08-1022,P08-1023,1,0.82596,"tion yˇu “and” is translated into English prep. “with”. VPB NPB CC NPB VV AS NPB B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an r1 ⇓ also inefficient to extract rules separately from each of these very similar trees (or from the cross-product of k 2 similar tree-pairs in tree-to-tree models). We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists. Experiments (Section 5) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system (Liu et al., 2006; Mi et al., 2008), which is also 0.5 points better than (and twice as fast as) extracting on 30-best parses. When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date. Besides tree-to-string systems, our method is also applicable to other paradigms such as the string-totree models (Galley et al., 2006) where the rules are in the reverse order, and easily generalizable to pairs of forests in tree-t"
D08-1022,P03-1021,0,0.0445397,"nd the last two terms are derivation and translation length penalties, respectively. The conditional probability P(d |T ) decomposes into the product of rule probabilities: Y P(r). (8) P(d |T ) = r∈d Each P(r) is in turn a product of five probabilities: P(r) = P(r |lhs(r))λ4 · P(r |rhs(r))λ5 · P(r |root(lhs(r)))λ6 · Plex (lhs(r) |rhs(r)) λ7 (9) · Plex (rhs(r) |lhs(r))λ8 where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.3, and the last two are lexical probabilities. These parameters λ1 . . . λ8 are tuned by minimum error rate training (Och, 2003) on the dev sets. We refer readers to Mi et al. (2008) for details of the decoding algorithm. 212 Figure 6: Comparison of extraction time and BLEU score: forest-based vs.1-best and 30-best. rules from... 1-best trees 30-best trees forest: pe =8 Pharaoh extraction 0.24 5.56 2.36 - decoding 1.74 3.31 3.40 - BLEU 0.2430 0.2488 0.2533 0.2297 Table 2: Results with different rule extraction methods. Extraction and decoding columns are running times in secs per 1000 sentences and per sentence, respectively. We use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Follo"
D08-1022,P05-1034,0,0.175825,"inguistically sophisticated counterparts. To alleviate this problem, an obvious idea is to extract rules from k-best parses instead. However, a k-best list, with its limited scope, has too few variations and too many redundancies (Huang, 2008). This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space. In addition, many subtrees are repeated across different parses, so it is 1 For example, in recent NIST Evaluations, some of these models (Galley et al., 2006; Quirk et al., 2005; Liu et al., 2006) ranked among top 10. See http://www.nist.gov/speech/tests/mt/. 206 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214, c Honolulu, October 2008. 2008 Association for Computational Linguistics IP NP x3 :VPB → x1 x3 with x2 (a) B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an (b) ⇓ 1-best parser IP x1 :NPB CC x2 :NPB NP yˇu Figure 1: Example translation rule r1 . The Chinese conjunction yˇu “and” is translated into English prep. “with”. VPB NPB CC NPB VV AS NPB B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an r1 ⇓ also inefficient to ext"
D08-1022,2008.amta-papers.18,0,0.06992,"al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below). This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding. 5 0.254 pe=8 0.252 BLEU score Our experiments will use both default 1-best decoding and forest-based decoding. As we will see in the next section, the best result comes when we combine the merits of both, i.e., using forests in both rule extraction and decoding. There is also a parallel work on extracting rules from k-best parses and k-best alignments (Venugopal et al., 2008), but both their experiments and our own below confirm that extraction on k-best parses is neither efficient nor effective. pe=5 0.250 0.248 k=30 pe=2 0.246 0.244 0.242 1-best forest extraction k-best extraction 0.240 0 1 2 3 4 5 6 average extracting time (secs/1000 sentences) Experiments 5.1 System Our experiments are on Chinese-to-English translation based on a tree-to-string system similar to (Huang et al., 2006; Liu et al., 2006). Given a 1best tree T , the decoder searches for the best derivation d∗ among the set of all possible derivations D: d∗ = arg max λ0 log P(d |T ) + λ1 log Plm (τ"
D08-1022,D07-1078,0,0.352611,"ially many parse trees. 3.1 Packed Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Earley, 1970; Billot and Lang, 1989). For example, consider again the Chinese sentence in Example (1) above, which has (at least) two readings depending on the part-ofspeech of the word yˇu: it can be either a conjunction (CC “and”) as shown in Figure 3, or a preposition (P “with”) as shown in Figure 5, with only PP and VPB swapped from the English word order. 3 Admissible set (Wang et al., 2007) is also known as “frontier set” (Galley et al., 2004). For simplicity of presentation, we assume every target word is aligned to at least one source word; see Galley et al. (2006) for handling unaligned target words. These two parse trees can be represented as a single forest by sharing common subtrees such as NPB0, 1 and VPB3, 6 , as shown in Figure 4. Such a forest has a structure of a hypergraph (Huang and Chiang, 2005), where items like NP0, 3 are called nodes, whose indices denote the source span, and combinations like e1 : IP0, 6 → NPB0, 3 VP3, 6 we call hyperedges. We denote head (e) a"
D08-1022,I05-1007,0,0.0292535,"λ1 . . . λ8 are tuned by minimum error rate training (Och, 2003) on the dev sets. We refer readers to Mi et al. (2008) for details of the decoding algorithm. 212 Figure 6: Comparison of extraction time and BLEU score: forest-based vs.1-best and 30-best. rules from... 1-best trees 30-best trees forest: pe =8 Pharaoh extraction 0.24 5.56 2.36 - decoding 1.74 3.31 3.40 - BLEU 0.2430 0.2488 0.2533 0.2297 Table 2: Results with different rule extraction methods. Extraction and decoding columns are running times in secs per 1000 sentences and per sentence, respectively. We use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we also modify this parser to output a packed forest for each sentence, which can be pruned by the marginal probability-based insideoutside algorithm (Charniak and Johnson, 2005; Huang, 2008). We will first report results trained on a small-scaled dataset with detailed analysis, and then scale to a larger one, where we also combine the technique of forest-based decoding (Mi et al., 2008). 5.2 Results and Analysis on Small Data To test the effect of forest-based rule extraction, we parse the training set into parse forests and use"
D08-1022,P05-1022,0,\N,Missing
D09-1127,D08-1092,0,0.679778,"not all languages are ambiguous in the same way. For example, prepositional phrase (PP) attachment is (notoriously) ambiguous in English (and related European languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in"
D09-1127,P04-1015,0,0.11193,"da V do 7: for act ∈ {shift, reduceL , reduceR } do 8: if act is applicable to config then 9: next ← apply act to config 10: insert next into buffer BUF 11: V ← top k configurations of BUF 12: Output: the tree of the best config in V c(st−1 , st ) =+; reduce is correct (b) saw Bill ::::::::: with a telescope . c(st−1 , st ) =−; reduce is wrong I saw ::::::::::: Bill with:::a:::::::::: telescope: . wo kandao le na wangyuanjin de Bi’er. cR (st , wi ) =+; shift is correct (d) We use the popular online learning algorithm of structured perceptron with parameter averaging (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the goldstandard action-sequence falls off the beam, with the rest of the sequence neglected. As a special case, for the deterministic mode, updates always co-occur with the first mistake made. The intuition behind this strategy is that future mistakes are often caused by previous ones, so with the parser on the wrong track, future actions become irrelevant for learning. See Section 5.3 for more discussions. I wo kandao le na wangyuanjin de Bi’er. into the agenda for the next step. The complexity of this algorithm is O("
D09-1127,P05-1066,0,0.0290422,"Missing"
D09-1127,W02-1001,0,0.00516533,"6: for each config in agenda V do 7: for act ∈ {shift, reduceL , reduceR } do 8: if act is applicable to config then 9: next ← apply act to config 10: insert next into buffer BUF 11: V ← top k configurations of BUF 12: Output: the tree of the best config in V c(st−1 , st ) =+; reduce is correct (b) saw Bill ::::::::: with a telescope . c(st−1 , st ) =−; reduce is wrong I saw ::::::::::: Bill with:::a:::::::::: telescope: . wo kandao le na wangyuanjin de Bi’er. cR (st , wi ) =+; shift is correct (d) We use the popular online learning algorithm of structured perceptron with parameter averaging (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the goldstandard action-sequence falls off the beam, with the rest of the sequence neglected. As a special case, for the deterministic mode, updates always co-occur with the first mistake made. The intuition behind this strategy is that future mistakes are often caused by previous ones, so with the parser on the wrong track, future actions become irrelevant for learning. See Section 5.3 for more discussions. I wo kandao le na wangyuanjin de Bi’er. into the agenda for the next step. Th"
D09-1127,P03-2041,0,0.0527296,"ch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP forcing existing approaches to employ complicated modeling and crude approximations. Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O(n6 ) as opposed to the monolingual O(n3 ) time. To make things worse, languages are non-isomorphic, i.e., there is no 1to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner, 2003; Galley et al., 2004). In fact, rather than joint parsing per se, Burkett and Klein (2008) resort to separate monolingual parsing and bilingual reranking over k 2 tree pairs, which covers a tiny fraction of the whole space (Huang, 2008). We instead propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser is extended to exploit the reorderings between languages as additional observation, but not bothering to build a tree for the target side simultaneously. To illustrate the idea, suppose we are parsing the sentence straightforward to imple"
D09-1127,2008.amta-srw.2,0,0.188831,"e at hand.” Figure 1: PP-attachment is unambiguous in Chinese, which can help English parsing. Introduction Ambiguity resolution is a central task in Natural Language Processing. Interestingly, not all languages are ambiguous in the same way. For example, prepositional phrase (PP) attachment is (notoriously) ambiguous in English (and related European languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese"
D09-1127,N04-1035,0,0.00750622,"hus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the “N1 or N2 ” case (Mitch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP forcing existing approa"
D09-1127,P05-1012,0,0.0881665,"Missing"
D09-1127,D08-1022,1,0.400432,"two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the “N1 or N2 ” case (Mitch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP forcing existing approaches to employ compli"
D09-1127,C04-1010,0,0.023255,"shapes with crossing brackets (for example, [saw Bill] vs. [Bill with a telescope]). We will see in Section 5.3 that this is indeed the case in practice, thus suggesting us to focus on shift-reduce resolution, which we will return to with the help of bilingual constraints in Section 3. 3. the “arc-standard” scan always succeeds, since at the end we can always reduce with empty queue, whereas the “arc-eager” style sometimes goes into deadends where no action can perform (prevented by preconditions, otherwise the result will not be a wellformed tree). This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. 2.3 Comparison with Arc-Eager The three action system was originally described by Yamada and Matsumoto (2003) (although their methods require multiple passes over the input), and then appeared as “arc-standard” in Nivre (2004), but was argued against in comparison to the four-action “arc-eager” variant. Most subsequent works on shift-reduce or “transition-based” dependency parsing followed “arc-eager” (Nivre and Scholz, 2004; Zhang and Clark, 2008), which now becomes the dominant style. But we argue that “arc-standard” is preferable because: 1. in th"
D09-1127,W04-0308,0,0.162902,"ependency Parsing with Three Actions The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination. This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the “arc-standard” version of Nivre (2004). 2.1 The Three Actions Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceL and reduceR , depending on which one of the two 1223 previous shift previous reduceL reduceR stack S S|wi S|st−1 |st S|st S|st−1 queue wi |Q Q Q Q Q 0 1 2 3 arcs A A A A ∪ {(st , st−1 )} A ∪ {(st−1 , st )} 4 2. reduceL : combine the top two items on the stack, st and st−1 (t ≥ 2), and replace them with st (as the head), and add a left arc (st , st−1 ) to A; 3. reduceR : combine the top two items on the stack, st and st−1 (t ≥ 2), and replace them with st−1 (as the head), and add"
D09-1127,J08-4003,0,0.114273,"Missing"
D09-1127,N07-1051,0,0.0178172,"Missing"
D09-1127,P06-2089,0,0.0462942,"s translated into a Chinese relative clause, but nevertheless all phrasal modifiers attach to the immediate right in Mandarin Chinese. 2 Simpler Shift-Reduce Dependency Parsing with Three Actions The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination. This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the “arc-standard” version of Nivre (2004). 2.1 The Three Actions Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceL and reduceR , depending on which one of the two 1223 previous shift previous reduceL reduceR stack S S|wi S|st−1 |st S|st S|st−1 queue wi |Q Q Q Q Q 0 1 2 3 arcs A A A A ∪ {(st , st−1 )} A ∪ {(st−1 , st )} 4 2. reduceL : combine the top two items on the stack, st and st−1 (t ≥ 2), and replace them with st (as the head), and add a le"
D09-1127,2003.mtsummit-papers.44,0,0.0549386,"attachment is unambiguous in Chinese, which can help English parsing. Introduction Ambiguity resolution is a central task in Natural Language Processing. Interestingly, not all languages are ambiguous in the same way. For example, prepositional phrase (PP) attachment is (notoriously) ambiguous in English (and related European languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese uses word-order to disam"
D09-1127,D09-1086,0,0.0420661,". This is because the extra work, with just 3 bilingual features, is not the bottleneck in practice, since the extraction of the vast amount of other features in Table 2 dominates the computation. 4 Related Work in Grammar Induction Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing). For example, Hwa et al. (2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees. Following this idea, Ganchev et al. (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity. 5 Experiments 5.1 Baseline Parser We implement our baseline monolingual parser (in C++) based on the shift-reduce algorithm in Section 2, with feature templates from Table 2. We evaluate its performance on the standard Penn English Treebank (PTB) dependency par"
D09-1127,W04-3207,0,0.147563,"Processing. Interestingly, not all languages are ambiguous in the same way. For example, prepositional phrase (PP) attachment is (notoriously) ambiguous in English (and related European languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” at"
D09-1127,J97-3002,0,0.204352,"se uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the “N1 or N2 ” case (Mitch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP forcing existing approaches to employ complicated modeling and crude approximations. Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O(n6 ) as opposed to the monolingual O(n3 ) time. To make things worse, languages are non-isomorphic, i.e., there is no 1to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner, 2003; Galley et al., 2004). In fact, rather than joint parsing per se, Burkett and Klein (2008) resort to separate monolingual parsing and bilingual reranking over k 2 tree pairs, which covers a tiny fraction of the whole space (Huang, 2008). We instead propose a much simpler alternative, bilingually-constrained monolingual p"
D09-1127,P09-1042,0,0.030299,"the new bilingual features. This is because the extra work, with just 3 bilingual features, is not the bottleneck in practice, since the extraction of the vast amount of other features in Table 2 dominates the computation. 4 Related Work in Grammar Induction Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing). For example, Hwa et al. (2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees. Following this idea, Ganchev et al. (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity. 5 Experiments 5.1 Baseline Parser We implement our baseline monolingual parser (in C++) based on the shift-reduce algorithm in Section 2, with feature templates from Table 2. We evaluate its performance on the standard Penn English T"
D09-1127,C02-1145,0,0.00973013,"and Clark (2008) our baseline at k=1 our baseline at k=16 accuracy 90.7 91.4 90.2 91.3 secs/sent 0.150 0.195 0.009 0.125 Table 4: Baseline parser performance on standard Penn English Treebank dependency parsing task. The speed numbers are not exactly comparable since they are reported on different machines. CTB Articles Bilingual Paris Training 1-270 2745 Dev 301-325 273 Test 271-300 290 Table 5: Training, dev, and test sets from bilingual Chinese Treebank a` la Burkett and Klein (2008). 5.2 Bilingual Data The bilingual data we use is the translated portion of the Penn Chinese Treebank (CTB) (Xue et al., 2002), corresponding to articles 1-325 of PTB, which have English translations with goldstandard parse trees (Bies et al., 2007). Table 5 shows the split of this data into training, development, and test subsets according to Burkett and Klein (2008). Note that not all sentence pairs could be included, since many of them are not oneto-one aligned at the sentence level. Our wordalignments are generated from the HMM aligner of Liang et al. (2006) trained on approximately 1.7M sentence pairs (provided to us by David Burkett, p.c.). This aligner outputs “soft alignments”, i.e., posterior probabilities f"
D09-1127,P08-1067,1,0.203752,"guages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1 Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the “N1 or N2 ” case (Mitch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP forcing existing approaches to employ compli"
D09-1127,W03-3023,0,0.377869,"immediate right in Mandarin Chinese. 2 Simpler Shift-Reduce Dependency Parsing with Three Actions The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination. This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the “arc-standard” version of Nivre (2004). 2.1 The Three Actions Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceL and reduceR , depending on which one of the two 1223 previous shift previous reduceL reduceR stack S S|wi S|st−1 |st S|st S|st−1 queue wi |Q Q Q Q Q 0 1 2 3 arcs A A A A ∪ {(st , st−1 )} A ∪ {(st−1 , st )} 4 2. reduceL : combine the top two items on the stack, st and st−1 (t ≥ 2), and replace them with st (as the head), and add a left arc (st , st−1 ) to A; 3. reduceR : combine the top two items on the stack, st and st−1 (t ≥ 2), an"
D09-1127,D08-1059,0,0.578158,"guration is hwj , ∅, Ai where wj is recognized as the root of the whole sentence, and A encodes a spanning tree rooted at wj . For a sentence of n words, there are exactly 2n − 1 actions: n shifts and n − 1 reductions, since every word must be pushed onto stack once, and every word except the root will eventually be popped in a reduction. The time complexity, as other shift-reduce instances, is clearly O(n). 2.2 Example of Shift-Reduce Conflict Figure 2 shows the trace of this paradigm on the example sentence. For the first two configurations 3 a “configuration” is sometimes called a “state” (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. saw saw saw saw Bill Bill Bill Bill with with with with a ... a ... a ... a ... shift saw Bill with a ... with a ... with a ... I 5a reduceR saw I 5b 1. shift: move the head of (a non-empty) queue Q onto stack S; I I I I Table 1: Formal description of the three actions. Note that shift requires non-empty queue while reduce requires at least two elements on the stack. items becomes the head after reduction. More formally, we describe a parser configuration by a tuple hS, Q, Ai where S is the s"
D09-1127,P06-1096,0,0.0276256,"Missing"
D09-1127,J03-4003,0,\N,Missing
D10-1027,J07-2003,0,0.385174,"al (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007). In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based). But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is slow in practice. Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-"
D10-1027,P81-1022,0,0.759931,"bles replaced by matched nodes: [NP@1  VP@2 ] [ held NP@2.2.3 with NP@2.1.2 ] Note that this is a reordering rule, and the stack always follows the English word order because we generate hypothesis incrementally left-to-right. Figure 4 works out the full example. We formalize this algorithm in Figure 5. Each item hs, ρi consists of a stack s and a hypothesis ρ. Similar to phrase-based dynamic programming, only the last g−1 words of ρ are part of the signature for decoding with g-gram LM. Each stack is a list of dotted rules, i.e., rules with dot positions indicting progress, in the style of Earley (1970). We call the last (rightmost) rule on the stack the top rule, which is the rule being processed currently. The symbol after the dot in the top rule is called the next symbol, since it is the symbol to expand or process next. Depending on the next symbol a, we can perform one of the three actions: • if a is a node η, we perform a Predict action which expands η using a rule r that can patternmatch the subtree rooted at η; we push r is to the stack, with the dot at the beginning; • if a is an English word, we perform a Scan action which immediately adds it to the current hypothesis, advancing th"
D10-1027,D08-1089,0,0.484862,"er only needs to do this at one end since the translation is always growing left-to-right. As a result, syntax-based models are often embarassingly slower than their phrase-based counterparts, preventing them from becoming widely useful. Can we combine the merits of both approaches? While other authors have explored the possibilities 273 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics of enhancing phrase-based decoding with syntaxaware reordering (Galley and Manning, 2008), we are more interested in the other direction, i.e., can syntax-based models learn from phrase-based decoding, so that they still model global reordering, but in an efficient (preferably linear-time) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibac"
D10-1027,N04-1035,0,0.040328,"of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we 280 also compare our incremental decoder with the standard cube pruning approach on the same tree-tostring decoder. 5.1 Data and System Preparation Our training corpus consists of 1.5M sentence pairs with about 38M/32M words in Chinese/English, respectively. We first word-align them by GIZA++ and then parse the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007), then apply the GHKM algorithm (Galley et al., 2004) to extract tree-to-string translation rules. We use SRILM Toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on the target side of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluate the translation quality using the BLEU-4 metric, wh"
D10-1027,P07-1019,1,0.829324,"ractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007). In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based). But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is slow in practice. Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-to-right. As a result, syntax-based models are often embarassingly slower than their phrase-based counterparts, preventing them from becoming widely useful. Can we combine the merits of both approaches? While other authors have explored the possibilities 273 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics of enhancing phrase-based decoding"
D10-1027,2006.amta-papers.8,1,0.733611,"e) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibach Normal Form, which is not always feasible in practice. We take a fresh look on this problem and turn our focus to one particular syntax-based paradigm, treeto-string translation (Liu et al., 2006; Huang et al., 2006), since this is the simplest and fastest among syntax-based approaches. We develop an incremental dynamic programming algorithm and make the following contributions: • we show that, unlike previous work, our incremental decoding algorithm runs in averagecase polynomial-time in theory for tree-tostring models, and the beam search version runs in linear-time in practice (see Table 1); • large-scale experiments on a tree-to-string system confirm that, with comparable translation quality, our incremental decoder (in Python) can run more than 30 times faster than the phrase-based system Moses (in C"
D10-1027,W07-0405,1,0.844387,"ebased case in Section 2, here we have to remember both sides the leftmost and the rightmost boundary words: each node is now split into +LM items like (η a ⋆ b ) where η is a tree node, and a and b are left and right English boundary words. For example, a bigram +LM item for node VP@2 might be (VP@2 held ⋆ Sharon ). This is also the case with other syntax-based models like Hiero or GHKM: language model integration overhead is the most significant factor that causes syntax-based decoding to be slow (Chiang, 2007). In theory +LM decoding is O(nc|V |4(g−1) ), where V denotes English vocabulary (Huang, 2007). In practice we have to resort to beam search again: at each node we would only allow top-b +LM items. With beam search, tree-to-string decoding with an integrated language model runs in time O(ncb2 ), where b is the size of the beam at each node, and c is (maximum) number of translation rules matched at each node (Huang, 2007). See Table 2 for a summary. 276 3.2 Incremental Decoding Can we borrow the idea of phrase-based decoding, so that we also grow the hypothesis strictly leftto-right, and only need to maintain the rightmost boundary words? The key intuition is to adapt the coverage-vecto"
D10-1027,J99-4005,0,0.195544,"able 1: [main result] Time complexity of our incremental tree-to-string decoding compared with phrase-based. In practice means “approximate search with beams.” Introduction Most efforts in statistical machine translation so far are variants of either phrase-based or syntax-based models. From a theoretical point of view, phrasebased models are neither expressive nor efficient: they typically allow arbitrary permutations and resort to language models to decide the best order. In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponentialtime algorithm (Knight, 1999). In practice, the decoder has to employ beam search to make it tractable (Koehn, 2004). However, even beam search runs in quadratic-time in general (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderin"
D10-1027,P07-2045,0,0.0437933,"sort to language models to decide the best order. In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponentialtime algorithm (Knight, 1999). In practice, the decoder has to employ beam search to make it tractable (Koehn, 2004). However, even beam search runs in quadratic-time in general (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007). In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based). But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is"
D10-1027,koen-2004-pharaoh,0,0.707006,"d with phrase-based. In practice means “approximate search with beams.” Introduction Most efforts in statistical machine translation so far are variants of either phrase-based or syntax-based models. From a theoretical point of view, phrasebased models are neither expressive nor efficient: they typically allow arbitrary permutations and resort to language models to decide the best order. In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponentialtime algorithm (Knight, 1999). In practice, the decoder has to employ beam search to make it tractable (Koehn, 2004). However, even beam search runs in quadratic-time in general (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguisticallymotivated subset, for example those"
D10-1027,P06-1077,0,0.630383,"ferably linear-time) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibach Normal Form, which is not always feasible in practice. We take a fresh look on this problem and turn our focus to one particular syntax-based paradigm, treeto-string translation (Liu et al., 2006; Huang et al., 2006), since this is the simplest and fastest among syntax-based approaches. We develop an incremental dynamic programming algorithm and make the following contributions: • we show that, unlike previous work, our incremental decoding algorithm runs in averagecase polynomial-time in theory for tree-tostring models, and the beam search version runs in linear-time in practice (see Table 1); • large-scale experiments on a tree-to-string system confirm that, with comparable translation quality, our incremental decoder (in Python) can run more than 30 times faster than the phrase-bas"
D10-1027,P08-1023,1,0.751406,"ning corpus consists of 1.5M sentence pairs with about 38M/32M words in Chinese/English, respectively. We first word-align them by GIZA++ and then parse the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007), then apply the GHKM algorithm (Galley et al., 2004) to extract tree-to-string translation rules. We use SRILM Toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on the target side of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluate the translation quality using the BLEU-4 metric, which is calculated by the script mteval-v13a.pl with its default setting which is caseinsensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. We first verify the assumptions we made in Section 3.3 in order to prove the"
D10-1027,P03-1021,0,0.0200701,"ide of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluate the translation quality using the BLEU-4 metric, which is calculated by the script mteval-v13a.pl with its default setting which is caseinsensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. We first verify the assumptions we made in Section 3.3 in order to prove the theorem that tree depth (as a random variable) is normally-distributed with O(log n) mean and variance. Qualitatively, we verified that for most n, tree depth d(n) does look like a normal distribution. Quantitatively, Figure 6 shows that average tree height correlates extremely well with 3.5 log n, while tree height variance is bounded by 5.5 log n. 5.2 Comparison with Cube pruning We implemented our incremental decoding algorithm in"
D10-1027,N07-1051,0,0.0292411,"CKY with cube pruning. 5 Experiments To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we 280 also compare our incremental decoder with the standard cube pruning approach on the same tree-tostring decoder. 5.1 Data and System Preparation Our training corpus consists of 1.5M sentence pairs with about 38M/32M words in Chinese/English, respectively. We first word-align them by GIZA++ and then parse the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007), then apply the GHKM algorithm (Galley et al., 2004) to extract tree-to-string translation rules. We use SRILM Toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on the target side of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluat"
D10-1027,N07-1063,0,0.0122181,"n run-time with beam search. Within syntax-based paradigms, cube pruning (Chiang, 2007; Huang and Chiang, 2007) has become the standard method to speed up +LM decoding, which has been shown by many authors to be highly effective; we will be comparing our incremental decoder with a baseline decoder using cube pruning in Section 5. It is also important to note that cube pruning and incremental decoding are not mutually exclusive, rather, they could potentially be combined to further speed up decoding. We leave this point to future work. Multipass coarse-to-fine decoding is another popular idea (Venugopal et al., 2007; Zhang and Gildea, 2008; Dyer and Resnik, 2010). In particular, Dyer and Resnik (2010) uses a two-pass approach, where their first-pass, −LM decoding is also incremental and polynomial-time (in the style of Earley (1970) algorithm), but their second-pass, +LM decoding is still bottom-up CKY with cube pruning. 5 Experiments To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we 280 also compare our incremental decoder with the standard cube prunin"
D10-1027,P06-1098,0,0.72306,". Can we combine the merits of both approaches? While other authors have explored the possibilities 273 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics of enhancing phrase-based decoding with syntaxaware reordering (Galley and Manning, 2008), we are more interested in the other direction, i.e., can syntax-based models learn from phrase-based decoding, so that they still model global reordering, but in an efficient (preferably linear-time) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibach Normal Form, which is not always feasible in practice. We take a fresh look on this problem and turn our focus to one particular syntax-based paradigm, treeto-string translation (Liu et al., 2006; Huang et al., 2006), since this is"
D10-1027,J97-3002,0,0.201309,"e in general (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007). In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based). But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is slow in practice. Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is alway"
D10-1027,P08-1025,0,0.0170771,"rch. Within syntax-based paradigms, cube pruning (Chiang, 2007; Huang and Chiang, 2007) has become the standard method to speed up +LM decoding, which has been shown by many authors to be highly effective; we will be comparing our incremental decoder with a baseline decoder using cube pruning in Section 5. It is also important to note that cube pruning and incremental decoding are not mutually exclusive, rather, they could potentially be combined to further speed up decoding. We leave this point to future work. Multipass coarse-to-fine decoding is another popular idea (Venugopal et al., 2007; Zhang and Gildea, 2008; Dyer and Resnik, 2010). In particular, Dyer and Resnik (2010) uses a two-pass approach, where their first-pass, −LM decoding is also incremental and polynomial-time (in the style of Earley (1970) algorithm), but their second-pass, +LM decoding is still bottom-up CKY with cube pruning. 5 Experiments To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we 280 also compare our incremental decoder with the standard cube pruning approach on the same t"
D10-1027,N10-1128,0,\N,Missing
D13-1071,J98-2004,0,0.0946364,"sing, which applies the DP idea of Huang and Sagae (2010) to the best-first parser of Sagae and Lavie (2006) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial. We prove the correctness of our algorithm rigorously. Experiments confirm that DP leads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time. 1 Liang Huang1,2 James Cross1 Introduction Best-first parsing, such as A* parsing, makes constituent parsing efficient, especially for bottom-up CKY style parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009). Traditional CKY parsing performs cubic time exact search over an exponentially large space. Best-first parsing significantly speeds up by always preferring to explore states with higher probabilities. In terms of incremental parsing, Sagae and Lavie (2006) is the first work to extend best-first search to shift-reduce constituent parsing. Unlike other very fast greedy parsers that produce suboptimal results, this best-first parser still guarantees optimality but requires exponential time for very long sentences in the worst case, which is intra"
D13-1071,P81-1022,0,0.777643,"uctive system (Figure 2). We have the following property as an alternative for superiority: Lemma 1. After derivation xk has been filled into chart, ∀x s.t. x ∈ Q, and x is a best derivation of state [x]∼ , then x’s descendants can not have a higher priority than xk . Proof. Note that when xk pops out, x is still in Q, so xk  x. Assume z is x’s direct descendant. • If z = sh(x) or z = re(x, ), based on the deductive system, x ≺ z, so xk  x ≺ z. • If z = re(y, x), y ∈ L(x), assume z ≺ xk . z.pre = y.pre + y.sh + x.ins + x.re DP best-first shift-reduce parsing is analogous to weighted Earley (Earley, 1970; Stolcke, 1995), because: 1) in Earley the P RED rule generates states similar to shifted states in shift-reduce parsing; and, 2) a newly completed state also needs to check all possible left expansions and right expansions, similar to a state popped from the priority queue in Algorithm 1. (see Figure 2) Our Algorithm 2 exploits lazy expansion, which reduces unnecessary expansions, and should be more efficient than pure Earley. 5 Note that each state has a unique feature signature. We want to prove that Algorithm 1 actually fills the chart by assigning a best derivation to its state. Without"
D13-1071,C96-1058,0,0.5,"In the above theorem, boundness means we can only extract finite information from a derivation, so that the atomic feature function e f (·) can only distinguish a finite number of different states. Monotonicity requires the feature representation fk subsumes fk+1 . This is necessary because we use the features as signature to match all possible left states and right states (Equation 1). Note that we add the vertical monotonicity condition following the suggestion from Kuhlmann et al. (2011), which fixes a flaw in the original theorem of Huang and Sagae (2010). We use the edge-factored model (Eisner, 1996; McDonald et al., 2005) with dynamic programming described in Figure 4 as a concrete example for complexity analysis. In the edge-factored model the feature set consists of only combinations of information from the roots of the two top trees s1 , s0 , and the queue. So the atomic feature function is e f (p) = (i, j, h(p.s1 ), h(p.s0 )) Experiments In experiments we compare our DP best-first parsing with non-DP best-first parsing, pure greedy parsing, and beam parser of Huang and Sagae (2010). Our underlying MaxEnt model is trained on the Penn Treebank (PTB) following the standard split: Secti"
D13-1071,P10-1110,1,0.953416,"sing. We merge states with the same features set globally to further reduce the number of possible states in the search graph. Thus, our DP best-first algorithm is significantly faster than non-DP best-first parsing, and, more importantly, it has a polynomial time complexity even in the worst case. We make the following contributions: Graduate Center City University of New York 365 Fifth Avenue, New York, NY 10016 {kzhao,jcross}@gc.cuny.edu We present the first provably optimal polynomial time dynamic programming (DP) algorithm for best-first shift-reduce parsing, which applies the DP idea of Huang and Sagae (2010) to the best-first parser of Sagae and Lavie (2006) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial. We prove the correctness of our algorithm rigorously. Experiments confirm that DP leads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time. 1 Liang Huang1,2 James Cross1 Introduction Best-first parsing, such as A* parsing, makes constituent parsing efficient, especially for bottom-up CKY style parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003; Pa"
D13-1071,N03-1016,0,0.171651,"a of Huang and Sagae (2010) to the best-first parser of Sagae and Lavie (2006) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial. We prove the correctness of our algorithm rigorously. Experiments confirm that DP leads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time. 1 Liang Huang1,2 James Cross1 Introduction Best-first parsing, such as A* parsing, makes constituent parsing efficient, especially for bottom-up CKY style parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009). Traditional CKY parsing performs cubic time exact search over an exponentially large space. Best-first parsing significantly speeds up by always preferring to explore states with higher probabilities. In terms of incremental parsing, Sagae and Lavie (2006) is the first work to extend best-first search to shift-reduce constituent parsing. Unlike other very fast greedy parsers that produce suboptimal results, this best-first parser still guarantees optimality but requires exponential time for very long sentences in the worst case, which is intractable in practice. Becau"
D13-1071,P11-1068,0,0.139205,"Missing"
D13-1071,P05-1012,0,0.0684405,"theorem, boundness means we can only extract finite information from a derivation, so that the atomic feature function e f (·) can only distinguish a finite number of different states. Monotonicity requires the feature representation fk subsumes fk+1 . This is necessary because we use the features as signature to match all possible left states and right states (Equation 1). Note that we add the vertical monotonicity condition following the suggestion from Kuhlmann et al. (2011), which fixes a flaw in the original theorem of Huang and Sagae (2010). We use the edge-factored model (Eisner, 1996; McDonald et al., 2005) with dynamic programming described in Figure 4 as a concrete example for complexity analysis. In the edge-factored model the feature set consists of only combinations of information from the roots of the two top trees s1 , s0 , and the queue. So the atomic feature function is e f (p) = (i, j, h(p.s1 ), h(p.s0 )) Experiments In experiments we compare our DP best-first parsing with non-DP best-first parsing, pure greedy parsing, and beam parser of Huang and Sagae (2010). Our underlying MaxEnt model is trained on the Penn Treebank (PTB) following the standard split: Sections 02-21 as the trainin"
D13-1071,J03-1006,0,0.0386319,"associated with state p. We sometimes abuse this notation to say C [x] to retrieve the derivation associated with signature e f (x) for derivation x. This is fine since we know derivation x’s state immediately from the signature. We say state p ∈ C if e f (p) is associated with some derivation in C . A derivation x ∈ C if C [x] = x. Chart C supports operation P USH, denoted as C [x] ← x, which associate a signature e f (x) with derivation x. Priority queue Q is defined similarly as C , except that it supports the operation P OP that pops the highest priority item. Following Stolcke (1995) and Nederhof (2003), we use the prefix score and the inside score as the priority in Q: can pick either one of them. This will not affect the correctness of our optimality proof in Section 5.1. In the DP best-first parsing algorithm, once a derivation x is popped from the priority queue Q, as usual we try to expand it with shift and reduce. Note that both left and right reduces are between the derivation x of state p = [x]∼ and an in-chart derivation y of left state q = [y]∼ ∈ L(p) (Line 10 of Algorithm 1), as shown in the deductive system (Figure 2). We call this kind of reduction left expansion. (x.pre = y.pre"
D13-1071,J08-4003,0,0.573689,"t to right generation of 758 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 758–768, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics input w0 . . . wn−1 scan of the input sentence, and at each step, chooses either to shift the next word onto the stack, or to reaxiom 0 : h0, i: 0 duce, i.e., combine the top two trees on stack, ei` : hj, Si : c ther with left as the root or right as the root. This sh j<n scheme is often called “arc-standard” in the litera` + 1 : hj + 1, S|wj i : c + sc sh (j, S) ture (Nivre, 2008), and is the basis of several state` : hj, S|s1 |s0 i : c of-the-art parsers, e.g. Huang and Sagae (2010). See rex x ` + 1 : hj, S|s1 s0 i : c + sc rex (j, S|s1 |s0 ) Figure 1 for the deductive system of shift-reduce dependency parsing. ` : hj, S|s1 |s0 i : c To improve on strictly greedy search, shift-reduce rey y ` + 1 : hj, S|s1 s0 i : c + sc rey (j, S|s1 |s0 ) parsing is often enhanced with beam search (Zhang and Clark, 2008), where b derivations develop in Figure 1: Deductive system of basic non-DP shift-reduce parallel. At each step we extend the derivations in parsing. Here ` is the ste"
D13-1071,N09-1063,0,0.129438,"0) to the best-first parser of Sagae and Lavie (2006) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial. We prove the correctness of our algorithm rigorously. Experiments confirm that DP leads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time. 1 Liang Huang1,2 James Cross1 Introduction Best-first parsing, such as A* parsing, makes constituent parsing efficient, especially for bottom-up CKY style parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009). Traditional CKY parsing performs cubic time exact search over an exponentially large space. Best-first parsing significantly speeds up by always preferring to explore states with higher probabilities. In terms of incremental parsing, Sagae and Lavie (2006) is the first work to extend best-first search to shift-reduce constituent parsing. Unlike other very fast greedy parsers that produce suboptimal results, this best-first parser still guarantees optimality but requires exponential time for very long sentences in the worst case, which is intractable in practice. Because it needs to explore a"
D13-1071,P06-2089,0,0.13456,"obally to further reduce the number of possible states in the search graph. Thus, our DP best-first algorithm is significantly faster than non-DP best-first parsing, and, more importantly, it has a polynomial time complexity even in the worst case. We make the following contributions: Graduate Center City University of New York 365 Fifth Avenue, New York, NY 10016 {kzhao,jcross}@gc.cuny.edu We present the first provably optimal polynomial time dynamic programming (DP) algorithm for best-first shift-reduce parsing, which applies the DP idea of Huang and Sagae (2010) to the best-first parser of Sagae and Lavie (2006) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial. We prove the correctness of our algorithm rigorously. Experiments confirm that DP leads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time. 1 Liang Huang1,2 James Cross1 Introduction Best-first parsing, such as A* parsing, makes constituent parsing efficient, especially for bottom-up CKY style parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009). Traditional CKY parsing perfo"
D13-1071,J95-2002,0,0.742786,"ations in the same state share the same atomic features. state p: sh h , j, sd ...s0 i: (c, ) hj, j + 1, sd−1 ...s0 , wj i : (c + ξ, 0) state q: rex hk, i, s0d ...s00 i: (c0 , v 0 ) h , j, A → α.Bβi : (c, ) j<n P RED hj, j, B → .γi : (c+s, s) state p: hi, j, sd ...s0 i: ( , v) hk, j, s0d ...s01 , s00 s0 i : (c0+v+δ, v 0+v+δ) x q ∈ L(p) (B → γ) ∈ G hk, i, A → α.Bβi : (c0 , v 0 ) hi, j, Bi : ( , v) C OMP hk, j, A → αB.βi : (c0+v, v 0+v) Figure 2: Deductive systems for dynamic programming shift-reduce parsing (Huang and Sagae, 2010) (left, omitting rey case), compared to weighted Earley parsing (Stolcke, 1995) (right). Here ξ = sc sh (p), δ = sc sh (q) + sc rex (p), s = sc(B → γ), G is the set of CFG rules, hi, j, Bi is a surrogate for any hi, j, B → γ.i, and is a wildcard that matches anything. ... ... sh p L(p) ... sh R(p) (a) L(p) and R(p) L(p) sh p ... T (p) (b) T (p) = R(L(p)) Figure 3: Illustrations of left states L(p), right states R(p), and left corner states T (p). (a) Left states L(p) is the set of states that can be reduced with p so that p.s0 will be the right child of the top tree of the result state. Right states R(p) is the set of states that can be reduced with p so that p.s0 will b"
D13-1071,D08-1059,0,0.0996204,"Missing"
D13-1093,E12-1009,0,0.153822,"onald (2012). 3 Experiments We ran a number of experiments on the cubepruning dependency parser of Zhang and McDonald (2012), whose search space can be represented as a hypergraph in which the nodes are the complete and incomplete states and the hyperedges are the instantiations of the two parsing rules in the Eisner algorithm (Eisner, 1996). The feature templates we used are a superset of Zhang and McDonald (2012). These features include first-, second-, and third-order features and their labeled counterparts, as well as valency features. In addition, we also included a feature template from Bohnet and Kuhn (2012). This template examines the leftmost child and the rightmost child of a modifier simultaneously. All other highorder features of Zhang and McDonald (2012) only look at arcs on the same side of their head. We trained the parser with hamming-loss-augmented MIRA (Crammer et al., 2006), following Martins et al. (2010). Based on results on the English validation data, in all the experiments, we trained MIRA with 8 epochs and used a beam of size 6 per node. To speed up the parser, we used an unlabeled first-order model to prune unlikely dependency arcs at both training and testing time (Koo and Col"
D13-1093,W06-2920,0,0.0316564,"1.49 93.80 89.65 87.79 83.59 91.62 85.00 80.60 70.12 76.86 66.56 92.00 87.07 92.19 88.40 86.46 78.55 85.77 76.62 88.48 82.38 79.61 71.65 86.49 81.67 91.79 89.28 83.35 80.09 87.80 82.14 Best Published† UAS LAS 87.48 84.05 94.07 89.09 93.72 91.793.50 88.23 87.47 83.50 91.44 85.42 81.12 66.977.55 65.791.86 84.893.03 87.70 86.05 77.87 86.95 73.490.32 80.280.23 73.18 86.81 81.86 92.41 88.42 86.19 79.2- Table 2: Parsing Results for languages from CoNLL 2006/2007 shared tasks. When a language is in both years, we use the 2006 data set. The best results with † are the maximum in the following papers: Buchholz and Marsi (2006), Nivre et al. (2007), Zhang and McDonald (2012), Bohnet and Kuhn (2012), and Martins et al. (2013), For consistency, we scored the CoNLL 2007 best systems with the CoNLL 2006 evaluation script. ZN 2011 (reimpl.) is our reimplementation of Zhang and Nivre (2011), with a beam of 64. Results in bold are the best among ZN 2011 reimplementation and different update strategies from this paper. 3.3 CoNLL Results We also report parsing results for 17 languages from the CoNLL 2006/2007 shared-task (Buchholz and Marsi, 2006; Nivre et al., 2007). The parser in our experiments can only produce projective"
D13-1093,D08-1024,0,0.190379,"Missing"
D13-1093,J07-2003,0,0.0261665,"Missing"
D13-1093,P04-1015,0,0.10193,"this, they generalized the original update rule to select an output y ′ within the pruned search space that scores higher than yˆ, but is not necessarily the highest among all possibilities, which represents a true violation of the model on that training instance. This violation fixing perceptron thus relaxes the argmax function to accommodate inexact search and becomes provably convergent as a result. In the sequential cases where yˆ has a linear structure such as tagging and incremental parsing, the violation fixing perceptron boils down to finding and updating along a certain prefix of yˆ. Collins and Roark (2004) locate the earliest position in a ′ chain structure where yˆpref is worse than ypref by a margin large enough to cause yˆ to be dropped from the beam. Huang et al. (2012) locate the position where the violation is largest among all prefixes of yˆ, where size of a violation is defined as ′ w · f (x, ypref ) − w · f (x, yˆpref ). For hypergraphs, the notion of prefix must be generalized to subtrees. Figure 1 shows the packedforest representation of the union of gold subtrees and highest-scoring (Viterbi) subtrees at every gold node for an input. At each gold node, there are two incoming hypered"
D13-1093,W02-1001,0,0.0711707,"ollins and Roark, 2004; Daum´e and Marcu, 2005; Zhang and Clark, 2008; Huang et al., 2012). However, sequential search algorithms, and in particular left-to-right beam search (Collins and Roark, 2004; Zhang and Clark, 2008), squeeze inference into a very narrow space. To address this, Huang (2008) formulated constituency parsing as approximate bottom-up inference in order to compactly represent an exponential number of outputs while scoring features of arbitrary scope. This idea was adapted to graph-based Structured Perceptron for Inexact Hypergraph Search The structured perceptron algorithm (Collins, 2002) is a general learning algorithm. Given training instances (x, yˆ), the algorithm first solves the decoding problem y ′ = argmaxy∈Y(x) w · f (x, y) given the weight vector w for the high-dimensional feature representation f of the mapping (x, y), where y ′ is the prediction under the current model, yˆ is the gold output and Y(x) is the space of all valid outputs for input x. The perceptron update rule is simply: w′ = w + f (x, yˆ) − f (x, y ′ ). The convergence of original perceptron algorithm relies on the argmax function being exact so that the condition w · f (x, y ′ ) &gt; w · f (x, yˆ) (modu"
D13-1093,de-marneffe-etal-2006-generating,0,0.00528498,"Missing"
D13-1093,C96-1058,0,0.144046,"Missing"
D13-1093,N12-1015,1,0.222192,"ecific instances of inexact hypergraph search. Typically, the approximation is accomplished by cube-pruning throughout the hypergraph (Chiang, 2007). Unfortunately, as the scope of features at each node increases, the inexactness of search and its negative impact on learning can potentially be exacerbated. Unlike sequential search, the impact on learning of approximate hypergraph search – as well as methods to mitigate any ill effects – has not been studied. Motivated by this, we develop online learning algorithms for inexact hypergraph search by generalizing the violation-fixing percepron of Huang et al. (2012). We empirically validate the benefit of this approach within the cube-pruning dependency parser of Zhang and McDonald (2012). Online learning algorithms like the perceptron are widely used for structured prediction tasks. For sequential search problems, like left-to-right tagging and parsing, beam search has been successfully combined with perceptron variants that accommodate search errors (Collins and Roark, 2004; Huang et al., 2012). However, perceptron training with inexact search is less studied for bottom-up parsing and, more generally, inference over hypergraphs. In this paper, we gener"
D13-1093,P08-1067,1,0.864538,"prediction problems generally deal with exponentially many outputs, often making exact search infeasible. For sequential search problems, such as tagging and incremental parsing, beam search coupled with perceptron algorithms that account for potential search errors have been shown to be a powerful combination (Collins and Roark, 2004; Daum´e and Marcu, 2005; Zhang and Clark, 2008; Huang et al., 2012). However, sequential search algorithms, and in particular left-to-right beam search (Collins and Roark, 2004; Zhang and Clark, 2008), squeeze inference into a very narrow space. To address this, Huang (2008) formulated constituency parsing as approximate bottom-up inference in order to compactly represent an exponential number of outputs while scoring features of arbitrary scope. This idea was adapted to graph-based Structured Perceptron for Inexact Hypergraph Search The structured perceptron algorithm (Collins, 2002) is a general learning algorithm. Given training instances (x, yˆ), the algorithm first solves the decoding problem y ′ = argmaxy∈Y(x) w · f (x, y) given the weight vector w for the high-dimensional feature representation f of the mapping (x, y), where y ′ is the prediction under the"
D13-1093,P10-1001,0,0.158246,"Missing"
D13-1093,C12-2077,0,0.171635,"Missing"
D13-1093,D10-1004,0,0.150365,"Missing"
D13-1093,P13-2109,0,0.225319,"ported by the fourth-order unlabeled dependency parser of Ma and Zhao (2012), although we did not utilize fourth-order features. The LAS score on Penn-YM is on par with the best reported by Bohnet and Kuhn (2012). On Penn-S, there are not many existing results to compare with, due to the tradition of reporting results on Penn-YM in the past. Nevertheless, our result is higher than the second best by a large margin. Our Chinese parsing scores are the highest reported results. 1 http://stp.lingfil.uu.se//∼nivre/research/Penn2Malt.html The data was prepared by Andr´e F. T. Martins as was done in Martins et al. (2013). 2 Parser Zhang and Nivre (2011) Zhang and Nivre (reimpl.) (beam=64) Zhang and Nivre (reimpl.) (beam=128) Koo and Collins (2010) Zhang and McDonald (2012) Rush and Petrov (2012) Martins et al. (2013) Qian and Liu (2013) Bohnet and Kuhn (2012) Ma and Zhao (2012) cube-pruning w/ skip w/ s-max w/ p-max UAS 92.993.00 92.94 93.04 93.06 93.07 93.17 93.39 93.493.21 93.50 93.44 Penn-YM LAS Toks/Sec † 680 91.891.98 800 91.91 400 91.86 220 740 180 † 120 92.38 92.07 300 92.41 300 92.33 300 UAS 92.96 93.11 92.792.82 92.92 93.59 93.64 Penn-S LAS Toks/Sec 90.74 500 90.84 250 4460 600 90.35 200 91.17 200 91"
D13-1093,Q13-1004,0,0.0970471,"Missing"
D13-1093,N12-1054,0,0.429959,"Missing"
D13-1093,D08-1059,0,0.0931437,"Missing"
D13-1093,D12-1030,1,0.913328,"the hypergraph (Chiang, 2007). Unfortunately, as the scope of features at each node increases, the inexactness of search and its negative impact on learning can potentially be exacerbated. Unlike sequential search, the impact on learning of approximate hypergraph search – as well as methods to mitigate any ill effects – has not been studied. Motivated by this, we develop online learning algorithms for inexact hypergraph search by generalizing the violation-fixing percepron of Huang et al. (2012). We empirically validate the benefit of this approach within the cube-pruning dependency parser of Zhang and McDonald (2012). Online learning algorithms like the perceptron are widely used for structured prediction tasks. For sequential search problems, like left-to-right tagging and parsing, beam search has been successfully combined with perceptron variants that accommodate search errors (Collins and Roark, 2004; Huang et al., 2012). However, perceptron training with inexact search is less studied for bottom-up parsing and, more generally, inference over hypergraphs. In this paper, we generalize the violation-fixing perceptron of Huang et al. (2012) to hypergraphs and apply it to the cube-pruning parser of Zhang"
D13-1093,P11-2033,0,0.103421,"eled dependency parser of Ma and Zhao (2012), although we did not utilize fourth-order features. The LAS score on Penn-YM is on par with the best reported by Bohnet and Kuhn (2012). On Penn-S, there are not many existing results to compare with, due to the tradition of reporting results on Penn-YM in the past. Nevertheless, our result is higher than the second best by a large margin. Our Chinese parsing scores are the highest reported results. 1 http://stp.lingfil.uu.se//∼nivre/research/Penn2Malt.html The data was prepared by Andr´e F. T. Martins as was done in Martins et al. (2013). 2 Parser Zhang and Nivre (2011) Zhang and Nivre (reimpl.) (beam=64) Zhang and Nivre (reimpl.) (beam=128) Koo and Collins (2010) Zhang and McDonald (2012) Rush and Petrov (2012) Martins et al. (2013) Qian and Liu (2013) Bohnet and Kuhn (2012) Ma and Zhao (2012) cube-pruning w/ skip w/ s-max w/ p-max UAS 92.993.00 92.94 93.04 93.06 93.07 93.17 93.39 93.493.21 93.50 93.44 Penn-YM LAS Toks/Sec † 680 91.891.98 800 91.91 400 91.86 220 740 180 † 120 92.38 92.07 300 92.41 300 92.33 300 UAS 92.96 93.11 92.792.82 92.92 93.59 93.64 Penn-S LAS Toks/Sec 90.74 500 90.84 250 4460 600 90.35 200 91.17 200 91.28 200 UAS 86.085.93 86.05 86.87"
D13-1093,D07-1096,1,\N,Missing
D13-1112,J92-4003,0,0.204004,"Missing"
D13-1112,P05-1022,0,0.130309,":7] = </s>, |c(r2 ) |= 3 ... (combos of the above atomic features) ... e(r0 ◦ r1 )[−2:] ◦ id(r2 ) id(r1 ) ◦ id(r2 ) Figure 5: Examples of WordEdges and non-local features. The notation uses the Python style subscript syntax. al., 1992), and w’s part-of-speech tag. If w is ChiWe first add the rule identification feature for each nese we also include its word type (punctuations, rule: id(ri ). We also introduce lexicalized Word- digits, alpha, or otherwise) and (leftmost or rightEdges features, which are shown to be very effec- most) character. In such a way, we significantly intive in parsing (Charniak and Johnson, 2005) and crease the feature coverage on unseen data. However, if we allow arbitrary combinations, we MT (Liu et al., 2008; He et al., 2008) literatures. We use the following atomic features when apply- can extract a hexalexical feature (4 Chinese + 2 Ening a rule ri = hc(ri ), e(ri )i: the source-side length glish words) for a local window in Figure 5, which |c(ri )|, the boundary words of both c(ri ) and e(ri ), is unlikely to be seen at test time. To control model and the surrounding words of c(ri ) on the input sen- complexity we introduce a feature budget for each tence x. See Figure 5 for exa"
D13-1112,D08-1024,0,0.0581263,"s, in this paper we use phrase-based translation, but our method has the potential to be applicable to other translation paradigms. Extensive experiments on both Chineseto-English and Spanish-to-English tasks show statistically significant gains in B LEU by up to +2.3/+2.0 on dev/test over M ERT, and up to +1.5/+1.5 over P RO, thanks to 20M+ sparse features. 2 Phrase-Based MT and Forced Decoding We first review the basic phrase-based decoding algorithm (Koehn, 2004), which will be adapted for forced decoding. 0 1 We will use the following running example from Chinese to English from Mi et al. (2008): 1113 3 4 5 6 Figure 1: Standard beam-search phrase-based decoding. B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an Bush with Sharon hold -ed meeting ‘Bush held a meeting with Sharon’ Phrase-based decoders generate partial targetlanguage outputs in left-to-right order in the form of hypotheses (or states) (Koehn, 2004). Each hypothesis has a coverage vector capturing the sourcelanguage words translated so far, and can be extended into a longer hypothesis by a phrase-pair translating an uncovered segment. For example, the following is one possible derivation: ) : (0, “”) r1 (•1 ) : (s1 , “Bush”) r2"
D13-1112,P04-1015,0,0.710243,"se features. This is the first successful effort of large-scale online discriminative training for MT. As a notable exception, Liang et al. (2006) do train a structured perceptron model on the training data with sparse features, but fail to outperform M ERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and M IRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). 1 Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2"
D13-1112,N12-1015,1,0.82011,"lish and Spanish-to-English tasks show substantial gains in B LEU by up to +2.3/+2.0 on dev/test over M ERT, thanks to 20M+ sparse features. This is the first successful effort of large-scale online discriminative training for MT. As a notable exception, Liang et al. (2006) do train a structured perceptron model on the training data with sparse features, but fail to outperform M ERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and M IRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). 1 Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-lik"
D13-1112,P08-1067,1,0.834225,"s are ICTCLAS for Chinese (Zhang et al., 2003) and Penn Treebank for English (Marcus et al., 1993). 4.2 Addressing Overfitting With large numbers of lexicalized combo features we will face the overfitting problem, where some combo features found in the training data are too rare to be seen in the test data. Thus we propose three ways to alleviate this problem. First, we introduce various levels of backoffs for each word w (see Table 1). We include w’s Brown cluster and its prefixes of lengths 4 and 6 (Brown et 1117 4.3 Non-Local Features Following the success of non-local features in parsing (Huang, 2008) and MT (Vaswani et al., 2011), we also introduce them to capture the contextual information in MT. Our non-local features, shown in Figure 5, include bigram rule-ids and the concatenation of a rule id with the translation history, i.e. the last two English words. Note that we also use backoffs (Table 1) for the words included. Experiments (Section 5.3) show that although the set of non-local features is just a tiny fraction of all features, it contributes substantially to the improvement in B LEU. Scale small large large Language Pair C H -E N S P -E N Training Data # sent. # words 30K 0.8M/1"
D13-1112,P07-2045,0,0.00940476,"Design We compare the above new update methods with the two existing ones from Liang et al. (2006). Standard update (also known as “bold update” in Liang et al. (2006)) simply updates at the very end, from the best derivation in the beam towards the best gold-standard derivation (regardless of whether Our feature set includes the following 11 dense features: LM, four conditional and lexical translation probabilities (pc (e|f ), pc (f |e), pl (e|f ), pl (f |e)), length and phrase penalties, distortion cost, and three lexicalized reordering features. All these features are inherited from Moses (Koehn et al., 2007). 1116 ,Bush ) : (s01 , “<s> Bush”) (•1 (• •••6 ,talks ) : (s02 , “<s> Bush held talks”) <s> B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an r1 <s> features for applying r2 on span x[3:6] r2 WordEdges </s> r2 Bush held talks non-local c(r2 )[0] = jˇux´ıng, c(r2 )[−1] = hu`ıt´an e(r2 )[0] = held, e(r2 )[−1] = talks x[2:3] = Sh¯al´ong, x[6:7] = </s>, |c(r2 ) |= 3 ... (combos of the above atomic features) ... e(r0 ◦ r1 )[−2:] ◦ id(r2 ) id(r1 ) ◦ id(r2 ) Figure 5: Examples of WordEdges and non-local features. The notation uses the Python style subscript syntax. al., 1992), and w’s part-of-speech tag. I"
D13-1112,koen-2004-pharaoh,0,0.104318,"set of sparse, lexicalized, and non-local features, and we propose various ways to alleviate overfitting. For simplicity and efficiency reasons, in this paper we use phrase-based translation, but our method has the potential to be applicable to other translation paradigms. Extensive experiments on both Chineseto-English and Spanish-to-English tasks show statistically significant gains in B LEU by up to +2.3/+2.0 on dev/test over M ERT, and up to +1.5/+1.5 over P RO, thanks to 20M+ sparse features. 2 Phrase-Based MT and Forced Decoding We first review the basic phrase-based decoding algorithm (Koehn, 2004), which will be adapted for forced decoding. 0 1 We will use the following running example from Chinese to English from Mi et al. (2008): 1113 3 4 5 6 Figure 1: Standard beam-search phrase-based decoding. B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an Bush with Sharon hold -ed meeting ‘Bush held a meeting with Sharon’ Phrase-based decoders generate partial targetlanguage outputs in left-to-right order in the form of hypotheses (or states) (Koehn, 2004). Each hypothesis has a coverage vector capturing the sourcelanguage words translated so far, and can be extended into a longer hypothesis by a phra"
D13-1112,P06-1096,0,0.252679,"Missing"
D13-1112,W02-1001,0,0.345469,"ng data with sparse features, but fail to outperform M ERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and M IRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). 1 Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), P RO (Hopkins and May, 2011), and R AMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexicalized, most of which can no"
D13-1112,D08-1010,0,0.0674233,"amples of WordEdges and non-local features. The notation uses the Python style subscript syntax. al., 1992), and w’s part-of-speech tag. If w is ChiWe first add the rule identification feature for each nese we also include its word type (punctuations, rule: id(ri ). We also introduce lexicalized Word- digits, alpha, or otherwise) and (leftmost or rightEdges features, which are shown to be very effec- most) character. In such a way, we significantly intive in parsing (Charniak and Johnson, 2005) and crease the feature coverage on unseen data. However, if we allow arbitrary combinations, we MT (Liu et al., 2008; He et al., 2008) literatures. We use the following atomic features when apply- can extract a hexalexical feature (4 Chinese + 2 Ening a rule ri = hc(ri ), e(ri )i: the source-side length glish words) for a local window in Figure 5, which |c(ri )|, the boundary words of both c(ri ) and e(ri ), is unlikely to be seen at test time. To control model and the surrounding words of c(ri ) on the input sen- complexity we introduce a feature budget for each tence x. See Figure 5 for examples. These atomic level of backoffs, shown in the last column in Tafeatures are concatenated to generate all kinds"
D13-1112,N13-1025,0,0.0872383,"updates in standard update. than half of the updates remain invalid even at a beam of 30. These analyses provide an alternative but theoretically more reasonable explanation to the findings of Liang et al. (2006): while they blame “unreasonable” gold derivations for the failure of standard update, we observe that it is the search errors that make the real difference, and that an update that respects search errors towards a gold subderivation is indeed helpful, even if that subderivation might be “unreasonable”. In order to speedup training, we use mini-batch parallelization of Zhao and Huang (2013) which has been shown to be much faster than previous parallelization methods. We set the mini-batch size to 24 and train M AX F ORCE with 1, 6, and 24 cores on a small subset of the our original reachable sen24 26 25 PRO-dense 24 BLEU BLEU MERT 23 minibatch(24-core) minibatch(6-core) minibatch(1 core) single processor MERT 23 22 21 +non-local +word-edges +ruleid dense 20 19 22 18 0 0.5 1 1.5 2 2.5 Time 3 3.5 4 Figure 10: Minibatch parallelization speeds up learning. 2 4 6 8 10 12 Number of iteration 14 16 Figure 12: Incremental contributions of different feature sets (dense features, ruleid,"
D13-1112,N13-1048,0,0.153221,"mparable to the improvements on the C H -E N task, since it is well accepted in MT literature that a change of δ in 1-reference B LEU is roughly equivalent to a change of 2δ with 4 references. 1121 Besides those discussed in Section 1, there are also some research on tuning sparse features on the training data, but they integrate those sparse features into the MT log-linear model as a single feature weight, and tune its weight on the dev set (e.g. (Liu et al., 2008; He et al., 2008; Wuebker et al., 2010; Simianer et al., 2012; Flanigan et al., 2013; Setiawan and Zhou, 2013; He and Deng, 2012; Gao and He, 2013)). By contrast, our approach learns sparse features only on the training set, and use dev set as heldout to know when to stop. Forced decoding has been used in the MT literature. For example, open source MT systems Moses and cdec have implemented it. Liang et al. (2012) also use the it to boost the M ERT tuning by adding more y-good derivations to the standard k-best list. 7 Conclusions and Future Work We have presented a simple yet effective approach of structured learning for machine translation which scales, for the first time, to a large portion of the whole training data, and enables us t"
D13-1112,N12-1023,0,0.0612216,"ive training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), P RO (Hopkins and May, 2011), and R AMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexicalized, most of which can not † 3 {huang@cs.qc,kzhao@gc}.cuny.edu Abstract ∗ Haitao Mi3 Work done while visiting City University of New York. Corresponding author. To address the search error problem we propose a very simple approach based on the recent framework of “violation-fixing perceptron” (Huang et al., 2012) which is designed specifically for inexact search, with a theoretical convergence guarantee and excellent empirical performance on beam search parsing and tagging. The basic idea is to update when search"
D13-1112,J93-2004,0,0.0461422,"for large datasets, we Brown cluster, prefix 4 4 4 2 simply remove all one-count rules, but for small 52 36 2 POS tag datasets where out-of-vocabulary words (OOVs) word type 4 1 abound, we use a simple leave-one-out method: Table 1: Various levels of backoff for WordEdges fea- when training on a sentence pair (x, y), do not use tures. Class size is estimated on the small Chinese- the one-count rules extracted from (x, y) itself. 4.1 Local Sparse Features: Ruleid & WordEdges English dataset (Sec. 5.3). The POS tagsets are ICTCLAS for Chinese (Zhang et al., 2003) and Penn Treebank for English (Marcus et al., 1993). 4.2 Addressing Overfitting With large numbers of lexicalized combo features we will face the overfitting problem, where some combo features found in the training data are too rare to be seen in the test data. Thus we propose three ways to alleviate this problem. First, we introduce various levels of backoffs for each word w (see Table 1). We include w’s Brown cluster and its prefixes of lengths 4 and 6 (Brown et 1117 4.3 Non-Local Features Following the success of non-local features in parsing (Huang, 2008) and MT (Vaswani et al., 2011), we also introduce them to capture the contextual infor"
D13-1112,P05-1012,0,0.107618,"tured perceptron model on the training data with sparse features, but fail to outperform M ERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and M IRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). 1 Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), P RO (Hopkins and May, 2011), and R AMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexic"
D13-1112,P08-1023,1,0.766752,"ncy reasons, in this paper we use phrase-based translation, but our method has the potential to be applicable to other translation paradigms. Extensive experiments on both Chineseto-English and Spanish-to-English tasks show statistically significant gains in B LEU by up to +2.3/+2.0 on dev/test over M ERT, and up to +1.5/+1.5 over P RO, thanks to 20M+ sparse features. 2 Phrase-Based MT and Forced Decoding We first review the basic phrase-based decoding algorithm (Koehn, 2004), which will be adapted for forced decoding. 0 1 We will use the following running example from Chinese to English from Mi et al. (2008): 1113 3 4 5 6 Figure 1: Standard beam-search phrase-based decoding. B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an Bush with Sharon hold -ed meeting ‘Bush held a meeting with Sharon’ Phrase-based decoders generate partial targetlanguage outputs in left-to-right order in the form of hypotheses (or states) (Koehn, 2004). Each hypothesis has a coverage vector capturing the sourcelanguage words translated so far, and can be extended into a longer hypothesis by a phrase-pair translating an uncovered segment. For example, the following is one possible derivation: ) : (0, “”) r1 (•1 ) : (s1 , “Bush”) r2"
D13-1112,P12-1031,0,0.0155333,"These gains are comparable to the improvements on the C H -E N task, since it is well accepted in MT literature that a change of δ in 1-reference B LEU is roughly equivalent to a change of 2δ with 4 references. 1121 Besides those discussed in Section 1, there are also some research on tuning sparse features on the training data, but they integrate those sparse features into the MT log-linear model as a single feature weight, and tune its weight on the dev set (e.g. (Liu et al., 2008; He et al., 2008; Wuebker et al., 2010; Simianer et al., 2012; Flanigan et al., 2013; Setiawan and Zhou, 2013; He and Deng, 2012; Gao and He, 2013)). By contrast, our approach learns sparse features only on the training set, and use dev set as heldout to know when to stop. Forced decoding has been used in the MT literature. For example, open source MT systems Moses and cdec have implemented it. Liang et al. (2012) also use the it to boost the M ERT tuning by adding more y-good derivations to the standard k-best list. 7 Conclusions and Future Work We have presented a simple yet effective approach of structured learning for machine translation which scales, for the first time, to a large portion of the whole training dat"
D13-1112,P03-1021,0,0.209362,"aining (see Table 3). In order to test our approach in different language pairs, we conduct three experiments, shown in Table 2, on two significantly different language pairs (long vs. short distance reorderings), Chinese-toEnglish (C H -E N) and Spanish-to-English (S P -E N). 5.1 System Preparation and Data We base our experiments on Cubit, a state-of-art phrase-based system in Python (Huang and Chiang, 2007).1 We set phrase-limit to 7 in rule extraction, and beam size to 30 and distortion limit 6 in decoding. We compare our violation-fixing perceptron with two popular tuning methods: M ERT (Och, 2003) and P RO (Hopkins and May, 2011). For word alignments we use GIZA++-`0 (Vaswani et al., 2012) which produces sparser alignments, alleviating the garbage collection problem. We use the SRILM toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on 1.5M English sentences. Our dev and test sets for C H -E N task are from the newswire portion of 2006 and 2008 NIST MT Evaluations (616/691 sentences, 18575/18875 words), with four references.2 The dev and test sets for S P E N task are from newstest2012 and newstest2013, with only one reference. Below both M ER"
D13-1112,C08-1041,0,0.0607494,"s and non-local features. The notation uses the Python style subscript syntax. al., 1992), and w’s part-of-speech tag. If w is ChiWe first add the rule identification feature for each nese we also include its word type (punctuations, rule: id(ri ). We also introduce lexicalized Word- digits, alpha, or otherwise) and (leftmost or rightEdges features, which are shown to be very effec- most) character. In such a way, we significantly intive in parsing (Charniak and Johnson, 2005) and crease the feature coverage on unseen data. However, if we allow arbitrary combinations, we MT (Liu et al., 2008; He et al., 2008) literatures. We use the following atomic features when apply- can extract a hexalexical feature (4 Chinese + 2 Ening a rule ri = hc(ri ), e(ri )i: the source-side length glish words) for a local window in Figure 5, which |c(ri )|, the boundary words of both c(ri ) and e(ri ), is unlikely to be seen at test time. To control model and the surrounding words of c(ri ) on the input sen- complexity we introduce a feature budget for each tence x. See Figure 5 for examples. These atomic level of backoffs, shown in the last column in Tafeatures are concatenated to generate all kinds of ble 1. The tota"
D13-1112,N13-1034,0,0.0544204,"3/+1.1 B LEU on dev/test. These gains are comparable to the improvements on the C H -E N task, since it is well accepted in MT literature that a change of δ in 1-reference B LEU is roughly equivalent to a change of 2δ with 4 references. 1121 Besides those discussed in Section 1, there are also some research on tuning sparse features on the training data, but they integrate those sparse features into the MT log-linear model as a single feature weight, and tune its weight on the dev set (e.g. (Liu et al., 2008; He et al., 2008; Wuebker et al., 2010; Simianer et al., 2012; Flanigan et al., 2013; Setiawan and Zhou, 2013; He and Deng, 2012; Gao and He, 2013)). By contrast, our approach learns sparse features only on the training set, and use dev set as heldout to know when to stop. Forced decoding has been used in the MT literature. For example, open source MT systems Moses and cdec have implemented it. Liang et al. (2012) also use the it to boost the M ERT tuning by adding more y-good derivations to the standard k-best list. 7 Conclusions and Future Work We have presented a simple yet effective approach of structured learning for machine translation which scales, for the first time, to a large portion of the"
D13-1112,D11-1125,0,0.288793,"ntroduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), P RO (Hopkins and May, 2011), and R AMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexicalized, most of which can not † 3 {huang@cs.qc,kzhao@gc}.cuny.edu Abstract ∗ Haitao Mi3 Work done while visiting City University of New York. Corresponding author. To address the search error problem we propose a very simple approach based on the recent framework of “violation-fixing perceptron” (Huang et al., 2012) which is designed specifically for inexact search, with a theoretical convergence guarantee and excellent empirical performance on beam search parsing and tagging. The"
D13-1112,P07-1019,1,0.333359,", 5.3 5.2, 5.4 5.5 Table 2: Overview of all experiments. The ∆B LEU column shows the absolute improvements of our method M AX F ORCE on dev/test sets over M ERT. The Chinese datasets also use prefix-pairs in training (see Table 3). In order to test our approach in different language pairs, we conduct three experiments, shown in Table 2, on two significantly different language pairs (long vs. short distance reorderings), Chinese-toEnglish (C H -E N) and Spanish-to-English (S P -E N). 5.1 System Preparation and Data We base our experiments on Cubit, a state-of-art phrase-based system in Python (Huang and Chiang, 2007).1 We set phrase-limit to 7 in rule extraction, and beam size to 30 and distortion limit 6 in decoding. We compare our violation-fixing perceptron with two popular tuning methods: M ERT (Och, 2003) and P RO (Hopkins and May, 2011). For word alignments we use GIZA++-`0 (Vaswani et al., 2012) which produces sparser alignments, alleviating the garbage collection problem. We use the SRILM toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on 1.5M English sentences. Our dev and test sets for C H -E N task are from the newswire portion of 2006 and 2008 NIST"
D13-1112,P12-1002,0,0.0244538,"oves the translation quality over M ERT by +1.3/+1.1 B LEU on dev/test. These gains are comparable to the improvements on the C H -E N task, since it is well accepted in MT literature that a change of δ in 1-reference B LEU is roughly equivalent to a change of 2δ with 4 references. 1121 Besides those discussed in Section 1, there are also some research on tuning sparse features on the training data, but they integrate those sparse features into the MT log-linear model as a single feature weight, and tune its weight on the dev set (e.g. (Liu et al., 2008; He et al., 2008; Wuebker et al., 2010; Simianer et al., 2012; Flanigan et al., 2013; Setiawan and Zhou, 2013; He and Deng, 2012; Gao and He, 2013)). By contrast, our approach learns sparse features only on the training set, and use dev set as heldout to know when to stop. Forced decoding has been used in the MT literature. For example, open source MT systems Moses and cdec have implemented it. Liang et al. (2012) also use the it to boost the M ERT tuning by adding more y-good derivations to the standard k-best list. 7 Conclusions and Future Work We have presented a simple yet effective approach of structured learning for machine translation which scale"
D13-1112,P11-1086,1,0.860759,"nese (Zhang et al., 2003) and Penn Treebank for English (Marcus et al., 1993). 4.2 Addressing Overfitting With large numbers of lexicalized combo features we will face the overfitting problem, where some combo features found in the training data are too rare to be seen in the test data. Thus we propose three ways to alleviate this problem. First, we introduce various levels of backoffs for each word w (see Table 1). We include w’s Brown cluster and its prefixes of lengths 4 and 6 (Brown et 1117 4.3 Non-Local Features Following the success of non-local features in parsing (Huang, 2008) and MT (Vaswani et al., 2011), we also introduce them to capture the contextual information in MT. Our non-local features, shown in Figure 5, include bigram rule-ids and the concatenation of a rule id with the translation history, i.e. the last two English words. Note that we also use backoffs (Table 1) for the words included. Experiments (Section 5.3) show that although the set of non-local features is just a tiny fraction of all features, it contributes substantially to the improvement in B LEU. Scale small large large Language Pair C H -E N S P -E N Training Data # sent. # words 30K 0.8M/1.0M 230K 6.9M/8.9M 174K 4.9M/4"
D13-1112,P12-1033,1,0.356264,"conduct three experiments, shown in Table 2, on two significantly different language pairs (long vs. short distance reorderings), Chinese-toEnglish (C H -E N) and Spanish-to-English (S P -E N). 5.1 System Preparation and Data We base our experiments on Cubit, a state-of-art phrase-based system in Python (Huang and Chiang, 2007).1 We set phrase-limit to 7 in rule extraction, and beam size to 30 and distortion limit 6 in decoding. We compare our violation-fixing perceptron with two popular tuning methods: M ERT (Och, 2003) and P RO (Hopkins and May, 2011). For word alignments we use GIZA++-`0 (Vaswani et al., 2012) which produces sparser alignments, alleviating the garbage collection problem. We use the SRILM toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on 1.5M English sentences. Our dev and test sets for C H -E N task are from the newswire portion of 2006 and 2008 NIST MT Evaluations (616/691 sentences, 18575/18875 words), with four references.2 The dev and test sets for S P E N task are from newstest2012 and newstest2013, with only one reference. Below both M ERT and P RO tune weights on the dev set, while our method on the training set. Specifically, ou"
D13-1112,D07-1080,0,0.134666,"rror is severe (Collins and Roark, 2004; Zhang et al., 2013). 1 Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), P RO (Hopkins and May, 2011), and R AMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexicalized, most of which can not † 3 {huang@cs.qc,kzhao@gc}.cuny.edu Abstract ∗ Haitao Mi3 Work done while visiting City University of New York. Corresponding author. To address the search error problem we propose a very simple approach based on the recent framework of “violation-fixing perceptron” (Huang et al., 2012) which is designed specifically for inexact search, with a theoretical convergence guarantee and excell"
D13-1112,P10-1049,0,0.0524699,"that M AX F ORCE improves the translation quality over M ERT by +1.3/+1.1 B LEU on dev/test. These gains are comparable to the improvements on the C H -E N task, since it is well accepted in MT literature that a change of δ in 1-reference B LEU is roughly equivalent to a change of 2δ with 4 references. 1121 Besides those discussed in Section 1, there are also some research on tuning sparse features on the training data, but they integrate those sparse features into the MT log-linear model as a single feature weight, and tune its weight on the dev set (e.g. (Liu et al., 2008; He et al., 2008; Wuebker et al., 2010; Simianer et al., 2012; Flanigan et al., 2013; Setiawan and Zhou, 2013; He and Deng, 2012; Gao and He, 2013)). By contrast, our approach learns sparse features only on the training set, and use dev set as heldout to know when to stop. Forced decoding has been used in the MT literature. For example, open source MT systems Moses and cdec have implemented it. Liang et al. (2012) also use the it to boost the M ERT tuning by adding more y-good derivations to the standard k-best list. 7 Conclusions and Future Work We have presented a simple yet effective approach of structured learning for machine"
D13-1112,W03-1730,0,0.0131616,"luster, prefix 6 6 8 2 ting due to one-count rules: for large datasets, we Brown cluster, prefix 4 4 4 2 simply remove all one-count rules, but for small 52 36 2 POS tag datasets where out-of-vocabulary words (OOVs) word type 4 1 abound, we use a simple leave-one-out method: Table 1: Various levels of backoff for WordEdges fea- when training on a sentence pair (x, y), do not use tures. Class size is estimated on the small Chinese- the one-count rules extracted from (x, y) itself. 4.1 Local Sparse Features: Ruleid & WordEdges English dataset (Sec. 5.3). The POS tagsets are ICTCLAS for Chinese (Zhang et al., 2003) and Penn Treebank for English (Marcus et al., 1993). 4.2 Addressing Overfitting With large numbers of lexicalized combo features we will face the overfitting problem, where some combo features found in the training data are too rare to be seen in the test data. Thus we propose three ways to alleviate this problem. First, we introduce various levels of backoffs for each word w (see Table 1). We include w’s Brown cluster and its prefixes of lengths 4 and 6 (Brown et 1117 4.3 Non-Local Features Following the success of non-local features in parsing (Huang, 2008) and MT (Vaswani et al., 2011), we"
D13-1112,D13-1093,1,0.086367,"first successful effort of large-scale online discriminative training for MT. As a notable exception, Liang et al. (2006) do train a structured perceptron model on the training data with sparse features, but fail to outperform M ERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and M IRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). 1 Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), P"
D13-1112,N13-1038,1,0.225664,"atio of invalid updates in standard update. than half of the updates remain invalid even at a beam of 30. These analyses provide an alternative but theoretically more reasonable explanation to the findings of Liang et al. (2006): while they blame “unreasonable” gold derivations for the failure of standard update, we observe that it is the search errors that make the real difference, and that an update that respects search errors towards a gold subderivation is indeed helpful, even if that subderivation might be “unreasonable”. In order to speedup training, we use mini-batch parallelization of Zhao and Huang (2013) which has been shown to be much faster than previous parallelization methods. We set the mini-batch size to 24 and train M AX F ORCE with 1, 6, and 24 cores on a small subset of the our original reachable sen24 26 25 PRO-dense 24 BLEU BLEU MERT 23 minibatch(24-core) minibatch(6-core) minibatch(1 core) single processor MERT 23 22 21 +non-local +word-edges +ruleid dense 20 19 22 18 0 0.5 1 1.5 2 2.5 Time 3 3.5 4 Figure 10: Minibatch parallelization speeds up learning. 2 4 6 8 10 12 Number of iteration 14 16 Figure 12: Incremental contributions of different feature sets (dense features, ruleid,"
D14-1209,N12-1047,0,0.593468,"=1..|x| where top1w (·) is defined in Eq. (4), and δyx (d), defined in Table 1, is the generic metric for evaluating a partial derivation d which has two implementations (partial bleu or potential bleu). In order words we can obtain two implementations of search-aware M ERT methods, SA-M ERT par and SA-M ERTpot . Notice that the traditional M ERT is a special case of SA-M ERT where i is fixed to |x|. 4.2 From M IRA to Search-Aware M IRA M IRA is another popular tuning method for SMT. It firstly introduced in (Watanabe et al., 2007), and then was improved in (Chiang et al., 2008; Chiang, 2012; Cherry and Foster, 2012). Its main idea is to optimize a weight such that the model score difference of a pair of derivations is greater than their loss difference. In this paper, we follow the objective function in (Chiang, 2012; Cherry and Foster, 2012), where only the violation between hope and fear derivations is concerned. Formally, we define d+ (x, y) and d− (x, y) as the hope and fear derivations in the final bin (i.e., complete derivations): d+ (x, y) = argmax w0 · Φ(d) − δy (d) (10) d∈B|x |(x) − d (x, y) = argmax w0 · Φ(d) + δy (d) (11) d∈B|x |(x) where w0 is the current model. The loss function   of M IRA"
D14-1209,D08-1024,0,0.0466324,"fter tuning. 4.1 From M ERT to Search-Aware M ERT Suppose D is a tuning set of hx, yi pairs. Traditional M ERT learns the weight by iteratively reranking the complete translations towards those with higher B LEU in the final bin B|x |(x) for each x in D. Formally, it tries to minimize the document-level error of 1-best translations:   M `M ERT (D, w) = δy top1w (B|x |(x)) , hx,yi∈D (4) where top1w (S) is the best derivation in S under model w, and δ· (·) is the full derivation metric as defined in Table 1; in this paper we use δy (y 0 ) = −B LEU(y, y 0 ). Here we follow Och (2003) and Lopez (2008) to simplify P the notations, where the ⊕ operator (similar to ) is an over-simplification for B LEU which, as a document-level metric, is actually not factorizable across sentences. Besides reranking the complete translations as traditional M ERT, our search-aware M ERT (SAM ERT) also reranks the partial translations such that potential translations may survive in the middle bins during search. Formally, its objective function is defined as follows: `SA-M ERT(D, w) = M L hx,yi∈D i=1..|x| where top1w (·) is defined in Eq. (4), and δyx (d), defined in Table 1, is the generic metric for evaluati"
D14-1209,P11-2031,0,0.0972672,"for us, and thus we use ssmt07 as the tuning set, which is provided at the Third Symposium on Statistical Machine Translation (http://mitlab.hit.edu.cn/ssmt2007.html). option. The LM for E N -C H is trained on its target side; and that for C H -E N is trained on the Xinhua portion of Gigaword. We use B LEU-4 (Papineni et al., 2002) with “average ref-len” to evaluate the translation performance for all experiments. In particular, the character-based B LEU-4 is employed for E N -C H task. Since all tuning methods involve randomness, all scores reported are average of three runs, as suggested by Clark et al. (2011) for fairer comparisons. 5.2 Main Results on C H -E N Task Table 2 depicts the main results of our methods on C H -E N translation task. On all five test sets, our methods consistently achieve substantial improvements with two pruning options: SA-M ERT pot gains +1.2 B LEU points over M ERT on average; and SA-M IRApot gains +1.8 B LEU points over M IRA on average as well. SA-P RO pot , however, does not work out of the box when we use the entire nist02 as the tuning set, which might be attributed to the “Monster” behavior (Nakov et al., 2013). To alleviate this problem, we only use the 109 sho"
D14-1209,P04-1015,0,0.107435,"acteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard training for many beam search based NLP systems (Huang et al., 2012). As a result, Collins and Roark (2004) and Huang et al. (2012) propose the early-update and max-violation update to deal with the search errors. Their idea is to update on prefix or partial hypotheses when the correct solution falls out of the beam. This idea has been successfully used in many NLP tasks and improves the performance over the state-of-art NLP systems (Huang and Sagae, 2010; Huang et al., 2012; Zhang et al., 2013). Goldberg and Nivre (2012) propose the concept of “dynamic oracle” which is the absolute best potential of a partial derivation, and is more akin to a strictly admissible heuristic. This idea inspired and i"
D14-1209,P13-1110,0,0.510575,"envelope calculation for exact line search (see (Och, 2003)) in M ERT is less efficient than the update based on (sub-)gradient with inexact line search in M IRA and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their de"
D14-1209,N13-1025,0,0.133581,"and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard training for many beam sear"
D14-1209,D11-1004,0,0.117729,"-M ERT is much slower than M ERT. The main reason is that, as the training examples increase dramatically, the envelope calculation for exact line search (see (Och, 2003)) in M ERT is less efficient than the update based on (sub-)gradient with inexact line search in M IRA and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search error"
D14-1209,D13-1201,0,0.449519,"an M ERT. The main reason is that, as the training examples increase dramatically, the envelope calculation for exact line search (see (Och, 2003)) in M ERT is less efficient than the update based on (sub-)gradient with inexact line search in M IRA and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential parti"
D14-1209,E14-1002,0,0.019705,"rrect solution falls out of the beam. This idea has been successfully used in many NLP tasks and improves the performance over the state-of-art NLP systems (Huang and Sagae, 2010; Huang et al., 2012; Zhang et al., 2013). Goldberg and Nivre (2012) propose the concept of “dynamic oracle” which is the absolute best potential of a partial derivation, and is more akin to a strictly admissible heuristic. This idea inspired and is closely related to our potential B LEU, except that in our case, computing an admissible heuristic is too costly, so our potential B LEU is more like an average potential. Gesmundo and Henderson (2014) also consider the rankings between partial translation pairs as well. However, they evaluate a partial translation through extending it to a complete translation by re-decoding, and thus they need many passes of decoding for many partial translations, while ours only need one pass of decoding for all partial translations and thus is much more efficient. In summary, our tuning framework is more general and has potential to be employed over all the state-ofart tuning methods mentioned above, even though ours is only tested on three popular methods. 7 Conclusions and Future Work We have presente"
D14-1209,N12-1023,0,0.0419101,"rease dramatically, the envelope calculation for exact line search (see (Och, 2003)) in M ERT is less efficient than the update based on (sub-)gradient with inexact line search in M IRA and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree"
D14-1209,N09-2003,0,0.0203737,"ims to promote not only the accurate translations in the final bin, but more importantly those potentially promising partial derivations in non-final bins. The key challenge, however, is how to evaluate the “promise” or “potential” of a partial derivation. In this Section, we develop two such measures, a simple “partial B LEU” (Section 3.1) and a more principled “potential B LEU” (Section 3.2). In Section 4, we will then adapt traditional tuning methods to their search-aware versions using these partial evaluation metrics. 3.1 Solution 1: Simple and Naive Partial B LEU Inspired by a trick in (Li and Khudanpur, 2009) {d ◦ r |d ∈ Bi−j (x), |c(r) |= j})and (Chiang, 2012) for oracle or hope extraction, j=1..i we use a very simple metric to evaluate partial translations for tuning. For a given derivation d, where r is a rule covering j Chinese words, and the basic idea is to evaluate the (short) partial transtopkw0 (·) returns the top k derivations according lation e(d) against the (full) reference y, but using to the current model w0 . As a special case, note a “prorated” reference length proportional to c(d) that top1w0 (S) = argmaxd∈S w0 · Φ(d), so which is the number of Chinese words covered so top1w0 (B|"
D14-1209,D13-1111,0,0.123484,"-best decoding on nist05 test set, and calculate the oracle over these two k-best lists. The oracle B LEU comparison is shown in Table 4. On nist05 test set, for M ERT the oracle B LEU is 41.1; while for SA-M ERT its oracle B LEU is 42.7, i.e. with 1.6 B LEU improvements. Although search-aware tuning employs the objective different from the objective of evaluation on nist02 tuning set, it still gains 0.5 B LEU improvements. Diversity. A k-best list with higher diversity can better represent the entire decoding space, and thus tuning on such a k-best list may lead to better tesing performance (Gimpel et al., 2013). Intuitively, tuning with all bins will encourage the diversity in prefix, infix and suffix of complete translations in the final bin. To testify this, we need a diversity metric. Indeed, Gimpel et al. (2013) define a diversity metric based on the n-gram matches between two sentences y and y 0 as follows: 1948 0 d(y, y ) = − |y|−q |y 0 |−q X X i=1 j=1 0 [[yi:i+q = yj:j+q ]] Methods M ERT SA-M ERTpot M AX F ORCE M AX F ORCE M ERT SA-M ERTpot set nist02 nist02 nist02-px train-r-part nist02-r nist02-r tuning set # refs # sents 4 878 4 878 1 434 1 1225 1 92 1 92 # words 23181 23181 6227 22684 117"
D14-1209,P13-1031,0,0.180276,"r derivations are defined in Equations 10–13, and we define ∆δy (d1 , d2 ) = δy (d1 ) − δy (d2 ), and ∆δyx (d1 , d2 ) = δyx (d1 ) − δyx (d2 ). In addition, [θ]+ = max{θ, 0}. and fear derivations from the final bin to all bins: d+ i (x, y) = argmax w0 · Φ(d) − δy (d) (12) d∈ Bi (x) d− i (x, y) = argmax w0 · Φ(d) + δy (d) (13) d∈ Bi (x) The new loss function for SA-M IRA is Eq. 7 in Figure 4. Now instead of one update per sentence, we will perform |x |updates, each based on a pair − d+ i (x, y) and di (x, y). 4.3 From P RO to Search-Aware P RO Finally, the P RO algorithm (Hopkins and May, 2011; Green et al., 2013) aims to correlate the ranking under model score and the ranking under B LEU score, among all complete derivations in the final bin. For each preference-pair d1 , d2 ∈ B|x |(x) such that d1 has a higher B LEU score than d2 (i.e., δy (d1 ) &lt; δy (d2 )), we add one positive example Φ(d1 ) − Φ(d2 ) and one negative example Φ(d2 ) − Φ(d1 ). Now to adapt it to search-aware P RO (SAP RO), we will have many more examples to consider: besides the final bin, we will include all preference-pairs in the non-final bins as well. For each bin Bi (x), for each preference-pairs d1 , d2 ∈ Bi (x) such that d1 ha"
D14-1209,D11-1125,0,0.597913,"g methods, and extensive experiments on Chinese-to-English and English-to-Chinese translation show up to +2.6 B LEU gains over search-agnostic baselines. 1 2 3 4 (a) (b) Figure 1: (a) Some potentially promising partial translations (in red) fall out of the beam (bin 2); (b) We identify such partial translations and assign them higher model scores so that they are more likely to survive the search. 1 Introduction Parameter tuning has been a key problem for machine translation since the statistical revolution. However, most existing tuning algorithms treat the decoder as a black box (Och, 2003; Hopkins and May, 2011; Chiang, 2012), ignoring the fact that many potentially promising partial translations are pruned by the decoder due to the prohibitively large search space. For example, the popular beam-search decoding algorithm for phrase-based MT (Koehn, 2004) only explores O(nb) items for a sentence of n words (with a beam width of b), while the full search space is O(2n n2 ) or worse (Knight, 1999). As one of the very few exceptions to the “search-agnostic” majority, Yu et al. (2013) and Zhao et al. (2014) propose a variant of the perceptron algorithm that learns to keep the reference translations in th"
D14-1209,W05-1506,1,0.697366,"w0 . As a special case, note a “prorated” reference length proportional to c(d) that top1w0 (S) = argmaxd∈S w0 · Φ(d), so which is the number of Chinese words covered so top1w0 (B|x |(x)) is the final 1-best result.1 See Fig- far in d: ure 2 for an illustration. |y |· |c(d)|/|x| Bi (x) = topkw0 ( [ 3 Challenge: Evaluating Partial Derivations As mentioned in Section 1, the current mainstream tuning methods such as M ERT, M IRA, and P RO are 1 Actually B|x |(x) is an approximation to the k-best list since some derivations are merged by dynamic programming; to recover those we can use Alg. 3 of Huang and Chiang (2005). For example, if d has covered 2 words on a 8word Chinese sentence with a 12-word English reference, then the “effective reference length” is 12 × 2/8 = 3. We call this method “partial B LEU” since it does not complete the translation, and denote it by  δ¯y|x |(d) = −δ y, e(d); reflen = |y |· |c(d)|/|x |. (1) 1943 δ(y, y 0 ) = −Bleu+1 (y, y 0 ) string distance metric δy (d) = δ(y, e(d)) ( |x| δ¯y (d) x δy (d) = δ(y, e¯x (d)) x= full derivations eval reordering e¯x (d) = partial bleu (Sec. 3.1) potential bleu (Sec. 3.2) Table 1: Notations for evaluating full and partial deriva|x| tions. Funct"
D14-1209,P07-1019,1,0.85133,"ch preference-pairs d1 , d2 ∈ Bi (x) such that d1 has a higher partial or potential B LEU score than d2 (i.e., δyx (d1 ) &lt; δyx (d2 )), we add one positive example Φ(d1 ) − Φ(d2 ) and one negative example Φ(d2 ) − Φ(d1 ). In sum, searchaware P RO has |x |times more examples than traditional P RO. The loss functions of P RO and searchaware P RO are defined in Figure 4. 5 Experiments We evaluate our new tuning methods on two large scale NIST translation tasks: Chinese-to-English (C H -E N) and English-to-Chinese (E N -C H) tasks. 5.1 System Preparation and Data We base our experiments on Cubit2 (Huang and Chiang, 2007), a state-of-art phrase-based system in Python. We set phrase-limit to 7, beam size to 30 and distortion limit 6. We use the 11 dense features from Moses (Koehn et al., 2007), which can lead to good performance and are widely used in almost all SMT systems. The baseline tuning methods M ERT (Och, 2003), M IRA (Cherry and Foster, 2012), and P RO (Hopkins and May, 2011) are from the Moses toolkit, which are batch tuning methods based on k-best translations. The searchaware tuning methods are called SA-M ERT, SAM IRA, and SA-P RO, respectively. Their partial B LEU versions are marked with supersc"
D14-1209,N12-1015,1,0.872703,"These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard training for many beam search based NLP systems (Huang et al., 2012). As a result, Collins and Roark (2004) and Huang et al. (2012) propose the early-update and max-violation update to deal with the search errors. Their idea is to update on prefix or partial hypotheses when the correct solution falls out of the beam. This idea has been successfully used in many NLP tasks and improves the performance over the state-of-art NLP systems (Huang and Sagae, 2010; Huang et al., 2012; Zhang et al., 2013). Goldberg and Nivre (2012) propose the concept of “dynamic oracle” which is the absolute best potential of a partial derivation, and is more akin to a strictly admissi"
D14-1209,J99-4005,0,0.0205655,". 1 Introduction Parameter tuning has been a key problem for machine translation since the statistical revolution. However, most existing tuning algorithms treat the decoder as a black box (Och, 2003; Hopkins and May, 2011; Chiang, 2012), ignoring the fact that many potentially promising partial translations are pruned by the decoder due to the prohibitively large search space. For example, the popular beam-search decoding algorithm for phrase-based MT (Koehn, 2004) only explores O(nb) items for a sentence of n words (with a beam width of b), while the full search space is O(2n n2 ) or worse (Knight, 1999). As one of the very few exceptions to the “search-agnostic” majority, Yu et al. (2013) and Zhao et al. (2014) propose a variant of the perceptron algorithm that learns to keep the reference translations in the beam or chart. However, there are several obstacles that prevent their method from becoming popular: First of all, they rely on “forced decoding” to track gold derivations that lead to the reference translation, but in practice only a small portion of (mostly very short) sentence pairs have at least one such derivation. Secondly, they learn the model on the training set, and while this"
D14-1209,P07-2045,0,0.0151024,"and one negative example Φ(d2 ) − Φ(d1 ). In sum, searchaware P RO has |x |times more examples than traditional P RO. The loss functions of P RO and searchaware P RO are defined in Figure 4. 5 Experiments We evaluate our new tuning methods on two large scale NIST translation tasks: Chinese-to-English (C H -E N) and English-to-Chinese (E N -C H) tasks. 5.1 System Preparation and Data We base our experiments on Cubit2 (Huang and Chiang, 2007), a state-of-art phrase-based system in Python. We set phrase-limit to 7, beam size to 30 and distortion limit 6. We use the 11 dense features from Moses (Koehn et al., 2007), which can lead to good performance and are widely used in almost all SMT systems. The baseline tuning methods M ERT (Och, 2003), M IRA (Cherry and Foster, 2012), and P RO (Hopkins and May, 2011) are from the Moses toolkit, which are batch tuning methods based on k-best translations. The searchaware tuning methods are called SA-M ERT, SAM IRA, and SA-P RO, respectively. Their partial B LEU versions are marked with superscript 1 and their potential B LEU versions are marked with superscript 2 , as explained in Section 3. All these search-aware tuning methods are implemented on the basis of Mos"
D14-1209,P13-2003,0,0.194064,"reported are average of three runs, as suggested by Clark et al. (2011) for fairer comparisons. 5.2 Main Results on C H -E N Task Table 2 depicts the main results of our methods on C H -E N translation task. On all five test sets, our methods consistently achieve substantial improvements with two pruning options: SA-M ERT pot gains +1.2 B LEU points over M ERT on average; and SA-M IRApot gains +1.8 B LEU points over M IRA on average as well. SA-P RO pot , however, does not work out of the box when we use the entire nist02 as the tuning set, which might be attributed to the “Monster” behavior (Nakov et al., 2013). To alleviate this problem, we only use the 109 short sentences with less than 10 words from nist02 as our new tuning data. To our supurise, this trick works really well (despite using much less data), and also made SA-P ROpot an order of magnitude faster. This further confirms that our search-aware tuning is consistent across all tuning methods and datasets. As discussed in Section 3, evaluation metrics of partial derivations are crucial for search-aware tuning. Besides the principled “potential B LEU” version of search-aware tuning (i.e. SA-M ERTpot , SA-M IRApot , and SA-P RO pot ), we als"
D14-1209,J03-1002,0,0.0118156,"-E N and E N -C H tasks is the same, and it is collected from the NIST2008 Open Machine Translation Campaign. It consists of about 1.8M sentence pairs, including about 40M/48M words in Chinese/English sides. For C H -E N task, the tuning set is nist02 (878 sents), and test sets are nist03 (919 sents), nist04 (1788 sents), nist05 (1082 sents), nist06 (616 sents from news portion) and nist08 (691 from news portion). For E N -C H task, the tuning set is ssmt07 (995 sents)3 , and the test set is nist08 (1859 sents). For both tasks, all the tuning and test sets contain 4 references. We use GIZA++ (Och and Ney, 2003) for word alignment, and SRILM (Stolcke, 2002) for 4-gram language models with the Kneser-Ney smoothing 3 On E N -C H task, there is only one test set available for us, and thus we use ssmt07 as the tuning set, which is provided at the Third Symposium on Statistical Machine Translation (http://mitlab.hit.edu.cn/ssmt2007.html). option. The LM for E N -C H is trained on its target side; and that for C H -E N is trained on the Xinhua portion of Gigaword. We use B LEU-4 (Papineni et al., 2002) with “average ref-len” to evaluate the translation performance for all experiments. In particular, the ch"
D14-1209,P03-1021,0,0.737286,"ioned tuning methods, and extensive experiments on Chinese-to-English and English-to-Chinese translation show up to +2.6 B LEU gains over search-agnostic baselines. 1 2 3 4 (a) (b) Figure 1: (a) Some potentially promising partial translations (in red) fall out of the beam (bin 2); (b) We identify such partial translations and assign them higher model scores so that they are more likely to survive the search. 1 Introduction Parameter tuning has been a key problem for machine translation since the statistical revolution. However, most existing tuning algorithms treat the decoder as a black box (Och, 2003; Hopkins and May, 2011; Chiang, 2012), ignoring the fact that many potentially promising partial translations are pruned by the decoder due to the prohibitively large search space. For example, the popular beam-search decoding algorithm for phrase-based MT (Koehn, 2004) only explores O(nb) items for a sentence of n words (with a beam width of b), while the full search space is O(2n n2 ) or worse (Knight, 1999). As one of the very few exceptions to the “search-agnostic” majority, Yu et al. (2013) and Zhao et al. (2014) propose a variant of the perceptron algorithm that learns to keep the refer"
D14-1209,P02-1040,0,0.10148,"12 × 2/8 = 3. We call this method “partial B LEU” since it does not complete the translation, and denote it by  δ¯y|x |(d) = −δ y, e(d); reflen = |y |· |c(d)|/|x |. (1) 1943 δ(y, y 0 ) = −Bleu+1 (y, y 0 ) string distance metric δy (d) = δ(y, e(d)) ( |x| δ¯y (d) x δy (d) = δ(y, e¯x (d)) x= full derivations eval reordering e¯x (d) = partial bleu (Sec. 3.1) potential bleu (Sec. 3.2) Table 1: Notations for evaluating full and partial deriva|x| tions. Functions δ¯y (·) and e¯x (·) are defined by Equations 1 and 3, respectively. where reflen is the effective length of reference translations, see (Papineni et al., 2002) for details. 3.1.1 Problem with Partial BLEU Simple as it is, this method does not work well in practice because comparison of partial derivations might be unfair for different derivations covering different set of Chinese words, as it will naturally favor those covering “easier” portions of the input sentence (which we do observe empirically). For instance, consider the following Chinese-toEnglish example which involves a reordering of the Chinese PP: (2) wˇo c´ong Sh`anghˇai f¯ei d`ao Bˇeij¯ıng I from Shanghai fly to Beijing “I flew from Shanghai to Beijing” Partial B LEU will prefer subtra"
D14-1209,P12-1002,0,0.0396503,"ct line search in M IRA and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard trai"
D14-1209,D08-1065,0,0.0705458,"ns are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard training for many beam search based NLP systems (Huang et al., 2012). As a result, Collins and Roark (2004) and Huang et al. (2012) propose the early-update and max-violation update to deal with the search errors. Their idea is to update on prefix or partial hypotheses when the correct solutio"
D14-1209,D07-1080,0,0.552656,"rmally, its objective function is defined as follows: `SA-M ERT(D, w) = M L hx,yi∈D i=1..|x| where top1w (·) is defined in Eq. (4), and δyx (d), defined in Table 1, is the generic metric for evaluating a partial derivation d which has two implementations (partial bleu or potential bleu). In order words we can obtain two implementations of search-aware M ERT methods, SA-M ERT par and SA-M ERTpot . Notice that the traditional M ERT is a special case of SA-M ERT where i is fixed to |x|. 4.2 From M IRA to Search-Aware M IRA M IRA is another popular tuning method for SMT. It firstly introduced in (Watanabe et al., 2007), and then was improved in (Chiang et al., 2008; Chiang, 2012; Cherry and Foster, 2012). Its main idea is to optimize a weight such that the model score difference of a pair of derivations is greater than their loss difference. In this paper, we follow the objective function in (Chiang, 2012; Cherry and Foster, 2012), where only the violation between hope and fear derivations is concerned. Formally, we define d+ (x, y) and d− (x, y) as the hope and fear derivations in the final bin (i.e., complete derivations): d+ (x, y) = argmax w0 · Φ(d) − δy (d) (10) d∈B|x |(x) − d (x, y) = argmax w0 · Φ(d)"
D14-1209,N12-1026,0,0.108006,"dient with inexact line search in M IRA and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can unde"
D14-1209,D13-1112,1,0.94509,"ce the statistical revolution. However, most existing tuning algorithms treat the decoder as a black box (Och, 2003; Hopkins and May, 2011; Chiang, 2012), ignoring the fact that many potentially promising partial translations are pruned by the decoder due to the prohibitively large search space. For example, the popular beam-search decoding algorithm for phrase-based MT (Koehn, 2004) only explores O(nb) items for a sentence of n words (with a beam width of b), while the full search space is O(2n n2 ) or worse (Knight, 1999). As one of the very few exceptions to the “search-agnostic” majority, Yu et al. (2013) and Zhao et al. (2014) propose a variant of the perceptron algorithm that learns to keep the reference translations in the beam or chart. However, there are several obstacles that prevent their method from becoming popular: First of all, they rely on “forced decoding” to track gold derivations that lead to the reference translation, but in practice only a small portion of (mostly very short) sentence pairs have at least one such derivation. Secondly, they learn the model on the training set, and while this does enable a sparse feature set, it is orders of magnitude slower compared to M ERT an"
D14-1209,koen-2004-pharaoh,0,0.704851,"beam (bin 2); (b) We identify such partial translations and assign them higher model scores so that they are more likely to survive the search. 1 Introduction Parameter tuning has been a key problem for machine translation since the statistical revolution. However, most existing tuning algorithms treat the decoder as a black box (Och, 2003; Hopkins and May, 2011; Chiang, 2012), ignoring the fact that many potentially promising partial translations are pruned by the decoder due to the prohibitively large search space. For example, the popular beam-search decoding algorithm for phrase-based MT (Koehn, 2004) only explores O(nb) items for a sentence of n words (with a beam width of b), while the full search space is O(2n n2 ) or worse (Knight, 1999). As one of the very few exceptions to the “search-agnostic” majority, Yu et al. (2013) and Zhao et al. (2014) propose a variant of the perceptron algorithm that learns to keep the reference translations in the beam or chart. However, there are several obstacles that prevent their method from becoming popular: First of all, they rely on “forced decoding” to track gold derivations that lead to the reference translation, but in practice only a small porti"
D14-1209,D13-1093,1,0.846073,"lt on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard training for many beam search based NLP systems (Huang et al., 2012). As a result, Collins and Roark (2004) and Huang et al. (2012) propose the early-update and max-violation update to deal with the search errors. Their idea is to update on prefix or partial hypotheses when the correct solution falls out of the beam. This idea has been successfully used in many NLP tasks and improves the performance over the state-of-art NLP systems (Huang and Sagae, 2010; Huang et al., 2012; Zhang et al., 2013). Goldberg and Nivre (2012) propose the concept of “dynamic oracle” which is the absolute best potential of a partial derivation, and is more akin to a strictly admissible heuristic. This idea inspired and is closely related to our potential B LEU, except that in our case, computing an admissible heuristic is too costly, so our potential B LEU is more like an average potential. Gesmundo and Henderson (2014) also consider the rankings between partial translation pairs as well. However, they evaluate a partial translation through extending it to a complete translation by re-decoding, and thus th"
D14-1209,P09-1019,0,0.17176,"evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard training for many beam search based NLP systems (Huang et al., 2012). As a result, Collins and Roark (2004) and Huang et al. (2012) propose the early-update and max-violation update to deal with the search errors. Their idea is to update on prefix or partial hypotheses when the correct solution falls out of the be"
D14-1209,P14-2127,1,0.832957,"volution. However, most existing tuning algorithms treat the decoder as a black box (Och, 2003; Hopkins and May, 2011; Chiang, 2012), ignoring the fact that many potentially promising partial translations are pruned by the decoder due to the prohibitively large search space. For example, the popular beam-search decoding algorithm for phrase-based MT (Koehn, 2004) only explores O(nb) items for a sentence of n words (with a beam width of b), while the full search space is O(2n n2 ) or worse (Knight, 1999). As one of the very few exceptions to the “search-agnostic” majority, Yu et al. (2013) and Zhao et al. (2014) propose a variant of the perceptron algorithm that learns to keep the reference translations in the beam or chart. However, there are several obstacles that prevent their method from becoming popular: First of all, they rely on “forced decoding” to track gold derivations that lead to the reference translation, but in practice only a small portion of (mostly very short) sentence pairs have at least one such derivation. Secondly, they learn the model on the training set, and while this does enable a sparse feature set, it is orders of magnitude slower compared to M ERT and P RO. We instead prop"
D14-1209,Q13-1033,0,\N,Missing
D14-1209,P10-1110,1,\N,Missing
D15-1149,N12-1047,0,0.0964134,"e-based translation to syntaxbased translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant B LEU improvements on M ERT, M IRA and P RO. 1 Kai Zhao Introduction Efforts in parameter tuning algorithms for SMT, such as M ERT (Och, 2003; Galley et al., 2013), M IRA (Watanabe et al., 2007; Chiang et al., 2009; Cherry and Foster, 2012; Chiang, 2012), and P RO (Hopkins and May, 2011) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding. Recent researche"
D15-1149,N09-1025,0,0.0785243,"Missing"
D15-1149,J07-2003,0,0.511423,"an be significantly improved so that correct intermediate partial translations are more likely to survive the beam. However, the underlying phrase-based decoding model suffers from limited distortion, and thus, may not be flexible enough for translating language pairs that are syntactically different, which require long distance reordering. In order to better handle long distance reordering which beyonds the capability of phrase-based MT, we extend the search-aware tuning framework from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (H IERO) (Chiang, 2007). One key advantage of search-aware tuning for previous phrase-based MT is the minimal change to existing parameter tuning algorithms, which is achieved by defining B LEU-like metrics for the intermediate decoding states with sequencestructured derivations. To keep our approach simple, we generalize these B LEU-like metrics to handle intermediate decoding states with treestructured derivations in H IERO, which are calculated by dynamic programming algorithms inspired by the inside-outside algorithm. We make the following contributions: 1. We extend the framework of search-aware tuning methods"
D15-1149,P11-2031,0,0.0197887,"ing M ERT (Och, 2003), M IRA (Cherry and Foster, 2012) and P RO (Hopkins and May, 2011) from Moses. Another baseline tuning method is hypergraph-M ERT from cdec toolkit (Dyer et al., 2010). To guarantee the tuning efficiency, we constrain the minimum length of spans for searchaware tuning to restrict the number of training instances. For the sentences with less than 20 words, we only use the spans longer than 0.75 times sentence length. For the ones with more than 20 words, the minimum span length is set to 18.7 All results are achieved by averaging three independent runs for fair comparison (Clark et al., 2011). 4.1 Translation Results Table 1 compares the main results of our M ERTbased search-aware tuning with traditional tuning: M ERT and hypergraph-M ERT. From the results, we can see that hypergraphM ERT is better than M ERT by 0.5 B LEU points, verifying the result of (Kumar et al., 2009). For search-aware tuning, partial B LEU (both full and span one) only gets comparable results with baseline tuning method, confirming our previous analysis in section 3.1, and the results are also consistent with (Liu and Huang, 2014). 7 As the decoder demands that spans longer than 20 can only be translated by"
D15-1149,P10-4002,0,0.0222809,"ntences from new portion) data as the test set. Our baseline system is an in-house hierarchical phrase-based system. The translation rules are extracted with Moses toolkit (Koehn et al., 2007) by default settings. For the decoder, we set the beam size to 30, nbest list to 50, and 20 as the maximum length of spans for using non-glue rules. The baseline tuning methods are batch tuning methods based on k-best translations, including M ERT (Och, 2003), M IRA (Cherry and Foster, 2012) and P RO (Hopkins and May, 2011) from Moses. Another baseline tuning method is hypergraph-M ERT from cdec toolkit (Dyer et al., 2010). To guarantee the tuning efficiency, we constrain the minimum length of spans for searchaware tuning to restrict the number of training instances. For the sentences with less than 20 words, we only use the spans longer than 0.75 times sentence length. For the ones with more than 20 words, the minimum span length is set to 18.7 All results are achieved by averaging three independent runs for fair comparison (Clark et al., 2011). 4.1 Translation Results Table 1 compares the main results of our M ERTbased search-aware tuning with traditional tuning: M ERT and hypergraph-M ERT. From the results,"
D15-1149,D13-1201,0,0.0161969,"to survive the inexact decoding beam. We extend this approach from phrase-based translation to syntaxbased translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant B LEU improvements on M ERT, M IRA and P RO. 1 Kai Zhao Introduction Efforts in parameter tuning algorithms for SMT, such as M ERT (Och, 2003; Galley et al., 2013), M IRA (Watanabe et al., 2007; Chiang et al., 2009; Cherry and Foster, 2012; Chiang, 2012), and P RO (Hopkins and May, 2011) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial transl"
D15-1149,D13-1111,0,0.0173227,"ing set and nist04 test set, guided backtrace method gains at lease 1.0 nist02 0.3960 0.4098 nist04 0.3791 0.3932 Table 3: The diversity comparison based on k-best list in the final beam on both tuning set (nist02) and nist04 test set. The higher the value is, the more diverse the k-best list is. B LEU points improvements over traditional M ERT on both k-best oracle and guided oracle. Moreover, k-best and guided oracle get more improvements than 1-best, indicating that by search-aware tuning, the decoder could generate a better k-best list, and has a higher upper bound. Diversity As shown in (Gimpel et al., 2013; Liu and Huang, 2014), diversity is important for MT tuning. An k-best list with higher diversity can better represent the entire decoding space, and thus tuning on it will lead to a better performance. Similar as (Liu and Huang, 2014), our method encourages the diversity of partial translations in each beam, including the ones in the final beam. We use the metric in (Liu and Huang, 2014) to compare the diversity of traditional M ERT and guided backtrace. The metric is based on the ngram matches between two sentences y and y 0 : ∆(y, y 0 ) = − |y|−q |y 0 |−q X X i=1 d(y, y 0 ) = 1 − 0 hyi:i+q"
D15-1149,D11-1125,0,0.135382,"generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant B LEU improvements on M ERT, M IRA and P RO. 1 Kai Zhao Introduction Efforts in parameter tuning algorithms for SMT, such as M ERT (Och, 2003; Galley et al., 2013), M IRA (Watanabe et al., 2007; Chiang et al., 2009; Cherry and Foster, 2012; Chiang, 2012), and P RO (Hopkins and May, 2011) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding. Recent researches reveal that the parameter tuning algorithms tai"
D15-1149,N12-1015,1,0.830303,"e the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding. Recent researches reveal that the parameter tuning algorithms tailored for specific decoding algorithms can be beneficial in general structured prediction problems (Huang et al., 2012), and in machine translation (Yu et al., 2013; Zhao et al., 2014; Liu and Huang, 2014). Particularly, Liu and Huang (2014) show that by requiring the conventional parameter tuning algorithms to consider the final decoding results (full translations) as well as the intermediate decoding states (partial translations) at the same time, the inexact decoding can be significantly improved so that correct intermediate partial translations are more likely to survive the beam. However, the underlying phrase-based decoding model suffers from limited distortion, and thus, may not be flexible enough for t"
D15-1149,P07-2045,0,0.00605761,"strategy. We train a 4-gram language model on the Xinhua portion of English Gigaword corpus by SRILM toolkit (Stolcke, 2002). We use B LEU 4 with “average reference length” to evaluate the translation performance for all experiments. We use the NIST MT 2002 evaluation data (878 sentences) as the tuning set, and adopt NIST MT04 (1,789 sentences), MT05 (1,082 sentences), MT06 (616 sentences from new portion) and MT08 (691 sentences from new portion) data as the test set. Our baseline system is an in-house hierarchical phrase-based system. The translation rules are extracted with Moses toolkit (Koehn et al., 2007) by default settings. For the decoder, we set the beam size to 30, nbest list to 50, and 20 as the maximum length of spans for using non-glue rules. The baseline tuning methods are batch tuning methods based on k-best translations, including M ERT (Och, 2003), M IRA (Cherry and Foster, 2012) and P RO (Hopkins and May, 2011) from Moses. Another baseline tuning method is hypergraph-M ERT from cdec toolkit (Dyer et al., 2010). To guarantee the tuning efficiency, we constrain the minimum length of spans for searchaware tuning to restrict the number of training instances. For the sentences with les"
D15-1149,P09-1019,0,0.0229925,"ng to restrict the number of training instances. For the sentences with less than 20 words, we only use the spans longer than 0.75 times sentence length. For the ones with more than 20 words, the minimum span length is set to 18.7 All results are achieved by averaging three independent runs for fair comparison (Clark et al., 2011). 4.1 Translation Results Table 1 compares the main results of our M ERTbased search-aware tuning with traditional tuning: M ERT and hypergraph-M ERT. From the results, we can see that hypergraphM ERT is better than M ERT by 0.5 B LEU points, verifying the result of (Kumar et al., 2009). For search-aware tuning, partial B LEU (both full and span one) only gets comparable results with baseline tuning method, confirming our previous analysis in section 3.1, and the results are also consistent with (Liu and Huang, 2014). 7 As the decoder demands that spans longer than 20 can only be translated by glue rule, for these spans, we only need to consider the ones beginning with 0 in search-aware tuning. 1287 M ERT M ERT-S hypergraph-M ERT full partial B LEU span partial B LEU concatenation top-down guided backtrace nist03 34.4 34.3 34.5 34.3 34.1 34.3 34.5 34.9 nist04 35.9 36.1 36.4"
D15-1149,P13-2003,0,0.0280311,"Missing"
D15-1149,P03-1021,0,0.1548,"more likely to survive the inexact decoding beam. We extend this approach from phrase-based translation to syntaxbased translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant B LEU improvements on M ERT, M IRA and P RO. 1 Kai Zhao Introduction Efforts in parameter tuning algorithms for SMT, such as M ERT (Och, 2003; Galley et al., 2013), M IRA (Watanabe et al., 2007; Chiang et al., 2009; Cherry and Foster, 2012; Chiang, 2012), and P RO (Hopkins and May, 2011) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that pr"
D15-1149,D07-1080,0,0.0271556,"ing beam. We extend this approach from phrase-based translation to syntaxbased translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant B LEU improvements on M ERT, M IRA and P RO. 1 Kai Zhao Introduction Efforts in parameter tuning algorithms for SMT, such as M ERT (Och, 2003; Galley et al., 2013), M IRA (Watanabe et al., 2007; Chiang et al., 2009; Cherry and Foster, 2012; Chiang, 2012), and P RO (Hopkins and May, 2011) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due"
D15-1149,D13-1112,1,0.856231,"oder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding. Recent researches reveal that the parameter tuning algorithms tailored for specific decoding algorithms can be beneficial in general structured prediction problems (Huang et al., 2012), and in machine translation (Yu et al., 2013; Zhao et al., 2014; Liu and Huang, 2014). Particularly, Liu and Huang (2014) show that by requiring the conventional parameter tuning algorithms to consider the final decoding results (full translations) as well as the intermediate decoding states (partial translations) at the same time, the inexact decoding can be significantly improved so that correct intermediate partial translations are more likely to survive the beam. However, the underlying phrase-based decoding model suffers from limited distortion, and thus, may not be flexible enough for translating language pairs that are syntactica"
D15-1149,P14-2127,1,0.842077,"ox. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding. Recent researches reveal that the parameter tuning algorithms tailored for specific decoding algorithms can be beneficial in general structured prediction problems (Huang et al., 2012), and in machine translation (Yu et al., 2013; Zhao et al., 2014; Liu and Huang, 2014). Particularly, Liu and Huang (2014) show that by requiring the conventional parameter tuning algorithms to consider the final decoding results (full translations) as well as the intermediate decoding states (partial translations) at the same time, the inexact decoding can be significantly improved so that correct intermediate partial translations are more likely to survive the beam. However, the underlying phrase-based decoding model suffers from limited distortion, and thus, may not be flexible enough for translating language pairs that are syntactically different, whic"
D15-1149,D14-1209,1,\N,Missing
D16-1001,P16-1231,0,0.0710369,"of any parser that does not use reranking or external data. 1 Introduction Parsing is an important problem in natural language processing which has been studied extensively for decades. Between the two basic paradigms of parsing, constituency parsing, the subject of this paper, has in general proved to be the more difficult than dependency parsing, both in terms of accuracy and the run time of parsing algorithms. There has recently been a huge surge of interest in using neural networks to make parsing decisions, and such models continue to dominate the state of the art in dependency parsing (Andor et al., 2016). In constituency parsing, however, neural approaches are still behind the state-of-the-art (Carreras et al., 2008; Shindo et al., 2012; Thang et al., 2015); see more details in Section 5. To remedy this, we design a new parsing framework that is more suitable for constituency parsing, and that can be accurately modeled by neural networks. Observing that constituency parsing is primarily focused on sentence spans (rather than individual words, as is dependency parsing), we propose a novel adaptation of the shift-reduce system which reflects this focus. In this system, the stack consists of sen"
D16-1001,D15-1041,0,0.0204561,"from interactions among their sequential inputs even when separated by a long distance, and thus are a natural fit for analyz3 ing natural language. LSTM models have proved to be a powerful tool for many learning tasks in natural language, such as language modeling (Sundermeyer et al., 2012) and translation (Sutskever et al., 2014). LSTMs have also been incorporated into parsing in a variety of ways, such as directly encoding an entire sentence (Vinyals et al., 2015), separately modeling the stack, buffer, and action history (Dyer et al., 2015), to encode words based on their character forms (Ballesteros et al., 2015), and as an element in a recursive structure to combine dependency subtrees with their left and right children (Kiperwasser and Goldberg, 2016a). For our parsing system, however, we need a way to model arbitrary sentence spans in the context of the rest of the sentence. We do this by representing each sentence span as the elementwise difference of the vector outputs of the LSTM outputs at different 1 time steps, which correspond to word boundaries. 1 1 If the sequential output of the recurrent network for 1 1 1 the sentence is f0 , ..., fn in the forward direction and 1 1 bn , ..., b0 in the b"
D16-1001,D16-1211,0,0.0724355,"es for a given sentence before updating, is “training with exploration” as proposed by Goldberg and Nivre (2013). This involves parsing each sentence according to the current model and using the oracle to determine correct actions for training. We saw very little improvement on the Penn treebank validation set using this method, however. Based on the parsing accuracy on the training sentences, this appears to be due to the model overfitting the training data early during training, thus negating the benefit of training on erroneous paths. Accordingly, we also used a method recently proposed by Ballesteros et al. (2016), which specifically addresses this problem. This method introduces stochasticity into the training data parses by randomly taking actions according to the softmax distribution over action scores. This introduces realistic mistakes into the training parses, which we found was also very effective in our case, leading to higher F1 scores, though it noticeably sacrifices 8 recall in favor of precision. This technique can also take a parameter α to flatten or sharpen the raw softmax distribution. The results on the Penn treebank development set for various values of α are presented in Table 3. We"
D16-1001,W14-6110,0,0.101936,"Missing"
D16-1001,W08-2102,0,0.0762639,"atural language processing which has been studied extensively for decades. Between the two basic paradigms of parsing, constituency parsing, the subject of this paper, has in general proved to be the more difficult than dependency parsing, both in terms of accuracy and the run time of parsing algorithms. There has recently been a huge surge of interest in using neural networks to make parsing decisions, and such models continue to dominate the state of the art in dependency parsing (Andor et al., 2016). In constituency parsing, however, neural approaches are still behind the state-of-the-art (Carreras et al., 2008; Shindo et al., 2012; Thang et al., 2015); see more details in Section 5. To remedy this, we design a new parsing framework that is more suitable for constituency parsing, and that can be accurately modeled by neural networks. Observing that constituency parsing is primarily focused on sentence spans (rather than individual words, as is dependency parsing), we propose a novel adaptation of the shift-reduce system which reflects this focus. In this system, the stack consists of sentence spans rather than partial trees. It is also factored into two types of parser actions, structural and label"
D16-1001,D14-1082,0,0.140381,"e embeddings are randomly initialized and learned from scratch together with all other network weights, and we would expect further performance improvement from incorporating embeddings pretrained from a large external corpus. The network structure after the the span features consists of a separate multilayer perceptron for each type of action (structural and label). For each action we use a single hidden layer with rectified linear (ReLU) activation. The model is trained on a peraction basis using a single correct action for each parser state, with a negative log softmax loss function, as in Chen and Manning (2014). 4 Dynamic Oracle The baseline method of training our parser is what is known as a static oracle: we simply generate the sequence of actions to correctly parse each training sentence, using a short-stack heuristic (i.e., combine first whenever there is a choice of shift and combine). This method suffers from a well-documeted problem, however, namely that it only “prepares” the model for the situation where no mistakes have been made during parsing, an inevitably incorrect assumption in practice. To alleviate this problem, Goldberg and Nivre (2013) define a dynamic oracle to return the best po"
D16-1001,P16-1017,0,0.621014,"Missing"
D16-1001,P16-2006,1,0.890038,"ures throughout the parse. Intuitively, this allows the span differences to learn to represent the sentence spans in the context of the rest of the sentence, not in isolation (especially true for LSTM given the extra hidden recurrent connection, typically described as a “memory cell”). In practice, we use a two-layer bi-directional LSTM, where the input to the second layer combines the forward and backward outputs from the first layer at that time step. For each direction, the components from the first and second layers are concatenated to form the vectors which go into the span features. See Cross and Huang (2016) for more details on this approach. For the particular case of our transition constituency parser, we use only four span features to determine a structural action, and three to determine a label action, in each case partitioning the sentence exactly. The reason for this is straightforward: when considering a structural action, the top two spans on the stack must be considered to determine whether they should be combined, while for a label action, only the top span on the stack is important, since that is the candidate for labeling. In both cases the remaining sentence prefix and suffix are als"
D16-1001,P15-1030,0,0.135351,"an the super-linear time oracle for arc-standard dependency parsing in Goldberg et al. (2014). 5 Related Work Neural networks have been used for constituency parsing in a number of previous instances. For example, Socher et al. (2013) learn a recursive network that combines vectors representing partial trees, Vinyals et al. (2015) adapt a sequence-tosequence model to produce parse trees, Watanabe and Sumita (2015) use a recursive model applying a shift-reduce system to constituency parsing with 7 beam search, and Dyer et al. (2016) adapt the StackLSTM dependency parsing approach to this task. Durrett and Klein (2015) combine both neural and sparse features for a CKY parsing system. Our own previous work (Cross and Huang, 2016) use a recurrent sentence representation in a head-driven transition system which allows for greedy parsing but does not achieve state-of-the-art results. The concept of “oracles” for constituency parsing (as the tree that is most similar to tG among all possible trees) was first defined and solved by Huang (2008) in bottom-up parsing. In transition-based parsing, the dynamic oracle for shift-reduce dependency parsing costs worst-case O(n3 ) time (Goldberg et al., 2014). On the other"
D16-1001,P15-1033,0,0.00879835,"ive for modeling sequences. They are able to capture and generalize from interactions among their sequential inputs even when separated by a long distance, and thus are a natural fit for analyz3 ing natural language. LSTM models have proved to be a powerful tool for many learning tasks in natural language, such as language modeling (Sundermeyer et al., 2012) and translation (Sutskever et al., 2014). LSTMs have also been incorporated into parsing in a variety of ways, such as directly encoding an entire sentence (Vinyals et al., 2015), separately modeling the stack, buffer, and action history (Dyer et al., 2015), to encode words based on their character forms (Ballesteros et al., 2015), and as an element in a recursive structure to combine dependency subtrees with their left and right children (Kiperwasser and Goldberg, 2016a). For our parsing system, however, we need a way to model arbitrary sentence spans in the context of the rest of the sentence. We do this by representing each sentence span as the elementwise difference of the vector outputs of the LSTM outputs at different 1 time steps, which correspond to word boundaries. 1 1 If the sequential output of the recurrent network for 1 1 1 the sent"
D16-1001,N16-1024,0,0.0904647,"r-step cost is amortized constant time. Thus our dynamic oracle is much faster than the super-linear time oracle for arc-standard dependency parsing in Goldberg et al. (2014). 5 Related Work Neural networks have been used for constituency parsing in a number of previous instances. For example, Socher et al. (2013) learn a recursive network that combines vectors representing partial trees, Vinyals et al. (2015) adapt a sequence-tosequence model to produce parse trees, Watanabe and Sumita (2015) use a recursive model applying a shift-reduce system to constituency parsing with 7 beam search, and Dyer et al. (2016) adapt the StackLSTM dependency parsing approach to this task. Durrett and Klein (2015) combine both neural and sparse features for a CKY parsing system. Our own previous work (Cross and Huang, 2016) use a recurrent sentence representation in a head-driven transition system which allows for greedy parsing but does not achieve state-of-the-art results. The concept of “oracles” for constituency parsing (as the tree that is most similar to tG among all possible trees) was first defined and solved by Huang (2008) in bottom-up parsing. In transition-based parsing, the dynamic oracle for shift-reduc"
D16-1001,Q13-1033,0,0.0130499,"ith a negative log softmax loss function, as in Chen and Manning (2014). 4 Dynamic Oracle The baseline method of training our parser is what is known as a static oracle: we simply generate the sequence of actions to correctly parse each training sentence, using a short-stack heuristic (i.e., combine first whenever there is a choice of shift and combine). This method suffers from a well-documeted problem, however, namely that it only “prepares” the model for the situation where no mistakes have been made during parsing, an inevitably incorrect assumption in practice. To alleviate this problem, Goldberg and Nivre (2013) define a dynamic oracle to return the best possible action(s) at any arbitrary configuration. In this section, we introduce an easy-to-compute optimal dynamic oracle for our constituency parser. We will first define some concepts upon which the dynamic oracle is built and then show how 1 optimal 1 actions can be very efficiently computed using this 1 framework. In broad strokes, in any arbitrary parser configuration c there is a set of brackets t∗ (c) from the gold tree which it is still possible to reach. By following dynamic oracle actions, all of those brackets and only those brackets will"
D16-1001,Q14-1010,0,0.0482304,"ser configuration c there is a set of brackets t∗ (c) from the gold tree which it is still possible to reach. By following dynamic oracle actions, all of those brackets and only those brackets will be predicted. Even though proving the optimality of our dynamic oracle (Sec. 4.3) is involved, computing the oracle actions is extremely simple (Secs. 4.2) and efficient (Sec. 4.4). 4.1 0 S5 1 VP5 Preliminaries and Notations Before describing the computation of our dynamic oracle, we first need to rigorously establish the desired optimality of dynamic oracle. The structure of this framework follows Goldberg et al. (2014). 3 S/VP5 4 NP5 Some text and0 the textsymbol and text the andsymbol the symbol 0 1or scaled 2or scaled 0SomeSome Definition 1. We denote c `τ c iff. c is the result of action τ on configuration c, also denoted functionally as c0 = τ (c). We denote ` to be the union of `τ for all actions τ , and `∗ to be the reflexive and transitive closure of `. Definition 2 (descendant/reachable trees). We denote D(c) to be the set of final descendant trees derivable from c, i.e., D(c) = {t |c `∗ hz, σ, ti}. This set is also called “reachable trees” from c. Definition 3 (F1 ). We define the standard F1 metri"
D16-1001,P04-1013,0,0.0281941,"Missing"
D16-1001,P10-1110,1,0.37444,"Missing"
D16-1001,P08-1067,1,0.401777,"ence spans. Our neural model processes the sentence once for each parse with a recurrent network. We represent parser configurations with a very small number of span features (4 for structural actions and 3 for label actions). Extending Wang and Chang (2016), each span is represented as the difference of recurrent output from multiple layers in each direction. No pretrained embeddings are required. We also extend the idea of dynamic oracles from dependency to constituency parsing. The latter is significantly more difficult than the former due to F1 being a combination of precision and recall (Huang, 2008), and yet we propose a simple and extremely efficient oracle (amortized O(1) time). This oracle is proved optimal for F1 as well as both of its components, precision and recall. Trained with this oracle, our parser achieves what we believe to be the best results for any parser without reranking which was trained only on the Penn Treebank and the French Treebank, despite the fact that it is not only lineartime, but also strictly greedy. We make the following main contributions: • A novel factored transition parsing system where the stack elements are sentence spans rather than partial trees (Se"
D16-1001,Q16-1032,0,0.0153596,"l language. LSTM models have proved to be a powerful tool for many learning tasks in natural language, such as language modeling (Sundermeyer et al., 2012) and translation (Sutskever et al., 2014). LSTMs have also been incorporated into parsing in a variety of ways, such as directly encoding an entire sentence (Vinyals et al., 2015), separately modeling the stack, buffer, and action history (Dyer et al., 2015), to encode words based on their character forms (Ballesteros et al., 2015), and as an element in a recursive structure to combine dependency subtrees with their left and right children (Kiperwasser and Goldberg, 2016a). For our parsing system, however, we need a way to model arbitrary sentence spans in the context of the rest of the sentence. We do this by representing each sentence span as the elementwise difference of the vector outputs of the LSTM outputs at different 1 time steps, which correspond to word boundaries. 1 1 If the sequential output of the recurrent network for 1 1 1 the sentence is f0 , ..., fn in the forward direction and 1 1 bn , ..., b0 in the backward1 direction then the span 1 1 (i, j) would be represented as1 the1 concatenation of 1 1 the vector differences (fj − f ) and (b − b )."
D16-1001,Q16-1023,0,0.0229275,"l language. LSTM models have proved to be a powerful tool for many learning tasks in natural language, such as language modeling (Sundermeyer et al., 2012) and translation (Sutskever et al., 2014). LSTMs have also been incorporated into parsing in a variety of ways, such as directly encoding an entire sentence (Vinyals et al., 2015), separately modeling the stack, buffer, and action history (Dyer et al., 2015), to encode words based on their character forms (Ballesteros et al., 2015), and as an element in a recursive structure to combine dependency subtrees with their left and right children (Kiperwasser and Goldberg, 2016a). For our parsing system, however, we need a way to model arbitrary sentence spans in the context of the rest of the sentence. We do this by representing each sentence span as the elementwise difference of the vector outputs of the LSTM outputs at different 1 time steps, which correspond to word boundaries. 1 1 If the sequential output of the recurrent network for 1 1 1 the sentence is f0 , ..., fn in the forward direction and 1 1 bn , ..., b0 in the backward1 direction then the span 1 1 (i, j) would be represented as1 the1 concatenation of 1 1 the vector differences (fj − f ) and (b − b )."
D16-1001,J93-2004,0,0.0642099,"Missing"
D16-1001,P06-1043,0,0.0783714,"Missing"
D16-1001,N15-1108,1,0.400337,"Missing"
D16-1001,N07-1051,0,0.034125,"Missing"
D16-1001,P06-2089,0,0.171947,"Missing"
D16-1001,W14-6111,0,0.0376164,"Missing"
D16-1001,P12-1046,0,0.136121,"ing which has been studied extensively for decades. Between the two basic paradigms of parsing, constituency parsing, the subject of this paper, has in general proved to be the more difficult than dependency parsing, both in terms of accuracy and the run time of parsing algorithms. There has recently been a huge surge of interest in using neural networks to make parsing decisions, and such models continue to dominate the state of the art in dependency parsing (Andor et al., 2016). In constituency parsing, however, neural approaches are still behind the state-of-the-art (Carreras et al., 2008; Shindo et al., 2012; Thang et al., 2015); see more details in Section 5. To remedy this, we design a new parsing framework that is more suitable for constituency parsing, and that can be accurately modeled by neural networks. Observing that constituency parsing is primarily focused on sentence spans (rather than individual words, as is dependency parsing), we propose a novel adaptation of the shift-reduce system which reflects this focus. In this system, the stack consists of sentence spans rather than partial trees. It is also factored into two types of parser actions, structural and label actions, which altern"
D16-1001,P13-1045,0,0.0446351,"g crossed by a structural action or mislabeled by a label action, needs to be updated. This update is simply tracing the parent link to the next smallest gold bracket repeatedly until the new bracket encompasses span (i, j). Since there are at most O(n) choices of next(c) and there are O(n) steps, the per-step cost is amortized constant time. Thus our dynamic oracle is much faster than the super-linear time oracle for arc-standard dependency parsing in Goldberg et al. (2014). 5 Related Work Neural networks have been used for constituency parsing in a number of previous instances. For example, Socher et al. (2013) learn a recursive network that combines vectors representing partial trees, Vinyals et al. (2015) adapt a sequence-tosequence model to produce parse trees, Watanabe and Sumita (2015) use a recursive model applying a shift-reduce system to constituency parsing with 7 beam search, and Dyer et al. (2016) adapt the StackLSTM dependency parsing approach to this task. Durrett and Klein (2015) combine both neural and sparse features for a CKY parsing system. Our own previous work (Cross and Huang, 2016) use a recurrent sentence representation in a head-driven transition system which allows for greed"
D16-1001,P15-1148,0,0.0542908,"udied extensively for decades. Between the two basic paradigms of parsing, constituency parsing, the subject of this paper, has in general proved to be the more difficult than dependency parsing, both in terms of accuracy and the run time of parsing algorithms. There has recently been a huge surge of interest in using neural networks to make parsing decisions, and such models continue to dominate the state of the art in dependency parsing (Andor et al., 2016). In constituency parsing, however, neural approaches are still behind the state-of-the-art (Carreras et al., 2008; Shindo et al., 2012; Thang et al., 2015); see more details in Section 5. To remedy this, we design a new parsing framework that is more suitable for constituency parsing, and that can be accurately modeled by neural networks. Observing that constituency parsing is primarily focused on sentence spans (rather than individual words, as is dependency parsing), we propose a novel adaptation of the shift-reduce system which reflects this focus. In this system, the stack consists of sentence spans rather than partial trees. It is also factored into two types of parser actions, structural and label actions, which alternate during a parse. T"
D16-1001,P16-1218,0,0.230609,"tack consists of sentence spans rather than partial trees. It is also factored into two types of parser actions, structural and label actions, which alternate during a parse. The structural actions are a simplified analogue of shift-reduce actions, omitting the directionality of reduce actions, while the label actions directly assign nonterminal symbols to sentence spans. Our neural model processes the sentence once for each parse with a recurrent network. We represent parser configurations with a very small number of span features (4 for structural actions and 3 for label actions). Extending Wang and Chang (2016), each span is represented as the difference of recurrent output from multiple layers in each direction. No pretrained embeddings are required. We also extend the idea of dynamic oracles from dependency to constituency parsing. The latter is significantly more difficult than the former due to F1 being a combination of precision and recall (Huang, 2008), and yet we propose a simple and extremely efficient oracle (amortized O(1) time). This oracle is proved optimal for F1 as well as both of its components, precision and recall. Trained with this oracle, our parser achieves what we believe to be"
D16-1001,P15-1113,0,0.0845003,"until the new bracket encompasses span (i, j). Since there are at most O(n) choices of next(c) and there are O(n) steps, the per-step cost is amortized constant time. Thus our dynamic oracle is much faster than the super-linear time oracle for arc-standard dependency parsing in Goldberg et al. (2014). 5 Related Work Neural networks have been used for constituency parsing in a number of previous instances. For example, Socher et al. (2013) learn a recursive network that combines vectors representing partial trees, Vinyals et al. (2015) adapt a sequence-tosequence model to produce parse trees, Watanabe and Sumita (2015) use a recursive model applying a shift-reduce system to constituency parsing with 7 beam search, and Dyer et al. (2016) adapt the StackLSTM dependency parsing approach to this task. Durrett and Klein (2015) combine both neural and sparse features for a CKY parsing system. Our own previous work (Cross and Huang, 2016) use a recurrent sentence representation in a head-driven transition system which allows for greedy parsing but does not achieve state-of-the-art results. The concept of “oracles” for constituency parsing (as the tree that is most similar to tG among all possible trees) was first"
D16-1001,P15-1032,0,0.0182652,"compare our parser with other recent work in Table 5. We achieve state-of-the-art results even in comparison to Bj¨orkelund et al. (2014), which utilized both external data and reranking in achieving the best results in the SPMRL 2014 shared task. 9 6.4 Notes on Experiments For these experiments, we performed very little hyperparameter tuning, due to time and resource contraints. We have every reason to believe that performance could be improved still further with such techniques as random restarts, larger hidden layers, external embeddings, and hyperparameter grid search, as demonstrated by Weiss et al. (2015). We also note that while our parser is very accurate even with greedy decoding, the model is easily adaptable for beam search, particularly since the parsing system already uses a fixed number of actions. Beam search could also be made considerably more efficient by caching post-hidden-layer feature components for sentence spans, essentially using the precomputation trick described by Chen and Manning (2014), but on a per-sentence basis. 7 Conclusion and Future Work We have developed a new transition-based constituency parser which is built around sentence spans. It uses a factored system alt"
D16-1001,P13-1043,0,0.0606106,"Missing"
D17-1002,P15-1033,0,0.167904,"ity Lillian Lee Cornell University tianze@cs.cornell.edu liang.huang.sh@gmail.com llee@cs.cornell.edu Abstract rithms do exist, having been introduced by Huang and Sagae (2010) and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6 q (originally reported as Opn7 q) for a length-n sentence. As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well. Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).1 Inspired by thei"
D17-1002,P16-1231,0,0.288465,"abeled attachment score reported (to our knowledge) on the Chinese Treebank and the “second-best-in-class” result on the English Penn Treebank. 1 Introduction It used to be the case that the most accurate dependency parsers made global decisions and employed exact decoding. But transition-based dependency parsers (TBDPs) have recently achieved state-of-the-art performance, despite the fact that for efficiency reasons, they are usually trained to make local, rather than global, decisions and the decoding process is done approximately, rather than exactly (Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016). The key efficiency issue for decoding is as follows. In order to make accurate (local) attachment decisions, historically, TBDPs have required a large set of features in order to access rich information about particular positions in the stack and buffer of the current parser configuration. But consulting many positions means that although polynomial-time exact-decoding algo1 We note that K&G were not focused on minimizing positions, although they explicitly noted the implications of doing so: “While not explored in this work, [fewer positions] results in very compact state signatures, [which"
D17-1002,C96-1058,0,0.279625,"o deploy them. Interestingly, for both the transition- and graphbased paradigms, the optimal algorithms build dependency trees bottom-up from local structures. It is thus natural to wonder if there are deeper, more formal connections between the two. In previous work, Kuhlmann et al. (2011) related the arc-standard system to the classic CKY algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) in a manner clearly suggested by Figure 1a; CKY can be viewed as a very simple graph-based approach. G´omez-Rodr´ıguez et al. (2008, 2011) formally prove that sequences of steps in the edgefactored GBDP (Eisner, 1996) can be used to emulate any individual step in the arc-hybrid system (Yamada and Matsumoto, 2003) and the Eisner and Satta (1999, Figure 1d) version. However, they did not draw an explicitly direct connection between Eisner and Satta (1999) and TBDPs. Here, we provide an update to these previous findings, stated in terms of the expressiveness of scoring functions, considered as parameterization. For the edge-factored GBDP, we write the score ÑÐ ÑÐ for an edge as fG p h, mq, where h is the head and m the modifier. A tree’s score is the sum of its edge scores. We say that a parameterized depende"
D17-1002,D16-1211,0,0.0597856,"Missing"
D17-1002,P99-1059,0,0.347613,") or not (b “ 0) after the transition sequence is applied. Kuhlmann et al. (2011) show that all three deduction systems can be directly “tabularized” and dynamic programming (DP) can be applied, such that, ignoring for the moment the issue of incorporating complex features (we return to this later), time and space needs are low-order polynomial. Specifically, as the two-index shorthand ri, js suggests, arc-eager and arc-hybrid systems can be implemented to take Opn2 q space and Opn3 q time; the arc-standard system requires Opn3 q space and Opn4 q time (if one applies the so-called hook trick (Eisner and Satta, 1999)). Since an Opn4 q running time is not sufficiently practical even in the simple-feature case, in the remainder of this paper we consider only the archybrid and arc-eager systems, not arc-standard. the collection of left modifiers before right modifiers via its b0 -modifier reð transition. This contrasts with arc-standard, where the attachment of left and right modifiers can be interleaved on the stack. shrpσ, b0 |β, Aqs “ pσ|b0 , β, Aq reñ rpσ|s1 |s0 , β, Aqs “ pσ|s1 , β, A Y tps1 , s0 quq reð rpσ|s0 , b0 |β, Aqs “ pσ, b0 |β, A Y tpb0 , s0 quq Arc-Eager In contrast to the former two systems,"
D17-1002,J13-1002,0,0.0275636,"rently see a concrete way to test the following hypothesis. ÑÐ ÑÐ With t s 0 , b 0 u, in the arc-standard case, situations can arise where there are multiple possible transitions with missing information. In contrast, in the arc-hybrid case, there is only one possible transition with missing information (namely, ÑÐ reñ , introduced in §3.1); perhaps s 1 is therefore not so crucial for arc-hybrid in practice? 4 For simplicity, we only present unlabeled parsing here. See Shi et al. (2017) for labeled-parsing results. 5 Other presentations place ROOT at the end of the buffer or omit it entirely (Ballesteros and Nivre, 2013). 14 assertion rib , js is an indicator variable for whether wi has been attached to its head (b “ 1) or not (b “ 0) after the transition sequence is applied. Kuhlmann et al. (2011) show that all three deduction systems can be directly “tabularized” and dynamic programming (DP) can be applied, such that, ignoring for the moment the issue of incorporating complex features (we return to this later), time and space needs are low-order polynomial. Specifically, as the two-index shorthand ri, js suggests, arc-eager and arc-hybrid systems can be implemented to take Opn2 q space and Opn3 q time; the"
D17-1002,D07-1101,0,0.0281178,"rceptron (MLP) with 1 hidden layer which has 256 hidden units and activation function tanh. We set the dropout rate for the bi-LSTM (Gal and Ghahramani, 2016) and MLP (Srivastava et al., 2014) for each model according to development-set performance.6 All parameters except the word embedThe extra expressiveness of the arc-eager model comes from the scoring functions fsh and fre that capture structural contexts other than headmodifier relations. Unlike traditional higher-order graph-based parsing that directly models relations such as siblinghood (McDonald and Pereira, 2006) or grandparenthood (Carreras, 2007), however, the arguments in those two functions do not have any fixed type of structural interactions. 6 Experiments Data and Evaluation We experimented with English and Chinese. For English, we used the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (via the Stanford parser 3.3.0) of the Penn Treebank (Marcus et al., 1993, PTB). As is standard, we used §2-21 of the Wall Street Journal for training, §22 for development, 6 For bi-LSTM input and recurrent connections, we consider dropout rates in t0., 0.2u, and for MLP, t0., 0.4u. 18 90.5 ?15 Our all global 90.0 ?5 Our arc-eage"
D17-1002,D14-1082,0,0.753131,"data structures in their configurations: (1) a stack of partially parsed subtrees and (2) a buffer (mostly) of unprocessed sentence tokens. To featurize configurations for use in a scoring function, it is common to have features that extract information about the first several elements on the stack and the buffer, such as their word forms and part-of-speech (POS) tags. We refer to these as positional features, as each feature relates to a particular position in the stack or buffer. Typically, millions of sparse indicator features (often developed via manual engineering) are used. In contrast, Chen and Manning (2014) introduce a feature set consisting of dense word-, POS-, and dependency-label embeddings. While dense, these features are for the same 18 positions that have been typically used in prior work. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) adopt bi-directional LSTMs, which have nice expressiveness and context-sensitivity properties, to reduce the number of positions considered down to four and three, for different transition systems, respectively. This naturally begs the question, what is the lower limit on the number of positional features necessary for a parser to pe"
D17-1002,C12-1059,0,0.0220663,"ng Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating from the gold transition path during training to better simulate the test-time environment. There are many directions for further exploration. Two possibilities are to create even better training methods, and to find some way to extend our run-time improvements to other transition systems. It would also be interesting to further investigate relationships between graph-based an"
D17-1002,D16-1238,0,0.0484593,"Missing"
D17-1002,Q13-1033,0,0.0161886,"hen, for a given parser configuration, stack positions ÑÐ ÑÐ are represented by s j , defined as wipsj q where ipsj q gives the position in the sentence of the token that is the head of the tree in sj . Similarly, ÑÐ buffer positions are represented by b j , defined as ÑÐ wipbj q for the token at buffer position j. Finally, as in Chen and Manning (2014), we use a multilayer perceptron to score possible transitions from the given configuration, where the input is the conÑÐ ÑÐ catenation of some selection of the s j s and b k s. We use greedy decoders, and train the models with dynamic oracles (Goldberg and Nivre, 2013). Table 1 reports the parsing accuracy that results for feature sets of size four, three, two, and one for three commonly-used transition systems. The data is the development section of the English Penn Treebank (PTB), and experimental settings are as described in our other experimental section, §6. We see that we can go down to three or, in the arc-hybrid and arc-eager transition systems, even two positions with very little loss in performance, ÑÐ ÑÐ but not further. We therefore call t s 0 , b 0 u our minimal feature set with respect to arc-hybrid and arc-eager, and empirically confirm that"
D17-1002,D16-1257,0,0.0242681,"for each head-modifier pair; a maximum spanning tree algorithm is used to find the tree with the highest sum of edge scores. For this model, we use Dozat and Man7 Comparison with State-of-the-Art Models Figure 2 compares our algorithms’ results with those of the state-of-the-art.9 Our models are competitive and an ensemble of 15 globallytrained models (5 models each for arc-eager DP, arc-hybrid DP and edge-factored) achieves 95.33 and 90.22 on PTB and CTB, respectively, reach8 The same architecture and model size as other transitionbased global models is used for fair comparison. 9 We exclude Choe and Charniak (2016), Kuncoro et al. (2017) and Liu and Zhang (2017), which convert constituentbased parses to dependency parses. They produce higher PTB UAS, but access more training information and do not directly apply to datasets without constituency annotation. See https://github.com/tzshi/dp-parser-emnlp17 . 19 ing the highest reported UAS on the CTB dataset, and the second highest reported on the PTB dataset among dependency-based approaches. 7 2011) have a run-time dependence on the number of positional features, using our mere two effective positional features results in a running time of Opn3 q, feasibl"
D17-1002,P08-1110,0,0.453412,"Missing"
D17-1002,J11-3004,0,0.120993,"Missing"
D17-1002,P16-2006,1,0.921336,") and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6 q (originally reported as Opn7 q) for a length-n sentence. As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well. Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).1 Inspired by their work, we arrive at a minimal feature set for arc-hybrid and arc-eager: it contains only two positional bi-LSTM vectors, suffers almost no loss in performance in comparison to l"
D17-1002,D16-1001,1,0.746358,") and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6 q (originally reported as Opn7 q) for a length-n sentence. As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well. Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).1 Inspired by their work, we arrive at a minimal feature set for arc-hybrid and arc-eager: it contains only two positional bi-LSTM vectors, suffers almost no loss in performance in comparison to l"
D17-1002,N03-1014,0,0.0345858,"ments to other transition systems. It would also be interesting to further investigate relationships between graph-based and dependency-based parsing. In §5 we have mentioned important earlier work in this regard, and provided an update to those formal findings. In our work, we have brought exact decoding, which was formerly the province solely of graphbased parsing, to the transition-based paradigm. We hope that the future will bring more inspiration from an integration of the two perspectives. Neural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers (Henderson, 2003, 2004; Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power. Recently, Stern et al. (2017) have proposed minimal span-based features for constituency parsing. Recurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree (Le and Zuidema, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016b), but these models cannot be readily combined with DP approaches, because their state spaces cannot be merged into smaller sets and"
D17-1002,P04-1013,0,0.0284527,"Missing"
D17-1002,P10-1110,1,0.662064,"tains only two positional bi-LSTM vectors, suffers almost no loss in performance in comparison to larger sets, and out-performs a single position. (Details regarding the situation with arc-standard can be found in §2.) Our minimal feature set plugs into Huang and Sagae’s and Kuhlmann et al.’s dynamic programWe first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae (2010) and Kuhlmann et al. (2011) to produce the first implementation of worst-case Opn3 q exact decoders for arc-hybrid and arceager transition systems. With our minimal features, we also present Opn3 q global training methods. Finally, using ensembles including our new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the “second-best-in-class” result on the English Penn Treebank. 1 Introduction It used to be the case that the most accurate dependency parsers made global decisions and employed exact decoding. But transition-based depend"
D17-1002,D16-1262,0,0.0437026,"Missing"
D17-1002,J93-2004,0,0.0606409,"Missing"
D17-1002,Q16-1023,0,0.23437,"n introduced by Huang and Sagae (2010) and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6 q (originally reported as Opn7 q) for a length-n sentence. As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well. Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).1 Inspired by their work, we arrive at a minimal feature set for arc-hybrid and arc-eager: it contains only two positional bi-LSTM vectors, suffers almost no loss in pe"
D17-1002,E06-1011,0,0.0166149,"ature vectors is passed through a multi-layer perceptron (MLP) with 1 hidden layer which has 256 hidden units and activation function tanh. We set the dropout rate for the bi-LSTM (Gal and Ghahramani, 2016) and MLP (Srivastava et al., 2014) for each model according to development-set performance.6 All parameters except the word embedThe extra expressiveness of the arc-eager model comes from the scoring functions fsh and fre that capture structural contexts other than headmodifier relations. Unlike traditional higher-order graph-based parsing that directly models relations such as siblinghood (McDonald and Pereira, 2006) or grandparenthood (Carreras, 2007), however, the arguments in those two functions do not have any fixed type of structural interactions. 6 Experiments Data and Evaluation We experimented with English and Chinese. For English, we used the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (via the Stanford parser 3.3.0) of the Penn Treebank (Marcus et al., 1993, PTB). As is standard, we used §2-21 of the Wall Street Journal for training, §22 for development, 6 For bi-LSTM input and recurrent connections, we consider dropout rates in t0., 0.2u, and for MLP, t0., 0.4u. 18 90.5 ?15"
D17-1002,Q16-1032,0,0.129429,"n introduced by Huang and Sagae (2010) and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6 q (originally reported as Opn7 q) for a length-n sentence. As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well. Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).1 Inspired by their work, we arrive at a minimal feature set for arc-hybrid and arc-eager: it contains only two positional bi-LSTM vectors, suffers almost no loss in pe"
D17-1002,P03-1054,0,0.070388,"Work Not Yet Mentioned • Combining exact decoding with global training — which is also enabled by our minimal feature set — with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank: these are, to our knowledge, the best and secondbest results to date on these data sets among “purely” dependency-based approaches. Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage conf"
D17-1002,P11-1068,0,0.0604016,"Missing"
D17-1002,E17-1117,0,0.0645445,"Missing"
D17-1002,W03-3017,0,0.0703598,"ning time is not sufficiently practical even in the simple-feature case, in the remainder of this paper we consider only the archybrid and arc-eager systems, not arc-standard. the collection of left modifiers before right modifiers via its b0 -modifier reð transition. This contrasts with arc-standard, where the attachment of left and right modifiers can be interleaved on the stack. shrpσ, b0 |β, Aqs “ pσ|b0 , β, Aq reñ rpσ|s1 |s0 , β, Aqs “ pσ|s1 , β, A Y tps1 , s0 quq reð rpσ|s0 , b0 |β, Aqs “ pσ, b0 |β, A Y tpb0 , s0 quq Arc-Eager In contrast to the former two systems, the arc-eager system (Nivre, 2003) makes attachments as early as possible — even if a modifier has not yet received all of its own modifiers. This behavior is accomplished by decomposing the right-reduce transition into two independent transitions, one making the attachment (ra) and one reducing the right-attached child (re). shrpσ, b0 |β, Aqs “ pσ|b0 , β, Aq reð rpσ|s0 , b0 |β, Aqs “ pσ, b0 |β, A Y tpb0 , s0 quq 4 Practical Optimal Algorithms Enabled By Our Minimal Feature Set (precondition: s0 not attached to any word) rarpσ|s0 , b0 |β, Aqs “ pσ|s0 |b0 , β, A Y tps0 , b0 quq Until now, no one had suggested a set of positiona"
D17-1002,W04-0308,0,0.156308,"erent parts of the buffer or stack; our convention is to depict the buffer first element leftmost, and to depict the stack first element rightmost). All terminal configurations have an empty buffer and a stack containing only ROOT. Dynamic Programming for TBDPs As stated in the introduction, our minimal feature set from §2 plugs into Huang and Sagae and Kuhlmann et al.’s dynamic programming (DP) framework. To help explain the connection, this section provides an overview of the DP framework. We draw heavily from the presentation of Kuhlmann et al. (2011). Arc-Standard The arc-standard system (Nivre, 2004) is motivated by bottom-up parsing: each dependent has to be complete before being attached. The three transitions, shift (sh, move a token from the buffer to the stack), right-reduce (reñ , reduce and attach a right modifier), and left-reduce (reð , reduce and attach a left modifier), are defined as: 3.1 Three Transition Systems shrpσ, b0 |β, Aqs “ pσ|b0 , β, Aq reñ rpσ|s1 |s0 , β, Aqs “ pσ|s1 , β, A Y tps1 , s0 quq Transition-based parsing (Nivre, 2008; K¨ubler et al., 2009) is an incremental parsing framework based on transitions between parser configurareð rpσ|s1 |s0 , β, Aqs “ pσ|s0 , β,"
D17-1002,D16-1180,0,0.0204462,"Missing"
D17-1002,J08-4003,0,0.0229758,"ides an overview of the DP framework. We draw heavily from the presentation of Kuhlmann et al. (2011). Arc-Standard The arc-standard system (Nivre, 2004) is motivated by bottom-up parsing: each dependent has to be complete before being attached. The three transitions, shift (sh, move a token from the buffer to the stack), right-reduce (reñ , reduce and attach a right modifier), and left-reduce (reð , reduce and attach a left modifier), are defined as: 3.1 Three Transition Systems shrpσ, b0 |β, Aqs “ pσ|b0 , β, Aq reñ rpσ|s1 |s0 , β, Aqs “ pσ|s1 , β, A Y tps1 , s0 quq Transition-based parsing (Nivre, 2008; K¨ubler et al., 2009) is an incremental parsing framework based on transitions between parser configurareð rpσ|s1 |s0 , β, Aqs “ pσ|s0 , β, A Y tps0 , s1 quq Arc-Hybrid The arc-hybrid system (Yamada and Matsumoto, 2003; G´omez-Rodr´ıguez et al., 2008; Kuhlmann et al., 2011) has the same definitions of sh and reñ as arc-standard, but forces 3 We tentatively conjecture that the following might explain the observed phenomena, but stress that we don’t currently see a concrete way to test the following hypothesis. ÑÐ ÑÐ With t s 0 , b 0 u, in the arc-standard case, situations can arise where ther"
D17-1002,D14-1081,0,0.0118321,"the future will bring more inspiration from an integration of the two perspectives. Neural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers (Henderson, 2003, 2004; Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power. Recently, Stern et al. (2017) have proposed minimal span-based features for constituency parsing. Recurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree (Le and Zuidema, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016b), but these models cannot be readily combined with DP approaches, because their state spaces cannot be merged into smaller sets and thus remain exponentially large. 8 Acknowledgments: an author-reviewer success story We sincerely thank all the reviewers for their extraordinarily careful and helpful comments. Indeed, this paper originated as a short paper submission by TS&LL to ACL 2017, where an anonymous reviewer explained in the review comments how, among other things, the DP runtime could be improved from Opn4 q to Opn3 q. In their author"
D17-1002,D14-1162,0,0.0970433,"oof Sketch. We leverage the fact that the arceager model divides the sh transition in the archybrid model into two separate transitions, sh and ra. When we constrain the parameters fsh “ fra in the arc-eager model, the model hypothesis space becomes exactly the same as arc-hybrid’s. Implementation Details Our model structures reproduce those of Kiperwasser and Goldberg (2016a). We use 2-layer bi-directional LSTMs with 256 hidden cell units. Inputs are concatenations of 28-dimensional randomly-initialized partof-speech embeddings and 100-dimensional word vectors initialized from GloVe vectors (Pennington et al., 2014) (English) and pre-trained skipgram-model vectors (Mikolov et al., 2013) (Chinese). The concatenation of the bi-LSTM feature vectors is passed through a multi-layer perceptron (MLP) with 1 hidden layer which has 256 hidden units and activation function tanh. We set the dropout rate for the bi-LSTM (Gal and Ghahramani, 2016) and MLP (Srivastava et al., 2014) for each model according to development-set performance.6 All parameters except the word embedThe extra expressiveness of the arc-eager model comes from the scoring functions fsh and fre that capture structural contexts other than headmodif"
D17-1002,P83-1021,0,0.424626,"pσ|s0 , b0 |β, Aqs “ pσ|s0 |b0 , β, A Y tps0 , b0 quq Until now, no one had suggested a set of positional features that was both information-rich enough for accurate parsing and small enough to obtain the Opn3 q running-time promised above. Fortunately, ÑÐ ÑÐ our bi-LSTM-based t s 0 , b 0 u feature set qualifies, and enables the fast optimal procedures described in this section. rerpσ|s0 , β, Aqs “ pσ, β, Aq (precondition: s0 has been attached to its head) 3.2 Deduction and Dynamic Programming Kuhlmann et al. (2011) reformulate the three transition systems just discussed as deduction systems (Pereira and Warren, 1983; Shieber et al., 1995), wherein transitions serve as inference rules; these are given as the lefthand sides of the first three subfigures in Figure 1. For a given w “ w1 , ..., wn , assertions take the form ri, j, ks (or, when applicable, a two-index shorthand to be discussed soon), meaning that there exists a sequence of transitions that, starting from a configuration wherein head ps0 q “ wi , results in an ending configuration wherein head ps0 q “ wj and head pb0 q “ wk . If we define w0 as ROOT and wn`1 as an endof-sentence marker, then the goal theorem can be stated as r0, 0, n ` 1s. For"
D17-1002,Q16-1014,0,0.0327473,"11), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating from the gold transition path during training to better simulate the test-time environment. There are many directions for further exploration. Two possibilities are to create even better training methods, and to find some way to extend our run-time improvements to other transition systems. It would also be interesting to further investigate relationships between graph-based and dependency-based parsing. In §5 we have mentioned important earlier work"
D17-1002,P06-2089,0,0.080573,"Combining exact decoding with global training — which is also enabled by our minimal feature set — with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank: these are, to our knowledge, the best and secondbest results to date on these data sets among “purely” dependency-based approaches. Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating fr"
D17-1002,P16-1218,0,0.024492,"Missing"
D17-1002,D07-1111,0,0.0247084,"ng with global training — which is also enabled by our minimal feature set — with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank: these are, to our knowledge, the best and secondbest results to date on these data sets among “purely” dependency-based approaches. Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating from the gold transition p"
D17-1002,P15-1032,0,0.0441803,"ur new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the “second-best-in-class” result on the English Penn Treebank. 1 Introduction It used to be the case that the most accurate dependency parsers made global decisions and employed exact decoding. But transition-based dependency parsers (TBDPs) have recently achieved state-of-the-art performance, despite the fact that for efficiency reasons, they are usually trained to make local, rather than global, decisions and the decoding process is done approximately, rather than exactly (Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016). The key efficiency issue for decoding is as follows. In order to make accurate (local) attachment decisions, historically, TBDPs have required a large set of features in order to access rich information about particular positions in the stack and buffer of the current parser configuration. But consulting many positions means that although polynomial-time exact-decoding algo1 We note that K&G were not focused on minimizing positions, although they explicitly noted the implications of doing so: “While not explored in this work, [fewer positions] results"
D17-1002,K17-3003,1,0.753254,", but forces 3 We tentatively conjecture that the following might explain the observed phenomena, but stress that we don’t currently see a concrete way to test the following hypothesis. ÑÐ ÑÐ With t s 0 , b 0 u, in the arc-standard case, situations can arise where there are multiple possible transitions with missing information. In contrast, in the arc-hybrid case, there is only one possible transition with missing information (namely, ÑÐ reñ , introduced in §3.1); perhaps s 1 is therefore not so crucial for arc-hybrid in practice? 4 For simplicity, we only present unlabeled parsing here. See Shi et al. (2017) for labeled-parsing results. 5 Other presentations place ROOT at the end of the buffer or omit it entirely (Ballesteros and Nivre, 2013). 14 assertion rib , js is an indicator variable for whether wi has been attached to its head (b “ 1) or not (b “ 0) after the transition sequence is applied. Kuhlmann et al. (2011) show that all three deduction systems can be directly “tabularized” and dynamic programming (DP) can be applied, such that, ignoring for the moment the issue of incorporating complex features (we return to this later), time and space needs are low-order polynomial. Specifically, a"
D17-1002,C02-1145,0,0.0500423,"s 0, b 0u Global t h , mu t s 0, b 0u t s 0, b 0u ÑÐ ÑÐ t s 0, b 0u t s 0, b 0u ÑÐ ÑÐ Table 2: Test set performance for different training regimes and feature sets. The models use the same decoders for testing and training. For each setting, the average and standard deviation across 5 runs with different random initializations are reported. Boldface: best (averaged) result per dataset/measure. ÑÐ ÑÐ ÑÐ ÑÐ ÑÐ ÑÐ and §23 for testing; POS tags were predicted using 10-way jackknifing with the Stanford max entropy tagger (Toutanova et al., 2003). For Chinese, we used the Penn Chinese Treebank 5.1 (Xue et al., 2002, CTB), with the same splits and head-finding rules for conversion to dependencies as Zhang and Clark (2008). We adopted the CTB’s goldstandard tokenization and POS tags. We report unlabeled attachment score (UAS) and sentencelevel unlabeled exact match (UEM). Following prior work, all punctuation is excluded from evaluation. For each model, we initialized the network parameters with 5 different random seeds and report performance average and standard deviation. fra (wk ,wi ) “ fG pwk , wi q and fre pwi , wj q “ 0. The parameterization we arrive at emulates exactly the scoring model of fG . We"
D17-1002,W03-3023,0,0.289532,"as to be complete before being attached. The three transitions, shift (sh, move a token from the buffer to the stack), right-reduce (reñ , reduce and attach a right modifier), and left-reduce (reð , reduce and attach a left modifier), are defined as: 3.1 Three Transition Systems shrpσ, b0 |β, Aqs “ pσ|b0 , β, Aq reñ rpσ|s1 |s0 , β, Aqs “ pσ|s1 , β, A Y tps1 , s0 quq Transition-based parsing (Nivre, 2008; K¨ubler et al., 2009) is an incremental parsing framework based on transitions between parser configurareð rpσ|s1 |s0 , β, Aqs “ pσ|s0 , β, A Y tps0 , s1 quq Arc-Hybrid The arc-hybrid system (Yamada and Matsumoto, 2003; G´omez-Rodr´ıguez et al., 2008; Kuhlmann et al., 2011) has the same definitions of sh and reñ as arc-standard, but forces 3 We tentatively conjecture that the following might explain the observed phenomena, but stress that we don’t currently see a concrete way to test the following hypothesis. ÑÐ ÑÐ With t s 0 , b 0 u, in the arc-standard case, situations can arise where there are multiple possible transitions with missing information. In contrast, in the arc-hybrid case, there is only one possible transition with missing information (namely, ÑÐ reñ , introduced in §3.1); perhaps s 1 is ther"
D17-1002,D08-1059,0,0.0124844,"t performance for different training regimes and feature sets. The models use the same decoders for testing and training. For each setting, the average and standard deviation across 5 runs with different random initializations are reported. Boldface: best (averaged) result per dataset/measure. ÑÐ ÑÐ ÑÐ ÑÐ ÑÐ ÑÐ and §23 for testing; POS tags were predicted using 10-way jackknifing with the Stanford max entropy tagger (Toutanova et al., 2003). For Chinese, we used the Penn Chinese Treebank 5.1 (Xue et al., 2002, CTB), with the same splits and head-finding rules for conversion to dependencies as Zhang and Clark (2008). We adopted the CTB’s goldstandard tokenization and POS tags. We report unlabeled attachment score (UAS) and sentencelevel unlabeled exact match (UEM). Following prior work, all punctuation is excluded from evaluation. For each model, we initialized the network parameters with 5 different random seeds and report performance average and standard deviation. fra (wk ,wi ) “ fG pwk , wi q and fre pwi , wj q “ 0. The parameterization we arrive at emulates exactly the scoring model of fG . We further claim that the arc-eager model is more expressive than not only the edge-factored GBDP, but also th"
D17-1002,P17-1076,0,0.0450955,"ed important earlier work in this regard, and provided an update to those formal findings. In our work, we have brought exact decoding, which was formerly the province solely of graphbased parsing, to the transition-based paradigm. We hope that the future will bring more inspiration from an integration of the two perspectives. Neural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers (Henderson, 2003, 2004; Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power. Recently, Stern et al. (2017) have proposed minimal span-based features for constituency parsing. Recurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree (Le and Zuidema, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016b), but these models cannot be readily combined with DP approaches, because their state spaces cannot be merged into smaller sets and thus remain exponentially large. 8 Acknowledgments: an author-reviewer success story We sincerely thank all the reviewers for their extraordinarily careful and helpful comments"
D17-1002,J11-1005,0,0.0162033,"ches. Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating from the gold transition path during training to better simulate the test-time environment. There are many directions for further exploration. Two possibilities are to create even better training methods, and to find some way to extend our run-time improvements to other transition systems. It would also be interesting to further inves"
D17-1002,D13-1071,1,0.883547,"— which is also enabled by our minimal feature set — with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank: these are, to our knowledge, the best and secondbest results to date on these data sets among “purely” dependency-based approaches. Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating from the gold transition path during training"
D17-1002,P15-1148,0,0.0288784,"Missing"
D17-1002,N03-1033,0,0.0951612,"0.52 Local Local Global t s 2, s 1, s 0, b 0u Local Local Global t s 2, s 1, s 0, b 0u Global t h , mu t s 0, b 0u t s 0, b 0u ÑÐ ÑÐ t s 0, b 0u t s 0, b 0u ÑÐ ÑÐ Table 2: Test set performance for different training regimes and feature sets. The models use the same decoders for testing and training. For each setting, the average and standard deviation across 5 runs with different random initializations are reported. Boldface: best (averaged) result per dataset/measure. ÑÐ ÑÐ ÑÐ ÑÐ ÑÐ ÑÐ and §23 for testing; POS tags were predicted using 10-way jackknifing with the Stanford max entropy tagger (Toutanova et al., 2003). For Chinese, we used the Penn Chinese Treebank 5.1 (Xue et al., 2002, CTB), with the same splits and head-finding rules for conversion to dependencies as Zhang and Clark (2008). We adopted the CTB’s goldstandard tokenization and POS tags. We report unlabeled attachment score (UAS) and sentencelevel unlabeled exact match (UEM). Following prior work, all punctuation is excluded from evaluation. For each model, we initialized the network parameters with 5 different random seeds and report performance average and standard deviation. fra (wk ,wi ) “ fG pwk , wi q and fre pwi , wj q “ 0. The param"
D17-1225,W12-1623,0,0.161747,"Missing"
D17-1225,D15-1263,0,0.0149098,"elined rather than end-to-end: they assume pre-segmented discourse, and worse yet, use gold-standard segmentations, except Hernault et al. (2010); 2. not self-contained: they rely on external syntactic parsers and pretrained word vectors; 3. complicated: they design sophisticated features, including those from parse-trees. Introduction Distinguishing the semantic relations between segments in a document can be greatly beneficial to many high-level NLP tasks, such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Voll and Taboada, 2007; Somasundaran et al., 2009; Bhatia et al., 2015), question answering (Ferrucci et al., 2010; Jansen et al., 2014), and textual quality evaluation (Tetreault et al., 2013; Li and Jurafsky, 2016). There has been a variety of research on discourse parsing (Marcu, 2000a; Soricut and Marcu, 2003; Pardo and Nunes, 2008; Hernault et al., We argue for the first time that discourse parsing should be viewed as an extension of, and be performed in conjunction with, constituency parsing. We propose the first joint syntacto-discourse treebank, by unifying constituency and discourse tree representations. Based on this, we propose the first end-to-end inc"
D17-1225,P16-2006,1,0.366306,"Jurafsky, 2016). There has been a variety of research on discourse parsing (Marcu, 2000a; Soricut and Marcu, 2003; Pardo and Nunes, 2008; Hernault et al., We argue for the first time that discourse parsing should be viewed as an extension of, and be performed in conjunction with, constituency parsing. We propose the first joint syntacto-discourse treebank, by unifying constituency and discourse tree representations. Based on this, we propose the first end-to-end incremental parser that jointly parses at both constituency and discourse levels. Our algorithm builds up on the span-based parser (Cross and Huang, 2016); it employs the strong generalization power of bi-directional LSTMs, and parses efficiently and robustly with an extremely simple span-based feature set that does not use any tree structure information. We make the following contributions: ∗ The source code and the joint treebank are available at https://github.com/kaayy/josydipa. † Current address: Google Inc., New York, NY, USA. 2117 1. We develop a combined representation of constituency and discourse trees to facilitate parsing at both levels without explicit conversion mechanism. Using this representation, we build and release a joint tr"
D17-1225,P14-1048,0,0.16174,"ckground” or “Purpose”. Figure 1(a) shows an example RST tree. There are also nonbinary-branching internal nodes whose children are conjunctions, e.g., a “List” of semantically similar EDUs (which are all nucleus nodes); see Figure 2(a) for an example. 2.2 Syntacto-Discourse Representation It is widely recognized that lower-level lexical and syntactic information can greatly help determining both the boundaries of the EDUs (i.e., discourse segmentation) (Bach et al., 2012) as well as the semantic relations between EDUs (Soricut and Marcu, 2003; Hernault et al., 2010; Joty and Moschitti, 2014; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a; Heilman and Sagae, 2015). While these previous approaches rely on pre-trained tools to provide both EDU segmentation and intra-EDU syntactic parse trees, we instead propose to directly determine the low-level segmentations, the syntactic parses, and the highlevel discourse parses using a single joint parser. This parser is trained on the combined trees of constituency and discourse structures. We first convert an RST tree to a format similar 2118 Elaboration • The metals sector outgained other industry groups. ◦ List • • Hecla Mining rose 5 8 • 3 4"
D17-1225,Q13-1033,0,0.0294753,"t the boundaries. The span features parsing, the stack only stores the boundaries of are then passed into fully connected networks with Some textofand text the andsymbol the...symbol subtrees, which are justSome a list indices . scaled i kor scaled jor softmax to calculate the likelihood of performIn other words, quite shockingly, no tree structure ing the corresponding action or marking the coris represented anywhere in the parser. Please refer responding label. Cross and Huang (2016) for details. We use the “training with exploration” stratSimilar to span-based constituency parsing, we egy (Goldberg and Nivre, 2013) and the dynamic alternate between structural (either shift or comoracle mechanism described in Cross and Huang bine) and label (labelX or nolabel) actions in an (2016) to make sure the model can handle unseen odd-even fashion. But different from Cross and parsing configurations properly. Huang (2016), after a structural action, we choose Some text and the symbol 4 Empirical Results to keep the last branching point k, i.e., i k jor scaled (mostly for combine, but also trivially for shift). We use the treebank described in Section 2 for emThis is because in our parsing mechanism, the dispirical"
D17-1225,P14-1092,0,0.032487,"e, and worse yet, use gold-standard segmentations, except Hernault et al. (2010); 2. not self-contained: they rely on external syntactic parsers and pretrained word vectors; 3. complicated: they design sophisticated features, including those from parse-trees. Introduction Distinguishing the semantic relations between segments in a document can be greatly beneficial to many high-level NLP tasks, such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Voll and Taboada, 2007; Somasundaran et al., 2009; Bhatia et al., 2015), question answering (Ferrucci et al., 2010; Jansen et al., 2014), and textual quality evaluation (Tetreault et al., 2013; Li and Jurafsky, 2016). There has been a variety of research on discourse parsing (Marcu, 2000a; Soricut and Marcu, 2003; Pardo and Nunes, 2008; Hernault et al., We argue for the first time that discourse parsing should be viewed as an extension of, and be performed in conjunction with, constituency parsing. We propose the first joint syntacto-discourse treebank, by unifying constituency and discourse tree representations. Based on this, we propose the first end-to-end incremental parser that jointly parses at both constituency and disc"
D17-1225,P14-1002,0,0.268383,". Figure 1(a) shows an example RST tree. There are also nonbinary-branching internal nodes whose children are conjunctions, e.g., a “List” of semantically similar EDUs (which are all nucleus nodes); see Figure 2(a) for an example. 2.2 Syntacto-Discourse Representation It is widely recognized that lower-level lexical and syntactic information can greatly help determining both the boundaries of the EDUs (i.e., discourse segmentation) (Bach et al., 2012) as well as the semantic relations between EDUs (Soricut and Marcu, 2003; Hernault et al., 2010; Joty and Moschitti, 2014; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a; Heilman and Sagae, 2015). While these previous approaches rely on pre-trained tools to provide both EDU segmentation and intra-EDU syntactic parse trees, we instead propose to directly determine the low-level segmentations, the syntactic parses, and the highlevel discourse parses using a single joint parser. This parser is trained on the combined trees of constituency and discourse structures. We first convert an RST tree to a format similar 2118 Elaboration • The metals sector outgained other industry groups. ◦ List • • Hecla Mining rose 5 8 • 3 4 to 14; Battle Mountain Go"
D17-1225,D14-1219,0,0.0181541,"-nucleus pair, such as “Background” or “Purpose”. Figure 1(a) shows an example RST tree. There are also nonbinary-branching internal nodes whose children are conjunctions, e.g., a “List” of semantically similar EDUs (which are all nucleus nodes); see Figure 2(a) for an example. 2.2 Syntacto-Discourse Representation It is widely recognized that lower-level lexical and syntactic information can greatly help determining both the boundaries of the EDUs (i.e., discourse segmentation) (Bach et al., 2012) as well as the semantic relations between EDUs (Soricut and Marcu, 2003; Hernault et al., 2010; Joty and Moschitti, 2014; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a; Heilman and Sagae, 2015). While these previous approaches rely on pre-trained tools to provide both EDU segmentation and intra-EDU syntactic parse trees, we instead propose to directly determine the low-level segmentations, the syntactic parses, and the highlevel discourse parses using a single joint parser. This parser is trained on the combined trees of constituency and discourse structures. We first convert an RST tree to a format similar 2118 Elaboration • The metals sector outgained other industry groups. ◦ List • • Hecla"
D17-1225,P13-1048,0,0.076539,"Missing"
D17-1225,D14-1220,0,0.0370952,"Missing"
D17-1225,P14-1003,0,0.109457,"ample RST tree. There are also nonbinary-branching internal nodes whose children are conjunctions, e.g., a “List” of semantically similar EDUs (which are all nucleus nodes); see Figure 2(a) for an example. 2.2 Syntacto-Discourse Representation It is widely recognized that lower-level lexical and syntactic information can greatly help determining both the boundaries of the EDUs (i.e., discourse segmentation) (Bach et al., 2012) as well as the semantic relations between EDUs (Soricut and Marcu, 2003; Hernault et al., 2010; Joty and Moschitti, 2014; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a; Heilman and Sagae, 2015). While these previous approaches rely on pre-trained tools to provide both EDU segmentation and intra-EDU syntactic parse trees, we instead propose to directly determine the low-level segmentations, the syntactic parses, and the highlevel discourse parses using a single joint parser. This parser is trained on the combined trees of constituency and discourse structures. We first convert an RST tree to a format similar 2118 Elaboration • The metals sector outgained other industry groups. ◦ List • • Hecla Mining rose 5 8 • 3 4 to 14; Battle Mountain Gold climbed to 16"
D17-1225,W10-4327,0,0.0184542,"segmentation or feature extraction), achieves the state-of-theart end-to-end discourse parsing accuracy. 1 1. pipelined rather than end-to-end: they assume pre-segmented discourse, and worse yet, use gold-standard segmentations, except Hernault et al. (2010); 2. not self-contained: they rely on external syntactic parsers and pretrained word vectors; 3. complicated: they design sophisticated features, including those from parse-trees. Introduction Distinguishing the semantic relations between segments in a document can be greatly beneficial to many high-level NLP tasks, such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Voll and Taboada, 2007; Somasundaran et al., 2009; Bhatia et al., 2015), question answering (Ferrucci et al., 2010; Jansen et al., 2014), and textual quality evaluation (Tetreault et al., 2013; Li and Jurafsky, 2016). There has been a variety of research on discourse parsing (Marcu, 2000a; Soricut and Marcu, 2003; Pardo and Nunes, 2008; Hernault et al., We argue for the first time that discourse parsing should be viewed as an extension of, and be performed in conjunction with, constituency parsing. We propose the first joint syntacto-discourse treeb"
D17-1225,J00-3005,0,0.669733,"ord vectors; 3. complicated: they design sophisticated features, including those from parse-trees. Introduction Distinguishing the semantic relations between segments in a document can be greatly beneficial to many high-level NLP tasks, such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Voll and Taboada, 2007; Somasundaran et al., 2009; Bhatia et al., 2015), question answering (Ferrucci et al., 2010; Jansen et al., 2014), and textual quality evaluation (Tetreault et al., 2013; Li and Jurafsky, 2016). There has been a variety of research on discourse parsing (Marcu, 2000a; Soricut and Marcu, 2003; Pardo and Nunes, 2008; Hernault et al., We argue for the first time that discourse parsing should be viewed as an extension of, and be performed in conjunction with, constituency parsing. We propose the first joint syntacto-discourse treebank, by unifying constituency and discourse tree representations. Based on this, we propose the first end-to-end incremental parser that jointly parses at both constituency and discourse levels. Our algorithm builds up on the span-based parser (Cross and Huang, 2016); it employs the strong generalization power of bi-directional LST"
D17-1225,J93-2004,0,0.0612687,"ion power of bi-directional LSTMs, and parses efficiently and robustly with an extremely simple span-based feature set that does not use any tree structure information. We make the following contributions: ∗ The source code and the joint treebank are available at https://github.com/kaayy/josydipa. † Current address: Google Inc., New York, NY, USA. 2117 1. We develop a combined representation of constituency and discourse trees to facilitate parsing at both levels without explicit conversion mechanism. Using this representation, we build and release a joint treebank based on the Penn Treebank (Marcus et al., 1993) and RST Treebank (Marcu, 2000a,b) (Section 2). 2. We propose a novel joint parser that parses at both constituency and discourse levels. Our Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2117–2123 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Background ◦ • Costa Rica had been negotiating with U.S. banks Purpose • ◦ but the debt plan was rushed to completion in order to be announced at the meeting (a) A discourse tree with 3 EDUs (•: nucleas; ◦: satellite) in the RST treebank (Marcu, 2000b) Backgroun"
D17-1225,D09-1018,0,0.0377161,"parsing accuracy. 1 1. pipelined rather than end-to-end: they assume pre-segmented discourse, and worse yet, use gold-standard segmentations, except Hernault et al. (2010); 2. not self-contained: they rely on external syntactic parsers and pretrained word vectors; 3. complicated: they design sophisticated features, including those from parse-trees. Introduction Distinguishing the semantic relations between segments in a document can be greatly beneficial to many high-level NLP tasks, such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Voll and Taboada, 2007; Somasundaran et al., 2009; Bhatia et al., 2015), question answering (Ferrucci et al., 2010; Jansen et al., 2014), and textual quality evaluation (Tetreault et al., 2013; Li and Jurafsky, 2016). There has been a variety of research on discourse parsing (Marcu, 2000a; Soricut and Marcu, 2003; Pardo and Nunes, 2008; Hernault et al., We argue for the first time that discourse parsing should be viewed as an extension of, and be performed in conjunction with, constituency parsing. We propose the first joint syntacto-discourse treebank, by unifying constituency and discourse tree representations. Based on this, we propose th"
D17-1225,N03-1030,0,0.640834,". complicated: they design sophisticated features, including those from parse-trees. Introduction Distinguishing the semantic relations between segments in a document can be greatly beneficial to many high-level NLP tasks, such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Voll and Taboada, 2007; Somasundaran et al., 2009; Bhatia et al., 2015), question answering (Ferrucci et al., 2010; Jansen et al., 2014), and textual quality evaluation (Tetreault et al., 2013; Li and Jurafsky, 2016). There has been a variety of research on discourse parsing (Marcu, 2000a; Soricut and Marcu, 2003; Pardo and Nunes, 2008; Hernault et al., We argue for the first time that discourse parsing should be viewed as an extension of, and be performed in conjunction with, constituency parsing. We propose the first joint syntacto-discourse treebank, by unifying constituency and discourse tree representations. Based on this, we propose the first end-to-end incremental parser that jointly parses at both constituency and discourse levels. Our algorithm builds up on the span-based parser (Cross and Huang, 2016); it employs the strong generalization power of bi-directional LSTMs, and parses efficiently"
D17-1225,P17-2029,0,0.0615783,"Missing"
D17-1225,D14-1196,0,0.0542107,"ture extraction), achieves the state-of-theart end-to-end discourse parsing accuracy. 1 1. pipelined rather than end-to-end: they assume pre-segmented discourse, and worse yet, use gold-standard segmentations, except Hernault et al. (2010); 2. not self-contained: they rely on external syntactic parsers and pretrained word vectors; 3. complicated: they design sophisticated features, including those from parse-trees. Introduction Distinguishing the semantic relations between segments in a document can be greatly beneficial to many high-level NLP tasks, such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Voll and Taboada, 2007; Somasundaran et al., 2009; Bhatia et al., 2015), question answering (Ferrucci et al., 2010; Jansen et al., 2014), and textual quality evaluation (Tetreault et al., 2013; Li and Jurafsky, 2016). There has been a variety of research on discourse parsing (Marcu, 2000a; Soricut and Marcu, 2003; Pardo and Nunes, 2008; Hernault et al., We argue for the first time that discourse parsing should be viewed as an extension of, and be performed in conjunction with, constituency parsing. We propose the first joint syntacto-discourse treebank, by unifying consti"
D17-1227,D13-1176,0,0.0398263,"stablished (finishing no later than the baseline). To counter neural generation’s tendency for shorter hypotheses, we also introduce a bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal. Experiments on neural machine translation demonstrate that our principled beam search algorithm leads to improvement in BLEU score over previously proposed alternatives. 1 Introduction In recent years, neural text generation using recurrent networks have witnessed rapid progress, quickly becoming the state-of-the-art paradigms in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), summarization (Rush et al., 2015; Ranzato et al., 2016), and image captioning (Vinyals et al., 2015; Xu et al., 2015). In the decoder of neural generation, beam search is widely employed to boost the output text quality, often leading to substantial improvement over greedy search (equivalent to beam size 1) in metrics such as BLEU or † Current address: Google Inc., New York, NY, USA. ROUGE; for example, Ranzato et al. (2016) reported +2.2 BLEU (on single reference) in translation and +3.5 ROUGE-2 in summarization, both using a beam of 10. Our o"
D17-1227,P17-4012,0,0.088904,"eturned hypothesis has the optimal score modulo beam size? There have not been satisfying answers to these questions, and existing beam search strategies are heuristic methods that do not guarantee optimality. For example, the widely influential RNNsearch (Bahdanau et al., 2014) employs a “shrinking beam” method: once a completed hypothesis is found, beam size shrinks by 1, and beam search would finish if beam size shrinks to 0 or if the number of steps hits a hard limit. The best scoring completed hypothesis among all completed ones encountered so far is returned. On the other hand, OpenNMT (Klein et al., 2017), whose PyTorch version will be the baseline in our experiments, uses a very different strategy: beam search terminates whenever the highest-ranking hypothesis in the current step is completed (which is also the one returned), without considering any other completed hypotheses. Neither of these two methods guarantee optimality of the returned hypothesis. We therefore propose a novel and simple beam search variant that will always return the optimalscore complete hypothesis (modulo beam size), and finish as soon as the optimality is established. 2134 Proceedings of the 2017 Conference on Empiri"
D17-1227,P07-2045,0,0.00779722,"hypothesis (modulo beam size), and finish as soon as the optimality is established. 2134 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2134–2139 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics However, another well-known problem remains, that the generated sentences are often too short, compared to previous paradigms such as SMT (Shen et al., 2016). To alleviate this problem, previous efforts introduce length normalization (as a switch in RNNsearch) or length reward (He et al., 2016) borrowed from SMT (Koehn et al., 2007). Unfortunately these changes will invalidate the optimal property of our proposed algorithm. So we introduce a bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal. Experiments on neural machine translation demonstrate that our principled beam search algorithm leads to improvement in BLEU score over previously proposed alternatives. 2 Neural Generation and Beam Search Here we briefly review neural text generation and then review existing beam search algorithms. Assume the input sentence, document, or image is embedded into a vector x,"
D17-1227,D15-1044,0,0.0423858,"shorter hypotheses, we also introduce a bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal. Experiments on neural machine translation demonstrate that our principled beam search algorithm leads to improvement in BLEU score over previously proposed alternatives. 1 Introduction In recent years, neural text generation using recurrent networks have witnessed rapid progress, quickly becoming the state-of-the-art paradigms in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), summarization (Rush et al., 2015; Ranzato et al., 2016), and image captioning (Vinyals et al., 2015; Xu et al., 2015). In the decoder of neural generation, beam search is widely employed to boost the output text quality, often leading to substantial improvement over greedy search (equivalent to beam size 1) in metrics such as BLEU or † Current address: Google Inc., New York, NY, USA. ROUGE; for example, Ranzato et al. (2016) reported +2.2 BLEU (on single reference) in translation and +3.5 ROUGE-2 in summarization, both using a beam of 10. Our own experiments on machine translation (see Sec. 5) show +4.2 BLEU (on four referen"
D17-1227,P16-1159,0,0.0435868,"hese two methods guarantee optimality of the returned hypothesis. We therefore propose a novel and simple beam search variant that will always return the optimalscore complete hypothesis (modulo beam size), and finish as soon as the optimality is established. 2134 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2134–2139 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics However, another well-known problem remains, that the generated sentences are often too short, compared to previous paradigms such as SMT (Shen et al., 2016). To alleviate this problem, previous efforts introduce length normalization (as a switch in RNNsearch) or length reward (He et al., 2016) borrowed from SMT (Koehn et al., 2007). Unfortunately these changes will invalidate the optimal property of our proposed algorithm. So we introduce a bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal. Experiments on neural machine translation demonstrate that our principled beam search algorithm leads to improvement in BLEU score over previously proposed alternatives. 2 Neural Generation and Beam"
D17-1227,1983.tc-1.13,0,0.752733,"Missing"
D18-1150,P14-2023,0,0.0309302,"stic model, e.g., conditional random field (Roark et al., 2004), to the space of text candidates, and maximize the probability at the desired candidate. The problem is often solved by perceptron algorithm. However, these methods all rely on ad-hoc choice of features, e.g., counts of n-grams where n varies in a small range (e.g.,1 to 3). Moreover, it is also not clear how these methods would take advantage of an existing language model (trained on large unsupervised corpus). Nevertheless, the same methodology can be extended to RNNLMs, thus avoiding the aforementioned limitations. For example, Auli and Gao (2014) train an RNNLM by favoring sentences with high BLEU scores and integrate it into a phrase-based MT decoder. If we cast the problem of picking the best text sequence as a ranking problem, the aforementioned works can be considered as “pointwise” learning-to-rank approaches (Cossock and Zhang, 2008). In contrast, the proposed method is a “pairwise” approach (Liu, 2009), as it learns a neural language model by comparison between pairs of sentences. Earlier works in this fashion may date back to (Collins and Koo, 2005), which improves a semantic parser. Learning “by pairwise comparison” is also s"
D18-1150,P96-1041,0,0.184566,"Missing"
D18-1150,J05-1003,0,0.0440463,"be extended to RNNLMs, thus avoiding the aforementioned limitations. For example, Auli and Gao (2014) train an RNNLM by favoring sentences with high BLEU scores and integrate it into a phrase-based MT decoder. If we cast the problem of picking the best text sequence as a ranking problem, the aforementioned works can be considered as “pointwise” learning-to-rank approaches (Cossock and Zhang, 2008). In contrast, the proposed method is a “pairwise” approach (Liu, 2009), as it learns a neural language model by comparison between pairs of sentences. Earlier works in this fashion may date back to (Collins and Koo, 2005), which improves a semantic parser. Learning “by pairwise comparison” is also seen in several MT literatures. For example, Hopkins and May (2011) propose to train a phrase-based MT system by minimizing a pairwise ranking loss. Wiseman and Rush (2016) optimize the beam search process in a Neural Machine Translation (NMT) system. They enforce the score of a reference to be higher than that of its decoded k-th candidate by at least a unit margin. Rather than optimizing the MT system itself, this work proposes a general method of training recurrent neural language models, which can benefit various"
D18-1150,D11-1125,0,0.0415694,"igh BLEU scores and integrate it into a phrase-based MT decoder. If we cast the problem of picking the best text sequence as a ranking problem, the aforementioned works can be considered as “pointwise” learning-to-rank approaches (Cossock and Zhang, 2008). In contrast, the proposed method is a “pairwise” approach (Liu, 2009), as it learns a neural language model by comparison between pairs of sentences. Earlier works in this fashion may date back to (Collins and Koo, 2005), which improves a semantic parser. Learning “by pairwise comparison” is also seen in several MT literatures. For example, Hopkins and May (2011) propose to train a phrase-based MT system by minimizing a pairwise ranking loss. Wiseman and Rush (2016) optimize the beam search process in a Neural Machine Translation (NMT) system. They enforce the score of a reference to be higher than that of its decoded k-th candidate by at least a unit margin. Rather than optimizing the MT system itself, this work proposes a general method of training recurrent neural language models, which can benefit various text generation tasks, including speech recognition and machine translation. 7 Conclusions We have proposed a large margin criterion for trainin"
D18-1150,P09-5002,0,0.0141049,"hine translation. 1 Introduction Language models (LMs) estimate the likelihood of a symbol sequence {xt }Tt=0 , based on the joint probability, p(x0 , . . . , xT ) = p(x0 ) T Y p(xt |x0 , . . . , xt−1 ). t=1 (1) To measure the quality of an LM, a commonly adopted metric is perplexity (PPL), defined as ) ( T 1X t 0 t−1 log p(x |x , . . . , x ) , PPL , exp − T t=0 A good language model has a small PPL, being able to assign higher likelihoods to sentences that are more likely to appear. LMs are widely applied in automatic speech recognition (ASR) (Yu and Deng, 2014) and machine translation (MT) (Koehn, 2009). Following Koehn (2009), one may interpret the language ∗ Contributions were made while at Baidu Research. model as prior knowledge on the text to be inferred, which provides information complementary to the ASR or MT system itself. In practice, there are several ways to incorporate the language model. The simplest way may be re-scoring an n-best list returned by the ASR or MT system (Mikolov et al., 2010; Sundermeyer et al., 2012). A slightly more sophisticated way is to jointly consider the ASR/MT and language model in a beam search decoder (Amodei et al., 2016). Specifically, at each time"
D18-1150,2008.amta-papers.12,0,0.0685433,") propose to predict the next symbol based on a fusion of the hidden states in the ASR/MT and language models. A gating mechanism is jointly trained to determine how much the language model should contribute. The afore-discussed language models are generative in the sense that they merely model the joint distribution of a symbol sequence (Eq. (1)). While the research community is mostly focused on pushing the limit of PPL (e.g., Jozefowicz et al., 2016), very limited attention has been paid to the discrimination power of language models when they are applied to real tasks, such as ASR and MT (Li and Khudanpur, 2008). By contrast, discriminative language modeling aims at enhancing the performance in downstream applications. For example, existing works (Roark et al., 2004, 2007) often target at improving ASR accuracy. The key motivation underlying them is that the model should be able to discriminate between “good” and “bad” sentences in a task-specific sense, instead 1183 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1183–1191 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics of just modeling grammatical ones."
D18-1150,P04-1007,0,0.677445,"how much the language model should contribute. The afore-discussed language models are generative in the sense that they merely model the joint distribution of a symbol sequence (Eq. (1)). While the research community is mostly focused on pushing the limit of PPL (e.g., Jozefowicz et al., 2016), very limited attention has been paid to the discrimination power of language models when they are applied to real tasks, such as ASR and MT (Li and Khudanpur, 2008). By contrast, discriminative language modeling aims at enhancing the performance in downstream applications. For example, existing works (Roark et al., 2004, 2007) often target at improving ASR accuracy. The key motivation underlying them is that the model should be able to discriminate between “good” and “bad” sentences in a task-specific sense, instead 1183 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1183–1191 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics of just modeling grammatical ones. The common methodology (Dikic et al., 2013) is to build a binary classifier upon hand-crafted features extracted from the sentences. However, it is not obvi"
D18-1150,P16-1159,0,0.0209091,"an in-domain 5-gram language model. This system achieves a WER of 19.17 on dev set, better than the reported 19.77 baseline in Liu et al. (2017). Based on the ASR, we repeat the same experiments in section 4.1. Tab. 5 reports WERs and PPLs on dev and test sets. Both LMLM and rLMLM outperform the other methods in WER, although their PPLs are higher. This trend is similar to that in Tab. 3. 5 Experiments on NMT In this section, we experiment the large-margin criterion trained LM with a competitive Chineseto-English NMT system. The NMT model is trained from 2M parallel sentence pairs. Following Shen et al. (2016), we use NIST 06 newswire portion (616 sentences) for development and NIST 08 newswire portion (691 sentences) for testing. We use OpenNMT-py2 package with the default configuration to train the model: batch size is 64; word embedding size is 500; dropout rate is 0.3; target vocabulary size is 50K; number of epochs is 20, after which a minimum dev perplexity of 7.72 1188 2 https://github.com/OpenNMT/OpenNMT-py is achieved. 5.1 BLEUs and PPLs We use a beam size of 10 for decoding, and report case-insensitive 4-reference BLEU-4 scores (by calling “multi bleu.perl”3 ). The NMT model achieves 35.1"
D18-1150,D16-1137,0,0.0354989,"est text sequence as a ranking problem, the aforementioned works can be considered as “pointwise” learning-to-rank approaches (Cossock and Zhang, 2008). In contrast, the proposed method is a “pairwise” approach (Liu, 2009), as it learns a neural language model by comparison between pairs of sentences. Earlier works in this fashion may date back to (Collins and Koo, 2005), which improves a semantic parser. Learning “by pairwise comparison” is also seen in several MT literatures. For example, Hopkins and May (2011) propose to train a phrase-based MT system by minimizing a pairwise ranking loss. Wiseman and Rush (2016) optimize the beam search process in a Neural Machine Translation (NMT) system. They enforce the score of a reference to be higher than that of its decoded k-th candidate by at least a unit margin. Rather than optimizing the MT system itself, this work proposes a general method of training recurrent neural language models, which can benefit various text generation tasks, including speech recognition and machine translation. 7 Conclusions We have proposed a large margin criterion for training recurrent neural language models. Rather than minimizing PPL, the proposed criterion is based on compar"
D18-1342,D17-1227,1,0.758904,"017) use beam sizes 3 and 5, respectively. Intuitively, the larger the beam size is, the more candidates it explores, and the better the translation quality should be. While this definitely holds for phrase-based MT systems, surprisingly, it is not the case for NMT: many researchers observe that translation quality degrades with beam sizes beyond 5 or 10 (Tu et al., 2017; Koehn and Knowles, 2017). We call this phenomenon the “beam search curse”, which is listed as one of the six biggest challenges for NMT (Koehn and Knowles, 2017). However, there has not been enough attention on this problem. Huang et al. (2017) hint that length ratio is the problem, but do not explain why larger beam sizes cause shorter lengths and worse BLEU. Ott et al. (2018) attribute it to two kinds of “uncertainties” in the training data, namely the copying of source sentence and the non-literal translations. However, the first problem is only found in European language datasets and the second problem occurs in all datasets but does not seem to bother pre-neural MT systems. Therefore, their explanations are not satisfactory. On the other hand, previous work adopts several heuristics to address this problem, but with various lim"
D18-1342,D13-1176,0,0.0618505,"ger than 5 hurt translation quality. We explain why this happens, and propose several methods to address this problem. Furthermore, we discuss the optimal stopping criteria for these methods. Results show that our hyperparameter-free methods outperform the widely-used hyperparameter-free heuristic of length normalization by +2.0 BLEU, and achieve the best results among all methods on Chinese-to-English translation. 1 Introduction In recent years, neural machine translation (NMT) has surpassed traditional phrase-based or syntaxbased machine translation, becoming the new state of the art in MT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). While NMT training is typically done in a “local” fashion which does not employ any search (bar notable exceptions such as Ranzato et al. (2016), Shen et al. (2016), and Wiseman and Rush (2016)), the decoding phase of all NMT systems universally adopts beam search, a widely used heuristic, to improve translation quality. Unlike phrase-based MT systems which enjoy the benefits of very large beam sizes (in the order of 100–500) (Koehn et al., 2007) , most NMT systems choose tiny beam sizes up to 5; for example, Google’s GNMT (Wu et al., 2016) and"
D18-1342,P17-4012,0,0.0624883,"Missing"
D18-1342,P07-2045,0,0.00616604,"hrase-based or syntaxbased machine translation, becoming the new state of the art in MT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). While NMT training is typically done in a “local” fashion which does not employ any search (bar notable exceptions such as Ranzato et al. (2016), Shen et al. (2016), and Wiseman and Rush (2016)), the decoding phase of all NMT systems universally adopts beam search, a widely used heuristic, to improve translation quality. Unlike phrase-based MT systems which enjoy the benefits of very large beam sizes (in the order of 100–500) (Koehn et al., 2007) , most NMT systems choose tiny beam sizes up to 5; for example, Google’s GNMT (Wu et al., 2016) and Facebook’s ConvS2S (Gehring et al., 2017) use beam sizes 3 and 5, respectively. Intuitively, the larger the beam size is, the more candidates it explores, and the better the translation quality should be. While this definitely holds for phrase-based MT systems, surprisingly, it is not the case for NMT: many researchers observe that translation quality degrades with beam sizes beyond 5 or 10 (Tu et al., 2017; Koehn and Knowles, 2017). We call this phenomenon the “beam search curse”, which is lis"
D18-1342,P16-1159,0,0.171356,"er-free methods outperform the widely-used hyperparameter-free heuristic of length normalization by +2.0 BLEU, and achieve the best results among all methods on Chinese-to-English translation. 1 Introduction In recent years, neural machine translation (NMT) has surpassed traditional phrase-based or syntaxbased machine translation, becoming the new state of the art in MT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). While NMT training is typically done in a “local” fashion which does not employ any search (bar notable exceptions such as Ranzato et al. (2016), Shen et al. (2016), and Wiseman and Rush (2016)), the decoding phase of all NMT systems universally adopts beam search, a widely used heuristic, to improve translation quality. Unlike phrase-based MT systems which enjoy the benefits of very large beam sizes (in the order of 100–500) (Koehn et al., 2007) , most NMT systems choose tiny beam sizes up to 5; for example, Google’s GNMT (Wu et al., 2016) and Facebook’s ConvS2S (Gehring et al., 2017) use beam sizes 3 and 5, respectively. Intuitively, the larger the beam size is, the more candidates it explores, and the better the translation quality should be. While th"
D18-1342,D16-1248,0,0.0224898,"are the n-gram precisions, and |y |and |y∗ |denote the hypothesis and reference lengths, while bp is the brevity penalty (penalizing short 20 40 60 80 beam size 100 0.70 Figure 1: As beam size increases beyond 3, BLEU score on the dev set gradually drops. All terms are calculated by multi-bleu.pl. 40 3rd &lt;/eos&gt; 2nd &lt;/eos&gt; 1st &lt;/eos&gt; 35 30 25 20 15 10 5 0 50 100 150 200 beam size 250 300 350 Figure 2: Searching algorithm with larger beams generates &lt;/eos&gt; earlier. We use the average first, second and third &lt;/eos&gt; positions on the dev set as an example. translations) and lr is the length ratio (Shi et al., 2016; Koehn and Knowles, 2017), respectively. With beam size increasing, |y |decrases, which causes the length ratio to drop, as shown in Fig. 1. Then the brevity penalty term, as a function of the length ratio, decreases even more severely. Since bp is a key factor in BLEU, this explains why the beam search curse happens.1 The reason why |y |decreases as beam size increases is actually twofold: 1. As beam size increases, the more candidates it could explore. Therefore, it becomes easier for the search algorithm to find the &lt;/eos&gt; symbol. Fig. 2 shows that the &lt;/eos&gt; indices decrease steadily with"
D18-1342,D16-1137,0,0.0706439,"rm the widely-used hyperparameter-free heuristic of length normalization by +2.0 BLEU, and achieve the best results among all methods on Chinese-to-English translation. 1 Introduction In recent years, neural machine translation (NMT) has surpassed traditional phrase-based or syntaxbased machine translation, becoming the new state of the art in MT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). While NMT training is typically done in a “local” fashion which does not employ any search (bar notable exceptions such as Ranzato et al. (2016), Shen et al. (2016), and Wiseman and Rush (2016)), the decoding phase of all NMT systems universally adopts beam search, a widely used heuristic, to improve translation quality. Unlike phrase-based MT systems which enjoy the benefits of very large beam sizes (in the order of 100–500) (Koehn et al., 2007) , most NMT systems choose tiny beam sizes up to 5; for example, Google’s GNMT (Wu et al., 2016) and Facebook’s ConvS2S (Gehring et al., 2017) use beam sizes 3 and 5, respectively. Intuitively, the larger the beam size is, the more candidates it explores, and the better the translation quality should be. While this definitely holds for phras"
D18-1342,1983.tc-1.13,0,0.774679,"Missing"
D18-1342,W17-3204,0,0.171404,"oy the benefits of very large beam sizes (in the order of 100–500) (Koehn et al., 2007) , most NMT systems choose tiny beam sizes up to 5; for example, Google’s GNMT (Wu et al., 2016) and Facebook’s ConvS2S (Gehring et al., 2017) use beam sizes 3 and 5, respectively. Intuitively, the larger the beam size is, the more candidates it explores, and the better the translation quality should be. While this definitely holds for phrase-based MT systems, surprisingly, it is not the case for NMT: many researchers observe that translation quality degrades with beam sizes beyond 5 or 10 (Tu et al., 2017; Koehn and Knowles, 2017). We call this phenomenon the “beam search curse”, which is listed as one of the six biggest challenges for NMT (Koehn and Knowles, 2017). However, there has not been enough attention on this problem. Huang et al. (2017) hint that length ratio is the problem, but do not explain why larger beam sizes cause shorter lengths and worse BLEU. Ott et al. (2018) attribute it to two kinds of “uncertainties” in the training data, namely the copying of source sentence and the non-literal translations. However, the first problem is only found in European language datasets and the second problem occurs in"
D18-1342,W18-6322,0,0.198984,"here is still a hyperparameter r (word reward), so we design two methods below to remove it. 4.2.2 Bounded Adaptive-Reward We propose Bounded Adaptive-Reward to automatically calculate proper reward based on the current beam. With beam size b, the reward for time step t is the average negative log-probability of the words in the current beam. rt = −(1/b) Pb i=1 log p(wordi ) (10) Its score is very similar to (7): P ∗ SˆAdaR (x, y) = S (x, y) + L t=1 rt 4.2.3 (11) BP-Norm Inspired by the BLEU score definition, we propose BP-Norm method as follows: Sˆbp (x, y) = log bp + S (x, y)/|y| (6) (12) 3 Murray and Chiang (2018) attribute the fact that beam search prefers shorter candidates to the label bias problem (Lafferty et al., 2001) due to NMT’s local normalization. bp is the same brevity penalty term as in (3). Here, we regard our predicted length as the reference 3056 40 length. The beauty of this method appears when we drop the logarithmic symbol in (12): Q|y| i=1 p(yi |...) 1/|y| BLEU score exp(Sˆbp (x, y)) = bp · 39   1 P|y| = bp ·exp log p(y |...) i |y |i=1 36 34 0 Stopping Criteria By default, OpenNMT-py (Klein et al., 2017) stops when the topmost beam candidate stops, because there will not be any f"
D18-1342,P02-1040,0,0.102452,"Missing"
D18-1357,N03-1003,0,0.124341,"we further propose a framework to generate many more pseudo-references automatically. In particular, we design a neural multiple-sequence alignment algo3188 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3188–3197 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics rithm to compress all existing human references into a lattice by merging similar words across different references (see examples in Fig. 1); this can be viewed as a modern, neural version of paraphrasing with multiple-sequence alignment (Barzilay and Lee, 2003, 2002). We can then generate theoretically exponentially more references from the lattice. We make the following main contributions: • Firstly, we investigate three different methods for multi-reference training on both MT and image captioning tasks (Section 2). • Secondly, we propose a novel neural network-based multiple sequence alignment model to compress the existing references into lattices. By traversing these lattices, we generate exponentially many new pseudoreferences (Section 3). • We report substantial improvements over strong baselines in both MT (+1.5 BLEU) and image captioning ("
D18-1357,D17-1227,1,0.732591,"ation and image captioning. 4.1 Machine Translation We evaluate our approach on NIST Chinese-toEnglish translation dataset which consists of 1M pairs of single reference data and 5974 pairs of 4 reference data (NIST 2002, 2003, 2004, 2005, 2006, 2008). Table 1 shows the statistics of this dataset. We first pre-train our model on a 1M pairs single reference dataset and then train on the NIST 2002, 2003, 2004, 2005. We use the NIST 2006 We employ byte-pair encoding (BPE) (Sennrich et al., 2015) which reduces the source and target language vocabulary sizes to 18k and 10k. We adopt length reward (Huang et al., 2017) to find optimal sentence length. We use a two layer bidirectional LSTM as the encoder and a two layer LSTM as the decoder. We perform pre-training for 20 epochs to minimize perplexity on the 1M dataset, with a batch size of 64, word embedding size of 500, beam size of 15, learning rate of 0.1, learning rate decay of 0.5 and dropout rate of 0.3. We then train the model in 30 epochs and use the best batch size among 100, 200, 400 for each update method. These batch sizes are multiple of the number of references used in experiments, so it is guaranteed that all the references of one single examp"
D18-1357,E17-1096,0,0.0206625,"he semantic similarity of each word pairs in context. Formally, given a sentence pair yi and yj , we build a semantic substitution matrix M = R|yi |×|yj |, whose cell Mu,v represents the similarity score between word yi,u and word yj,v . We propose a new neural network-based multiple sequence alignment algorithm to take context into consideration. We first build a language model (LM) to obtain the semantic representation of each word, then these word representations are used to construct the semantic substitution matrix between sentences. Fig. 3 shows the architecture of the bidirectional LM (Mousa and Schuller, 2017). The optimization goal of our LM is to minimize the ith word’s prediction error given the surrounding word’s hidden state: −−→ ←−− p(wi |hi−1 ⊕ hi+1 ) (1) For any new given sentences, we concatenate both forward and backward hidden states to represent each word yi,u in a sentence yi . We then calculate the normalized cosine similarity score of word yi,u and yj,v as: − → ← − − → ← − Mu,v = cosine(hu ⊕ hu , hv ⊕ hv ) (2) Measuring Word Similarity in Context To tackle the above listed two problems of hard alignment, we need to identify synonyms and words with similar meanings. Barzilay and Lee ("
D18-1357,D14-1162,0,0.0816961,"lyzes the number and quality of generated references using our proposed approach. We set the global penalty as 0.9 and only calculate the top 50 generated references for the average BLEU analysis. From the figure, we can see that when the sentence length grows, the number of generated references grows exponentially. To generate enough references for the following experiments, we set an initial global penalty as 0.9 and gradually decrease it by 0.05 until we collect no less than 100 references. We train a bidirectional language model on the pre-training dataset and training dataset with Glove (Pennington et al., 2014) word embedding size of 300 dimension, for 20 epochs to minimize the perplexity 103 0.950 0.925 102 0.900 0.875 101 0.850 0.825 20 40 60 Avg. Length of Original References 1.000 0.975 6 × 102 0.950 4 × 102 0.925 3 × 102 0.900 2 × 102 0.875 0.850 0.825 Avg. # of Generated Referneces Avg. BLEU of Generated Referneces (a) Machine Translation Dataset 7.5 10.0 12.5 15.0 102 17.5 Avg. Length of Original References (b) Image Captioning Dataset Figure 6: Analysis of generated references K 0 . For those examples generating k pseudoreferences and k &gt; K 0 , we calculate all pseudoreferences’ BLEU scores"
D18-1357,D15-1044,0,0.0340386,"ersing them to generate new pseudo-references. These approaches lead to substantial improvements over strong baselines in both machine translation (+1.5 BLEU) and image captioning (+3.1 BLEU / +11.7 CIDEr). 1 Introduction Neural text generation has attracted much attention in recent years thanks to its impressive generation accuracy and wide applicability. In addition to demonstrating compelling results for machine translation (MT) (Sutskever et al., 2014; Bahdanau et al., 2014), by simple adaptation, practically very same or similar models have also proven to be successful for summarization (Rush et al., 2015; Nallapati et al., 2016) and image or video captioning (Venugopalan et al., 2015; Xu et al., 2015a). The most common neural text generation model is based on the encoder-decoder framework (Sutskever et al., 2014) which generates a variable-length output sequence using an RNNbased decoder with attention mechanisms (Bahdanau et al., 2014; Xu et al., 2015b). There are many recent efforts in improving the generation accuracy, e.g., ConvS2S (Gehring et al., 2017) and Transformer (Vaswani et al., 2017). However, all these efforts are limited to training with a single reference even when multiple re"
D18-1357,N12-1017,0,\N,Missing
D18-1357,P16-1162,0,\N,Missing
D18-1460,P05-1033,0,0.126753,"target hidden state sj is given by   ∗ sj = f eyj−1 , sj−1 , cj (6) The probability distribution Dj over all the words in the target vocabulary is predicted conditioned on the previous ground truth words, the context vector cj and the unrolled target information sj .   ∗ tj = g eyj−1 , cj , sj (7) oj = Wo tj (8) Dj = softmax (oj ) (9) where g stands for a linear transformation, Wo is used to map tj to oj so that each target word has one corresponding dimension in oj . 2.2 Cube Pruning The cube pruning algorithm, proposed by Chiang (2007) based on the k-best parsing algorithm of Huang and Chiang (2005), is actually an accelerated extension based on the naive beam search algorithm. Beam search, a heuristic dynamic programming searching algorithm, explores a graph by expanding the most promising nodes in a limited set and searches approximate optimal results from candidates. For the sequence-to-sequence learning task, given a pre-trained model, the beam search algorithm finds a sequence that approximately maximizes the conditional probability (Graves, 2012; Boulanger-Lewandowski et al., 2013). Both Sutskever et al. (2014) and Bahdanau et al. (2015) employed the beam search algorithm into the"
D18-1460,W14-4012,0,0.15052,"Missing"
D18-1460,D14-1179,0,0.0310691,"Missing"
D18-1460,P14-1129,0,0.0999653,"Missing"
D18-1460,P06-1121,0,0.0451297,"owski et al., 2013). Both Sutskever et al. (2014) and Bahdanau et al. (2015) employed the beam search algorithm into the NMT decoding to produce translations with relatively larger conditional probability with respect to the optimized model parameters. Remarkably, Huang and Chiang (2007) successfully applied the cube pruning algorithm to the decoding of SMT. They found that the beam search algorithm in SMT can be extended, and they utilized the cube pruning and some variants to optimize the search process in the decoding phase of phrase-based (Och and Ney, 2004) and syntaxbased (Chiang, 2005; Galley et al., 2006) systems, 4285 n) 0.1 0.2 1.1 0.1 0.2 1.1 0.1 0.2 1.1 3.2 2.1 2.2 2.3 3.2 2.1 2.2 2.3 3.2 2.1 2.2 2.3 3.2 (NP1,2 : The airplane) 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 3.5 3.6 4.5 3.4 3.5 3.6 4.5 3.4 3.5 3.6 4.5 3.4 3.5 3.6 4.5 (a) (b) (c) do w 4 3, P (V 1.1 2.3 3.4 ro ps off ) :t 3, 0.2 2.2 (NP1,2 : The apple) :d off ) ak es oo k :t 4 P 3, P (V (V 4 3, P (V 4 ) ps ro :d :t 4 3, P P (V (V off off ) ak es oo k :t 4 3, 4 3, P (V do w n) n) ) :d :t 4 3, ro ps off ) off ak es k oo :t 4 P 3, P (V (V 4 3, P (V do w n) do w ) ro :d :t 4 3, ps off off ) ak es oo k :t 4 3, P (V"
D18-1460,P17-1012,0,0.0219113,"translation to speed up the translation. To construct the equivalence class, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less softmax operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by 3.3× on GPUs and 3.5× on CPUs. 1 Introduction Neural machine translation (NMT) has shown promising results and drawn more attention recently (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Bahdanau et al., 2015; Gehring et al., 2017a,b; Vaswani et al., 2017). A widely used architecture is the attention-based encoder-decoder framework (Cho et al., 2014b; Bahdanau et al., 2015) which assumes there is a common semantic space between the source and target language pairs. The encoder encodes the source sentence to a representation in the common space with the recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997) and the decoder decodes this representation to generate the target sentence word by word. To generate a target word, a probability distribution over the target vocabulary is drawn based on the attention ov"
D18-1460,P82-1020,0,0.808202,"Missing"
D18-1460,2015.mtsummit-papers.23,0,0.737459,"Missing"
D18-1460,W05-1506,1,0.519405,"step, the target hidden state sj is given by   ∗ sj = f eyj−1 , sj−1 , cj (6) The probability distribution Dj over all the words in the target vocabulary is predicted conditioned on the previous ground truth words, the context vector cj and the unrolled target information sj .   ∗ tj = g eyj−1 , cj , sj (7) oj = Wo tj (8) Dj = softmax (oj ) (9) where g stands for a linear transformation, Wo is used to map tj to oj so that each target word has one corresponding dimension in oj . 2.2 Cube Pruning The cube pruning algorithm, proposed by Chiang (2007) based on the k-best parsing algorithm of Huang and Chiang (2005), is actually an accelerated extension based on the naive beam search algorithm. Beam search, a heuristic dynamic programming searching algorithm, explores a graph by expanding the most promising nodes in a limited set and searches approximate optimal results from candidates. For the sequence-to-sequence learning task, given a pre-trained model, the beam search algorithm finds a sequence that approximately maximizes the conditional probability (Graves, 2012; Boulanger-Lewandowski et al., 2013). Both Sutskever et al. (2014) and Bahdanau et al. (2015) employed the beam search algorithm into the"
D18-1460,P07-1019,1,0.863396,"employed to avoid unknown words (UNKs), which leads to a large number of normalization factors for the softmax operation when drawing the probability distribution. To accelerate the translation, the widely used method is to trade off between the translation quality and the decoding speed by reducing the size of vocabulary (Mi et al., 2016a) or/and the number of parameters, which can not realize the full potential of NMT. In this paper, we borrow ideas from phrasebased and syntax-based machine translation where cube pruning has been successfully applied to speed up the decoding (Chiang, 2007; Huang and Chiang, 2007). Informally, cube pruning “coarsens” the search space by clustering similar states according to some equivalence relations. To apply this idea to NMT, however, is much more involved. Specifically, in the process of beam search, we cluster similar target hidden states to construct equivalence classes, the three dimensions of which are target words in the target vocabulary, part translations retained in the beam search and different combinations of similar target hidden states, respectively. The clustering operation 4284 Proceedings of the 2018 Conference on Empirical Methods in Natural Languag"
D18-1460,P15-1001,0,0.0577112,"Missing"
D18-1460,W13-3214,0,0.0226871,"a popular technique to speed up dynamic programming, into neural machine translation to speed up the translation. To construct the equivalence class, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less softmax operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by 3.3× on GPUs and 3.5× on CPUs. 1 Introduction Neural machine translation (NMT) has shown promising results and drawn more attention recently (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Bahdanau et al., 2015; Gehring et al., 2017a,b; Vaswani et al., 2017). A widely used architecture is the attention-based encoder-decoder framework (Cho et al., 2014b; Bahdanau et al., 2015) which assumes there is a common semantic space between the source and target language pairs. The encoder encodes the source sentence to a representation in the common space with the recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997) and the decoder decodes this representation to generate the target sentence word by word. To generate a target word, a probability distributi"
D18-1460,D16-1096,0,0.0152537,"rd requires extensive computation to go through all the source words to calculate the attention. Worse still, due to the recurrence of RNNs, target words can only be generated sequentially rather than in parallel. The second reason is that large vocabulary on target side is employed to avoid unknown words (UNKs), which leads to a large number of normalization factors for the softmax operation when drawing the probability distribution. To accelerate the translation, the widely used method is to trade off between the translation quality and the decoding speed by reducing the size of vocabulary (Mi et al., 2016a) or/and the number of parameters, which can not realize the full potential of NMT. In this paper, we borrow ideas from phrasebased and syntax-based machine translation where cube pruning has been successfully applied to speed up the decoding (Chiang, 2007; Huang and Chiang, 2007). Informally, cube pruning “coarsens” the search space by clustering similar states according to some equivalence relations. To apply this idea to NMT, however, is much more involved. Specifically, in the process of beam search, we cluster similar target hidden states to construct equivalence classes, the three dimen"
D18-1460,P16-2021,0,0.0190433,"rd requires extensive computation to go through all the source words to calculate the attention. Worse still, due to the recurrence of RNNs, target words can only be generated sequentially rather than in parallel. The second reason is that large vocabulary on target side is employed to avoid unknown words (UNKs), which leads to a large number of normalization factors for the softmax operation when drawing the probability distribution. To accelerate the translation, the widely used method is to trade off between the translation quality and the decoding speed by reducing the size of vocabulary (Mi et al., 2016a) or/and the number of parameters, which can not realize the full potential of NMT. In this paper, we borrow ideas from phrasebased and syntax-based machine translation where cube pruning has been successfully applied to speed up the decoding (Chiang, 2007; Huang and Chiang, 2007). Informally, cube pruning “coarsens” the search space by clustering similar states according to some equivalence relations. To apply this idea to NMT, however, is much more involved. Specifically, in the process of beam search, we cluster similar target hidden states to construct equivalence classes, the three dimen"
D18-1460,J04-4002,0,0.0733736,"tional probability (Graves, 2012; Boulanger-Lewandowski et al., 2013). Both Sutskever et al. (2014) and Bahdanau et al. (2015) employed the beam search algorithm into the NMT decoding to produce translations with relatively larger conditional probability with respect to the optimized model parameters. Remarkably, Huang and Chiang (2007) successfully applied the cube pruning algorithm to the decoding of SMT. They found that the beam search algorithm in SMT can be extended, and they utilized the cube pruning and some variants to optimize the search process in the decoding phase of phrase-based (Och and Ney, 2004) and syntaxbased (Chiang, 2005; Galley et al., 2006) systems, 4285 n) 0.1 0.2 1.1 0.1 0.2 1.1 0.1 0.2 1.1 3.2 2.1 2.2 2.3 3.2 2.1 2.2 2.3 3.2 2.1 2.2 2.3 3.2 (NP1,2 : The airplane) 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 3.5 3.6 4.5 3.4 3.5 3.6 4.5 3.4 3.5 3.6 4.5 3.4 3.5 3.6 4.5 (a) (b) (c) do w 4 3, P (V 1.1 2.3 3.4 ro ps off ) :t 3, 0.2 2.2 (NP1,2 : The apple) :d off ) ak es oo k :t 4 P 3, P (V (V 4 3, P (V 4 ) ps ro :d :t 4 3, P P (V (V off off ) ak es oo k :t 4 3, 4 3, P (V do w n) n) ) :d :t 4 3, ro ps off ) off ak es k oo :t 4 P 3, P (V (V 4 3, P (V do w n) do w"
D18-1460,P02-1040,0,0.104328,"lish (ZhEn) translation task. 4.1 Data Preparation The Chinese-English training dataset consists of 1.25M sentence pairs3 . We used the NIST 2002 (MT02) dataset as the validation set with 878 sentences, and the NIST 2003 (MT03) dataset as the test dataset, which contains 919 sentences. The lengths of the sentences on both sides were limited up to 50 tokens, then actually 1.11M sentence pairs were left with 25.0M Chinese words and 27.0M English words. We extracted 30k most frequent words as the source and target vocabularies for both sides. In all the experiments, case-insensitive 4-gram BLEU (Papineni et al., 2002) was employed for the automatic evaluation, we used the script mteval-v11b.pl4 to calculate the BLEU score. 4.2 System The system is an improved version of attentionbased NMT system named RNNsearch (Bahdanau et al., 2015) where the decoder employs a conditional GRU layer with attention, consisting of two GRUs and an attention module for each step5 . Specifically, Equation (6) is replaced with the following two equations: ∗ s˜j = GRU1 (eyj−1 , sj−1 ) (13) sj = GRU2 (cj , s˜j ) (14) Besides, for the calculation of relevance in Equation (4), sj−1 is replaced with s˜j−1 . The other components of t"
D18-1460,1983.tc-1.13,0,0.635263,"Missing"
D19-1137,D18-1337,0,0.569519,"et al., 2013; Bangalore et al., 2012; Fügen et al., 2007; Sridhar ∗ These authors contributed equally. zrenj11@gmail.com et al., 2013; Jaitly et al., 2016; Graves et al., 2013). Recent simultaneous translation research focuses on obtaining a strategy, called a policy, to decide whether to wait for another source word (READ action) or emit a target word (WRITE action). The obtained policies fall into two main categories: (1) fixed-latency policies (Ma et al., 2019; Dalvi et al., 2018) and (2) context-dependent adaptive policies (Grissom II et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Alinejad et al., 2018; Arivazhagan et al., 2019; Zheng et al., 2019a). As an example of fixed-latency policies, wait-? (Ma et al., 2019) starts by waiting for the first few source words and then outputs one target word after receiving each new source word until the source sentence ends. It is easy to see that this kind of policy will inevitably need to guess the future content, which can often be incorrect. Thus, an adaptive policy (see Table 1 as an example), which can decides on the fly whether to take READ action or WRITE action, is more desirable for simultaneous translation. Moreover, the widely-used beam sea"
D19-1137,P19-1126,0,0.492816,"viding lower latency; and with smaller ?, the policy prefers to take READ actions, and provides more conservative translation with larger latency. 27 26 BLEU given sequence ?≤? . Then the generating process can be summarized as Algorithm 1. Although we can generate action sequences balancing the two wanted properties with appropriate value of parameter ?, the latency of generated action sequence may still be large due to the word order difference between the two sentences. To avoid this issue, we filter the generated sequences with the latency metric Average Lagging (AL) proposed by Ma et al. (2019), which quantifies the latency in terms of the number of source words and avoids some flaws of other metrics like Average Proportion (AP) (Cho and Esipova, 2016) and Consecutive Wait (CW) (Gu et al., 2017). Another issue we observed is that, the pre-trained model may be too aggressive for some sentence pair, meaning that it may write all target words without seeing the whole source sentence. This may be because the model is also trained on the same dataset. To overcome this, we only keep the action sequences that will receive all the source words before the last WRITE action. After the filteri"
D19-1137,N18-2079,0,0.169925,"se. Researchers previously study simultaneous translation as a part of real-time speech-to-speech translation system (Yarmohammadi et al., 2013; Bangalore et al., 2012; Fügen et al., 2007; Sridhar ∗ These authors contributed equally. zrenj11@gmail.com et al., 2013; Jaitly et al., 2016; Graves et al., 2013). Recent simultaneous translation research focuses on obtaining a strategy, called a policy, to decide whether to wait for another source word (READ action) or emit a target word (WRITE action). The obtained policies fall into two main categories: (1) fixed-latency policies (Ma et al., 2019; Dalvi et al., 2018) and (2) context-dependent adaptive policies (Grissom II et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Alinejad et al., 2018; Arivazhagan et al., 2019; Zheng et al., 2019a). As an example of fixed-latency policies, wait-? (Ma et al., 2019) starts by waiting for the first few source words and then outputs one target word after receiving each new source word until the source sentence ends. It is easy to see that this kind of policy will inevitably need to guess the future content, which can often be incorrect. Thus, an adaptive policy (see Table 1 as an example), which can decides on the"
D19-1137,D14-1140,0,0.295885,"Missing"
D19-1137,E17-1099,0,0.474649,"tem (Yarmohammadi et al., 2013; Bangalore et al., 2012; Fügen et al., 2007; Sridhar ∗ These authors contributed equally. zrenj11@gmail.com et al., 2013; Jaitly et al., 2016; Graves et al., 2013). Recent simultaneous translation research focuses on obtaining a strategy, called a policy, to decide whether to wait for another source word (READ action) or emit a target word (WRITE action). The obtained policies fall into two main categories: (1) fixed-latency policies (Ma et al., 2019; Dalvi et al., 2018) and (2) context-dependent adaptive policies (Grissom II et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Alinejad et al., 2018; Arivazhagan et al., 2019; Zheng et al., 2019a). As an example of fixed-latency policies, wait-? (Ma et al., 2019) starts by waiting for the first few source words and then outputs one target word after receiving each new source word until the source sentence ends. It is easy to see that this kind of policy will inevitably need to guess the future content, which can often be incorrect. Thus, an adaptive policy (see Table 1 as an example), which can decides on the fly whether to take READ action or WRITE action, is more desirable for simultaneous translation. Moreover, t"
D19-1137,D15-1006,0,0.0869305,"nces In this section, we show how to generate action sequences for parallel text. Our simultaneous translation policy can take two actions: READ (receive a new source word) and WRITE (output a new target word). A sequence of such actions for a sentence pair (?, ?) defines one way to translate ? into ?. Thus, such a sequence must have |? |number of WRITE actions. However, not every action sequence is good for simultaneous translation. For instance, a sequence without any READ action will not provide any source information, while a sequence with all |? |number of READ actions be1 Previous work (He et al., 2015; Niehues et al., 2018) shows that carefully generated parallel training data can help improve simultaneous MT or low-latency speech translation. This is different from our work in that we generates action sequences for training policy model instead of parallel translation data for MT model. fore all WRITE actions usually has large latency. Thus the ideal sequences for simultaneous translation should have the following two properties: • there is no anticipation during translation, i.e. when choosing WRITE action, there is enough source information for the MT model to generate the correct targe"
D19-1137,P17-4012,0,0.0312141,"l-sentence translation model, ■: RL with CW = 2, ▾: RL with CW = 5, ▴: RL with CW = 8, ×: WID for ?0 ∈ {2, 4, 6} and ? ∈ {2, 4}, +: WIW for ?0 ∈ {2, 4, 6} and ? ∈ {1, 2}. WMT 15 for training, newstest-2013 for validation and newstest-2015 for testing.4 All datasets are tokenized and segmented into sub-word units with byte-pair encoding (BPE) (Sennrich et al., 2016), and we only use the sentence pairs of lengths less than 50 (on both sides) for training. Model Configuration We use Transformer-base (Vaswani et al., 2017) as our NMT model and our implementation is based on PyTorch-based OpenNMT (Klein et al., 2017). We add an &lt;eos&gt; token on the source side, which is not included in the original OpenNMT codebase. Our recurrent policy model consists of one GRU layer with 512 units, one fully-connected layer of dimension 64 followed by ReLU activation, and one fully-connected layer of dimension 2 followed by a softmax function to produce the action distribution. We use BLEU (Papineni et al., 2002) as the translation quality metric and Averaged Lagging (AL) (Ma et al., 2019) as the latency metric. Effects of Generated Action Sequences We first analyze the effects of the two parameters in the generation proc"
D19-1137,P02-1040,0,0.105657,"the sentence pairs of lengths less than 50 (on both sides) for training. Model Configuration We use Transformer-base (Vaswani et al., 2017) as our NMT model and our implementation is based on PyTorch-based OpenNMT (Klein et al., 2017). We add an &lt;eos&gt; token on the source side, which is not included in the original OpenNMT codebase. Our recurrent policy model consists of one GRU layer with 512 units, one fully-connected layer of dimension 64 followed by ReLU activation, and one fully-connected layer of dimension 2 followed by a softmax function to produce the action distribution. We use BLEU (Papineni et al., 2002) as the translation quality metric and Averaged Lagging (AL) (Ma et al., 2019) as the latency metric. Effects of Generated Action Sequences We first analyze the effects of the two parameters in the generation process of action sequences: the rank ? and the filtering latency ?. We fix ? = 3 and choose the rank ? ∈ {5, 50}; then we fix ? = 50 and choose the latency ? ∈ {3, 7, ∞} to generate action sequences on DE→EN direction.Figure 1 shows the performances of resulting models with different probability thresholds ?. We find that smaller ? helps achieve better performance and our model is not ve"
D19-1137,P16-1162,0,0.0930881,"ults of greedy decoding (solid shapes) and beam search (empty shapes, beam-size = 5). ◆: wait-? models for ? ∈ {1, 2, 3, 4, 5, 6}, ●: test-time wait-? for ? ∈ {1, 2, 3, 4, 5, 6}, ◂: our SL model with threshold ? ∈ {0.65, 0.6, 0.55, 0.5, 0.45, 0.4}, ★: full-sentence translation model, ■: RL with CW = 2, ▾: RL with CW = 5, ▴: RL with CW = 8, ×: WID for ?0 ∈ {2, 4, 6} and ? ∈ {2, 4}, +: WIW for ?0 ∈ {2, 4, 6} and ? ∈ {1, 2}. WMT 15 for training, newstest-2013 for validation and newstest-2015 for testing.4 All datasets are tokenized and segmented into sub-word units with byte-pair encoding (BPE) (Sennrich et al., 2016), and we only use the sentence pairs of lengths less than 50 (on both sides) for training. Model Configuration We use Transformer-base (Vaswani et al., 2017) as our NMT model and our implementation is based on PyTorch-based OpenNMT (Klein et al., 2017). We add an &lt;eos&gt; token on the source side, which is not included in the original OpenNMT codebase. Our recurrent policy model consists of one GRU layer with 512 units, one fully-connected layer of dimension 64 followed by ReLU activation, and one fully-connected layer of dimension 2 followed by a softmax function to produce the action distributi"
D19-1137,N13-1023,0,0.395617,"Missing"
D19-1137,P19-1582,1,0.640676,"l., 2007; Sridhar ∗ These authors contributed equally. zrenj11@gmail.com et al., 2013; Jaitly et al., 2016; Graves et al., 2013). Recent simultaneous translation research focuses on obtaining a strategy, called a policy, to decide whether to wait for another source word (READ action) or emit a target word (WRITE action). The obtained policies fall into two main categories: (1) fixed-latency policies (Ma et al., 2019; Dalvi et al., 2018) and (2) context-dependent adaptive policies (Grissom II et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Alinejad et al., 2018; Arivazhagan et al., 2019; Zheng et al., 2019a). As an example of fixed-latency policies, wait-? (Ma et al., 2019) starts by waiting for the first few source words and then outputs one target word after receiving each new source word until the source sentence ends. It is easy to see that this kind of policy will inevitably need to guess the future content, which can often be incorrect. Thus, an adaptive policy (see Table 1 as an example), which can decides on the fly whether to take READ action or WRITE action, is more desirable for simultaneous translation. Moreover, the widely-used beam search technique become non-trivial for fixed pol"
D19-1137,D19-1144,1,0.731753,"l., 2007; Sridhar ∗ These authors contributed equally. zrenj11@gmail.com et al., 2013; Jaitly et al., 2016; Graves et al., 2013). Recent simultaneous translation research focuses on obtaining a strategy, called a policy, to decide whether to wait for another source word (READ action) or emit a target word (WRITE action). The obtained policies fall into two main categories: (1) fixed-latency policies (Ma et al., 2019; Dalvi et al., 2018) and (2) context-dependent adaptive policies (Grissom II et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Alinejad et al., 2018; Arivazhagan et al., 2019; Zheng et al., 2019a). As an example of fixed-latency policies, wait-? (Ma et al., 2019) starts by waiting for the first few source words and then outputs one target word after receiving each new source word until the source sentence ends. It is easy to see that this kind of policy will inevitably need to guess the future content, which can often be incorrect. Thus, an adaptive policy (see Table 1 as an example), which can decides on the fly whether to take READ action or WRITE action, is more desirable for simultaneous translation. Moreover, the widely-used beam search technique become non-trivial for fixed pol"
D19-1144,P19-1126,0,0.13857,"Missing"
D19-1144,N18-2079,0,0.330434,"tive where topb (·) returns the top-scoring b pairs, and ◦ is the string concatenation operator. Now Bt = nextb1 (Bt−1 ). As a shorthand, we also define the multi-step beam search function recursively: nextbi (B) = nextb1 (nextbi−1 (B)) 2.2 RL MILk (Gu et al., 2017) (Arivazhagan et al., 2019) Supervised Learning Imitation Learning (Zheng et al., 2019a) (Zheng et al., 2019b) In terms of modeling (which is orthogonal to decoding policies), we can also divide most simultaneous translatoin efforts into two camps: 1. Use the standard full-sentence translation model trained by classical seq-to-seq (Dalvi et al., 2018; Gu et al., 2017; Zheng et al., 2019a). For example, the “test-time waitk” scheme (Ma et al., 2019a) uses the full-sentence translation model and performs wait-k decoding at test time. However, the obvious training-testing mismatch in this scheme usually leads to inferior quality. (2) (3) Simultaneous MT: Policies and Models 2. Use a genuinely simultaneous model trained by the recently proposed prefix-to-prefix framework (Ma et al., 2019a; Arivazhagan et al., 2019; Zheng et al., 2019b). There is no training-testing mismatch in this new scheme, with the cost of slower training. There are two m"
D19-1144,D14-1140,0,0.35836,"Missing"
D19-1144,E17-1099,0,0.134174,"lation. At a very high level, to generate a single word, instead of simply choosing the highest-scoring one (as in greedy search), we further speculate w steps into the future, and use the ranking at step w + 1 to reach a more informed decision for step 1 (the current step); this method implicitly benefits from a target language model, alleviating the label bias problem in neural generation (Murray and Chiang, 2018; Ma et al., 2019b). We apply this algorithm to two representative approaches to simultaneous translation: the fixed policy method (Ma et al., 2019a) and the adaptive policy method (Gu et al., 2017). In both cases, we show that SBS improves translation quality while maintaining latency (i.e., simultaneity). 2 Preliminaries We first review standard full-sentence NMT and beam search to set up the notations, and then review different approaches to simultaneous MT. 2.1 Full Sentence NMT and Beam Search The encoder processes the input sequence x = (x1 , ..., xn ), where xi ∈ Rd represents an input token as a d dimensional vector, and produces a new list of hidden states h = f (x) = (h1 , ..., hn ) 1395 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and"
D19-1144,D17-1227,1,0.856187,"a tail beam search on the remaining target words, but beam search is seemingly impossible before the source sentence ends. et al., 2018; Ma et al., 2019a) 2. The second method learns an adaptive policy which uses either supervised (Zheng et al., 2019a) or reinforcement learning (Grissom II et al., 2014; Gu et al., 2017) to decide whether to READ (the next source word) or WRITE (the next target word) . Here the decoder can commit a chunk of multiple words for a series of consecutive WRITEs. Full-sentence beam search (over a maximum of T steps) yields the best hypothesis y ∗ with score s∗ (see Huang et al. (2017) for stopping criteria): hy ∗ , s∗ i = top1 (nextbT ([h&lt;s&gt;, 1i])) prefix-to-prefix (simultaneous model) wait-k (Ma et al., 2019a) Table 1: Recent advances in simultaneous translation. As greedy search only explores one single path among exponential many alternatives, beam search is used to improve the search. At each step t, it maintains a beam Bt of size b, which is an ordered list of hhypothesis, probabilityi pairs; for example B0 = [h&lt;s&gt;, 1i]. We then define one-step transition from the previous beam to the next as nextb1 (B) = topb {hy◦ v, s·p(v|x, y)i |hy, si ∈ B} sequence-to-sequence (fu"
D19-1144,P17-4012,0,0.0455762,"nese↔English simultaneous translation tasks. For the training data, we use the NIST corpus for Chinese↔English (2M sentence pairs). We first apply BPE (Sennrich et al., 2015) on all texts in order to reduce the vocabulary sizes. For Chinese↔English evaluation, we use NIST 2006 and NIST 2008 as our dev and test sets with 4 English references. For English→Chinese, we use the second among the four English references as the source text. We re-implement wait-k model (Ma et al., 2019a), test-time wait-k model (Dalvi et al., 2018) and adaptive policy (Gu et al., 2017) based on PyTorch-based OpenNMT (Klein et al., 2017). To reach state-of-the-art performance, we use Transformer based wait-k model and also use Transformer based pre-trained full sentence model for learning adaptive policy. The architecture of Transformer is the same as the base model from the original paper (Vaswani et al., 2017). We use Average Lagging (AL) (Ma et al., 2019a) as the latency metrics. AL measures the number of words delay for translating a given source sentence. w Chunk-based SBS The RL-based adaptive policy system (Gu et al., 2017) can commit a chunk of multiple words whenever there is a series of consecutive WRITEs, and conve"
D19-1144,N19-1187,1,0.855169,"Missing"
D19-1144,W18-6322,0,0.0356692,"he fly. How to adapt beam search for such incremental tasks in order to improve their generation quality? We propose a general technique of speculative beam search (SBS), and apply it to simultaneous translation. At a very high level, to generate a single word, instead of simply choosing the highest-scoring one (as in greedy search), we further speculate w steps into the future, and use the ranking at step w + 1 to reach a more informed decision for step 1 (the current step); this method implicitly benefits from a target language model, alleviating the label bias problem in neural generation (Murray and Chiang, 2018; Ma et al., 2019b). We apply this algorithm to two representative approaches to simultaneous translation: the fixed policy method (Ma et al., 2019a) and the adaptive policy method (Gu et al., 2017). In both cases, we show that SBS improves translation quality while maintaining latency (i.e., simultaneity). 2 Preliminaries We first review standard full-sentence NMT and beam search to set up the notations, and then review different approaches to simultaneous MT. 2.1 Full Sentence NMT and Beam Search The encoder processes the input sequence x = (x1 , ..., xn ), where xi ∈ Rd represents an input"
D19-1144,P15-1020,0,0.144477,"beginning and the output sequence only needs to be revealed in full at the end. By contrast, in language and speech processing, there are many incremental processing tasks with simultaneity requirements, where the output needs to be revealed to the user incrementally without revision (word by word, or in chunks) and the input is also being ∗ These authors contributed equally. received incrementally. Two most salient examples are streaming speech recognition (Chiu et al., 2018), widely used in speech input and dialog systems (such as Siri), and simultaneous translation (Bangalore et al., 2012; Oda et al., 2015; Grissom II et al., 2014; Jaitly et al., 2016), widely used in international conferences and negotiations. In these tasks, the use of full-sentence beam search becomes seemingly impossible as output words need to be committed on the fly. How to adapt beam search for such incremental tasks in order to improve their generation quality? We propose a general technique of speculative beam search (SBS), and apply it to simultaneous translation. At a very high level, to generate a single word, instead of simply choosing the highest-scoring one (as in greedy search), we further speculate w steps into"
D19-1144,D15-1044,0,0.0491146,"search seemingly impossible. To address this challenge, we propose a speculative beam search algorithm that hallucinates several steps into the future in order to reach a more accurate decision, implicitly benefiting from a target language model. This makes beam search applicable for the first time to the generation of a single word in each step. Experiments over diverse language pairs show large improvements over previous work. 1 Introduction Beam search has been widely used in neural text generation such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), summarization (Rush et al., 2015; Ranzato et al., 2016), and image captioning (Vinyals et al., 2015; Xu et al., 2015). It often leads to substantial improvement over greedy search and becomes an essential component in almost all text generation systems. However, beam search is easy for the above tasks because they are all full-sequence problems, where the whole input sequence is available at the beginning and the output sequence only needs to be revealed in full at the end. By contrast, in language and speech processing, there are many incremental processing tasks with simultaneity requirements, where the output needs to be"
D19-1144,D19-1137,1,0.856042,"urce representation h and previously generated target tokens, y&lt;t = (y1 , ..., yt−1 ). The greedy search continues until it emits &lt;eos&gt;, and the final hypothesis y = (y1 , ..., yt ) with yt = &lt;eos&gt; Q|y| p(y |x) = t=1 p(yt |x, y&lt;t ) (1) model policy fixedlatency adaptive where topb (·) returns the top-scoring b pairs, and ◦ is the string concatenation operator. Now Bt = nextb1 (Bt−1 ). As a shorthand, we also define the multi-step beam search function recursively: nextbi (B) = nextb1 (nextbi−1 (B)) 2.2 RL MILk (Gu et al., 2017) (Arivazhagan et al., 2019) Supervised Learning Imitation Learning (Zheng et al., 2019a) (Zheng et al., 2019b) In terms of modeling (which is orthogonal to decoding policies), we can also divide most simultaneous translatoin efforts into two camps: 1. Use the standard full-sentence translation model trained by classical seq-to-seq (Dalvi et al., 2018; Gu et al., 2017; Zheng et al., 2019a). For example, the “test-time waitk” scheme (Ma et al., 2019a) uses the full-sentence translation model and performs wait-k decoding at test time. However, the obvious training-testing mismatch in this scheme usually leads to inferior quality. (2) (3) Simultaneous MT: Policies and Models 2. Use"
D19-1144,P19-1582,1,0.877011,"urce representation h and previously generated target tokens, y&lt;t = (y1 , ..., yt−1 ). The greedy search continues until it emits &lt;eos&gt;, and the final hypothesis y = (y1 , ..., yt ) with yt = &lt;eos&gt; Q|y| p(y |x) = t=1 p(yt |x, y&lt;t ) (1) model policy fixedlatency adaptive where topb (·) returns the top-scoring b pairs, and ◦ is the string concatenation operator. Now Bt = nextb1 (Bt−1 ). As a shorthand, we also define the multi-step beam search function recursively: nextbi (B) = nextb1 (nextbi−1 (B)) 2.2 RL MILk (Gu et al., 2017) (Arivazhagan et al., 2019) Supervised Learning Imitation Learning (Zheng et al., 2019a) (Zheng et al., 2019b) In terms of modeling (which is orthogonal to decoding policies), we can also divide most simultaneous translatoin efforts into two camps: 1. Use the standard full-sentence translation model trained by classical seq-to-seq (Dalvi et al., 2018; Gu et al., 2017; Zheng et al., 2019a). For example, the “test-time waitk” scheme (Ma et al., 2019a) uses the full-sentence translation model and performs wait-k decoding at test time. However, the obvious training-testing mismatch in this scheme usually leads to inferior quality. (2) (3) Simultaneous MT: Policies and Models 2. Use"
J09-4009,P05-1033,0,0.869126,"mars and present a linear-time algorithm for binarizing synchronous rules when possible. In our large-scale experiments, we found that almost all rules are binarizable and the resulting binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntaxbased machine translation system. We also discuss the more general, and computationally more difficult, problem of finding good parsing strategies for non-binarizable rules, and present an approximate polynomial-time algorithm for this problem. 1. Introduction Several recent syntax-based models for machine translation (Chiang 2005; Galley et al. 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right-hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization as a technique to factor each synchronous grammar rule into a series of binary rules. Although monolingual context-free grammars (CFGs) can always be binarized, this is not the case ∗ Informatio"
J09-4009,P03-2041,0,0.106836,"reorderings of nonterminals are denoted by variables xi . In the treetransducer formalism of Rounds (1970), the right-hand (target) side subtree can have multiple levels, as in the first rule above. This system can model non-isomorphic transformations on English parse trees to “fit” another language, learning, for example, that the (V S O) structure in Arabic should be transformed into a (S (V O)) structure in English, by looking at two-level tree fragments (Knight and Graehl 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner 2003; Shieber 2004) (see Figure 3). This larger locality captures more linguistic phenomena and leads to better parameter estimation. By creating a 563 Computational Linguistics Volume 35, Number 4 Figure 3 Two equivalent representations of the first rule in Example (5): (a) tree transducer; (b) Synchronous Tree Subsitution Grammar (STSG). The ↓ arrows denote substitution sites, which correspond to variables in tree transducers. nonterminal for each right-hand-side tree, we can convert the transducer representation to an SCFG with the same generative capacity. We can again create a projected CFG w"
J09-4009,N04-1035,1,0.843738,"ent a linear-time algorithm for binarizing synchronous rules when possible. In our large-scale experiments, we found that almost all rules are binarizable and the resulting binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntaxbased machine translation system. We also discuss the more general, and computationally more difficult, problem of finding good parsing strategies for non-binarizable rules, and present an approximate polynomial-time algorithm for this problem. 1. Introduction Several recent syntax-based models for machine translation (Chiang 2005; Galley et al. 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right-hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization as a technique to factor each synchronous grammar rule into a series of binary rules. Although monolingual context-free grammars (CFGs) can always be binarized, this is not the case ∗ Information Science Institute,"
J09-4009,N07-1019,1,0.850542,"Missing"
J09-4009,W05-1506,1,0.824427,"ecting the constraints from the other side. This scheme generalizes to the case where we have n nonterminals in a SCFG rule, and the decoder conservatively assumes nothing can be done on language model scoring (because target-language spans are non-contiguous in general) until the real nonterminal has been recognized. In other words, target-language boundary words 2 An alternative to integrated decoding is rescoring, where one first computes the k-best translations according to the TM only, and then reranks the k-best list with the language model costs. This method runs very fast in practice (Huang and Chiang 2005), but often produces a considerable number of search errors because the true best translation is often outside of the k-best list, especially for longer sentences. 562 Huang et al. Binarization of Synchronous Context-Free Grammars from each child nonterminal of the rule will be cached in all virtual nonterminals derived from this rule. In the case of m-gram integrated decoding, we have to maintain 2(m − 1) boundary words for each child nonterminal, which leads to a prohibitive overall complexity of O(|w|3+2n(m−1) ), which is exponential in rule size (Huang, Zhang, and Gildea 2005). Aggressive"
J09-4009,W05-1507,1,0.919163,"Missing"
J09-4009,P05-1057,0,0.0471912,"Missing"
J09-4009,N03-1021,0,0.294945,"real examples of non-binarizable cases verified by native speakers. In the final, theoretical, sections of this article, we investigate the general problem of finding the most efficient synchronous parsing or decoding strategy for arbitrary synchronous context-free grammar (SCFG) rules, including non-binarizable cases. Although this problem is believed to be NP-complete, we prove two results that substantially reduce the search space over strategies. We also present an optimal algorithm that runs tractably in practice and a polynomial-time algorithm that is a good approximation of the former. Melamed (2003) discusses binarization of multi-text grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rule"
J09-4009,W03-0301,0,0.0464804,"Missing"
J09-4009,J03-1006,0,0.1198,"Missing"
J09-4009,J04-4002,0,0.117917,"e 13 Comparing the two binarization methods in terms of translation quality against search effort. Table 2 Machine translation results for syntax-based systems vs. the phrase-based Alignment Template System. System BLEU monolingual binarization synchronous binarization alignment-template system 36.25 38.44 37.00 decoding is used as a measure of the size of search space, or time efficiency. Our system is consistently faster and more accurate than the baseline system. We also compare the top result of our synchronous binarization system with the state-of-the-art alignment-template system (ATS) (Och and Ney 2004). The results are shown in Table 2. Our system has a promising improvement over the ATS system, which is trained on a larger data set but tuned independently. A larger-scale system based on our best result performs very well in the 2006 NIST MT Evaluation (ISI Machine Translation Team 2006), achieving the best overall BLEU scores in the Chineseto-English track among all participants.4 The readers are referred to Galley et al. (2004) for details of the decoder and the overall system. 6. One-Sided Binarization In this section and the following section, we discuss techniques for handling rules th"
J09-4009,C69-0101,0,0.65771,"th source- and target-sides, so that we can generate a binary-branching SCFG: (4) S PP-VP → → NP 1 PP-VP 2 , VP 1 PP 2 , NP 1 PP-VP 2 PP 2 VP 1 In this case m-gram integrated decoding can be done in O(|w|3+4(m−1) ) time, which is a much lower-order polynomial and no longer depends on rule size (Wu 1996), allowing the search to be much faster and more accurate, as is evidenced in the Hiero system of Chiang (2005), which restricts the hierarchical phrases to form binary-branching SCFG rules. Some recent syntax-based MT systems (Galley et al. 2004) have adopted the formalism of tree transducers (Rounds 1970), modeling translation as a set of rules for a transducer that takes a syntax tree in one language as input and transforms it into a tree (or string) in the other language. The same decoding algorithms are used for machine translation in this formalism, and the following example shows that the same issues of binarization arise. Suppose we have the following transducer rules: (5) S(x1 :NP x2 :PP x3 :VP) NP( / B`aow¯eier) VP( / jux´ ˇ ıng le hu`ıt´an) PP( / yuˇ Sh¯al´ong)  >L     → → → → S(x1 VP(x3 x2 )) NP(NNP(Powell)) VP(VBD(held) NP(DT(a) NPS(meeting))) PP(TO(with) NP(NNP(Sharon))) w"
J09-4009,H05-1101,0,0.806039,"binarizing a tree-transducer rule, and consider only the alignment (or permutation) of the nonterminal variables. Again, rightmost binarization is preferable for the first rule. In SCFG-based frameworks, the problem of finding a word-level alignment between two sentences is an instance of the synchronous parsing problem: Given two strings and a synchronous grammar, find a parse tree that generates both input strings. The benefit of binary grammars also applies in this case. Wu (1997) shows that parsing a binary-branching SCFG is in O(|w|6 ), while parsing SCFG with arbitrary rules is NP-hard (Satta and Peserico 2005). For example, in Figure 2, the complexity of synchronous parsing for the original grammar (a) is O(|w|8 ), because we have to maintain four indices on either side, giving a total of eight; parsing the monolingually binarized grammar (b) involves seven indices, three on the Chinese side and four on the English side. In contrast, the synchronously binarized version (c) requires only 3 + 3 = 6 indices, which can be thought of as “CKY in two dimensions.” An efficient alignment algorithm is guaranteed if a binarization is found, and the same binarization can be used for decoding and alignment. We"
J09-4009,W04-3312,0,0.0458485,"of nonterminals are denoted by variables xi . In the treetransducer formalism of Rounds (1970), the right-hand (target) side subtree can have multiple levels, as in the first rule above. This system can model non-isomorphic transformations on English parse trees to “fit” another language, learning, for example, that the (V S O) structure in Arabic should be transformed into a (S (V O)) structure in English, by looking at two-level tree fragments (Knight and Graehl 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner 2003; Shieber 2004) (see Figure 3). This larger locality captures more linguistic phenomena and leads to better parameter estimation. By creating a 563 Computational Linguistics Volume 35, Number 4 Figure 3 Two equivalent representations of the first rule in Example (5): (a) tree transducer; (b) Synchronous Tree Subsitution Grammar (STSG). The ↓ arrows denote substitution sites, which correspond to variables in tree transducers. nonterminal for each right-hand-side tree, we can convert the transducer representation to an SCFG with the same generative capacity. We can again create a projected CFG which will be ex"
J09-4009,C90-3045,0,0.379979,"arizable SCFGs, and are mainly of theoretical interest. Algorithms 1–3 make fewer and fewer assumptions on the strategy space, and produce parsing strategies closer and closer to the optimal. Algorithm 4 further improves Algorithm 3. Section Algorithm Assumptions of Strategy Space Complexity 3–4 6 Alg. 1 (synchronous) Alg. 2 (one-sided, CKY) Alg. 3 (optimal) ⇒ Alg. 4 (best-first) Contiguous on both sides Contiguous on one side O(n) O(n3 ) O(3n ) O(9k n2k ) 7.2 No assumptions systems improve. Synchronous grammars that go beyond the power of SCFG (and therefore binary SCFG) have been defined by Shieber and Schabes (1990) and Rambow and Satta (1999), and motivated for machine translation by Melamed (2003), although previous work has not given algorithms for finding efficient and optimal parsing strategies for general SCFGs, which we believe is an important problem. In the remainder of this section and the next section, we will present a series of algorithms that produce increasingly faster parsing strategies, by gradually relaxing the strong “continuity” constraint made by the synchronous binarization technique. As that technique requires continuity on both languages, we will first study a relaxation where bin"
J09-4009,P06-1123,0,0.0494632,"Missing"
J09-4009,P96-1021,0,0.347375,"NP and PP into an intermediate state which contains a gap on the English side. (c) This scheme groups PP and VP into an intermediate state which is contiguous on both sides. These two binarizations are no different in the translation-model-only decoding described previously, just as in monolingual parsing. However, in the source-channel approach to machine translation, we need to combine probabilities from the translation model (TM) (an SCFG) with the language model (an n-gram), which has been shown to be very important for translation quality (Chiang 2005). To do bigram-integrated decoding (Wu 1996), we need to augment each chart item (X, i, j) with twoÃ target! u ··· v language boundary words u and v to produce a bigram-item which we denote i X j .2 Now the two binarizations have very different effects. In the first case, we first combine NP with PP. This step is written as follows in the weighted deduction notation of Nederhof (2003): ¶ µ ¶ µ Powell ··· Powell with ··· Sharon :q :p NP PP 2 4 2 µ1 ¶ Powell ··· Powell ··· with ··· Sharon : pq NP-PP 1 4 where p and q are the scores of antecedent items. This situation is unpleasant because in the target language NP and PP are not contiguou"
J09-4009,J97-3002,0,0.790505,"e cases. Although this problem is believed to be NP-complete, we prove two results that substantially reduce the search space over strategies. We also present an optimal algorithm that runs tractably in practice and a polynomial-time algorithm that is a good approximation of the former. Melamed (2003) discusses binarization of multi-text grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses. This approach results in rules with many nonterminals, making good binarization techniques critical. We explain how synchronous rule binarization interacts with n-gram language models and affects decoding for machine translation in Section 2. We define binarization formally in"
J09-4009,W07-0404,1,0.868562,"g one nonterminal at a time. The optimal grouping of nonterminals is shown on the right. time O(|w|10 ) by adding one nonterminal at a time. All permutations of less than eight elements can be optimally parsed by adding one element at a time. 7.4 Discontinuous Parsing Is Necessary Only for Non-Decomposable Permutations In this subsection, we show that an optimal parsing strategy can be found by first factoring an SCFG rule into a sequence of shorter SCFG rules, if possible, and then considering each of the new rules independently. The first step can be done efficiently using the algorithms of Zhang and Gildea (2007). The second step can be done in time O(9kc · n2kc ) using Algorithm 4, where kc is the complexity of the longest SCFG rule after factorizations, implying that kc ≤ (n + 4). We show that this two-step process is optimal, by proving that the optimal parsing strategy for the initial rule will not need to build subsets of children that cross the boundaries of the factorization into shorter SCFG rules. Figure 19 shows a permutation that contains permutations of fewer numbers within itself so that the entire permutation can be decomposed hierarchically. We prove that if there is a contiguous block"
J09-4009,N06-1033,1,0.518184,"present a decoding strategy for these rules in Section 6. Section 7 gives a solution to the general theoretical problem of finding optimal decoding and synchronous parsing strategies for arbitrary SCFGs, and presents complexity results on the nonbinarizable rules from our Chinese–English data. These final two sections are of primarily theoretical interest, as nonbinarizable rules have not been shown to benefit real-world machine translation systems. However, the algorithms presented may become relevant as machine translation systems improve. 1 A preliminary version of Section 1–5 appeared in Zhang et al. (2006). 560 Huang et al. Binarization of Synchronous Context-Free Grammars 2. Motivation Consider the following Chinese sentence and its English translation: (1)    >L  B`aow¯eier yuˇ Sh¯al´ong jux´ ˇ ıng le Powell with Sharon hold [past.] “Powell held a meeting with Sharon”  hu`ıt´an meeting Suppose we have the following SCFG, where superscripts indicate reorderings (formal definitions of SCFGs with a more flexible notation can be found in Section 3): (2) S NP VP PP → → → → NP 1 PP 2 VP 3 , / B`aow¯eier, / jux´ ˇ ıng le hu`ıt´an, / yuˇ Sh¯al´ong,  >L     NP 1 VP 3 PP 2 Powell held"
J09-4009,W07-0405,1,\N,Missing
J09-4009,P06-1121,1,\N,Missing
J09-4009,W90-0102,0,\N,Missing
J15-1005,W06-1615,0,0.233269,"Missing"
J15-1005,J95-4004,0,0.372531,"point of view, the practical value of the current work on bilingual projection and unsupervised induction may be underestimated, and annotation adaptation could make better use of the projected or induced knowledge.3 7. Related Work There has already been some preliminary work tackling the divergence between different annotation guidelines. Gao et al. (2004) described a transformation-based converter to transfer a certain word segmentation result to another annotation guideline. They designed class-type transformation templates and used the transformation-based error-driven learning method of Brill (1995) to learn what word delimiters should be modified. Many efforts have been devoted to manual treebank transformation, where PTB is adapted to other grammar formalisms, such as CCG and LFG (Cahill et al. 2002; Hockenmaier and Steedman 2007). However, all these are heuristic-based—that is, they need manually designed transformation templates and involve heavy human engineering. Such strategies are hard to be generalized to POS tagging, not to mention other complicated structural prediction tasks. We investigated the automatic integration of word segmentation knowledge in differently annotated cor"
J15-1005,W06-2920,0,0.037011,"expensive to build, especially on a large scale. The creation of treebanks is a prime example (Marcus, Santorini, and Marcinkiewicz 1993). However, the linguistic theories motivating these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly different and incompatible annotation philosophies. For example, there are several treebanks for English, including the Chomskian-style Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), the HPSG LinGo Redwoods Treebank (Oepen et al. 2002), and a smaller dependency treebank (Buchholz and Marsi 2006). From the perspective of resource accumulation, it seems a waste in human efforts.1 A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from financial news (Penn Treebank/Wall Street Journal) to transcribed dialog (LinGo). It would be nice if a system could be automatically ported from one set of guidelines and/or domain to another, in order to exploit a much larger data set. The second problem, domain adaptation, is very well studied (e.g., Blitzer, McDonald, & Pereira 2006; Daum´e III 2007). This work focuses on the widely"
J15-1005,S12-1050,0,0.314988,"sident into two words and combines the phrase visited-China as a compound, compared with the segmentation following the CTB annotation guideline. It is preferable to transfer knowledge from PD to CTB because the latter also annotates tree structures, which are useful for downstream applications like parsing, summarization, and machine translation, yet it is much smaller in size. For dependency parsing, we use the dependency treebank (DCTB) extracted from CTB according to the rules of Yamada and Matsumoto (2003), and the Semantic Dependency Treebank (SDT) built on a small part of the CTB text (Che et al. 2012). Compared with the automatically extracted dependencies in DCTB, semantic dependencies in SDT reveal semantic relationships between words, rather than the syntactic relationships in syntactic dependencies. Figure 2 shows an example. Experiments on both word segmentation and dependency parsing show that annotation adaptation results in significant improvement over the baselines, and achieves the state-of-the-art with only local features. The rest of the article is organized as follows. Section 2 gives a description of the problem of annotation adaptation. Section 3 briefly introduces the tasks"
J15-1005,W02-1001,0,0.184222,"ion: y˜ = argmax f(x, y) · w y = argmax y X f(xi , yi ) · w (1) xi ∈x,yi ∈y Where function f maps (x, y) into a feature vector, w is the parameter vector generated by the training algorithm, and f(x, y) · w is the inner product of f(x, y) and w. The score of the sentence is further factorized into each character, where yi is the character classification label of character xi . The training procedure of perceptron learns a discriminative model mapping from the inputs x to the outputs y. Algorithm 1 shows the perceptron algorithm for tuning the parameter w. The “averaged parameters” technology (Collins 2002) is used for better performance. The feature templates of the classifier is shown in Table 1. The function Pu(· ) returns true for a punctuation character and false for others; the function T(· ) classifies a character into four types: number, date, English letter, and others, corresponding to function values 1, 2, 3, and 4, respectively. 3.2 Dependency Parsing and Spanning Tree Method Dependency parsing aims to link each word to its arguments so as to form a directed graph spanning the whole sentence. Normally, the directed graph is restricted to a 123 Computational Linguistics Volume 41, Num"
J15-1005,P11-1061,0,0.0168485,"h FBIS as the bilingual corpus. By annotation adaptation, the projected corpus for word segmentation brings a significant F-measure increment of nearly 0.6 points over the baseline trained on CTB only. 143 Computational Linguistics Volume 41, Number 1 The training procedure for an annotation adaptation model requires a parallel annotated corpus (which may be automatically generated); this fact puts the method into the neighborhood of the family of approaches known as annotation projection (Hwa et al. 2002, 2005; Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009; Jiang and Liu 2010; Das and Petrov 2011). Essentially, annotation adaptation and annotation projection tackle different problems; the former aims to transform the annotations from one guideline to another (of course in the same language), whereas the latter aims to project the annotation (as well as the annotation guideline) from one language to another. Therefore, the machine learning methods for annotation adaptation pay attention to automatic transformation of annotations, while for annotation projection, the machine learning methods focus on the bilingual projection across languages. Co-training (Sarkar 2001) and classifier comb"
J15-1005,P07-1033,0,0.456207,"Missing"
J15-1005,C96-1058,0,0.160427,"root r ∈ x in the tree y has no head word, and each of the other words, j(j ∈ x and j 6= r), depends on a head word i(i ∈ x and i 6= j). For many languages, the dependency structures are supposed to be projective. If xj is dependent on xi , then all the words between i and j must be directly or indirectly dependent on xi . Therefore, if we put the words in their linear order, preceded by the root, all edges can be drawn above the words without crossing. We follow this constraint because the dependency treebanks in our experiments are projective. Following the edge-based factorization method (Eisner 1996), the score of a dependency tree can be factorized into the dependency edges in the tree. The spanning Table 1 Feature templates and instances for character classification-based word segmentation model. C0 denotes the current character, and C−i /Ci denote the ith character to the left/right of C0 . Suppose we are considering the third character “总” in “美-副-总-统-访-华”. Type Templates Instances n-gram C−2 C−1 C0 C1 C2 C−2 C−1 C−1 C0 C0 C1 C1 C2 C−1 C1 C−2 =美 C−1 =副 C0 =总 C1 =统 C2 =访 C−2 C−1 =美副 C−1 C0 =副总 C0 C1 =总统 C1 C2 =统访 C−1 C1 =副统 function Pu(C0 ) T(C−2:2 ) Pu(C0 )=true T(C−2:2 )= 44444 124 J"
J15-1005,P09-1042,0,0.0605038,"Missing"
J15-1005,P11-2095,0,0.0204332,"ation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future Work We have described the problem of annotation adaptation and the intrinsic principles of its solutions, and proposed a series of successively enhanced models that can automatically adapt the divergence between different annotation formats. These models learn the statistical regularities of adaptation between different"
J15-1005,J07-3004,0,0.0188995,"Related Work There has already been some preliminary work tackling the divergence between different annotation guidelines. Gao et al. (2004) described a transformation-based converter to transfer a certain word segmentation result to another annotation guideline. They designed class-type transformation templates and used the transformation-based error-driven learning method of Brill (1995) to learn what word delimiters should be modified. Many efforts have been devoted to manual treebank transformation, where PTB is adapted to other grammar formalisms, such as CCG and LFG (Cahill et al. 2002; Hockenmaier and Steedman 2007). However, all these are heuristic-based—that is, they need manually designed transformation templates and involve heavy human engineering. Such strategies are hard to be generalized to POS tagging, not to mention other complicated structural prediction tasks. We investigated the automatic integration of word segmentation knowledge in differently annotated corpora (Jiang, Huang, and Liu 2009; Jiang et al. 2012), which can be seen as the preliminary work of automatic annotation adaptation. Motivated by our initial investigation, researchers applied similar methodologies to constituency parsing"
J15-1005,P02-1050,0,0.0651491,"on word segmentation. Bilingual projection was conducted from English to Chinese with the Chinese–English FBIS as the bilingual corpus. By annotation adaptation, the projected corpus for word segmentation brings a significant F-measure increment of nearly 0.6 points over the baseline trained on CTB only. 143 Computational Linguistics Volume 41, Number 1 The training procedure for an annotation adaptation model requires a parallel annotated corpus (which may be automatically generated); this fact puts the method into the neighborhood of the family of approaches known as annotation projection (Hwa et al. 2002, 2005; Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009; Jiang and Liu 2010; Das and Petrov 2011). Essentially, annotation adaptation and annotation projection tackle different problems; the former aims to transform the annotations from one guideline to another (of course in the same language), whereas the latter aims to project the annotation (as well as the annotation guideline) from one language to another. Therefore, the machine learning methods for annotation adaptation pay attention to automatic transformation of annotations, while for annotation projection, the machine lear"
J15-1005,P09-1059,1,0.817336,"Missing"
J15-1005,P08-1102,1,0.875465,"Missing"
J15-1005,P10-1002,1,0.821867,"h the Chinese–English FBIS as the bilingual corpus. By annotation adaptation, the projected corpus for word segmentation brings a significant F-measure increment of nearly 0.6 points over the baseline trained on CTB only. 143 Computational Linguistics Volume 41, Number 1 The training procedure for an annotation adaptation model requires a parallel annotated corpus (which may be automatically generated); this fact puts the method into the neighborhood of the family of approaches known as annotation projection (Hwa et al. 2002, 2005; Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009; Jiang and Liu 2010; Das and Petrov 2011). Essentially, annotation adaptation and annotation projection tackle different problems; the former aims to transform the annotations from one guideline to another (of course in the same language), whereas the latter aims to project the annotation (as well as the annotation guideline) from one language to another. Therefore, the machine learning methods for annotation adaptation pay attention to automatic transformation of annotations, while for annotation projection, the machine learning methods focus on the bilingual projection across languages. Co-training (Sarkar 200"
J15-1005,D12-1038,1,0.621231,"Missing"
J15-1005,N09-1036,0,0.0347124,"I 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future Work We have described the problem of annotation adaptation and the intrinsic principles of its solutions, and proposed a series of successively enhanced models that can automatically adapt the divergence between different annotation formats. These models learn"
J15-1005,P09-1058,0,0.0211221,"redict-self methodology is implicit in many unsupervised learning approaches; it has been successfully used in unsupervised dependency parsing (Daum´e III 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future Work We have described the problem of annotation adaptation and the intrinsic principles of its solutions"
J15-1005,P11-1141,0,0.0132605,"optimization of the parallel annotated corpora used to train the transfer classifiers. The predict-self methodology is implicit in many unsupervised learning approaches; it has been successfully used in unsupervised dependency parsing (Daum´e III 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future"
J15-1005,J93-2004,0,0.0517662,"Missing"
J15-1005,D08-1017,0,0.0206003,"est training iterations for the source classifier and the transfer classifier are determined on the development sets of the source corpus and target corpus. In the decoding procedure, a sequence of characters (for word segmentation) or words (for dependency parsing) is input into the source classifier to obtain a classification result under the source guideline; then it is input into the transfer classifier with this classification result as the guiding information to get the final result following the target guideline. This coincides with the stacking method for combining dependency parsers (Martins et al. 2008; Nivre and McDonald 2008), and is also similar to the Pred baseline for domain adaptation in Daum´e et al. (Daum´e III and Marcu 2006; Daum´e III 2007). Figure 5 shows the pipeline for decoding. raw sentence source classifier result with source guideline transfer classifier result with target guideline Figure 5 The pipeline for decoding of Model 1. 129 Computational Linguistics Volume 41, Number 1 4.3 Model 2 The previous model has a drawback: It has to cascade two classifiers in decoding to integrate the knowledge in two corpora, which seriously degrades the processing speed. Here we describ"
J15-1005,P05-1012,0,0.160861,"Missing"
J15-1005,E06-1011,0,0.0205287,"dditional guiding features. For word segmentation, the most intuitive guiding feature is the source annotation label itself. For dependency parsing, an effective guiding feature is the dependency path between the hypothetic head and modifier, as shown in Figure 3. However, our effort is not limited to this, and more special features are introduced: A classification label or dependency path is attached to each feature of the baseline classifier to generate combined guiding features. This is similar to the feature design in discriminative dependency parsing (McDonald, Crammer, and Pereira 2005; McDonald and Pereira 2006), where the basic features, composed of words and POSs in the context, are also conjoined with link direction and distance in order to generate more special features. Table 3 shows an example of guide features (as well as baseline features) for word segmentation, where “α = B” indicates that the source classification label of the current character is B, demarcating the beginning of a word. The combination strategy derives a series of specific features, helping the transfer classifier to produce more precise classifications. The parameter-tuning procedure of the transfer classifier will automat"
J15-1005,P09-1012,0,0.073578,"Missing"
J15-1005,P07-2055,0,0.0227552,"transfer classifiers. The predict-self methodology is implicit in many unsupervised learning approaches; it has been successfully used in unsupervised dependency parsing (Daum´e III 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future Work We have described the problem of annotation adaptation and the intrinsic pri"
J15-1005,W04-3236,0,0.0280485,"no explicit word boundaries, thus word segmentation is a fundamental task for the processing and understanding of these languages. Given a sentence as a sequence of n characters: x = x1 x2 .. xn where xi is a character, word segmentation aims to split the sequence into m(≤ n) words: x1:e1 xe1 +1:e2 .. xem−1+1:em where each subsequence xi:j indicates a Chinese word spanning from characters xi to xj . Word segmentation can be formalized as a sequence labeling problem (Xue and Shen 2003), where each character in the sentence is given a boundary tag representing its position in a word. Following Ng and Low (2004), joint word segmentation and partof-speech (POS) tagging can also be solved using a character classification approach by extending boundary tags to include POS information. For word segmentation we adopt the four boundary tags of Ng and Low (2004), B, M, E, and S, where B, M, and E mean the beginning, the middle, and the end of a word, respectively, and S indicates a singlecharacter word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern S or BM∗ E, indicating single-character words or multi-character words, respectively. Gi"
J15-1005,P08-1108,0,0.60101,"ns for the source classifier and the transfer classifier are determined on the development sets of the source corpus and target corpus. In the decoding procedure, a sequence of characters (for word segmentation) or words (for dependency parsing) is input into the source classifier to obtain a classification result under the source guideline; then it is input into the transfer classifier with this classification result as the guiding information to get the final result following the target guideline. This coincides with the stacking method for combining dependency parsers (Martins et al. 2008; Nivre and McDonald 2008), and is also similar to the Pred baseline for domain adaptation in Daum´e et al. (Daum´e III and Marcu 2006; Daum´e III 2007). Figure 5 shows the pipeline for decoding. raw sentence source classifier result with source guideline transfer classifier result with target guideline Figure 5 The pipeline for decoding of Model 1. 129 Computational Linguistics Volume 41, Number 1 4.3 Model 2 The previous model has a drawback: It has to cascade two classifiers in decoding to integrate the knowledge in two corpora, which seriously degrades the processing speed. Here we describe a variant of the previou"
J15-1005,C02-2025,0,0.0192385,"to train models, but annotated resources are extremely expensive to build, especially on a large scale. The creation of treebanks is a prime example (Marcus, Santorini, and Marcinkiewicz 1993). However, the linguistic theories motivating these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly different and incompatible annotation philosophies. For example, there are several treebanks for English, including the Chomskian-style Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), the HPSG LinGo Redwoods Treebank (Oepen et al. 2002), and a smaller dependency treebank (Buchholz and Marsi 2006). From the perspective of resource accumulation, it seems a waste in human efforts.1 A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from financial news (Penn Treebank/Wall Street Journal) to transcribed dialog (LinGo). It would be nice if a system could be automatically ported from one set of guidelines and/or domain to another, in order to exploit a much larger data set. The second problem, domain adaptation, is very well studied (e.g., Blitzer, McDonald, & Pe"
J15-1005,N01-1023,0,0.0589285,"d Liu 2010; Das and Petrov 2011). Essentially, annotation adaptation and annotation projection tackle different problems; the former aims to transform the annotations from one guideline to another (of course in the same language), whereas the latter aims to project the annotation (as well as the annotation guideline) from one language to another. Therefore, the machine learning methods for annotation adaptation pay attention to automatic transformation of annotations, while for annotation projection, the machine learning methods focus on the bilingual projection across languages. Co-training (Sarkar 2001) and classifier combination (Nivre and McDonald 2008) are two techniques for training improved dependency parsers. The co-training technique lets two different parsing models learn from each other during the parsing of unlabeled text: One model selects some unlabeled sentences it can confidently parse, and provides them to the other model as additional training data in order to train more powerful parsers. The classifier combination lets graph-based and transition-based dependency parsers utilize the features extracted from each other’s parsing results, to obtain combined, enhanced parsers. Th"
J15-1005,D09-1086,0,0.0250397,"English to Chinese with the Chinese–English FBIS as the bilingual corpus. By annotation adaptation, the projected corpus for word segmentation brings a significant F-measure increment of nearly 0.6 points over the baseline trained on CTB only. 143 Computational Linguistics Volume 41, Number 1 The training procedure for an annotation adaptation model requires a parallel annotated corpus (which may be automatically generated); this fact puts the method into the neighborhood of the family of approaches known as annotation projection (Hwa et al. 2002, 2005; Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009; Jiang and Liu 2010; Das and Petrov 2011). Essentially, annotation adaptation and annotation projection tackle different problems; the former aims to transform the annotations from one guideline to another (of course in the same language), whereas the latter aims to project the annotation (as well as the annotation guideline) from one language to another. Therefore, the machine learning methods for annotation adaptation pay attention to automatic transformation of annotations, while for annotation projection, the machine learning methods focus on the bilingual projection across languages. Co-"
J15-1005,P11-1139,0,0.0142567,"ervised learning approaches; it has been successfully used in unsupervised dependency parsing (Daum´e III 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future Work We have described the problem of annotation adaptation and the intrinsic principles of its solutions, and proposed a series of successiv"
J15-1005,P12-1025,0,0.2503,"ned transformation templates and involve heavy human engineering. Such strategies are hard to be generalized to POS tagging, not to mention other complicated structural prediction tasks. We investigated the automatic integration of word segmentation knowledge in differently annotated corpora (Jiang, Huang, and Liu 2009; Jiang et al. 2012), which can be seen as the preliminary work of automatic annotation adaptation. Motivated by our initial investigation, researchers applied similar methodologies to constituency parsing (Sun, Wang, and Zhang 2010; Zhu, Zhu, and Hu 2011) and word segmentation (Sun and Wan 2012). This previous work verified the effectiveness of automatic annotation adaptation, but did not reveal the essential definition of the problem nor the intrinsic principles of the solutions. Instead, this work clearly defines the problem of annotation adaptation, reveals the intrinsic principles of the solutions, and systematically describes a series of gradually improved models. The most advanced model learns transformation regularities much better and achieves significant higher accuracy for both word segmentation and dependency parsing, without slowing down the final language processors. The"
J15-1005,W10-4144,0,0.0623024,"Missing"
J15-1005,C10-1132,0,0.0500624,"Missing"
J15-1005,W03-1728,0,0.0305531,"Segmentation and Dependency Parsing 3.1 Word Segmentation and Character Classification Method In many Asian languages there are no explicit word boundaries, thus word segmentation is a fundamental task for the processing and understanding of these languages. Given a sentence as a sequence of n characters: x = x1 x2 .. xn where xi is a character, word segmentation aims to split the sequence into m(≤ n) words: x1:e1 xe1 +1:e2 .. xem−1+1:em where each subsequence xi:j indicates a Chinese word spanning from characters xi to xj . Word segmentation can be formalized as a sequence labeling problem (Xue and Shen 2003), where each character in the sentence is given a boundary tag representing its position in a word. Following Ng and Low (2004), joint word segmentation and partof-speech (POS) tagging can also be solved using a character classification approach by extending boundary tags to include POS information. For word segmentation we adopt the four boundary tags of Ng and Low (2004), B, M, E, and S, where B, M, and E mean the beginning, the middle, and the end of a word, respectively, and S indicates a singlecharacter word. The word segmentation result can be generated by splitting the labeled character"
J15-1005,W03-3023,0,0.587227,"005). They utilize very different segmentation guidelines; for example, as shown in Figure 1, PD breaks VicePresident into two words and combines the phrase visited-China as a compound, compared with the segmentation following the CTB annotation guideline. It is preferable to transfer knowledge from PD to CTB because the latter also annotates tree structures, which are useful for downstream applications like parsing, summarization, and machine translation, yet it is much smaller in size. For dependency parsing, we use the dependency treebank (DCTB) extracted from CTB according to the rules of Yamada and Matsumoto (2003), and the Semantic Dependency Treebank (SDT) built on a small part of the CTB text (Che et al. 2012). Compared with the automatically extracted dependencies in DCTB, semantic dependencies in SDT reveal semantic relationships between words, rather than the syntactic relationships in syntactic dependencies. Figure 2 shows an example. Experiments on both word segmentation and dependency parsing show that annotation adaptation results in significant improvement over the baselines, and achieves the state-of-the-art with only local features. The rest of the article is organized as follows. Section 2"
J15-1005,P07-1106,0,0.026795,"rative training for annotation adaptation emphasizes the iterative optimization of the parallel annotated corpora used to train the transfer classifiers. The predict-self methodology is implicit in many unsupervised learning approaches; it has been successfully used in unsupervised dependency parsing (Daum´e III 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmen"
J15-1005,D10-1082,0,0.0316826,"Missing"
J15-1005,I08-4017,0,0.0347548,"parsing (Daum´e III 2009). We adapt this idea to the scenario of annotation adaptation to improve transformation accuracy. In recent years much effort has been devoted to the improvement of word segmentation and dependency parsing. For example, the introduction of global training or complicated features (Zhang and Clark 2007, 2010); the investigation of word structures (Li 2011); the strategies of hybrid, joint, or stacked modeling (Nakagawa and Uchimoto 2007; Kruengkrai et al. 2009; Wang, Zong, and Su 2010; Sun 2011); and the semi-supervised and unsupervised technologies utilizing raw text (Zhao and Kit 2008; Johnson and Goldwater 2009; Mochihashi, Yamada, and Ueda 2009; Hewlett and Cohen 2011). We believe that the annotation adaptation technologies can be adopted jointly with complicated features, system combination, and semi-supervised/unsupervised technologies to further improve the performance of word segmentation and dependency parsing. 8. Conclusion and Future Work We have described the problem of annotation adaptation and the intrinsic principles of its solutions, and proposed a series of successively enhanced models that can automatically adapt the divergence between different annotation"
J15-1005,P11-2126,0,0.0386,"Missing"
J15-1005,C08-1049,1,\N,Missing
J15-1005,P04-1059,0,\N,Missing
N06-1033,P03-2041,0,0.286386,"juxing le huitan PP(TO(with), NP(NNP(Sharon))) → yu Shalong where the reorderings of nonterminals are denoted by variables xi . Notice that the first rule has a multi-level lefthand side subtree. This system can model nonisomorphic transformations on English parse trees to “fit” another language, for example, learning that 258 the (S (V O)) structure in English should be transformed into a (V S O) structure in Arabic, by looking at two-level tree fragments (Knight and Graehl, 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner, 2003). This larger locality is linguistically motivated and leads to a better parameter estimation. By imagining the left-hand-side trees as special nonterminals, we can virtually create an SCFG with the same generative capacity. The technical details will be explained in Section 3.2. In general, if we are given an arbitrary synchronous rule with many nonterminals, what are the good decompositions that lead to a binary grammar? Figure 2 suggests that a binarization is good if every virtual nonterminal has contiguous spans on both sides. We formalize this idea in the next section. 2 Synchronous Bina"
N06-1033,N04-1035,1,0.655736,"re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system. 1 • We examine the effect of this binarization method on end-to-end machine translation quality, compared to a more typical baseline method. • We examine cases of non-binarizable rules in a large, empirically-derived rule set, and we investigate the effect on translation quality when excluding such rules. Introduction Several recent syntax-based models for machine translation (Chiang, 2005; Galley et al., 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation. In particular: Melamed (2003) discusses binarizati"
N06-1033,W05-1507,1,0.837659,"e n nonterminals in a SCFG rule, and the decoder conservatively assumes nothing can be done on language model scoring (because target-language spans are non-contiguous in general) until the real nonterminal has been recognized. In other words, targetlanguage boundary words from each child nonterminal of the rule will be cached in all virtual nonterminals derived from this rule. In the case of m-gram integrated decoding, we have to maintain 2(m − 1) boundary words for each child nonterminal, which leads to a prohibitive overall complexity of O(|w|3+2n(m−1) ), which is exponential in rule size (Huang et al., 2005). Aggressive pruning must be used to make it tractable in practice, which in general introduces many search errors and adversely affects translation quality. In the second case, however:  with ··· Sharon 2  PP held 2 4 ··· VPP-VP  :r Sharon 7   held ··· meeting 4 VP 7  :s : rs · Pr(with |meeting) Here since PP and VP are contiguous (but swapped) in the target-language, we can include the source (Chinese) NP NP PP VP VP PP target (English) English boundary words VPP-VP VPP-VP Sharon PP with meeting held Powell Powell VP NP 1 2 4 7 Chinese indices Figure 2: The alignment pattern (left) and"
N06-1033,N03-1021,0,0.0662649,"(Chiang, 2005; Galley et al., 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation. In particular: Melamed (2003) discusses binarization of multitext grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997) and the binary synchronous context-free grammar (SCFG) employed by the Hiero system (Chiang, 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language"
N06-1033,J04-4002,0,0.236624,"Missing"
N06-1033,H05-1101,0,0.681458,"VPP-VP → NP(1) VP(1) (2) , NP(1) VPP-VP VPP-VP (2) (2) PP , PP VP(1) (2) In this case m-gram integrated decoding can be done in O(|w|3+4(m−1) ) time which is much lowerorder polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG. The benefit of binary grammars also lies in synchronous parsing (alignment). Wu (1997) shows that parsing a binary SCFG is in O(|w|6 ) while parsing SCFG is NP-hard in general (Satta and Peserico, 2005). The same reasoning applies to tree transducer rules. Suppose we have the following tree-to-string rules, following Galley et al. (2004): (3) S(x0 :NP, VP(x2 :VP, x1 :PP)) → x0 x1 x2 NP(NNP(Powell)) → Baoweier VP(VBD(held), NP(DT(a) NPS(meeting))) → juxing le huitan PP(TO(with), NP(NNP(Sharon))) → yu Shalong where the reorderings of nonterminals are denoted by variables xi . Notice that the first rule has a multi-level lefthand side subtree. This system can model nonisomorphic transformations on English parse trees to “fit” another language, for example, learning that 258 the (S (V O)) struct"
N06-1033,C90-3045,0,0.251755,"Missing"
N06-1033,P96-1021,0,0.41387,"s production. language model score by adding Pr(with |meeting), and the resulting item again has two boundary words. Later we add Pr(held | Powell) whenthe Powell ··· Powell resulting item is combined with to NP 1 2 form an S item. As illustrated in Figure 2, VPP-VP has contiguous spans on both source and target sides, so that we can generate a binary-branching SCFG: (2) S→ VPP-VP → NP(1) VP(1) (2) , NP(1) VPP-VP VPP-VP (2) (2) PP , PP VP(1) (2) In this case m-gram integrated decoding can be done in O(|w|3+4(m−1) ) time which is much lowerorder polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG. The benefit of binary grammars also lies in synchronous parsing (alignment). Wu (1997) shows that parsing a binary SCFG is in O(|w|6 ) while parsing SCFG is NP-hard in general (Satta and Peserico, 2005). The same reasoning applies to tree transducer rules. Suppose we have the following tree-to-string rules, following Galley et al. (2004): (3) S(x0 :NP, VP(x2 :VP, x1 :PP)) → x0 x1 x2 NP(NNP(Powell)) → Baow"
N06-1033,J97-3002,0,0.851118,"s on the right hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation. In particular: Melamed (2003) discusses binarization of multitext grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997) and the binary synchronous context-free grammar (SCFG) employed by the Hiero system (Chiang, 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses. This approach results in rules with many nonterminals, making good binarization techniques critical. Suppose we have the following SCFG, where superscripts indicate reorderings (formal definitions of 256 Proceedings of the Human Language Tech"
N06-1033,W90-0102,0,\N,Missing
N06-1033,P05-1033,0,\N,Missing
N06-3004,J93-2003,0,0.0161413,"P PUNC (.) ) → x1 x2 ◦ Then, the rule r2 grabs the whole sub-tree for “the gunman” and translates it as a phrase: (r2 ) NP-C ( DT (the) NN (gunman) ) → qiangshou Now we get a “partial Chinese, partial English” sentence “qiangshou VP ◦ ” as shown in Fig. 3 (c). Our recursion goes on to translate the VP sub-tree. Here we use the rule r3 for the passive construction: 1 we will use LHS and source-side interchangeably (so are RHS and target-side). In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al. (1993). 225 , S (1) (2) VP NP↓ (2) VB↓ S VB↓ (1) NP↓ (3) NP↓ (3) NP↓ Figure 4: An example of complex re-ordering. ◦ VP VBD David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of the 43rd ACL. VP-C (r3 ) was x1 :VBN PP IN → Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of ICML, pages 175–182. bei x2 x1 Yuan Ding and Martha Palmer. 2005. Machine translation using probablisitic synchronous dependency insertion grammars. In Proceedings of the 43rd ACL. x2 :NP-C by which captures the fact that the agent (NP-C,"
N06-3004,H05-1036,0,0.0162422,"he Collins/Bikel parser (Bikel, 2004) and Chiang’s CKY-based Hiero decoder (Chiang, 2005), this algorithm is shown to have very little overhead even for quite large k (say, 106 ) (See Fig. 1 for experiments on Bikel parser). These algorithms have been re-implemented by other researchers in the field, including Eugene Charniak for his n-best parser, Ryan McDonald for his dependency parser (McDonald et al., 2005), Microsoft Research NLP group (Simon Corston-Oliver and Kevin Duh, p.c.) for a similar model, Jonathan Graehl for the ISI syntax-based MT decoder, David A. Smith for the Dyna language (Eisner et al., 2005), 223 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 223–226, c New York, June 2006. 2006 Association for Computational Linguistics 2 VPP-VP source (Chinese) NP Synchronous Binarization for MT NP Machine Translation has made very good progress in recent times, especially, the so-called “phrasebased” statistical systems (Och and Ney, 2004). In order to take a substantial next-step it will be necessary to incorporate several aspects of syntax. Many researchers have explored syntax-based methods, for instance, Wu (1996) and Chiang (2005) bo"
N06-3004,N04-1035,0,0.273734,"uistics 2 VPP-VP source (Chinese) NP Synchronous Binarization for MT NP Machine Translation has made very good progress in recent times, especially, the so-called “phrasebased” statistical systems (Och and Ney, 2004). In order to take a substantial next-step it will be necessary to incorporate several aspects of syntax. Many researchers have explored syntax-based methods, for instance, Wu (1996) and Chiang (2005) both uses binary-branching synchronous context-free grammars (SCFGs). However, to be more expressive and flexible, it is often easier to start with a general SCFG or tree-transducer (Galley et al., 2004). In this case, binarization of the input grammar is required for the use of the CKY algorithm (in order to get cubic-time complexity), just as we convert a CFG into the Chomsky Normal Form (CNF) for monolingual parsing. For synchronous grammars, however, different binarization schemes may result in very different-looking chart items that greatly affect decoding efficiency. For example, consider the following SCFG rule: PP VP VP PP target (English) English boundary words and Jonathan May for ISI’s tree automata package Tiburon. All of these experiments confirmed the findings in our work. VPP-V"
N06-3004,J02-3001,0,0.00597129,"5 5.5 4.5 3.5 2.5 1.5 1 10 100 k 1000 10000 Figure 1: Average parsing speed on the Section 23 of Penn Treebank (Algorithms 0, 1, and 3, log-log). k-best Parsing and Hypergraphs NLP systems are often cascades of several modules, e.g., part-of-speech tagging, then syntactic parsing, and finally semantic interpretation. It is often the case that the 1-best output from one module is not always optimal for the next module. So one might want to postpone some disambiguation by propagating k-best lists (instead of 1-best solutions) to subsequent phases, as in joint parsing and semantic role-labeling (Gildea and Jurafsky, 2002). This is also true for reranking and discriminative training, where the k-best list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al., 2005). In this way we can optimize some complicated objective function on the k-best set, rather than on the full search space which is usually exponentially large. Previous algorithms for k-best parsing (Collins, 2000; Charniak and Johnson, 2005) are either suboptimal or slow and rely significantly on pruning techniques to make them tractable. So I codeveloped several fast and exact algorithms for kbest parsin"
N06-3004,W05-1506,1,0.805188,"where the k-best list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al., 2005). In this way we can optimize some complicated objective function on the k-best set, rather than on the full search space which is usually exponentially large. Previous algorithms for k-best parsing (Collins, 2000; Charniak and Johnson, 2005) are either suboptimal or slow and rely significantly on pruning techniques to make them tractable. So I codeveloped several fast and exact algorithms for kbest parsing in the general framework of directed monotonic hypergraphs (Huang and Chiang, 2005). This formulation extends and refines Klein and Manning’s work (2001) by introducing monotonic weight functions, which is closely related to the optimal subproblem property in dynamic programming. We first generalize the classical 1-best Viterbi algorithm to hypergraphs, and then present four k-best algorithms, each improving its predessor by delaying more work until necessary. The final one, Algorithm 3, starts with a normal 1-best search for each vertex (or item, as in deductive frameworks), and then works backwards from the target vertex (final item) for its 2nd, 3rd, . . ., kth best deriv"
N06-3004,W05-1507,1,0.788687,"ese indices Figure 2: The alignment pattern (left) and alignment matrix (right) of the SCFG rule. system monolingual binarization synchronous binarization BLEU 36.25 38.44 Table 1: Synchronous vs. monolingual binarization in terms of translation quality (BLEU score). for it. Experiments show that the resulting rule set significantly improves the speed and accuracy over monolingual binarization (see Table 1) in a stateof-the-art syntax-based machine translation system (Galley et al., 2004). We also propose another trick (hook) for further speeding up the decoding with integrated n-gram models (Huang et al., 2005). (1) S → NP(1) VP(2) PP(3) , NP(1) PP(3) VP(2) 3 We can binarize it either left-to-right or right-to-left: Syntax-directed translation was originally proposed for compiling programming languages (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a syntax-tree that guides the generation of the object code. These translations have been formalized as a synchronous context-free grammar (SCFG) that generates two languages simultaneously (Aho and Ullman, 1972), and equivalently, as a top-down tree-to-string transducer (G´ecseg and Steinby, 1984). We adapt this syntaxdir"
N06-3004,2006.amta-papers.8,1,0.836348,"is parsed into a syntax-tree that guides the generation of the object code. These translations have been formalized as a synchronous context-free grammar (SCFG) that generates two languages simultaneously (Aho and Ullman, 1972), and equivalently, as a top-down tree-to-string transducer (G´ecseg and Steinby, 1984). We adapt this syntaxdirected transduction process to statistical MT by applying stochastic operations at each node of the source-language parse-tree and searching for the best derivation (a sequence of translation steps) that converts the whole tree into some target-language string (Huang et al., 2006). S→ VNP-PP → VNP-PP VP NP PP or S→ VPP-VP → NP VPP-VP PP VP The intermediate symbols (e.g. VPP-VP ) are called virtual nonterminals. We would certainly prefer the right-to-left binarization because the virtual nonterminal has consecutive span (see Fig. 2). The left-toright binarization causes discontinuities on the target side, which results in an exponential time complexity when decoding with an integrated n-gram model. We develop this intuition into a technique called synchronous binarization (Zhang et al., 2006) which binarizes a synchronous production or treetranduction rule on both sourc"
N06-3004,W01-1812,0,0.0826476,"Missing"
N06-3004,C04-1090,0,0.0426327,"Missing"
N06-3004,P05-1012,0,0.201125,"ten cascades of several modules, e.g., part-of-speech tagging, then syntactic parsing, and finally semantic interpretation. It is often the case that the 1-best output from one module is not always optimal for the next module. So one might want to postpone some disambiguation by propagating k-best lists (instead of 1-best solutions) to subsequent phases, as in joint parsing and semantic role-labeling (Gildea and Jurafsky, 2002). This is also true for reranking and discriminative training, where the k-best list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al., 2005). In this way we can optimize some complicated objective function on the k-best set, rather than on the full search space which is usually exponentially large. Previous algorithms for k-best parsing (Collins, 2000; Charniak and Johnson, 2005) are either suboptimal or slow and rely significantly on pruning techniques to make them tractable. So I codeveloped several fast and exact algorithms for kbest parsing in the general framework of directed monotonic hypergraphs (Huang and Chiang, 2005). This formulation extends and refines Klein and Manning’s work (2001) by introducing monotonic weight fun"
N06-3004,J04-4002,0,0.0235073,"d et al., 2005), Microsoft Research NLP group (Simon Corston-Oliver and Kevin Duh, p.c.) for a similar model, Jonathan Graehl for the ISI syntax-based MT decoder, David A. Smith for the Dyna language (Eisner et al., 2005), 223 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 223–226, c New York, June 2006. 2006 Association for Computational Linguistics 2 VPP-VP source (Chinese) NP Synchronous Binarization for MT NP Machine Translation has made very good progress in recent times, especially, the so-called “phrasebased” statistical systems (Och and Ney, 2004). In order to take a substantial next-step it will be necessary to incorporate several aspects of syntax. Many researchers have explored syntax-based methods, for instance, Wu (1996) and Chiang (2005) both uses binary-branching synchronous context-free grammars (SCFGs). However, to be more expressive and flexible, it is often easier to start with a general SCFG or tree-transducer (Galley et al., 2004). In this case, binarization of the input grammar is required for the use of the CKY algorithm (in order to get cubic-time complexity), just as we convert a CFG into the Chomsky Normal Form (CNF)"
N06-3004,P03-1021,0,0.00743554,"tems are often cascades of several modules, e.g., part-of-speech tagging, then syntactic parsing, and finally semantic interpretation. It is often the case that the 1-best output from one module is not always optimal for the next module. So one might want to postpone some disambiguation by propagating k-best lists (instead of 1-best solutions) to subsequent phases, as in joint parsing and semantic role-labeling (Gildea and Jurafsky, 2002). This is also true for reranking and discriminative training, where the k-best list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al., 2005). In this way we can optimize some complicated objective function on the k-best set, rather than on the full search space which is usually exponentially large. Previous algorithms for k-best parsing (Collins, 2000; Charniak and Johnson, 2005) are either suboptimal or slow and rely significantly on pruning techniques to make them tractable. So I codeveloped several fast and exact algorithms for kbest parsing in the general framework of directed monotonic hypergraphs (Huang and Chiang, 2005). This formulation extends and refines Klein and Manning’s work (2001) by introduc"
N06-3004,P96-1021,0,0.0474354,"language (Eisner et al., 2005), 223 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 223–226, c New York, June 2006. 2006 Association for Computational Linguistics 2 VPP-VP source (Chinese) NP Synchronous Binarization for MT NP Machine Translation has made very good progress in recent times, especially, the so-called “phrasebased” statistical systems (Och and Ney, 2004). In order to take a substantial next-step it will be necessary to incorporate several aspects of syntax. Many researchers have explored syntax-based methods, for instance, Wu (1996) and Chiang (2005) both uses binary-branching synchronous context-free grammars (SCFGs). However, to be more expressive and flexible, it is often easier to start with a general SCFG or tree-transducer (Galley et al., 2004). In this case, binarization of the input grammar is required for the use of the CKY algorithm (in order to get cubic-time complexity), just as we convert a CFG into the Chomsky Normal Form (CNF) for monolingual parsing. For synchronous grammars, however, different binarization schemes may result in very different-looking chart items that greatly affect decoding efficiency. F"
N06-3004,N06-1033,1,0.830411,"nslation steps) that converts the whole tree into some target-language string (Huang et al., 2006). S→ VNP-PP → VNP-PP VP NP PP or S→ VPP-VP → NP VPP-VP PP VP The intermediate symbols (e.g. VPP-VP ) are called virtual nonterminals. We would certainly prefer the right-to-left binarization because the virtual nonterminal has consecutive span (see Fig. 2). The left-toright binarization causes discontinuities on the target side, which results in an exponential time complexity when decoding with an integrated n-gram model. We develop this intuition into a technique called synchronous binarization (Zhang et al., 2006) which binarizes a synchronous production or treetranduction rule on both source and target sides simultaneously. It essentially converts an SCFG into an equivalent ITG (the synchronous extension of CNF) if possible. We reduce this problem to the binarization of the permutation of nonterminal symbols between the source and target sides of a synchronous rule and devise a linear-time algorithm 224 Syntax-Directed Translation 3.1 Extended Domain of Locality From a modeling perspective, however, the structural divergence across languages results in nonisomorphic parse-trees that are not captured b"
N06-3004,J04-4004,0,\N,Missing
N06-3004,P05-1067,0,\N,Missing
N06-3004,P05-1022,0,\N,Missing
N06-3004,P05-1033,0,\N,Missing
N12-1015,W02-1001,0,0.98687,"ical guarantee for convergence under new separability conditions. This framework subsumes and justifies the popular heuristic “early-update” for perceptron with beam search (Collins and Roark, 2004). We also propose several new update methods within this framework, among which the “max-violation” method dramatically reduces training time (by 3 fold as compared to earlyupdate) on state-of-the-art part-of-speech tagging and incremental parsing systems. 1 Introduction Discriminative structured prediction algorithms such as conditional random fields (Lafferty et al., 2001), structured perceptron (Collins, 2002), maxmargin markov networks (Taskar et al., 2003), and structural SVMs (Tsochantaridis et al., 2005) lead to state-of-the-art performance on many structured prediction problems such as part-of-speech tagging, sequence labeling, and parsing. But despite their success, there remains a major problem: these learning algorithms all assume exact inference (over an exponentially-large search space), which is needed to ensure their theoretical properties such as convergence. This exactness assumption, however, rarely holds in practice since exact inference is often intractable in many important proble"
N12-1015,P04-1015,0,0.861619,"m suphan.ying@gmail.com yangg86@gmail.com Abstract Most existing theory of structured prediction assumes exact inference, which is often intractable in many practical problems. This leads to the routine use of approximate inference such as beam search but there is not much theory behind it. Based on the structured perceptron, we propose a general framework of “violation-fixing” perceptrons for inexact search with a theoretical guarantee for convergence under new separability conditions. This framework subsumes and justifies the popular heuristic “early-update” for perceptron with beam search (Collins and Roark, 2004). We also propose several new update methods within this framework, among which the “max-violation” method dramatically reduces training time (by 3 fold as compared to earlyupdate) on state-of-the-art part-of-speech tagging and incremental parsing systems. 1 Introduction Discriminative structured prediction algorithms such as conditional random fields (Lafferty et al., 2001), structured perceptron (Collins, 2002), maxmargin markov networks (Taskar et al., 2003), and structural SVMs (Tsochantaridis et al., 2005) lead to state-of-the-art performance on many structured prediction problems such as"
N12-1015,P08-1067,1,0.824635,"ms such as part-of-speech tagging, sequence labeling, and parsing. But despite their success, there remains a major problem: these learning algorithms all assume exact inference (over an exponentially-large search space), which is needed to ensure their theoretical properties such as convergence. This exactness assumption, however, rarely holds in practice since exact inference is often intractable in many important problems such as machine translation (Liang et al., 2006), incremental parsing (Collins and Roark, 2004; Huang and Sagae, 2010), and bottom-up parsing (McDonald and Pereira, 2006; Huang, 2008). This leads to routine use of approximate inference such as beam search as evidenced in the above-cited papers, but the inexactness unfortunately abandons existing theoretical guarantees of the learning algorithms, and besides notable exceptions discussed below and in Section 7, little is known for theoretical properties of structured prediction under inexact search. Among these notable exceptions, many examine how and which approximations break theoretical guarantees of existing learning algorithms (Kulesza and Pereira, 2007; Finley and Joachims, 2008), but we ask a deeper and practically mo"
N12-1015,P10-1110,1,0.857988,") lead to state-of-the-art performance on many structured prediction problems such as part-of-speech tagging, sequence labeling, and parsing. But despite their success, there remains a major problem: these learning algorithms all assume exact inference (over an exponentially-large search space), which is needed to ensure their theoretical properties such as convergence. This exactness assumption, however, rarely holds in practice since exact inference is often intractable in many important problems such as machine translation (Liang et al., 2006), incremental parsing (Collins and Roark, 2004; Huang and Sagae, 2010), and bottom-up parsing (McDonald and Pereira, 2006; Huang, 2008). This leads to routine use of approximate inference such as beam search as evidenced in the above-cited papers, but the inexactness unfortunately abandons existing theoretical guarantees of the learning algorithms, and besides notable exceptions discussed below and in Section 7, little is known for theoretical properties of structured prediction under inexact search. Among these notable exceptions, many examine how and which approximations break theoretical guarantees of existing learning algorithms (Kulesza and Pereira, 2007; F"
N12-1015,P06-1096,0,0.0542697,"Missing"
N12-1015,J93-2004,0,0.0512932,"Missing"
N12-1015,E06-1011,0,0.0616198,"structured prediction problems such as part-of-speech tagging, sequence labeling, and parsing. But despite their success, there remains a major problem: these learning algorithms all assume exact inference (over an exponentially-large search space), which is needed to ensure their theoretical properties such as convergence. This exactness assumption, however, rarely holds in practice since exact inference is often intractable in many important problems such as machine translation (Liang et al., 2006), incremental parsing (Collins and Roark, 2004; Huang and Sagae, 2010), and bottom-up parsing (McDonald and Pereira, 2006; Huang, 2008). This leads to routine use of approximate inference such as beam search as evidenced in the above-cited papers, but the inexactness unfortunately abandons existing theoretical guarantees of the learning algorithms, and besides notable exceptions discussed below and in Section 7, little is known for theoretical properties of structured prediction under inexact search. Among these notable exceptions, many examine how and which approximations break theoretical guarantees of existing learning algorithms (Kulesza and Pereira, 2007; Finley and Joachims, 2008), but we ask a deeper and"
N12-1015,W96-0213,0,0.785399,"Missing"
N12-1015,P07-1096,0,0.054581,"Missing"
N12-1015,D08-1059,0,0.25654,"reira, 2007; Finley and Joachims, 2008), but we ask a deeper and practically more useful question: can we modify existing learning algorithms to accommodate the inexactness in inference, so that the theoretical properties are still maintained? For the structured perceptron, Collins and Roark (2004) provides a partial answer: they suggest variant called “early update” for beam search, which updates on partial hypotheses when the correct solution falls out of the beam. This method works significantly better than standard perceptron, and is followed by later incremental parsers, for instance in (Zhang and Clark, 2008; Huang and Sagae, 2010). However, two problems remain: first, up till now there has been no theoretical justification for early update; and secondly, it makes learning extremely slow as witnessed by the above-cited papers because it only learns on partial examples and often requires 15–40 iterations to converge while normal perceptron converges in 5–10 iterations (Collins, 2002). We develop a theoretical framework of “violationfixing” perceptron that addresses these challenges. In particular, we make the following contributions: • We show that, somewhat surprisingly, exact 142 2012 Conference"
N13-1038,P04-1015,0,0.232995,"pensive inference on each example for 10 or more passes over millions of examples, which often makes training too slow in practice; for example systems such as the popular (2nd-order) MST parser 2 Liang Huang2,1 Computer Science Dept, Queens College City University of New York huang@cs.qc.cuny.edu (McDonald and Pereira, 2006) usually require the order of days to train on the Treebank on a commodity machine (McDonald et al., 2010). There are mainly two ways to address this scalability problem. On one hand, researchers have been developing modified learning algorithms that allow inexact search (Collins and Roark, 2004; Huang et al., 2012). However, the learner still needs to loop over the whole training data (on the order of millions of sentences) many times. For example the best-performing method in Huang et al. (2012) still requires 5-6 hours to train a very fast parser. On the other hand, with the increasing popularity of multicore and cluster computers, there is a growing interest in speeding up training via parallelization. While batch learning such as CRF (Lafferty et al., 2001) is often trivially parallelizable (Chu et al., 2007) since each update is a batch-aggregate of the update from each (indepe"
N13-1038,W02-1001,0,0.63564,"cy, and the speedup is typically very small (∼3) even with many (10+) processors. We instead present a much simpler architecture based on “mini-batches”, which is trivially parallelizable. We show that, unlike previous methods, minibatch learning (in serial mode) actually improves the converged accuracy for both perceptron and MIRA learning, and when combined with simple parallelization, minibatch leads to very significant speedups (up to 9x on 12 processors) on stateof-the-art parsing and tagging systems. 1 Introduction Online structured learning algorithms such as the structured perceptron (Collins, 2002) and k-best MIRA (McDonald et al., 2005) have become more and more popular for many NLP tasks such as dependency parsing and part-of-speech tagging. This is because, compared to their batch learning counterparts, online learning methods offer faster convergence rates and better scalability to large datasets, while using much less memory and a much simpler architecture which only needs 1-best or k-best decoding. However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which often makes training too slow in practice;"
N13-1038,P08-1109,0,0.0705599,"Missing"
N13-1038,W10-2925,0,0.306414,"ion over many more constraints than in pure online MIRA, which could potentially lead to a better margin. In other words we can view MIRA as an online version or stepwise approximation of SVM, and minibatch MIRA can be seen as a better approximation as well as a middleground between pure MIRA and SVM.1 More interestingly, the minibatch architecture is trivially parallelizable since the examples within each minibatch could be decoded in parallel on multiple processors (while the update is still done in serial). This is known as “synchronous minibatch” and has been explored by many researchers (Gimpel et al., 2010; Finkel et al., 2008), but all previous works focus on probabilistic models along with SGD or EM learning methods while our work is the first effort on large-margin methods. We make the following contributions: • Theoretically, we present a serial minibatch framework (Section 3) for online large-margin learning and prove the convergence theorems for minibatch perceptron and minibatch MIRA. • Empirically, we show that serial minibatch could speed up convergence and improve the converged accuracy for both MIRA and perceptron on state-of-the-art dependency parsing and part-of-speech tagging syst"
N13-1038,P10-1110,1,0.879007,"typical structured prediction problems: incremental dependency parsing and part-of-speech tagging; both are done on state-of-the-art baseline. We also compare our parallelized minibatch algorithm with the iterative parameter mixing (IPM) method of McDonald et al. (2010). We perform our experiments on a commodity 64-bit Dell Precision T7600 workstation with two 3.1GHz 8-core CPUs (16 processors in total) and 64GB RAM. We use Python 2.7’s multiprocessing module in all experiments.4 5.1 Dependency Parsing with MIRA We base our experiments on our dynamic programming incremental dependency parser (Huang and Sagae, 2010).5 Following Huang et al. (2012), we use max-violation update and beam size b = 8. We evaluate on the standard Penn Treebank (PTB) using the standard split: Sections 02-21 for training, and Section 22 as the held-out set (which is indeed the test-set in this setting, following McDonald et al. (2010) and Gimpel et al. (2010)). We then extend it to employ 1-best MIRA learning. As stated in Section 2, MIRA separates the gold label y from the incorrect label z with a margin at least as large as the loss `(y, z). Here in incremental dependency parsing we define the loss function between a gold tree"
N13-1038,N12-1015,1,0.925236,"example for 10 or more passes over millions of examples, which often makes training too slow in practice; for example systems such as the popular (2nd-order) MST parser 2 Liang Huang2,1 Computer Science Dept, Queens College City University of New York huang@cs.qc.cuny.edu (McDonald and Pereira, 2006) usually require the order of days to train on the Treebank on a commodity machine (McDonald et al., 2010). There are mainly two ways to address this scalability problem. On one hand, researchers have been developing modified learning algorithms that allow inexact search (Collins and Roark, 2004; Huang et al., 2012). However, the learner still needs to loop over the whole training data (on the order of millions of sentences) many times. For example the best-performing method in Huang et al. (2012) still requires 5-6 hours to train a very fast parser. On the other hand, with the increasing popularity of multicore and cluster computers, there is a growing interest in speeding up training via parallelization. While batch learning such as CRF (Lafferty et al., 2001) is often trivially parallelizable (Chu et al., 2007) since each update is a batch-aggregate of the update from each (independent) example, onlin"
N13-1038,N09-1069,0,0.0862513,"tructured learning such as perceptron and MIRA. We argue that minibatch is advantageous in both serial and parallel settings. First, for minibatch perceptron in the serial set370 Proceedings of NAACL-HLT 2013, pages 370–379, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics ting, our intuition is that, although decoding is done independently within one minibatch, updates are done by averaging update vectors in batch, providing a “mixing effect” similar to “averaged parameters” of Collins (2002) which is also found in IPM (McDonald et al., 2010), and online EM (Liang and Klein, 2009). Secondly, minibatch MIRA in the serial setting has an advantage that, different from previous methods such as SGD which simply sum up the updates from all examples in a minibatch, a minibatch MIRA update tries to simultaneously satisfy an aggregated set of constraints that are collected from multiple examples in the minibatch. Thus each minibatch MIRA update involves an optimization over many more constraints than in pure online MIRA, which could potentially lead to a better margin. In other words we can view MIRA as an online version or stepwise approximation of SVM, and minibatch MIRA can"
N13-1038,E06-1011,0,0.027457,"pared to their batch learning counterparts, online learning methods offer faster convergence rates and better scalability to large datasets, while using much less memory and a much simpler architecture which only needs 1-best or k-best decoding. However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which often makes training too slow in practice; for example systems such as the popular (2nd-order) MST parser 2 Liang Huang2,1 Computer Science Dept, Queens College City University of New York huang@cs.qc.cuny.edu (McDonald and Pereira, 2006) usually require the order of days to train on the Treebank on a commodity machine (McDonald et al., 2010). There are mainly two ways to address this scalability problem. On one hand, researchers have been developing modified learning algorithms that allow inexact search (Collins and Roark, 2004; Huang et al., 2012). However, the learner still needs to loop over the whole training data (on the order of millions of sentences) many times. For example the best-performing method in Huang et al. (2012) still requires 5-6 hours to train a very fast parser. On the other hand, with the increasing popu"
N13-1038,P05-1012,0,0.527618,"very small (∼3) even with many (10+) processors. We instead present a much simpler architecture based on “mini-batches”, which is trivially parallelizable. We show that, unlike previous methods, minibatch learning (in serial mode) actually improves the converged accuracy for both perceptron and MIRA learning, and when combined with simple parallelization, minibatch leads to very significant speedups (up to 9x on 12 processors) on stateof-the-art parsing and tagging systems. 1 Introduction Online structured learning algorithms such as the structured perceptron (Collins, 2002) and k-best MIRA (McDonald et al., 2005) have become more and more popular for many NLP tasks such as dependency parsing and part-of-speech tagging. This is because, compared to their batch learning counterparts, online learning methods offer faster convergence rates and better scalability to large datasets, while using much less memory and a much simpler architecture which only needs 1-best or k-best decoding. However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which often makes training too slow in practice; for example systems such as the popular"
N13-1038,N10-1069,0,0.0580612,"calability to large datasets, while using much less memory and a much simpler architecture which only needs 1-best or k-best decoding. However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which often makes training too slow in practice; for example systems such as the popular (2nd-order) MST parser 2 Liang Huang2,1 Computer Science Dept, Queens College City University of New York huang@cs.qc.cuny.edu (McDonald and Pereira, 2006) usually require the order of days to train on the Treebank on a commodity machine (McDonald et al., 2010). There are mainly two ways to address this scalability problem. On one hand, researchers have been developing modified learning algorithms that allow inexact search (Collins and Roark, 2004; Huang et al., 2012). However, the learner still needs to loop over the whole training data (on the order of millions of sentences) many times. For example the best-performing method in Huang et al. (2012) still requires 5-6 hours to train a very fast parser. On the other hand, with the increasing popularity of multicore and cluster computers, there is a growing interest in speeding up training via paralle"
N15-1017,P06-1009,0,0.0317446,"show that unsupervised alignment is feasible, they considered the mappings between nouns and blobs only, and ignored the verbs and other relations in the sentences. Moreover, incorporating domain knowledge is not straightforward in these generative models. 2.2 Discriminative Word Alignment In machine translation, alignment of the words in source language with the words in target language has traditionally been done using the IBM word alignment models (Brown et al., 1993), which are generative models, and typically trained using Expectation Maximization (Dempster et al., 1977). Early attempts (Blunsom and Cohn, 2006; Taskar et al., 2005) towards discriminative word alignment relied on supervised hand-aligned parallel corpora. Dyer et al. (2011) first applied a latent variable conditional random field (LCRF) to perform unsupervised discriminative word alignment. They treated the words’ alignments as latent variables, and formulated the task as predicting the target sentence, given the source sentence. We apply similar latent variable discriminative models for unsupervised alignment of sentences with video segments. 3 Problem Formulation and Notations The input to our system is a dataset containing N pairs"
N15-1017,J93-2003,0,0.0625682,"lab experiment videos with associated text protocols, without any direct supervision. They proposed a hierarchical generative model to infer the alignment between each video segment with corresponding protocol sentence, and also the mapping of each blob with corresponding noun in that sentence. First, it models the generation of each video segment from one of the sentences in the protocol using a Hidden Markov Model (HMM) (Rabiner, 1989; Vogel et al., 1996). Next, each tracked object or blob in a video segment is generated from one of the nouns in the corresponding sentence using IBM Model 1 (Brown et al., 1993), a generative model frequently used in machine translation. The IBM Model 1 probabilities are incorporated as emission probabilities in HMM. The transition probabilities are parameterized using the jump size, i.e., the difference between the alignments of two consecutive video segments. They also extended IBM Model 1 by introducing latent variables for each noun, allowing some of the nonobject nouns to be unobserved in the video. While the alignment results are encouraging, and show that unsupervised alignment is feasible, they considered the mappings between nouns and blobs only, and ignored"
N15-1017,P05-1022,0,0.0138272,"ocols are not necessarily unique, as we have multiple videos of different people carrying out the same protocol. 166 xi = Xi,1 hi = 1 2 yi = Yi,1 Yi,2 Xi,2 Xi,3 2 3 Yi,3 Yi,4 Figure 2: Each Xi,m is a sentence in the protocol, consisting of the nouns and verbs in the sentence, and each Yi,n is a video chunk represented by the set of blobs touched by hands in that chunk. The alignment hi = [1, 2, 2, 3] maps each video chunk to the corresponding sentence. We apply similar data preprocessing as Naim et al. (2014). First, we parse each protocol sentence using the two-stage Charniak-Johnson parser (Charniak and Johnson, 2005), and extract the head nouns and verbs from each sentence. Let mi be the number of sentences in the protocol xi . We represent xi as a sequence of sets xi = [Xi,1 , . . . , Xi,mi ], where Xi,m is the set of nouns and verbs in the mth sentence of xi . Each video yi is segmented into a sequence of chunks, each one second long. For each video chunk, we determine the set of objects touched by the participant’s hands using automated image segmentation and tracking. We ignore the chunks over which no object is touched by a hand. Let ni be the number of chunks in yi . We represent the video yi as a s"
N15-1017,W02-1001,0,0.043586,"utational Linguistics tion to its corresponding video segment, and to align nouns in each instruction to their corresponding objects in the video. We extend this generative alignment framework by applying several discriminative models with latent variables. Discriminative models are attractive as they can easily incorporate domain knowledge by adding many diverse, overlapping, and complex features. By incorporating a large number of features and regularizing their weights properly, discriminative models have been shown to outperform generative models in many natural language processing tasks (Collins, 2002; Dyer et al., 2011; Yu et al., 2013). Similar to Naim et al. (2014), we applied our algorithm to align the natural language instructions for biological experiments in “wet laboratories” with recorded videos of people performing these experiments. Typically, each wetlab experiment has a protocol written in natural language, describing the sequence of steps necessary for that experiment. However, these instructions are often incomplete, and do not spell out implicit assumptions and knowledge, causing the results to be difficult to reproduce (Begley and Ellis, 2012). Given a set of such wetlab e"
N15-1017,P11-1042,0,0.203753,"istics tion to its corresponding video segment, and to align nouns in each instruction to their corresponding objects in the video. We extend this generative alignment framework by applying several discriminative models with latent variables. Discriminative models are attractive as they can easily incorporate domain knowledge by adding many diverse, overlapping, and complex features. By incorporating a large number of features and regularizing their weights properly, discriminative models have been shown to outperform generative models in many natural language processing tasks (Collins, 2002; Dyer et al., 2011; Yu et al., 2013). Similar to Naim et al. (2014), we applied our algorithm to align the natural language instructions for biological experiments in “wet laboratories” with recorded videos of people performing these experiments. Typically, each wetlab experiment has a protocol written in natural language, describing the sequence of steps necessary for that experiment. However, these instructions are often incomplete, and do not spell out implicit assumptions and knowledge, causing the results to be difficult to reproduce (Begley and Ellis, 2012). Given a set of such wetlab experiment protocols"
N15-1017,W13-1302,0,0.0200422,"Missing"
N15-1017,Q13-1016,0,0.0233218,"blobs, and ignored verbs, we incorporated the co-occurrences of verbs with blobs as features in our model. Finally, we propose a constrained variant of the standard LSP and LSSVM update rule, which provided better alignment accuracy and more stable convergence on our datasets. 165 2 2.1 Background Research Unsupervised Grounded Language Learning Most existing grounded language learning algorithms for integrating language with vision rely on either a fully supervised (Kollar et al., 2010; Matuszek et al., 2012) or a weakly supervised training stage (Yu and Ballard, 2004; Kate and Mooney, 2007; Krishnamurthy and Kollar, 2013; Yu and Siskind, 2013; Krishnamoorthy et al., 2013; Rohrbach et al., 2013; Tellex et al., 2013). The fully supervised methods assume that each sentence in the training data is manually paired with the corresponding image or video segment, and furthermore, each word or phrase in a sentence is already mapped to its corresponding blob or action in the image or video segment. Given the detailed annotations, these methods train a set of classifiers to recognize perceptual representations for commonly used words or phrases. After the initial fully supervised training stage, these methods can learn"
N15-1017,P06-1096,0,0.0370273,"Missing"
N15-1017,H05-1010,0,0.0253354,"lignment is feasible, they considered the mappings between nouns and blobs only, and ignored the verbs and other relations in the sentences. Moreover, incorporating domain knowledge is not straightforward in these generative models. 2.2 Discriminative Word Alignment In machine translation, alignment of the words in source language with the words in target language has traditionally been done using the IBM word alignment models (Brown et al., 1993), which are generative models, and typically trained using Expectation Maximization (Dempster et al., 1977). Early attempts (Blunsom and Cohn, 2006; Taskar et al., 2005) towards discriminative word alignment relied on supervised hand-aligned parallel corpora. Dyer et al. (2011) first applied a latent variable conditional random field (LCRF) to perform unsupervised discriminative word alignment. They treated the words’ alignments as latent variables, and formulated the task as predicting the target sentence, given the source sentence. We apply similar latent variable discriminative models for unsupervised alignment of sentences with video segments. 3 Problem Formulation and Notations The input to our system is a dataset containing N pairs of observations {(xi"
N15-1017,C96-2141,0,0.135175,"extend to new domains, as this may require collecting new annotated data. Recently, Naim et al. (2014) proposed a fully unsupervised approach for aligning wetlab experiment videos with associated text protocols, without any direct supervision. They proposed a hierarchical generative model to infer the alignment between each video segment with corresponding protocol sentence, and also the mapping of each blob with corresponding noun in that sentence. First, it models the generation of each video segment from one of the sentences in the protocol using a Hidden Markov Model (HMM) (Rabiner, 1989; Vogel et al., 1996). Next, each tracked object or blob in a video segment is generated from one of the nouns in the corresponding sentence using IBM Model 1 (Brown et al., 1993), a generative model frequently used in machine translation. The IBM Model 1 probabilities are incorporated as emission probabilities in HMM. The transition probabilities are parameterized using the jump size, i.e., the difference between the alignments of two consecutive video segments. They also extended IBM Model 1 by introducing latent variables for each noun, allowing some of the nonobject nouns to be unobserved in the video. While t"
N15-1017,P13-1006,0,0.168923,"orporated the co-occurrences of verbs with blobs as features in our model. Finally, we propose a constrained variant of the standard LSP and LSSVM update rule, which provided better alignment accuracy and more stable convergence on our datasets. 165 2 2.1 Background Research Unsupervised Grounded Language Learning Most existing grounded language learning algorithms for integrating language with vision rely on either a fully supervised (Kollar et al., 2010; Matuszek et al., 2012) or a weakly supervised training stage (Yu and Ballard, 2004; Kate and Mooney, 2007; Krishnamurthy and Kollar, 2013; Yu and Siskind, 2013; Krishnamoorthy et al., 2013; Rohrbach et al., 2013; Tellex et al., 2013). The fully supervised methods assume that each sentence in the training data is manually paired with the corresponding image or video segment, and furthermore, each word or phrase in a sentence is already mapped to its corresponding blob or action in the image or video segment. Given the detailed annotations, these methods train a set of classifiers to recognize perceptual representations for commonly used words or phrases. After the initial fully supervised training stage, these methods can learn the meaning of new wor"
N15-1017,D13-1112,1,0.889793,"corresponding video segment, and to align nouns in each instruction to their corresponding objects in the video. We extend this generative alignment framework by applying several discriminative models with latent variables. Discriminative models are attractive as they can easily incorporate domain knowledge by adding many diverse, overlapping, and complex features. By incorporating a large number of features and regularizing their weights properly, discriminative models have been shown to outperform generative models in many natural language processing tasks (Collins, 2002; Dyer et al., 2011; Yu et al., 2013). Similar to Naim et al. (2014), we applied our algorithm to align the natural language instructions for biological experiments in “wet laboratories” with recorded videos of people performing these experiments. Typically, each wetlab experiment has a protocol written in natural language, describing the sequence of steps necessary for that experiment. However, these instructions are often incomplete, and do not spell out implicit assumptions and knowledge, causing the results to be difficult to reproduce (Begley and Ellis, 2012). Given a set of such wetlab experiment protocols and associated vi"
N15-1108,W08-2102,0,0.0434111,"Missing"
N15-1108,A00-2018,0,0.703316,"Missing"
N15-1108,D11-1114,0,0.0443331,"Missing"
N15-1108,P04-1015,0,0.110605,"Missing"
N15-1108,I11-1136,0,0.0140991,"Missing"
N15-1108,P10-1110,1,0.573589,"the head word of its leftmost child). Figure 1: Shift-reduce system, omitting rexy . c is the model score, and csh , crexx , etc. are the action scores. NP state q: l0 : hS 0 |s01 , Q0 i : (c0 , v 0 ) l : hS|s1 |s0 , Qi : (c, v) l+1 : hS 0 |x(s01 , s0 ), Qi : (c0 +v+δ, v 0 +v+δ) l and l0 are even, p ∈ π(q) Figure 2: DP shift-reduce, omitting rexy and st. c and v are prefix and inside scores, and δ = csh (p) + crexx (q). State equivalence is defined below in Section 3. goal 2(2n − 1) : hs0 , i : c • l : hS, (t, w)|Qi : (c, v) l is even l+1 : hS|t(w), Qi : (c+csh , 0) VP VP0 PP V NP Following Huang and Sagae (2010), we represent feature templates as functions f (·, ·) on stack S and queue Q. Table 1 shows the 43 feature templates we use in this paper, all adopted from Zhu et al. (2013). They are combinations of the 32 atomic features ˜ f (S, Q) (e.g. s0 .t and s0 .c denote the head tag and 1031 3 Dynamic Programming The key idea towards DP is the merging of equivalent states, after which the stacks are organized in a “graph-structured stack” (GSS)(Tomita, 1988). Following Huang and Sagae (2010), “equivalent states” ∼ in a same beam are defined by the atomic features ˜ f (S, Q) and the span of s0 : hS, Q"
N15-1108,N12-1015,1,0.560902,"Missing"
N15-1108,P11-1068,0,0.0271685,"nd achieves the best accuracies among empirical linear-time parsers. 1 2 Introduction Incremental parsing has gained popularity in both dependency (Nivre, 2004; Zhang and Clark, 2008) and constituency parsing (Zhu et al., 2013; Wang and Xue, 2014). However, the greedy or beam search algorithms used in these parsers can only explore a tiny fraction of trees among exponentially many candidates. To alleviate this problem, Huang and Sagae (2010) propose a dynamic programming (DP) algorithm, reducing the search space to a polynomial size by merging equivalent states. This idea has been extended by Kuhlmann et al. (2011) and Cohen et al. (2011) to other dependency parsing paradigms. In constituency parsing, however, DP has not yet been applied to incremental parsing, and the bigger search space in constituency parsing suggests a potentially even bigger advantage by DP. However, with unary rules and more-than-binary branchings, constituency parsing presents challenges not found in dependency parsing that must be addressed before applying DP. Thus, we first present an odd-even Odd-Even Shift-Reduce CFG Parser One major challenge in constituency parsing is unary rules. Unlike dependency parsing where shiftreduce"
N15-1108,D11-1109,0,0.0130639,"Missing"
N15-1108,W04-0308,0,0.039793,"ent the first dynamic programming (DP) algorithm for shift-reduce constituency parsing, which extends the DP idea of Huang and Sagae (2010) to context-free grammars. To alleviate the propagation of errors from part-of-speech tagging, we also extend the parser to take a tag lattice instead of a fixed tag sequence. Experiments on both English and Chinese treebanks show that our DP parser significantly improves parsing quality over non-DP baselines, and achieves the best accuracies among empirical linear-time parsers. 1 2 Introduction Incremental parsing has gained popularity in both dependency (Nivre, 2004; Zhang and Clark, 2008) and constituency parsing (Zhu et al., 2013; Wang and Xue, 2014). However, the greedy or beam search algorithms used in these parsers can only explore a tiny fraction of trees among exponentially many candidates. To alleviate this problem, Huang and Sagae (2010) propose a dynamic programming (DP) algorithm, reducing the search space to a polynomial size by merging equivalent states. This idea has been extended by Kuhlmann et al. (2011) and Cohen et al. (2011) to other dependency parsing paradigms. In constituency parsing, however, DP has not yet been applied to incremen"
N15-1108,N07-1051,0,0.13874,"Missing"
N15-1108,W97-0301,0,0.35554,"Missing"
N15-1108,P06-2089,0,0.844202,"Missing"
N15-1108,P88-1031,0,0.704449,"ection 3. goal 2(2n − 1) : hs0 , i : c • l : hS, (t, w)|Qi : (c, v) l is even l+1 : hS|t(w), Qi : (c+csh , 0) VP VP0 PP V NP Following Huang and Sagae (2010), we represent feature templates as functions f (·, ·) on stack S and queue Q. Table 1 shows the 43 feature templates we use in this paper, all adopted from Zhu et al. (2013). They are combinations of the 32 atomic features ˜ f (S, Q) (e.g. s0 .t and s0 .c denote the head tag and 1031 3 Dynamic Programming The key idea towards DP is the merging of equivalent states, after which the stacks are organized in a “graph-structured stack” (GSS)(Tomita, 1988). Following Huang and Sagae (2010), “equivalent states” ∼ in a same beam are defined by the atomic features ˜ f (S, Q) and the span of s0 : hS, Qi ∼ hS 0 , Q0 i ⇔˜ f (S, Q) = ˜ f (S 0 , Q0 ) and s0 .span = s00 .span. Similarly, for each state p, π(p) is a set of predictor states, each of which can be combined with p in a rexx or rexy action. For each action, we have different operations on π(p). If a state p makes a sh action and generates a state p0 , then π(p0 ) = {p}. If two shifted states p0 and p00 are equivalent, p0 ∼ p00 , we merge π(p0 ) and π(p00 ). If a state p makes a reduce (rexx o"
N15-1108,P14-1069,0,0.130842,"ver, DP has not yet been applied to incremental parsing, and the bigger search space in constituency parsing suggests a potentially even bigger advantage by DP. However, with unary rules and more-than-binary branchings, constituency parsing presents challenges not found in dependency parsing that must be addressed before applying DP. Thus, we first present an odd-even Odd-Even Shift-Reduce CFG Parser One major challenge in constituency parsing is unary rules. Unlike dependency parsing where shiftreduce always finishes in 2n−1 steps, existing incremental constituency parsers (Zhu et al., 2013; Wang and Xue, 2014) reach the goal state (full parse tree) in different steps due to different number of unary rules. So we propose a new, synchronized, “oddeven” system to reach the goal in the same 4n − 2 steps. A state is notated p = hS, Qi, where S is a stack of trees ..., s1 , s0 , and Q is a queue of wordtag pairs. At even steps (when step index is even) we can choose one of the three standard actions • sh: shift the head of Q, a word-tag pair (t, w), onto S as a singleton tree t(w); • rexx : combine the top two trees on the stack and replace them with a new tree x(s1 , s0 ), x being the root nonterminal,"
N15-1108,D08-1059,0,0.0150903,"dynamic programming (DP) algorithm for shift-reduce constituency parsing, which extends the DP idea of Huang and Sagae (2010) to context-free grammars. To alleviate the propagation of errors from part-of-speech tagging, we also extend the parser to take a tag lattice instead of a fixed tag sequence. Experiments on both English and Chinese treebanks show that our DP parser significantly improves parsing quality over non-DP baselines, and achieves the best accuracies among empirical linear-time parsers. 1 2 Introduction Incremental parsing has gained popularity in both dependency (Nivre, 2004; Zhang and Clark, 2008) and constituency parsing (Zhu et al., 2013; Wang and Xue, 2014). However, the greedy or beam search algorithms used in these parsers can only explore a tiny fraction of trees among exponentially many candidates. To alleviate this problem, Huang and Sagae (2010) propose a dynamic programming (DP) algorithm, reducing the search space to a polynomial size by merging equivalent states. This idea has been extended by Kuhlmann et al. (2011) and Cohen et al. (2011) to other dependency parsing paradigms. In constituency parsing, however, DP has not yet been applied to incremental parsing, and the big"
N15-1108,N13-1038,1,0.665188,"Missing"
N15-1108,P13-1043,0,0.79332,"uce constituency parsing, which extends the DP idea of Huang and Sagae (2010) to context-free grammars. To alleviate the propagation of errors from part-of-speech tagging, we also extend the parser to take a tag lattice instead of a fixed tag sequence. Experiments on both English and Chinese treebanks show that our DP parser significantly improves parsing quality over non-DP baselines, and achieves the best accuracies among empirical linear-time parsers. 1 2 Introduction Incremental parsing has gained popularity in both dependency (Nivre, 2004; Zhang and Clark, 2008) and constituency parsing (Zhu et al., 2013; Wang and Xue, 2014). However, the greedy or beam search algorithms used in these parsers can only explore a tiny fraction of trees among exponentially many candidates. To alleviate this problem, Huang and Sagae (2010) propose a dynamic programming (DP) algorithm, reducing the search space to a polynomial size by merging equivalent states. This idea has been extended by Kuhlmann et al. (2011) and Cohen et al. (2011) to other dependency parsing paradigms. In constituency parsing, however, DP has not yet been applied to incremental parsing, and the bigger search space in constituency parsing su"
N15-1108,J03-4003,0,\N,Missing
N15-1162,P05-1022,0,0.0266748,"e difference between the best reference partial derivation and the Viterbi partial derivation: ∆ − i∗ = argmax w · ∆Φ(x, d+ i (x, y), di (x, y)), i − and do update w ← w + ∆Φ(x, d+ i∗ (x, y), di∗ (x, y)) ∆ where ∆Φ(x, d, d0 ) = Φ(x, d) − Φ(x, d0 ). We also use minibatch parallelization of Zhao and Huang (2013); in practice we use 24 cores. 5 Experiments We implement our type-driven incremental semantic parser (T ISP) using Python, and evaluate its performance on G EO Q UERY, J OBS, and ATIS datasets. Our feature design is inspired by the very effective Word-Edge features in syntactic parsing (Charniak and Johnson, 2005) and MT (He et al., 2008). From each parsing state, we collect atomic features including the types and the leftmost and rightmost words of the span of the top 3 MR expressions on the stack, the top 3 words on the queue, the grounded predicate names and the ID of the MR template used in the shift action. We use budget scheme similar to (Yu et al., 2013) to alleviate the overfitting problem caused by feature sparsity. We get 84 combined feature templates in total. Our final system contains 62 MR expression templates, of which 33 are triggered by POS tags, and 29 are triggered by specific phrases"
N15-1162,C08-1041,0,0.00996385,"nce partial derivation and the Viterbi partial derivation: ∆ − i∗ = argmax w · ∆Φ(x, d+ i (x, y), di (x, y)), i − and do update w ← w + ∆Φ(x, d+ i∗ (x, y), di∗ (x, y)) ∆ where ∆Φ(x, d, d0 ) = Φ(x, d) − Φ(x, d0 ). We also use minibatch parallelization of Zhao and Huang (2013); in practice we use 24 cores. 5 Experiments We implement our type-driven incremental semantic parser (T ISP) using Python, and evaluate its performance on G EO Q UERY, J OBS, and ATIS datasets. Our feature design is inspired by the very effective Word-Edge features in syntactic parsing (Charniak and Johnson, 2005) and MT (He et al., 2008). From each parsing state, we collect atomic features including the types and the leftmost and rightmost words of the span of the top 3 MR expressions on the stack, the top 3 words on the queue, the grounded predicate names and the ID of the MR template used in the shift action. We use budget scheme similar to (Yu et al., 2013) to alleviate the overfitting problem caused by feature sparsity. We get 84 combined feature templates in total. Our final system contains 62 MR expression templates, of which 33 are triggered by POS tags, and 29 are triggered by specific phrases. In the experiments, we"
N15-1162,N12-1015,1,0.513866,"argmax : ('a→t)→('a→i)→'a. And after the shift in step 8, the stack becomes capital : st→ct argmax : ('a→t)→('a→i)→'a state : st→t At step 9, in order to apply argmax onto state : st→t, we simply bind type variable 'a to type st, results in (argmax state) : (st→i)→st. After the shift in step 11, the stack becomes: capital : st→ct (argmax state) : (st→i)→st size : lo→i. 1419 A &lt;: B B→C &lt;: A→C 4 (4) Training: Latent Variable Perceptron We follow the latent variable max-violation perceptron algorithm of Yu et al. (2013) for training. This algorithm is based on the “violation-fixing” framework of Huang et al. (2012) which is tailored to structured learning problems with abundant search errors such as parsing or machine translation. The key challenge in the training is that, for each question, there might be many different unknown derivations that lead to its annotated MR, which is known as the spurious ambiguity. In our task, the spurious ambiguity is caused by how the MR templates are chosen and grounded during the shift step, and the different reduce orders that lead to the same result. We treat this unknown information as latent variable. More formally, we denote D(x) to be the set of all partial and"
N15-1162,P14-1112,0,0.0164357,"ntributions: • We design the first linear-time incremental semantic parsing algorithm (§2), which is much more efficient than the existing semantic parsers that are cubictime and CKY-based. 1416 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1416–1421, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics • In line with classical Montague theory (Heim and Kratzer, 1998), our parser is type-driven instead of syntax-driven as in CCG-based efforts (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014) (§2.3). • We introduce parametric polymorphism into natural language semantics (§3), along with proper treatment of subtype polymorphism, and implement Hindley-Milner style type inference (Pierce, 2005, Chap. 10) during parsing (§3.3).1 • We adapt the latent-variable max-violation perceptron training from machine translation (Yu et al., 2013), which is a perfect fit for semantic parsing due to its huge search space (§4). 2 Type-Driven Incremental Parsing We start with the simplest meaning representation (MR), untyped lambda calculus, and introduce typing and the incremental parsing algorithm"
N15-1162,D10-1119,0,0.060262,") Figure 3: Type hierarchy for J OBS domain (slightly simplified). Recall = 5.1 # of correctly parsed questions . # of questions Evaluation on G EO Q UERY Dataset We first evaluate T ISP on G EO Q UERY dataset. In the training and evaluating time, we use a very small beam size of 16, which gives us very fast decoding. In serial mode, our parser takes ∼ 83s to decode the 280 sentences (2,147 words) in the testing set, which means ∼ 0.3s per sentence, or ∼ 0.04s per word. We compare the our accuracy performance with existing methods (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) in Table 2. Given that all other methods use CKY-style parsing, our method is well balanced between accuracy and speed. In addition, to unveil the helpfulness of our type system, we train a parser with only simple types. (Table 2) In this setting, the predicates only have primitive types of location lo, integer i, and boolean t, while the constants still keep their types. It still has the type system, but it is weaker than the polymorphic one. Its accuracy is lower than the standard one, mostly caused by that the type system can not help pruning the wrong applicatio"
N15-1162,D11-1140,0,0.756859,". We make the following contributions: • We design the first linear-time incremental semantic parsing algorithm (§2), which is much more efficient than the existing semantic parsers that are cubictime and CKY-based. 1416 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1416–1421, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics • In line with classical Montague theory (Heim and Kratzer, 1998), our parser is type-driven instead of syntax-driven as in CCG-based efforts (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014) (§2.3). • We introduce parametric polymorphism into natural language semantics (§3), along with proper treatment of subtype polymorphism, and implement Hindley-Milner style type inference (Pierce, 2005, Chap. 10) during parsing (§3.3).1 • We adapt the latent-variable max-violation perceptron training from machine translation (Yu et al., 2013), which is a perfect fit for semantic parsing due to its huge search space (§4). 2 Type-Driven Incremental Parsing We start with the simplest meaning representation (MR), untyped lambda calculus, and introduce typing and"
N15-1162,D13-1161,0,0.0558532,"re: 3.2 which states the input side is reversed (contravariant). This might look counterintuitive, but the intuition is that, it is safe to allow the function size : lo→i to be used in the context where another type st→i is expected, since in that context the argument passed to size will be state type (st), which is a subtype of location type (lo) that size expects, which in turn will not surprise size. See the classical type theory textbook (Pierce, 2002, Chap. 15.2) for details. Several works in literature (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2013) employ some primitive type hierarchies and parse with typed lambda calculus. However, simply introducing subtyped predicates without polymorphism will cause type checking failures in handling high-order functions, as we discussed above. Semantics with Parametric Polymorphism The above type system works smoothly for first-order functions (i.e., predicates taking atomic type arguments), but the situation with higher-order functions (i.e., predicates that take functions as input) is more involved. What is the type of argmax in the context “the capital of largest state ...”? One possibility is to"
N15-1162,P11-1060,0,0.347221,"be of type state; similarly knowing mayor maps city to person disambiguates the second New York to be of type city. This can not be done using a simple type system with just entities and booleans. Now let us consider a more complex question which will be our running example in this paper: (3) What is the capital of the largest state by area? Introduction Most existing semantic parsing efforts employ a CKYstyle bottom-up parsing strategy to generate a meaning representation in simply typed lambda calculus (Zettlemoyer and Collins, 2005; Lu and Ng, 2011) or its variants (Wong and Mooney, 2007; Liang et al., 2011). Although these works led to fairly accurate semantic parsers, there are two major drawbacks: efficiency and expressiveness. First, as many researches in syntactic parsing (Nivre, 2008; Zhang and Clark, 2011) have shown, compared to cubic-time CKY-style parsing, incremental parsing can achieve comparable accuracies while being linear-time, and orders of magnitude faster in practice. We therefore introduce the first incremental parsing algorithm for semantic parsing. More interestingly, unlike syntactic parsing, our incremental semantic parsing algorithm, being strictly type-driven, directly e"
N15-1162,D11-1149,0,0.0461659,"on from state to person, then the first New York can only be of type state; similarly knowing mayor maps city to person disambiguates the second New York to be of type city. This can not be done using a simple type system with just entities and booleans. Now let us consider a more complex question which will be our running example in this paper: (3) What is the capital of the largest state by area? Introduction Most existing semantic parsing efforts employ a CKYstyle bottom-up parsing strategy to generate a meaning representation in simply typed lambda calculus (Zettlemoyer and Collins, 2005; Lu and Ng, 2011) or its variants (Wong and Mooney, 2007; Liang et al., 2011). Although these works led to fairly accurate semantic parsers, there are two major drawbacks: efficiency and expressiveness. First, as many researches in syntactic parsing (Nivre, 2008; Zhang and Clark, 2011) have shown, compared to cubic-time CKY-style parsing, incremental parsing can achieve comparable accuracies while being linear-time, and orders of magnitude faster in practice. We therefore introduce the first incremental parsing algorithm for semantic parsing. More interestingly, unlike syntactic parsing, our incremental semant"
N15-1162,J08-4003,0,0.0154958,"ans. Now let us consider a more complex question which will be our running example in this paper: (3) What is the capital of the largest state by area? Introduction Most existing semantic parsing efforts employ a CKYstyle bottom-up parsing strategy to generate a meaning representation in simply typed lambda calculus (Zettlemoyer and Collins, 2005; Lu and Ng, 2011) or its variants (Wong and Mooney, 2007; Liang et al., 2011). Although these works led to fairly accurate semantic parsers, there are two major drawbacks: efficiency and expressiveness. First, as many researches in syntactic parsing (Nivre, 2008; Zhang and Clark, 2011) have shown, compared to cubic-time CKY-style parsing, incremental parsing can achieve comparable accuracies while being linear-time, and orders of magnitude faster in practice. We therefore introduce the first incremental parsing algorithm for semantic parsing. More interestingly, unlike syntactic parsing, our incremental semantic parsing algorithm, being strictly type-driven, directly employs type checking to automatically determine the direction of function application on-the-fly, thus reducing the search space and elimi∗We thank the reviewers for helpful suggestions"
N15-1162,P07-1121,0,0.832774,"first New York can only be of type state; similarly knowing mayor maps city to person disambiguates the second New York to be of type city. This can not be done using a simple type system with just entities and booleans. Now let us consider a more complex question which will be our running example in this paper: (3) What is the capital of the largest state by area? Introduction Most existing semantic parsing efforts employ a CKYstyle bottom-up parsing strategy to generate a meaning representation in simply typed lambda calculus (Zettlemoyer and Collins, 2005; Lu and Ng, 2011) or its variants (Wong and Mooney, 2007; Liang et al., 2011). Although these works led to fairly accurate semantic parsers, there are two major drawbacks: efficiency and expressiveness. First, as many researches in syntactic parsing (Nivre, 2008; Zhang and Clark, 2011) have shown, compared to cubic-time CKY-style parsing, incremental parsing can achieve comparable accuracies while being linear-time, and orders of magnitude faster in practice. We therefore introduce the first incremental parsing algorithm for semantic parsing. More interestingly, unlike syntactic parsing, our incremental semantic parsing algorithm, being strictly ty"
N15-1162,D13-1112,1,0.834992,"ciation for Computational Linguistics • In line with classical Montague theory (Heim and Kratzer, 1998), our parser is type-driven instead of syntax-driven as in CCG-based efforts (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014) (§2.3). • We introduce parametric polymorphism into natural language semantics (§3), along with proper treatment of subtype polymorphism, and implement Hindley-Milner style type inference (Pierce, 2005, Chap. 10) during parsing (§3.3).1 • We adapt the latent-variable max-violation perceptron training from machine translation (Yu et al., 2013), which is a perfect fit for semantic parsing due to its huge search space (§4). 2 Type-Driven Incremental Parsing We start with the simplest meaning representation (MR), untyped lambda calculus, and introduce typing and the incremental parsing algorithm for it. Later in §3, we add subtyping and type polymorphism to enrich the system. 2.1 Meaning Representation with Types The untyped MR for the running example is: Q: What is the capital of the largest state by area? MR: (capital (argmax state size)) Note the binary function argmax(·, ·) is a higher-order function that takes two other functions"
N15-1162,D07-1071,0,0.929507,"s allowed, we have to shift (or skip). This observation suggests that our incremental parser is more deterministic than those syntactic incremental parsers where each step always faces a three-way decision (shift, left-reduce, right-reduce). We also note that this type-checking mechanism, inspired by the classical type-driven theory in linguistics (Heim and Kratzer, 1998), eliminates the need for an explicit encoding of direction as in CCG, which makes our formalism much simpler than the synchronous syntactic-semantic ones in most other semantic parsing efforts (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007).3 3 Subtype and Parametric Polymorphisms Currently in simply typed lambda calculus representation function capital can apply to any entity type, for example capital(boston), which should have been disallowed by the type checker. So we need a more sophisticated system that helps ground with refined types, which will in turn help type-driven parsing. 3 We need to distinguish between two concepts: a) “direction of reduction”: f (g) or g(f ). Obviously at any given time, between the top two (unarized) functions f and g on the stack, at most one reduction is possible. b) “o"
N15-1162,P11-1069,0,0.0163997,"us consider a more complex question which will be our running example in this paper: (3) What is the capital of the largest state by area? Introduction Most existing semantic parsing efforts employ a CKYstyle bottom-up parsing strategy to generate a meaning representation in simply typed lambda calculus (Zettlemoyer and Collins, 2005; Lu and Ng, 2011) or its variants (Wong and Mooney, 2007; Liang et al., 2011). Although these works led to fairly accurate semantic parsers, there are two major drawbacks: efficiency and expressiveness. First, as many researches in syntactic parsing (Nivre, 2008; Zhang and Clark, 2011) have shown, compared to cubic-time CKY-style parsing, incremental parsing can achieve comparable accuracies while being linear-time, and orders of magnitude faster in practice. We therefore introduce the first incremental parsing algorithm for semantic parsing. More interestingly, unlike syntactic parsing, our incremental semantic parsing algorithm, being strictly type-driven, directly employs type checking to automatically determine the direction of function application on-the-fly, thus reducing the search space and elimi∗We thank the reviewers for helpful suggestions. We are also grateful t"
N15-1162,N13-1038,1,0.350419,"hile the Viterbi partial derivation is ∆ d− i (x, y) = argmax w · Φ(x, d), d∈bad i (x,y) (6) where Φ(x, d) is the defined feature set for derivation d. In practice, to compute Eq. 6 exactly is intractable, and we resort to beam search. Following Yu et al. (2013), we then find the step i∗ with the maximal score difference between the best reference partial derivation and the Viterbi partial derivation: ∆ − i∗ = argmax w · ∆Φ(x, d+ i (x, y), di (x, y)), i − and do update w ← w + ∆Φ(x, d+ i∗ (x, y), di∗ (x, y)) ∆ where ∆Φ(x, d, d0 ) = Φ(x, d) − Φ(x, d0 ). We also use minibatch parallelization of Zhao and Huang (2013); in practice we use 24 cores. 5 Experiments We implement our type-driven incremental semantic parser (T ISP) using Python, and evaluate its performance on G EO Q UERY, J OBS, and ATIS datasets. Our feature design is inspired by the very effective Word-Edge features in syntactic parsing (Charniak and Johnson, 2005) and MT (He et al., 2008). From each parsing state, we collect atomic features including the types and the leftmost and rightmost words of the span of the top 3 MR expressions on the stack, the top 3 words on the queue, the grounded predicate names and the ID of the MR template used"
N19-1187,2014.iwslt-evaluation.1,0,0.123509,"Missing"
N19-1187,D17-1227,1,0.926496,"1 There are two types of “length ratios” in this paper: (a) target to reference ratio (|y|/|y∗ |), which is used in BLEU, and (b) target to source ratio (|y|/|x|). By default, the term “length ratio” in this paper refers to the former. 1884 Proceedings of NAACL-HLT 2019, pages 1884–1889 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics BSO relies on unnormalized raw scores instead of locally-normalized probabilities to get rid of the label bias problem. However, since the raw score can be either positive or negative, the optimal stopping criteria (Huang et al., 2017) no longer holds, e.g., one extra decoding step would increase the entire unfinished hypothesis’s model score when we have positive word score. This leads to two consequences: we do not know when to stop the beam search and it could return overlength translations (Fig. 1) or underlength translations (Fig. 3) in practice. As shown in Fig. 1, the BLEU score of BSO drops significantly when beam size gets larger as a result of overlong translations (as evidenced by length ratios larger than 1). Furthermore, BSO performs poorly (shown in Section 4) on hard translation pairs, e.g., Chinese→English ("
N19-1187,P17-4012,0,0.0608893,"ord’s probabilistic score is upper bounded by 1, in practice, the score usually far less than one. As described in (Huang et al., 2017; Yang et al., 2018), decoder always prefers short sentence when we use the probabilistic score function. To overcome the above so-called “beam search curse”, we propose to penalize early-stopped hypothesis within the beam during training. The procedure during training is illustrated in Fig. 2. Experiments We showcase the performance comparisons over three different datasets. We implement seq2seq model, BSO and our proposed model based on PyTorch-based OpenNMT (Klein et al., 2017). We use a two-layer bidirectional LSTM as the encoder and a two layer LSTM as the decoder. We train Seq2seq model for 20 epochs to minimize perplexity on the training dataset, with a batch size of 64, word embedding size of 512, the learning rate of 0.1, learning rate decay of 0.5 and dropout rate of 0.2. Following Wiseman and Rush (2016), we then train BSO and our model based on the previous Seq2seq model with the learning rate of 1886 Length Ratio Seq2seq† Train BSO† This work Beam BLEU Len. Beam BLEU Len. BLEU Len. 1 30.65 1.00 2 29.79 0.95 31.01 0.95 3 31.38 0.97 4 31.79 1.01 32.26 0.96 5"
N19-1187,P16-1159,0,0.0159988,"from a few crucial limitations, namely the label bias, the exposure bias, and the loss-evaluation mismatch (Lafferty et al., 2001; Bengio et al., ∗ Equal contribution 2015a; Venkatraman et al., 2015). In addition, more importantly, at decoding time, beam search is universally adopted to improve the search quality, while training is fundamentally local and greedy. Several researchers have proposed different approaches to alleviate above problems, such as reinforcement learning-based methods (Ranzato et al., 2016; Rennie et al., 2017; Zheng et al., 2018b), training with alternative references (Shen et al., 2016; Zheng et al., 2018a). Recently, Wiseman and Rush (2016) attempt to address these issues with a structured training method, Beam Search Optimization (BSO). While BSO outperforms other proposed methods on German-toEnglish translation, it also brings a different set of problems as partially discussed in (Ma, 2018) which we present with details below. 1 There are two types of “length ratios” in this paper: (a) target to reference ratio (|y|/|y∗ |), which is used in BLEU, and (b) target to source ratio (|y|/|x|). By default, the term “length ratio” in this paper refers to the former. 1884 Proceed"
N19-1187,D16-1137,0,0.39444,"ias, the exposure bias, and the loss-evaluation mismatch (Lafferty et al., 2001; Bengio et al., ∗ Equal contribution 2015a; Venkatraman et al., 2015). In addition, more importantly, at decoding time, beam search is universally adopted to improve the search quality, while training is fundamentally local and greedy. Several researchers have proposed different approaches to alleviate above problems, such as reinforcement learning-based methods (Ranzato et al., 2016; Rennie et al., 2017; Zheng et al., 2018b), training with alternative references (Shen et al., 2016; Zheng et al., 2018a). Recently, Wiseman and Rush (2016) attempt to address these issues with a structured training method, Beam Search Optimization (BSO). While BSO outperforms other proposed methods on German-toEnglish translation, it also brings a different set of problems as partially discussed in (Ma, 2018) which we present with details below. 1 There are two types of “length ratios” in this paper: (a) target to reference ratio (|y|/|y∗ |), which is used in BLEU, and (b) target to source ratio (|y|/|x|). By default, the term “length ratio” in this paper refers to the former. 1884 Proceedings of NAACL-HLT 2019, pages 1884–1889 c Minneapolis, Mi"
N19-1187,D18-1342,1,0.879867,"Missing"
N19-1187,D18-1357,1,0.773219,"models. However, this word-level training objective suffers from a few crucial limitations, namely the label bias, the exposure bias, and the loss-evaluation mismatch (Lafferty et al., 2001; Bengio et al., ∗ Equal contribution 2015a; Venkatraman et al., 2015). In addition, more importantly, at decoding time, beam search is universally adopted to improve the search quality, while training is fundamentally local and greedy. Several researchers have proposed different approaches to alleviate above problems, such as reinforcement learning-based methods (Ranzato et al., 2016; Rennie et al., 2017; Zheng et al., 2018b), training with alternative references (Shen et al., 2016; Zheng et al., 2018a). Recently, Wiseman and Rush (2016) attempt to address these issues with a structured training method, Beam Search Optimization (BSO). While BSO outperforms other proposed methods on German-toEnglish translation, it also brings a different set of problems as partially discussed in (Ma, 2018) which we present with details below. 1 There are two types of “length ratios” in this paper: (a) target to reference ratio (|y|/|y∗ |), which is used in BLEU, and (b) target to source ratio (|y|/|x|). By default, the term “len"
N19-1187,W18-6443,1,0.833581,"models. However, this word-level training objective suffers from a few crucial limitations, namely the label bias, the exposure bias, and the loss-evaluation mismatch (Lafferty et al., 2001; Bengio et al., ∗ Equal contribution 2015a; Venkatraman et al., 2015). In addition, more importantly, at decoding time, beam search is universally adopted to improve the search quality, while training is fundamentally local and greedy. Several researchers have proposed different approaches to alleviate above problems, such as reinforcement learning-based methods (Ranzato et al., 2016; Rennie et al., 2017; Zheng et al., 2018b), training with alternative references (Shen et al., 2016; Zheng et al., 2018a). Recently, Wiseman and Rush (2016) attempt to address these issues with a structured training method, Beam Search Optimization (BSO). While BSO outperforms other proposed methods on German-toEnglish translation, it also brings a different set of problems as partially discussed in (Ma, 2018) which we present with details below. 1 There are two types of “length ratios” in this paper: (a) target to reference ratio (|y|/|y∗ |), which is used in BLEU, and (b) target to source ratio (|y|/|x|). By default, the term “len"
N19-1187,P17-2053,1,0.488937,"ves </eos> red brown </eos> runs quickly a dogs cat barks with Data Figure 2: Training illustration with beam size b = 3 and gold reference “a brown dog runs with the wolves”. The gold reference is highlighted in blue solid boxes. We penalize the under-length translation (short) hypotheses by expelling out the early </eos> out of beam (red dashed boxes). The beam search restarts with gold when gold falls off the beam (at step 5). However, without using locally-normalized score does not mean that we should stop using the probabilistic value function. Similar with multi-label classification in (Ma et al., 2017), instead of using locally normalized softmax-based score and non-probabilistic raw scores, we propose to use another form of probabilistic scoring function, sigmoid function, which is defined as follows: gx (yt |y<t ) = (1 + e w·fx (yt |y<t ) −1 ) |x| Split Train Synthetic Valid Test Train De→En Valid Test∗ Train Zh→En Valid Test 9.47 9.54 9.51 17.53 17.55 18.89 23.21 29.53 26.53 σ(|x|) ( |y |) |x| 5.45 3.0 5.42 3.0 5.49 3.0 9.93 1.07 9.97 1.07 12.82 1.06 13.44 1.30 16.62 1.34 15.99 1.4 |y| σ( |x| ) # sents 0.52 0.53 0.52 0.16 0.16 0.16 0.33 0.22 0.24 5K 1K 1K 153K 7K 6.5K 1M 0.6K 0.7K Table"
P07-1019,W05-1506,1,0.657272,"Missing"
P07-1019,P05-1033,1,0.799037,"good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. 1 Introduction Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively t"
P07-1019,J07-2003,1,0.669096,"uage boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively to reduce it to a reasonable size. A much simpler alternative method to incorporate the LM is rescoring: we first decode without the LM (henceforth −LM decoding) to produce a k-best list of candidate translations, and then rerank the k-best list using the LM. This method runs much faster in practice but often produces a considerable number of search errors since the true best translation (taking LM into account) is often outside of the k-best list. Cube pruning (Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system. We push the idea behind this method further and make the following contributions in this paper: • We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al.,"
P07-1019,P97-1003,0,0.0193947,"2006). For instance, the following rule translates an English passive construction into Chinese: VP VBD was VP-C x1 :VBN IN → b`ei x2 x1 PP x2 :NP-C by Our tree-to-string system performs slightly better than the state-of-the-art phrase-based system Pharaoh on the above data set. Although different from the SCFG-based systems in Section 2, its 150 derivation trees remain context-free and the search space is still a hypergraph, where we can adapt the methods presented in Sections 3 and 4. The data set is same as in Section 5.1, except that we also parsed the English-side using a variant of the Collins (1997) parser, and then extracted 24.7M tree-to-string rules using the algorithm of (Galley et al., 2006). Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007). All the three +LM decoding methods to be compared below take these binarized forests as input. For cube growing, we use a non-duplicate k-best method (Huang et al., 2006) to get 100-best unique translations according to −LM to estimate the lower-bound heuristics.4 This preprocessing step takes on average 0.12 seconds per sentence, which is negligible in"
P07-1019,P06-1121,0,0.106096,"on quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. 1 Introduction Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively to reduce it to a reaso"
P07-1019,2006.amta-papers.8,1,0.690898,"(Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system. We push the idea behind this method further and make the following contributions in this paper: • We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al., 2006). • We also devise a faster variant of cube pruning, called cube growing, which uses a lazy version of k-best parsing (Huang and Chiang, 2005) that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the root. ∗ The authors would like to thank Dan Gildea, Jonathan Graehl, Mark Johnson, Kevin Knight, Daniel Marcu, Bob Moore and Hao Zhang. L. H. was partially supported by NSF ITR grants IIS-0428020 while visiting USC/ISI and EIA0205456 at UPenn. D. C. was partially supported under the GALE/DARPA program, contract HR0011-06-C-0022. Cube pruning and c"
P07-1019,W07-0405,1,0.699746,"ystem Pharaoh on the above data set. Although different from the SCFG-based systems in Section 2, its 150 derivation trees remain context-free and the search space is still a hypergraph, where we can adapt the methods presented in Sections 3 and 4. The data set is same as in Section 5.1, except that we also parsed the English-side using a variant of the Collins (1997) parser, and then extracted 24.7M tree-to-string rules using the algorithm of (Galley et al., 2006). Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007). All the three +LM decoding methods to be compared below take these binarized forests as input. For cube growing, we use a non-duplicate k-best method (Huang et al., 2006) to get 100-best unique translations according to −LM to estimate the lower-bound heuristics.4 This preprocessing step takes on average 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time. Figure 8(a) compares cube growing and cube pruning against full-integration under various beam settings in the same fashion of Figure 7(a). At the lowest level of search error, the relative speed-up from c"
P07-1019,J99-4005,0,0.310507,"the same levels of search error and translation accuracy. 1 Introduction Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively to reduce it to a reasonable size. A much simpler alternative method to incorporate the LM is rescoring: we first decode without the LM (henceforth −LM decoding) to produce a k-best list of candidate translations, and then rerank the k-best list using the LM. This method runs much faster in practice but often produces a considerable number"
P07-1019,koen-2004-pharaoh,0,0.421362,"outside of the k-best list. Cube pruning (Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system. We push the idea behind this method further and make the following contributions in this paper: • We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al., 2006). • We also devise a faster variant of cube pruning, called cube growing, which uses a lazy version of k-best parsing (Huang and Chiang, 2005) that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the root. ∗ The authors would like to thank Dan Gildea, Jonathan Graehl, Mark Johnson, Kevin Knight, Daniel Marcu, Bob Moore and Hao Zhang. L. H. was partially supported by NSF ITR grants IIS-0428020 while visiting USC/ISI and EIA0205456 at UPenn. D. C. was partially supported under the GALE/DARPA progr"
P07-1019,P06-1077,0,0.056508,"or of 10. The speed-up at the lowest searcherror level is a factor of 32. Figure 7(b) makes a similar comparison but measures search quality by BLEU, which shows an even larger relative speed-up for a given BLEU score, because translations with very different model costs might have similar BLEU scores. It also shows that our full-integration implementation in Cubit faithfully reproduces Pharaoh’s performance. Fixing the stack size to 100 and varying the threshold yielded a similar result. 5.2 Tree-to-string Decoding In tree-to-string (also called syntax-directed) decoding (Huang et al., 2006; Liu et al., 2006), the source string is first parsed into a tree, which is then recursively converted into a target string according to transfer rules in a synchronous grammar (Galley et al., 2006). For instance, the following rule translates an English passive construction into Chinese: VP VBD was VP-C x1 :VBN IN → b`ei x2 x1 PP x2 :NP-C by Our tree-to-string system performs slightly better than the state-of-the-art phrase-based system Pharaoh on the above data set. Although different from the SCFG-based systems in Section 2, its 150 derivation trees remain context-free and the search space is still a hypergr"
P07-1019,J04-4002,0,0.686321,"uage model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. 1 Introduction Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004). In practice, one must"
P07-1019,W05-0636,0,0.0127953,"Missing"
P07-1019,P96-1021,0,0.274875,"ecially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively to reduce it to a reasonable size. A much simpler alternative method to incorporate the LM is rescoring: we first decode without the LM (henceforth −LM decoding) to produce a k-best list of candidate translations, and then rerank the k-best list using the LM. This method runs much faster in practice but often produces a considerable number of search errors since the true best translation (taking LM into account) is often outside of the k-best list. Cube pruning (Chiang, 2007) is a compromise between rescoring and f"
P08-1023,P89-1018,0,0.664511,"re head (e) ∈ V is the consequent node in the deductive step, and tails(e) ∈ V ∗ is the list of antecedent nodes. For example, the hyperedge for deduction (*) is notated: h(NPB0,1 , CC1,2 , NPB2,3 ), NP0,3 i. There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S0,l where S is the start symbol and l is the sentence length. 3.2 Translation Forest 3.1 Parse Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Billot and Lang, 1989). For 194 Given a parse forest and a translation rule set R, we can generate a translation forest which has a similar hypergraph structure. Basically, just as the depthfirst traversal procedure in tree-based decoding (Figure 2), we visit in top-down order each node v in the IP0,6 VP1,6 NP0,3 (a) PP1,3 P1,2 CC1,2 NPB0,1 VPB3,6 NPB2,3 VV3,4 AS4,5 NR2,3 NR0,1 Sh¯al´ong yˇu B`ush´ı NPB5,6 NN5,6 jˇux´ıng le hu`ıt´an ⇓ translation rule set R IP0,6 e1 e2 NP0,3 e5 (b) VP1,6 e4 e3 PP1,3 NPB0,1 (c) translation hyperedge e1 e2 e3 e4 e5 e6 CC1,2 r1 r6 r3 r7 r8 r9 P1,2 VPB3,6 NPB2,3 VV3,4 AS4,5 e6 NPB5,6 t"
P08-1023,P05-1033,0,0.909811,"tially many parses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary"
P08-1023,J07-2003,0,0.655297,"he subtranslations of the matched variable nodes will be substituted for the variables in s(r) to get a complete translation for node v. So a translation hyperedge e is a triple htails(e), head (e), si where s is the target string from the rule, for example, up the computation. An +LM item of node v has the form (v a⋆b ), where a and b are the target-language ⋆ Sharon boundary words. For example, (VP held ) is an 1,6 +LM item with its translation starting with “held” and ending with “Sharon”. This scheme can be easily extended to work with a general n-gram by storing n − 1 words at both ends (Chiang, 2007). For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation forest and the LM, with its nodes being the +LM items during cube pruning. Although this new forest is prohibitively large, Algorithm 3 is very efficient with minimal overhead on top of 1-best. 3.4 Forest Pruning Algorithm We use the prun"
P08-1023,P05-1066,0,0.13289,"Missing"
P08-1023,P05-1067,0,0.390349,"1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and"
P08-1023,N04-1035,0,0.515361,"ide tree, whose internal nodes are labeled by nonterminal symbols in N , and whose frontier nodes are labeled by source-side terminals in Σ or variables from a set X = {x1 , x2 , . . .}; s ∈ (X ∪ ∆)∗ is the target-side string where ∆ is the target language terminal set; and φ is a mapping from X to nonterminals in N . Each variable xi ∈ X occurs exactly once in t and exactly once in s. We denote R to be the translation rule set. A similar formalism appears in another form in (Liu et al., 2006). These rules are in the reverse direction of the original string-to-tree transducer rules defined by Galley et al. (2004). Finally, from step (d) we apply rules r4 and r5 example, consider the Chinese sentence in Example (2) above, which has (at least) two readings depending on the part-of-speech of the word yˇu, which can be either a preposition (P “with”) or a conjunction (CC “and”). The parse tree for the preposition case is shown in Figure 2(b) as the 1-best parse, while for the conjunction case, the two proper nouns (B`ush´ı and Sh¯al´ong) are combined to form a coordinated NP NPB0,1 CC1,2 NP0,3 NPB2,3 (*) which functions as the subject of the sentence. In this case the Chinese sentence is translated into ("
P08-1023,P06-1121,0,0.50156,"rses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as"
P08-1023,W05-1506,1,0.816055,"ize. The pruned parse forest will then be used to direct the translation. In the decoding step, we first convert the parse forest into a translation forest using the translation rule set, by similar techniques of pattern-matching from tree-based decoding (Section 3.2). Then the decoder searches for the best derivation on the translation forest and outputs the target string (Section 3.3). Shown in Figure 3(a), these two parse trees can be represented as a single forest by sharing common subtrees such as NPB0,1 and VPB3,6 . Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like NP0,3 are called nodes, and deductive steps like (*) correspond to hyperedges. More formally, a forest is a pair hV, Ei, where V is the set of nodes, and E the set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of X i,j , which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head (e)i, where head (e) ∈ V is the consequent node in the deductive step, and tails(e) ∈ V ∗ is the list of antecedent nodes. For example, the hypered"
P08-1023,P07-1019,1,0.473411,"or each node v, and then compute the merit αβ(e) for each hyperedge: X β(ui ) (4) αβ(e) = α(head (e)) + ui ∈tails(e) e3 = h(NPB2,3 , NPB5,6 ), VP1,6 , “held x2 with x1 ”i. This procedure is summarized in Pseudocode 1. 3.3 Decoding Algorithms The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and k-best search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing (Huang and Chiang, 2005). For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Huang and Chiang (2005) to speed 196 Intuitively, this merit is the cost of the best derivation that traverses e, and the difference δ(e) = αβ(e) − β(TOP) can be seen as the distance away from the globally best derivation. We prune away a hyperedge e if δ(e) &gt; p for a threshold p. Nodes with all incoming hyperedges pruned are also pruned. 4 Experiments We can extend the s"
P08-1023,2006.amta-papers.8,1,0.895534,"decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2"
P08-1023,P08-1067,1,0.711475,"languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem is to take as input k-best parses, instead of a single tree. This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse. However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 25 < 50 < 26 ), and many subtrees are repeated across different parses (Huang, 2008). It is thus inefficient either to decode separately with each of these very similar trees. Longer sentences will also aggravate this situation as the number of parses grows exponentially with the sentence length. We instead propose a new approach, forest-based translation (Section 3), where the decoder translates a packed forest of exponentially many parses,1 1 There has been some confusion in the MT literature regarding the term forest: the word “forest” in “forest-to-string rules” 192 Proceedings of ACL-08: HLT, pages 192–199, c Columbus, Ohio, USA, June 2008. 2008 Association for Computati"
P08-1023,W01-1812,0,0.351606,"duce it to a reasonable size. The pruned parse forest will then be used to direct the translation. In the decoding step, we first convert the parse forest into a translation forest using the translation rule set, by similar techniques of pattern-matching from tree-based decoding (Section 3.2). Then the decoder searches for the best derivation on the translation forest and outputs the target string (Section 3.3). Shown in Figure 3(a), these two parse trees can be represented as a single forest by sharing common subtrees such as NPB0,1 and VPB3,6 . Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like NP0,3 are called nodes, and deductive steps like (*) correspond to hyperedges. More formally, a forest is a pair hV, Ei, where V is the set of nodes, and E the set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of X i,j , which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head (e)i, where head (e) ∈ V is the consequent node in the deductive step, and tails(e) ∈ V ∗ is the list of antecedent nodes."
P08-1023,N03-1017,0,0.0444445,"babilities of each parse hyperedge ep covered in the pattern-match of t against Hp (which can be recorded at conversion time): Y P(t |Hp ) = P(ep ). (8) ep ∈Hp , ep covered by t 4.1 Data preparation Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram language model with Kneser-Ney smoothing on the English side of the bitext. We use the 2002 NIST MT Evaluation test set as our development set (878 sentences) and the 2005 197 BLEU score on target translation. The derivation probability conditioned on 1-best tree"
P08-1023,koen-2004-pharaoh,0,0.0609331,"Missing"
P08-1023,C04-1090,0,0.192459,"ts over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free gram"
P08-1023,P06-1077,1,0.882144,"oints higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the la"
P08-1023,P07-1089,1,0.746035,"nguage string among all possible derivations D: ∗ d = arg max P(d|T ). d∈D À Æ (d) Bush (1) We will now proceed with a running example translating from Chinese to English: (2) r2 ⇓  &gt;L   B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an Bush with/and Sharon1 hold pass. talk2 “Bush held a talk2 with Sharon1 ” Figure 2 shows how this process works. The Chinese sentence (a) is first parsed into tree (b), which will be converted into an English string in 5 steps. First, at the root node, we apply rule r1 preserving top-level word-order between English and Chinese, (r1 ) IP(x1 :NPB x2 :VP) → x1 x2 (Liu et al., 2007) was a misnomer which actually refers to a set of several unrelated subtrees over disjoint spans, and should not be confused with the standard concept of packed forest. 193 hu`ıt´an r3 ⇓ held NPB with NN NR hu`ıt´an Sh¯al´ong r4 ⇓ (e) Bush NPB [held a talk]2 r5 ⇓ [with Sharon]1 Figure 2: An example derivation of tree-to-string translation. Shaded regions denote parts of the tree that is pattern-matched with the rule being applied. which results in two unfinished subtrees in (c). Then rule r2 grabs the B`ush´ı subtree and transliterate it (r2 ) NPB(NR(B`ush´ı)) → Bush. Similarly, rule r3 shown"
P08-1023,P03-1021,0,0.0947585,"bilities of translation rules r ∈ d: Y P(r) (6) P(d |Hp ) = 0.250 0.248 0.246 0.244 0.242 0.240 0.238 0.236 0.234 0.232 0.230 p=12 p=5 k=30 k=100 k=10 1-best k-best trees forests decoding 0 5 10 15 20 25 30 35 average decoding time (secs/sentence) Figure 4: Comparison of decoding on forests with decoding on k-best trees. NIST MT Evaluation test set as our test set (1082 sentences), with on average 28.28 and 26.31 words per sentence, respectively. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002). We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on the dev set. On dev and test sets, we prune the Chinese parse forests by the forest pruning algorithm in Section 3.4 with a threshold of p = 12, and then convert them into translation forests using the algorithm in Section 3.2. To increase the coverage of the rule set, we also introduce a default translation hyperedge for each parse hyperedge by monotonically translating each tail node, so that we can always at least get a complete translation in the end. 4.2 Results The BLEU score of the baseline 1-best decoding is 0.2325, wh"
P08-1023,P02-1040,0,0.104735,"where Hp is the parse forest, which decomposes into the product of probabilities of translation rules r ∈ d: Y P(r) (6) P(d |Hp ) = 0.250 0.248 0.246 0.244 0.242 0.240 0.238 0.236 0.234 0.232 0.230 p=12 p=5 k=30 k=100 k=10 1-best k-best trees forests decoding 0 5 10 15 20 25 30 35 average decoding time (secs/sentence) Figure 4: Comparison of decoding on forests with decoding on k-best trees. NIST MT Evaluation test set as our test set (1082 sentences), with on average 28.28 and 26.31 words per sentence, respectively. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002). We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on the dev set. On dev and test sets, we prune the Chinese parse forests by the forest pruning algorithm in Section 3.4 with a threshold of p = 12, and then convert them into translation forests using the algorithm in Section 3.2. To increase the coverage of the rule set, we also introduce a default translation hyperedge for each parse hyperedge by monotonically translating each tail node, so that we can always at least get a complete translation in the end. 4.2 Results"
P08-1023,W06-1608,0,0.0649947,"ne Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). This situation becomes worse with resource-poor source languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem is to take as input k-best parses, instead of a single tree. This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse. However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 25 < 50 < 26 ),"
P08-1023,P05-1034,0,0.485947,"result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution"
P08-1023,J97-3002,0,0.0539771,"of exponentially many parses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not re"
P08-1023,I05-1007,1,0.648768,"of rule r, respectively, P(t |s) and P(s |t) are the two translation probabilities, and Plex (·) are the lexical probabilities. The only extra term in forest-based decoding is P(t |Hp ) denoting the source side parsing probability of the current translation rule r in the parse forest, which is the product of probabilities of each parse hyperedge ep covered in the pattern-match of t against Hp (which can be recorded at conversion time): Y P(t |Hp ) = P(ep ). (8) ep ∈Hp , ep covered by t 4.1 Data preparation Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Mo"
P08-1023,N06-1033,1,0.506875,"tems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). This situation becomes worse with resource-poor source languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem is"
P08-1067,P89-1018,0,0.761695,"Missing"
P08-1067,E03-1005,0,0.0251313,"Missing"
P08-1067,A00-2018,0,0.836489,"587 We also denote IN (v) to be the set of incoming hyperedges of node v, which represent the different ways of deriving v. For example, in the forest in Figure 1, IN (VP1,6 ) is {e1 , e2 }, with e2 = h(VBD1,2 , NP2,6 ), VP1,6 i. We call |e |the arity of hyperedge e, which counts the number of tail nodes in e. The arity of a hypergraph is the maximum arity over all hyperedges. A CKY forest has an arity of 2, since the input grammar is required to be binary branching (cf. Chomsky Normal Form) to ensure cubic time parsing complexity. However, in this work, we use forests from a Treebank parser (Charniak, 2000) whose grammar is often flat in many productions. For example, the arity of the forest in Figure 1 is 3. Such a Treebank-style forest is easier to work with for reranking, since many features can be directly expressed in it. There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S0,l where S is the start symbol and l is the sentence length. 3 Forest Reranking 3.1 Generic Reranking with the Perceptron We first establish a unified framework for parse reranking with both n-best lists and packed forests. For a given sentence s, a generic rera"
P08-1067,J07-2003,0,0.261958,"gramming (DP) if all features are restricted to be local, that is, only looking at a local window within the factored search space (Taskar et al., 2004; McDonald et al., 2005). However, we miss the benefits of non-local features that are not representable here. Ideally, we would wish to combine the merits of both approaches, where an efficient inference algorithm could integrate both local and non-local features. Unfortunately, exact search is intractable (at least in theory) for features with unbounded scope. So we propose forest reranking, a technique inspired by forest rescoring (Huang and Chiang, 2007) that approximately reranks the packed forest of exponentially many parses. The key idea is to compute non-local features incrementally from bottom up, so that we can rerank the n-best subtrees at all internal nodes, instead of only at the root node as in conventional reranking (see Table 1). This method can thus be viewed as a step towards the integration of discriminative reranking with traditional chart parsing. Although previous work on discriminative parsing has mainly focused on short sentences (≤ 15 words) (Taskar et al., 2004; Turian and Melamed, 2007), our work scales to the whole Tre"
P08-1067,W02-1001,0,0.901649,"e y. For example, one such feature f2000 might be a question “how many times is a VP of length 5 surrounded by the word ‘has’ and the period? ” which is an instance of the WordEdges feature (see Figure 2(c) and Section 3.2 for details). Using a machine learning algorithm, the weight vector w can be estimated from the training data where each sentence si is labelled with its correct (“gold-standard”) parse yi∗ . As for the learner, Collins (2000) uses the boosting algorithm and Charniak and Johnson (2005) use the maximum entropy estimator. In this work we use the averaged perceptron algorithm (Collins, 2002) since it is an online algorithm much simpler and orders of magnitude faster than Boosting and MaxEnt methods. Shown in Pseudocode 1, the perceptron algorithm makes several passes over the whole training data, and in each iteration, for each sentence si , it tries to predict a best parse yˆi among the candidates cand (si ) using the current weight setting. Intuitively, we want the gold parse yi∗ to be picked, but in general it is not guaranteed to be within cand (si ), because the grammar may fail to cover the gold parse, and because the gold parse may be pruned away due to the limited scope o"
P08-1067,P04-1013,0,0.0129267,"Missing"
P08-1067,W05-1506,1,0.898664,"der the following sentence 0 I 1 saw 2 him 3 with 4 a 5 mirror 6 where the numbers between words denote string positions. Shown in Figure 1, this sentence has (at least) two derivations depending on the attachment of the prep. phrase PP3,6 “with a mirror”: it can either be attached to the verb “saw”, VBD1,2 NP2,3 VP1,6 PP3,6 , (*) or be attached to “him”, which will be further combined with the verb to form the same VP as above. These two derivations can be represented as a single forest by sharing common sub-derivations. Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like PP3,6 are called nodes, and deductive steps like (*) correspond to hyperedges. More formally, a forest is a pair hV, Ei, where V is the set of nodes, and E the set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of X i,j , which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head (e)i, where 587 We also denote IN (v) to be the set of incoming hyperedges of node v, which represent the different ways of deriving v. For example"
P08-1067,P07-1019,1,0.342957,"ynamic programming (DP) if all features are restricted to be local, that is, only looking at a local window within the factored search space (Taskar et al., 2004; McDonald et al., 2005). However, we miss the benefits of non-local features that are not representable here. Ideally, we would wish to combine the merits of both approaches, where an efficient inference algorithm could integrate both local and non-local features. Unfortunately, exact search is intractable (at least in theory) for features with unbounded scope. So we propose forest reranking, a technique inspired by forest rescoring (Huang and Chiang, 2007) that approximately reranks the packed forest of exponentially many parses. The key idea is to compute non-local features incrementally from bottom up, so that we can rerank the n-best subtrees at all internal nodes, instead of only at the root node as in conventional reranking (see Table 1). This method can thus be viewed as a step towards the integration of discriminative reranking with traditional chart parsing. Although previous work on discriminative parsing has mainly focused on short sentences (≤ 15 words) (Taskar et al., 2004; Turian and Melamed, 2007), our work scales to the whole Tre"
P08-1067,W01-1812,0,0.699144,"Missing"
P08-1067,J93-2004,0,0.0370864,"Missing"
P08-1067,N06-1020,0,0.465299,"Missing"
P08-1067,P05-1012,0,0.689292,"Missing"
P08-1067,N07-1051,0,0.147762,"Missing"
P08-1067,W04-3201,0,0.0426136,"Missing"
P08-1067,N04-1023,0,\N,Missing
P08-1067,P05-1022,0,\N,Missing
P08-1102,W02-1001,0,0.671743,"Introduction Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages. Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM, and usually behaves the best in the two tasks. Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron. Qun Liu † ‡ Yajuan Lu¨ † Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a bounda"
P08-1102,P06-1043,0,0.0212833,"Missing"
P08-1102,W04-3236,0,0.625941,"Lu¨ † Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004). Besides the usual character-based features, additional features dependent on POS’s or words can also be employed to improve the performance. However, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable"
P08-1102,J04-4002,0,0.00572952,"ever, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications. As a result, many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently. To cope with this problem, we propose a cascaded linear model inspired by the log-linear model (Och and Ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources. Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models. We 897 Proceedings of ACL-08: HLT, pages 897–904, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics f1 f2 f|R| α ~ Core Linear Model (Perceptron) P g1 = i αi × fi g1 Word LM: g2 = Pwlm (W ) g2 POS LM: g3 = Ptlm (T ) g3 Labelling: g4 = P (T |W ) g4 Generating: g5 = P (W |T ) g5 Length: g6"
P08-1102,P03-1021,0,0.00342095,"Missing"
P08-1102,W03-1728,0,0.0484926,"achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron. Qun Liu † ‡ Yajuan Lu¨ † Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004). Besides the usual character-based features, additional features dependent on POS’s or words can also be employed to improve the performance. However, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases ra"
P08-1102,P07-1106,0,0.142969,") C−1 C1 Lexical-target C0 Cn (n = −2..2) C0 Cn Cn+1 (n = −2..1) C0 C−1 C1 Instances C−2 =e, C−1 = , C0 =U, C1 =/, C2 =¡ C−2 C−1 =e , C−1 C0 = U, C0 C1 =U/, C1 C2 =/¡ C−1 C1 = / Instances C0 C−2 =Ue, C0 C−1 =U , C0 C0 =UU, C0 C1 =U/, C0 C2 =U¡ C0 C−2 C−1 =Ue , C0 C−1 C0 =U U, C0 C0 C1 =UU/, C0 C1 C2 =U/¡ C0 C−1 C1 = U / Table 1: Feature templates and instances. Suppose we are considering the third character ”U” in ”e U /¡”. to CRFs, while with much faster training. The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on. We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T. In following subsections, we describe the feature templates and the perceptron training algorithm. 3.1 Feature Templates The feature templates we adopted are selected from those of Ng and Low (2004). To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, P u(C0 ), indicating whether character C0 is a punctuation. All feature templates and their ins"
P09-1059,W06-2920,0,0.0564951,"Missing"
P09-1059,J07-2003,0,0.0113011,"Missing"
P09-1059,P04-1015,0,0.0199976,"ining Algorithm and Features Input: Training examples (xi , yi ) α ~ ←0 for t ← 1 .. T do for i ← 1 .. N do zi ← argmaxz∈GEN(xi ) Φ(xi , z) · α ~ if zi 6= yi then α ~ ←α ~ + Φ(xi , yi ) − Φ(xi , zi ) Output: Parameters α ~ Now we will show the training algorithm of the classifier and the features used. Several classification models can be adopted here, however, we choose the averaged perceptron algorithm (Collins, 2002) because of its simplicity and high accuracy. It is an online training algorithm and has been successfully used in many NLP tasks, such as POS tagging (Collins, 2002), parsing (Collins and Roark, 2004), Chinese word segmentation (Zhang and Clark, 2007; Jiang et al., 2008), and so on. Similar to the situation in other sequence labeling problems, the training procedure is to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results. Following Collins, we use a function GEN(x) enumerating the candidate results of an input x , a representation Φ mapping each training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd , and a parameter vector α ~ ∈ Rd corresponding to th"
P09-1059,W02-1001,0,0.343902,"e2 .. Cem−1 +1:em where each subsequence Ci:j indicates a Chinese word spanning from characters Ci to Cj (both in523 Algorithm 1 Perceptron training algorithm. 1: 2: 3: 4: 5: 6: 7: 8: 2.2 Training Algorithm and Features Input: Training examples (xi , yi ) α ~ ←0 for t ← 1 .. T do for i ← 1 .. N do zi ← argmaxz∈GEN(xi ) Φ(xi , z) · α ~ if zi 6= yi then α ~ ←α ~ + Φ(xi , yi ) − Φ(xi , zi ) Output: Parameters α ~ Now we will show the training algorithm of the classifier and the features used. Several classification models can be adopted here, however, we choose the averaged perceptron algorithm (Collins, 2002) because of its simplicity and high accuracy. It is an online training algorithm and has been successfully used in many NLP tasks, such as POS tagging (Collins, 2002), parsing (Collins and Roark, 2004), Chinese word segmentation (Zhang and Clark, 2007; Jiang et al., 2008), and so on. Similar to the situation in other sequence labeling problems, the training procedure is to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results. Following Collins, we use a function GEN(x)"
P09-1059,P07-1033,0,0.0655148,"Missing"
P09-1059,J93-2004,0,0.0333518,"Missing"
P09-1059,P08-1102,1,0.732523,"Missing"
P09-1059,W00-1201,0,0.0159542,"Missing"
P09-1059,D08-1017,0,0.0491519,"source standard to target standard. This regularity is incorporated together with the knowledge learnt from the target corpus itself, so as to obtain enhanced predication accuracy. For a given un-classified character sequence, the decoding is analogous to the training. First, the character sequence is input into the source classifier to obtain an source standard annotated classification result, then it is input into the target classifier with this classification result as additional information to get the final result. This coincides with the stacking method for combining dependency parsers (Martins et al., 2008; Nivre and McDonsource classifier source annotation classification result target classifier target annotation classification result Figure 3: The pipeline for decoding. ald, 2008), and is also similar to the Pred baseline for domain adaptation in (Daum´e III and Marcu, 2006; Daum´e III, 2007). Figures 2 and 3 show the flow charts for training and decoding. The utilization of the source classifier’s classification result as additional guide information resorts to the introduction of new features. For the current considering character waiting for classification, the most intuitive guide feature"
P09-1059,W06-1615,0,0.051839,"3) the HPSG LinGo Redwoods Treebank (Oepen et al., 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006). A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from financial news (PTB/WSJ) to transcribed dialog (LinGo). These two problems seem be a great waste in human efforts, and it would be nice if one could automatically adapt from one annotation standard and/or domain to another in order to exploit much larger datasets for better training. The second problem, domain adaptation, is very well-studied, e.g. by Blitzer et al. (2006) and Daum´e III (2007) (and see below for discussions), so in this paper we focus on the less studied, but equally important problem of annotationstyle adaptation. Introduction Much of statistical NLP research relies on some sort of manually annotated corpora to train their models, but these resources are extremely expensive to build, especially at a large scale, for example in treebanking (Marcus et al., 1993). However the linguistic theories underlying these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly differen"
P09-1059,E06-1011,0,0.0211674,"Missing"
P09-1059,J95-4004,0,0.212485,"e source classifier’s output, guide features can also be the classification results of several successive characters. We leave them as future research. 4 Table 2: An example of basic features and guide features of standard-adaptation for word segmentation. Suppose we are considering the third character “o” in “{B o Úu”. annotation-styles. Gao et al. (2004) described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style. They design some class-type transformation templates and use the transformation-based errordriven learning method of Brill (1995) to learn what word delimiters should be modified. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging, not to mention other structure labeling tasks. Moreover, the processing procedure is divided into two isolated steps, conversion after segmentation, which suffers from error propagation and wastes the knowledge in the corpora. On the contrary, our strategy is automatic, generalizable and effective. Related Works Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training impr"
P09-1059,W04-3236,0,0.320016,"tor Φ(x, y) ∈ Rd , and a parameter vector α ~ ∈ Rd corresponding to the feature vector. For an input character sequence x, we aim to find an output F (x) that satisfies: clusive). While in Joint S&T, each word is further annotated with a POS tag: C1:e1 /t1 Ce1 +1:e2 /t2 .. Cem−1 +1:em /tm where tk (k = 1..m) denotes the POS tag for the word Cek−1 +1:ek . 2.1 Character Classification Method Xue and Shen (2003) describe for the first time the character classification approach for Chinese word segmentation, where each character is given a boundary tag denoting its relative position in a word. In Ng and Low (2004), Joint S&T can also be treated as a character classification problem, where a boundary tag is combined with a POS tag in order to give the POS information of the word containing these characters. In addition, Ng and Low (2004) find that, compared with POS tagging after word segmentation, Joint S&T can achieve higher accuracy on both segmentation and POS tagging. This paper adopts the tag representation of Ng and Low (2004). For word segmentation only, there are four boundary tags: F (x) = argmax Φ(x, y) · α ~ (1) y∈GEN(x) where Φ(x, y)· α ~ denotes the inner product of feature vector Φ(x, y)"
P09-1059,P08-1108,0,0.0403739,"transformation-based errordriven learning method of Brill (1995) to learn what word delimiters should be modified. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging, not to mention other structure labeling tasks. Moreover, the processing procedure is divided into two isolated steps, conversion after segmentation, which suffers from error propagation and wastes the knowledge in the corpora. On the contrary, our strategy is automatic, generalizable and effective. Related Works Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. The co-training technology lets two different parsing models learn from each other during parsing an unlabelled corpus: one model selects some unlabelled sentences it can confidently parse, and provide them to the other model as additional training corpus in order to train more powerful parsers. The classifier combination lets graph-based and transition-based dependency parsers to utilize the features extracted from each other’s parsing results, to obtain combined, enhanced parsers. The two technologies aim to let two models learn"
P09-1059,C02-2025,0,0.0196693,"Missing"
P09-1059,N01-1023,0,0.0304051,"type transformation templates and use the transformation-based errordriven learning method of Brill (1995) to learn what word delimiters should be modified. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging, not to mention other structure labeling tasks. Moreover, the processing procedure is divided into two isolated steps, conversion after segmentation, which suffers from error propagation and wastes the knowledge in the corpora. On the contrary, our strategy is automatic, generalizable and effective. Related Works Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. The co-training technology lets two different parsing models learn from each other during parsing an unlabelled corpus: one model selects some unlabelled sentences it can confidently parse, and provide them to the other model as additional training corpus in order to train more powerful parsers. The classifier combination lets graph-based and transition-based dependency parsers to utilize the features extracted from each other’s parsing results, to obtain combined, enhanced par"
P09-1059,I05-1007,1,0.604558,"Missing"
P09-1059,W03-1728,0,0.0997578,"corresponding labelled results. Following Collins, we use a function GEN(x) enumerating the candidate results of an input x , a representation Φ mapping each training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd , and a parameter vector α ~ ∈ Rd corresponding to the feature vector. For an input character sequence x, we aim to find an output F (x) that satisfies: clusive). While in Joint S&T, each word is further annotated with a POS tag: C1:e1 /t1 Ce1 +1:e2 /t2 .. Cem−1 +1:em /tm where tk (k = 1..m) denotes the POS tag for the word Cek−1 +1:ek . 2.1 Character Classification Method Xue and Shen (2003) describe for the first time the character classification approach for Chinese word segmentation, where each character is given a boundary tag denoting its relative position in a word. In Ng and Low (2004), Joint S&T can also be treated as a character classification problem, where a boundary tag is combined with a POS tag in order to give the POS information of the word containing these characters. In addition, Ng and Low (2004) find that, compared with POS tagging after word segmentation, Joint S&T can achieve higher accuracy on both segmentation and POS tagging. This paper adopts the tag rep"
P09-1059,P07-1106,0,0.224874,"(xi , yi ) α ~ ←0 for t ← 1 .. T do for i ← 1 .. N do zi ← argmaxz∈GEN(xi ) Φ(xi , z) · α ~ if zi 6= yi then α ~ ←α ~ + Φ(xi , yi ) − Φ(xi , zi ) Output: Parameters α ~ Now we will show the training algorithm of the classifier and the features used. Several classification models can be adopted here, however, we choose the averaged perceptron algorithm (Collins, 2002) because of its simplicity and high accuracy. It is an online training algorithm and has been successfully used in many NLP tasks, such as POS tagging (Collins, 2002), parsing (Collins and Roark, 2004), Chinese word segmentation (Zhang and Clark, 2007; Jiang et al., 2008), and so on. Similar to the situation in other sequence labeling problems, the training procedure is to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results. Following Collins, we use a function GEN(x) enumerating the candidate results of an input x , a representation Φ mapping each training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd , and a parameter vector α ~ ∈ Rd corresponding to the feature vector. For an input character sequence"
P09-1059,P05-1012,0,\N,Missing
P09-1059,P04-1059,0,\N,Missing
P09-1059,J07-3004,0,\N,Missing
P10-1110,P89-1018,0,0.37637,"Missing"
P10-1110,A00-2018,0,0.284436,"implementation word 90.2 90.9 92.0 91.4 92.1 92.5 92.4 92.1 93.2 L Ja Ja − C Py C Ja C − time 0.12 0.15 − 0.11 0.04 0.49 0.21 − − comp. O(n2 ) O(n3 ) O(n4 ) O(n)‡ O(n) O(n5 ) O(n3 ) O(n2 )‡ O(n4 ) Table 3: Final test results on English (PTB). Our parser (in pure Python) has the highest accuracy among dependency parsers trained on the Treebank, and is also much faster than major parsers. † converted from constituency trees. C=C/C++, Py=Python, Ja=Java. Time is in seconds per sentence. Search spaces: ‡ linear; others exponential. (on a 3.2GHz Xeon CPU). Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower. Figure 8 shows the parse time in seconds for each test sentence. The observed time complexity of our DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly com"
P10-1110,P04-1015,0,0.840681,"ose dotproducts with the weight vector decide the best action (see Eqs. (1-3) in Fig. 1). 2.3 Beam Search and Early Update To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel. At each step we extend the states in the current beam by applying one of the three actions, and then choose the best b resulting states for the next step. Our dynamic programming algorithm also runs on top of beam search in practice. To train the model, we use the averaged perceptron algorithm (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the gold-standard action-sequence falls off the beam, with the rest of the sequence neglected.3 The intuition behind this strategy is that later mistakes are often caused by previous ones, and are irrelevant when the parser is on the wrong track. Dynamic programming turns out to be a great fit for early updating (see Section 4.3 for details). 3 Dynamic Programming (DP) 3.1 Merging Equivalent States The key observation for dynamic programming is to merge “equivalent states” in the same beam 3 As a special case, for the d"
P10-1110,W02-1001,0,0.299237,"ined feature instances, whose dotproducts with the weight vector decide the best action (see Eqs. (1-3) in Fig. 1). 2.3 Beam Search and Early Update To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel. At each step we extend the states in the current beam by applying one of the three actions, and then choose the best b resulting states for the next step. Our dynamic programming algorithm also runs on top of beam search in practice. To train the model, we use the averaged perceptron algorithm (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the gold-standard action-sequence falls off the beam, with the rest of the sequence neglected.3 The intuition behind this strategy is that later mistakes are often caused by previous ones, and are irrelevant when the parser is on the wrong track. Dynamic programming turns out to be a great fit for early updating (see Section 4.3 for details). 3 Dynamic Programming (DP) 3.1 Merging Equivalent States The key observation for dynamic programming is to merge “equivalent states” in the same"
P10-1110,P81-1022,0,0.822739,"Way Marina del Rey, CA 90292 sagae@ict.usc.edu Abstract that runs in (almost) linear-time, yet searches over a huge space with dynamic programming? Theoretically, the answer is negative, as Lee (2002) shows that context-free parsing can be used to compute matrix multiplication, where sub-cubic algorithms are largely impractical. We instead propose a dynamic programming alogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice. The key idea is to merge equivalent stacks according to feature functions, inspired by Earley parsing (Earley, 1970; Stolcke, 1995) and generalized LR parsing (Tomita, 1991). However, our formalism is more flexible and our algorithm more practical. Specifically, we make the following contributions: Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on featu"
P10-1110,P99-1059,0,0.381041,"e inside cost of the current state p, and the combo cost. Note the prefix cost of q is irrelevant. The combo cost δ = ξ ′ + λ consists of shift cost ξ ′ of p and reduction cost λ of q. The cost in the non-DP shift-reduce algorithm (Fig. 1) is indeed a prefix cost, and the DP algorithm subsumes the non-DP one as a special case where no two states are equivalent. 3.5 Example: Edge-Factored Model As a concrete example, Figure 4 simulates an edge-factored model (Eisner, 1996; McDonald et al., 2005a) using shift-reduce with dynamic programming, which is similar to bilexical PCFG parsing using CKY (Eisner and Satta, 1999). Here the kernel feature function is e f (j, S) = (j, h(s1 ), h(s0 )) 5 Note that using inside cost v for ordering would be a bad idea, as it will always prefer shorter derivations like in best-first parsing. As in A* search, we need some estimate of “outside cost” to predict which states are more promising, and the prefix cost includes an exact cost for the left outside context, but no right outside context. 1081 ℓ : h , h i : (c, ) quality; in Sec. 4.3 we will retrain the model with DP and compare it against training with non-DP. ...j j<n ℓ + 1 : hh, ji : (c, 0) sh ′′ ′ ′ ′ : hh , h i : (c"
P10-1110,C96-1058,0,0.954766,"combo cost δ from the combination, while the resulting prefix cost c′ + v + δ is the sum of the prefix cost of the predictor state q, the inside cost of the current state p, and the combo cost. Note the prefix cost of q is irrelevant. The combo cost δ = ξ ′ + λ consists of shift cost ξ ′ of p and reduction cost λ of q. The cost in the non-DP shift-reduce algorithm (Fig. 1) is indeed a prefix cost, and the DP algorithm subsumes the non-DP one as a special case where no two states are equivalent. 3.5 Example: Edge-Factored Model As a concrete example, Figure 4 simulates an edge-factored model (Eisner, 1996; McDonald et al., 2005a) using shift-reduce with dynamic programming, which is similar to bilexical PCFG parsing using CKY (Eisner and Satta, 1999). Here the kernel feature function is e f (j, S) = (j, h(s1 ), h(s0 )) 5 Note that using inside cost v for ordering would be a bad idea, as it will always prefer shorter derivations like in best-first parsing. As in A* search, we need some estimate of “outside cost” to predict which states are more promising, and the prefix cost includes an exact cost for the left outside context, but no right outside context. 1081 ℓ : h , h i : (c, ) quality; in S"
P10-1110,W05-1506,1,0.790183,"k and Johnson, 2005; Huang, 2008).8 4.3 Perceptron Training and Early Updates Another interesting advantage of DP over non-DP is the faster training with perceptron, even when both parsers use the same beam width. This is due to the use of early updates (see Sec. 2.3), which happen much more often with DP, because a goldstandard state p is often merged with an equivalent (but incorrect) state that has a higher model score, which triggers update immediately. By contrast, in non-DP beam search, states such as p might still 8 DP’s k-best lists are extracted from the forest using the algorithm of Huang and Chiang (2005), rather than those in the final beam as in the non-DP case, because many derivations have been merged during dynamic programming. 1082 b=16 dependency accuracy DP non-DP 0 avg. model score b=64 93.1 93 92.9 92.8 92.7 92.6 92.5 92.4 92.3 92.2 0.05 0.1 0.15 0.2 0.25 0.3 0.35 b=16 b=64 DP non-DP 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 (a) search quality vs. time (full model) (b) parsing accuracy vs. time (full model) 2335 2330 2325 2320 2315 2310 2305 2300 2295 2290 93.5 93 92.5 92 91.5 91 90.5 full, DP 90 full, non-DP 89.5 edge-factor, DP 89 edge-factor, non-DP 88.5 2280 2300 2320 2340 2360 2380 2400"
P10-1110,D09-1127,1,0.86707,"ing. Shift-Reduce Parsing 2.1 Vanilla Shift-Reduce Shift-reduce parsing performs a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items at the end of the stack (Aho and Ullman, 1972). To adapt it to dependency parsing, we split the reduce action into two cases, rex and rey, depending on which one of the two items becomes the head after reduction. This procedure is known as “arc-standard” (Nivre, 2004), and has been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state hj, Si where S is a stack of trees s0 , s1 , ... where s0 is the top tree, and j is the 2 stack Figure 2: A trace of vanilla shift-reduce. After step (4), the parser branches off into (5a) or (5b). ℓ : hj, S|s1 |s0 i : c where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: 2 action sh sh rex sh rey sh There is another popular variant, “arc-eager” (Nivre, 2004; Zhang and Clark, 2008), which is more complicated and less similar to the classical"
P10-1110,P08-1067,1,0.84341,"Fig. 5d shows the (almost linear) correlation between dependency accuracy and search quality, confirming that better search yields better parsing. 4.2 Search Space, Forest, and Oracles DP achieves better search quality because it expores an exponentially large search space rather than only b trees allowed by the beam (see Fig. 6a). As a by-product, DP can output a forest encoding these exponentially many trees, out of which we can draw longer and better (in terms of oracle) kbest lists than those in the beam (see Fig. 6b). The forest itself has an oracle of 98.15 (as if k → ∞), computed a` la Huang (2008, Sec. 4.1). These candidate sets may be used for reranking (Charniak and Johnson, 2005; Huang, 2008).8 4.3 Perceptron Training and Early Updates Another interesting advantage of DP over non-DP is the faster training with perceptron, even when both parsers use the same beam width. This is due to the use of early updates (see Sec. 2.3), which happen much more often with DP, because a goldstandard state p is often merged with an equivalent (but incorrect) state that has a higher model score, which triggers update immediately. By contrast, in non-DP beam search, states such as p might still 8 DP’"
P10-1110,J98-4004,0,0.0478774,"window and can only extract bounded information on each tree, which is always the case in practice since we can not have infinite models. Monotonicity, on the other hand, says that features drawn from trees farther away from the top should not be more refined than from those closer to the top. This is also natural, since the information most relevant to the current decision is always around the stack top. For example, the kernel feature function in Eq. 5 is bounded and monotonic, since f2 is less refined than f1 and f0 . These two requirements are related to grammar refinement by annotation (Johnson, 1998), where annotations must be bounded and monotonic: for example, one cannot refine a grammar by only remembering the grandparent but not the parent symbol. The difference here is that the annotations are not vertical ((grand-)parent), but rather horizontal (left context). For instance, a context-free rule A → B C would become D A → D B B C for some D if there exists a rule E → αDAβ. This resembles the reduce step in Fig. 3. The very high-level idea of the proof is that boundedness is crucial for polynomial-time, while monotonicity is used for the optimal substructure property required by the co"
P10-1110,P08-1068,0,0.280845,"is in seconds per sentence. Search spaces: ‡ linear; others exponential. (on a 3.2GHz Xeon CPU). Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower. Figure 8 shows the parse time in seconds for each test sentence. The observed time complexity of our DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access to extra information like unlabeled data. Our technique is orthogonal to theirs, and combining these techniques could potentially lead to even better results. We also test our final parser on the Penn Chinese Treebank (CTB5). Following the set-up of Duan et al. (2007) and Zhang and Clark (2008), we split CTB5 into training (secs 001-815 and 1001- 1084 parsing time (secs) 1.4 Cha Berk MST DP 1.2 1 0.8 0.6 0.4 0.2 0 0 10"
P10-1110,P05-1012,0,0.949212,"ic programming which includes the dominant CKY algorithm, and greedy search which includes most incremental parsing methods such as shift-reduce.1 Both have pros and cons: the former performs an exact search (in cubic time) over an exponentially large space, while the latter is much faster (in linear-time) and is psycholinguistically motivated (Frazier and Rayner, 1982), but its greedy nature may suffer from severe search errors, as it only explores a tiny fraction of the whole space even with a beam. Can we combine the advantages of both approaches, that is, construct an incremental parser 1 McDonald et al. (2005b) is a notable exception: the MST algorithm is exact search but not dynamic programming. • theoretically, we show that for a large class of modern shift-reduce parsers, dynamic programming is in fact possible and runs in polynomial time as long as the feature functions are bounded and monotonic (which almost always holds in practice); • practically, dynamic programming is up to five times faster (with the same accuracy) as conventional beam-search on top of a stateof-the-art shift-reduce dependency parser; • as a by-product, dynamic programming can output a forest encoding exponentially many"
P10-1110,H05-1066,0,0.581495,"Missing"
P10-1110,J03-1006,0,0.0061058,"he concept of prefix cost from Stolcke (1995), originally developed for weighted Earley parsing. As shown in Fig. 3, the prefix cost c is the total cost of the best action sequence from the initial state to the end of state p, i.e., it includes both the inside cost v (for Viterbi inside derivation), and the cost of the (best) path leading towards the beginning of state p. We say that a state p with prefix cost c is better than a state p′ with prefix cost c′ , notated p ≺ p′ in Fig. 3, if c < c′ . We can also prove (by contradiction) that optimizing for prefix cost implies optimal inside cost (Nederhof, 2003, Sec. 4). 5 As shown in Fig. 3, when a state q with costs (c, v) is combined with a predictor state p with costs (c′ , v ′ ), the resulting state r will have costs (c′ + v + δ, v ′ + v + δ), where the inside cost is intuitively the combined inside costs plus an additional combo cost δ from the combination, while the resulting prefix cost c′ + v + δ is the sum of the prefix cost of the predictor state q, the inside cost of the current state p, and the combo cost. Note the prefix cost of q is irrelevant. The combo cost δ = ξ ′ + λ consists of shift cost ξ ′ of p and reduction cost λ of q. The c"
P10-1110,W04-0308,0,0.329304,"paper, though our formalism and algorithm can also be applied to phrase-structure parsing. Shift-Reduce Parsing 2.1 Vanilla Shift-Reduce Shift-reduce parsing performs a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items at the end of the stack (Aho and Ullman, 1972). To adapt it to dependency parsing, we split the reduce action into two cases, rex and rey, depending on which one of the two items becomes the head after reduction. This procedure is known as “arc-standard” (Nivre, 2004), and has been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state hj, Si where S is a stack of trees s0 , s1 , ... where s0 is the top tree, and j is the 2 stack Figure 2: A trace of vanilla shift-reduce. After step (4), the parser branches off into (5a) or (5b). ℓ : hj, S|s1 |s0 i : c where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: 2 action sh sh rex sh rey sh There is another popular variant, “arc-eager” (Nivr"
P10-1110,N07-1051,0,0.162627,"92.0 91.4 92.1 92.5 92.4 92.1 93.2 L Ja Ja − C Py C Ja C − time 0.12 0.15 − 0.11 0.04 0.49 0.21 − − comp. O(n2 ) O(n3 ) O(n4 ) O(n)‡ O(n) O(n5 ) O(n3 ) O(n2 )‡ O(n4 ) Table 3: Final test results on English (PTB). Our parser (in pure Python) has the highest accuracy among dependency parsers trained on the Treebank, and is also much faster than major parsers. † converted from constituency trees. C=C/C++, Py=Python, Ja=Java. Time is in seconds per sentence. Search spaces: ‡ linear; others exponential. (on a 3.2GHz Xeon CPU). Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower. Figure 8 shows the parse time in seconds for each test sentence. The observed time complexity of our DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access"
P10-1110,N09-1073,0,0.060558,"Missing"
P10-1110,J95-2002,0,0.9297,"Rey, CA 90292 sagae@ict.usc.edu Abstract that runs in (almost) linear-time, yet searches over a huge space with dynamic programming? Theoretically, the answer is negative, as Lee (2002) shows that context-free parsing can be used to compute matrix multiplication, where sub-cubic algorithms are largely impractical. We instead propose a dynamic programming alogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice. The key idea is to merge equivalent stacks according to feature functions, inspired by Earley parsing (Earley, 1970; Stolcke, 1995) and generalized LR parsing (Tomita, 1991). However, our formalism is more flexible and our algorithm more practical. Specifically, we make the following contributions: Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values. Empir"
P10-1110,P88-1031,0,0.915394,"ǫi: (0, 0, ∅) state p: ℓ : h , j, sd ...s0 i: (c, , ) sh ℓ + 1 : hj, j + 1, sd−1 ...s0 , wj i : (c + ξ, 0, {p}) state p: rex goal j<n state q: : hk, i, s′d ...s′0 i: (c′ , v ′ , π ′ ) ℓ : hi, j, sd ...s0 i: ( , v, π) ℓ + 1 : hk, j, s′d ...s′1 , s′0 s0 i : (c′ + v + δ, v ′ + v + δ, π ′ ) x p∈π 2n − 1 : h0, n, sd ...s0 i: (c, c, {p0 }) where ξ = w · fsh (j, sd ...s0 ), and δ = ξ ′ + λ, with ξ ′ = w · fsh (i, s′d ...s′0 ) and λ = w · frex (j, sd ...s0 ). Figure 3: Deductive system for shift-reduce parsing with dynamic programming. The predictor state set π is an implicit graph-structured stack (Tomita, 1988) while the prefix cost c is inspired by Stolcke (1995). The rey case is similar, replacing s′0 xs0 with s′0 ys0 , and λ with ρ = w · frey (j, sd ...s0 ). Irrelevant information in a deduction step is marked as an underscore ( ) which means “can match anything”. tag of the next word q1 . Since the queue is static information to the parser (unlike the stack, which changes dynamically), we can use j to replace features from the queue. So in general we write e f (j, S) = (j, fd (sd ), . . . , f0 (s0 )) if the feature window looks at top d + 1 trees on stack, and where fi (si ) extracts kernel feat"
P10-1110,W03-3023,0,0.947319,"g et al. (2009) in Python (henceforth “non-DP”), and then extended it to do dynamic programing (henceforth “DP”). We evaluate their performances on the standard Penn Treebank (PTB) English dependency parsing task7 using the standard split: secs 02-21 for training, 22 for development, and 23 for testing. Both DP and non-DP parsers use the same feature templates in Table 1. For Secs. 4.1-4.2, we use a baseline model trained with non-DP for both DP and non-DP, so that we can do a side-by-side comparison of search 6 7 Or O(n2 ) with MST, but including non-projective trees. Using the head rules of Yamada and Matsumoto (2003). To compare parsing speed between DP and nonDP, we run each parser on the development set, varying the beam width b from 2 to 16 (DP) or 64 (non-DP). Fig. 5a shows the relationship between search quality (as measured by the average model score per sentence, higher the better) and speed (average parsing time per sentence), where DP with a beam width of b=16 achieves the same search quality with non-DP at b=64, while being 5 times faster. Fig. 5b shows a similar comparison for dependency accuracy. We also test with an edge-factored model (Sec. 3.5) using feature templates (1)-(3) in Tab. 1, whi"
P10-1110,D08-1059,0,0.904673,"been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state hj, Si where S is a stack of trees s0 , s1 , ... where s0 is the top tree, and j is the 2 stack Figure 2: A trace of vanilla shift-reduce. After step (4), the parser branches off into (5a) or (5b). ℓ : hj, S|s1 |s0 i : c where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: 2 action sh sh rex sh rey sh There is another popular variant, “arc-eager” (Nivre, 2004; Zhang and Clark, 2008), which is more complicated and less similar to the classical shift-reduce algorithm. Note that the shorthand notation txt′ denotes a new tree by “attaching tree t′ as the leftmost child of the root of tree t”. This procedure can be summarized as a deductive system in Figure 1. States are organized according to step ℓ, which denotes the number of actions accumulated. The parser runs in linear-time as there are exactly 2n−1 steps for a sentence of n words. As an example, consider the sentence “I saw Al with Joe” in Figure 2. At step (4), we face a shiftreduce conflict: either combine “saw” and"
P10-1110,P05-1022,0,\N,Missing
P10-5002,2006.amta-papers.8,1,\N,Missing
P10-5002,N04-1035,0,\N,Missing
P10-5002,D08-1022,1,\N,Missing
P10-5002,J93-2003,0,\N,Missing
P10-5002,D09-1037,0,\N,Missing
P10-5002,W05-1506,1,\N,Missing
P10-5002,P09-1020,0,\N,Missing
P10-5002,P07-1089,1,\N,Missing
P10-5002,P08-1023,1,\N,Missing
P10-5002,P06-1077,1,\N,Missing
P10-5002,P08-1064,0,\N,Missing
P10-5002,J04-4002,0,\N,Missing
P10-5002,P09-1063,1,\N,Missing
P10-5002,W06-1606,0,\N,Missing
P10-5002,P05-1033,0,\N,Missing
P10-5002,N03-1017,0,\N,Missing
P10-5002,P07-1019,1,\N,Missing
P10-5002,P06-1121,0,\N,Missing
P10-5002,P08-1066,0,\N,Missing
P10-5002,P89-1018,0,\N,Missing
P10-5002,C10-1080,1,\N,Missing
P10-5002,J07-2003,0,\N,Missing
P10-5002,W01-1812,0,\N,Missing
P10-5002,D07-1078,0,\N,Missing
P10-5002,P09-1065,1,\N,Missing
P10-5002,2008.amta-papers.18,0,\N,Missing
P11-1086,E03-1005,0,0.0230277,"scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and grandparent rules, not just nonterminals. Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003). To our knowledge, there has been no direct comparison between a history-based PCFG approach and DOP approach in the parsing literature. 6 Conclusion In this paper, we have investigated whether we can eliminate composed rules without any loss in translation quality. We have developed a rule Markov model that captures vertical bigrams and trigrams of minimal rules, and tested it in the framework of treeto-string translation. We draw three main conclusions from our experiments. First, our rule Markov models dramatically improve a grammar of minimal rules, giving an improvement of 2.3 Bleu. Seco"
P11-1086,P05-1067,0,0.0482976,"us settings.2 In the second line (2.9 million rules), the drop in Bleu score resulting from adding the rule Markov model is not statistically significant. 5 Related Work Besides the Quirk and Menezes (2006) work discussed in Section 1, there are two other previous 2 For these experiments, a beam size of 100 was used. 863 efforts both using a rule bigram model in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and g"
P11-1086,W08-0306,0,0.0127101,"V is the target-language vocabulary, and g is the order of the n-gram language model (Huang and Mi, 2010). However, if one were to use rule Markov models with a conventional CKY-style 861 Experiments and results 4.1 Setup The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. Our development set is the newswire portion of the 2006 NIST MT Evaluation test set (616 sentences), and our test set is the newswire portion of the 2008 NIST MT Evaluation test set (691 sentences). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained usin"
P11-1086,N04-1035,0,0.687748,"ause a combinatorial explosion in the number of rules. To avoid this, ad-hoc limits are placed during composition, like upper bounds on the number of nodes in the composed rule, or the height of the rule. Under such limits, the grammar size is manageable, but still much larger than the minimal-rule grammar. Second, due to large grammars, the decoder has to consider many more hypothesis translations, which slows it down. Nevertheless, the advantages outweigh the disadvantages, and to our knowledge, all top-performing systems, both phrase-based and syntax-based, use composed rules. For example, Galley et al. (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al., 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context. In this work, we restrict a rule’s context to the vertical chain of ancestors of the rule. This ancestral context would play the same role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk"
P11-1086,P06-1121,0,0.121012,"n, like upper bounds on the number of nodes in the composed rule, or the height of the rule. Under such limits, the grammar size is manageable, but still much larger than the minimal-rule grammar. Second, due to large grammars, the decoder has to consider many more hypothesis translations, which slows it down. Nevertheless, the advantages outweigh the disadvantages, and to our knowledge, all top-performing systems, both phrase-based and syntax-based, use composed rules. For example, Galley et al. (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al., 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context. In this work, we restrict a rule’s context to the vertical chain of ancestors of the rule. This ancestral context would play the same role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk and Menezes (2006) takes such an approach within the framework of dependency translation. However, their study"
P11-1086,D10-1027,1,0.903437,"rts at step 1, where we predict rule r1 (the shaded rule) with probability P(r1 |ǫ) and push its English side onto the stack, with variables replaced by the corresponding tree nodes: x1 becomes NP@1 and x2 becomes VP@2 . This gives us the following stack: In this paper, we use our rule Markov model framework in the context of tree-to-string translation. Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) have gained popularity in recent years due to their speed and simplicity. The input to the translation system is a source parse tree and the output is the target string. Huang and Mi (2010) have recently introduced an efficient incremental decoding algorithm for tree-to-string translation. The decoder operates top-down and maintains a derivation history of translation rules encountered. The history s = [ NP@1 VP@2 ] is exactly the vertical chain of ancestors corresponding to the contexts in our rule Markov model. This The dot () indicates the next symbol to process in 859 stack hyp. 0 [<s>  IP@ǫ 1 [<s>  IP </s> 2 [<s>  IP@ǫ </s> 3 [<s>  IP@ǫ </s> 4 [<s>  IP@ǫ </s> 5 [<s>  IP@ǫ </s> 6 [<s>  IP@ǫ </s> 7 [<s>  IP@ǫ </s> @1 @2 <s> P(r1 |ǫ) ] [ NP@1 VP@2 ] [ Bush] <s> P(r"
P11-1086,2006.amta-papers.8,1,0.944897,"is above another threshold. 3 Tree-to-string decoding with rule Markov models IP@ǫ VP@2 NP@1 B`ush´ı PP@2.1 VP@2.2 P@2.1.1 NP@2.1.2 VV@2.2.1 AS@2.2.2 NP@2.2.3 yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an Figure 3: Example input parse tree with tree addresses. makes incremental decoding a natural fit with our generative story. In this section, we describe how to integrate our rule Markov model into this incremental decoding algorithm. Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al., 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead. Algorithm Given the input parse tree in Figure 3, Figure 4 illustrates the search process of the incremental decoder with the grammar of Figure 1. We write X @η for a tree node with label X at tree address η (Shieber et al., 1995). The root node has address ǫ, and the ith child of node η has address η.i. At each step, the decoder maintains a stack of active rules, which are rules that have not been completed yet, and the rightmost"
P11-1086,J98-4004,0,0.0855515,"el in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and grandparent rules, not just nonterminals. Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003). To our knowledge, there has been no direct comparison between a history-based PCFG approach and DOP approach in the parsing literature. 6 Conclusion In this paper, we have"
P11-1086,W04-3250,0,0.135584,".3+0.5 4.9+7.6 0.3+0.6 176.8 1.3 17.5 1.6 448.7 3.3 448.7+7.6 3.3+1.0 Bleu test 24.2 25.7 26.5 26.5 26.4 27.5 28.0 time (sec/sent) 1.2 1.8 2.0 2.9 2.2 6.8 9.2 Table 1: Main results. Our trigram rule Markov model strongly outperforms minimal rules, and performs at the same level as composed and vertically composed rules, but is smaller and faster. The number of parameters is shown for both the full model and the model filtered for the concatenation of the development and test sets (dev+test). These gains are statistically significant with p < 0.01, using bootstrap resampling with 1000 samples (Koehn, 2004). We find that by just using bigram context, we are able to get at least 1 Bleu point higher than the minimal rule grammar. It is interesting to see that using just bigram rule interactions can give us a reasonable boost. We get our highest gains from using trigram context where our best performing rule Markov model gives us 2.3 Bleu points over minimal rules. This suggests that using longer contexts helps the decoder to find better translations. We also compared rule Markov models against composed rules. Since our models are currently limited to conditioning on vertical context, the closest c"
P11-1086,W08-0308,0,0.0492019,"he rule Markov model is not statistically significant. 5 Related Work Besides the Quirk and Menezes (2006) work discussed in Section 1, there are two other previous 2 For these experiments, a beam size of 100 was used. 863 efforts both using a rule bigram model in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and grandparent rules, not just nonterminals. Second, we compare against a composed-rule system, which i"
P11-1086,P06-1077,0,0.337664,"f the decoder. The last column in the figure shows the rule Markov model probabilities with the conditioning context. In this example, we use a trigram rule Markov model. After initialization, the process starts at step 1, where we predict rule r1 (the shaded rule) with probability P(r1 |ǫ) and push its English side onto the stack, with variables replaced by the corresponding tree nodes: x1 becomes NP@1 and x2 becomes VP@2 . This gives us the following stack: In this paper, we use our rule Markov model framework in the context of tree-to-string translation. Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) have gained popularity in recent years due to their speed and simplicity. The input to the translation system is a source parse tree and the output is the target string. Huang and Mi (2010) have recently introduced an efficient incremental decoding algorithm for tree-to-string translation. The decoder operates top-down and maintains a derivation history of translation rules encountered. The history s = [ NP@1 VP@2 ] is exactly the vertical chain of ancestors corresponding to the contexts in our rule Markov model. This The dot () indicates the next symbol to process in 8"
P11-1086,P08-1023,1,0.888401,"es). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule patternmatching (Mi et al., 2008). We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the script mteval-v13a.pl. 4.2 Results Table 1 presents the main results of our paper. We used grammars of minimal rules and composed rules of maximum height 3 as our baseline"
P11-1086,P03-1021,0,0.0444013,"entences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule patternmatching (Mi et al., 2008). We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the script mteval-v13a.pl. 4.2 Results Table 1 presents the main results of our paper. We used grammars of minimal rules and composed rules of maximum height 3 as our baselines. For decoding, we used a beam size of 50. Using the best bigram rule Markov models and the minimal rule grammar gives us a"
P11-1086,N07-1051,0,0.0208804,"uang and Mi, 2010). However, if one were to use rule Markov models with a conventional CKY-style 861 Experiments and results 4.1 Setup The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. Our development set is the newswire portion of the 2006 NIST MT Evaluation test set (616 sentences), and our test set is the newswire portion of the 2008 NIST MT Evaluation test set (691 sentences). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development se"
P11-1086,N06-1002,0,0.516674,"2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al., 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context. In this work, we restrict a rule’s context to the vertical chain of ancestors of the rule. This ancestral context would play the same role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk and Menezes (2006) takes such an approach within the framework of dependency translation. However, their study leaves unanswered whether a rule Markov model can take the place of composed rules. In this work, we investigate the use of rule Markov models in the context of treeProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 856–864, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics to-string translation (Liu et al., 2006; Huang et al., 2006). We make three new contributions. First, we carry out a detailed comparison of rule Markov m"
P12-1033,W09-1804,0,0.0332356,"o investigate the impact of hyperparameter tuning on translation quality. For the smaller Arabic-English corpus, we symmetrized all combinations of the two top-scoring alignments (according to F1) in each direction, yielding four sets of alignments. Table 4 shows Bleu scores for translation models learned from these alignments. Unfortunately, we find that optimizing F1 is not optimal for Bleu—using the second-best alignments yields a further improvement of 0.5 Bleu on the NIST 2009 data, which is statistically significant (p < 0.05). 4 Related Work Schoenemann (2011a), taking inspiration from Bodrumlu et al. (2009), uses integer linear programming to optimize IBM Model 1–2 and the HMM with the ℓ0 -norm. This method, however, does not outperform GIZA++. In later work, Schoenemann (2011b) used projected gradient descent for the ℓ1 norm. Here, we have adopted his use of projected gradient descent, but using a smoothed ℓ0 -norm. Liang et al. (2006) show how to train IBM models in both directions simultaneously by adding a term to the log-likelihood that measures the agreement between the two directions. Grac¸a et al. (2010) explore modifications to the HMM model that encourage bijectivity and symmetry. The"
P12-1033,bojar-prokopova-2006-czech,0,0.0655725,"Missing"
P12-1033,J93-2003,0,0.142348,", in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 Bleu). 1 Introduction Automatic word alignment is a vital component of nearly all current statistical translation pipelines. Although state-of-the-art translation models use rules that operate on units bigger than words (like phrases or tree fragments), they nearly always use word alignments to drive extraction of those translation rules. The dominant approach to word alignment has been the IBM models (Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar"
P12-1033,D08-1024,1,0.676974,"Missing"
P12-1033,J07-2003,1,0.0816809,"Missing"
P12-1033,P11-1042,0,0.241029,"Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an ℓ0 prior, inspired by the principle of minimum description length (Barron et al., 1998), to encourage sparsity in the word-to-word translation model (Section 2.2). This extension follows our previous work on unsupervised part-ofspeech tag"
P12-1033,J07-3002,0,0.0950827,"and β = 0.05. We did not choose a large α, as the dataset was small, and we chose a conservative value for β. We ran word alignment in both directions and symmetrized using grow-diag-final (Koehn et al., 2003). For models with the smoothed ℓ0 prior, we tuned α and β separately in each direction. 3.2 Alignment First, we evaluated alignment accuracy directly by comparing against gold-standard word alignments. 4 This data is available at http://statmt.org/wmt10. The results are shown in the alignment F1 column of Table 1. We used balanced F-measure rather than alignment error rate as our metric (Fraser and Marcu, 2007). Following Dyer et al. (2011), we also measured the average fertility, φ˜ sing. , of once-seen source words in the symmetrized alignments. Our alignments show smaller fertility for once-seen words, suggesting that they suffer from “garbage collection” effects less than the baseline alignments do. The fact that we had to use hand-aligned data to tune the hyperparameters α and β means that our method is no longer completely unsupervised. However, our observation is that alignment accuracy is actually fairly robust to the choice of these hyperparameters, as shown in Table 2. As we will see below"
P12-1033,J10-3007,0,0.119524,"Missing"
P12-1033,N03-1017,0,0.00539255,"s.4 We set the hyperparameters α and β by tuning on gold-standard word alignments (to maximize F1) when possible. For Arabic-English and ChineseEnglish, we used 346 and 184 hand-aligned sentences from LDC2006E86 and LDC2006E93. Similarly, for Czech-English, 515 hand-aligned sentences were available (Bojar and Prokopov´a, 2006). But for Urdu-English, since we did not have any gold alignments, we used α = 10 and β = 0.05. We did not choose a large α, as the dataset was small, and we chose a conservative value for β. We ran word alignment in both directions and symmetrized using grow-diag-final (Koehn et al., 2003). For models with the smoothed ℓ0 prior, we tuned α and β separately in each direction. 3.2 Alignment First, we evaluated alignment accuracy directly by comparing against gold-standard word alignments. 4 This data is available at http://statmt.org/wmt10. The results are shown in the alignment F1 column of Table 1. We used balanced F-measure rather than alignment error rate as our metric (Fraser and Marcu, 2007). Following Dyer et al. (2011), we also measured the average fertility, φ˜ sing. , of once-seen source words in the symmetrized alignments. Our alignments show smaller fertility for once"
P12-1033,W04-3250,0,0.0242181,"e used for word alignment. The development data that were used for discriminative training were: for Chinese-English and Arabic-English, data from the NIST 2004 and NIST 2006 test sets, plus newsgroup data from the GALE program (LDC2006E92); for Urdu-English, half of the NIST 2008 test set; for Czech-English, a training set of 2051 sentences provided by the WMT10 translation workshop. The results are shown in the Bleu column of Table 1. We used case-insensitive IBM Bleu (closest reference length) as our metric. Significance testing was carried out using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). All of the tests showed significant improvements (p < 0.01), ranging from +0.4 Bleu to +1.4 Bleu. For Urdu, even though we didn’t have manual alignments to tune hyperparameters, we got significant gains over a good baseline. This is promising for languages that do not have any manually aligned data. Ideally, one would want to tune α and β to maximize Bleu. However, this is prohibitively expensive, especially if we must tune them separately in each alignment direction before symmetrization. We ran some contrastive experiments to investigate the impact of hyperparameter tu"
P12-1033,N06-1014,0,0.728874,"nated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an ℓ0 prior, inspired by the principle of minimum description length (Barron et al., 1998), to encourage sparsity in the word-to-word translation model (Section 2.2). This extension follows our previo"
P12-1033,P11-2032,0,0.432532,"Missing"
P12-1033,H05-1011,0,0.058597,"(Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM"
P12-1033,J04-4002,0,0.0987219,"nent of nearly all current statistical translation pipelines. Although state-of-the-art translation models use rules that operate on units bigger than words (like phrases or tree fragments), they nearly always use word alignments to drive extraction of those translation rules. The dominant approach to word alignment has been the IBM models (Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011)"
P12-1033,P10-1017,0,0.0398842,"the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an ℓ0 prior, inspired"
P12-1033,W11-0320,0,0.0721951,"the constraints (8). The count C(e, f ) is the number of times that f occurs aligned to e. For MAP-EM, it is: θˆ = arg min − θ X E[C(e, f )] log t( f |e) − e, f α X e, f −t( f |e) exp β ! (10) This optimization problem is non-convex, and we do not know of a closed-form solution. Previously (Vaswani et al., 2010), we used ALGENCAN, a nonlinear optimization toolkit, but this solution does not scale well to the number of parameters involved in word alignment models. Instead, we use a simpler and more scalable method which we describe in the next section. 2.3 Projected gradient descent Following Schoenemann (2011b), we use projected gradient descent (PGD) to solve the M-step (but with the ℓ0 -norm instead of the ℓ1 -norm). Gradient projection methods are attractive solutions to constrained optimization problems, particularly when the constraints on the parameters are simple (Bertsekas, 1999). Let F(θ) be the objective function in (11) This moves θ in the direction of steepest descent (∇F) with step size s, and then the function [·]∆ projects the resulting point onto the simplex; that is, it finds the nearest point that satisfies the constraints (8). The gradient ∇F(θk ) is E[C( f, e)] α −t( f |e) ∂F ="
P12-1033,I11-1147,0,0.282807,"the constraints (8). The count C(e, f ) is the number of times that f occurs aligned to e. For MAP-EM, it is: θˆ = arg min − θ X E[C(e, f )] log t( f |e) − e, f α X e, f −t( f |e) exp β ! (10) This optimization problem is non-convex, and we do not know of a closed-form solution. Previously (Vaswani et al., 2010), we used ALGENCAN, a nonlinear optimization toolkit, but this solution does not scale well to the number of parameters involved in word alignment models. Instead, we use a simpler and more scalable method which we describe in the next section. 2.3 Projected gradient descent Following Schoenemann (2011b), we use projected gradient descent (PGD) to solve the M-step (but with the ℓ0 -norm instead of the ℓ1 -norm). Gradient projection methods are attractive solutions to constrained optimization problems, particularly when the constraints on the parameters are simple (Bertsekas, 1999). Let F(θ) be the objective function in (11) This moves θ in the direction of steepest descent (∇F) with step size s, and then the function [·]∆ projects the resulting point onto the simplex; that is, it finds the nearest point that satisfies the constraints (8). The gradient ∇F(θk ) is E[C( f, e)] α −t( f |e) ∂F ="
P12-1033,H05-1010,0,0.106405,", 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporati"
P12-1033,P10-2039,1,0.934169,"have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an ℓ0 prior, inspired by the principle of minimum description length (Barron et al., 1998), to encourage sparsity in the word-to-word translation model (Section 2.2). This extension follows our previous work on unsupervised part-ofspeech tagging (Vaswani et al., 2010), but enables it to scale to the large datasets typical in word alignment, using an efficient training method based on projected gradient descent (Section 2.3). Experiments on Czech-, Arabic-, Chinese- and UrduEnglish translation (Section 3) demonstrate consistent significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 Bleu). Our implementation has been released as a simple modification to the GIZA++ toolkit that can be used as a drop-in replacement for GIZA++ in any existing MT pipeline. 2 Method We start with a brief review of the"
P12-1033,C96-2141,0,0.953728,"rdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 Bleu). 1 Introduction Automatic word alignment is a vital component of nearly all current statistical translation pipelines. Although state-of-the-art translation models use rules that operate on units bigger than words (like phrases or tree fragments), they nearly always use word alignments to drive extraction of those translation rules. The dominant approach to word alignment has been the IBM models (Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although ma"
P12-1033,zhang-etal-2004-interpreting,0,0.0273054,"rd alignment. The development data that were used for discriminative training were: for Chinese-English and Arabic-English, data from the NIST 2004 and NIST 2006 test sets, plus newsgroup data from the GALE program (LDC2006E92); for Urdu-English, half of the NIST 2008 test set; for Czech-English, a training set of 2051 sentences provided by the WMT10 translation workshop. The results are shown in the Bleu column of Table 1. We used case-insensitive IBM Bleu (closest reference length) as our metric. Significance testing was carried out using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). All of the tests showed significant improvements (p < 0.01), ranging from +0.4 Bleu to +1.4 Bleu. For Urdu, even though we didn’t have manual alignments to tune hyperparameters, we got significant gains over a good baseline. This is promising for languages that do not have any manually aligned data. Ideally, one would want to tune α and β to maximize Bleu. However, this is prohibitively expensive, especially if we must tune them separately in each alignment direction before symmetrization. We ran some contrastive experiments to investigate the impact of hyperparameter tuning on translation q"
P12-1033,P04-1066,0,\N,Missing
P13-1008,W06-0901,0,0.87674,"he application of early-update in this work. To further investigate the difference between early-update and standardupdate, we tested the performance of both strategies, which is summarized in Table 5. As we can see the performance of standard-update is generally worse than early-update. When the beam size is increased (b = 4), the gap becomes smaller as the ratio of invalid updates is reduced. 4.6 5 Related Work Most recent studies about ACE event extraction rely on staged pipeline which consists of separate local classifiers for trigger labeling and argument labeling (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012a; Chen and Ng, 2012). To the best of our knowledge, our work is the first attempt to jointly model these two ACE event subtasks. Overall performance Table 6 shows the overall performance on the blind test set. In addition to our baseline, we compare against the sentence-level system reported in Hong et al. (2011), which, to the best of our knowledge, 80 Methods Sentence-level in Hong et al. (2011) Staged MaxEnt classifiers Joint w/ local features Joint w/ local + global features Cross-entity"
P13-1008,J92-4003,0,0.194657,"a simple example of global features:   1 if yg(i) = Attack and f101 (x, i, k, y) = y has only one “Attacker”   0 otherwise Category Type Lexical Trigger Syntactic Entity Information Basic Argument Syntactic Feature Description 1. unigrams/bigrams of the current and context words within the window of size 2 2. unigrams/bigrams of part-of-speech tags of the current and context words within the window of size 2 3. lemma and synonyms of the current token 4. base form of the current token extracted from Nomlex (Macleod et al., 1998) 5. Brown clusters that are learned from ACE English corpus (Brown et al., 1992; Miller et al., 2004; Sun et al., 2011). We used the clusters with prefixes of length 13, 16 and 20 for each token. 6. dependent and governor words of the current token 7. dependency types associated the current token 8. whether the current token is a modifier of job title 9. whether the current token is a non-referential pronoun 10. unigrams/bigrams normalized by entity types 11. dependency features normalized by entity types 12. nearest entity type and string in the sentence/clause 1. context words of the entity mention 2. trigger word and subtype 3. entity type, subtype and entity role if"
P13-1008,P10-1081,0,0.844465,"look at this problem and formulate it, for the first time, as a structured learning problem. We propose a novel joint event extraction algorithm to predict the triggers and arguments simultaneously, and use the structured perceptron (Collins, 2002) to train the joint model. This way we can capture the dependencies between triggers and argument as well as explore Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Most state-of-the-art approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) use sequential pipelines as building blocks, which break down the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification. As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream classifiers. The downstream components, however, cannot impact earlier deci73 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73–82, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Lingu"
P13-1008,N09-2053,1,0.817299,"this work. To further investigate the difference between early-update and standardupdate, we tested the performance of both strategies, which is summarized in Table 5. As we can see the performance of standard-update is generally worse than early-update. When the beam size is increased (b = 4), the gap becomes smaller as the ratio of invalid updates is reduced. 4.6 5 Related Work Most recent studies about ACE event extraction rely on staged pipeline which consists of separate local classifiers for trigger labeling and argument labeling (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012a; Chen and Ng, 2012). To the best of our knowledge, our work is the first attempt to jointly model these two ACE event subtasks. Overall performance Table 6 shows the overall performance on the blind test set. In addition to our baseline, we compare against the sentence-level system reported in Hong et al. (2011), which, to the best of our knowledge, 80 Methods Sentence-level in Hong et al. (2011) Staged MaxEnt classifiers Joint w/ local features Joint w/ local + global features Cross-entity in Hong et al. (2011)† Trigger Identifica"
P13-1008,P11-1163,0,0.323068,"Missing"
P13-1008,C12-1033,0,0.474159,"Missing"
P13-1008,N04-1043,0,0.0143919,"global features:   1 if yg(i) = Attack and f101 (x, i, k, y) = y has only one “Attacker”   0 otherwise Category Type Lexical Trigger Syntactic Entity Information Basic Argument Syntactic Feature Description 1. unigrams/bigrams of the current and context words within the window of size 2 2. unigrams/bigrams of part-of-speech tags of the current and context words within the window of size 2 3. lemma and synonyms of the current token 4. base form of the current token extracted from Nomlex (Macleod et al., 1998) 5. Brown clusters that are learned from ACE English corpus (Brown et al., 1992; Miller et al., 2004; Sun et al., 2011). We used the clusters with prefixes of length 13, 16 and 20 for each token. 6. dependent and governor words of the current token 7. dependency types associated the current token 8. whether the current token is a modifier of job title 9. whether the current token is a non-referential pronoun 10. unigrams/bigrams normalized by entity types 11. dependency features normalized by entity types 12. nearest entity type and string in the sentence/clause 1. context words of the entity mention 2. trigger word and subtype 3. entity type, subtype and entity role if it is a geo-political"
P13-1008,P04-1015,0,0.530784,"tel. Die Attack Figure 1: Event mentions of example (1). There are two event mentions that share three arguments, namely the Die event mention triggered by “died”, and the Attack event mention triggered by “fired”. arbitrary global features over multiple local predictions. However, different from easier tasks such as part-of-speech tagging or noun phrase chunking where efficient dynamic programming decoding is feasible, here exact joint inference is intractable. Therefore we employ beam search in decoding, and train the model using the early-update perceptron variant tailored for beam search (Collins and Roark, 2004; Huang et al., 2012). We make the following contributions: • Event mention: an occurrence of an event with a particular type and subtype. • Event trigger: the word most clearly expresses the event mention. • Event argument: an entity mention, temporal expression or value (e.g. Job-Title) that serves as a participant or attribute with a specific role in an event mention. • Event mention: an instance that includes one event trigger and some arguments that appear within the same sentence. 1. Different from traditional pipeline approach, we present a novel framework for sentencelevel event extrac"
P13-1008,D09-1016,0,0.471219,"Missing"
P13-1008,W02-1001,0,0.671658,", we can propagate the Victim argument of the Die event to the Target argument of the Attack event. As another example, knowing that an Attack event usually only has one Attacker argument, we could penalize assignments in which one trigger has more than one Attacker. Such global features cannot be easily exploited by a local classifier. Therefore, we take a fresh look at this problem and formulate it, for the first time, as a structured learning problem. We propose a novel joint event extraction algorithm to predict the triggers and arguments simultaneously, and use the structured perceptron (Collins, 2002) to train the joint model. This way we can capture the dependencies between triggers and argument as well as explore Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Most state-of-the-art approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) use sequential pipelines as building blocks, which break down the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification. As a common d"
P13-1008,D11-1001,0,0.0273916,"Missing"
P13-1008,de-marneffe-etal-2006-generating,0,0.0254867,"Missing"
P13-1008,W11-1807,0,0.015807,"Missing"
P13-1008,W09-1406,0,0.0844449,"Missing"
P13-1008,P11-1113,0,0.809091,"formulate it, for the first time, as a structured learning problem. We propose a novel joint event extraction algorithm to predict the triggers and arguments simultaneously, and use the structured perceptron (Collins, 2002) to train the joint model. This way we can capture the dependencies between triggers and argument as well as explore Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Most state-of-the-art approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) use sequential pipelines as building blocks, which break down the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification. As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream classifiers. The downstream components, however, cannot impact earlier deci73 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73–82, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Place Target"
P13-1008,N12-1015,1,0.772036,"Missing"
P13-1008,P11-1053,0,0.0118276,"1 if yg(i) = Attack and f101 (x, i, k, y) = y has only one “Attacker”   0 otherwise Category Type Lexical Trigger Syntactic Entity Information Basic Argument Syntactic Feature Description 1. unigrams/bigrams of the current and context words within the window of size 2 2. unigrams/bigrams of part-of-speech tags of the current and context words within the window of size 2 3. lemma and synonyms of the current token 4. base form of the current token extracted from Nomlex (Macleod et al., 1998) 5. Brown clusters that are learned from ACE English corpus (Brown et al., 1992; Miller et al., 2004; Sun et al., 2011). We used the clusters with prefixes of length 13, 16 and 20 for each token. 6. dependent and governor words of the current token 7. dependency types associated the current token 8. whether the current token is a modifier of job title 9. whether the current token is a non-referential pronoun 10. unigrams/bigrams normalized by entity types 11. dependency features normalized by entity types 12. nearest entity type and string in the sentence/clause 1. context words of the entity mention 2. trigger word and subtype 3. entity type, subtype and entity role if it is a geo-political entity mention 4."
P13-1008,P08-1030,1,0.505369,"efore, we take a fresh look at this problem and formulate it, for the first time, as a structured learning problem. We propose a novel joint event extraction algorithm to predict the triggers and arguments simultaneously, and use the structured perceptron (Collins, 2002) to train the joint model. This way we can capture the dependencies between triggers and argument as well as explore Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Most state-of-the-art approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) use sequential pipelines as building blocks, which break down the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification. As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream classifiers. The downstream components, however, cannot impact earlier deci73 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73–82, c Sofia, Bulgaria, August 4-9 2013. 2013 Associatio"
P13-1008,D12-1092,0,0.173965,"Missing"
P13-2111,A00-2018,0,0.312412,"enn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (as they claim) a linear runtime O(kn) where k is the beam width. However, we will demonstrate below that, contrary to popular ∗ 1 The Huang-Sagae DP parser (http://acl.cs.qc.edu) does run in O(kn), which inspired this paper when we experimented with simulating non-DP beam search using GSS. 2 Our notion of TSS is crucially different from the data Supported in part by DARPA FA8750-13-2-0041 (DEFT). 628"
P13-2111,P04-1015,0,0.222597,"ime, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine t"
P13-2111,N10-1115,1,0.839844,"Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set (Kuhlmann et al., 2011; Zhang and Nivre, 2011)) and it benefits more from our improved algorithms. The dependence on the beam-size k is because one needs to do k-times the number of basic operations (feature-extractions, dot-products, and statetransitions) relative to a greedy parser (Nivre and Scholz, 2004; Goldberg and Elhadad, 2010). Note that in a beam setting, the same state can expand to several new states in the next step, which is usually achieved by copying the state prior to making a transition, whereas greedy search only stores one state which is modified in-place. Copying amounts to a large fraction of the slowdown of beam-based with respect to greedy parsers. Copying is expensive, because the state keeps track of (a) a stack and (b) the set of dependency-arcs added so far. Both the arc-set and the stack can grow to O(n) size in the worst-case, making the state-copy (and hence state-transition) an O(n) operation"
P13-2111,P10-1110,1,0.944848,"can grow to O(n) size in the worst-case, making the state-copy (and hence state-transition) an O(n) operation. Thus, beam search implementations that copy the entire state are in fact quadratic O(kn2 ) and not linear, with a slowdown factor of O(kn) with respect to greedy parsers, which is confirmed empirically in Figure 4. We present a way of decreasing the O(n) transition cost to O(1) achieving strictly linear-time parsing, using a data structure of Tree-Structured Stack (TSS) that is inspired by but simpler than the graph-structured stack (GSS) of Tomita (1985) used in dynamic programming (Huang and Sagae, 2010).2 On average Treebank sentences, the TSS Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O(n2 ), rather than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and"
P13-2111,D09-1127,1,0.83124,"plemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (a"
P13-2111,koen-2004-pharaoh,0,0.0615061,"Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (as they claim) a linear runtime O(kn) where k is the beam width. However, we will demonstrate below that, contrary to popular ∗ 1 The Huang-Sagae DP parser (http://acl.cs.qc.edu) does run in O(kn), which inspired this paper when we experimented with simulating non-DP beam search using GSS. 2 Our notion of TSS is crucially different from the data Supported in part by DARPA FA8750-13-2-0041 (DEFT). 628 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 628–633, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Lin"
P13-2111,P11-1068,0,0.110933,"Aviv, 5290002 Israel yoav.goldberg@gmail.com Kai Zhao Liang Huang Graduate Center and Queens College City University of New York {kzhao@gc, lhuang@cs.qc}.cuny.edu {kzhao.hf, liang.huang.sh}.gmail.com Abstract belief, in most standard implementations their actual runtime is in fact O(kn2 ) rather than linear. Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set (Kuhlmann et al., 2011; Zhang and Nivre, 2011)) and it benefits more from our improved algorithms. The dependence on the beam-size k is because one needs to do k-times the number of basic operations (feature-extractions, dot-products, and statetransitions) relative to a greedy parser (Nivre and Scholz, 2004; Goldberg and Elhadad, 2010). Note that in a beam setting, the same state can expand to several new states in the next step, which is usually achieved by copying the state prior to making a transition, whereas greedy search only stores one state which is modified in-place. Copying amounts to a large fraction of"
P13-2111,C04-1010,0,0.10583,"n2 ) rather than linear. Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set (Kuhlmann et al., 2011; Zhang and Nivre, 2011)) and it benefits more from our improved algorithms. The dependence on the beam-size k is because one needs to do k-times the number of basic operations (feature-extractions, dot-products, and statetransitions) relative to a greedy parser (Nivre and Scholz, 2004; Goldberg and Elhadad, 2010). Note that in a beam setting, the same state can expand to several new states in the next step, which is usually achieved by copying the state prior to making a transition, whereas greedy search only stores one state which is modified in-place. Copying amounts to a large fraction of the slowdown of beam-based with respect to greedy parsers. Copying is expensive, because the state keeps track of (a) a stack and (b) the set of dependency-arcs added so far. Both the arc-set and the stack can grow to O(n) size in the worst-case, making the state-copy (and hence state-"
P13-2111,J08-4003,0,0.544581,"rser, k items (hypotheses) are maintained. Items are composed of a state and a score. At step i, each of the k items is extended by applying all possible transitions to the given state, resulting in k × a items, a being the number of possible transitions. Of these, the top scoring k items are kept and used in step i + 1. Finally, the tree associated with the highest-scoring item is returned. j<n ` : hj, S|s1 |s0 i : A ` + 1 : hj, S|s0 i : A ∪ {s1 x s0 } ` : hj, S|s1 |s0 i : A ` + 1 : hj, S|s1 i : A ∪ {s1 y s0 } 2n − 1 : hn, s0 i: A Figure 1: An abstraction of the arc-standard deductive system Nivre (2008). The stack S is a list of heads, j is the index of the token at the front of the buffer, and ` is the step number (beam index). A is the arc-set of dependency arcs accumulated so far, which we will get rid of in Section 4.1. version, being linear time, leads to a speedup of 2x∼2.7x over the naive implementation, and about 1.3x∼1.7x over the optimized baseline presented in Section 5. Having achieved efficient state-transitions, we turn to feature extraction and dot products (Section 6). We present a simple scheme of sharing repeated scoring operations across different beam items, resulting in"
P13-2111,J01-2004,0,0.0756405,"than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beam"
P13-2111,D08-1059,0,0.10756,"ansition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004"
P13-2111,P11-1069,0,0.256278,"ion, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (as they claim) a linear runtime O(kn) where k is the beam width. Howeve"
P13-2111,P11-2033,0,0.33336,"av.goldberg@gmail.com Kai Zhao Liang Huang Graduate Center and Queens College City University of New York {kzhao@gc, lhuang@cs.qc}.cuny.edu {kzhao.hf, liang.huang.sh}.gmail.com Abstract belief, in most standard implementations their actual runtime is in fact O(kn2 ) rather than linear. Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set (Kuhlmann et al., 2011; Zhang and Nivre, 2011)) and it benefits more from our improved algorithms. The dependence on the beam-size k is because one needs to do k-times the number of basic operations (feature-extractions, dot-products, and statetransitions) relative to a greedy parser (Nivre and Scholz, 2004; Goldberg and Elhadad, 2010). Note that in a beam setting, the same state can expand to several new states in the next step, which is usually achieved by copying the state prior to making a transition, whereas greedy search only stores one state which is modified in-place. Copying amounts to a large fraction of the slowdown of beam-bas"
P13-2111,J03-4003,0,\N,Missing
P14-2127,N12-1015,1,0.95011,"r ChEn experiments) of the bitext in training. To better utilize the large training set, we propose to generalize from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (H IERO) (Chiang, 2005), in order to exploit sentence pairs beyond the expressive capacity of phrase-based MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT"
P14-2127,2007.mtsummit-papers.3,0,0.366235,"extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1. We generalize the latent variable violationfixing perceptron framework to inexact search over hypergraphs, which subsumes previous a"
P14-2127,P08-1067,1,0.885443,"Missing"
P14-2127,P08-1024,0,0.059643,"tent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1. We generalize the latent variable violationfixing perceptron framework to inexact search over hypergraphs, which subsumes previous algorithms for PBMT and"
P14-2127,P07-2045,0,0.0114073,"Missing"
P14-2127,P05-1022,0,0.167409,"Missing"
P14-2127,P09-1019,0,0.238999,"Missing"
P14-2127,D08-1024,0,0.312341,"Missing"
P14-2127,P13-1008,1,0.823747,"latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that as"
P14-2127,P05-1033,0,0.0899251,"such as M ERT and P RO. 1 ‡ Abe Ittycheriah‡ ↓ hypergraph Zhang et al. (13) latent −→ variable −→ variable Yu et al. (13) ↓ this work Figure 1: Relationship with previous work. small-scale M ERT/P RO by a large margin for the first time). However, the underlying phrase-based model suffers from limited distortion and thus can only employ a small portion (about 1/3 in their ChEn experiments) of the bitext in training. To better utilize the large training set, we propose to generalize from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (H IERO) (Chiang, 2005), in order to exploit sentence pairs beyond the expressive capacity of phrase-based MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural langua"
P14-2127,P06-1096,0,0.146431,"Missing"
P14-2127,W02-1001,0,0.0903093,"e capacity of phrase-based MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: i"
P14-2127,P05-1012,0,0.0712727,"d MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitab"
P14-2127,P10-4002,0,0.078474,"for each such index.2 We have a similar deductive system for forced decoding. For the previous example, rule r5 in Figure 2 (a) is rewritten as X → hyˇu X 1 jˇux´ıng X 2 , 1 X 2 4 X 1 i, where 1 and 4 are the indexes for reference words “held” and “with” respectively. The deduction for X[1:5] in Figure 2 (b) is X5?5 [2:3] : s1 X2?3 [4:5] : s2 1?5 X[1:5] : s(r5 ) + λ + s1 + s2 where λ = log w · Φ(x, d), 4 Q i∈{1,3,4} Pforced (i r5 , + 1 |i) = 0. Experiments Following Yu et al. (2013), we call our maxviolation method M AX F ORCE. Our implementation is mostly in Python on top of the cdec system (Dyer et al., 2010) via the pycdec interface (Chahuneau et al., 2012). In addition, we use minibatch parallelization of (Zhao and Huang, w · Φ(x, d), where Φ(x, d) is the feature vector for derivation d. Then it finds the group N[i∗∗ :j ∗ ] with the maximal score difference between the Viterbi derivation and the best y-good derivation: 1 We only consider single reference in this paper. Our formulation of index-based language model fixes a bug in the word-based LM of Yu et al. (2013) when a substring appears more than once in the reference (e.g. “the man...the man...”); thanks to Dan Gildea for pointing it out. ∆"
P14-2127,P03-1021,0,0.191299,"Missing"
P14-2127,D07-1080,0,0.256905,"e just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1. We generalize the latent variable violationfixing perceptron framework to inexact search over hypergraphs, whi"
P14-2127,N13-1025,0,0.281361,"ibutions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1. We generalize the latent variable violationfixing perceptron framework to inexact search over hypergraphs, which subsumes previous algorithms for PBMT and bottomup parsing as special cases (see Fig."
P14-2127,D13-1112,1,0.891392,"c}.cuny.edu T. J. Watson Research Center IBM {hmi,abei}@us.ibm.com inexact Abstract Collins (02) −→ Huang et al. (12) search Large-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus; for example the recent effort in phrase-based MT (Yu et al., 2013) significantly outperforms mainstream methods that only train on small tuning sets. However, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit. To address this problem, we extend Yu et al. (2013) to syntax-based MT by generalizing their latent variable “violation-fixing” perceptron from graphs to hypergraphs. Experiments confirm that our method leads to up to +1.2 B LEU improvement over mainstream methods such as M ERT and P RO. 1 ‡ Abe Ittycheriah‡ ↓ hypergraph Zhang et al. (13) latent −→ variable −→ variable Yu et al. (13) ↓ this work Figure 1: Relationship with previous work. small-scale M ERT/P RO by a large margin for the first time). However, the underlying phrase-based model suffers from limited distortion and thus can only employ a small portion (about 1/3 in their ChEn experi"
P14-2127,P13-1031,0,0.044397,"Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1. We generalize the latent variable violationfixing perceptron framework to inexact search over hypergraphs, which subsumes previous algorithms for PBMT and bottomup parsing as special cases (see Fig. 1). 2. We show that"
P14-2127,D13-1093,1,0.94016,"suffers from limited distortion and thus can only employ a small portion (about 1/3 in their ChEn experiments) of the bitext in training. To better utilize the large training set, we propose to generalize from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (H IERO) (Chiang, 2005), in order to exploit sentence pairs beyond the expressive capacity of phrase-based MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been el"
P14-2127,D11-1125,0,0.16939,"Missing"
P14-2127,N13-1038,1,0.898412,"Missing"
P14-2127,P07-1019,1,0.875942,"anslation (i.e., spurious ambiguity), Yu et al. (2013) extends it to handle latent variables which correspond to phrase-based derivations. On the other hand, Zhang et al. (2013) has generalized Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles H IERO decoding. So we just need to combine the two generalizing directions (latent variable and hypergraph, see Fig. 1). Review: Syntax-based MT Decoding For clarity reasons we will describe H IERO decoding as a two-pass process, first without a language model, and then integrating the LM. This section mostly follows Huang and Chiang (2007). In the first, −LM phase, the decoder parses the source sentence using the source projection of the synchronous grammar (see Fig. 2 (a) for an example), producing a −LM hypergraph where each node has a signature N[i:j] , where N is the nonterminal type (either X or S in H IERO) and [i : j] is the span, and each hyperedge e is an application of the translation rule r(e) (see Figure 3). To incorporate the language model, each node also needs to remember its target side boundary words. Thus a −LM node N[i:j] is split into mula?b , where a and tiple +LM nodes of signature N[i:j] b are the boundar"
P15-2029,E06-1011,0,0.0423355,"Missing"
P15-2029,C04-1121,0,0.176218,"Missing"
P15-2029,D15-1279,0,0.0480131,"but while his sequential CNNs put a word in its sequential context, ours considers a word and its parent, grandparent, great-grand-parent, and siblings on the dependency tree. This way we incorporate longdistance information that are otherwise unavailable on the surface string. Experiments on three classification tasks demonstrate the superior performance of our DCNNs over the baseline sequential CNNs. In particular, our accuracy on the TREC dataset outperforms all previously published results in the literature, including those with heavy hand-engineered features. Independently of this work, Mou et al. (2015, unpublished) reported related efforts; see Sec. 3.3. In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To combine deep learning with linguistic structures, we propose a dependency-based convolution approach, making use of tree-based n-grams rather than surface ones, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achi"
P15-2029,P05-1015,0,0.29457,"Missing"
P15-2029,D14-1070,0,0.0374971,"Missing"
P15-2029,P06-1063,0,0.0715932,"Missing"
P15-2029,P14-1062,0,0.550006,", thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC. 1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the latter play an important role in many linguistic phenomena such as negation, subordination, and wh-extraction, all of which might dully a"
P15-2029,D14-1181,0,0.224626,"ements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al., 2003; Kudo and Matsumoto, 2004). As a result, syntactic features have yet to become popular in the sentiment analysis community. We suspect one of the reasons for this is data sparsity (according to our experiments, tree n-grams are significantly sparser than surface n-grams), but this problem has largely been alleviated by the recent advances in word embedding. Can we combine the advantages of both worlds? So we propose a very simple dependency-based convolutional neural networks (DCNNs). Our model is similar to Kim (2014), but while his sequential CNNs put a word in its sequential context, ours considers a word and its parent, grandparent, great-grand-parent, and siblings on the dependency tree. This way we incorporate longdistance information that are otherwise unavailable on the surface string. Experiments on three classification tasks demonstrate the superior performance of our DCNNs over the baseline sequential CNNs. In particular, our accuracy on the TREC dataset outperforms all previously published results in the literature, including those with heavy hand-engineered features. Independently of this work,"
P15-2029,D11-1014,0,0.694798,"being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the latter play an important role in many linguistic phenomena such as negation, subordination, and wh-extraction, all of which might dully affect the sentiment, subjectivity, or other categorization of the sentence. 2 Dependency-based Convolution The original CNN, first proposed by LeCun et al. (1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al. (2011). Following Kim (2014), one dimensional convolution operates the convolution kernel in sequential order in Equation 1, where xi ∈ Rd represents the d dimensional word representation for the i-th word in ∗ This work was done at both IBM and CUNY, and was supported in part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions. 174 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages"
P15-2029,P10-1001,0,0.014268,"Missing"
P15-2029,W04-3239,0,0.100597,"Missing"
P15-2029,D13-1170,0,0.0564206,"Missing"
P15-2029,P14-2105,0,0.0131604,"stic structures, we propose a dependency-based convolution approach, making use of tree-based n-grams rather than surface ones, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC. 1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the lat"
P15-2029,C02-1150,0,0.030806,"Missing"
P15-2029,P14-5010,0,0.00964441,"Missing"
P16-2006,P15-1032,0,0.354221,"oth English and Chinese. 1 Introduction Recently, neural network-based parsers have become popular, with the promise of reducing the burden of manual feature engineering. For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features. However, this approach has two related limitations. First, it still depends on a large number of carefully designed atomic features. For example, Chen and Manning (2014) and subsequent work such as Weiss et al. (2015) use 48 atomic features from Zhang and Nivre (2011), including select thirdorder dependencies. More importantly, this approach inevitably leaves out some nonlocal information which could be useful. In particular, 2 LSTM Position Features f1 ;b1 f2 ;b2 f3 ;b3 f4 ;b4 f5 ;b5 w1 ;t1 w2 ;t2 w3 ;t3 w4 ;t4 w5 ;t5 Figure 1: The sentence is modeled with an LSTM in each direction whose input vectors at each time step are word and part-of-speech tag embeddings. 32 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 32–37, c Berlin, Germany, August 7-12, 2016. 20"
P16-2006,D14-1082,0,0.882674,"er state with only three sentence positions, which automatically identifies important aspects of the entire sentence. This model achieves state-of-the-art results among greedy dependency parsers for English. We also introduce a novel transition system for constituency parsing which does not require binarization, and together with the above architecture, achieves state-of-the-art results among greedy parsers for both English and Chinese. 1 Introduction Recently, neural network-based parsers have become popular, with the promise of reducing the burden of manual feature engineering. For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features. However, this approach has two related limitations. First, it still depends on a large number of carefully designed atomic features. For example, Chen and Manning (2014) and subsequent work such as Weiss et al. (2015) use 48 atomic features from Zhang and Nivre (2011), including select thirdorder dependencies. More importantly, this approach inevitably leaves out some nonlocal information which could b"
P16-2006,P00-1058,0,0.031585,"n to delay a right-reduce until the top tree on the stack is fully formed, shifting instead. While most previous work binarizes the constituency tree in a preprocessing step (Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015), we propose a novel “Shift-PromoteAdjoin” paradigm which does not require any binariziation or transformation of constituency trees (see Figure 5). Note in particular that, in our case only the Promote action produces a new tree node (with a non-terminal label), while the Adjoin action is the linguistically-motivated “sisteradjunction” operation, i.e., attachment (Chiang, 2000; Henderson, 2003). By comparison, in previous work, both Unary-X and Reduce-L/R-X actions produce new labeled nodes (some of which are auxiliary nodes due to binarization). Thus our paradigm has two advantages: Hierarchical Classification The structure of our network model after computing positional features is fairly straightforward and similar to previous neural-network parsing approaches such as Chen and Manning (2014) and Weiss et al. (2015). It consists of a multilayer perceptron using a single ReLU hidden layer followed by a linear classifier over the action space, with the training obj"
P16-2006,P11-2033,0,0.118008,"-art results among greedy dependency parsers for English. We also introduce a novel transition system for constituency parsing which does not require binarization, and together with the above architecture, achieves state-of-the-art results among greedy parsers for both English and Chinese. 1 Introduction Recently, neural network-based parsers have become popular, with the promise of reducing the burden of manual feature engineering. For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features. However, this approach has two related limitations. First, it still depends on a large number of carefully designed atomic features. For example, Chen and Manning (2014) and subsequent work such as Weiss et al. (2015) use 48 atomic features from Zhang and Nivre (2011), including select thirdorder dependencies. More importantly, this approach inevitably leaves out some nonlocal information which could be useful. In particular, 2 LSTM Position Features f1 ;b1 f2 ;b2 f3 ;b3 f4 ;b4 f5 ;b5 w1 ;t1 w2 ;t2 w3 ;t3 w4 ;t4 w5 ;t5 Figure 1: The sentence is mod"
P16-2006,P15-1033,0,0.131348,"representations for each word in the sentence. This approach allows the model to learn arbitrary patterns from the entire sentence, effectively extending the generalization power of embedding individual words to longer sequences. Since such a feature representation is less dependent on earlier parser decisions, it is also more resilient to local mistakes. With just three positional features we can build a greedy shift-reduce dependency parser that is on par with the most accurate parser in the published literature for English Treebank. This effort is similar in motivation to the stack-LSTM of Dyer et al. (2015), but uses a much simpler architecture. We also extend this model to predict phrasestructure trees with a novel shift-promote-adjoin system tailored to greedy constituency parsing, and with just two more positional features (defining tree span) and nonterminal label embeddings we achieve the most accurate greedy constituency parser for both English and Chinese. Recently, neural network approaches for parsing have largely automated the combination of individual features, but still rely on (often a larger number of) atomic features created from human linguistic intuition, and potentially omittin"
P16-2006,P13-1043,0,0.585462,"P) 5 shift (sports) 2 pro 7 NP 6 6 pro (NP) 7 adj y 8 pro (S) 9 adj x NNS hS |t |X(t1 ...tk ), ji Figure 5: Shift-Promote-Adjoin parsing example. Upward and downward arrows indicate promote and (sister-)adjunction actions, respectively. hS |X(t, t1 ...tk ), ji hs0 , ni • we also need to predict the nonterminal labels • the tree is not binarized (with many unary rules and more than binary branching rules) the model to learn to delay a right-reduce until the top tree on the stack is fully formed, shifting instead. While most previous work binarizes the constituency tree in a preprocessing step (Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015), we propose a novel “Shift-PromoteAdjoin” paradigm which does not require any binariziation or transformation of constituency trees (see Figure 5). Note in particular that, in our case only the Promote action produces a new tree node (with a non-terminal label), while the Adjoin action is the linguistically-motivated “sisteradjunction” operation, i.e., attachment (Chiang, 2000; Henderson, 2003). By comparison, in previous work, both Unary-X and Reduce-L/R-X actions produce new labeled nodes (some of which are auxiliary nodes due to binarization). Thus"
P16-2006,P16-2060,1,0.824387,"English and Chinese.1,2 Parser b Zhu et al. (2013) Mi & Huang (05) Vinyals et al. (05) Bi-LSTM 2-Layer Bi-LSTM 16 32 10 - English Chinese greedy beam greedy beam 86.08 84.95 89.75 89.95 75.99 75.61 79.44 80.13 90.4 90.8 90.5 - 50 20 No 100 100 100 No 200 200 / decision 200 1000 10 10 0.5 none 0.99 1 × 10−7 10 10 0.5 1 × 10−8 0.99 1 × 10−7 Related Work Because recurrent networks are such a natural fit for modeling languages (given the sequential nature of the latter), bi-directional LSTM networks are becoming increasingly common in all sorts of linguistic tasks, for example event detection in Ghaeini et al. (2016). In fact, we discovered after submission that Kiperwasser and Goldberg (2016) have concurrently developed an extremely similar approach to our dependency parser. Instead of extending it to constituency parsing, they also apply the same idea to graph-based dependency parsing. Table 4: Development and test set results for shiftreduce dependency parser on Penn Chinese Treebank (CTB-5) using only (s1 , s0 , q0 ) position features (trained and tested with gold POS tags). 5.2 Constituency Table 6: Hyperparameters and training settings. 6 Parser Dependency Conclusions We have presented a simple bi-d"
P16-2006,N03-1014,0,0.651041,"ight-reduce until the top tree on the stack is fully formed, shifting instead. While most previous work binarizes the constituency tree in a preprocessing step (Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015), we propose a novel “Shift-PromoteAdjoin” paradigm which does not require any binariziation or transformation of constituency trees (see Figure 5). Note in particular that, in our case only the Promote action produces a new tree node (with a non-terminal label), while the Adjoin action is the linguistically-motivated “sisteradjunction” operation, i.e., attachment (Chiang, 2000; Henderson, 2003). By comparison, in previous work, both Unary-X and Reduce-L/R-X actions produce new labeled nodes (some of which are auxiliary nodes due to binarization). Thus our paradigm has two advantages: Hierarchical Classification The structure of our network model after computing positional features is fairly straightforward and similar to previous neural-network parsing approaches such as Chen and Manning (2014) and Weiss et al. (2015). It consists of a multilayer perceptron using a single ReLU hidden layer followed by a linear classifier over the action space, with the training objective being negat"
P16-2006,Q16-1023,0,0.119229,"Vinyals et al. (05) Bi-LSTM 2-Layer Bi-LSTM 16 32 10 - English Chinese greedy beam greedy beam 86.08 84.95 89.75 89.95 75.99 75.61 79.44 80.13 90.4 90.8 90.5 - 50 20 No 100 100 100 No 200 200 / decision 200 1000 10 10 0.5 none 0.99 1 × 10−7 10 10 0.5 1 × 10−8 0.99 1 × 10−7 Related Work Because recurrent networks are such a natural fit for modeling languages (given the sequential nature of the latter), bi-directional LSTM networks are becoming increasingly common in all sorts of linguistic tasks, for example event detection in Ghaeini et al. (2016). In fact, we discovered after submission that Kiperwasser and Goldberg (2016) have concurrently developed an extremely similar approach to our dependency parser. Instead of extending it to constituency parsing, they also apply the same idea to graph-based dependency parsing. Table 4: Development and test set results for shiftreduce dependency parser on Penn Chinese Treebank (CTB-5) using only (s1 , s0 , q0 ) position features (trained and tested with gold POS tags). 5.2 Constituency Table 6: Hyperparameters and training settings. 6 Parser Dependency Conclusions We have presented a simple bi-directional LSTM sentence representation model for minimal features in both inc"
P16-2006,N15-1108,1,0.92661,"(NP) 7 adj y 8 pro (S) 9 adj x NNS hS |t |X(t1 ...tk ), ji Figure 5: Shift-Promote-Adjoin parsing example. Upward and downward arrows indicate promote and (sister-)adjunction actions, respectively. hS |X(t, t1 ...tk ), ji hs0 , ni • we also need to predict the nonterminal labels • the tree is not binarized (with many unary rules and more than binary branching rules) the model to learn to delay a right-reduce until the top tree on the stack is fully formed, shifting instead. While most previous work binarizes the constituency tree in a preprocessing step (Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015), we propose a novel “Shift-PromoteAdjoin” paradigm which does not require any binariziation or transformation of constituency trees (see Figure 5). Note in particular that, in our case only the Promote action produces a new tree node (with a non-terminal label), while the Adjoin action is the linguistically-motivated “sisteradjunction” operation, i.e., attachment (Chiang, 2000; Henderson, 2003). By comparison, in previous work, both Unary-X and Reduce-L/R-X actions produce new labeled nodes (some of which are auxiliary nodes due to binarization). Thus our paradigm has two advantages: Hierarch"
P16-2006,nivre-etal-2006-maltparser,0,0.0601182,"chieves state-of-the-art results among greedy dependency parsers for English. We also introduce a novel transition system for constituency parsing which does not require binarization, and together with the above architecture, achieves state-of-the-art results among greedy parsers for both English and Chinese. 1 Introduction Recently, neural network-based parsers have become popular, with the promise of reducing the burden of manual feature engineering. For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features. However, this approach has two related limitations. First, it still depends on a large number of carefully designed atomic features. For example, Chen and Manning (2014) and subsequent work such as Weiss et al. (2015) use 48 atomic features from Zhang and Nivre (2011), including select thirdorder dependencies. More importantly, this approach inevitably leaves out some nonlocal information which could be useful. In particular, 2 LSTM Position Features f1 ;b1 f2 ;b2 f3 ;b3 f4 ;b4 f5 ;b5 w1 ;t1 w2 ;t2 w3 ;t3 w4 ;t4 w5 ;t5 Figur"
P16-2006,J08-4003,0,0.188594,"rk parameters in this work. In our initial experiments, we used one LSTM layer in each direction (forward and backward), and then concatenate the output at each time step to represent that sentence position: that word in the entire context of the sentence. This network is illustrated in Figure 1. h1 h2 h3 h4 h5 f12 ;b21 f22 ;b22 f32 ;b23 f42 ;b24 f52 ;b25 f11 ;b11 f21 ;b12 f31 ;b13 f41 ;b14 f51 ;b15 input: w0 . . . wn−1 axiom h, 0i: ∅ shift rex goal hS, ji : A hS|j, j + 1i : A j<n hS|s1 |s0 , ji : A hS|s0 , ji : A ∪ {s1 x s0 } hs0 , ni: A Figure 3: The arc-standard dependency parsing system (Nivre, 2008) (rey omitted). Stack S is a list of heads, j is the start index of the queue, and s0 and s1 are the top two head indices on S. positional labels dependency constituency s1 , s0 , q0 - s1 , s0 , q0 , s1 .left, s0 .left s0 .{left, right, root, head} s1 .{left, right, root, head} Table 1: Feature templates. Note that, remarkably, even though we do labeled dependency parsing, we do not include arc label as features. ture is shown in Figure 2. Intuitively, this represents the sentence position by the word in the context of the sentence up to that point and the sentence after that point in the firs"
P16-2006,P14-1069,0,0.184505,") 2 pro 7 NP 6 6 pro (NP) 7 adj y 8 pro (S) 9 adj x NNS hS |t |X(t1 ...tk ), ji Figure 5: Shift-Promote-Adjoin parsing example. Upward and downward arrows indicate promote and (sister-)adjunction actions, respectively. hS |X(t, t1 ...tk ), ji hs0 , ni • we also need to predict the nonterminal labels • the tree is not binarized (with many unary rules and more than binary branching rules) the model to learn to delay a right-reduce until the top tree on the stack is fully formed, shifting instead. While most previous work binarizes the constituency tree in a preprocessing step (Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015), we propose a novel “Shift-PromoteAdjoin” paradigm which does not require any binariziation or transformation of constituency trees (see Figure 5). Note in particular that, in our case only the Promote action produces a new tree node (with a non-terminal label), while the Adjoin action is the linguistically-motivated “sisteradjunction” operation, i.e., attachment (Chiang, 2000; Henderson, 2003). By comparison, in previous work, both Unary-X and Reduce-L/R-X actions produce new labeled nodes (some of which are auxiliary nodes due to binarization). Thus our paradigm has two"
P16-2060,P13-1008,1,0.938356,"methods on the ACE 2005 and the Rich ERE 2015 event detection tasks. 1 • Event nugget: a word or a phrase of multiple words that most clearly expresses the occurrence of an event. This scheme is recently introduced to remove the limitation of singletoken event triggers and has been adopted by the rich ERE data for event annotation. Existing event extraction work often heavily relies on a rich set of hand-designed features and utilizes existing NLP toolkits and resources (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). Consequently, it is often challenging to adapt prior methods to multi-lingual or nonEnglish settings since they require extensive linguistic knowledge for feature engineering and mature NLP toolkits for extracting the features without severe error propagation. By contrast, deep learning has recently emerged as a compelling solution to avoid the aforementioned problems by automatically extracting meaningful features from raw text without relying on existing NLP toolkits. There have been some limited attempts in using deep learning for event detection (Nguy"
P16-2060,P10-1081,0,0.531287,"results demonstrate that FBRNN is competitive with the state-of-the-art methods on the ACE 2005 and the Rich ERE 2015 event detection tasks. 1 • Event nugget: a word or a phrase of multiple words that most clearly expresses the occurrence of an event. This scheme is recently introduced to remove the limitation of singletoken event triggers and has been adopted by the rich ERE data for event annotation. Existing event extraction work often heavily relies on a rich set of hand-designed features and utilizes existing NLP toolkits and resources (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). Consequently, it is often challenging to adapt prior methods to multi-lingual or nonEnglish settings since they require extensive linguistic knowledge for feature engineering and mature NLP toolkits for extracting the features without severe error propagation. By contrast, deep learning has recently emerged as a compelling solution to avoid the aforementioned problems by automatically extracting meaningful features from raw text without relying on existing NLP toolkits. There have been some l"
P16-2060,P15-1017,0,0.191891,"et al., 2014). Consequently, it is often challenging to adapt prior methods to multi-lingual or nonEnglish settings since they require extensive linguistic knowledge for feature engineering and mature NLP toolkits for extracting the features without severe error propagation. By contrast, deep learning has recently emerged as a compelling solution to avoid the aforementioned problems by automatically extracting meaningful features from raw text without relying on existing NLP toolkits. There have been some limited attempts in using deep learning for event detection (Nguyen and Grishman, 2015; Chen et al., 2015) which apply Convolutional Neural Networks (CNNs) to a window of text around potential triggers to identify events. These efforts outperform traditional methods, but there remain two major limitations: Introduction Automatic event extraction from natural text is an important and challenging task for natural language understanding. Given a set of ontologized event types, the goal of event extraction is to identify the mentions of different event types and their arguments from natural texts. In this paper we focus on the problem of extracting event mentions, which can be in the form of a single"
P16-2060,P16-2006,1,0.822127,"be applied to inputs of variable length which eliminates both the requirement of single-token event trigger and the need for a fixed window size. Using recurrent nodes with Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Units (GRU) (Cho et al., 2014), RNN is potentially capable of selectively deciding the relevant context to consider for detecting events. In this paper we present a forward-backward recurrent neural network (FBRNN) to extract (possibly multi-word) event mentions from raw text. Although RNNs have been studied extensively in other NLP tasks (Cross and Huang, 2016; Tai et al., 2015; Socher et al., 2014; Paulus et al., 2014), to the best of our knowledge, this is the first work to use RNNs for event detection. This is also one of the first efforts to handle multi-word event nuggets. Experimental results confirm that FBRNN is competitive compared to the state-ofthe-art on the ACE 2005 dataset and the Rich ERE 2015 event detection task. 2 • Word embedding: Several studies have investigated methods for representing words as real-valued vectors in order to capture the hidden semantic and syntactic properties of words (Collobert and Weston, 2008; Mikolov et"
P16-2060,D09-1016,0,0.0763818,"vent detection. Experimental results demonstrate that FBRNN is competitive with the state-of-the-art methods on the ACE 2005 and the Rich ERE 2015 event detection tasks. 1 • Event nugget: a word or a phrase of multiple words that most clearly expresses the occurrence of an event. This scheme is recently introduced to remove the limitation of singletoken event triggers and has been adopted by the rich ERE data for event annotation. Existing event extraction work often heavily relies on a rich set of hand-designed features and utilizes existing NLP toolkits and resources (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). Consequently, it is often challenging to adapt prior methods to multi-lingual or nonEnglish settings since they require extensive linguistic knowledge for feature engineering and mature NLP toolkits for extracting the features without severe error propagation. By contrast, deep learning has recently emerged as a compelling solution to avoid the aforementioned problems by automatically extracting meaningful features from raw text without relying on existing NLP toolkit"
P16-2060,P08-1030,0,0.324426,"tempt to use RNNs for event detection. Experimental results demonstrate that FBRNN is competitive with the state-of-the-art methods on the ACE 2005 and the Rich ERE 2015 event detection tasks. 1 • Event nugget: a word or a phrase of multiple words that most clearly expresses the occurrence of an event. This scheme is recently introduced to remove the limitation of singletoken event triggers and has been adopted by the rich ERE data for event annotation. Existing event extraction work often heavily relies on a rich set of hand-designed features and utilizes existing NLP toolkits and resources (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). Consequently, it is often challenging to adapt prior methods to multi-lingual or nonEnglish settings since they require extensive linguistic knowledge for feature engineering and mature NLP toolkits for extracting the features without severe error propagation. By contrast, deep learning has recently emerged as a compelling solution to avoid the aforementioned problems by automatically extracting meaningful features from raw text without re"
P16-2060,P15-1150,0,0.0580348,"Missing"
P16-2060,P13-1145,0,0.131004,"Missing"
P16-2060,P11-1163,0,\N,Missing
P16-2060,Q14-1017,0,\N,Missing
P16-2060,D14-1198,0,\N,Missing
P16-2060,P15-2060,0,\N,Missing
P17-2053,1993.eamt-1.1,0,0.647058,"Missing"
P17-2053,P15-2029,1,0.851551,"ictionary learning to CNN, we first develop a neural version of SGL, Group Sparse Autoencoders (GSAs), which to the best of our knowledge, is the first full neural model with group sparse constraints. The encoding matrix of GSA (like the dictionary in SGL) is grouped into different categories. The bases in different groups can be either initialized randomly or by Introduction Question classification has applications in many domains ranging from question answering to dialog systems, and has been increasingly popular in recent years. Several recent efforts (Kim, 2014; Kalchbrenner et al., 2014; Ma et al., 2015) treat questions as general sentences and employ Convolutional Neural Networks (CNNs) to achieve remarkably strong performance in the TREC question classification task. We argue, however, that those general sentence modeling frameworks neglect two unique properties of question classification. First, different from the flat and coarse categories in most sentence classification tasks (i.e. sentimental classification), question classes often have a hierarchical structure such as those from the New York State DMV FAQ1 (see Fig. 1). Another unique aspect of question classification is the well prepa"
P17-2053,P14-1062,0,0.101124,"n et al., 2013). To apply dictionary learning to CNN, we first develop a neural version of SGL, Group Sparse Autoencoders (GSAs), which to the best of our knowledge, is the first full neural model with group sparse constraints. The encoding matrix of GSA (like the dictionary in SGL) is grouped into different categories. The bases in different groups can be either initialized randomly or by Introduction Question classification has applications in many domains ranging from question answering to dialog systems, and has been increasingly popular in recent years. Several recent efforts (Kim, 2014; Kalchbrenner et al., 2014; Ma et al., 2015) treat questions as general sentences and employ Convolutional Neural Networks (CNNs) to achieve remarkably strong performance in the TREC question classification task. We argue, however, that those general sentence modeling frameworks neglect two unique properties of question classification. First, different from the flat and coarse categories in most sentence classification tasks (i.e. sentimental classification), question classes often have a hierarchical structure such as those from the New York State DMV FAQ1 (see Fig. 1). Another unique aspect of question classification"
P17-2053,D14-1181,0,0.151024,"(SGL) (Simon et al., 2013). To apply dictionary learning to CNN, we first develop a neural version of SGL, Group Sparse Autoencoders (GSAs), which to the best of our knowledge, is the first full neural model with group sparse constraints. The encoding matrix of GSA (like the dictionary in SGL) is grouped into different categories. The bases in different groups can be either initialized randomly or by Introduction Question classification has applications in many domains ranging from question answering to dialog systems, and has been increasingly popular in recent years. Several recent efforts (Kim, 2014; Kalchbrenner et al., 2014; Ma et al., 2015) treat questions as general sentences and employ Convolutional Neural Networks (CNNs) to achieve remarkably strong performance in the TREC question classification task. We argue, however, that those general sentence modeling frameworks neglect two unique properties of question classification. First, different from the flat and coarse categories in most sentence classification tasks (i.e. sentimental classification), question classes often have a hierarchical structure such as those from the New York State DMV FAQ1 (see Fig. 1). Another unique aspect"
P18-2076,P10-1110,1,0.919536,"space, and on the other hand, a chartbased span parser (Stern et al., 2017a) performs exact search and achieves state-of-the-art accuracy, but in cubic time, which is too slow for longer sentences and for applications that go beyond sentence boundaries such as end-to-end discourse parsing (Hernault et al., 2010; Zhao and Huang, 2017) and integrated sentence boundary detection and parsing (Bj¨orkelund et al., 2016). We propose to combine the merits of both greedy and chart-based approaches and design a linear-time span-based neural parser that searches over exponentially large space. Following Huang and Sagae (2010), we perform left-to-right dynamic programming in an action-synchronous style, with (2n − 1) actions (i.e., steps) for a sentence of n words. While previous non-neural work in this area requires sophisticated features (Huang and Sagae, 2010; Mi and Huang, 2015) and thus high time complexity such as O(n11 ), our states are as simple as ` : (i, j) where ` is the step index and (i, j) is the span, modeled using bidirectional RNNs without any syntactic features. This gives a running time of O(n4 ), with the extra O(n) for step index. We further employ beam search to have a practical runtime of O(n"
P18-2076,W12-1623,0,0.0622536,"Missing"
P18-2076,Q16-1023,0,0.02596,"nts We present experiments on the Penn Treebank (Marcus et al., 1993) and the PTB-RST discourse treebank (Zhao and Huang, 2017). In both cases, the training set is shuffled before each epoch, and dropout (Hinton et al., 2012) is employed with probability 0.4 to the recurrent outputs for regularization. Updates with minibatches of size 10 and 1 are used for PTB and the PTB-RST respectively. We use Adam (Kingma and Ba, 2014) with default settings to schedule learning rates for all the weights. To address unknown words during training, we adopt the strategy described by Kiperwasser and Goldberg (Kiperwasser and Goldberg, 2016); words in the training set are replaced with the unknown word symbol UNK with probability punk = 1+f1(w) , with f (w) being the number of (i,j,X)∈t ∨ cross(i, j, t∗ ) Max Violation Updates Given that we maintain loss-augmented scores even for partial trees, we can perform a training update on a given example sentence by choosing to take the loss where it is the greatest along the parse trajectory. At each parsing time-step `, the violation is the difference between the highest augmented-scoring parse trajectory up to that point and the gold trajectory (Huang et al., 2012; Yu et al., 2013). No"
P18-2076,P16-1181,0,0.0404128,"Missing"
P18-2076,J93-2004,0,0.060935,"∅, and t∗(i,j) denotes the gold label for span (i, j), which could also be ∅.6 However, there are two cases where t∗(i,j) = ∅: a subspan (i, j) due to binarization (e.g., a span combining the first two subtrees in a ternary branching node), or an invalid span in t that crosses a gold span in t∗ . In the baseline function above, these two cases are treated equivalently; for example, a span (3, 5, ∅) ∈ t is not penalized even if there is a gold span (4, 6, VP) ∈ t∗ . So we revise our loss function as:  X ∆new (t, t∗ ) = 1 X 6= t∗(i,j) 5 Experiments We present experiments on the Penn Treebank (Marcus et al., 1993) and the PTB-RST discourse treebank (Zhao and Huang, 2017). In both cases, the training set is shuffled before each epoch, and dropout (Hinton et al., 2012) is employed with probability 0.4 to the recurrent outputs for regularization. Updates with minibatches of size 10 and 1 are used for PTB and the PTB-RST respectively. We use Adam (Kingma and Ba, 2014) with default settings to schedule learning rates for all the weights. To address unknown words during training, we adopt the strategy described by Kiperwasser and Goldberg (Kiperwasser and Goldberg, 2016); words in the training set are replac"
P18-2076,J07-2003,0,0.0686678,"s are as simple as ` : (i, j) where ` is the step index and (i, j) is the span, modeled using bidirectional RNNs without any syntactic features. This gives a running time of O(n4 ), with the extra O(n) for step index. We further employ beam search to have a practical runtime of O(nb2 ) at the cost of exact search where b is the beam size. However, on the Penn Treebank, most sentences are less than 40 words (n &lt; 40), and even with a small beam size of b = 10, the observed complexity of an O(nb2 ) parser is not exactly linear in n (see Experiments). To solve this problem, we apply cube pruning (Chiang, 2007; Huang and Chiang, 2007) to improve the runtime to O(nb log b) which Recently, span-based constituency parsing has achieved competitive accuracies with extremely simple models by using bidirectional RNNs to model “spans”. However, the minimal span parser of Stern et al. (2017a) which holds the current state of the art accuracy is a chart parser running in cubic time, O(n3 ), which is too slow for longer sentences and for applications beyond sentence boundaries such as end-toend discourse parsing and joint sentence boundary detection and parsing. We propose a linear-time constituency parser wi"
P18-2076,D16-1257,0,0.192639,"Missing"
P18-2076,N15-1108,1,0.858915,"-to-end discourse parsing (Hernault et al., 2010; Zhao and Huang, 2017) and integrated sentence boundary detection and parsing (Bj¨orkelund et al., 2016). We propose to combine the merits of both greedy and chart-based approaches and design a linear-time span-based neural parser that searches over exponentially large space. Following Huang and Sagae (2010), we perform left-to-right dynamic programming in an action-synchronous style, with (2n − 1) actions (i.e., steps) for a sentence of n words. While previous non-neural work in this area requires sophisticated features (Huang and Sagae, 2010; Mi and Huang, 2015) and thus high time complexity such as O(n11 ), our states are as simple as ` : (i, j) where ` is the step index and (i, j) is the span, modeled using bidirectional RNNs without any syntactic features. This gives a running time of O(n4 ), with the extra O(n) for step index. We further employ beam search to have a practical runtime of O(nb2 ) at the cost of exact search where b is the beam size. However, on the Penn Treebank, most sentences are less than 40 words (n &lt; 40), and even with a small beam size of b = 10, the observed complexity of an O(nb2 ) parser is not exactly linear in n (see Exp"
P18-2076,D16-1001,1,0.719955,"ection and parsing. We propose a linear-time constituency parser with RNNs and dynamic programming using graph-structured stack and beam search, which runs in time O(nb2 ) where b is the beam size. We further speed this up to O(nb log b) by integrating cube pruning. Compared with chart parsing baselines, this linear-time parser is substantially faster for long sentences on the Penn Treebank and orders of magnitude faster for discourse parsing, and achieves the highest F1 accuracy on the Penn Treebank among single model end-to-end systems. 1 Introduction Span-based neural constituency parsing (Cross and Huang, 2016; Stern et al., 2017a) has attracted attention due to its high accuracy and extreme simplicity. Compared with other recent neural constituency parsers (Dyer et al., 2016; Liu and Zhang, 2016; Durrett and Klein, 2015) which use neural networks to model tree structures, the spanbased framework is considerably simpler, only using bidirectional RNNs to model the input sequence and not the output tree. Because of this factorization, the output space is decomposable 477 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 477–483 c Melbourne,"
P18-2076,N16-1024,0,0.055137,"here b is the beam size. We further speed this up to O(nb log b) by integrating cube pruning. Compared with chart parsing baselines, this linear-time parser is substantially faster for long sentences on the Penn Treebank and orders of magnitude faster for discourse parsing, and achieves the highest F1 accuracy on the Penn Treebank among single model end-to-end systems. 1 Introduction Span-based neural constituency parsing (Cross and Huang, 2016; Stern et al., 2017a) has attracted attention due to its high accuracy and extreme simplicity. Compared with other recent neural constituency parsers (Dyer et al., 2016; Liu and Zhang, 2016; Durrett and Klein, 2015) which use neural networks to model tree structures, the spanbased framework is considerably simpler, only using bidirectional RNNs to model the input sequence and not the output tree. Because of this factorization, the output space is decomposable 477 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 477–483 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics renders an observed complexity that is linear in n (with minor extra inexactness). We make"
P18-2076,D17-1002,1,0.900251,"Missing"
P18-2076,P17-2025,0,0.325167,"Missing"
P18-2076,P13-1045,0,0.0923946,"Missing"
P18-2076,P17-1076,0,0.121462,"tituency Parsing with RNNs and Dynamic Programming Juneki Hong 1 1 School of EECS Oregon State University, Corvallis, OR Liang Huang 1,2 2 Silicon Valley AI Lab Baidu Research, Sunnyvale, CA {juneki.hong, liang.huang.sh}@gmail.com Abstract which enables efficient dynamic programming algorithm such as CKY. But existing span-based parsers suffer from a crucial limitation in terms of search: on the one hand, a greedy span parser (Cross and Huang, 2016) is fast (linear-time) but only explores one single path in the exponentially large search space, and on the other hand, a chartbased span parser (Stern et al., 2017a) performs exact search and achieves state-of-the-art accuracy, but in cubic time, which is too slow for longer sentences and for applications that go beyond sentence boundaries such as end-to-end discourse parsing (Hernault et al., 2010; Zhao and Huang, 2017) and integrated sentence boundary detection and parsing (Bj¨orkelund et al., 2016). We propose to combine the merits of both greedy and chart-based approaches and design a linear-time span-based neural parser that searches over exponentially large space. Following Huang and Sagae (2010), we perform left-to-right dynamic programming in an"
P18-2076,P07-1019,1,0.719581,"e as ` : (i, j) where ` is the step index and (i, j) is the span, modeled using bidirectional RNNs without any syntactic features. This gives a running time of O(n4 ), with the extra O(n) for step index. We further employ beam search to have a practical runtime of O(nb2 ) at the cost of exact search where b is the beam size. However, on the Penn Treebank, most sentences are less than 40 words (n &lt; 40), and even with a small beam size of b = 10, the observed complexity of an O(nb2 ) parser is not exactly linear in n (see Experiments). To solve this problem, we apply cube pruning (Chiang, 2007; Huang and Chiang, 2007) to improve the runtime to O(nb log b) which Recently, span-based constituency parsing has achieved competitive accuracies with extremely simple models by using bidirectional RNNs to model “spans”. However, the minimal span parser of Stern et al. (2017a) which holds the current state of the art accuracy is a chart parser running in cubic time, O(n3 ), which is too slow for longer sentences and for applications beyond sentence boundaries such as end-toend discourse parsing and joint sentence boundary detection and parsing. We propose a linear-time constituency parser with RNNs and dynamic progr"
P18-2076,N12-1015,1,0.929291,"r shift to push the next singleton span (j, j + 1) on the stack, or it can reduce to combine the top two spans, (k, i) and (i, j), forming the larger span (k, j). After each shift/reduce action, the top-most span is labeled as either a constituent or with a null label ∅, which means that the subsequence is not a subtree in the final decoded parse. Parsing initializes with an empty stack and continues until (0, n) is formed, representing the entire sentence. 1 state reduce • We devise a novel loss function which penalizes wrong spans that cross gold-tree spans, and employ max-violation update (Huang et al., 2012) to train this parser with structured SVM and beam search. 2.1 w0 . . . wn−1 shift • We are the first to apply cube pruning to incremental parsing, and achieves, for the first time, the complexity of O(nb log b), i.e., linear in sentence length and (almost) linear in beam size. This leads to an observed complexity strictly linear in sentence length n. 2 input (i,j,X)∈t = X (i,j)∈t max s((fj − fi ; bi − bj ), X) (2) X Note that X is a nonterminal label, a unary chain (e.g., S-VP), or null label ∅.2 In a shift-reduce setting, there are 2n − 1 steps (n shifts and n − 1 reduces) and after each ste"
P18-2076,D17-1178,0,0.349732,"Missing"
P18-2076,D13-1112,1,0.846304,"and Goldberg, 2016); words in the training set are replaced with the unknown word symbol UNK with probability punk = 1+f1(w) , with f (w) being the number of (i,j,X)∈t ∨ cross(i, j, t∗ ) Max Violation Updates Given that we maintain loss-augmented scores even for partial trees, we can perform a training update on a given example sentence by choosing to take the loss where it is the greatest along the parse trajectory. At each parsing time-step `, the violation is the difference between the highest augmented-scoring parse trajectory up to that point and the gold trajectory (Huang et al., 2012; Yu et al., 2013). Note that computing the violation gives us the max-margin loss described above. Taking the largest violation from all time-steps gives us the max-violation loss. where s∆ (·) is the loss-augmented score. If tˆ = t∗ , then all constraints are satisfied (which implies arg maxt s(t) = t∗ ), otherwise we perform an update by backpropagating from s∆ (tˆ) − s(t∗ ). 4.1 Chart Time (sec) 3000 1.5 0.0 Parsin g 3500 Cha Chart Parsing: O(n 2. 26 ) Beam 20 No Cube-Pruning: O(n 1. 26 ) Beam 20 Cube Pruned: O(n 1. 08 ) Beam 5 Cube Pruned: O(n 0. 97 ) 2.0  6 Note that the predicted tree t has exactly 2n −"
P18-2076,D17-1225,1,0.940979,"cient dynamic programming algorithm such as CKY. But existing span-based parsers suffer from a crucial limitation in terms of search: on the one hand, a greedy span parser (Cross and Huang, 2016) is fast (linear-time) but only explores one single path in the exponentially large search space, and on the other hand, a chartbased span parser (Stern et al., 2017a) performs exact search and achieves state-of-the-art accuracy, but in cubic time, which is too slow for longer sentences and for applications that go beyond sentence boundaries such as end-to-end discourse parsing (Hernault et al., 2010; Zhao and Huang, 2017) and integrated sentence boundary detection and parsing (Bj¨orkelund et al., 2016). We propose to combine the merits of both greedy and chart-based approaches and design a linear-time span-based neural parser that searches over exponentially large space. Following Huang and Sagae (2010), we perform left-to-right dynamic programming in an action-synchronous style, with (2n − 1) actions (i.e., steps) for a sentence of n words. While previous non-neural work in this area requires sophisticated features (Huang and Sagae, 2010; Mi and Huang, 2015) and thus high time complexity such as O(n11 ), our"
P19-1289,D18-1337,0,0.06727,"ranslation” model which also outputs target-side words before the whole input sentence is fed in, but there are several crucial differences: (a) their work still aims to translate full sentences using beam search, and is therefore, as the authors admit, “not a simultaneous translation model”; (b) their work does not anticipate future words; and (c) they use word alignments to learn the reordering and achieve it in decoding by emitting the  token, while our work integrates reordering into a single wait-k prediction model that is agnostic of, yet capable of, reordering. In another recent work, Alinejad et al. (2018) adds a prediction action to the work of Gu et al. (2017). Unlike Grissom II et al. (2014) who predict the source verb which might come after several words, they instead predict the immediate next source words, which we argue is not as useful in SOV-to-SVO translation. 4 In any case, we are the first to predict directly on the target side, thus integrating anticipation in a single translation model. Jaitly et al. (2016) propose an online neural transducer for speech recognition that is conditioned on prefixes. This problem does not have reorderings and thus no anticipation is needed. 8 Conclus"
P19-1289,J82-2005,0,0.638759,"Missing"
P19-1289,D14-1140,0,0.371479,"Missing"
P19-1289,K16-1010,0,0.0322637,"Missing"
P19-1289,E17-1099,0,0.472658,"hu`ıw`u. (a) Our wait-k policy (here k = 2) translates concurrently with the source sentence, but always k words behind. It correclty predicts the English verb given just the first 4 Chinese words (in bold), lit. “Bush president in Moscow”, because it is trained in a prefix-to-prefix fashion (Sec. 3), and the training data contains many prefix-pairs in the form of (X z`ai Y ..., X met ...). (c) The test-time wait-k decoding (Sec. 3.2) using the full-sentence model in (b) can not anticipate and produces nonsense translation. (d) A simultaneous translator without anticipation such as Gu et al. (2017) has to wait 5 words. (Grissom II et al., 2016), and have attempted to reduce latency by explicitly predicting the sentencefinal German (Grissom II et al., 2014) or English verbs (Matsubarayx et al., 2000), which is limited to this particular case, or unseen syntactic constituents (Oda et al., 2015; He et al., 2015), which requires incremental parsing on the source sentence. Some researchers propose to translate on an optimized sentence segment level to get better translation accuracy (Oda et al., 2014; Fujita et al., 2013; Bangalore et al., 2012). More recently, Gu et al. (2017) propose a two"
P19-1289,D15-1006,0,0.0494593,"Missing"
P19-1289,D17-1227,1,0.906619,"Missing"
P19-1289,P17-4012,0,0.0164908,"German↔English evaluation, we use newstest-2013 (dev) as our dev set and newstest-2015 (test) as our test set, with 3,000 and 2,169 sentence pairs, respectively. For Chinese↔English evaluation, we use NIST 2006 and NIST 2008 as our dev and test sets. They contain 616 and 691 Chinese sentences, each with 4 English references. When translating from Chinese to English, we report 4-reference BLEU scores, and in the reverse direction, we use the second among the four English references as the source text, and report 1-reference BLEU scores. Our implementation is adapted from PyTorchbased OpenNMT (Klein et al., 2017). Our Transformer is essentially the same as the base model from the original paper (Vaswani et al., 2017). 6.2 Quality and Latency of Wait-k Model Tab. 1 shows the results of a model trained with wait-k 0 but decoded with wait-k (where ∞ means full-sentence). Our wait-k is the diagonal, and the last row is the “test-time wait-k” decoding. Also, the best results of wait-k decoding is often from a model trained with a slightly larger k 0 . Figs. 5–8 plot translation quality (in BLEU) against latency (in AL and CW) for full-sentence baselines, our wait-k, test-time wait-k (using fullsentence mod"
P19-1289,P14-2090,0,0.0597317,"nd produces nonsense translation. (d) A simultaneous translator without anticipation such as Gu et al. (2017) has to wait 5 words. (Grissom II et al., 2016), and have attempted to reduce latency by explicitly predicting the sentencefinal German (Grissom II et al., 2014) or English verbs (Matsubarayx et al., 2000), which is limited to this particular case, or unseen syntactic constituents (Oda et al., 2015; He et al., 2015), which requires incremental parsing on the source sentence. Some researchers propose to translate on an optimized sentence segment level to get better translation accuracy (Oda et al., 2014; Fujita et al., 2013; Bangalore et al., 2012). More recently, Gu et al. (2017) propose a two-stage model whose base model is a full-sentence model, On top of that, they use a READ/WRITE (R/W) model to decide, at every step, whether to wait for another source word (READ) or to emit a target word using the pretrained base model (WRITE), and this R/W model is trained by reinforcement learning to prefer (rather than enforce) a specific latency, without updating the base model. All these efforts have the following major limitations: (a) none of them can achieve any arbitrary given latency such as"
P19-1289,P15-1020,0,0.0583849,"hion (Sec. 3), and the training data contains many prefix-pairs in the form of (X z`ai Y ..., X met ...). (c) The test-time wait-k decoding (Sec. 3.2) using the full-sentence model in (b) can not anticipate and produces nonsense translation. (d) A simultaneous translator without anticipation such as Gu et al. (2017) has to wait 5 words. (Grissom II et al., 2016), and have attempted to reduce latency by explicitly predicting the sentencefinal German (Grissom II et al., 2014) or English verbs (Matsubarayx et al., 2000), which is limited to this particular case, or unseen syntactic constituents (Oda et al., 2015; He et al., 2015), which requires incremental parsing on the source sentence. Some researchers propose to translate on an optimized sentence segment level to get better translation accuracy (Oda et al., 2014; Fujita et al., 2013; Bangalore et al., 2012). More recently, Gu et al. (2017) propose a two-stage model whose base model is a full-sentence model, On top of that, they use a READ/WRITE (R/W) model to decide, at every step, whether to wait for another source word (READ) or to emit a target word using the pretrained base model (WRITE), and this R/W model is trained by reinforcement learnin"
P19-1289,D18-1342,1,0.898779,"Missing"
P19-1289,P19-1582,1,0.895955,"Missing"
P19-1291,P18-1163,0,0.124846,"0.95, with and without data augmentation. Not surprisingly, data augmentation significantly improves the robustness of NMT models to homophone noises. However, the noises in training data seem to hurt the performance of the baseline model (from 45.97 to 43.94), and its effect on our model seems to be much smaller, probably because our model mainly uses the phonetic information. 4 Related Work Formiga and Fonollosa (2012) proposed to use a character-level translator to deal with misspelled words in the input sentences, but in general their method cannot deal with homophone noises effectively. Cheng et al. (2018) proposed to use adversarial stability training to improve the robustness of NMT systems, but their method does not specifically target homophone noises and do not use phonetic information. The effects of ASR errors on machine translation have been extensively analyzed (Ruiz et al., 2017; Ruiz and Federico, 2015). In a parallel work, Li et al. (2018) also proposed to utilize both textual and phonetic information to improve the robustness of NMT systems, but their method is different with ours in how textual and phonetic information are combined. 5 Conclusion In this paper, we propose to use bo"
P19-1291,C12-2032,0,0.0323855,"without data augmentation. tence pairs in the training set, resulting in a training dataset with about 2.8M sentence pairs. In Table 4, we report the performance of baseline model and our model with β = 0.95, with and without data augmentation. Not surprisingly, data augmentation significantly improves the robustness of NMT models to homophone noises. However, the noises in training data seem to hurt the performance of the baseline model (from 45.97 to 43.94), and its effect on our model seems to be much smaller, probably because our model mainly uses the phonetic information. 4 Related Work Formiga and Fonollosa (2012) proposed to use a character-level translator to deal with misspelled words in the input sentences, but in general their method cannot deal with homophone noises effectively. Cheng et al. (2018) proposed to use adversarial stability training to improve the robustness of NMT systems, but their method does not specifically target homophone noises and do not use phonetic information. The effects of ASR errors on machine translation have been extensively analyzed (Ruiz et al., 2017; Ruiz and Federico, 2015). In a parallel work, Li et al. (2018) also proposed to utilize both textual and phonetic in"
P19-1291,P16-1162,0,0.141296,"n LSTM network to merge them; however, we did not see obvious improvements in translation quality. 4 For simplicity reasons, tone information is discarded. 3045 or subwords. Note that when there are multiple pronunciations, we just randomly pick one in both training and testing. For symbols or entries without pronunciation, we use a special pronunciation unit, hunki, to represent them. 3.3 Translation Results For the dataset, we use an extended NIST corpus which consists of 2M sentence pairs with about 51M Mandarin words and 62M English words, respectively. We apply byte-pair encodings (BPE) (Sennrich et al., 2016) on both Mandarin and English sides to reduce the vocabulary size down to 18K and 10K, respectively. Sentences longer than 256 subwords or words are excluded. 50 48 46 BLEU Score 44 42 40 38 Baseline = 0.2 = 0.4 = 0.6 = 0.8 = 0.95 = 1.0 36 34 32 10000 20000 30000 40000 50000 Iteration 60000 70000 80000 90000 Figure 1: BLEU scores on the dev set for the baseline model (Transformer-base) and our models with different β. The x-axis is the number of iterations and the y-axis in the case-insensitive BLEU scores on multiple references. In Figure 1, we compare the performances, measured by BLEU score"
P19-1291,D15-1166,0,0.0606929,") augmenting the training dataset with homophone noises. Interestingly, to achieve better translation quality and more robustness, we found that most (though not all) weights should be put on the phonetic rather than textual information. Experiments show that our method not only significantly improves the robustness of NMT to homophone noises, but also surprisingly improves the translation quality on some clean test sets. 1 Introduction Recently we witnessed tremendous progresses in the field of neural machine translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Gehring et al., 2017), especially the birth of transformer network (Vaswani et al., 2017). Despite tremendous success, NMT models are very sensitive to the noises in input sentences (Belinkov and Bisk, 2017). The causes of such vulnerability are multifold, and some of them are: 1) neural networks are inherently sensitive to noises, such as adversarial examples (Goodfellow et al., 2014; Szegedy et al., 2013), 2) every input word can affect every output word generated by the decoder due to the global effects of attention, and 3) all NMT models have an input embedding layer, which is sensitive"
P19-1291,P02-1040,0,0.108744,"ments, is a very large β close to but not 1. 3 3.1 Experiments Models In our experiments, we use Transformer as baseline. Specifically, we use the PyTorch version (PyTorch 0.4.0) of OpenNMT. All models are trained with 8 GPUs, and the values of important parameters are: 6 layers, 8 heads attention, 2048 neurons in feed-forward layer, and 512 neurons in other layers, dropout rate is 0.1, label smoothing rate is 0.1, Adam optimizer, learning rate is 2 with NOAM decay. 3.2 Translation Tasks We evaluate our method on the translation task of Mandarin to English, and reported the 4-gram BLEU score (Papineni et al., 2002) as calculated by multi-bleu.perl. Pinyin is used as pronunciation units (Du and Way, 2017; Yang et al., 2018), and there are 404 types of pinyin syllables in total 4 . A large Mandarin lexicon is used. For words or subwords not in the lexicon, if all of their characters have pinyins, the concatenation of these characters’s pinyins are used as the pinyin of the whole words 3 We tried other approaches, such as using an LSTM network to merge them; however, we did not see obvious improvements in translation quality. 4 For simplicity reasons, tone information is discarded. 3045 or subwords. Note t"
P19-1582,D16-1001,1,0.737528,"ation process. So we propose to remove those delay token in the attention layers except for the current input one. However, this removal may reduce the explicit latency information which will affect the predictions of the model since the model cannot observe previous output delay tokens. Therefore, to provide this information explicitly, we embed the number of previous delay tokens to a vector and add this to the sum of the word embedding and position embedding as the input of the decoder. 4 4.1 Methods Training via Restricted Imitation Learning We first introduce a restricted dynamic oracle (Cross and Huang, 2016) based on our extended vocabulary. Then we show how to use this dynamic oracle to train a simultaneous translation model via imitation learning. Note that we do not need to train this oracle. Restricted Dynamic Oracle Given a pair of full sequences (x, y) in data, the input state of our restricted dynamic oracle will be a pair of prefixes (s, t) where s  x, t  y and (s, t) 6= (x, y). The whole action set is V+ defined in the last section. The objective of our dynamic oracle is to obtain the full sequence pair (x, y) and maintain a reasonably low latency. For a prefix pair (s, t), the differe"
P19-1582,N18-2079,0,0.163481,"vocabulary, and if the model emits this delay token, it will read one source word; the second idea is to train the model using (restricted) imitation learning by designing a (restricted) dynamic oracle as the expert policy. Table 2 summarizes different approaches for simultaneous translation using neural machine translation (NMT) model. 5816 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5816–5822 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics fixed policy adaptive policy seq-to-seq static Read-Write (Dalvi et al., 2018) test-time wait-k (Ma et al., 2018) RL (Gu et al., 2017) prefix-to-prefix wait-k (Ma et al., 2018) imitation learning (this work) Table 2: Different approaches for simultaneous translation. 2 Preliminaries Let x = (x1 , . . . , xn ) be a sequence of words. For an integer 0 ≤ i ≤ n, we denote the sequence consisting of the first consecutive i − 1 words in x by x<i = (x1 , . . . , xi−1 ). We say such a sequence x<i is a prefix of the sequence x, and define s  x if sequence s is a prefix of x. Conventional Machine Translation Given a sequence x from the source language, the conventional machine"
P19-1582,D14-1140,0,0.437545,"Missing"
P19-1582,E17-1099,0,0.486939,"f the holy grails of AI (Grissom II et al., 2014). A major challenge in simultaneous translation is the word order difference between the source and target languages, e.g., between SOV languages (German, Japanese, etc.) and SVO languages (English, Chinese, etc.). Simultaneous translation is previously studied as a part of real-time speech recognition system (Yarmohammadi et al., 2013; Bangalore et al., 2012; F¨ugen et al., 2007; Sridhar et al., 2013; Jaitly et al., 2016; Graves et al., 2013). Recently, there have been two encouraging efforts in this problem with promising but limited success. Gu et al. (2017) propose a complicated two-stage model that is also trained in two stages. The base model, responsible for producing target words, is a conventional full-sentence seq2seq model, and on top of that, the READ/WRITE (R/W) model decides, at every step, whether to wait for another source word (READ) or to emit a target word ∗ These authors contributed equally. zrenj11@gmail.com (WRITE) using the pretrained base model. This R/W model is trained by reinforcement learning (RL) method without updating the base model. Ma et al. (2018), on the other hand, propose a much simpler architecture, which only n"
P19-1582,D17-1227,1,0.857638,"cle is restricted in the sense that it is only defined on the prefix pair instead of any sequence pair. And since we only want to obtain the exact sequence from data, this oracle can only choose the next groundtruth target word other than hεi. In many cases, the assumption |x |= |y |does not hold. To overcome this limitation, we can utilize the length ratio γ = |x|/|y |to modify the length difference: d0 = |s |− γ|t|, and use this new difference d0 in our dynamic oracle. Although we cannot obtain this ratio during testing time, we may use the averaged length ratio obtained from training data (Huang et al., 2017). Training with Restricted Dynamic Oracle We apply imitation learning to train our translation model, using the proposed dynamic oracle as the expert policy. Recall that the prediction of our model depends on the whole generated prefix including hεi (as the input contains the embedding of the number of hεi), which is also an action sequence. If an action sequence a is obtained from our oracle, then applying this sequence will result in a prefix pair, say sa and ta , of x and y. Let p(a |sa , ta ) be the probability of choosing action a given the prefix pair obtained by applying action sequence"
P19-1582,P17-4012,0,0.0347514,"e Experiments To investigate the empirical performance of our proposed method, we conduct experiments on NIST corpus for Chinese-English. We use NIST 06 (616 sentence pairs) as our development set and NIST 08 (691 sentence pairs) as our testing set. We apply tokenization and byte-pair encoding (BPE) (Sennrich et al., 2015) on both source and target languages to reduce their vocabularies. For training data, we only include 1 million sentence pairs with length larger than 50. We use Transformer (Vaswani et al., 2017) as our NMT model, and our implementation is adapted from PyTorchbased OpenNMT (Klein et al., 2017). The architecture of our Transformer model is the same as the base model in the original paper. We use BLEU (Papineni et al., 2002) as the translation quality metric and Average Lagging (AL) introduced by Ma et al. (2018) as our latency metrics, which measures the average delayed words. AL avoids some limitations of other existing metrics, such as insensitivity to actual lagging like Consecutive Wait (CW) (Gu et al., 2017), and sensitivity to input length like Average Proportion (AP) (Cho and Esipova, 2016) . Results We tried three different pairs for α and β: (1, 5), (3, 5) and (3, 7), and s"
P19-1582,P17-2053,1,0.817634,"tight. Let aα(x,y) (aβ(x,y) ) be such an action sequence for (x, y) and α (β). We replace S(x, y) with {aα(x,y) , aβ(x,y) }, then the above loss for dataset D becomes P `(aα(x,y) |x,y)+`(aβ(x,y) |x,y) `α,β (D) = . 2 (x,y)∈D This is the loss we use in our training process. Note that there are some steps where our oracle will return two actions, so for such steps we will have a multi-label classification problem where labels are the actions from our oracle. In such cases, Sigmoid function for each action is more appropriate than the Softmax function for the actions will not compete each other (Ma et al., 2017; Zheng et al., 2018; Ma et al., 2019). Therefore, we apply Sigmoid for each action instead of using Softmax function to generate a distribution for all actions. 4.2 designated band in prefix grid. To avoid such case, we force the model to choose actions such that it will always satisfy the latency constraints. That is, if the model reaches the aggressive bound, it must choose a target word other than hεi with highest score, even if hεi has higher score; if the model reaches the conservative bound, it can only choose hεi at that step. We also apply a temperature constant et to the score of hεi"
P19-1582,N19-1187,1,0.835752,"h an action sequence for (x, y) and α (β). We replace S(x, y) with {aα(x,y) , aβ(x,y) }, then the above loss for dataset D becomes P `(aα(x,y) |x,y)+`(aβ(x,y) |x,y) `α,β (D) = . 2 (x,y)∈D This is the loss we use in our training process. Note that there are some steps where our oracle will return two actions, so for such steps we will have a multi-label classification problem where labels are the actions from our oracle. In such cases, Sigmoid function for each action is more appropriate than the Softmax function for the actions will not compete each other (Ma et al., 2017; Zheng et al., 2018; Ma et al., 2019). Therefore, we apply Sigmoid for each action instead of using Softmax function to generate a distribution for all actions. 4.2 designated band in prefix grid. To avoid such case, we force the model to choose actions such that it will always satisfy the latency constraints. That is, if the model reaches the aggressive bound, it must choose a target word other than hεi with highest score, even if hεi has higher score; if the model reaches the conservative bound, it can only choose hεi at that step. We also apply a temperature constant et to the score of hεi, which can implicitly control the lat"
P19-1582,P02-1040,0,0.110539,"English. We use NIST 06 (616 sentence pairs) as our development set and NIST 08 (691 sentence pairs) as our testing set. We apply tokenization and byte-pair encoding (BPE) (Sennrich et al., 2015) on both source and target languages to reduce their vocabularies. For training data, we only include 1 million sentence pairs with length larger than 50. We use Transformer (Vaswani et al., 2017) as our NMT model, and our implementation is adapted from PyTorchbased OpenNMT (Klein et al., 2017). The architecture of our Transformer model is the same as the base model in the original paper. We use BLEU (Papineni et al., 2002) as the translation quality metric and Average Lagging (AL) introduced by Ma et al. (2018) as our latency metrics, which measures the average delayed words. AL avoids some limitations of other existing metrics, such as insensitivity to actual lagging like Consecutive Wait (CW) (Gu et al., 2017), and sensitivity to input length like Average Proportion (AP) (Cho and Esipova, 2016) . Results We tried three different pairs for α and β: (1, 5), (3, 5) and (3, 7), and summarize the results on testing sets in Figure 2. Figure 2 (a) shows the results on Chinese-to-English translation. In this directio"
P19-1582,N13-1023,0,0.132745,"ul in many scenarios such as international conferences, summits, and negotiations. However, it is widely considered one of the most challenging tasks in NLP, and one of the holy grails of AI (Grissom II et al., 2014). A major challenge in simultaneous translation is the word order difference between the source and target languages, e.g., between SOV languages (German, Japanese, etc.) and SVO languages (English, Chinese, etc.). Simultaneous translation is previously studied as a part of real-time speech recognition system (Yarmohammadi et al., 2013; Bangalore et al., 2012; F¨ugen et al., 2007; Sridhar et al., 2013; Jaitly et al., 2016; Graves et al., 2013). Recently, there have been two encouraging efforts in this problem with promising but limited success. Gu et al. (2017) propose a complicated two-stage model that is also trained in two stages. The base model, responsible for producing target words, is a conventional full-sentence seq2seq model, and on top of that, the READ/WRITE (R/W) model decides, at every step, whether to wait for another source word (READ) or to emit a target word ∗ These authors contributed equally. zrenj11@gmail.com (WRITE) using the pretrained base model. This R/W model is tr"
P19-1582,D18-1357,1,0.914273,"Missing"
P19-1582,I13-1141,0,\N,Missing
P19-1582,P16-1162,0,\N,Missing
W05-1506,J04-4004,0,0.471205,"t (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class. In our experiments, we apply the algorithms to the lexicalized PCFG parser of Bikel (2004), which is very similar to Collins’ Model 2 (Collins, 2003), and to a synchronous CFG based machine translation system (Chiang, 2005). 2 Previous Work As pointed out by Charniak and Johnson (2005), the major difficulty in k-best parsing is dynamic programming. The simplest method is to abandon dynamic programming and rely on aggressive pruning to maintain tractability, as is used in (Collins, 2000; Bikel, 2004). But this approach is prohibitively slow, and produces rather lowquality k-best lists (see Sec. 5.1.2). Gildea and Jurafsky (2002) described an O(k2 )-overhead extension for the CKY alg"
W05-1506,C00-1011,0,0.0172235,"l the metric will not be compatible with the parsing algorithm, the k-best lists can be used to approximate the full distribution f 0 . A similar situation occurs when the parser can produce multiple derivations that are regarded as equivalent (e.g., multiple lexicalized parse trees corresponding to the same unlexicalized parse tree); if we want the maximum a posteriori parse, we have to sum over equivalent derivations. Again, the equivalence relation will in general not be compatible with the parsing algorithm, so the k-best lists can be used to approximate f 0 , as in Data Oriented Parsing (Bod, 2000) and in speech recognition (Mohri and Riley, 2002). Another instance of this k-best approach is cascaded optimization. NLP systems are often cascades of modules, where we want to optimize the modules’ objective functions jointly. However, often a module is incompatible with the packed representation of the previous module due to factors like non-local dependencies. So we might want to postpone some disambiguation by propagating k-best lists to subsequent phases, as in joint parsing and semantic role labeling (Gildea and Jurafsky, 2002; Sutton and McCallum, 2005), information extraction and cor"
W05-1506,A00-2018,0,0.399259,"lgorithm turns out to be a special case of our Algorithm 0 (Sec. 4.1), and is reported to also be prohibitively slow. Since the original design of the algorithm described below, we have become aware of two efforts that are very closely related to ours, one by Jim´enez and Marzal (2000) and another done in parallel to ours by Charniak and Johnson (2005). Jim´enez and Marzal present an algorithm very similar to our Algorithm 3 (Sec. 4.4) while Charniak and Johnson propose using an algorithm similar to our Algorithm 0, but with multiple passes to improve efficiency. They apply this method to the Charniak (2000) parser to get 50-best lists for reranking, yielding an improvement in parsing accuracy. Our work differs from Jim´enez and Marzal’s in the following three respects. First, we formulate the parsing problem in the more general framework of hypergraphs (Klein and Manning, 2001), making it applicable to a very wide variety of parsing algorithms, whereas Jim´enez and Marzal define their algorithm as an extension of CKY, for CFGs in Chomsky Normal Form (CNF) only. This generalization is not only of theoretical importance, but also critical in the application to state-of-theart parsers such as (Coll"
W05-1506,P05-1033,1,0.45832,"solution (Wellner et al., 2004), and formal semantics of TAG (Joshi and Vijay-Shanker, 1999). Moreover, much recent work on discriminative training uses k-best lists; they are sometimes used to approximate the normalization constant or partition function (which would otherwise be intractable), or to train a model by optimizing some metric incompatible with the packed representation. For example, Och (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score (among other metrics) of the model on the data. Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. For algorithms whose packed representations are graphs, such as Hidden Markov Models and other finitestate methods, Ratnaparkhi’s MXPARSE parser (Ratnaparkhi, 1997), and many stack-based machine translation decoders (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Ri"
W05-1506,J03-4003,0,0.838742,"for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class. In our experiments, we apply the algorithms to the lexicalized PCFG parser of Bikel (2004), which is very similar to Collins’ Model 2 (Collins, 2003), and to a synchronous CFG based machine translation system (Chiang, 2005). 2 Previous Work As pointed out by Charniak and Johnson (2005), the major difficulty in k-best parsing is dynamic programming. The simplest method is to abandon dynamic programming and rely on aggressive pruning to maintain tractability, as is used in (Collins, 2000; Bikel, 2004). But this approach is prohibitively slow, and produces rather lowquality k-best lists (see Sec. 5.1.2). Gildea and Jurafsky (2002) described an O(k2 )-overhead extension for the CKY algorithm and reimplemented Collins’ Model 1 to obtain k-best"
W05-1506,W01-1812,0,0.700021,"maximize BLEU. For algorithms whose packed representations are graphs, such as Hidden Markov Models and other finitestate methods, Ratnaparkhi’s MXPARSE parser (Ratnaparkhi, 1997), and many stack-based machine translation decoders (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class. In our experiments, we apply the algorithms to the lexicalized PCFG parser of Bikel (2004), which is very similar to Collins’ Model 2 (Collins, 2003), and to a synchronous CFG based machine translation system (Chiang, 2005). 2 Previous Work As pointed out by Charniak and Johnson (2005), the major difficulty in k-best parsing is dynamic programming. The si"
W05-1506,N04-1022,0,0.0508739,"ter Studies University of Maryland 3161 AV Williams College Park, MD 20742 dchiang@umiacs.umd.edu f 0 to produce a k-best list (the top k candidates under f 0 ), which serves as an approximation to the full set. Then, in the second phase, optimize f over all the analyses in the k-best list. A typical example is discriminative reranking on k-best lists from a generative module, such as (Collins, 2000) for parsing and (Shen et al., 2004) for translation, where the reranking model has nonlocal features that cannot be computed during parsing proper. Another example is minimum-Bayes-risk decoding (Kumar and Byrne, 2004; Goodman, 1998),where, assuming f 0 defines a probability distribution over all candidates, one seeks the candidate with the highest expected score according to an arbitrary metric (e.g., PARSEVAL or BLEU); since in general the metric will not be compatible with the parsing algorithm, the k-best lists can be used to approximate the full distribution f 0 . A similar situation occurs when the parser can produce multiple derivations that are regarded as equivalent (e.g., multiple lexicalized parse trees corresponding to the same unlexicalized parse tree); if we want the maximum a posteriori pars"
W05-1506,J93-2004,0,0.0295307,"wer 10−3 beam and further applied a cell limit of 100,7 but, as we will show below, this has a detrimental effect on the quality of the output. We therefore omit this method from our speed comparisons, and use our implementation of Algorithm 0 (na¨ıve) as the baseline. We implemented our k-best Algorithms 0, 1, and 3 on top of Bikel’s parser and conducted experiments on a 2.4 GHz 64-bit AMD Opteron with 32 GB memory. The program is written in Java 1.5 running on the Sun JVM in server mode with a maximum heap size of 5 GB. For this experiment, we used sections 02–21 of the Penn Treebank (PTB) (Marcus et al., 1993) as the training data and section 23 (2416 sentences) for evaluation, as is now standard. We ran Bikel’s parser using its settings to emulate Model 2 of (Collins, 2003). 5.1.1 Efficiency We tested our algorithms under various conditions. We first did a comparison of the average parsing time per 6 In beam search, or threshold pruning, each cell in the chart (typically containing all the items corresponding to a span [i, j]) is reduced by discarding all items that are worse than β times the score of the best item in the cell. This β is known as the beam width. 7 In this type of pruning, also kno"
W05-1506,P05-1012,0,0.429744,"shortest hyperpath problem and Nielsen et al. (2005) extend it to k shortest hyperpath. Our work differes from (Nielsen et al., 2005) in two aspects. First, we solve the problem of k-best derivations (i.e., trees), not the k-best hyperpaths, although in many cases they coincide (see Sec. 3 for further discussions). Second, their work assumes non-negative costs (or probabilities ≤ 1) so that they can apply Dijkstra-like algorithms. Although generative models, being probabilitybased, do not suffer from this problem, more general models (e.g., log-linear models) may require negative edge costs (McDonald et al., 2005; Taskar et al., 2004). Our work, based on the Viterbi algorithm, is still applicable as long as the hypergraph is acyclic, and is used by McDonald et al. (2005) to get the k-best parses. 3 Formulation Following Klein and Manning (2001), we use weighted directed hypergraphs (Gallo et al., 1993) as an abstraction of the probabilistic parsing problem. Definition 1. An ordered hypergraph (henceforth hypergraph) H is a tuple hV, E, t, Ri, where V is a finite set of vertices, E is a finite set of hyperarcs, and R is the set of weights. Each hyperarc e ∈ E is a triple e = hT (e), h(e), f (e)i, where"
W05-1506,J02-3001,0,0.186032,"k-best lists can be used to approximate f 0 , as in Data Oriented Parsing (Bod, 2000) and in speech recognition (Mohri and Riley, 2002). Another instance of this k-best approach is cascaded optimization. NLP systems are often cascades of modules, where we want to optimize the modules’ objective functions jointly. However, often a module is incompatible with the packed representation of the previous module due to factors like non-local dependencies. So we might want to postpone some disambiguation by propagating k-best lists to subsequent phases, as in joint parsing and semantic role labeling (Gildea and Jurafsky, 2002; Sutton and McCallum, 2005), information extraction and coreference resolution (Wellner et al., 2004), and formal semantics of TAG (Joshi and Vijay-Shanker, 1999). Moreover, much recent work on discriminative training uses k-best lists; they are sometimes used to approximate the normalization constant or partition function (which would otherwise be intractable), or to train a model by optimizing some metric incompatible with the packed representation. For example, Och (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the"
W05-1506,J99-4004,0,0.543574,"anslation decoders (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class. In our experiments, we apply the algorithms to the lexicalized PCFG parser of Bikel (2004), which is very similar to Collins’ Model 2 (Collins, 2003), and to a synchronous CFG based machine translation system (Chiang, 2005). 2 Previous Work As pointed out by Charniak and Johnson (2005), the major difficulty in k-best parsing is dynamic programming. The simplest method is to abandon dynamic programming and rely on aggressive pruning to maintain tractability, as is used in (Collins, 2000; Bikel, 2004). But this approach is prohibitively slow, and produces"
W05-1506,J03-1006,0,0.628235,"ers (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class. In our experiments, we apply the algorithms to the lexicalized PCFG parser of Bikel (2004), which is very similar to Collins’ Model 2 (Collins, 2003), and to a synchronous CFG based machine translation system (Chiang, 2005). 2 Previous Work As pointed out by Charniak and Johnson (2005), the major difficulty in k-best parsing is dynamic programming. The simplest method is to abandon dynamic programming and rely on aggressive pruning to maintain tractability, as is used in (Collins, 2000; Bikel, 2004). But this approach is prohibitively slow, and produces rather lowqualit"
W05-1506,P03-1021,0,0.147152,"uation by propagating k-best lists to subsequent phases, as in joint parsing and semantic role labeling (Gildea and Jurafsky, 2002; Sutton and McCallum, 2005), information extraction and coreference resolution (Wellner et al., 2004), and formal semantics of TAG (Joshi and Vijay-Shanker, 1999). Moreover, much recent work on discriminative training uses k-best lists; they are sometimes used to approximate the normalization constant or partition function (which would otherwise be intractable), or to train a model by optimizing some metric incompatible with the packed representation. For example, Och (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score (among other metrics) of the model on the data. Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. For algorithms whose packed representations are graphs, such as Hidden Markov Models and other finitestate methods, Ratnaparkhi’s MXPARSE parser (Ratnaparkhi, 1997), and many stack-based machine translation decoders (Brown et al., 1995; Och and N"
W05-1506,J04-4002,0,0.0505172,"ch (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score (among other metrics) of the model on the data. Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. For algorithms whose packed representations are graphs, such as Hidden Markov Models and other finitestate methods, Ratnaparkhi’s MXPARSE parser (Ratnaparkhi, 1997), and many stack-based machine translation decoders (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class. In o"
W05-1506,W97-0301,0,0.285053,"model by optimizing some metric incompatible with the packed representation. For example, Och (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score (among other metrics) of the model on the data. Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. For algorithms whose packed representations are graphs, such as Hidden Markov Models and other finitestate methods, Ratnaparkhi’s MXPARSE parser (Ratnaparkhi, 1997), and many stack-based machine translation decoders (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deducti"
W05-1506,N04-1023,0,0.0458131,"avid A. Smith and Jonathan May for pointing it out. In the actual implementions, an earlier version is used which has the correct behavior. ∗ David Chiang Inst. for Advanced Computer Studies University of Maryland 3161 AV Williams College Park, MD 20742 dchiang@umiacs.umd.edu f 0 to produce a k-best list (the top k candidates under f 0 ), which serves as an approximation to the full set. Then, in the second phase, optimize f over all the analyses in the k-best list. A typical example is discriminative reranking on k-best lists from a generative module, such as (Collins, 2000) for parsing and (Shen et al., 2004) for translation, where the reranking model has nonlocal features that cannot be computed during parsing proper. Another example is minimum-Bayes-risk decoding (Kumar and Byrne, 2004; Goodman, 1998),where, assuming f 0 defines a probability distribution over all candidates, one seeks the candidate with the highest expected score according to an arbitrary metric (e.g., PARSEVAL or BLEU); since in general the metric will not be compatible with the parsing algorithm, the k-best lists can be used to approximate the full distribution f 0 . A similar situation occurs when the parser can produce mult"
W05-1506,W05-0636,0,0.0227026,"approximate f 0 , as in Data Oriented Parsing (Bod, 2000) and in speech recognition (Mohri and Riley, 2002). Another instance of this k-best approach is cascaded optimization. NLP systems are often cascades of modules, where we want to optimize the modules’ objective functions jointly. However, often a module is incompatible with the packed representation of the previous module due to factors like non-local dependencies. So we might want to postpone some disambiguation by propagating k-best lists to subsequent phases, as in joint parsing and semantic role labeling (Gildea and Jurafsky, 2002; Sutton and McCallum, 2005), information extraction and coreference resolution (Wellner et al., 2004), and formal semantics of TAG (Joshi and Vijay-Shanker, 1999). Moreover, much recent work on discriminative training uses k-best lists; they are sometimes used to approximate the normalization constant or partition function (which would otherwise be intractable), or to train a model by optimizing some metric incompatible with the packed representation. For example, Och (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score (among other met"
W05-1506,W04-3201,0,0.0297109,"blem and Nielsen et al. (2005) extend it to k shortest hyperpath. Our work differes from (Nielsen et al., 2005) in two aspects. First, we solve the problem of k-best derivations (i.e., trees), not the k-best hyperpaths, although in many cases they coincide (see Sec. 3 for further discussions). Second, their work assumes non-negative costs (or probabilities ≤ 1) so that they can apply Dijkstra-like algorithms. Although generative models, being probabilitybased, do not suffer from this problem, more general models (e.g., log-linear models) may require negative edge costs (McDonald et al., 2005; Taskar et al., 2004). Our work, based on the Viterbi algorithm, is still applicable as long as the hypergraph is acyclic, and is used by McDonald et al. (2005) to get the k-best parses. 3 Formulation Following Klein and Manning (2001), we use weighted directed hypergraphs (Gallo et al., 1993) as an abstraction of the probabilistic parsing problem. Definition 1. An ordered hypergraph (henceforth hypergraph) H is a tuple hV, E, t, Ri, where V is a finite set of vertices, E is a finite set of hyperarcs, and R is the set of weights. Each hyperarc e ∈ E is a triple e = hT (e), h(e), f (e)i, where h(e) ∈ V is its head"
W05-1506,P05-1022,0,\N,Missing
W05-1507,P97-1003,0,0.0620211,"m number of interacting variables is 4, implying that the algorithmic complexity is O(n4 ) after binarizing the factors cleverly. The intermediate result max [β(B[i, k, h0 ]) · P (A[h] → B[h0 ]C[h])] 0 h ,B A C[h] k can be represented pictorially as i . The same trick works for the second max term in Equation 1. The intermediate result coming from binarizing the second term can be visualized as A[h] → B[h]C[h0 ] or A[h] → B[h0 ]C[h] A depending on which child is the head child that agrees with the parent on head word selection. Bilexical CFG is at the heart of most modern statistical parsers (Collins, 1997; Charniak, 1997), because the statistics associated with word-specific rules are more informative for disambiguation purposes. If we use A[i, j, h] to represent a lexicalized constituent, β(·) to represent the Viterbi score function applicable to any constituent, and P (·) to represent the rule probability function applicable to any rule, Figure 2 shows the equation for the dynamic programming computation of the Viterbi parse. The two terms of the outermost max operator are symmetric cases for heads coming from left and right. Containing five free variables i,j,k,h0 ,h, ranging over 1 to n, t"
W05-1507,P99-1059,0,0.0890971,"Bilexical Parsing A traditional CFG generates words at the bottom of a parse tree and uses nonterminals as abstract representations of substrings to build higher level tree nodes. Nonterminals can be made more specific to the actual substrings they are covering by associating a representative word from the nonterminal’s yield. When the maximum number of lexicalized nonterminals in any rule is two, a CFG is bilexical. A typical bilexical CFG in Chomsky normal form has two types of rule templates: instantiated in n5 possible ways, implying that the complexity of the parsing algorithm is O(n5 ). Eisner and Satta (1999) pointed out we don’t have to enumerate k and h0 simultaneously. The trick, shown in mathematical form in Figure 2 (bottom) is very simple. When maximizing over h0 , j is irrelevant. After getting the intermediate result of maximizing over h0 , we have one less free variable than before. Throughout the two steps, the maximum number of interacting variables is 4, implying that the algorithmic complexity is O(n4 ) after binarizing the factors cleverly. The intermediate result max [β(B[i, k, h0 ]) · P (A[h] → B[h0 ]C[h])] 0 h ,B A C[h] k can be represented pictorially as i . The same trick works"
W05-1507,N03-1021,0,0.0689172,"se A[i, j, h] to represent a lexicalized constituent, β(·) to represent the Viterbi score function applicable to any constituent, and P (·) to represent the rule probability function applicable to any rule, Figure 2 shows the equation for the dynamic programming computation of the Viterbi parse. The two terms of the outermost max operator are symmetric cases for heads coming from left and right. Containing five free variables i,j,k,h0 ,h, ranging over 1 to n, the length of input sentence, both terms can be 67 B[h] k j. The shape of the intermediate results gave rise to the nickname of “hook”. Melamed (2003) discussed the applicability of the hook trick for parsing bilexical multitext grammars. The analysis of the hook trick in this section shows that it is essentially an algebraic manipulation. We will formulate the ITG Viterbi decoding algorithm in a dynamic programming equation in the following section and apply the same algebraic manipulation to produce hooks that are suitable for ITG decoding. 4 Hook Trick for ITG Decoding We start from the bigram case, in which each decoding constituent keeps a left boundary word and X X [Y u11 u12 &lt;Y Z] v11 v12 u21 u22 s S u21 u22 v21 v22 t Z> v21 v22 u11"
W05-1507,P96-1021,0,0.532503,"g the chart with an item for each possible translation of each foreign word in f , and then applying ITG rules from the bottom up. However, ITG’s independence assumptions are too strong to use the ITG probability alone for machine translation. In particular, the context-free assumption that each foreign word’s translation is chosen independently will lead to simply choosing each foreign word’s single most probable English translation with no reordering. In practice it is beneficial to combine the probability given by ITG with a local m-gram language model for English: and q e 2.1 ITG Decoding Wu (1996) presented a polynomial-time algorithm for decoding ITG combined with an m-gram lan66 e∗ = argmax max P (e, f, q)Plm (e)α e q with some constant language model weight α. The language model will lead to more fluent output by influencing both the choice of English words and the reordering, through the choice of straight or inverted rules. While the use of a language model complicates the CKY-based algorithm for finding the best translation, a dynamic programming solution is still possible. We extend the algorithm by storing in each chart item the English boundary words that will affect the m-gra"
W05-1507,J97-3002,0,0.829471,"en translation and monolingual parsing with lexicalized grammars. Chart items in translation must be augmented with words from the output language in order to capture language model state. This can be thought of as a form of lexicalization with some similarity to that of head-driven lexicalized grammars, despite being unrelated to any notion of syntactic head. We show 1 We speak of m-gram language models to avoid confusion with n, which here is the length of the input sentence for translation. Machine Translation using Inversion Transduction Grammar The Inversion Transduction Grammar (ITG) of Wu (1997) is a type of context-free grammar (CFG) for generating two languages synchronously. To model the translational equivalence within a sentence pair, ITG employs a synchronous rewriting mechanism to relate two sentences recursively. To deal with the syntactic divergence between two languages, ITG allows the inversion of rewriting order going from one language to another at any recursive level. ITG in Chomsky normal form consists of unary production rules that are responsible for generating word pairs: X → e/f X → e/ X → /f where e is a source language word, f is a foreign language word, and"
W06-3601,N03-1017,0,0.00595915,"Missing"
W06-3601,J00-1004,0,0.0735197,"Missing"
W06-3601,koen-2004-pharaoh,0,0.0407508,"Missing"
W06-3601,J93-2003,0,0.0121903,"Missing"
W06-3601,A00-2018,0,0.0653872,"Missing"
W06-3601,P05-1033,0,0.0367601,"Missing"
W06-3601,N03-1019,0,0.0196045,"Missing"
W06-3601,C04-1090,0,0.0934174,"Missing"
W06-3601,N06-1045,1,0.849681,"Missing"
W06-3601,P05-1067,0,0.185749,"Missing"
W06-3601,P02-1038,0,0.0734509,"Missing"
W06-3601,P03-2041,0,0.0809973,"Missing"
W06-3601,J04-4002,0,0.0561881,"Missing"
W06-3601,W02-1039,0,0.136926,"Missing"
W06-3601,N04-1035,1,0.402231,"Missing"
W06-3601,P03-1021,0,0.00859701,"Missing"
W06-3601,P05-1034,0,0.293239,"Missing"
W06-3601,C90-3045,0,0.289084,"Missing"
W06-3601,N04-1014,1,0.880617,"Missing"
W06-3601,W05-1506,1,0.399291,"Missing"
W06-3601,J97-3002,0,0.0826936,"Missing"
W06-3601,P01-1067,1,0.837173,"Missing"
W06-3601,J03-4003,0,\N,Missing
W06-3601,J08-3004,1,\N,Missing
W06-3601,W90-0102,0,\N,Missing
W07-0405,P05-1033,0,0.553461,"Street, Levine Hall Philadelphia, PA 19104 lhuang3@cis.upenn.edu Abstract Binarization is essential for achieving polynomial time complexities in parsing and syntax-based machine translation. This paper presents a new binarization scheme, target-side binarization, and compares it with source-side and synchronous binarizations on both stringbased and tree-based systems using synchronous grammars. In particular, we demonstrate the effectiveness of targetside binarization on a large-scale tree-tostring translation system. 1 Introduction Several recent syntax-based models for machine translation (Chiang, 2005; Galley et al., 2006) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of a grammar rule. To alleviate this problem, one can borrow from parsing the technique of binarizing context-free grammars (into Chomsky Normal Form) to reduce the complexity. With synchronous context-free grammars (SCFG), however, this problem becomes more complicated with the additional dimension of target-side pe"
W07-0405,J07-2003,0,0.0949756,"items, plus a combination cost which is the negative log probability of the bigrams formed in combining adjacent boundary words of antecedents. Now that we keep track of target-side boundary words, an additional complexity, called target-side complexity, is introduced. In Deduction (11), four target words are enumerated, and each +LM item stores two boundary words; this is also true in general for synchronous and target-side binarized grammars where we always combine two consecutive target strings in a deduction. More generally, this scheme can be easily extended to work with an mgram model (Chiang, 2007) where m is usually ≥ 3 (trigram or higher) in practice. The target-side complexity for this case is thus where V is the target language vocabulary. This is because each constituent must store its initial and final (m − 1)-grams, which yields four (m − 1)grams in a binary combination. In practice, it is often assumed that there are only a constant number of translations for each input word, which reduces this complexity into O(|w|4(m−1) ). However, for source-side binarization which leaves gaps on the target-side, the situation becomes more complicated. Consider Deduction (8), where the sub-tr"
W07-0405,P03-2041,0,0.0684963,"h is symmetric from both sides. 3.2 Tree-based Approaches The tree-based approaches include the tree-to-string (also called syntax-directed) systems (Liu et al., 2006; Huang et al., 2006). This approach takes a source-language parse tree, instead of the plain string, as input, and tries to find the best derivation that recursively rewrites the input tree into a target 38 Sη : t 1 t 3 t 2 NPη·1 : t1 ... PPη·2 : t2 ... VPη·3 : t3 ... Figure 4: Illustration of tree-to-string deduction. string, using the SCFG as a tree-transducer. In this setting, the −LM decoding phase is a tree-parsing problem (Eisner, 2003) which aims to cover the entire tree by a set of rules. For example, a deduction of the first rule in Example 1 would be: (NPη·1 ) : (w1 , t1 ) (PPη·2 ) : (w2 , t2 ) (VPη·3 ) : (w3 , t3 ) (Sη ) : (w1 + w2 + w3 , t1 t3 t2 ) (13) where η and η · i(i = 1, 2, 3) are tree addresses (Shieber et al., 1995), with η · i being the ith child of η (the address of the root node is ǫ). The nonterminal labels at these tree nodes must match those in the SCFG rule, e.g., the input tree must have a PP at node η · 2. The semantics of this deduction is the following: if the label of the current node in the input"
W07-0405,P06-1121,0,0.302056,"Hall Philadelphia, PA 19104 lhuang3@cis.upenn.edu Abstract Binarization is essential for achieving polynomial time complexities in parsing and syntax-based machine translation. This paper presents a new binarization scheme, target-side binarization, and compares it with source-side and synchronous binarizations on both stringbased and tree-based systems using synchronous grammars. In particular, we demonstrate the effectiveness of targetside binarization on a large-scale tree-tostring translation system. 1 Introduction Several recent syntax-based models for machine translation (Chiang, 2005; Galley et al., 2006) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of a grammar rule. To alleviate this problem, one can borrow from parsing the technique of binarizing context-free grammars (into Chomsky Normal Form) to reduce the complexity. With synchronous context-free grammars (SCFG), however, this problem becomes more complicated with the additional dimension of target-side permutation. The simples"
W07-0405,W05-1507,1,0.947764,"s combined with a PP (see also Figure 3 (b)): (NP-VPi,j⊔k,l ) : (w1 , t1 ) (PPj,k ) : (w2 , t2 ) (Si,l ) : (w1 + w2 , t1 t2 ) (10) Both of the above deductions have four free indices, and thus of complexity O(|w|4 ) instead of cubic in the length of the input string w. More generally, the complexity of a binarization scheme depends on its source arity. In the worstcase, a binarized grammar with a source arity of s will require at most (2s + 1) free indices in a deduction, because otherwise if one rule needs (2s + 2) indices, then there are s+1 spans, which contradicts the definition of arity (Huang et al., 2005).4 These deductive systems represent the search space of decoding without a language model. When one is instantiated for a particular input string, it defines a set of derivations, called a forest, represented in a compact structure that has a structure of a hypergraph. Accordingly we call items like (PP1,3 ) nodes in the forest, and an instantiated deduction like (PP-VP1,6 ) → (PP1,3 )(VP3,6 ) we call a hyperedge that connects one or more antecedent nodes to a consequent node. In this representation, the time complexity of −LM decoding, which we refer to as source-side complexity, is proporti"
W07-0405,2006.amta-papers.8,1,0.929776,"noting that with the hook trick of Huang et al. (2005), the target-side complexity can be reduced to O(|w|(2t+1)(m−1) ), making it more analogous to its source-side counterpart: if we consider the decoding problem as intersecting the SCFG with a source-side DFA which has |S |= |w|+1 states, and a target-side DFA which has |T |= O(|w|m−1 ) states, then the intersected grammar has a parsing complexity of O(|S|2s+1 |T |2t+1 ), which is symmetric from both sides. 3.2 Tree-based Approaches The tree-based approaches include the tree-to-string (also called syntax-directed) systems (Liu et al., 2006; Huang et al., 2006). This approach takes a source-language parse tree, instead of the plain string, as input, and tries to find the best derivation that recursively rewrites the input tree into a target 38 Sη : t 1 t 3 t 2 NPη·1 : t1 ... PPη·2 : t2 ... VPη·3 : t3 ... Figure 4: Illustration of tree-to-string deduction. string, using the SCFG as a tree-transducer. In this setting, the −LM decoding phase is a tree-parsing problem (Eisner, 2003) which aims to cover the entire tree by a set of rules. For example, a deduction of the first rule in Example 1 would be: (NPη·1 ) : (w1 , t1 ) (PPη·2 ) : (w2 , t2 ) (VPη·3 )"
W07-0405,koen-2004-pharaoh,0,0.0647432,"Figure 5: Number of nodes in the forests. Input sentences are grouped into bins according to their lengths (5-9, 10-14, 15-20, etc.). O(|F |T ) = O(|w|1+2(t+1)(m−1) ). 4 15 20 25 30 length of the input sentence VBD was VP-C x1 :VBN IN PP x2 :NP-C by translates an English passive construction into Chinese. Although the rules are actually in a synchronous tree-substitution grammar (STSG) instead of an SCFG, its derivation structure is still a hypergraph and all the analysis in Section 3.2 still applies. This system performs slightly better than the state-of-the-art phrase-based system Pharaoh (Koehn, 2004) on English to Chinese translation. A very similar system for the reverse direction is described in (Liu et al., 2006). Our data preparation follows (Huang et al., 2006): the training data is a parallel corpus of 28.3M words on the English side, from which we extracted 24.7M tree-to-string rules using the algorithm of (Galley et al., 2006), and trained a Chinese trigram model on the Chinese side. We test our methods on the same test-set as in (Huang et al., 2006) which is a 140 sentence subset of NIST 2003 MT evaluation with 9–36 words on the English side. The weights for the loglinear model i"
W07-0405,P05-1057,0,0.0812885,"Missing"
W07-0405,P06-1077,0,0.29915,"It is also worth noting that with the hook trick of Huang et al. (2005), the target-side complexity can be reduced to O(|w|(2t+1)(m−1) ), making it more analogous to its source-side counterpart: if we consider the decoding problem as intersecting the SCFG with a source-side DFA which has |S |= |w|+1 states, and a target-side DFA which has |T |= O(|w|m−1 ) states, then the intersected grammar has a parsing complexity of O(|S|2s+1 |T |2t+1 ), which is symmetric from both sides. 3.2 Tree-based Approaches The tree-based approaches include the tree-to-string (also called syntax-directed) systems (Liu et al., 2006; Huang et al., 2006). This approach takes a source-language parse tree, instead of the plain string, as input, and tries to find the best derivation that recursively rewrites the input tree into a target 38 Sη : t 1 t 3 t 2 NPη·1 : t1 ... PPη·2 : t2 ... VPη·3 : t3 ... Figure 4: Illustration of tree-to-string deduction. string, using the SCFG as a tree-transducer. In this setting, the −LM decoding phase is a tree-parsing problem (Eisner, 2003) which aims to cover the entire tree by a set of rules. For example, a deduction of the first rule in Example 1 would be: (NPη·1 ) : (w1 , t1 ) (PPη·2 )"
W07-0405,N03-1021,0,0.101498,"Missing"
W07-0405,H05-1101,0,0.271873,"Powell held a meeting with Sharon captures the re-ordering of PP and VP between Chinese (source) and English (target). The sourceprojection of the first rule, for example, is S → NP PP VP. Decoding with an SCFG (e.g., translating from Chinese to English using the above grammar) can be cast as a parsing problem (see Section 3 for details), in which case we need to binarize a synchronous rule with more than two nonterminals to achieve polynomial time algorithms (Zhang et al., 2006). We will next present the three different binarization schemes using Example 1. 1 An alternative notation, used by Satta and Peserico (2005), allows co-indexed nonterminals to take different symbols across languages, which is convenient in describing syntactic divergences (see Figure 2). 34 2.1 Source-side Binarization The first and simplest scheme, source-side binarization, works left-to-right on the source projection of the SCFG without respecting the re-orderings on the target-side. So it will binarize the first rule as: (2) S NP-PP → → NP-PP VP NP PP which corresponds to Figure 1 (b). Notice that the virtual nonterminal NP-PP representing the intermediate symbol is discontinuous with two spans on the target (English) side, bec"
W07-0405,P06-1123,0,0.0535267,"Missing"
W07-0405,P96-1021,0,0.186056,"stantiated deduction like (PP-VP1,6 ) → (PP1,3 )(VP3,6 ) we call a hyperedge that connects one or more antecedent nodes to a consequent node. In this representation, the time complexity of −LM decoding, which we refer to as source-side complexity, is proportional to the size of the forest F , i.e., the number of hyperedges (instantiated deductions) in F . To summarize, the source-side complexity for a binarized grammar of source arity s is |F |= O(|w| 2s+1 O(|V |4(m−1) ) ). 3.1.2 Adding a Language Model To integrate with a bigram language model, we can use the dynamic-programming algorithm of Wu (1996), which we may think of as proceeding in two passes. The first pass is as above, and the second pass traverses the first-pass forest, assigning to each node v a set of augmented items, which we call +LM items, of the form (v a⋆b ), where a and b are target words and ⋆ is a placeholder symbol for an elided part of a target-language string. This item indicates that a possible translation of the part of the input spanned by v is a target string that starts with a and ends with b. Here is an example deduction in the synchronously binarized grammar (4), for a +LM item for the node (PP-VP1,6 ) based"
W07-0405,N06-1033,1,0.943514,"t-to-right) on the source-side as if treating it as a monolingual CFG for the sourcelangauge. However, this approach does not guaran∗ This work is partially supported by NSF ITR grants IIS0428020 (while I was visiting USC/ISI) and EIA-0205456. I also wish to thank Jonathan Graehl, Giorgio Satta, Hao Zhang, and the three anonymous reviewers for helpful comments. tee contiguous spans on the target-side, due to the arbitrary re-ordering of nonterminals between the two languages. As a result, decoding with an integrated language model still has an exponential complexity. Synchronous binarization (Zhang et al., 2006) solves this problem by simultaneously binarizing both source and target-sides of a synchronous rule, making sure of contiguous spans on both sides whenever possible. Neglecting the small amount of non-binarizable rules, the decoding complexity with an integrated language model becomes polynomial and translation quality is significantly improved thanks to the better search. However, this method is more sophisticated to implement than the previous method and binarizability ratio decreases on freer word-order languages (Wellington et al., 2006). This paper presents a third alternative, targetsid"
W17-4751,P02-1040,0,0.0986688,"translations and corresponding image. The system is only trained on 466 probability-based scores for evaluating different candidates. In this paper, we use Optimal Beam Search (Huang et al., 2017) (OBS) during decoding time. OBS uses bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal. Figure 2 and Figure 3 show the BLEU score and length ratio with different rewards for different beam size. We choose beam size equals to 5 and reward equals to 0.1 during decoding. 3.4 WMT organization provides three different evaluating metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2006). Table 2 to Table 5 summarize the performance with their corresponding rank among all other systems. We only show a few top performing systems in the tables to make a comparison. OSU1 is our proposed model and OSU2 is our baseline system without any image information. For MSCOCO dataset, the translation from English to German (Table 3), which is the hardest tasks compared with others since it is from English to German on OOD dataset, we achieve best TER score across all other systems. Rank 1 2 3&4 5 6 8 TER 47.5 48.1 48.2 50.7"
W17-4751,D15-1044,0,0.0374204,"Mingbo Ma, Dapeng Li, Kai Zhao† and Liang Huang Department of EECS Oregon State University Corvallis, OR 97331, USA {mam, lidap, zhaok, liang.huang}@oregonstate.edu Abstract More recently, attention-based encoderdecoder models (Bahdanau et al., 2014) have been proposed to provide the decoder more accurate alignments to generate more relevant words. The remarkable ability of attention mechanisms quickly update the state-of-theart performance on variety of NLG tasks, such as machine translation (Luong et al., 2015), image captioning (Xu et al., 2015; Yang et al., 2016), and text summarization (Rush et al., 2015; Nallapati et al., 2016). However, for multimodal translation (Elliott et al., 2015), where we translate a caption from one language into another given a corresponding image, we need to design a new model since the decoder needs to consider both language and images at the same time. This paper describes our participation in the WMT 2017 multimodal task 1. Our model feeds the image information to both the encoder and decoder, to ground their hidden representation within the same context of image during training. In this way, during testing time, the decoder would generate more relevant words g"
W17-4751,2006.amta-papers.25,0,0.0295698,"466 probability-based scores for evaluating different candidates. In this paper, we use Optimal Beam Search (Huang et al., 2017) (OBS) during decoding time. OBS uses bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal. Figure 2 and Figure 3 show the BLEU score and length ratio with different rewards for different beam size. We choose beam size equals to 5 and reward equals to 0.1 during decoding. 3.4 WMT organization provides three different evaluating metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2006). Table 2 to Table 5 summarize the performance with their corresponding rank among all other systems. We only show a few top performing systems in the tables to make a comparison. OSU1 is our proposed model and OSU2 is our baseline system without any image information. For MSCOCO dataset, the translation from English to German (Table 3), which is the hardest tasks compared with others since it is from English to German on OOD dataset, we achieve best TER score across all other systems. Rank 1 2 3&4 5 6 8 TER 47.5 48.1 48.2 50.7 50.7 51.6 METEOR 53.5 53.9 53.8 51 50.6 48.9 BLEU 33.3 31.9 33.2 3"
W17-4751,P04-1011,0,0.0741511,"ce in TER for English-German for MSCOCO dataset. 1 Introduction Natural language generation (NLG) is one of the most important tasks in natural language processing (NLP). It can be applied to a lot of interesting applications such like machine translation, image captioning, question answering. In recent years, Recurrent Neural Networks (RNNs) based approaches have shown promising performance in generating more fluent and meaningful sentences compared with conventional models such as rulebased model (Mirkovic et al., 2011), corpusbased n-gram models (Wen et al., 2015) and trainable generators (Stent et al., 2004). 2 Model Description For the neural-based machine translation model, the encoder needs to map sequence of word embeddings from the source side into another representation of the entire sequence using recurrent networks. Then, in the second stage, decoder generates one word at a time with considering global (sentence representation) and local information (weighted context) from source side. For simplicity, our proposed model is based on the attention-based encoderdecoder framework in (Luong et al., 2015), ref† Current address: Google Inc., 111 8th Avenue, New York, New York, USA. 465 Proceedin"
W17-4751,W16-3210,0,0.0460356,"Missing"
W17-4751,D17-1227,1,0.888069,"Missing"
W17-4751,W15-4639,0,0.0128767,"sets. Our system achieves the best performance in TER for English-German for MSCOCO dataset. 1 Introduction Natural language generation (NLG) is one of the most important tasks in natural language processing (NLP). It can be applied to a lot of interesting applications such like machine translation, image captioning, question answering. In recent years, Recurrent Neural Networks (RNNs) based approaches have shown promising performance in generating more fluent and meaningful sentences compared with conventional models such as rulebased model (Mirkovic et al., 2011), corpusbased n-gram models (Wen et al., 2015) and trainable generators (Stent et al., 2004). 2 Model Description For the neural-based machine translation model, the encoder needs to map sequence of word embeddings from the source side into another representation of the entire sequence using recurrent networks. Then, in the second stage, decoder generates one word at a time with considering global (sentence representation) and local information (weighted context) from source side. For simplicity, our proposed model is based on the attention-based encoderdecoder framework in (Luong et al., 2015), ref† Current address: Google Inc., 111 8th"
W17-4751,P17-4012,0,0.0262684,"encoder and decoder for initialization. I (in red) represents the image feature that are generated by CNN. 3 3.1 Test 1, 000 461 OOD ? No Yes Training details For preprocessing, we convert all of the sentences to lower case, normalize the punctuation, and do the tokenization. For simplicity, our vocabulary keeps all the words that show in training set. For image representation, we use ResNet (He et al., 2016) generated image features which are provided by the WMT organization. In our experiments, we only use average pooled features. Our implementation is adapted from on Pytorch-based OpenNMT (Klein et al., 2017). We use two layered bi-LSTM (Sutskever et al., 2014) on the source side as encoder. Our batch size is 64, with SGD optimization and a learning rate at 1. For English to German, the dropout rate is 0.6, and for English to French, the dropout rate is 0.4. These two parameters are selected by observing the performance on development set. Our word embeddings are randomly initialized with 500 dimensions. The source side vocabulary is 10,214 and the target side vocabulary is 18,726 for German and 11,222 for French. … x0 x1 x2 x3 x4 Dev 1, 014 - Table 1: Summary of datasets statistics. s1 I Train 29"
W17-4751,D15-1166,0,0.0709568,"ovic et al., 2011), corpusbased n-gram models (Wen et al., 2015) and trainable generators (Stent et al., 2004). 2 Model Description For the neural-based machine translation model, the encoder needs to map sequence of word embeddings from the source side into another representation of the entire sequence using recurrent networks. Then, in the second stage, decoder generates one word at a time with considering global (sentence representation) and local information (weighted context) from source side. For simplicity, our proposed model is based on the attention-based encoderdecoder framework in (Luong et al., 2015), ref† Current address: Google Inc., 111 8th Avenue, New York, New York, USA. 465 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 465–469 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics Flickr30K datasets but are also tested on MSCOCO besides Flickr30K. MSCOCO datasets are considered out-of-domain (OOD) testing while Flickr30K dataset are considered in-domain testing. The datasets’ statics is shown in Table 1 ereed as “Global attention”. On the other hand, for the early work of neural-basic caption gener"
W18-6443,W17-4746,0,0.0666162,"Missing"
W18-6443,D15-1044,0,0.031442,"rning which lead to substantial improvements. Our systems ensemble several models using different architectures and training methods and achieve the best performance for three subtasks: En-De and En-Cs in task 1 and (En+De+Fr)-Cs task 1B. 1 Introduction In recent years, neural text generation has attracted much attention due to its impressive generation accuracy and wide applicability. In addition to demonstrating compelling results for machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), by simple adaptation, similar models have also proven to be successful for summarization (Rush et al., 2015; Nallapati et al., 2016), image or video captioning (Venugopalan et al., 2015; Xu et al., 2015) and multimodal machine translation (Elliott et al., 2017; Caglayan et al., 2017; Calixto and Liu, 2017; Ma et al., 2017), which aims to translate the caption from one language to another with the help of the corresponding image. However, the conventional neural text generation models suffer from two major drawbacks. First, they are typically trained by predicting the next word given the previous ground-truth word. But at test time, the models recurrently feed their own predictions into it. This “ex"
W18-6443,D17-1105,0,0.178343,"En-Cs in task 1 and (En+De+Fr)-Cs task 1B. 1 Introduction In recent years, neural text generation has attracted much attention due to its impressive generation accuracy and wide applicability. In addition to demonstrating compelling results for machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), by simple adaptation, similar models have also proven to be successful for summarization (Rush et al., 2015; Nallapati et al., 2016), image or video captioning (Venugopalan et al., 2015; Xu et al., 2015) and multimodal machine translation (Elliott et al., 2017; Caglayan et al., 2017; Calixto and Liu, 2017; Ma et al., 2017), which aims to translate the caption from one language to another with the help of the corresponding image. However, the conventional neural text generation models suffer from two major drawbacks. First, they are typically trained by predicting the next word given the previous ground-truth word. But at test time, the models recurrently feed their own predictions into it. This “exposure bias” (Ranzato et al., 2015) leads to error accumulation ∗ † Equal contribution Contributions made while at Baidu Research 632 Proceedings of the Third Conference on Machine Translation (WMT),"
W18-6443,W16-3210,0,0.147567,"Missing"
W18-6443,W17-4718,0,0.22909,"reinforcement learning (Sutton et al., 1998) is proven to be helpful to directly optimize the evaluation metrics in neural text generation models training. Ranzato et al. (2015) successfully use the REINFORCE algorithm to directly optimize the evaluation metric over multiple text generation tasks. Rennie et al. (2017); Liu et al. (2017) achieve state-of-the-art on image captioning using REINFORCE with baseline to reduce training variance. Moreover, many existing works show that neural text generation models can benefit from model ensembling by simply averaging the outputs of different models (Elliott et al., 2017; Rennie et al., 2017). Garmash and Monz (2016) claim that it is essential to introduce diverse models into the ensemble. To this end, we ensemble models with various architectures and training methods. This paper describes our participation in the WMT 2018 multimodal tasks. Our submitted systems include a series of models which only consider text information, as well as multimodal models which also include image information to initialize the decoders. We train these models using scheduled sampling and reinforcement learning. The final outputs are decoded by ensembling those models. To the bes"
W18-6443,C16-1133,0,0.0739037,"is proven to be helpful to directly optimize the evaluation metrics in neural text generation models training. Ranzato et al. (2015) successfully use the REINFORCE algorithm to directly optimize the evaluation metric over multiple text generation tasks. Rennie et al. (2017); Liu et al. (2017) achieve state-of-the-art on image captioning using REINFORCE with baseline to reduce training variance. Moreover, many existing works show that neural text generation models can benefit from model ensembling by simply averaging the outputs of different models (Elliott et al., 2017; Rennie et al., 2017). Garmash and Monz (2016) claim that it is essential to introduce diverse models into the ensemble. To this end, we ensemble models with various architectures and training methods. This paper describes our participation in the WMT 2018 multimodal tasks. Our submitted systems include a series of models which only consider text information, as well as multimodal models which also include image information to initialize the decoders. We train these models using scheduled sampling and reinforcement learning. The final outputs are decoded by ensembling those models. To the best of our knowledge, this is the first multimoda"
W18-6443,D18-1357,1,0.792208,".5 58.8 BLEU 30.2 30.1 29.0 28.3 28.2 27.8 26.5 METEOR 29.5 29.7 29.4 29.1 29.1 29.2 27.7 TER 50.7 51.2 51.1 51.7 51.7 52.4 54.4 total. (Only including constrained models). En-Cs 31.27 30.84 Fr-Cs 28.48 27.02 De-Cs 26.96 25.99 (En+Fr+De)-Cs 29.47 29.23 Table 7: BLEU scores on validation set for task 1B System En-De En-Fr En-Cs En-Cs (1B) NMT MIX NMT MIX NMT MIX NMT MIX Num † 11 11 11 11 6 6 6 6 Model Rank BLEU MET. 1 4 2 5 4 9 9 10 1 1 2 2 1 2 1 1 TER 2 1 1 3 1 3 1 5 Num ‡ Team Rank BLEU MET. TER 5 1 3 1 6 3 5 1 3 1 1 1 3 1 1 1 ‡ represents the total number cussed in multi-reference training (Zheng et al., 2018), we randomly shuffle the source data in all languages and train using a traditional attention based-neural machine translation model in every epoch. Since we do BPE on the whole training data, we can share the vocabulary of different languages during training. The results show that models trained using single English to Czech data perform much better than the rest. Table 8 shows results on test set. The submitted systems are the same as those used in En-Cs task of task 1. Although we only consider the English source during training, our proposed systems still rank first among all the submissi"
W18-6443,P17-2060,0,0.0467639,"ch. As shown in Table 1, both tasks have 2900 training and 1014 validation examples. For preprocessing, we convert all of the sentences to lower case, normalize the punctuation, and tokenize. We employ byte-pair encoding (BPE) (Sennrich et al., 2015) on the whole training data including the four languages and reduce the source and target language vocabulary sizes to 20k in total. 3.2 En-Cs 31.27 31.38 31.73 30.84 31.21 31.36 33.15 33.11 the validation set. Details of the ensemble models are described in Table 9. where yˆti denotes the output distribution of ith model at position t. Similar to Zhou et al. (2017), we can ensemble models trained with different architectures and training algorithms. 3.1 En-Fr 58.36 58.67 58.80 57.92 58.80 58.78 61.43 61.45 Table 2: BLEU scores of different approaches on i=1 3 En-De 39.64 40.19 40.60 39.27 39.87 40.39 42.54 42.45 3.3 Results for task 1 To study the performance of different approaches, we conduct an ablation study. Table 2 shows the BLEU scores on validation set with different models and training methods. Generally, models with scheduled sampling perform better than baseline models, and reinforcement learning further improves the performance. Ensemble mod"
W18-6443,D17-1227,1,0.806831,"training dataset, the translation qualities of models with different initializations can vary notably. To make the performance much more stable and improve the translation quality, we ensemble different models during decoding to achieve better translation. To ensemble, we take the average of all model outputs: N ˆi X yt yˆt = (7) N trained on the ImageNet dataset. Our implementation is adapted from Pytorch-based OpenNMT (Klein et al., 2017). We use two layered bi-LSTM (Sutskever et al., 2014) as the encoder and share the vocabulary between the encoder and the decoder. We adopt length reward (Huang et al., 2017) on En-Cs task to find the optimal sentence length. We use a batch size of 50, SGD optimization, dropout rate as 0.1 and learning rate as 1.0. Our word embeddings are randomly initialized of dimension 500. To train the model with scheduled sampling, we first set probability  as 0, and then gradually increase it 0.05 every 5 epochs until it’s 0.25. The reinforcement learning models are trained based on those models pre-trained by scheduled sampling. Experiments Datasets We perform experiments using Flickr30K (Elliott et al., 2016) which are provided by the WMT organization. Task 1 (Multimodal"
W18-6443,P17-4012,0,0.0316377,"8,459 NMT NMT+SS NMT+SS+RL MNMT MNMT+SS MNMT+SS+RL NMT Ensemble MIX Ensemble Table 1: Statistics of Flickr30K Dataset 2.3 Ensembling In our experiments with relatively small training dataset, the translation qualities of models with different initializations can vary notably. To make the performance much more stable and improve the translation quality, we ensemble different models during decoding to achieve better translation. To ensemble, we take the average of all model outputs: N ˆi X yt yˆt = (7) N trained on the ImageNet dataset. Our implementation is adapted from Pytorch-based OpenNMT (Klein et al., 2017). We use two layered bi-LSTM (Sutskever et al., 2014) as the encoder and share the vocabulary between the encoder and the decoder. We adopt length reward (Huang et al., 2017) on En-Cs task to find the optimal sentence length. We use a batch size of 50, SGD optimization, dropout rate as 0.1 and learning rate as 1.0. Our word embeddings are randomly initialized of dimension 500. To train the model with scheduled sampling, we first set probability  as 0, and then gradually increase it 0.05 every 5 epochs until it’s 0.25. The reinforcement learning models are trained based on those models pre-tra"
W18-6443,W17-4751,1,0.853966,"n+De+Fr)-Cs task 1B. 1 Introduction In recent years, neural text generation has attracted much attention due to its impressive generation accuracy and wide applicability. In addition to demonstrating compelling results for machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), by simple adaptation, similar models have also proven to be successful for summarization (Rush et al., 2015; Nallapati et al., 2016), image or video captioning (Venugopalan et al., 2015; Xu et al., 2015) and multimodal machine translation (Elliott et al., 2017; Caglayan et al., 2017; Calixto and Liu, 2017; Ma et al., 2017), which aims to translate the caption from one language to another with the help of the corresponding image. However, the conventional neural text generation models suffer from two major drawbacks. First, they are typically trained by predicting the next word given the previous ground-truth word. But at test time, the models recurrently feed their own predictions into it. This “exposure bias” (Ranzato et al., 2015) leads to error accumulation ∗ † Equal contribution Contributions made while at Baidu Research 632 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared"
W18-6443,P16-1162,0,\N,Missing
W19-5367,W17-4712,0,0.021333,"ask, we also make the use of English portion of parallel data from KFTT, TED and JESC used in (Michel and Neubig, 2018). 3.1 Methods Comparison 3.3 Final Results Table 5 and Table 6 show the final results of our submission in Fr-En and En-Fr. Our system ranks third in both directions. Table 7 shows the human judgments over all submitted systems which are done by Li et al. (2019) who also analyze and discuss all submitted systems. 4 Related Work The method proposed in this paper is a kind of domain adaptation technique. There are many previous work on domain adaptation for machine translation (Britz et al., 2017; Wang et al., 2017; Chu et al., 2017; Chu and Wang, 2018), which leverages out-of-domain parallel corpora and indomain monolingual corpora to improve translation. The difference between our method and previous work lies in that we use back-translation (Sennrich et al., 2015a) for domain adaptation. Different from some previous work using adversarial training (Liu et al., 2017) or different attention (Zheng et al., 2018a) to differentiate multiple tasks, we simply assign different starting symbol for multiple tasks (Lample et al., 2018). A similar method was proposed in (Xie et al., Noisy Data"
W19-5367,P17-2061,0,0.0519504,"Missing"
W19-5367,C18-1111,0,0.0210322,"data from KFTT, TED and JESC used in (Michel and Neubig, 2018). 3.1 Methods Comparison 3.3 Final Results Table 5 and Table 6 show the final results of our submission in Fr-En and En-Fr. Our system ranks third in both directions. Table 7 shows the human judgments over all submitted systems which are done by Li et al. (2019) who also analyze and discuss all submitted systems. 4 Related Work The method proposed in this paper is a kind of domain adaptation technique. There are many previous work on domain adaptation for machine translation (Britz et al., 2017; Wang et al., 2017; Chu et al., 2017; Chu and Wang, 2018), which leverages out-of-domain parallel corpora and indomain monolingual corpora to improve translation. The difference between our method and previous work lies in that we use back-translation (Sennrich et al., 2015a) for domain adaptation. Different from some previous work using adversarial training (Liu et al., 2017) or different attention (Zheng et al., 2018a) to differentiate multiple tasks, we simply assign different starting symbol for multiple tasks (Lample et al., 2018). A similar method was proposed in (Xie et al., Noisy Data Generation To make use of monolingual target data, we wan"
W19-5367,N13-1037,0,0.0308333,"10 BLEU improvement in both En-Fr and Fr-En translation compared with the baseline methods. 1 Introduction Translation of social media is very challenging. First, there are various types of noises, such as abbreviations, spelling errors, obfuscated profanities, inconsistent capitalization, Internet slang and emojis (Michel and Neubig, 2018). Second, the amount of parallel data is limited. These characteristics of social media make existing neural machine translation systems extremely vulnerable. The noise issue of social media has been investigated in some previous work (Baldwin et al., 2013; Eisenstein, 2013). Most recently, Belinkov and Bisk (2017) demonstrated the vulnerability of neural machine translation system to both synthetic and natural noises. However, the noises tested in (Belinkov and Bisk, 2017) are not real noises in social media. To our best knowledge, there seems ∗ 1. We found that “social-media-style” sentences can be generated by training a translation model with different “start-of-sentence” symbols for sentences in different domains in the decoder side. The model is trained with data from all domains, especially News domain, which has a large amount of parallel data, but also a"
W19-5367,D17-1227,1,0.901069,"Missing"
W19-5367,P17-4012,0,0.02853,"s trained with different architectures and training algorithms. 3 Experiments To investigate the empirical performances of our proposed methods, we conduct experiments on MTNT dataset (Michel and Neubig, 2018) using Transformer (Vaswani et al., 2017). We first apply BPE (Sennrich et al., 2015b) on both sides in order to reduce the vocabulary for both source and target sides. We then exclude the sentences pairs whose length are longer than 256 words or subwords. We use length reward (Huang et al., 2017) to find the optimal target length. Our implementation is adapted from PyTorchbased OpenNMT (Klein et al., 2017). Our Transformer’s parameters are as the same as the base model’s parameter settings in the original paper (Vaswani et al., 2017). In all experiments, our evaluation uses sacreBLEU 1 , a standardized BLEU score evaluation Noisy Pseudo-Sources Generation with Back-Translation To further make the use of monolingual data, we regard them as target data and generate it’s corresponding source data by back-translation (Sennrich et al., 2015a). However, different from Sennrich et al. (2015a) who uses this method in both clean source and target sentences, the source side sentences in our test set is m"
W19-5367,D18-1549,0,0.0199571,"previous work on domain adaptation for machine translation (Britz et al., 2017; Wang et al., 2017; Chu et al., 2017; Chu and Wang, 2018), which leverages out-of-domain parallel corpora and indomain monolingual corpora to improve translation. The difference between our method and previous work lies in that we use back-translation (Sennrich et al., 2015a) for domain adaptation. Different from some previous work using adversarial training (Liu et al., 2017) or different attention (Zheng et al., 2018a) to differentiate multiple tasks, we simply assign different starting symbol for multiple tasks (Lample et al., 2018). A similar method was proposed in (Xie et al., Noisy Data Generation To make use of monolingual target data, we want to generate the corresponding parallel pseudo noisy source data and put them into training set. Table 3 shows the performance of our noisy data generation models. In this experiment, we mix the clean and noisy dataset as the training set, but use the target sentences in reversed direction of noisy dataset (training, validation, test) set as source and source sentences as target. The domain insensitive method simply mix the clean and noisy dataset in training while the domain se"
W19-5367,W19-5303,0,0.0633713,"nd Fr-En dataset, the clean parallel data is from WMT15 news translation task. The noisy data is from (Michel and Neubig, 2018) collected from social network. Except the French and English monolingual data from WMT15 news translation task, we also make the use of English portion of parallel data from KFTT, TED and JESC used in (Michel and Neubig, 2018). 3.1 Methods Comparison 3.3 Final Results Table 5 and Table 6 show the final results of our submission in Fr-En and En-Fr. Our system ranks third in both directions. Table 7 shows the human judgments over all submitted systems which are done by Li et al. (2019) who also analyze and discuss all submitted systems. 4 Related Work The method proposed in this paper is a kind of domain adaptation technique. There are many previous work on domain adaptation for machine translation (Britz et al., 2017; Wang et al., 2017; Chu et al., 2017; Chu and Wang, 2018), which leverages out-of-domain parallel corpora and indomain monolingual corpora to improve translation. The difference between our method and previous work lies in that we use back-translation (Sennrich et al., 2015a) for domain adaptation. Different from some previous work using adversarial training ("
W19-5367,P19-1291,1,0.777899,"verLabs FOKUS† En-Fr Score 71.5 66.3 75.5 52.5 En-Fr Rank 2 3 1 4 Fr-En BLEU 80.6 58.2 82.0 76.3 85.3 62.6 Fr-En Rank 3 6 2 4 1 5 Table 7: Human judgments over all submitted systems (the higher the better) ∗ Our submission. † Unconstrained. 2018) in the context of grammar correction, where a model is trained to add noises on original sentences to produce noisy sentences. However, instead of learn how to generate arbitrary “noises”, our goal is to learn “social-media-style” translations. Singh et al. (2019) injects artificial noise in the clean data according to the distribution of noisy data. Liu et al. (2019a) propose to leverage phonetic information to reduce the noises in data. Another group of work related to this paper is data augmentation in machine translation. Although data augmentation is very popular in gen562 eral learning tasks, such as image processing, it is non-trivial to do so in machine translation because even slight modifications of sentences can make huge difference in semantics. To our best knowledge, there are two categories of successful data augmentation approaches for machine translation. The first one is based on back-translation ((Sennrich et al., 2015a)) which augments"
W19-5367,P17-1001,0,0.0263679,"who also analyze and discuss all submitted systems. 4 Related Work The method proposed in this paper is a kind of domain adaptation technique. There are many previous work on domain adaptation for machine translation (Britz et al., 2017; Wang et al., 2017; Chu et al., 2017; Chu and Wang, 2018), which leverages out-of-domain parallel corpora and indomain monolingual corpora to improve translation. The difference between our method and previous work lies in that we use back-translation (Sennrich et al., 2015a) for domain adaptation. Different from some previous work using adversarial training (Liu et al., 2017) or different attention (Zheng et al., 2018a) to differentiate multiple tasks, we simply assign different starting symbol for multiple tasks (Lample et al., 2018). A similar method was proposed in (Xie et al., Noisy Data Generation To make use of monolingual target data, we want to generate the corresponding parallel pseudo noisy source data and put them into training set. Table 3 shows the performance of our noisy data generation models. In this experiment, we mix the clean and noisy dataset as the training set, but use the target sentences in reversed direction of noisy dataset (training, va"
W19-5367,I13-1041,0,0.0315462,"We achieve more than 10 BLEU improvement in both En-Fr and Fr-En translation compared with the baseline methods. 1 Introduction Translation of social media is very challenging. First, there are various types of noises, such as abbreviations, spelling errors, obfuscated profanities, inconsistent capitalization, Internet slang and emojis (Michel and Neubig, 2018). Second, the amount of parallel data is limited. These characteristics of social media make existing neural machine translation systems extremely vulnerable. The noise issue of social media has been investigated in some previous work (Baldwin et al., 2013; Eisenstein, 2013). Most recently, Belinkov and Bisk (2017) demonstrated the vulnerability of neural machine translation system to both synthetic and natural noises. However, the noises tested in (Belinkov and Bisk, 2017) are not real noises in social media. To our best knowledge, there seems ∗ 1. We found that “social-media-style” sentences can be generated by training a translation model with different “start-of-sentence” symbols for sentences in different domains in the decoder side. The model is trained with data from all domains, especially News domain, which has a large amount of parall"
W19-5367,D18-1357,1,0.902625,"Missing"
W19-5367,D18-1050,0,0.256373,"Task System Report Renjie Zheng ∗ ‡ Hairong Liu ∗† Mingbo Ma † Baigong Zheng † Liang Huang †,‡ † Baidu Research, Sunnyvale, CA ‡ School of EECS, Oregon State University, Corvallis, OR {zrenj11, lhrbss, cosmmb, zbgzbg2007, liang.huang.sh}@gmail.com Abstract to be a lack of translation methods systematically targeting noises in social media. Existing neural machine translation systems are famous for their hungry of data. However, the amount of parallel data in social media domain is very limited. Just recently, a dataset collected from Reddit has been published and attracted a lot of attention (Michel and Neubig, 2018). The amount of data in this dataset is still very small, compared to the large amount of data from News domain. Naturally, how to utilize the large amount of parallel data from the News domain become a central problem in improving the translation of social meida. In this paper, inspired by the success of backtranslation technique (Sennrich et al., 2015a), we propose to learn a model to generate “socialmedia-style” translation in source language from clean sentences in target language. Since the amount of parallel data in social media domain is limited, we utilize the large amount of parallel"
W19-5367,W18-6319,0,0.0218782,"n sensitive method in previous experiment. By adding these noisy back-translation data, we achieve more than 2 BLEU improvement. Our final submission ensembles 5 models trained with the domain sensitive method and including the noisy back translation data. Test 1,022 Table 2: Statistics of Fr2En Dataset, Monolingual data is English only. Domain Insensitive Domain Sensitive En2Fr 31.3 35.7 Fr2En 34.6 39.5 Table 3: Results of noisy data generation. We reverse the source and target direction of MTNT Fr2En (En2Fr) dev-set to evaluate the ability of noisy data generation for En2Fr (Fr2En). tool by Post (2018). We specify the intl tokenization option during BLEU evaluation. We also uses detokenization and normalization tools in Moses. Table 1 and 2 show statistics of En2Fr and Fr2En datasets. For both En-Fr and Fr-En dataset, the clean parallel data is from WMT15 news translation task. The noisy data is from (Michel and Neubig, 2018) collected from social network. Except the French and English monolingual data from WMT15 news translation task, we also make the use of English portion of parallel data from KFTT, TED and JESC used in (Michel and Neubig, 2018). 3.1 Methods Comparison 3.3 Final Results"
W19-5367,W18-6443,1,0.911078,"de. The intuition of injecting domain label in source side is based on the noise occurrence statistics from (Michel and Neubig, 2018), which shows much more spelling and grammar errors in the source side of noisy text translation dataset. Thus the clean and noisy start symbols work as a meaningful sign of source text style for encoder. Compared with the source side sentences, the human translation of target side sentences are less noisier with less spelling and grammar errors. 2.2 Ensemble where yˆti denotes the output distribution of ith model at position t. Similar to Zhou et al. (2017) and Zheng et al. (2018c), we can ensemble models trained with different architectures and training algorithms. 3 Experiments To investigate the empirical performances of our proposed methods, we conduct experiments on MTNT dataset (Michel and Neubig, 2018) using Transformer (Vaswani et al., 2017). We first apply BPE (Sennrich et al., 2015b) on both sides in order to reduce the vocabulary for both source and target sides. We then exclude the sentences pairs whose length are longer than 256 words or subwords. We use length reward (Huang et al., 2017) to find the optimal target length. Our implementation is adapted fr"
W19-5367,P16-1009,0,0.0806473,"Missing"
W19-5367,P17-2060,0,0.022869,"art symbol in source side. The intuition of injecting domain label in source side is based on the noise occurrence statistics from (Michel and Neubig, 2018), which shows much more spelling and grammar errors in the source side of noisy text translation dataset. Thus the clean and noisy start symbols work as a meaningful sign of source text style for encoder. Compared with the source side sentences, the human translation of target side sentences are less noisier with less spelling and grammar errors. 2.2 Ensemble where yˆti denotes the output distribution of ith model at position t. Similar to Zhou et al. (2017) and Zheng et al. (2018c), we can ensemble models trained with different architectures and training algorithms. 3 Experiments To investigate the empirical performances of our proposed methods, we conduct experiments on MTNT dataset (Michel and Neubig, 2018) using Transformer (Vaswani et al., 2017). We first apply BPE (Sennrich et al., 2015b) on both sides in order to reduce the vocabulary for both source and target sides. We then exclude the sentences pairs whose length are longer than 256 words or subwords. We use length reward (Huang et al., 2017) to find the optimal target length. Our imple"
W19-5367,W16-2323,0,0.0524641,"Missing"
W19-5367,N19-1190,0,0.0197401,"0.614 Table 6: Semi-blind test results of En-Fr. ∗ Our submission. † Unconstrained. BD-OSU∗ CMU CUNI JHU NaverLabs FOKUS† En-Fr Score 71.5 66.3 75.5 52.5 En-Fr Rank 2 3 1 4 Fr-En BLEU 80.6 58.2 82.0 76.3 85.3 62.6 Fr-En Rank 3 6 2 4 1 5 Table 7: Human judgments over all submitted systems (the higher the better) ∗ Our submission. † Unconstrained. 2018) in the context of grammar correction, where a model is trained to add noises on original sentences to produce noisy sentences. However, instead of learn how to generate arbitrary “noises”, our goal is to learn “social-media-style” translations. Singh et al. (2019) injects artificial noise in the clean data according to the distribution of noisy data. Liu et al. (2019a) propose to leverage phonetic information to reduce the noises in data. Another group of work related to this paper is data augmentation in machine translation. Although data augmentation is very popular in gen562 eral learning tasks, such as image processing, it is non-trivial to do so in machine translation because even slight modifications of sentences can make huge difference in semantics. To our best knowledge, there are two categories of successful data augmentation approaches for m"
W19-5367,D17-1155,0,0.022612,"e use of English portion of parallel data from KFTT, TED and JESC used in (Michel and Neubig, 2018). 3.1 Methods Comparison 3.3 Final Results Table 5 and Table 6 show the final results of our submission in Fr-En and En-Fr. Our system ranks third in both directions. Table 7 shows the human judgments over all submitted systems which are done by Li et al. (2019) who also analyze and discuss all submitted systems. 4 Related Work The method proposed in this paper is a kind of domain adaptation technique. There are many previous work on domain adaptation for machine translation (Britz et al., 2017; Wang et al., 2017; Chu et al., 2017; Chu and Wang, 2018), which leverages out-of-domain parallel corpora and indomain monolingual corpora to improve translation. The difference between our method and previous work lies in that we use back-translation (Sennrich et al., 2015a) for domain adaptation. Different from some previous work using adversarial training (Liu et al., 2017) or different attention (Zheng et al., 2018a) to differentiate multiple tasks, we simply assign different starting symbol for multiple tasks (Lample et al., 2018). A similar method was proposed in (Xie et al., Noisy Data Generation To make"
W19-5367,D18-1100,0,0.0404436,"Missing"
W19-5367,N18-1057,0,0.0516069,"Missing"
W19-5367,P19-1582,1,0.887687,"Missing"
W19-5367,P16-1162,0,\N,Missing
