2007.mtsummit-papers.36,N03-1017,0,0.0829746,"pany accounts database retrieval service dialog boxes network access path name security information Table 2: Sample compound nouns 3. Phrasal Alignment Process Independently of the English compound noun extraction, we extracted a phrase table from the bilingual corpus data as shown in Table 1. The Giza++ (Och and Ney, 2000c) toolkit for IBM models (Brown et al., 1994) and HMM (Och and Ney, 2000a) is used to provide word alignment for the bilingual data. During implementation, we performed five iterations of model 1, followed by five iterations of HMM and then five iterations of model 4. As in Koehn et al. (2003), the bilingual corpus is aligned bidirectionally, i.e., first we perform word alignment from English to Japanese and second from Japanese to English, respectively. Then these two alignments are combined to form the final word alignment with the heuristics described in Och and Ney (Och and Ney, 2000b). From the bidirectional word alignment, we extracted phrasal translation pairs that are consistent with the word alignment. This means that words in a phrase pair are only aligned to each other (Och et al., 1999). The maximum phrase length used in our experiments was set to four. We now have two"
2007.mtsummit-papers.36,W97-0207,0,0.174088,"Missing"
2007.mtsummit-papers.36,W97-0200,0,0.0618787,"Missing"
2007.mtsummit-papers.36,C00-2163,0,0.0247294,"ed to five. We identified a total of 38,519 compound nouns from the English data of the English-Japanese corpus. Table 2 provides some samples of extracted compound nouns. form template disk space user name tools menu color scheme web browser start date file type file type data connection company accounts database retrieval service dialog boxes network access path name security information Table 2: Sample compound nouns 3. Phrasal Alignment Process Independently of the English compound noun extraction, we extracted a phrase table from the bilingual corpus data as shown in Table 1. The Giza++ (Och and Ney, 2000c) toolkit for IBM models (Brown et al., 1994) and HMM (Och and Ney, 2000a) is used to provide word alignment for the bilingual data. During implementation, we performed five iterations of model 1, followed by five iterations of HMM and then five iterations of model 4. As in Koehn et al. (2003), the bilingual corpus is aligned bidirectionally, i.e., first we perform word alignment from English to Japanese and second from Japanese to English, respectively. Then these two alignments are combined to form the final word alignment with the heuristics described in Och and Ney (Och and Ney, 2000b). F"
2007.mtsummit-papers.36,P00-1056,0,0.0938438,"ed to five. We identified a total of 38,519 compound nouns from the English data of the English-Japanese corpus. Table 2 provides some samples of extracted compound nouns. form template disk space user name tools menu color scheme web browser start date file type file type data connection company accounts database retrieval service dialog boxes network access path name security information Table 2: Sample compound nouns 3. Phrasal Alignment Process Independently of the English compound noun extraction, we extracted a phrase table from the bilingual corpus data as shown in Table 1. The Giza++ (Och and Ney, 2000c) toolkit for IBM models (Brown et al., 1994) and HMM (Och and Ney, 2000a) is used to provide word alignment for the bilingual data. During implementation, we performed five iterations of model 1, followed by five iterations of HMM and then five iterations of model 4. As in Koehn et al. (2003), the bilingual corpus is aligned bidirectionally, i.e., first we perform word alignment from English to Japanese and second from Japanese to English, respectively. Then these two alignments are combined to form the final word alignment with the heuristics described in Och and Ney (Och and Ney, 2000b). F"
2007.mtsummit-papers.36,W99-0604,0,0.0555834,"l 1, followed by five iterations of HMM and then five iterations of model 4. As in Koehn et al. (2003), the bilingual corpus is aligned bidirectionally, i.e., first we perform word alignment from English to Japanese and second from Japanese to English, respectively. Then these two alignments are combined to form the final word alignment with the heuristics described in Och and Ney (Och and Ney, 2000b). From the bidirectional word alignment, we extracted phrasal translation pairs that are consistent with the word alignment. This means that words in a phrase pair are only aligned to each other (Och et al., 1999). The maximum phrase length used in our experiments was set to four. We now have two lists: (i) the list of the English compound noun described in Section 2.2 and (ii) the list of the bilingual phrasal translation pairs. We simply extracted the overlap between the two lists; that is, the phrase pairs whose English side strings are the same strings as those in the English compound noun list. The overview of the bilingual compound noun extraction process is described in Figure 1 below. Source (English) Target (Japanese) Word-alignment Extraction of compound nouns Extraction of phrase pair transl"
2007.mtsummit-papers.36,P05-1034,0,0.0288319,"that the translator should not easily pick up one of the multiple translation variations without fully examining the context. 6. Concluding Remarks This paper explored a way to extract phrase pair translations for compound nouns from a English-Japanese parallel corpus and to check translation. As mentioned at the outset of the paper, inconsistent terminology translations could lead to MT quality issues. In this connection, we would like to address some of our future plans. At Microsoft, we have been localizing technical documents for several years, using a statistical MT system called MSR-MT (Quirk, et al., 2005). To enhance the quality of terminology translation of MSR-MT, we hope to be able to plug the proposed method into our MT workflow. We envision that this will help us to create an MT-friendly eco-system across different product groups. Also, we are entertaining some application ideas of the proposed term mining process in post-editing contexts. One of the major issues that post-editors face in postediting contexts lies in the verification and correction of the terminology in MT output. To facilitate this process, we can create a bilingual terminology look-up tool for any language pairs, using"
2007.mtsummit-papers.36,J93-2003,0,\N,Missing
2011.iwslt-evaluation.6,D11-1033,1,0.884452,"explaining broad topical variety in a corpus. The importance of this model is that it is unsupervised, and that after training it can be used to perform statistical inference on the new input. This allows previously-unheard utterances to be related to the topics learned during training. In the past, topic models have been used to select additional monolingual data to create a topic-specific language model [19], and these models have been applied to the task of statistical machine translation (SMT) [17][18]. Combining topic models with prior work on selecting relevant out-ofdomain sub-corpora [1][7], we propose a method for selecting additional parallel corpora using an unsupervised topic model. In IWSLT2011, we have submitted the topic-adaptive phrasebased translation system as our contrastive system 2. In order to address the second challenge, we develop a discriminative training method to estimate the translation channel models more accurately. The machine translation problem is commonly modeled by a log-linear model with multiple features that capture different dependencies between the source language and the target language [15]. Although the log-linear model is discriminative in"
2011.iwslt-evaluation.6,P05-1033,0,0.189656,", the ML estimation could lead to sub-optimal distribution [10]. In order to address this problem, we introduce a discriminative training method for these generative translation models based on a technique called growth transformation (GT). In IWSLT2011, we have submitted a phrase-based system with discriminative translation models as our contrastive system 1. Our primary submission is a combination of four systems, including the topic-adaptive system and the discriminative translation model system described above, plus a regular phrase-based machine translation system [11] and a Hiero system [3]. System combination is performed based on the incremental indirect hidden Markov model proposed in [20][21]. 2. Data For training, we use exclusively the monolingual and parallel texts supplied by the evaluation campaign. No additional 57 datasets, web data, or other resources were used.  2.1. TED relevant training data  The TED parallel corpus consists of about 110K sentences of English transcription and their Chinese translation of archived TED talks (www.ted.com) as provided by the IWSLT evaluation campaign. 2.2. Supplementary training data In addition, the IWSLT evaluation campaign also"
2011.iwslt-evaluation.6,N09-1025,0,0.0495805,"of (2) is discriminative in nature, many of the feature functions, such as the translation models based features, are derived from generative models. Conventionally, these features are usually trained by maximum likelihood (ML) estimation [11]. However, when data are sparse, the ML training could lead to sub-optimal estimation of probability distributions [10]. Recently, effort has been made to further extend the maxBLEU training method. In [12], model parameters are optimized with a perceptron using the best possible translation hypothesis as the approximated reference. On the other hand, in [4], the linear model is extended to include tens of thousands of fine-grained features, where most of them are binary indicators. In order to effectively training the weights of this many features, an MIRA-based optimization method is used. In this work, we introduce a discriminative training method for the estimation of translation models based on a technique called growth transformation (GT) [9]. Unlike [12], we use the expected BLEU score as the objective function and the true reference is used without approximation. Compared to [4], our focus is on discriminative training of the phrase and l"
2011.iwslt-evaluation.6,W07-0711,1,0.905617,"14]:  ( |) {∑ ( )} (2) ∑ {∑ where ( )} being the normalization denominator to ensure that the probabilities sum to one. In the log-linear model, { ( )} are the feature functions constructed from E and F. In our system, features include hypothesis length, number of phrases, lexicalized reordering model scores, language model scores, and translation model scores. Details of these models are described in the following sections. 3.1.1 Translation phrase tables In our system, we first perform word alignment on the TED parallel corpus using the word-dependent HMM-based alignment method proposed in [6]. Then, a phrase table is constructed from the word aligned TED corpus as described in [11]. In the phrase table, each phrase pair has four translation model scores. They are: ( )  Forward phrase translation feature: ( |) ∏ (  | ) where  and  are the kth phrase in E and F, respectively, and (  | ) is the probability of translating  to  . This is usually modeled by a multinomial model. ( ( ) ) (3) where is the translation reference(s), and  ( ) is the translation output. In our system, dev2010 is used for MERT training. 3.2 Topic-Adaptive Phrase-based translation system Latent Dirich"
2011.iwslt-evaluation.6,N03-1017,0,0.0202808,"the translation channel models more accurately. The machine translation problem is commonly modeled by a log-linear model with multiple features that capture different dependencies between the source language and the target language [15]. Although the log-linear model is discriminative in nature, many of the feature functions, such as the phrase-level translation probability features and the lexicon-level translation probability features (e.g., lexical weighting), are derived from generative models. Further, these features are usually trained by conventional maximum likelihood (ML) estimation [11]. In the case of sparse training data, the ML estimation could lead to sub-optimal distribution [10]. In order to address this problem, we introduce a discriminative training method for these generative translation models based on a technique called growth transformation (GT). In IWSLT2011, we have submitted a phrase-based system with discriminative translation models as our contrastive system 1. Our primary submission is a combination of four systems, including the topic-adaptive system and the discriminative translation model system described above, plus a regular phrase-based machine transl"
2011.iwslt-evaluation.6,P06-1096,0,0.0595086,"four models were tuned for each topic in a log-linear combination. 3.3 Discriminative translation model based phrasal system Although the log-linear model of (2) is discriminative in nature, many of the feature functions, such as the translation models based features, are derived from generative models. Conventionally, these features are usually trained by maximum likelihood (ML) estimation [11]. However, when data are sparse, the ML training could lead to sub-optimal estimation of probability distributions [10]. Recently, effort has been made to further extend the maxBLEU training method. In [12], model parameters are optimized with a perceptron using the best possible translation hypothesis as the approximated reference. On the other hand, in [4], the linear model is extended to include tens of thousands of fine-grained features, where most of them are binary indicators. In order to effectively training the weights of this many features, an MIRA-based optimization method is used. In this work, we introduce a discriminative training method for the estimation of translation models based on a technique called growth transformation (GT) [9]. Unlike [12], we use the expected BLEU score as"
2011.iwslt-evaluation.6,P03-1021,0,0.122218,"the log-linear model at decoding. 3.1.2 Language Models Two language models (LM) are used in our system. The first is a 3-gram LM trained on the English side of the TED parallel corpus. In addition, we also trained a LM based on the 115M of monolingual English sentences. Since there are much more data in this monolingual English dataset, a 5-gram LM can be trained to capture longer contextual information without severe data sparsity issue. Both LMs use Kneser-Ney smoothing. 3.1.3 Tuning of Lambdas The linear weights of these features, e.g., { }, are tuned by minimum error rate training (MERT) [14]:  ( |) {∑ ( )} (2) ∑ {∑ where ( )} being the normalization denominator to ensure that the probabilities sum to one. In the log-linear model, { ( )} are the feature functions constructed from E and F. In our system, features include hypothesis length, number of phrases, lexicalized reordering model scores, language model scores, and translation model scores. Details of these models are described in the following sections. 3.1.1 Translation phrase tables In our system, we first perform word alignment on the TED parallel corpus using the word-dependent HMM-based alignment method proposed in [6]"
2011.iwslt-evaluation.6,P02-1038,0,0.0669388,"work on selecting relevant out-ofdomain sub-corpora [1][7], we propose a method for selecting additional parallel corpora using an unsupervised topic model. In IWSLT2011, we have submitted the topic-adaptive phrasebased translation system as our contrastive system 2. In order to address the second challenge, we develop a discriminative training method to estimate the translation channel models more accurately. The machine translation problem is commonly modeled by a log-linear model with multiple features that capture different dependencies between the source language and the target language [15]. Although the log-linear model is discriminative in nature, many of the feature functions, such as the phrase-level translation probability features and the lexicon-level translation probability features (e.g., lexical weighting), are derived from generative models. Further, these features are usually trained by conventional maximum likelihood (ML) estimation [11]. In the case of sparse training data, the ML estimation could lead to sub-optimal distribution [10]. In order to address this problem, we introduce a discriminative training method for these generative translation models based on a"
2011.iwslt-evaluation.6,P02-1040,0,0.0852441,"Missing"
2011.iwslt-evaluation.6,W11-2133,0,0.0354719,"use of a single domain-specific system for the task. A topic model [2] is a generative model for explaining broad topical variety in a corpus. The importance of this model is that it is unsupervised, and that after training it can be used to perform statistical inference on the new input. This allows previously-unheard utterances to be related to the topics learned during training. In the past, topic models have been used to select additional monolingual data to create a topic-specific language model [19], and these models have been applied to the task of statistical machine translation (SMT) [17][18]. Combining topic models with prior work on selecting relevant out-ofdomain sub-corpora [1][7], we propose a method for selecting additional parallel corpora using an unsupervised topic model. In IWSLT2011, we have submitted the topic-adaptive phrasebased translation system as our contrastive system 2. In order to address the second challenge, we develop a discriminative training method to estimate the translation channel models more accurately. The machine translation problem is commonly modeled by a log-linear model with multiple features that capture different dependencies between the s"
2011.iwslt-evaluation.6,P07-1066,0,0.0591736,"of a single domain-specific system for the task. A topic model [2] is a generative model for explaining broad topical variety in a corpus. The importance of this model is that it is unsupervised, and that after training it can be used to perform statistical inference on the new input. This allows previously-unheard utterances to be related to the topics learned during training. In the past, topic models have been used to select additional monolingual data to create a topic-specific language model [19], and these models have been applied to the task of statistical machine translation (SMT) [17][18]. Combining topic models with prior work on selecting relevant out-ofdomain sub-corpora [1][7], we propose a method for selecting additional parallel corpora using an unsupervised topic model. In IWSLT2011, we have submitted the topic-adaptive phrasebased translation system as our contrastive system 2. In order to address the second challenge, we develop a discriminative training method to estimate the translation channel models more accurately. The machine translation problem is commonly modeled by a log-linear model with multiple features that capture different dependencies between the sourc"
2011.iwslt-evaluation.6,D08-1011,1,0.904507,"ntroduce a discriminative training method for these generative translation models based on a technique called growth transformation (GT). In IWSLT2011, we have submitted a phrase-based system with discriminative translation models as our contrastive system 1. Our primary submission is a combination of four systems, including the topic-adaptive system and the discriminative translation model system described above, plus a regular phrase-based machine translation system [11] and a Hiero system [3]. System combination is performed based on the incremental indirect hidden Markov model proposed in [20][21]. 2. Data For training, we use exclusively the monolingual and parallel texts supplied by the evaluation campaign. No additional 57 datasets, web data, or other resources were used.  2.1. TED relevant training data  The TED parallel corpus consists of about 110K sentences of English transcription and their Chinese translation of archived TED talks (www.ted.com) as provided by the IWSLT evaluation campaign. 2.2. Supplementary training data In addition, the IWSLT evaluation campaign also provides out-of-domain data for potential usage. These include about 7.7M parallel sentences of UN proc"
2011.iwslt-evaluation.6,P09-1107,1,0.906396,"duce a discriminative training method for these generative translation models based on a technique called growth transformation (GT). In IWSLT2011, we have submitted a phrase-based system with discriminative translation models as our contrastive system 1. Our primary submission is a combination of four systems, including the topic-adaptive system and the discriminative translation model system described above, plus a regular phrase-based machine translation system [11] and a Hiero system [3]. System combination is performed based on the incremental indirect hidden Markov model proposed in [20][21]. 2. Data For training, we use exclusively the monolingual and parallel texts supplied by the evaluation campaign. No additional 57 datasets, web data, or other resources were used.  2.1. TED relevant training data  The TED parallel corpus consists of about 110K sentences of English transcription and their Chinese translation of archived TED talks (www.ted.com) as provided by the IWSLT evaluation campaign. 2.2. Supplementary training data In addition, the IWSLT evaluation campaign also provides out-of-domain data for potential usage. These include about 7.7M parallel sentences of UN proceedi"
2011.iwslt-evaluation.6,D09-1125,1,0.886687,"f the MT_SC_CE track by the organizer), system-1 seems improved significantly after the preliminary run. These issues make the tuning of the combination parameters difficult. In the MSR submission, we submitted one primary submission and two contrastive submissions. In all three submissions, only the translations from the four sites who have submitted preliminary runs are used for combination. In our primary submission, we jointly optimize the word alignment, ordering, and lexical selection decisions according to a set of feature functions combined in a single log-linear model as described in [22]. Regarding tuning of combination parameters, due to the severe mismatch of performances of individual systems in the preliminary run and the formal evaluation, the system weights estimated from the preliminary run is not reliable. Therefore, in our primary run, we heuristically set the system weights (according to the rank of systems in the formal run from a notice by the organizer), i.e., 0.25 : 0.20 : 0.35 : 0.20. All other parameters such as LM weight, word-voting weight etc. are still tuned on the data of the preliminary run. In contrast, contrastive-1 uses system weights trained on the p"
2011.iwslt-evaluation.6,2007.mtsummit-papers.43,0,0.329957,"Missing"
2011.iwslt-evaluation.6,federico-etal-2012-iwslt,0,\N,Missing
2011.iwslt-evaluation.6,2011.iwslt-evaluation.1,0,\N,Missing
2011.iwslt-evaluation.6,2010.iwslt-evaluation.1,0,\N,Missing
2013.iwslt-evaluation.4,2007.mtsummit-papers.43,0,0.123153,"ments, including a novel method for processing bilingual data used to train MT systems for use on ASR output. Our primary submission is a system combination of five individual systems, combining the output of multiple ASR engines with multiple MT techniques. There are two contrastive submissions to help place the combined system in context. We describe the systems used and present results on the test sets. 1. Introduction Our work for IWSLT 2013 [1] began with a baseline system that consisted of piping the 1-best output from FBK ASR system [2] through a phrase-based machine translation system [3]. We made a series of additive improvements to both the ASR and MT components, culminating in a combined system that significantly outperformed our baseline on the tst2010 test set. The biggest MT improvements came from augmenting the training data with data normalized to make it more similar to ASR output. The biggest ASR improvements came from using DNNs and doing speaker and language model adaptation. We used three different ASR systems, which we will refer to in this paper as FBK, MSRA and MSRA-2. The FBK system is described in section 2.1. The MSRA and MSRA-2 systems are described in sect"
2013.iwslt-evaluation.4,D08-1077,0,0.154813,"est MT improvements came from augmenting the training data with data normalized to make it more similar to ASR output. The biggest ASR improvements came from using DNNs and doing speaker and language model adaptation. We used three different ASR systems, which we will refer to in this paper as FBK, MSRA and MSRA-2. The FBK system is described in section 2.1. The MSRA and MSRA-2 systems are described in section 2.2. We used four different MT systems, referred to hereafter as TREELET, PHRASE-BASED, PHONEME and OODPHONEME. The TREELET system is a tree-to-string translation system as described in [4]. The PHRASE-BASED system is a phrase-based machine translation system as described in [3]. The PHONEME system is a phrase-based system where the source side of the in-domain training data has been altered using a novel technique that makes it look more like ASR output. The technique used to alter the training data is novel. The OOD-PHONEME system is the same as the PHONEME system, but with the addition of out-of-domain normalized data. 2 Fondazione Bruno Kessler University of Trento Trento, TN, Italy nicruiz@fbk.eu Our primary submission was a system combination of five systems: FBK-TREELET,"
2013.iwslt-evaluation.4,D08-1011,1,0.903698,"m where the source side of the in-domain training data has been altered using a novel technique that makes it look more like ASR output. The technique used to alter the training data is novel. The OOD-PHONEME system is the same as the PHONEME system, but with the addition of out-of-domain normalized data. 2 Fondazione Bruno Kessler University of Trento Trento, TN, Italy nicruiz@fbk.eu Our primary submission was a system combination of five systems: FBK-TREELET, FBK-PHRASE-BASED, FBK-PHONEME, MSRA-PHONEME, and MSRA2-OODPHONEME. The system combination was performed using techniques described in [5]. In section 2 we discuss the ASR systems we used. Section 3 describes the work we did to insert punctuation into the ASR output. In section 4 we describe the machine translation systems we used. Results are discussed in section 5. 2. ASR Systems Our system combination used the output from two different ASR engines. The first is the FBK engine described in [6]. The second is a system developed at Microsoft Research. 2.1. FBK ASR System The FBK English speech recognizer is an HMM-based triphone large-vocabulary continuous-speech recognition system with acoustic models trained on both TED talks"
2013.iwslt-evaluation.4,rousseau-etal-2012-ted,0,0.0306466,"s developed out of a speakerindependent Switchboard system trained on 2000h of data (the SWBD and Fisher corpora), as described in [9]. That same model was used (with minor vocabulary tweaks) for a live demonstration of speech-to-speech [10], where one can get a subjective impression for its accuracy. In the following, we will describe how this system was adapted to the IWSLT task. 2.2.1. IWSLT Acoustic Model The SWBD acoustic model is suboptimal for TED talks in that they are wideband recordings with a large variation of non-native accents. We switched training data to the TEDLium collection [11], which consists of about 56000 utterances from 774 talks, which amounts to 118 hours of usable training speech after segmentation. The resulting DNN has 7 hidden layers of dimension 2048, and 9304 output classes. The feature extraction was updated for wideband recordings and to reflect the latest experience w.r.t. DNNs. We used a raw 40-channel Mel-filterbank instead of PLPs, 10-th root non-linearity, and a wider frame window of 23 frames or about 1/4 of a second), instead of derivatives. This was followed by the usual mean-variance normalization. The model training consisted of a first train"
2013.iwslt-evaluation.4,2007.iwslt-1.10,1,0.896676,"Missing"
2013.iwslt-evaluation.4,P07-2045,0,0.00510046,"unctuation except for periods, commas, semi-colons, question marks, apostrophes and exclamation points. This processed data represents the target side of our MT system. The source side of the translation data is obtained by removing the sentence boundary punctuation (periods, commas, semi-colons, question marks and exclamation 3. French Gigaword V3 4. European Language Newspaper Text LDC95T11 4.2. Baseline System Our baseline system is a typical phrase-based statistical machine translation system. Details of the system are described in [3]. The decoder is very similar to the one used by Moses [15]. 4.3. Treelet System In addition to our phrase-based baseline system, we also used a syntax-based tree to string MT system, as described in [4]. Although the BLEU score of this system individually is somewhat lower than that of the baseline phrase-based system, it is able to capture certain phenomena that are hard to capture in phrase-based systems. It is thus a very useful component for system combinations. 4.4. Phoneme-motivated Text Normalization Machine translation relies heavily on the data it uses in training. Simply training a MT system on text corpora and applying it to spoken languag"
2013.iwslt-evaluation.4,P96-1041,0,0.131619,"phrase-based MT system trained using components from the system we want to simulate. 4.4.1. System Configuration Inspired by the expositions of [16, 17], we first normalize each word in FBK’s ASR lexicon into a phoneme sequence by performing text-to-speech (TTS) analyses with an in-house synthesizer. The phoneme sequences and their target lexical forms are used respectively as source and target parallel training data for a monotonic phoneme-to-word phrase-based MT system. We use two 4-gram LMs from FBK’s IWSLT 2012 primary submission [6], which were trained with modified Kneser-Ney smoothing [18] on TED and WMT data. Due to the small amount of training data, we assign uniform forward and backward phrase probabilities to each phoneme sequence to word mapping. We omit lexical probabilities. With the aforementioned components, we can now tune a phrase-based machine translation system that translates from the actual phoneme sequence into ASR text output. The optimization can be done by randomly sampling a small development set from the 1-best ASR outputs on dev2010 from FBK’s IWSLT 2012 primary submission. The corresponding transcripts are used as input in MERT. We apply the tuned phoneme"
2013.iwslt-evaluation.4,P09-1107,1,0.86002,"eline MT system containing only un-normalized text. Our first system (TED norm) performs the normalization technique in 4.4.1, using uniform phrase translation probabilities. In the second system, we normalize all of the MT training data and use the phrase-based translation probability features estimated from the TED data, as described in 4.4.2. Both the original and improved channel model results are provided in Table 4 using FBK’s output. 4.5. System Combination In testing, we combined outputs from the five single systems using the incremental indirect hidden Markov model (IHMM) proposed in [5, 19], which has been shown to give superior performance in several MT benchmark tests [20]. The parameters of the IHMM are estimated indirectly from a variety of sources including semantic word similarity, surface word similarity, and a distance-based distortion penalty. The pairwise IHMM was extended to operate incrementally in [19], where the confusion network is initialized by formHere we present the results of testing our various systems on test sets. 5.0.1. Test Data Because we observed mismatches between the dev2010 and tst2010 test sets which made dev2010 unsuitable for use in tuning our sy"
2013.iwslt-evaluation.4,W12-3124,1,0.853489,"the normalization technique in 4.4.1, using uniform phrase translation probabilities. In the second system, we normalize all of the MT training data and use the phrase-based translation probability features estimated from the TED data, as described in 4.4.2. Both the original and improved channel model results are provided in Table 4 using FBK’s output. 4.5. System Combination In testing, we combined outputs from the five single systems using the incremental indirect hidden Markov model (IHMM) proposed in [5, 19], which has been shown to give superior performance in several MT benchmark tests [20]. The parameters of the IHMM are estimated indirectly from a variety of sources including semantic word similarity, surface word similarity, and a distance-based distortion penalty. The pairwise IHMM was extended to operate incrementally in [19], where the confusion network is initialized by formHere we present the results of testing our various systems on test sets. 5.0.1. Test Data Because we observed mismatches between the dev2010 and tst2010 test sets which made dev2010 unsuitable for use in tuning our system combination, we decided to use half of tst2010 as a development test set and the"
2013.iwslt-evaluation.4,D09-1125,1,0.860784,"tion techniques use TTS analysis to convert input data into phoneme sequences, followed by channel modeling trained from the ASR lexicon (L EX) and optionally the TED training data to generate normalized text. ing a simple graph with one word per link from the skeleton hypothesis, and each remaining hypothesis is aligned with the partial confusion network. This allows words from all previous hypotheses be considered as matches and leads to better performance compared to the pairwise IHMM. The incremental IHMM is also more computationally efficient than fully joint optimization methods such as [21], and provides a good trade-off between accuracy and runtime cost. In our implementation, each of these five systems produces a 10-best output for system combination. The semantic word similarity of the IHMM is derived from the French/English word translation probabilities learned on the TED parallel training data using the word-dependent HMM-based alignment method proposed in [22]. The language model is a trigram LM trained on the French side of the TED parallel data. The system combination parameters are tuned on the first half of the IWSLT tst2010 set, while the second half is reserved as t"
2013.iwslt-evaluation.4,W07-0711,1,0.817051,"l previous hypotheses be considered as matches and leads to better performance compared to the pairwise IHMM. The incremental IHMM is also more computationally efficient than fully joint optimization methods such as [21], and provides a good trade-off between accuracy and runtime cost. In our implementation, each of these five systems produces a 10-best output for system combination. The semantic word similarity of the IHMM is derived from the French/English word translation probabilities learned on the TED parallel training data using the word-dependent HMM-based alignment method proposed in [22]. The language model is a trigram LM trained on the French side of the TED parallel data. The system combination parameters are tuned on the first half of the IWSLT tst2010 set, while the second half is reserved as the devtest set. 5. Results 4.4.3. Results We compare the normalization techniques described above against a baseline MT system containing only un-normalized text. Our first system (TED norm) performs the normalization technique in 4.4.1, using uniform phrase translation probabilities. In the second system, we normalize all of the MT training data and use the phrase-based translatio"
2013.iwslt-evaluation.4,2006.iwslt-evaluation.4,0,\N,Missing
2013.iwslt-evaluation.4,2012.iwslt-evaluation.9,0,\N,Missing
2013.iwslt-evaluation.4,2011.iwslt-papers.7,0,\N,Missing
2020.acl-main.125,N16-1012,0,0.325277,"some importance words, such as “obama” and “nominees”. Introduction The explosion of information has expedited the rapid development of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summ"
2020.acl-main.125,D19-1383,0,0.0257256,"atural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) that combines extractive and abstractive techniques with a pointer (Vinyals et al., 2015) enabling the model to copy words from the source text directly. Although, copy mechanism has been widely used in summarization task, how to guarantee that important tokens in the source are copied remains a challenge. In our experiments, we find that the transformer-based summarization model with the copy mechanism may miss some"
2020.acl-main.125,N19-1423,0,0.187679,"sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) that combines extractive and abstractive techniques with a pointer (Vinyals et al., 2015) enabling the model to copy words from the source text directly. Although, copy mechanism has been widely used in summarization task, how to guarantee that important tokens in the source are copied remains a challenge. In our experiments, we find that the transformer-based summarization model with the copy mec"
2020.acl-main.125,D18-1443,0,0.184695,"f-the-art on the public text summarization dataset. 2 (2) where ct is a context vector generated based on the attention distribution (Bahdanau et al., 2015): Pvocab (w) = softmax(Wa st + Va ct ) • We propose a centrality-aware attention and a guidance loss to encourage the model to pay attention to important source words. (1) P (yt ) = pgen Pvocab (yt ) + (1 − pgen )Pcopy (yt ) (7) pgen = sigmoid(waT ct + uTa st + vaT yt−1 ) (8) Related Work Neural network based models (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Nallapati et al., 2017; Zhou et al., 2017; Tan et al., 2017; Gehrmann et al., 2018; Zhu et al., 2019; Li et al., 2020b,a) achieve promising results for the abstractive text summarization. Copy mechanism (Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017; Zhou et al., 2018) enables the summarizers with the ability to copy from the source into the target via pointing (Vinyals et al., 2015). Recently, pre-training based methods (Devlin et al., 2019; Copy distribution Pcopy determines where to attend in time step t. In the most previous work, encoder-decoder attention weight αt is serves as the copy distribution (See et al., 2017): X Pcopy (w) = αt,i (9) i:xi =w The loss"
2020.acl-main.125,P16-1154,0,0.201178,"s, such as “obama” and “nominees”. Introduction The explosion of information has expedited the rapid development of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is"
2020.acl-main.125,P16-1014,1,0.867966,"et al., 2015): Pvocab (w) = softmax(Wa st + Va ct ) • We propose a centrality-aware attention and a guidance loss to encourage the model to pay attention to important source words. (1) P (yt ) = pgen Pvocab (yt ) + (1 − pgen )Pcopy (yt ) (7) pgen = sigmoid(waT ct + uTa st + vaT yt−1 ) (8) Related Work Neural network based models (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Nallapati et al., 2017; Zhou et al., 2017; Tan et al., 2017; Gehrmann et al., 2018; Zhu et al., 2019; Li et al., 2020b,a) achieve promising results for the abstractive text summarization. Copy mechanism (Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017; Zhou et al., 2018) enables the summarizers with the ability to copy from the source into the target via pointing (Vinyals et al., 2015). Recently, pre-training based methods (Devlin et al., 2019; Copy distribution Pcopy determines where to attend in time step t. In the most previous work, encoder-decoder attention weight αt is serves as the copy distribution (See et al., 2017): X Pcopy (w) = αt,i (9) i:xi =w The loss function L is the average negative log likelihood of the ground-truth target word yt for each timestep t: 1 XT L=− logP (yt ) (10) t=0 T 1356"
2020.acl-main.125,C18-1121,1,0.880781,"edited the rapid development of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) that combines extractive and abstracti"
2020.acl-main.125,W04-1013,0,0.0500472,"nn et al., 2018) is a sequence-to-sequence model augmented with a bottom-up content selector. MASS (Song et al., 2019) is a sequence-tosequence pre-trained model based on the Transformer. ABS (Rush et al., 2015) relies on an CNN encoder and a NNLM decoder. ABS+ (Rush et al., 2015) enhances the ABS model with extractive summarization features. SEASS (Zhou et al., 2017) controls the information flow from the encoder to the decoder with the selective encoding strategy. SeqCopyNet (Zhou et al., 2018) extends the copy mechanism that can copy sequences from the source. We adopt ROUGE (RG) F1 score (Lin, 2004) as the evaluation metric. As shown in Table 2 and Table 3, SAGCopy with both outdegree and indegree centrality based guidance significantly outperform the baseline models, which prove the effectiveness of self-attention guided copy mechanism. The basic indegree centrality (indegree-1) is more favorable, considering the ROUGE score and computation complexity. Besides ROUGE evaluation, we further investigate the guidance from the view of the loss function. For each sample in the Gigaword test set, we measure the KL divergence between the centrality score and the copy distribution, and we calcul"
2020.acl-main.125,W04-3252,0,0.103894,"20 Association for Computational Linguistics We propose a Self-Attention Guided Copy mechanism (SAGCopy) that aims to encourage the summarizer to copy important source words. Selfattention layer in the Transformer (Vaswani et al., 2017) builds a directed graph whose vertices represent the source words and edges are defined in terms of the relevance score between each pair of source words by dot-product attention (Vaswani et al., 2017) between the query Q and the key K. We calculate the centrality of each source words based on the adjacency matrices. A straightforward method is using TextRank (Mihalcea and Tarau, 2004) algorithm that assumes a word receiving more relevance score from others are more likely to be important. This measure is known as the indegree centrality. We also adopt another measure assuming that a word sends out more relevance score to others is likely to be more critical, namely outdegree centrality, to calculate the source word centrality. We utilize the centrality score as guidance for copy distribution. Specifically, we extend the dotproduct attention to a centrality-aware function. Furthermore, we introduce an auxiliary loss computed by the divergence between the copy distribution a"
2020.acl-main.125,K16-1028,1,0.938984,"” and “nominees”. Introduction The explosion of information has expedited the rapid development of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Netwo"
2020.acl-main.125,D15-1044,0,0.335781,"-copying switch pgen ∈ [0, 1], the final probability distribution of the ground-truth target word yt is: • We achieve state-of-the-art on the public text summarization dataset. 2 (2) where ct is a context vector generated based on the attention distribution (Bahdanau et al., 2015): Pvocab (w) = softmax(Wa st + Va ct ) • We propose a centrality-aware attention and a guidance loss to encourage the model to pay attention to important source words. (1) P (yt ) = pgen Pvocab (yt ) + (1 − pgen )Pcopy (yt ) (7) pgen = sigmoid(waT ct + uTa st + vaT yt−1 ) (8) Related Work Neural network based models (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Nallapati et al., 2017; Zhou et al., 2017; Tan et al., 2017; Gehrmann et al., 2018; Zhu et al., 2019; Li et al., 2020b,a) achieve promising results for the abstractive text summarization. Copy mechanism (Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017; Zhou et al., 2018) enables the summarizers with the ability to copy from the source into the target via pointing (Vinyals et al., 2015). Recently, pre-training based methods (Devlin et al., 2019; Copy distribution Pcopy determines where to attend in time step t. In the most previous work, e"
2020.acl-main.125,P17-1099,0,0.761152,"The explosion of information has expedited the rapid development of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) th"
2020.acl-main.125,P16-1162,0,0.0341165,"encoder-decoder attention. Thus, we adopt a centrality-aware auxiliary loss to encourage the consistency between the overall copy distribution and the word centrality distribution based on the Kullback-Leibler (KL) divergence: L=− 5 5.1 1X 1X logP (yt ) + λKL( αt , score) t t T T (18) Experiments Experimental Setting We evaluate our model in CNN/Daily Mail dataset (Hermann et al., 2015) and Gigaword dataset (Rush et al., 2015). Our experiments are conducted with 4 NVIDIA P40 GPU. We adopt 6 layer encoder and 6 layers decoder with 12 attention heads, and hmodel = 768. Byte Pair Encoding (BPE) (Sennrich et al., 2016) word segmentation is used for data pre-processing. We warm-start the model parameter with MASS pre-trained base model1 and trains about 10 epoches for convergence. During decoding, we use beam search with a beam size of 5. 5.2 Experimental Results We compare our proposed Self-Attention Guided Copy (SAGCopy) model with the following comparative models. Lead-3 uses the first three sentences of the article as its summary. PGNet (See et al., 2017) is the PointerGenerator Network. Bottom-Up (Gehrmann et al., 2018) is a sequence-to-sequence model augmented with a bottom-up content selector. MASS (S"
2020.acl-main.125,P17-1108,0,0.040341,"Missing"
2020.acl-main.125,P19-1499,0,0.0130405,"current neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) that combines extractive and abstractive techniques with a pointer (Vinyals et al., 2015) enabling the model to copy words from the source text directly. Although, copy mechanism has been widely used in summarization task, how to guarantee that important tokens in the source are copied remains a challenge. In our experiments, we find that the transformer-based summarization model with the copy mechanism may miss some important words. As shown in Table 1, words like “nominees” and"
2020.acl-main.125,P19-1628,0,0.0165674,"eman, 1978; Bonacich, 1987; Borgatti and Everett, 2006; Kiss and Bichler, 2008; Li et al., 2011). Degree centrality is one of the simplest centrality measures that can be distinguished as indegree centrality and outdegree centrality (Freeman, 1978), which are determined based on the edges coming into and leaving a node, respectively. Indegree centrality of a word is proportional to the number of relevance scores incoming from other words, which can be measured by the sum of the indegree scores or by graph-based extractive summarization methods (Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Zheng and Lapata, 2019). Outdegree centrality of a word is proportional to the number of relevance scores outgoing to other words, which can be computed by the sum of the outdegree scores. Formally, let G = (V, D) be a directed graph representing self-attention, where vertices V is the word set and edge Di,j is represented by the encoder self-attention P weight from the word xi to the word xj , where i Di,j = 1. Next, we introduce the approaches to calculate the word centrality with the graph G. We first construct a transition probability matrix T as follows: X Ti,j = Di,j / Di,j . (13) j We introduce two approaches"
2020.acl-main.125,P17-1101,0,0.39448,"information has expedited the rapid development of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) that combines extract"
2020.acl-main.125,D19-1302,0,0.0678277,"elopment of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) that combines extractive and abstractive techniques with a p"
2020.acl-main.241,D19-1522,0,0.0929269,"t al., 2013) is the first and most representative translational distance model. A series of work is conducted along this line such as TransH (Wang et al., 2014), TransR (Lin et al., 2015) and TransD (Ji et al., 2015) etc. RotatE (Sun et al., 2019) further extends the computation into complex domain and is currently the state-of-art in this category. On the other hand, Semantic matching models usually take multiplicative score functions to compute the plausibility of the given triple, such as DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), TuckER (Balazevic et al., 2019) and QuatE (Zhang et al., 2019). ConvKB (Nguyen et al., 2017) and CapsE (Nguyen et al., 2019) further took the triple as a whole, and fed head, relation and tail embeddings into convolutional models or capsule networks. The above knowledge graph embedding methods focused on modeling individual triples. However, they ignored knowledge graph structure and did not take advantage of context from neighbouring nodes and edges. This issue inspired the usage of graph neural networks (Kipf and Welling, 2016; Veliˇckovi´c et al., 2017) for graph context modeling. Encoder-decoder framework was adopted in"
2020.acl-main.241,P19-1431,0,0.133392,"al., 2017) and CapsE (Nguyen et al., 2019) further took the triple as a whole, and fed head, relation and tail embeddings into convolutional models or capsule networks. The above knowledge graph embedding methods focused on modeling individual triples. However, they ignored knowledge graph structure and did not take advantage of context from neighbouring nodes and edges. This issue inspired the usage of graph neural networks (Kipf and Welling, 2016; Veliˇckovi´c et al., 2017) for graph context modeling. Encoder-decoder framework was adopted in (Schlichtkrull et al., 2017; Shang et al., 2019; Bansal et al., 2019). The knowledge graph structure is first encoded via graph neural networks and the output with rich structure information is passed to the following graph embedding model for prediction. The graph model and the scoring model could be end-to-end trained together, or the graph encoder output was only used to initialize the entity embedding (Nathani et al., 2019). We take another approach in this paper: we integrate the graph context directly into the distance scoring function. 2.2 Orthogonal Transform make the training faster and more stable in different tasks. On the other hand, some work has b"
2020.acl-main.241,P15-1067,0,0.409122,"ledge graph embedding could be roughly categorized into two classes (Wang et al., 2017): distance-based models and semantic matching models. Distance-based model is also known as additive models, since it projects head and tail enti2714 ties into the same embedding space and the distance scoring between two entity embeddings is used to measure the plausibility of the given triple. TransE (Bordes et al., 2013) is the first and most representative translational distance model. A series of work is conducted along this line such as TransH (Wang et al., 2014), TransR (Lin et al., 2015) and TransD (Ji et al., 2015) etc. RotatE (Sun et al., 2019) further extends the computation into complex domain and is currently the state-of-art in this category. On the other hand, Semantic matching models usually take multiplicative score functions to compute the plausibility of the given triple, such as DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), TuckER (Balazevic et al., 2019) and QuatE (Zhang et al., 2019). ConvKB (Nguyen et al., 2017) and CapsE (Nguyen et al., 2019) further took the triple as a whole, and fed head, relation and tail embeddings into convolutional m"
2020.acl-main.241,P19-1466,0,0.0266288,"es and edges. This issue inspired the usage of graph neural networks (Kipf and Welling, 2016; Veliˇckovi´c et al., 2017) for graph context modeling. Encoder-decoder framework was adopted in (Schlichtkrull et al., 2017; Shang et al., 2019; Bansal et al., 2019). The knowledge graph structure is first encoded via graph neural networks and the output with rich structure information is passed to the following graph embedding model for prediction. The graph model and the scoring model could be end-to-end trained together, or the graph encoder output was only used to initialize the entity embedding (Nathani et al., 2019). We take another approach in this paper: we integrate the graph context directly into the distance scoring function. 2.2 Orthogonal Transform make the training faster and more stable in different tasks. On the other hand, some work has been done to achieve strict orthogonal during optimization by applying special gradient update scheme. Harandi and Fernando (2016) proposed a Stiefel layer to guarantee fully connected layers to be orthogonal by using Reimannian gradients. Huang et al. (2017) consider the estimation of orthogonal matrix as an optimization over multiple dependent stiefel manifol"
2020.acl-main.241,N19-1226,0,0.058281,"is conducted along this line such as TransH (Wang et al., 2014), TransR (Lin et al., 2015) and TransD (Ji et al., 2015) etc. RotatE (Sun et al., 2019) further extends the computation into complex domain and is currently the state-of-art in this category. On the other hand, Semantic matching models usually take multiplicative score functions to compute the plausibility of the given triple, such as DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), TuckER (Balazevic et al., 2019) and QuatE (Zhang et al., 2019). ConvKB (Nguyen et al., 2017) and CapsE (Nguyen et al., 2019) further took the triple as a whole, and fed head, relation and tail embeddings into convolutional models or capsule networks. The above knowledge graph embedding methods focused on modeling individual triples. However, they ignored knowledge graph structure and did not take advantage of context from neighbouring nodes and edges. This issue inspired the usage of graph neural networks (Kipf and Welling, 2016; Veliˇckovi´c et al., 2017) for graph context modeling. Encoder-decoder framework was adopted in (Schlichtkrull et al., 2017; Shang et al., 2019; Bansal et al., 2019). The knowledge graph s"
2020.acl-main.241,W15-4007,0,0.375974,"ive explanation for the success of GC-OTE. Optimization Self-adversarial negative sampling loss (Sun et al., 2019) is used to optimize the embedding in this work, 2717 L = − ∑ p(h , r, t ) log σ(dall (h , r, t ) − γ) ′ ′ − log σ(γ − dall (h, r, t)) ′ ′ (13) where γ is a fixed margin, σ is sigmoid function, ′ ′ ′ ′ (h , r, t ) is negative triple, and p(h , r, t ) is the negative sampling weight defined in (Sun et al., 2019). 4 4.1 Experiments Datasets Two commonly used benchmark datasets (FB15k237 and WN18RR) are employed in this study to evaluate the performance of link prediction. FB15k-237 (Toutanova and Chen, 2015) dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs. The knowledge base triples are a subset of the FB15K (Bordes et al., 2013), originally derived from Freebase. The inverse relations are removed in FB15k-237. WN18RR (Dettmers et al., 2018) is derived from WN18 (Bordes et al., 2013), which is a subset of WordNet. WN18 consists of 18 relations and 40,943 entities. However, many text triples obtained by inverting triples from the training set. Thus WN18RR (Dettmers et al., 2018) is created to ensure that the evaluation dataset does not have test leaka"
2020.coling-main.496,W18-6402,0,0.0186341,"e highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image will benefit text summarization system. Figure 1 illustrates this phenomenon. For a given source sentence, a paired image visualizes a set of event highlight words, which highly correlates with the reference summary. Multimodal sequence-to-sequence (seq2seq) learning has been widely explored in machine translation (MT) (Calixto et al., 2017; Caglayan et al., 2017; Helcl et al., 2018; Gr¨onroos et al., 2018) in recent years, and the performances of their models surpass text-only models (Barrault et al., 2018). The main difference between multimodal MT and multimodal text summarization is: the model for MT is required to convert the same semantics from the input sentence and the paired image to the output, while for summarization, the model is expected to select the important information from the input. Li et al. (2018a) propose a hierarchical attention model for the multimodal sentence summarization task, while the image is not involved in the process of text encoding. Obviously, it will be easier for the decoder to generate This work is licensed under a Creative Commons Attribution 4.0 Internatio"
2020.coling-main.496,W17-4746,0,0.0458668,"Missing"
2020.coling-main.496,D17-1105,0,0.0235329,"ect-level visual sGate Figure 2: The framework of our model. We design visual selective gates including global-level, gridlevel and object-level visual gates to select salient encoding information. We also integrate the textual and visual selective gates to construct multimodal selective gates. In this figure, the summary is generated with the text-based decoder, and we also apply multimodal selective gates to the multimodal-based decoder (Li et al., 2018a) in our work. 2.2 Multimodal Seq2seq Models Libovick´y and Helcl (2017) propose multi-source seq2seq learning with hierarchical attention. Calixto and Liu (2017) use images as source words to improve translation quality. Delbrouck and Dupont (2017) adjust various attention for the visual modality. Calixto et al. (2017) propose attention mechanisms for textual and visual modalities and combine them to decode target words. Narayan et al. (2017) develop extractive summarization with side information including images and captions. Zhu et al. (2018), Chen and Zhuge (2018) and Zhu et al. (2020) propose to generate multimodal summary for multimodal news document. Li et al. (2018a) first introduce the multimodal sentence summarization task, and they propose a"
2020.coling-main.496,P17-1175,0,0.389415,"rce) product description generation (Chen et al., 2019; Elad et al., 2019; Zhang et al., 2019; Li et al., 2020a), etc. Intuitively, it is easier for a reader to grasp the highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image will benefit text summarization system. Figure 1 illustrates this phenomenon. For a given source sentence, a paired image visualizes a set of event highlight words, which highly correlates with the reference summary. Multimodal sequence-to-sequence (seq2seq) learning has been widely explored in machine translation (MT) (Calixto et al., 2017; Caglayan et al., 2017; Helcl et al., 2018; Gr¨onroos et al., 2018) in recent years, and the performances of their models surpass text-only models (Barrault et al., 2018). The main difference between multimodal MT and multimodal text summarization is: the model for MT is required to convert the same semantics from the input sentence and the paired image to the output, while for summarization, the model is expected to select the important information from the input. Li et al. (2018a) propose a hierarchical attention model for the multimodal sentence summarization task, while the image is not i"
2020.coling-main.496,D18-1438,0,0.0891796,"; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017; Zhu et al., 2018; Chen and Zhuge, 2018), and electronic commerce (e-commerce) product description generation (Chen et al., 2019; Elad et al., 2019; Zhang et al., 2019; Li et al., 2020a), etc. Intuitively, it is easier for a reader to grasp the highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image will benefit text summarization system. Figure 1 illustrates this phenomenon. For a given source sentence, a paired image visualizes a set of event highlight words, which highly correlates with the reference summary. Multimodal sequence-to-sequence (seq2seq) learning has been widely expl"
2020.coling-main.496,N16-1012,0,0.201591,"highlights embedded in the image more accurately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015)"
2020.coling-main.496,N18-2097,0,0.0120218,"e between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ... h1 x1 h2 x2 h3 ... x3 ... → [hn;h1] hi hn MLP xn hi Global-level visual sGate MLP hi sGateo MLP hi ... ... Grid-level visual sGate sGateg Selective Gate (sGate) max-pooling sGateM hi Textual sGate hn＇ sGate2 MLP MLP hi"
2020.coling-main.496,D17-1095,0,0.0234216,"ctive gates including global-level, gridlevel and object-level visual gates to select salient encoding information. We also integrate the textual and visual selective gates to construct multimodal selective gates. In this figure, the summary is generated with the text-based decoder, and we also apply multimodal selective gates to the multimodal-based decoder (Li et al., 2018a) in our work. 2.2 Multimodal Seq2seq Models Libovick´y and Helcl (2017) propose multi-source seq2seq learning with hierarchical attention. Calixto and Liu (2017) use images as source words to improve translation quality. Delbrouck and Dupont (2017) adjust various attention for the visual modality. Calixto et al. (2017) propose attention mechanisms for textual and visual modalities and combine them to decode target words. Narayan et al. (2017) develop extractive summarization with side information including images and captions. Zhu et al. (2018), Chen and Zhuge (2018) and Zhu et al. (2020) propose to generate multimodal summary for multimodal news document. Li et al. (2018a) first introduce the multimodal sentence summarization task, and they propose a hierarchical attention model, which can pay different attention to image patches, word"
2020.coling-main.496,D19-1301,0,0.0122215,"ploy a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ... h1 x1 h2 x2 h3 ... x3 ... → [hn;h1] hi hn MLP xn hi Global-level visual sGate MLP hi sGateo MLP hi ... ... Grid-level visual sGate sGateg Selective Gate (sGate) max-pooling sGateM hi Textual sGate hn＇ sGate2 MLP MLP hi Attention h1＇ sGate1 → yt-1 sGate1 sGate2 MLP MLP max-pooling sGateN ... ... hi MLP hi Object-level visu"
2020.coling-main.496,W18-6439,0,0.0553968,"Missing"
2020.coling-main.496,P16-1154,0,0.024788,"the important information from the source text. • We propose a visual-guided modality regularization module to encourage the model focus on the key information in the source. • The experimental results on a multimodal sentence summarization dataset demonstrate that our proposed system can take advantage of multimodal information and outperform baseline methods. 2 2.1 Related Work Abstractive Sentence Summarization Rush et al. (2015) first propose a seq2seq model to generate the summary for a sentence. Chopra et al. (2016) and Nallapati et al. (2016) further develop the seq2seq for this task. Gu et al. (2016), Zeng et al. (2016), and Gulcehre et al. (2016) incorporate a copying mechanism into the seq2seq. See et al. (2017) incorporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao"
2020.coling-main.496,P16-1014,0,0.0210833,"text. • We propose a visual-guided modality regularization module to encourage the model focus on the key information in the source. • The experimental results on a multimodal sentence summarization dataset demonstrate that our proposed system can take advantage of multimodal information and outperform baseline methods. 2 2.1 Related Work Abstractive Sentence Summarization Rush et al. (2015) first propose a seq2seq model to generate the summary for a sentence. Chopra et al. (2016) and Nallapati et al. (2016) further develop the seq2seq for this task. Gu et al. (2016), Zeng et al. (2016), and Gulcehre et al. (2016) incorporate a copying mechanism into the seq2seq. See et al. (2017) incorporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the pr"
2020.coling-main.496,W18-6441,0,0.0326239,"Missing"
2020.coling-main.496,N16-1082,0,0.0779981,"Missing"
2020.coling-main.496,D17-1222,0,0.0734802,"curately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Nar"
2020.coling-main.496,C18-1121,1,0.321043,"select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017; Zhu et al., 2018; Chen and Zhuge, 2018), and electronic commerce (e-commerce) product description generation (Chen et al., 2019; Elad et al., 2019; Zhang et al., 2019; Li et al., 2020a), etc. Intuitively, it is easier for a reader to grasp the highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image wi"
2020.coling-main.496,P17-2031,0,0.0523496,"Missing"
2020.coling-main.496,W04-1013,0,0.0202043,"on samples. 4.2 Experimental Settings We set word embedding size to 300 and GRU hidden state size to 512. We use the full source and target vocabularies collected from the training data, which have 36,916 source words and 26,168 target words, respectively. The mini-batch size is 64, and beam search size is 10. Adam optimizer is applied with the learning rate of 0.0005, momentum parameters β1 = 0.9 and β1 = 0.999, and  = 10−8 . We use dropout (Srivastava et al., 2014) with probability of 0.2 and gradient clipping (Pascanu et al., 2013) with range [−1, 1]. During training, we test the ROUGE-2 (Lin, 2004) F1-score on the validation set for every 5,000 batches, and we halve the learning rate if the score drops for 5 consecutive testings. 5660 Methods RG-1 RG-2 RG-L Non-Selective T-Selective Global V-Selective Object V-Selective Grid V-Selective 44.53 (± 0.11) 44.81 (± 0.14) 45.31 (± 0.15) 44.83 (± 0.12) 45.11 (± 0.16) 22.67 (± 0.12) 23.13 (± 0.13) 23.39 (± 0.13) 23.28 (± 0.13) 23.33 (± 0.15) 41.91 (± 0.09) 41.88 (± 0.11) 42.48 (± 0.11) 42.03 (± 0.12) 42.21 (± 0.12) T + Global V-Selective T + Object V-Selective T + Grid V-Selective T + Grid V-Selective + MR 45.51 (± 0.13) 45.33 (± 0.14) 45.58 (±"
2020.coling-main.496,P17-2100,0,0.0165761,"advantage of multimodal information and outperform baseline methods. 2 2.1 Related Work Abstractive Sentence Summarization Rush et al. (2015) first propose a seq2seq model to generate the summary for a sentence. Chopra et al. (2016) and Nallapati et al. (2016) further develop the seq2seq for this task. Gu et al. (2016), Zeng et al. (2016), and Gulcehre et al. (2016) incorporate a copying mechanism into the seq2seq. See et al. (2017) incorporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations"
2020.coling-main.496,W04-3252,0,0.0347575,"overlapping words, except for stop-words, between the input sentence and the reference summary as the ground-truth keywords). Following Zhou et al. (2017), we use the method of Li et al. (2016) to calculate the contribution of the selective gates to the final summary. Considering that the average word count of the summaries in the validation set is eight, we take the words with top-8 contribution values as the activated keywords by the selective mechanisms. Table 3 shows the results for keyword extraction. Our models with selective encoding perform better than unsupervised TextRank algorithm (Mihalcea and Tarau, 2004) (we also take the top-8 scored words as the keywords), and Multimodal Selective show advantages over Textual Selective. We further train a BiLSTM-CRF model (Huang et al., 2015) using sentence-keyword samples of the multimodal sentence summarization dataset, and the keyword extraction result for BiLSTM-CRF is better than our model, indicating a promising prospect for further development of the selective mechanism. In the future, we will dedicate our efforts to explore whether the selective gate can benefit from supervision signals of a special keyword extractor. A feasible research direction m"
2020.coling-main.496,K16-1028,0,0.0569473,"Missing"
2020.coling-main.496,D15-1044,0,0.34564,"ary to capture the highlights embedded in the image more accurately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015;"
2020.coling-main.496,P17-1099,0,0.210618,"ourage the model focus on the key information in the source. • The experimental results on a multimodal sentence summarization dataset demonstrate that our proposed system can take advantage of multimodal information and outperform baseline methods. 2 2.1 Related Work Abstractive Sentence Summarization Rush et al. (2015) first propose a seq2seq model to generate the summary for a sentence. Chopra et al. (2016) and Nallapati et al. (2016) further develop the seq2seq for this task. Gu et al. (2016), Zeng et al. (2016), and Gulcehre et al. (2016) incorporate a copying mechanism into the seq2seq. See et al. (2017) incorporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the inpu"
2020.coling-main.496,C18-1146,0,0.0135528,"(2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ... h1 x1 h2 x2 h3 ... x3 ... → [hn;h1] hi hn MLP xn hi Global-level visual sGate"
2020.coling-main.496,P17-1108,0,0.0123044,"fy the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017;"
2020.coling-main.496,P19-1207,0,0.0147737,"generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ... h1 x1 h2 x2 h3 ... x3 ... → [hn;h1] hi hn MLP xn hi Global-level visual sGate MLP hi sGateo MLP hi ... ... Grid-level visual sGate sGateg Selective Gate (sGate) max-pooling sGateM hi Textual sGate hn＇ sGate2 MLP MLP hi Attention h1＇ sGate1 → yt-1 sGate1 sGate2 MLP MLP max-pooling sGateN ... ... hi MLP hi Object-level visual sGate Figure 2: The framework of our model. We design visual selective gates including global-level, grid"
2020.coling-main.496,P17-1101,0,0.258595,"ion of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017; Zhu et al., 2018;"
2020.coling-main.496,D18-1400,0,0.0969879,"corporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ..."
2020.coling-main.496,D18-1448,1,0.820738,"Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017; Zhu et al., 2018; Chen and Zhuge, 2018), and electronic commerce (e-commerce) product description generation (Chen et al., 2019; Elad et al., 2019; Zhang et al., 2019; Li et al., 2020a), etc. Intuitively, it is easier for a reader to grasp the highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image will benefit text summarization system. Figure 1 illustrates this phenomenon. For a given source sentence, a paired image visualizes a set of event highlight words, which highly correlates with the reference summary. Multimodal sequence-to-sequence (seq2seq) learni"
2020.coling-main.502,N16-1012,0,0.0675896,"with text that contains the most valuable information about products, which is of practical value to address the problem of information overload. In the e-commerce scenarios, unfaithful product summaries that are inconsistent with the corresponding product attributes, e.g., generating “cotton” for a “silk dress”, mislead the users and decrease public credibility of the e-commerce platform. Thus, the faithfulness is a basic requirement for product summarization. Recently, sequence-to-sequence (seq2seq) methods show promising performances for general text summarization tasks (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Li et al., 2018; Zhang et al., 2018; Li et al., 2020b), which has been adopted to text generation tasks in the field of ecommerce (Khatri et al., 2018; Yang et al., 2018; Daultani et al., 2019; Chen et al., 2019; Li et al., 2020a). Although applicable, they do not attempt to improve the faithfulness for product summarization. In this paper, we aim to produce faithful product summaries with heterogeneous data, i.e., textual product description and product attribute table, as shown in Figure 1. First, as the words in product attribute tables are explicit indicators for the p"
2020.coling-main.502,P16-1154,0,0.0370054,"den state at timestep t, and cxt is the source sequence context vector that is generated by attention (Bahdanau et al., 2015) mechanism as follows: x αt,i = softmax(uTa tanh(Wa hxi + Va st−1 )) X x x cxt = αt,i hi i 5713 (2) (3) x is the attention for i-th word in the source at timestep t. Similarly, we can get the attention where αt,i v . We calculate the attribute attention and attribute context vector as follows: over each attribute word αt,i,j k αt,i = softmax(uTa tanh(Wa X X k hki,j αt,i ckt = i X j hki,j + Va st−1 )) (4) (5) j Our model is based on the pointer-generator network (PGNet) (Gu et al., 2016; See et al., 2017) that predicts words based on the probability distributions of the generator and the pointer (Vinyals et al., 2015). The generator produces vocabulary distribution Pgen over a fixed target vocabulary as follows: Pgen (w) = softmax(Wb st + Vb cxt ) (6) The dual-pointer copy the word w from both the source sequence and attribute table. The copy distribution from the source sequence is obtained by the attention distribution over the source sequence: X x αt,i (7) Pcopy (w) = i:xi =w We adopt a coarse-to-fine attention (Liu et al., 2019) to calculate the final copy distribution f"
2020.coling-main.502,C18-1121,1,0.838642,"e information about products, which is of practical value to address the problem of information overload. In the e-commerce scenarios, unfaithful product summaries that are inconsistent with the corresponding product attributes, e.g., generating “cotton” for a “silk dress”, mislead the users and decrease public credibility of the e-commerce platform. Thus, the faithfulness is a basic requirement for product summarization. Recently, sequence-to-sequence (seq2seq) methods show promising performances for general text summarization tasks (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Li et al., 2018; Zhang et al., 2018; Li et al., 2020b), which has been adopted to text generation tasks in the field of ecommerce (Khatri et al., 2018; Yang et al., 2018; Daultani et al., 2019; Chen et al., 2019; Li et al., 2020a). Although applicable, they do not attempt to improve the faithfulness for product summarization. In this paper, we aim to produce faithful product summaries with heterogeneous data, i.e., textual product description and product attribute table, as shown in Figure 1. First, as the words in product attribute tables are explicit indicators for the product attributes, we propose a dual"
2020.coling-main.502,D15-1044,0,0.0835946,"-commerce platform with text that contains the most valuable information about products, which is of practical value to address the problem of information overload. In the e-commerce scenarios, unfaithful product summaries that are inconsistent with the corresponding product attributes, e.g., generating “cotton” for a “silk dress”, mislead the users and decrease public credibility of the e-commerce platform. Thus, the faithfulness is a basic requirement for product summarization. Recently, sequence-to-sequence (seq2seq) methods show promising performances for general text summarization tasks (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Li et al., 2018; Zhang et al., 2018; Li et al., 2020b), which has been adopted to text generation tasks in the field of ecommerce (Khatri et al., 2018; Yang et al., 2018; Daultani et al., 2019; Chen et al., 2019; Li et al., 2020a). Although applicable, they do not attempt to improve the faithfulness for product summarization. In this paper, we aim to produce faithful product summaries with heterogeneous data, i.e., textual product description and product attribute table, as shown in Figure 1. First, as the words in product attribute tables are explicit"
2020.coling-main.502,P17-1099,0,0.0613729,"step t, and cxt is the source sequence context vector that is generated by attention (Bahdanau et al., 2015) mechanism as follows: x αt,i = softmax(uTa tanh(Wa hxi + Va st−1 )) X x x cxt = αt,i hi i 5713 (2) (3) x is the attention for i-th word in the source at timestep t. Similarly, we can get the attention where αt,i v . We calculate the attribute attention and attribute context vector as follows: over each attribute word αt,i,j k αt,i = softmax(uTa tanh(Wa X X k hki,j αt,i ckt = i X j hki,j + Va st−1 )) (4) (5) j Our model is based on the pointer-generator network (PGNet) (Gu et al., 2016; See et al., 2017) that predicts words based on the probability distributions of the generator and the pointer (Vinyals et al., 2015). The generator produces vocabulary distribution Pgen over a fixed target vocabulary as follows: Pgen (w) = softmax(Wb st + Vb cxt ) (6) The dual-pointer copy the word w from both the source sequence and attribute table. The copy distribution from the source sequence is obtained by the attention distribution over the source sequence: X x αt,i (7) Pcopy (w) = i:xi =w We adopt a coarse-to-fine attention (Liu et al., 2019) to calculate the final copy distribution from attribute word"
2020.coling-main.502,C18-1095,0,0.0213421,"t summaries that are inconsistent with the corresponding product attributes, e.g., generating “cotton” for a “silk dress”, mislead the users and decrease public credibility of the e-commerce platform. Thus, the faithfulness is a basic requirement for product summarization. Recently, sequence-to-sequence (seq2seq) methods show promising performances for general text summarization tasks (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Li et al., 2018; Zhang et al., 2018; Li et al., 2020b), which has been adopted to text generation tasks in the field of ecommerce (Khatri et al., 2018; Yang et al., 2018; Daultani et al., 2019; Chen et al., 2019; Li et al., 2020a). Although applicable, they do not attempt to improve the faithfulness for product summarization. In this paper, we aim to produce faithful product summaries with heterogeneous data, i.e., textual product description and product attribute table, as shown in Figure 1. First, as the words in product attribute tables are explicit indicators for the product attributes, we propose a dual-copy mechanism that can selectively copy the tokens in textual product descriptions and product attribute words into the summaries. Second, for the produ"
2020.coling-main.502,P17-1101,0,0.0583451,"ns the most valuable information about products, which is of practical value to address the problem of information overload. In the e-commerce scenarios, unfaithful product summaries that are inconsistent with the corresponding product attributes, e.g., generating “cotton” for a “silk dress”, mislead the users and decrease public credibility of the e-commerce platform. Thus, the faithfulness is a basic requirement for product summarization. Recently, sequence-to-sequence (seq2seq) methods show promising performances for general text summarization tasks (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Li et al., 2018; Zhang et al., 2018; Li et al., 2020b), which has been adopted to text generation tasks in the field of ecommerce (Khatri et al., 2018; Yang et al., 2018; Daultani et al., 2019; Chen et al., 2019; Li et al., 2020a). Although applicable, they do not attempt to improve the faithfulness for product summarization. In this paper, we aim to produce faithful product summaries with heterogeneous data, i.e., textual product description and product attribute table, as shown in Figure 1. First, as the words in product attribute tables are explicit indicators for the product attributes,"
2020.coling-main.510,D17-1181,0,0.0649095,"Missing"
2020.coling-main.510,P17-1152,0,0.228905,"urther proposes a hybrid attention scheme which includes an instance-level attention and a feature-level attention, where the former is used to highlight the crucial support sentences in calculating the prototype, and the latter is to select more efficient features when calculating distances. MLMAN Different from the Proto and Proto-HATT, MLMAN encodes each query and the supporting set in an interactive way by considering their matching information on multiple levels. At local level, the representations of an instance and a supporting set are matched following the sentence matching framework (Chen et al., 2017b) and aggregated by max and average pooling. At instance level, the matching degree is first calculated via a multi-layer perception (MLP). Then, taking the matching degrees as weights, the instances in a supporting set are aggregated to obtain the class prototype for final classification. BERT-PAIR This model is based on the sentence classification model in BERT. The sentence to be classified is first paired with all the supporting instances, and then each pair is concatenated to a sequence. BERT takes this sequence as input and returns a relevance score, which is used to measure whether the"
2020.coling-main.510,P19-1012,0,0.0489319,"Missing"
2020.coling-main.510,D19-1649,0,0.305434,"ng the relations with insufficient instances. Therefore, making the RC models capable of identifying relations with few training instances becomes a crucial challenge. Inspired by the success of few-shot learning methods in the computer vision community (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016) and some other natural language processing tasks (Chen et al., 2016; Qin et al., 2020; Zhou et al., 2019), Han et al. (2018) first introduce the few-shot learning to RC task and propose the FewRel dataset. Recently, many works focus on this task and achieve remarkable performance (Gao et al., 2019a; Snell et al., 2017; Ye and Ling, 2019). Previous few-shot relation classifiers perform well on sentences with only one relation of a single entity pair. However, in real natural language, a sentence usually jointly describes multiple relations of different entity pairs. Since these relations usually keep high co-occurrence in the same context, previous few-shot RC models struggle to distinguish them with few annotated instances. For example, Table 1 shows three instances from the FewRel dataset, where each sentence describes multiple relations with corresponding keyphrases highlighted (colo"
2020.coling-main.510,D18-1514,0,0.25064,"on between two specified entities in a sentence. Previous supervised approaches on this task heavily depend on human-annotated data, which limit their performance on classifying the relations with insufficient instances. Therefore, making the RC models capable of identifying relations with few training instances becomes a crucial challenge. Inspired by the success of few-shot learning methods in the computer vision community (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016) and some other natural language processing tasks (Chen et al., 2016; Qin et al., 2020; Zhou et al., 2019), Han et al. (2018) first introduce the few-shot learning to RC task and propose the FewRel dataset. Recently, many works focus on this task and achieve remarkable performance (Gao et al., 2019a; Snell et al., 2017; Ye and Ling, 2019). Previous few-shot relation classifiers perform well on sentences with only one relation of a single entity pair. However, in real natural language, a sentence usually jointly describes multiple relations of different entity pairs. Since these relations usually keep high co-occurrence in the same context, previous few-shot RC models struggle to distinguish them with few annotated i"
2020.coling-main.510,P19-1135,0,0.0302267,"/oPos” CAT and w/o relative position and the syntax position bring significant improvements. In addition, compared with our full model, the performance of “w/o CAT” proves that the CAT help to decouple the confusing relations. 4 Related Work Few-shot Relation Classification Relation classification (RC) aims to identify the semantic relation between two entities in a sentence, which is the basis of many natural language processing task, such as question answering (Yu et al., 2017) and knowledge graph completion (Shang et al., 2019). It has attracted more and more attention over past few years (Jia et al., 2019; Feng et al., 2018; Vinyals et al., 2018; Adel and Sch¨utze, 2017; Yang et al., 2016a). Previous supervised approaches on this task heavily rely on labeled data for training, that limits their ability to classify the relations with insufficient instances. To address this problem, Han et al. (2018) first introduce few-shot learning to RC task, which has been proved effective in the computer vision community and has many applications (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016). Earlier works on few-shot RC are based on the widely used model prototypical network (Snell et al."
2020.coling-main.510,2020.acl-main.512,0,0.0425707,"re-trained LM BERT (Devlin et al., 2018) to few-shot RC, and their work shows that BERT brings significant improvements on classification performance. Furthermore, the approach proposed by Soares et al. (2019) are also based on BERT and achieve the state-of-art result on the few-shot RC task. Syntactic Relation Previous RC models usually use the relative position information to identify which words are the entities in a sentence, e.g., Zeng et al. (2015b). In addition, the syntax information of the sentences is proved useful in many natural language processing tasks (Fale´nska and Kuhn, 2019; Ma et al., 2020; Chen et al., 2017a). Inspired by Yang et al. (2016b), which adopt the dependency parse tree for RC (Ma et al., 2020), we also introduce the dependency relation as another type of position to emphasize the specific entities, and propose a novel application of the syntax positions. 5 Conclusions In this paper, we propose CTEG equipped with two novel mechanisms, namely the Entity-Guided Attention (EGA) and the Confusion-Aware Training (CAT), to address the relation confusion problem in few-shot relation classification (RC). We conduct extensive experiments on benchmark dataset FewRel, and exper"
2020.coling-main.510,P09-1113,0,0.13173,"d attention (EGA) and confusion-aware training (CAT) through the ablation studies in Section 3.4. In order to more intuitively and clearly show the role of EGA and CAT, we show their visualized examples in case study in Section 3.5. Furthermore, we verify that our model is capable of addressing the relation confusion problem to some extent in Section 3.6. 3.1 Implementation Details Dataset The FewRel dataset (Han et al., 2018) contains 100 relations, which are split up into 64 for training, 16 for validation and 20 for testing. Each relation has 700 instances generated by distant supervision (Mintz et al., 2009). All the instances are annotated with a specified entity pair. Settings The dimension of word embedding is set to 768 for consistency with the base model of BERT (Devlin et al., 2018). The max length of the input is set to 100. Following BERT, the layer number M of the transformer encoder with EGA is 12, and all parameters in it is initialized with the pretrained BERT model. The relative position and syntactic relation embedding dimensions are both set to 50, and the transformer encoder for obtaining entity-guided gates is set up with hidden size as 230, head number of self-attention as 2. In"
2020.coling-main.510,P19-1279,0,0.0191409,"on community and has many applications (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016). Earlier works on few-shot RC are based on the widely used model prototypical network (Snell et al., 2017; Ye and Ling, 2019). Recently, the pre-trained language models (LM) has shown significant power in many natural language processing tasks. To this end, Gao et al. (2019c) adopt the most representative pre-trained LM BERT (Devlin et al., 2018) to few-shot RC, and their work shows that BERT brings significant improvements on classification performance. Furthermore, the approach proposed by Soares et al. (2019) are also based on BERT and achieve the state-of-art result on the few-shot RC task. Syntactic Relation Previous RC models usually use the relative position information to identify which words are the entities in a sentence, e.g., Zeng et al. (2015b). In addition, the syntax information of the sentences is proved useful in many natural language processing tasks (Fale´nska and Kuhn, 2019; Ma et al., 2020; Chen et al., 2017a). Inspired by Yang et al. (2016b), which adopt the dependency parse tree for RC (Ma et al., 2020), we also introduce the dependency relation as another type of position to e"
2020.coling-main.510,D18-1250,0,0.0221617,"and the syntax position bring significant improvements. In addition, compared with our full model, the performance of “w/o CAT” proves that the CAT help to decouple the confusing relations. 4 Related Work Few-shot Relation Classification Relation classification (RC) aims to identify the semantic relation between two entities in a sentence, which is the basis of many natural language processing task, such as question answering (Yu et al., 2017) and knowledge graph completion (Shang et al., 2019). It has attracted more and more attention over past few years (Jia et al., 2019; Feng et al., 2018; Vinyals et al., 2018; Adel and Sch¨utze, 2017; Yang et al., 2016a). Previous supervised approaches on this task heavily rely on labeled data for training, that limits their ability to classify the relations with insufficient instances. To address this problem, Han et al. (2018) first introduce few-shot learning to RC task, which has been proved effective in the computer vision community and has many applications (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016). Earlier works on few-shot RC are based on the widely used model prototypical network (Snell et al., 2017; Ye and Ling, 2019). Recently, the"
2020.coling-main.510,D16-1007,0,0.330763,"vements. In addition, compared with our full model, the performance of “w/o CAT” proves that the CAT help to decouple the confusing relations. 4 Related Work Few-shot Relation Classification Relation classification (RC) aims to identify the semantic relation between two entities in a sentence, which is the basis of many natural language processing task, such as question answering (Yu et al., 2017) and knowledge graph completion (Shang et al., 2019). It has attracted more and more attention over past few years (Jia et al., 2019; Feng et al., 2018; Vinyals et al., 2018; Adel and Sch¨utze, 2017; Yang et al., 2016a). Previous supervised approaches on this task heavily rely on labeled data for training, that limits their ability to classify the relations with insufficient instances. To address this problem, Han et al. (2018) first introduce few-shot learning to RC task, which has been proved effective in the computer vision community and has many applications (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016). Earlier works on few-shot RC are based on the widely used model prototypical network (Snell et al., 2017; Ye and Ling, 2019). Recently, the pre-trained language models (LM) has shown"
2020.coling-main.510,P19-1277,0,0.48141,"ances. Therefore, making the RC models capable of identifying relations with few training instances becomes a crucial challenge. Inspired by the success of few-shot learning methods in the computer vision community (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016) and some other natural language processing tasks (Chen et al., 2016; Qin et al., 2020; Zhou et al., 2019), Han et al. (2018) first introduce the few-shot learning to RC task and propose the FewRel dataset. Recently, many works focus on this task and achieve remarkable performance (Gao et al., 2019a; Snell et al., 2017; Ye and Ling, 2019). Previous few-shot relation classifiers perform well on sentences with only one relation of a single entity pair. However, in real natural language, a sentence usually jointly describes multiple relations of different entity pairs. Since these relations usually keep high co-occurrence in the same context, previous few-shot RC models struggle to distinguish them with few annotated instances. For example, Table 1 shows three instances from the FewRel dataset, where each sentence describes multiple relations with corresponding keyphrases highlighted (colored) as evidence. When specified two enti"
2020.coling-main.510,D15-1203,0,0.314328,"inspired by the success of pre-trained language models, our approaches are based on BERT (Devlin et al., 2018), which has been proved effective especially for few-shot learning tasks. Specifically, the backbone of the encoder of our model is a transformer equipped with the proposed EGA which guides the calculation of self-attention distributions by weighting the attention logits with entity-guided gates. The gates are used to measure the relevance between each word and the given two entities. Two types of information for each word are used to calculate its gate. One is the relative position (Zeng et al., 2015a) information, which is the relative distance between a word and an entity in the input sequence. The other is syntactic relation which is proposed in this paper, defined as the dependency relations between each word and the entities. Based on these information, the entity-guided gates in EGA are able to select those important words and control the contribution of each word in self-attention. We also propose CAT to explicitly force the model to asynchronously learn the classification from an instance to its true relation and its confusing relation. After each training step, the CAT first sele"
2020.emnlp-main.166,N18-2118,0,0.0293538,"the textual product description, and we label the values by exactly matching. We denote this subset of the MAE dataset as MAE-text and the rest as MAE-image (values can be only inferred by the images). 4 Experiment We compare our proposed methods with the following baselines: WSM is the method that uses attribute values in the training set to retrieve the attribute values in the testing set by word matching. Sep-BERT is the pretrained BERT model with feed-forward layers to perform these two subtasks separately. RNN-LSTM (Hakkani-T¨ur et al., 2016), Attn-BiRNN (Liu and Lane, 2016), SlotGated (Goo et al., 2018), and Joint-BERT (Chen et al., 2019) are the models to address intent classification and slot filling tasks, which are similar to the attribute prediction and value extraction, and Value 768 2048 49 (7*7) 200 46 0.0001 sigmoid 0.5 128 50 112M 1x NVIDIA Tesla P40 50 minutes Table 2: Details about hyper-parameters. Model WSM Sep-BERT RNN-LSTM (Hakkani-T¨ur et al., 2016) Attn-BiRNN (Liu and Lane, 2016) Slot-Gated (Goo et al., 2018) Joint-BERT (Chen et al., 2019) ScalingUp (Xu et al., 2019) JAVE (LSTM based) JAVE (BERT based) M-JAVE (LSTM based) M-JAVE (BERT based) Attribute 77.20 86.34 85.76 86.1"
2020.emnlp-main.166,N19-1423,0,0.0311235,"the corresponding values for ecommerce products. The input of the task is a “textual product description, product image” pair, and the outputs are the product attributes (there may be more than one attribute in the descriptions) and the corresponding values. We model the product attribute prediction task as a sequence-level multilabel classification task and the value extraction task as a sequence labeling task. The framework of our proposed Multimodal Joint Attribute Prediction and Value Extraction model (M-JAVE) is shown in Figure 2. The input sentence is encoded by a pretrained BERT model (Devlin et al., 2019), and the image is encoded by a pretrained ResNet model (He et al., 2016). The global-gated cross-modality attention layer encodes text and image into the multimodal hidden representations. Then, the M-JAVE model predicts the product attributes based on the multimodal representations. Next, the model extracts the values based on the previously predicted product attributes and the multimodal representations obtained through the regional-gated cross-modality attention layer. We apply the multitask learning framework to jointly model the product attribute prediction and value extraction. Consider"
2020.emnlp-main.166,D18-1329,0,0.028579,"M-JAVE w/o Regional Visual Gate underperform the models thoroughly removing visual-related modules. To sum up, using the visual product information indiscriminately poses detrimental effects on the model, and selectively utilizing visual product information with global and regional visual gates are essential for our tasks. 2135 4.3 Adversarial Evaluation of Attribute Prediction and Value Extraction Value Attribute To further verify whether the visual product information can improve the performance of product attribute prediction and value extraction, we adopt an adversarial evaluation method (Elliott, 2018) that measures the performance variation when our model is presented with a random incongruent image. The awareness score of a model M on an evaluation dataset D is defined as follows: |D| ∆Awareness = 1 X aM (xi , yi , vi , v¯i ) |D| (15) i Where ∆Awareness denotes the image awareness. x, y denote the the text and the values of the product, respectively. v, ¯v denote the congruent image and the incongruent image, respectively. We use the F1 score to calculate awareness score for a single instance: aM = F1 (xi , yi , vi ) − F1 (xi , yi , v¯i ) (16) Under this definition, the output of the eval"
2020.emnlp-main.166,D17-1114,1,0.821444,"d with black shoes”, the term “golden” can be ambiguous for predicting the product attributes. While by viewing the product image, we can easily recognize the attribute corresponding to “golden” is “Color” instead of “Material”. Moreover, the product image can indicate that the term “black” is not an attribute value of the current product; thus, it should not be extracted. This may be tricky for the model based on purely textual descriptions, but leveraging the visual information can make it easier. In addition, multimodal information shows promising efficiency on many tasks (Lu et al., 2016; Li et al., 2017; Anderson et al., 2018; Li et al., 2018; Yu et al., 2019; Li et al., 2019; Tan and Bansal, 2019; Liu et al., 2019; Su et al., 2020; Li et al., 2020). Therefore, we propose to incorporate visual information into our task. First, we selectively enhance the semantic representation of the textual product descriptions with a global-gated cross-modality attention module that is anticipated to benefit attribute prediction task with visually grounded semantics. Moreover, for different values, our model selectively utilizes visual information with a regional-gated cross-modality attention module to im"
2020.emnlp-main.166,N19-2005,0,0.0281588,"he product image, we can easily recognize the attribute corresponding to “golden” is “Color” instead of “Material”. Moreover, the product image can indicate that the term “black” is not an attribute value of the current product; thus, it should not be extracted. This may be tricky for the model based on purely textual descriptions, but leveraging the visual information can make it easier. In addition, multimodal information shows promising efficiency on many tasks (Lu et al., 2016; Li et al., 2017; Anderson et al., 2018; Li et al., 2018; Yu et al., 2019; Li et al., 2019; Tan and Bansal, 2019; Liu et al., 2019; Su et al., 2020; Li et al., 2020). Therefore, we propose to incorporate visual information into our task. First, we selectively enhance the semantic representation of the textual product descriptions with a global-gated cross-modality attention module that is anticipated to benefit attribute prediction task with visually grounded semantics. Moreover, for different values, our model selectively utilizes visual information with a regional-gated cross-modality attention module to improve the accuracy of values extraction. Our main contributions are threefold: 2130 • We propose an end-to-end mod"
2020.emnlp-main.166,D11-1144,0,0.229489,"to our statistics on a mainstream e-commerce platform in China, there are over 40 attributes for the products in clothing category, but the average count of attributes present for each product is fewer than 8. The absence of the product attributes seriously affects customers’ shopping experience and reduces the potential of successful trading. In this paper, we propose a method to jointly predict product attributes and extract the corresponding values with multimodal product information, as shown in Figure 1. Though plenty of systems have been proposed to supplement product attribute values (Putthividhya and Hu, 2011; More, 2016; Shinzato and Sekine, 2013; Zheng et al., 2018; Xu et al., 2019), the relationship between product attributes and values are not sufficiently explored, and most of these approaches primarily focus on the text information. Attributes and values are, however, known to strongly depend on each other, and vision can play a particularly essential role for this task. 2129 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2129–2139, c November 16–20, 2020. 2020 Association for Computational Linguistics Attribute Prediction Value Extraction Mater"
2020.emnlp-main.166,I13-1190,0,0.0235928,"mmerce platform in China, there are over 40 attributes for the products in clothing category, but the average count of attributes present for each product is fewer than 8. The absence of the product attributes seriously affects customers’ shopping experience and reduces the potential of successful trading. In this paper, we propose a method to jointly predict product attributes and extract the corresponding values with multimodal product information, as shown in Figure 1. Though plenty of systems have been proposed to supplement product attribute values (Putthividhya and Hu, 2011; More, 2016; Shinzato and Sekine, 2013; Zheng et al., 2018; Xu et al., 2019), the relationship between product attributes and values are not sufficiently explored, and most of these approaches primarily focus on the text information. Attributes and values are, however, known to strongly depend on each other, and vision can play a particularly essential role for this task. 2129 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2129–2139, c November 16–20, 2020. 2020 Association for Computational Linguistics Attribute Prediction Value Extraction Material, Collar Type ya Value of Material O"
2020.emnlp-main.166,D19-1514,0,0.0200863,"es. While by viewing the product image, we can easily recognize the attribute corresponding to “golden” is “Color” instead of “Material”. Moreover, the product image can indicate that the term “black” is not an attribute value of the current product; thus, it should not be extracted. This may be tricky for the model based on purely textual descriptions, but leveraging the visual information can make it easier. In addition, multimodal information shows promising efficiency on many tasks (Lu et al., 2016; Li et al., 2017; Anderson et al., 2018; Li et al., 2018; Yu et al., 2019; Li et al., 2019; Tan and Bansal, 2019; Liu et al., 2019; Su et al., 2020; Li et al., 2020). Therefore, we propose to incorporate visual information into our task. First, we selectively enhance the semantic representation of the textual product descriptions with a global-gated cross-modality attention module that is anticipated to benefit attribute prediction task with visually grounded semantics. Moreover, for different values, our model selectively utilizes visual information with a regional-gated cross-modality attention module to improve the accuracy of values extraction. Our main contributions are threefold: 2130 • We propose"
2020.emnlp-main.166,P19-1514,0,0.230766,"ributes for the products in clothing category, but the average count of attributes present for each product is fewer than 8. The absence of the product attributes seriously affects customers’ shopping experience and reduces the potential of successful trading. In this paper, we propose a method to jointly predict product attributes and extract the corresponding values with multimodal product information, as shown in Figure 1. Though plenty of systems have been proposed to supplement product attribute values (Putthividhya and Hu, 2011; More, 2016; Shinzato and Sekine, 2013; Zheng et al., 2018; Xu et al., 2019), the relationship between product attributes and values are not sufficiently explored, and most of these approaches primarily focus on the text information. Attributes and values are, however, known to strongly depend on each other, and vision can play a particularly essential role for this task. 2129 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2129–2139, c November 16–20, 2020. 2020 Association for Computational Linguistics Attribute Prediction Value Extraction Material, Collar Type ya Value of Material O Bm O Value of Collar Type O O O Bc O"
2020.emnlp-main.166,P15-1128,1,0.735642,"l results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset are available at https://github. com/jd-aig/JAVE. 1 shirt can be paired with black shoes …… ” Product attribute values that provide details of the product are crucial parts of e-commerce, which help customers to make purchasing decisions and facilitate retailers on many applications, such as question answering system (Yih et al., 2015; Yu et al., 2017), product recommendations (Gong, Corresponding author. Value Collar Type lapel Color golden Figure 1: An example of predicting attributes and extracting values from the textual product description with the aid of the visual product information. Introduction ∗ Attribute 2009; Cao et al., 2018), and product retrieval (Liao et al., 2018; Magnani et al., 2019). While product attribute values are pervasively incomplete for a massive number of products on the e-commerce platform. According to our statistics on a mainstream e-commerce platform in China, there are over 40 attributes"
2020.emnlp-main.166,P17-1053,1,0.843333,"dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset are available at https://github. com/jd-aig/JAVE. 1 shirt can be paired with black shoes …… ” Product attribute values that provide details of the product are crucial parts of e-commerce, which help customers to make purchasing decisions and facilitate retailers on many applications, such as question answering system (Yih et al., 2015; Yu et al., 2017), product recommendations (Gong, Corresponding author. Value Collar Type lapel Color golden Figure 1: An example of predicting attributes and extracting values from the textual product description with the aid of the visual product information. Introduction ∗ Attribute 2009; Cao et al., 2018), and product retrieval (Liao et al., 2018; Magnani et al., 2019). While product attribute values are pervasively incomplete for a massive number of products on the e-commerce platform. According to our statistics on a mainstream e-commerce platform in China, there are over 40 attributes for the products i"
2020.findings-emnlp.141,D13-1180,0,0.151751,"ssociation for Computational Linguistics: EMNLP 2020, pages 1560–1569 c November 16 - 20, 2020. 2020 Association for Computational Linguistics long-distance dependency. Besides, shallow neural networks trained on a small volume of labeled data are hard to learn deep semantics. As for score constraints, prediction and ranking are two popular solutions. From the prediction perspective, the task is a regression or classiﬁcation problem (Taghipour and Ng, 2016; Tay et al., 2018; Dong et al., 2017). Besides, from the recommendation perspective, learning-to-rank methods (Yannakoudakis et al., 2011; Chen and He, 2013) aim to rank all essays in the same order as that ranked by gold scores. However, without precise score mapping functions, only regression constraints could not ensure the right ranking order. And only ranking based models could not guarantee accurate scores. In general, there are two key challenges for the AES task. One is how to learn better essay representations to evaluate the writing quality, the other one is how to learn a more accurate score mapping function. Motivated by the great success of pre-trained language models such as BERT in learning text representations with deep semantics,"
2020.findings-emnlp.141,D19-1383,0,0.0260784,"em, e-rater (Chodorow and Burstein, 2004), has been used to score TOEFL writings. Recently, large pre-trained language models, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), etc. have shown the extraordinary ability of representation and generalization. These models have gained better performance in lots of downstream tasks such as text classiﬁcation and regression. There are many new approaches to ﬁne-tune pre-trained language models. Sun et al. (2019a) proposed to construct an auxiliary sentence to solve aspect-based sentiment classiﬁcation tasks. Cohan et al. (2019) added extra separate tokens to obtain representations of each sentence to solve sequential sentence classiﬁcation tasks. Sun et al. (2019b) summarized several ﬁne-tuning methods, including fusing text representations from different layers, utilizing multi-task learning, etc. To our knowledge, there are no existing works to improve AES tasks with pre-trained language models. Before introducing our new way to use pre-trained language models, we brieﬂy review existing works in AES ﬁrstly. Existing works utilize different methods to learn text representations and constrain scores, which are the t"
2020.findings-emnlp.141,P18-2080,0,0.215435,"ch as statistical features and linguistic features. There are several early AES systems including e-rater (Chodorow and Burstein, 2004), PEG (Project Essay Grade) (Shermis and Burstein, 2003), and IntelliMetric (Elliot, 2003). e-rater utilized ten linguistic features, including eight representing aspects of writing quality and two representing content. PEG used a larger feature set with more than 30 elements of writing quality. IntelliMetric aggregated all the features into ﬁve types, namely Focus/Coherence, Organization, Elaboration/Development, Sentence Structure, and Mechanics/Conventions. Cozma et al. (2018) combined string kernel and word embeddings to extract features. With the success of deep learning, researchers start to utilize various neural networks to learn text representations. Taghipour and Ng (2016) explored several neural networks, such as Long Short-Term Memory (LSTM) and CNN. Finally, they found that the ensemble model combining LSTM and CNN performs best. Dong et al. (2017) proposed a hierarchical text model that utilized CNN to learn sentence representations, and LSTM was used to learn text representations. Tay et al. (2018) introduced a model called SKIPFLOW, which aimed to capt"
2020.findings-emnlp.141,N19-1423,0,0.178355,"Essay assignments evaluation costs lots of time. Besides, the same instructor scoring the same essay at different times may assign different scores (intra-rater variation), different raters scoring the same essay may assign different scores (inter-rater variation) (Smolentzov, 2013). To alleviate teachers’ burden and avoid intra-rater variation, as well as interrater variation, AES is necessary and essential. An early AES system, e-rater (Chodorow and Burstein, 2004), has been used to score TOEFL writings. Recently, large pre-trained language models, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), etc. have shown the extraordinary ability of representation and generalization. These models have gained better performance in lots of downstream tasks such as text classiﬁcation and regression. There are many new approaches to ﬁne-tune pre-trained language models. Sun et al. (2019a) proposed to construct an auxiliary sentence to solve aspect-based sentiment classiﬁcation tasks. Cohan et al. (2019) added extra separate tokens to obtain representations of each sentence to solve sequential sentence classiﬁcation tasks. Sun et al. (2019b) summarized several ﬁne-tuning"
2020.findings-emnlp.141,K17-1017,0,0.280681,"orks to improve AES tasks with pre-trained language models. Before introducing our new way to use pre-trained language models, we brieﬂy review existing works in AES ﬁrstly. Existing works utilize different methods to learn text representations and constrain scores, which are the two key steps in AES models. For text representation learning, various neural networks are used to learn essay representations, such as Recurrent Neural Network (RNN) (Taghipour and Ng, 2016; Tay et al., 2018), Convolutional Neural Network (CNN) (Taghipour and Ng, 2016), Recurrent Convolutional Neural Network (RCNN) (Dong et al., 2017), etc. However, simple neural networks like RNN and CNN focus on word-level information, which is difﬁcult to capture word connections in 1560 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1560–1569 c November 16 - 20, 2020. 2020 Association for Computational Linguistics long-distance dependency. Besides, shallow neural networks trained on a small volume of labeled data are hard to learn deep semantics. As for score constraints, prediction and ranking are two popular solutions. From the prediction perspective, the task is a regression or classiﬁcation problem (Ta"
2020.findings-emnlp.141,D14-1162,0,0.0860772,"of histogram vectors of the two ratings. The matrix E is then normalized such that the sum of elements in E is the same as that of elements in O. Finally, with given matrices O and E, the QWK score is calculated according to Formula 12. • LSTM We use two layers LSTM and biLSTM, as well as mean pooling and last output to obtain the essay representations. Mean pooling means the average vector of all the hidden states, while the last output refers to the last hidden state. Then, a fully connected linear layer, as well as σ activation function, is used to gain scores. In these four models, GloVe (Pennington et al., 2014) is used to initialize the word embedding matrix, and the dimension is 300. Wi,j = (i − j)2 (N − 1)2  Wi,j Oi,j i,j Wi,j Ei,j i,j κ=1−  4.4 (12) Baselines and Implementation Details In this section, we list several baseline models as well as state-of-the-art models. • *EASE A statistical model called Enhanced AI Scoring Engine (EASE) is an AES system that is publicly available, open-source3 , and also achieved excellent results in the ASAP competition. EASE utilizes hand-crafted features such as length-based features, POS tags, and word overlap, as well as different regression techniques. Fo"
2020.findings-emnlp.141,W13-1705,0,0.0207157,"dataset. Our model outperforms not only state-of-the-art neural models near 3 percent but also the latest statistic model. Especially on the two narrative prompts, our model performs much better than all other state-of-theart models. 1 Introduction Automated Essay Scoring (AES) automatically evaluates the writing quality of essays. Essay assignments evaluation costs lots of time. Besides, the same instructor scoring the same essay at different times may assign different scores (intra-rater variation), different raters scoring the same essay may assign different scores (inter-rater variation) (Smolentzov, 2013). To alleviate teachers’ burden and avoid intra-rater variation, as well as interrater variation, AES is necessary and essential. An early AES system, e-rater (Chodorow and Burstein, 2004), has been used to score TOEFL writings. Recently, large pre-trained language models, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), etc. have shown the extraordinary ability of representation and generalization. These models have gained better performance in lots of downstream tasks such as text classiﬁcation and regression. There are many new approaches to ﬁne-tun"
2020.findings-emnlp.141,N19-1035,0,0.134585,"and avoid intra-rater variation, as well as interrater variation, AES is necessary and essential. An early AES system, e-rater (Chodorow and Burstein, 2004), has been used to score TOEFL writings. Recently, large pre-trained language models, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), etc. have shown the extraordinary ability of representation and generalization. These models have gained better performance in lots of downstream tasks such as text classiﬁcation and regression. There are many new approaches to ﬁne-tune pre-trained language models. Sun et al. (2019a) proposed to construct an auxiliary sentence to solve aspect-based sentiment classiﬁcation tasks. Cohan et al. (2019) added extra separate tokens to obtain representations of each sentence to solve sequential sentence classiﬁcation tasks. Sun et al. (2019b) summarized several ﬁne-tuning methods, including fusing text representations from different layers, utilizing multi-task learning, etc. To our knowledge, there are no existing works to improve AES tasks with pre-trained language models. Before introducing our new way to use pre-trained language models, we brieﬂy review existing works in A"
2020.findings-emnlp.141,D16-1193,0,0.136293,"ing methods, including fusing text representations from different layers, utilizing multi-task learning, etc. To our knowledge, there are no existing works to improve AES tasks with pre-trained language models. Before introducing our new way to use pre-trained language models, we brieﬂy review existing works in AES ﬁrstly. Existing works utilize different methods to learn text representations and constrain scores, which are the two key steps in AES models. For text representation learning, various neural networks are used to learn essay representations, such as Recurrent Neural Network (RNN) (Taghipour and Ng, 2016; Tay et al., 2018), Convolutional Neural Network (CNN) (Taghipour and Ng, 2016), Recurrent Convolutional Neural Network (RCNN) (Dong et al., 2017), etc. However, simple neural networks like RNN and CNN focus on word-level information, which is difﬁcult to capture word connections in 1560 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1560–1569 c November 16 - 20, 2020. 2020 Association for Computational Linguistics long-distance dependency. Besides, shallow neural networks trained on a small volume of labeled data are hard to learn deep semantics. As for score co"
2020.findings-emnlp.141,W18-5446,0,0.0560956,"Missing"
2020.findings-emnlp.141,D18-1090,0,0.248345,"aimed to capture neural coherence features of the text via considering the adjacent hidden states in the LSTM model. In the recommendation view, learning to rank approaches is another popular method to solve this task. Yannakoudakis et al. (2011) ﬁrstly addressed this problem as a rank preference problem. Based on statistical features, RankSVM, a pairwise learning to rank model, was used as score constraint. Chen and He (2013) utilized listwise learning to rank model to learn a ranking model based on several linguistic features. Reinforcement learning based models are also possible solutions. Wang et al. (2018b) utilized dilated LSTM to learn text representations. Then scores calculation was guided by quadratic weighted kappa based reward function. For text representation, previous works only consider the relations among sentences. In this paper, we focus on all the interactions between any two words. Besides, existing works only utilize regression or ranking loss, respectively. We combine two losses dynamically in our model. 3 R2 BERT In this section, we ﬁrst introduce the framework of our model, brieﬂy review the BERT model, as well as self-attention. In addition, we will illustrate the regressio"
2020.findings-emnlp.141,P11-1019,0,0.104806,"ns in 1560 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1560–1569 c November 16 - 20, 2020. 2020 Association for Computational Linguistics long-distance dependency. Besides, shallow neural networks trained on a small volume of labeled data are hard to learn deep semantics. As for score constraints, prediction and ranking are two popular solutions. From the prediction perspective, the task is a regression or classiﬁcation problem (Taghipour and Ng, 2016; Tay et al., 2018; Dong et al., 2017). Besides, from the recommendation perspective, learning-to-rank methods (Yannakoudakis et al., 2011; Chen and He, 2013) aim to rank all essays in the same order as that ranked by gold scores. However, without precise score mapping functions, only regression constraints could not ensure the right ranking order. And only ranking based models could not guarantee accurate scores. In general, there are two key challenges for the AES task. One is how to learn better essay representations to evaluate the writing quality, the other one is how to learn a more accurate score mapping function. Motivated by the great success of pre-trained language models such as BERT in learning text representations w"
2020.lrec-1.58,P96-1009,0,0.175808,"e uncertainties make dialogue task extremely different from traditional machine learning tasks which usually have explicit targets and clearly defined evaluation metrics. To tackle this challenging problem, constructing a dialogue dataset is the most essential work. Especially for popular deep learning based approaches, large scale of training corpus in real scenario becomes decisive. However, existing datasets are still deficient. Datasets with structured annotations (e.g., slots and corresponding values) are often smallscale and in a limited capacity. Either traditional domainspecific ones (Allen et al., 1996; Petukhova et al., 2014; Bordes et al., 2016; Dodge et al., 2015) or recent multidomain ones (Budzianowski et al., 2018; Shah et al., 2018; El Asri et al., 2017) are usually built for task-oriented dialogue systems. Another typical branch of works collects the dialogue corpus from movie subtitles, such as OpenSubtitles (Tiedemann, 2009) and Cornell (Danescu-NiculescuMizil and Lee, 2011), which contain long sessions (over 100 turns) and some expressions like individual monologues may be not suitable for dialogue systems. More recently, some researchers construct dialogue datasets from social m"
2020.lrec-1.58,D18-1547,0,0.0642854,"Missing"
2020.lrec-1.58,W11-0609,0,0.0813466,"Missing"
2020.lrec-1.58,N19-1423,0,0.084852,"veral candidate answers (10 candidate answers for Challenge Set I and II, and 3 for Challenge Set III), as for dialogue task, the groundtruth is usually not limited to one. What’s more, different weights are provided for each candidate answer, so evaluation metrics (e.g., BLEU score) can be calculated more accurately. We hope these 3 challenge sets can help evaluate the dialogue systems on a fine-grained level. 4. In this section, we conduct experiments on the JDDC dataset. We focus on two categories of models used in datadriven dialogue systems: retrieval-based models based on BM25 and BERT (Devlin et al., 2019) and generative models (Gu et al., 2016). We will introduce some empirical settings, including dataset preparation, baseline methods, parameter settings. Then we introduce the experimental results on this dataset. 4.1. Figure 3: The explanation of our 3 challenge sets. The responses (r) in red color are required to be answered by the dialogue system. http://jddc.jd.com/ Experimental Setup We first divide the around 1 million conversation sessions into training, validation and testing set. Then we construct I-R pairs from each set into the {I, R} = {q1 , r1 , q2 , r2 , Q, R} format, where I = {"
2020.lrec-1.58,W17-5526,0,0.0668821,"Missing"
2020.lrec-1.58,P16-1154,0,0.171491,"s for Challenge Set I and II, and 3 for Challenge Set III), as for dialogue task, the groundtruth is usually not limited to one. What’s more, different weights are provided for each candidate answer, so evaluation metrics (e.g., BLEU score) can be calculated more accurately. We hope these 3 challenge sets can help evaluate the dialogue systems on a fine-grained level. 4. In this section, we conduct experiments on the JDDC dataset. We focus on two categories of models used in datadriven dialogue systems: retrieval-based models based on BM25 and BERT (Devlin et al., 2019) and generative models (Gu et al., 2016). We will introduce some empirical settings, including dataset preparation, baseline methods, parameter settings. Then we introduce the experimental results on this dataset. 4.1. Figure 3: The explanation of our 3 challenge sets. The responses (r) in red color are required to be answered by the dialogue system. http://jddc.jd.com/ Experimental Setup We first divide the around 1 million conversation sessions into training, validation and testing set. Then we construct I-R pairs from each set into the {I, R} = {q1 , r1 , q2 , r2 , Q, R} format, where I = {C, Q} stands for input, C = {q1 , r1 , q"
2020.lrec-1.58,P17-4012,0,0.0116607,"e and negative sample is 1:1. Then the constructed positive and negative I-R pairs are used to fine-tune the BERT model. Our model implementation for BERT is based on Google’s work (Devlin et al., 2019) and follows the hyperparameter settings in the original model. For generative models, we first clean the training set to decrease the portion of short responses (shorter than 3 Chinese characters) and generic responses (e.g., “What else can I do for you?”). Then all remaining I-R pairs are used for training the model. Our code implementation is based on the machine translation toolkit OpenNMT (Klein et al., 2017). In all generative experiments, we set 100,000 for vocabulary size and 200 for word embedding dimension. The source length is 128 and target length is decreased to 40 to avoid generating too long response. Other training parameters are set as default. 4.2. Comparable Models In this subsection, we will introduce the detailed information on retrieval-based models and generative models used for our experiment. 4.2.1. Retrieval-based Models BM25 To make the retrieval baseline more efficient, we firstly index all the Input-Response pairs in the training set using ElasticSearch4 . Then we use BM25"
2020.lrec-1.58,N16-1014,0,0.226314,"(Gu et al., 2016) to the attention-based Seq2Seq baseline (Seq2Seq-Copy). The copy mechanism can explicitly extract words or phrases like certain entities from the input. 4.3. In order to provide comparable baseline results for future research, we use some quantitative metrics for automatic evaluation. BLEU and ROUGE scores, which are widely used in NLP and multi-turn dialogue generation tasks (Tian et al., 2017; Luo et al., 2018; Shen et al., 2019), are used to measure the quality of generated responses via the comparison with the ground truths. The recently proposed Distinct (Distinct-1/2) (Li et al., 2016), is used to evaluate the degree of diversity by calculating the ratio of unique unigrams and bigrams in the generated responses. 4.4. 4.4.1. Automatic Evaluation Results BM25 BERTRetrieval i 4.2.2. Generative Models Vanilla Seq2Seq We implement the vanilla Sequence-toSequence (Seq2Seq) model (Shang et al., 2015b) with 512unit 4-layer Bi-LSTMs for both the encoder and decoder. The input is the concatenated context and query, while the output is the response. Attention-based Seq2Seq To improve our baseline, we applied attention mechanism (Luong et al., 2015) in the Experimental Results In this"
2020.lrec-1.58,W15-4640,0,0.236256,"systems. Another typical branch of works collects the dialogue corpus from movie subtitles, such as OpenSubtitles (Tiedemann, 2009) and Cornell (Danescu-NiculescuMizil and Lee, 2011), which contain long sessions (over 100 turns) and some expressions like individual monologues may be not suitable for dialogue systems. More recently, some researchers construct dialogue datasets from social media networks (e.g., Twitter Dialogue Corpus (Ritter et al., 2011) and Chinese Weibo dataset (Wang et al., 2013)), or online forums (e.g., Chinese Douban dataset (Wu et al., 2017) and Ubuntu Dialogue Corpus (Lowe et al., 2015)). Although in large scale, they are different from real scenario conversations, as posts and replies are informal, single-turn or short-term related. In this work, we construct a large-scale multi-turn Chinese dialogue dataset, namely JDDC (Jing Dong Dialogue Corpus), with more than 1 million multi-turn dialogues, 20 million utterances, and 150 million words, which contains conversations about after-sales topics between users and customer service staffs in E-commerce scenario. Different from existing datasets mentioned above, the JDDC dataset illustrates the complexity of conversations in E-c"
2020.lrec-1.58,D18-1075,0,0.0374738,"Missing"
2020.lrec-1.58,D15-1166,0,0.0164212,"ently proposed Distinct (Distinct-1/2) (Li et al., 2016), is used to evaluate the degree of diversity by calculating the ratio of unique unigrams and bigrams in the generated responses. 4.4. 4.4.1. Automatic Evaluation Results BM25 BERTRetrieval i 4.2.2. Generative Models Vanilla Seq2Seq We implement the vanilla Sequence-toSequence (Seq2Seq) model (Shang et al., 2015b) with 512unit 4-layer Bi-LSTMs for both the encoder and decoder. The input is the concatenated context and query, while the output is the response. Attention-based Seq2Seq To improve our baseline, we applied attention mechanism (Luong et al., 2015) in the Experimental Results In this section, we analyze different baselines’ performance based on automatic evaluation measures and present indepth case study. Wi · S1 (wi , Idoc ) · S2 (wi , Itest ) (1) where Itest stands for the test input including context C and query Q, wi is the i-th word in the Itest , Idoc is the document input in the repository, Wi represents the weight of wi (such as inverse document frequency), and S(·) calculates the relevance score of the two elements. Therefore, S(Itest , Idoc ) is the similarity score between the test input and the existing I-R pairs in the repo"
2020.lrec-1.58,petukhova-etal-2014-dbox,0,0.0203159,"dialogue task extremely different from traditional machine learning tasks which usually have explicit targets and clearly defined evaluation metrics. To tackle this challenging problem, constructing a dialogue dataset is the most essential work. Especially for popular deep learning based approaches, large scale of training corpus in real scenario becomes decisive. However, existing datasets are still deficient. Datasets with structured annotations (e.g., slots and corresponding values) are often smallscale and in a limited capacity. Either traditional domainspecific ones (Allen et al., 1996; Petukhova et al., 2014; Bordes et al., 2016; Dodge et al., 2015) or recent multidomain ones (Budzianowski et al., 2018; Shah et al., 2018; El Asri et al., 2017) are usually built for task-oriented dialogue systems. Another typical branch of works collects the dialogue corpus from movie subtitles, such as OpenSubtitles (Tiedemann, 2009) and Cornell (Danescu-NiculescuMizil and Lee, 2011), which contain long sessions (over 100 turns) and some expressions like individual monologues may be not suitable for dialogue systems. More recently, some researchers construct dialogue datasets from social media networks (e.g., Twi"
2020.lrec-1.58,N10-1020,0,0.311105,"has kept active for decades. The growth of this field has been consistently supported by the development of new datasets. We briefly review existing dialogue datasets, and roughly divide them into three categories according to data features: 1) large scale data extracted from social media or forums, 2) artificial dialogue corpus constructed from crowd workers, and 3) corpus collected from real human-human coversation scenario. A list of related large-scale datasets discussed is provided in Table 2. Traditional methods tend to extract conversation alike information from social media or forum (Ritter et al., 2010; Shang et al., 2015a; Wu et al., 2017; Lowe et al., 2015; Li et al., 2018; Al-Rfou et al., 2016). Despite of the massive number of utterances included in these datasets, they usually provide ambiguous dialogue flows. It’s due to the fact that these datasets mainly comprise post-reply pairs on social networks or forums where people interact with others more freely (often more than two speakers are involved in the conversation). Moreover, the replies in these datasets are most or only related to the post and there are very few context information provided for query understanding. To imitate the"
2020.lrec-1.58,D11-1054,0,0.0539628,"Dodge et al., 2015) or recent multidomain ones (Budzianowski et al., 2018; Shah et al., 2018; El Asri et al., 2017) are usually built for task-oriented dialogue systems. Another typical branch of works collects the dialogue corpus from movie subtitles, such as OpenSubtitles (Tiedemann, 2009) and Cornell (Danescu-NiculescuMizil and Lee, 2011), which contain long sessions (over 100 turns) and some expressions like individual monologues may be not suitable for dialogue systems. More recently, some researchers construct dialogue datasets from social media networks (e.g., Twitter Dialogue Corpus (Ritter et al., 2011) and Chinese Weibo dataset (Wang et al., 2013)), or online forums (e.g., Chinese Douban dataset (Wu et al., 2017) and Ubuntu Dialogue Corpus (Lowe et al., 2015)). Although in large scale, they are different from real scenario conversations, as posts and replies are informal, single-turn or short-term related. In this work, we construct a large-scale multi-turn Chinese dialogue dataset, namely JDDC (Jing Dong Dialogue Corpus), with more than 1 million multi-turn dialogues, 20 million utterances, and 150 million words, which contains conversations about after-sales topics between users and custo"
2020.lrec-1.58,P15-1152,0,0.141836,"decades. The growth of this field has been consistently supported by the development of new datasets. We briefly review existing dialogue datasets, and roughly divide them into three categories according to data features: 1) large scale data extracted from social media or forums, 2) artificial dialogue corpus constructed from crowd workers, and 3) corpus collected from real human-human coversation scenario. A list of related large-scale datasets discussed is provided in Table 2. Traditional methods tend to extract conversation alike information from social media or forum (Ritter et al., 2010; Shang et al., 2015a; Wu et al., 2017; Lowe et al., 2015; Li et al., 2018; Al-Rfou et al., 2016). Despite of the massive number of utterances included in these datasets, they usually provide ambiguous dialogue flows. It’s due to the fact that these datasets mainly comprise post-reply pairs on social networks or forums where people interact with others more freely (often more than two speakers are involved in the conversation). Moreover, the replies in these datasets are most or only related to the post and there are very few context information provided for query understanding. To imitate the natural conversatio"
2020.lrec-1.58,P19-1549,1,0.835626,"ng and contains a lot of rare terminologies like “京东白条” (Jing Dong IOU(I owe you)), which may be OOV (out of vocabulary) words. Therefore, we add the copy mechanism (Gu et al., 2016) to the attention-based Seq2Seq baseline (Seq2Seq-Copy). The copy mechanism can explicitly extract words or phrases like certain entities from the input. 4.3. In order to provide comparable baseline results for future research, we use some quantitative metrics for automatic evaluation. BLEU and ROUGE scores, which are widely used in NLP and multi-turn dialogue generation tasks (Tian et al., 2017; Luo et al., 2018; Shen et al., 2019), are used to measure the quality of generated responses via the comparison with the ground truths. The recently proposed Distinct (Distinct-1/2) (Li et al., 2016), is used to evaluate the degree of diversity by calculating the ratio of unique unigrams and bigrams in the generated responses. 4.4. 4.4.1. Automatic Evaluation Results BM25 BERTRetrieval i 4.2.2. Generative Models Vanilla Seq2Seq We implement the vanilla Sequence-toSequence (Seq2Seq) model (Shang et al., 2015b) with 512unit 4-layer Bi-LSTMs for both the encoder and decoder. The input is the concatenated context and query, while th"
2020.lrec-1.58,P17-2036,0,0.0199818,"The context-query input is usually long and contains a lot of rare terminologies like “京东白条” (Jing Dong IOU(I owe you)), which may be OOV (out of vocabulary) words. Therefore, we add the copy mechanism (Gu et al., 2016) to the attention-based Seq2Seq baseline (Seq2Seq-Copy). The copy mechanism can explicitly extract words or phrases like certain entities from the input. 4.3. In order to provide comparable baseline results for future research, we use some quantitative metrics for automatic evaluation. BLEU and ROUGE scores, which are widely used in NLP and multi-turn dialogue generation tasks (Tian et al., 2017; Luo et al., 2018; Shen et al., 2019), are used to measure the quality of generated responses via the comparison with the ground truths. The recently proposed Distinct (Distinct-1/2) (Li et al., 2016), is used to evaluate the degree of diversity by calculating the ratio of unique unigrams and bigrams in the generated responses. 4.4. 4.4.1. Automatic Evaluation Results BM25 BERTRetrieval i 4.2.2. Generative Models Vanilla Seq2Seq We implement the vanilla Sequence-toSequence (Seq2Seq) model (Shang et al., 2015b) with 512unit 4-layer Bi-LSTMs for both the encoder and decoder. The input is the co"
2020.lrec-1.58,D13-1096,0,0.0214639,"(Budzianowski et al., 2018; Shah et al., 2018; El Asri et al., 2017) are usually built for task-oriented dialogue systems. Another typical branch of works collects the dialogue corpus from movie subtitles, such as OpenSubtitles (Tiedemann, 2009) and Cornell (Danescu-NiculescuMizil and Lee, 2011), which contain long sessions (over 100 turns) and some expressions like individual monologues may be not suitable for dialogue systems. More recently, some researchers construct dialogue datasets from social media networks (e.g., Twitter Dialogue Corpus (Ritter et al., 2011) and Chinese Weibo dataset (Wang et al., 2013)), or online forums (e.g., Chinese Douban dataset (Wu et al., 2017) and Ubuntu Dialogue Corpus (Lowe et al., 2015)). Although in large scale, they are different from real scenario conversations, as posts and replies are informal, single-turn or short-term related. In this work, we construct a large-scale multi-turn Chinese dialogue dataset, namely JDDC (Jing Dong Dialogue Corpus), with more than 1 million multi-turn dialogues, 20 million utterances, and 150 million words, which contains conversations about after-sales topics between users and customer service staffs in E-commerce scenario. Dif"
2020.lrec-1.58,P17-1046,0,0.185368,"are usually built for task-oriented dialogue systems. Another typical branch of works collects the dialogue corpus from movie subtitles, such as OpenSubtitles (Tiedemann, 2009) and Cornell (Danescu-NiculescuMizil and Lee, 2011), which contain long sessions (over 100 turns) and some expressions like individual monologues may be not suitable for dialogue systems. More recently, some researchers construct dialogue datasets from social media networks (e.g., Twitter Dialogue Corpus (Ritter et al., 2011) and Chinese Weibo dataset (Wang et al., 2013)), or online forums (e.g., Chinese Douban dataset (Wu et al., 2017) and Ubuntu Dialogue Corpus (Lowe et al., 2015)). Although in large scale, they are different from real scenario conversations, as posts and replies are informal, single-turn or short-term related. In this work, we construct a large-scale multi-turn Chinese dialogue dataset, namely JDDC (Jing Dong Dialogue Corpus), with more than 1 million multi-turn dialogues, 20 million utterances, and 150 million words, which contains conversations about after-sales topics between users and customer service staffs in E-commerce scenario. Different from existing datasets mentioned above, the JDDC dataset ill"
2020.lrec-1.58,P19-1369,0,0.102398,"ber of utterances included in these datasets, they usually provide ambiguous dialogue flows. It’s due to the fact that these datasets mainly comprise post-reply pairs on social networks or forums where people interact with others more freely (often more than two speakers are involved in the conversation). Moreover, the replies in these datasets are most or only related to the post and there are very few context information provided for query understanding. To imitate the natural conversation flows in real life, some datasets are collected with pre-defined prompts or guided schema. The DuConv (Wu et al., 2019) and PERSONACHAT (Zhang et al., 2018a) datasets are collected with Wizard-of-Oz technique (Kelley, 1984). The former one is collected during knowledge-driven conversation with one person playing as the conversation leader and the other one playing as the follower. However, the conversation goal is defined in advance. The later one collects data from two crowd workers with different persona information provided during conversation. Apart from the WOZ, the SGD (Rastogi et al., 2019) dataset is constructed by firstly generating dialogue outlines by simulator, then uses a crowd-sourcing procedure"
2020.lrec-1.58,N16-1174,1,0.430747,"Missing"
2020.lrec-1.58,P18-1205,0,0.366805,"e datasets, they usually provide ambiguous dialogue flows. It’s due to the fact that these datasets mainly comprise post-reply pairs on social networks or forums where people interact with others more freely (often more than two speakers are involved in the conversation). Moreover, the replies in these datasets are most or only related to the post and there are very few context information provided for query understanding. To imitate the natural conversation flows in real life, some datasets are collected with pre-defined prompts or guided schema. The DuConv (Wu et al., 2019) and PERSONACHAT (Zhang et al., 2018a) datasets are collected with Wizard-of-Oz technique (Kelley, 1984). The former one is collected during knowledge-driven conversation with one person playing as the conversation leader and the other one playing as the follower. However, the conversation goal is defined in advance. The later one collects data from two crowd workers with different persona information provided during conversation. Apart from the WOZ, the SGD (Rastogi et al., 2019) dataset is constructed by firstly generating dialogue outlines by simulator, then uses a crowd-sourcing procedure to paraphrase the outlines to natura"
2020.lrec-1.58,C18-1317,0,0.380799,"e datasets, they usually provide ambiguous dialogue flows. It’s due to the fact that these datasets mainly comprise post-reply pairs on social networks or forums where people interact with others more freely (often more than two speakers are involved in the conversation). Moreover, the replies in these datasets are most or only related to the post and there are very few context information provided for query understanding. To imitate the natural conversation flows in real life, some datasets are collected with pre-defined prompts or guided schema. The DuConv (Wu et al., 2019) and PERSONACHAT (Zhang et al., 2018a) datasets are collected with Wizard-of-Oz technique (Kelley, 1984). The former one is collected during knowledge-driven conversation with one person playing as the conversation leader and the other one playing as the follower. However, the conversation goal is defined in advance. The later one collects data from two crowd workers with different persona information provided during conversation. Apart from the WOZ, the SGD (Rastogi et al., 2019) dataset is constructed by firstly generating dialogue outlines by simulator, then uses a crowd-sourcing procedure to paraphrase the outlines to natura"
2021.emnlp-main.336,2020.emnlp-main.700,0,0.0296039,"Missing"
2021.emnlp-main.336,N18-1150,1,0.904233,"Missing"
2021.emnlp-main.336,P18-1063,0,0.0140842,"ith a small margin, indicating that general pre-training with selected data is not effective, and correlational copying is essential for pretraining. Fourth, we study the effectiveness of semantic and positional correlation between source words (i.e., SemCorrelation and PosCorrelation, respectively), we can observe that semantic and positional correlation are both useful, and depriving positional correlation decreases the performance larger. 4.3.2 Results on SAMSum The results on the SAMSum dataset are shown in Table 3. • Longest-3 takes three longest utterances as the summary. • Fast Abs RL (Chen and Bansal, 2018) is a hybrid extractive-abstractive model with the policy-based reinforcement learning. • TransformerABS (Vaswani et al., 2017) is the basic Transformer-based Seq2Seq model without pre-training. • DynamicConv (Wu et al., 2018) is a dynamic convolution model based on lightweight convolutions. • D-HGN (Feng et al., 2020) is a dialogue heterogeneous graph network modeling the utterance and commonsense knowledge. • TGDGA (Zhao et al., 2020) is a topic-word guided dialogue method based on the graph attention model. Models RG-1 RG-2 RG-L 32.46 37.27 41.03 42.37 45.41 42.03 43.11 51.53 51.58 10.27 14"
2021.emnlp-main.336,N16-1012,0,0.0290636,"tly, the sequence-to-sequence (Seq2Seq) framework has become the mainstream for performing abstractive summarization tasks. However, it suffers from handling out-of-vocabulary words (OOV). As it has been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step indepenout reading the full text. There are two types of dently of the former ones, neglecting the guidance summarizers: ext"
2021.emnlp-main.336,D18-1443,0,0.0276445,"Missing"
2021.emnlp-main.336,D19-5409,0,0.0146854,"pan to 128 and 32, respectively, After ranking with the ROUGE score, we select the top 20M samples as our final pre-training data. We believe this data selection strategy towards pre-training can make sure that there are enough output words that can be generated by copying from the input, which resembles the downstream task and learns our proposed correlational copying mechanism better. 4 4.1 Experiments Dataset sentence summaries. We use the non-anonymized version used in See et al. (2017), which has 287,226 training samples, 13,368 validation samples and 11,490 test samples. SAMSum dataset (Gliwa et al., 2019) contains 16K chat dialogues with manually annotated summaries, splited into 14,732 training samples, 818 validation samples, and 819 test samples. We use the version of the dataset with artificial separator (Gliwa et al., 2019), in which utterances are separated with “|”. 4.2 Experimental Settings For simplicity, we warm-start the model parameters with the publicly released pre-trained BART (large) model1 with 12 layers in both the encoder and decoder, and the hidden size is 1024. The learning rate is set to 3e-5, and learning decay is applied. We use Adam optimizer with β1 = 0.9, β1 = 0.999,"
2021.emnlp-main.336,P16-1154,0,0.126663,"ns (such as“into a red Honda”). how humans would summarize a text, but it is far more challenging to achieve. Currently, the sequence-to-sequence (Seq2Seq) framework has become the mainstream for performing abstractive summarization tasks. However, it suffers from handling out-of-vocabulary words (OOV). As it has been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step inde"
2021.emnlp-main.336,P16-1014,1,0.927021,"lowing copying operations (such as“into a red Honda”). how humans would summarize a text, but it is far more challenging to achieve. Currently, the sequence-to-sequence (Seq2Seq) framework has become the mainstream for performing abstractive summarization tasks. However, it suffers from handling out-of-vocabulary words (OOV). As it has been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at ea"
2021.emnlp-main.336,2020.acl-main.703,0,0.50856,"rent from Yang et al. (2018), we do not apply the predicted central position, because we argue that the information of relative position is strongly associated with the word correlations. In addition, following Shaw et al. (2018), we perform a relative distance clipping to improve the generalization of our model. 3.5 Correlational Copying Pre-training (CoCoPretrain) Pre-training with self-supervised objectives on raw text corpora has demonstrated the effectiveness of attCopy coCopy a broad range of text generation tasks (Song et al., = gt · Pt (w) + (1 − gt ) · Pt (w) 2019; Dong et al., 2019; Lewis et al., 2020; Zhang (14) X et al., 2020). In this paper, we enhance CoCoNet gt = sigmoid(Wg (αt,i + PtcoCopy (xi )) · Vi )) through correlational copying pre-training (CoCoi (15) Pretrain) on text span generation. The process of 4094 Ptf inalCopy (w) Original Text t1 : The formation of the Central American Isthmus closed the Central American Seaway Output Span Top-K Pre-training Data … Input Span si sj sk s1 … Overlap Score s1 Figure 2: The process of constructing the pre-training data. Given a piece of text, we divide it into an input span and an output span, and we calculate the overlap score of them by"
2021.emnlp-main.336,C18-1121,1,0.89257,"Missing"
2021.emnlp-main.336,W04-1013,0,0.286925,"pretraining with a self-supervised objective of text span generation with copying on the raw text corpora. Motivated by the work of Zhang et al. (2020), which has proven that pre-training resembling the downstream task leads to better and faster fine-tuning performances, we make sure our pretraining simulates the copying behaviors desired for the downstream summarization tasks. We divide each sequence in the corpora into two spans with some overlapping words, and the first span is used to generate the second in pre-training. We measure the overlap between the two spans based on ROUGE scores (Lin, 2004) to ensure that there are enough words to be generated by copying. Our main contributions are as follows: • We propose a Correlational Copying Network (CoCoNet) for abstractive summarization. It tracks the copying history and copies the next word from the input based on its relevance with the previously copied one. • We further enhance CoCoNet’s learning of copying through self-supervised pre-training on text span generation with copying. summarization tasks, and experimental results show that CoCoNet can copy more accurately. 2 2.1 Related work Copying Mechanism The copying mechanism is widel"
2021.emnlp-main.336,D19-1387,0,0.015656,"eported) BART (Our implement) BART + Cont. Pre-train Pre-trained Models + Copying BART + AttnCopy BART + SAGCopy CoCoNet CoCoNet - SemCorrelation CoCoNet - PosCorrelation CoCoNet + CoCoPretrain Table 2: ROUGE F1 scores on the CNN/DailyMail dataset. For a fair comparison, we continue pretraining BART with the same pre-training data but without copying mechanism (i.e., BART + Cont. Pretrain). • Lead-3 baseline that simply selects the first three sentences in the input document. • PGNet (See et al., 2017) is a hybrid pointergenerator model applying an attentional copy mechanism. • BERTSUMEXTABS (Liu and Lapata, 2019) applies BERT in text summarization. It is a two-stage fine-tuned model that first finetunes the encoder on the extractive summarization task and then on the abstractive summarization task. • SAGCopy (Xu et al., 2020b) fine-tunes MASS by incorporating the importance score for source words into the copying module. • PEGASUS (Zhang et al., 2020) adopts gapsentence generation as the pre-training objective. • T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) are models with denoising Seq2Seq pre-training. • ProphetNet (Qi et al., 2020) proposes to simultaneously predict the future n-gram at e"
2021.emnlp-main.336,P15-1002,0,0.0232746,"rd from the input based on its relevance with the previously copied one. • We further enhance CoCoNet’s learning of copying through self-supervised pre-training on text span generation with copying. summarization tasks, and experimental results show that CoCoNet can copy more accurately. 2 2.1 Related work Copying Mechanism The copying mechanism is widely used in abstractive summarization. It allows models to directly copy words from the input to the output. Vinyals et al. (2015) present the pointer network that uses attention distribution to select tokens in the input sequence as the output. Luong et al. (2015) propose to copy source words to the target sentence by a fixed-size softmax layer over a relative copying range. Gulcehre et al. (2016) leverage the attention mechanism to predict the location of the word to copy and apply a copying gate to determine whether to copy or not. Gu et al. (2016) propose to predict output words by combining copying and generating modes through a shared softmax function. See et al. (2017) introduce a copying probability to incorporate copying and generating distributions dynamically. Bi et al. (2020) adopt the copy mechanism in the language model pre-training. Exist"
2021.emnlp-main.336,P16-1008,0,0.0238253,"not attempt to calculate the copying distributions based on the copying history, which is our focus. 2.2 Temporal Attention Mechanism Our proposed copying mechanism is partially inspired by the temporal attention mechanism (Sankaran et al., 2016) that keeps track of previous attention scores and adjusts the future attention distribution by normalization with historical attention scores. This model has been proven effective in the text summarization task (Nallapati et al., 2016). Similar ideas are also adopted by the coverage mechanism for image caption (Xu et al., 2015), machine translation (Tu et al., 2016), and text summarization (See et al., 2017), maintaining a coverage vector to record the attention history to compute future attention distributions. Temporal attention mechanism is designed to avoid repetitive or insufficient attentions. While our work aims to learn a better copying mechanism from the copying history. 3 3.1 Model Overview The input of the text summarization task is a longer • CoCoNet achieves new state-of-the-art perfor- text, x = (x1 , x2 , ..., xS ) of S tokens, and the outmances on news summarization and dialogue put is a condensed summary, y = (y1 , y2 , ..., yT ) 4092 in"
2021.emnlp-main.336,K16-1028,1,0.927465,"ng and generating distributions dynamically. Bi et al. (2020) adopt the copy mechanism in the language model pre-training. Existing works do not attempt to calculate the copying distributions based on the copying history, which is our focus. 2.2 Temporal Attention Mechanism Our proposed copying mechanism is partially inspired by the temporal attention mechanism (Sankaran et al., 2016) that keeps track of previous attention scores and adjusts the future attention distribution by normalization with historical attention scores. This model has been proven effective in the text summarization task (Nallapati et al., 2016). Similar ideas are also adopted by the coverage mechanism for image caption (Xu et al., 2015), machine translation (Tu et al., 2016), and text summarization (See et al., 2017), maintaining a coverage vector to record the attention history to compute future attention distributions. Temporal attention mechanism is designed to avoid repetitive or insufficient attentions. While our work aims to learn a better copying mechanism from the copying history. 3 3.1 Model Overview The input of the text summarization task is a longer • CoCoNet achieves new state-of-the-art perfor- text, x = (x1 , x2 , ..."
2021.emnlp-main.336,2020.findings-emnlp.217,0,0.0220871,"Missing"
2021.emnlp-main.336,D15-1044,0,0.0533162,"summarize a text, but it is far more challenging to achieve. Currently, the sequence-to-sequence (Seq2Seq) framework has become the mainstream for performing abstractive summarization tasks. However, it suffers from handling out-of-vocabulary words (OOV). As it has been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step indepenout reading the full text. There are two types"
2021.emnlp-main.336,P17-1099,0,0.6045,"but it is far more challenging to achieve. Currently, the sequence-to-sequence (Seq2Seq) framework has become the mainstream for performing abstractive summarization tasks. However, it suffers from handling out-of-vocabulary words (OOV). As it has been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step indepenout reading the full text. There are two types of dently of the fo"
2021.emnlp-main.336,N18-2074,0,0.0232705,"ias, which considers the relative distances between different source words and range of local context suitable for copying: −(pstj −psti )2 1 2δ 2 j e pj,i = √ 2πδj |x| δj = sigmoid(Wδ Qj ) 2 (20) (21) where pstj and psti denote the positions for source word xj and xi , respectively. δj denotes the standard deviation that conditions on the length of the source sequence, i.e., |x|. Different from Yang et al. (2018), we do not apply the predicted central position, because we argue that the information of relative position is strongly associated with the word correlations. In addition, following Shaw et al. (2018), we perform a relative distance clipping to improve the generalization of our model. 3.5 Correlational Copying Pre-training (CoCoPretrain) Pre-training with self-supervised objectives on raw text corpora has demonstrated the effectiveness of attCopy coCopy a broad range of text generation tasks (Song et al., = gt · Pt (w) + (1 − gt ) · Pt (w) 2019; Dong et al., 2019; Lewis et al., 2020; Zhang (14) X et al., 2020). In this paper, we enhance CoCoNet gt = sigmoid(Wg (αt,i + PtcoCopy (xi )) · Vi )) through correlational copying pre-training (CoCoi (15) Pretrain) on text span generation. The proce"
2021.emnlp-main.336,2020.acl-main.125,1,0.167147,"been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step indepenout reading the full text. There are two types of dently of the former ones, neglecting the guidance summarizers: extractive and abstractive. Extractive of the copying history. Our work demonstrates that methods produce a summary by taking important the copying history can provide crucial clues of the sentence"
2021.emnlp-main.336,D18-1475,0,0.129476,"n the Transformer-based Seq2Seq architecture (Vaswani et al., 2017) , which has shown superiority in various text generation tasks, such as machine translation and text summarization. More specifically, CoCoNet copies from the input text at each time step by selecting what is relevant to the previously copied word. It keeps track of the prior copying distribution and explicitly models the correlation between different source words by integrating semantic and positional correlations. We obtain the semantic correlations based on the encoder selfattention matrix as Xu et al. (2020b). Inspired by Yang et al. (2018), we represent positional correlations as a Gaussian bias, which considers the relative distances between source words and the scope of the local context when copying. The framework of our model is shown in Figure 1. Furthermore, we enhance CoCoNet through pretraining with a self-supervised objective of text span generation with copying on the raw text corpora. Motivated by the work of Zhang et al. (2020), which has proven that pre-training resembling the downstream task leads to better and faster fine-tuning performances, we make sure our pretraining simulates the copying behaviors desired fo"
2021.emnlp-main.336,2020.coling-main.502,1,0.806885,"t some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step indepenout reading the full text. There are two types of dently of the former ones, neglecting the guidance summarizers: extractive and abstractive. Extractive of the copying history. Our work demonstrates that methods produce a summary by taking important the copying history can provide crucial clues of the sentences from the original t"
2021.emnlp-main.336,2020.coling-main.39,0,0.0269741,". 4.3.2 Results on SAMSum The results on the SAMSum dataset are shown in Table 3. • Longest-3 takes three longest utterances as the summary. • Fast Abs RL (Chen and Bansal, 2018) is a hybrid extractive-abstractive model with the policy-based reinforcement learning. • TransformerABS (Vaswani et al., 2017) is the basic Transformer-based Seq2Seq model without pre-training. • DynamicConv (Wu et al., 2018) is a dynamic convolution model based on lightweight convolutions. • D-HGN (Feng et al., 2020) is a dialogue heterogeneous graph network modeling the utterance and commonsense knowledge. • TGDGA (Zhao et al., 2020) is a topic-word guided dialogue method based on the graph attention model. Models RG-1 RG-2 RG-L 32.46 37.27 41.03 42.37 45.41 42.03 43.11 51.53 51.58 10.27 14.42 16.93 18.44 20.65 18.07 19.15 26.48 26.49 29.92 34.36 39.05 39.27 41.45 39.56 40.49 47.22 47.11 52.03 52.12 52.28 52.21 52.16 52.68 26.69 26.82 26.97 26.87 26.79 27.89 47.55 47.80 48.14 48.01 47.94 48.67 Baseline Methods Longest-3 PGNet Fast Abs RL TransformerABS DynamicConv D-HGN TGDGA BART (Our implement) BART + Cont. Pre-train Pre-trained Models + Copying BART + AttnCopy BART + SAGCopy CoCoNet CoCoNet - SemCorrelation CoCoNet - P"
2021.emnlp-main.336,P17-1101,0,0.0217972,"sequence (Seq2Seq) framework has become the mainstream for performing abstractive summarization tasks. However, it suffers from handling out-of-vocabulary words (OOV). As it has been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step indepenout reading the full text. There are two types of dently of the former ones, neglecting the guidance summarizers: extractive and abstrac"
2021.findings-acl.99,D19-1189,0,0.376618,"recommendation (CR) systems have attracted widespread attention for being a tool providing users potential items of interest through dialogue-based interactions. Though existing studies (Sun and Zhang, 2018; Zhang et al., 2018; Lei et al., 2020) proposed to integrate recommender and dialogue components for providing ( ) Corresponding Author Our code will release in https://github.com/ JD-AI-Research-NLP/RevCore. 1 user-specific suggestions through conversations, CR remains challengeable because (i) typical dialogues are short and lack sufficient item information for user preference capturing (Chen et al., 2019; Zhou et al., 2020), and (ii) difficulties exist in generating informative responses with item-related descriptions (Shao et al., 2017; Ghazvininejad et al., 2018; Wang et al., 2019b). Thus, recently, external information in the form of structured knowledge graphs (KG) is introduced to enhance item representations by using rich entity information in KG (Chen et al., 2019; Zhou et al., 2020). While KGbased methods improve CR to some extent, they are still limited in (i) worse versatility resulted from a high cost of KG construction; and (ii) inadequate integration of knowledge and response gen"
2021.findings-acl.99,P16-1014,0,0.036373,"he decoding stage takes them and the entity embedding E(C) as inputs of attention layers. These attention layers aim to fuse the external information from KG and reviews R into the context information, inspired by the work of Zhou et al. (2020). Given the decoding output of last time unit Pr3 (yi |Yi , R), (11) where Pr1 (·) is a generation probability function over the vocabulary, with Yi as the input. G and R represents the knowledge graph and reviews we use. Pr2 (·), Pr3 (·) are copy probability functions from KG entities and reviews, respectively, implemented by a standard copy mechanism (Gulcehre et al., 2016) (computing the distributions over the KG 1164 words or review words). Both probability functions are implemented with a softmax operation. To learn the response generation in the dialogue component, we set a cross-entropy loss: Lgen = − N  1 X log Pr(st |s1 , · · ·, st−1 ) , N (12) t=1 where N is the number of turns, st represents the tth utterance in the conversation. To train the whole model, it includes three steps: (i) pre-training the sentiment predictor in the review retrieval module; (ii) training the recommender component by minimizing Lrec ; (iii) training the dialogue component by"
2021.findings-acl.99,2020.tacl-1.5,0,0.0207558,"Missing"
2021.findings-acl.99,D19-1203,0,0.0270635,"g systems (Dodge et al., 2016; Yan et al., 2016; Benni et al., 2016; Bordes et al., 2017; Song et al., 2020) and structured knowledge-based info-seeking technics including question answering (Bao et al., 2014, 1168 2016; Yin et al., 2015; Yih et al., 2015; Shao et al., 2019) and question generation (Serban et al., 2016; Bao et al., 2018; Duˇsek et al., 2020) have encouraged the development of conversational recommendation systems, which dynamically obtain user preferences through interactive conversation with users. Multiple datasets have been constructed (Dodge et al., 2016; Li et al., 2018; Kang et al., 2019) to facilitate the study of this task. Li et al. (2018) collect a standard human-to-human multi-turn dialog dataset focusing on providing movie recommendations. Based on these datasets, various approaches are proposed to address different issues in CR systems. Specifically, external information is introduced to alleviate the coldstart problem, including knowledge bases (Wang et al., 2018), social networks (Daramola et al.), and knowledge graphs (Chen et al., 2019). Christakopoulou et al. (2016) use bandit-based exploreexploit strategy to minimize the number of user queries. Liu et al. (2020) c"
2021.findings-acl.99,N16-1014,0,0.0536031,"Dist-3 Dist-4 PPL Redial 2.4 3.1 3.9 4.2 6.1 14.0 15.0 18.3 22.7 23.6 32.0 33.6 37.8 43.3 45.4 Trans 0.148 0.225 0.263 0.289 0.373 0.424 0.151 0.236 0.368 0.434 0.527 0.558 0.137 0.228 0.423 0.519 0.615 0.612 17.0 28.1 17.9 9.8 10.7 10.2 KBRD KGSF RevCore (−KG) RevCore (+KG) Redial KBRD KGSF RevCore (−KG) Table 1: Results on the recommendation task. Best results are in bold. provided by human recommenders. Conversation evaluation comprises automatic and human evaluation. The metrics for automatic evaluation are perplexity (PPL) (Jelinek et al., 1977) and distinct n-gram (Dist-n, n = 2, 3, 4) (Li et al., 2016). Perplexity is a measurement for the fluency of natural language, where lower perplexity refers to higher fluency. Distinct n-gram is a measurement for the diversity of generated utterances. Specifically, we use distinct 3-gram and 4-gram at the sentence level to evaluate the diversity. The main purpose of our dialog component is a successful recommendation rather than imitating the ground truth responses. Therefore, we provide annotators to manually evaluate the results instead of using BLEU scores. The annotators evaluate the quality of generated dialogue responses from 3 aspects, i.e., coh"
2021.findings-acl.99,2020.acl-main.6,0,0.0429589,"al., 2020), and (ii) difficulties exist in generating informative responses with item-related descriptions (Shao et al., 2017; Ghazvininejad et al., 2018; Wang et al., 2019b). Thus, recently, external information in the form of structured knowledge graphs (KG) is introduced to enhance item representations by using rich entity information in KG (Chen et al., 2019; Zhou et al., 2020). While KGbased methods improve CR to some extent, they are still limited in (i) worse versatility resulted from a high cost of KG construction; and (ii) inadequate integration of knowledge and response generation (Lin et al., 2020). Given that, nowadays, users are greatly encouraged to share their consumption experience (e.g., restaurant, traveling, movie, etc.), reviews are easily accessed over the internet. Such reviews often provide rich and detailed user comments on different factors of interest, which are crucial in suggest1161 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1161–1173 August 1–6, 2021. ©2021 Association for Computational Linguistics Context KG entity emb Will Ferrell … S2: Yup I really liked it. Paul Feig Old School Bad Boys lookup table MLP classifier SA Wall-E …"
2021.findings-acl.99,2020.acl-main.98,0,0.157773,"Kang et al., 2019) to facilitate the study of this task. Li et al. (2018) collect a standard human-to-human multi-turn dialog dataset focusing on providing movie recommendations. Based on these datasets, various approaches are proposed to address different issues in CR systems. Specifically, external information is introduced to alleviate the coldstart problem, including knowledge bases (Wang et al., 2018), social networks (Daramola et al.), and knowledge graphs (Chen et al., 2019). Christakopoulou et al. (2016) use bandit-based exploreexploit strategy to minimize the number of user queries. Liu et al. (2020) conduct multi-goal planning to make a proactive conversational recommendation over multi-type dialogues. A multi-view method is proposed in Chen et al. (2020b) for the explainable conversational recommendation. The work of Pecune et al. (2020) builds a socially aware CR system engaging its users through a rapportbuilding dialogue to improve users’ perception. Different from all aforementioned previous work, we offer an alternative to AIG with an augmented conversational recommendation system by incorporating reviews that highly relevant to items. Particularly, our model is able to learn bette"
2021.findings-acl.99,P11-1015,0,0.04957,"Zhou et al. (2020) for fair comparison4 . Besides, the “review sentence” is selected according to the sentiment value and in a sentence-wise manner, and the token number of incorporated review sentences is set to 20, considering the balance between the original source and external source. We add the retrieved review sentences after the mentioned items in the dialogue component training to guide it to generate review-aware responses. The sentiment predictor for reviews is trained on the collected reviews. The sentiment predictor for dialog context is trained on the IMDb Movie Reviews Dataset (Maas et al., 2011) and then finetuned on the REDIAL dataset. 3.3 3 Experiment Settings 3.1 Dataset REDIAL (Li et al., 2018) is a widely-used dataset of real-world conversations around the theme of providing movie recommendations generated by the human in seeker-recommender pairs. REDIAL contains 10,021 conversations related to 64,362 movies, split into training, validation, and test sets using a ratio of 8:1:12 . To construct a review database, we crawled 30 reviews for each movie from IMDb3 website, which is one of the most popular and authoritative movie databases. Each review can be queried according to the"
2021.findings-acl.99,N18-1202,0,0.0196138,"a more precise recommendation. Review-augmented Response Generation Reviews can also augment the response generation in the conversation component. We build an encoder-decoder framework to handle the generation task. Retrieved reviews and context are encoded separately first, for the purpose of maintaining the dialog consistency. In the decoding stage, the review embedding is fused via an attention layer to generate informative responses. Considering that a good modeling of the input plays an important role to achieve an outstanding model performance (Mikolov et al., 2013; Song and Shi, 2018; Peters et al., 2018; Devlin et al., 2019; Song et al., 2021) and transformer-based approaches have achieved state-of-the-art in many NLP tasks (Vaswani et al., 2017; Chen et al., 2019; Zhou et al., 2020; Chen et al., 2020a; Joshi et al., 2020; Wang et al., 2020; Tian et al., 2020), we adopt two transformers as the encoders for context and reviews. Given a context C and the retrieved reviews R, the context embedding X(C) and review embedding R(C) are first obtained: X(C) = TransformerθX (C), R(C) = TransformerθR (R), Ai0 = MHA(Yi−1 , Yi−1 , Yi−1 ), Ai1 = MHA(Ai0 , X(C) , X(C) ), Ai2 = MHA(Ai1 , E(C) , E(C) ), (8)"
2021.findings-acl.99,D17-1235,0,0.0286127,"alogue-based interactions. Though existing studies (Sun and Zhang, 2018; Zhang et al., 2018; Lei et al., 2020) proposed to integrate recommender and dialogue components for providing ( ) Corresponding Author Our code will release in https://github.com/ JD-AI-Research-NLP/RevCore. 1 user-specific suggestions through conversations, CR remains challengeable because (i) typical dialogues are short and lack sufficient item information for user preference capturing (Chen et al., 2019; Zhou et al., 2020), and (ii) difficulties exist in generating informative responses with item-related descriptions (Shao et al., 2017; Ghazvininejad et al., 2018; Wang et al., 2019b). Thus, recently, external information in the form of structured knowledge graphs (KG) is introduced to enhance item representations by using rich entity information in KG (Chen et al., 2019; Zhou et al., 2020). While KGbased methods improve CR to some extent, they are still limited in (i) worse versatility resulted from a high cost of KG construction; and (ii) inadequate integration of knowledge and response generation (Lin et al., 2020). Given that, nowadays, users are greatly encouraged to share their consumption experience (e.g., restaurant,"
2021.findings-acl.99,2020.coling-main.63,1,0.657101,"users may have similar interests. Afterward, more sophisticated methods using neural networks are proposed and prove effective. For instance, neural factorization machines (He and Chua, 2017) and deep interest networks (Zhou et al., 2018) are used to estimate user preferences based on historical user-item interactions. Graphs are adopted in Wang et al. (2019b,a) to model complex relations among users, items, and attributes for a better representation of data. In recent years, major advances made in dialog systems (Dodge et al., 2016; Yan et al., 2016; Benni et al., 2016; Bordes et al., 2017; Song et al., 2020) and structured knowledge-based info-seeking technics including question answering (Bao et al., 2014, 1168 2016; Yin et al., 2015; Yih et al., 2015; Shao et al., 2019) and question generation (Serban et al., 2016; Bao et al., 2018; Duˇsek et al., 2020) have encouraged the development of conversational recommendation systems, which dynamically obtain user preferences through interactive conversation with users. Multiple datasets have been constructed (Dodge et al., 2016; Li et al., 2018; Kang et al., 2019) to facilitate the study of this task. Li et al. (2018) collect a standard human-to-human"
2021.findings-acl.99,2020.emnlp-main.487,1,0.657614,"rately first, for the purpose of maintaining the dialog consistency. In the decoding stage, the review embedding is fused via an attention layer to generate informative responses. Considering that a good modeling of the input plays an important role to achieve an outstanding model performance (Mikolov et al., 2013; Song and Shi, 2018; Peters et al., 2018; Devlin et al., 2019; Song et al., 2021) and transformer-based approaches have achieved state-of-the-art in many NLP tasks (Vaswani et al., 2017; Chen et al., 2019; Zhou et al., 2020; Chen et al., 2020a; Joshi et al., 2020; Wang et al., 2020; Tian et al., 2020), we adopt two transformers as the encoders for context and reviews. Given a context C and the retrieved reviews R, the context embedding X(C) and review embedding R(C) are first obtained: X(C) = TransformerθX (C), R(C) = TransformerθR (R), Ai0 = MHA(Yi−1 , Yi−1 , Yi−1 ), Ai1 = MHA(Ai0 , X(C) , X(C) ), Ai2 = MHA(Ai1 , E(C) , E(C) ), (8) Ai3 = MHA(Ai2 , R(C) , R(C) ), Yi = FFN(Ai3 ), E (C) = {EC , ER }, 2.3 Yi−1 , the current one Yi is generated by: where MHA(Q, K, V) represents the multi-head attention function (Vaswani et al., 2017), which takes a query, key, and value as input: MHA(Q, K, V)"
2021.findings-acl.99,2020.coling-main.510,1,0.729302,"xt are encoded separately first, for the purpose of maintaining the dialog consistency. In the decoding stage, the review embedding is fused via an attention layer to generate informative responses. Considering that a good modeling of the input plays an important role to achieve an outstanding model performance (Mikolov et al., 2013; Song and Shi, 2018; Peters et al., 2018; Devlin et al., 2019; Song et al., 2021) and transformer-based approaches have achieved state-of-the-art in many NLP tasks (Vaswani et al., 2017; Chen et al., 2019; Zhou et al., 2020; Chen et al., 2020a; Joshi et al., 2020; Wang et al., 2020; Tian et al., 2020), we adopt two transformers as the encoders for context and reviews. Given a context C and the retrieved reviews R, the context embedding X(C) and review embedding R(C) are first obtained: X(C) = TransformerθX (C), R(C) = TransformerθR (R), Ai0 = MHA(Yi−1 , Yi−1 , Yi−1 ), Ai1 = MHA(Ai0 , X(C) , X(C) ), Ai2 = MHA(Ai1 , E(C) , E(C) ), (8) Ai3 = MHA(Ai2 , R(C) , R(C) ), Yi = FFN(Ai3 ), E (C) = {EC , ER }, 2.3 Yi−1 , the current one Yi is generated by: where MHA(Q, K, V) represents the multi-head attention function (Vaswani et al., 2017), which takes a query, key, and value as"
2021.findings-acl.99,P16-1049,1,0.826327,"lsus, 2007; Wang et al., 2019b), which assumes that similar users may have similar interests. Afterward, more sophisticated methods using neural networks are proposed and prove effective. For instance, neural factorization machines (He and Chua, 2017) and deep interest networks (Zhou et al., 2018) are used to estimate user preferences based on historical user-item interactions. Graphs are adopted in Wang et al. (2019b,a) to model complex relations among users, items, and attributes for a better representation of data. In recent years, major advances made in dialog systems (Dodge et al., 2016; Yan et al., 2016; Benni et al., 2016; Bordes et al., 2017; Song et al., 2020) and structured knowledge-based info-seeking technics including question answering (Bao et al., 2014, 1168 2016; Yin et al., 2015; Yih et al., 2015; Shao et al., 2019) and question generation (Serban et al., 2016; Bao et al., 2018; Duˇsek et al., 2020) have encouraged the development of conversational recommendation systems, which dynamically obtain user preferences through interactive conversation with users. Multiple datasets have been constructed (Dodge et al., 2016; Li et al., 2018; Kang et al., 2019) to facilitate the study of t"
2021.findings-acl.99,P15-1128,1,0.674076,"factorization machines (He and Chua, 2017) and deep interest networks (Zhou et al., 2018) are used to estimate user preferences based on historical user-item interactions. Graphs are adopted in Wang et al. (2019b,a) to model complex relations among users, items, and attributes for a better representation of data. In recent years, major advances made in dialog systems (Dodge et al., 2016; Yan et al., 2016; Benni et al., 2016; Bordes et al., 2017; Song et al., 2020) and structured knowledge-based info-seeking technics including question answering (Bao et al., 2014, 1168 2016; Yin et al., 2015; Yih et al., 2015; Shao et al., 2019) and question generation (Serban et al., 2016; Bao et al., 2018; Duˇsek et al., 2020) have encouraged the development of conversational recommendation systems, which dynamically obtain user preferences through interactive conversation with users. Multiple datasets have been constructed (Dodge et al., 2016; Li et al., 2018; Kang et al., 2019) to facilitate the study of this task. Li et al. (2018) collect a standard human-to-human multi-turn dialog dataset focusing on providing movie recommendations. Based on these datasets, various approaches are proposed to address differen"
2021.findings-emnlp.1,K16-1028,1,0.825257,"-2 (RG-2), and ROUGE-L (RG-L) F1 scores (Lin and Hovy, 2003). K-PLUG clearly performs better than other text-based methods. E-commerce knowledge plays a significant role in the abstractive product summarization task, and domain-specific pre-training data and knowledge-injected pre-training objectives both enhance the model. K-PLUG achieves 4.2.2 Abstractive Product Summarization Task Definition. Abstractive product summarization task aims to capture the most attractive information of a product that resonates with potential purchasers. Similar to the text summarization task (Rush et al., 2015; Nallapati et al., 2016; Li 6 Model Home Applications RG-1 RG-2 RG-L RG-1 Clothing RG-2 RG-L Cases&Bags RG-1 RG-2 RG-L LexRank Seq2seq MASS PG 24.06 21.57 28.19 31.11 10.01 7.18 8.02 10.93 18.19 17.61 18.73 21.11 26.87 23.05 26.73 29.11 9.01 6.84 8.03 9.24 17.76 16.82 17.72 19.92 27.09 23.18 27.19 31.31 9.87 6.94 9.03 10.27 18.03 17.29 18.17 21.79 Aspect MMPG* 34.36 12.52 22.35 31.93 11.09 21.54 33.78 12.51 22.43 C-PLUG E-PLUG K-PLUG 32.75 33.11 33.56 11.62 12.07 12.50 21.76 22.01 22.15 31.73 32.61 33.00 10.86 11.03 11.24 20.37 20.98 21.43 32.04 32.37 33.87 10.75 11.14 11.83 21.85 21.98 22.35 Table 2: Experimental r"
2021.findings-emnlp.1,2020.acl-main.703,0,0.182112,"D AI Research University of California, Berkeley 3 Renmin University of China {xusong28, lihaoran24, yuanpeng29}@jd.com 2 Abstract language understanding (NLU) tasks, including text classification, reading comprehension, and natural language inference. These models are trained on large-scale text corpora with self-supervision based on either bi-directional or auto-regressive pre-training. Equally promising performances have been achieved in natural language generation (NLG) tasks, such as machine translation and text summarization, by MASS (Song et al., 2019), UniLM (Dong et al., 2019), BART (Lewis et al., 2020), T5 (Raffel et al., 2020), PEGASUS (Zhang et al., 2020), and ProphetNet (Qi et al., 2020). In contrast, these approaches adopt Transformerbased sequence-to-sequence models to jointly pretrain for both the encoder and the decoder. While these PLMs can learn rich semantic patterns from raw text data and thereby enhance downstream NLP applications, many of them do not explicitly model domain-specific knowledge. As a result, they may not be as sufficient for capturing human-curated or domain-specific knowledge that is necessary for tasks in a certain domain, such as tasks in e-commerce scenarios."
2021.findings-emnlp.1,N18-1202,0,0.261068,"roduct entities, and unique selling propositions of product entities. K-PLUG achieves new state-of-the-art results on a suite of domain-specific NLP tasks, including product knowledge base completion, abstractive product summarization, and multiturn dialogue, significantly outperforms baselines across the board, which demonstrates that the proposed method effectively learns a diverse set of domain-specific knowledge for both language understanding and generation tasks. Our code is available at https:// github.com/xu-song/k-plug. 1 Introduction Pre-trained language models (PLMs), such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019), have made remarkable breakthroughs in many natural ∗ Equal contribution. 1 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1–17 November 7–11, 2021. ©2021 Association for Computational Linguistics K-PLUG integrates knowledge into pre-training for both the encoder and the decoder, and thus K-PLUG can be adopted to both downstream knowledge-driven NLU and NLG tasks. We verify the performance of the proposed method in various e-commerce scenarios. In th"
2021.findings-emnlp.1,D19-1005,0,0.0507863,"Missing"
2021.findings-emnlp.1,2020.findings-emnlp.217,0,0.0116008,"oran24, yuanpeng29}@jd.com 2 Abstract language understanding (NLU) tasks, including text classification, reading comprehension, and natural language inference. These models are trained on large-scale text corpora with self-supervision based on either bi-directional or auto-regressive pre-training. Equally promising performances have been achieved in natural language generation (NLG) tasks, such as machine translation and text summarization, by MASS (Song et al., 2019), UniLM (Dong et al., 2019), BART (Lewis et al., 2020), T5 (Raffel et al., 2020), PEGASUS (Zhang et al., 2020), and ProphetNet (Qi et al., 2020). In contrast, these approaches adopt Transformerbased sequence-to-sequence models to jointly pretrain for both the encoder and the decoder. While these PLMs can learn rich semantic patterns from raw text data and thereby enhance downstream NLP applications, many of them do not explicitly model domain-specific knowledge. As a result, they may not be as sufficient for capturing human-curated or domain-specific knowledge that is necessary for tasks in a certain domain, such as tasks in e-commerce scenarios. In order to overcome this limitation, several recent studies have proposed to enrich PLMs"
2021.findings-emnlp.1,C18-1121,1,0.880096,"Missing"
2021.findings-emnlp.1,N03-1020,0,0.132811,"Missing"
2021.findings-emnlp.1,J85-2015,0,0.808299,"onal Linguistics: EMNLP 2021, pages 1–17 November 7–11, 2021. ©2021 Association for Computational Linguistics K-PLUG integrates knowledge into pre-training for both the encoder and the decoder, and thus K-PLUG can be adopted to both downstream knowledge-driven NLU and NLG tasks. We verify the performance of the proposed method in various e-commerce scenarios. In the proposed K-PLUG, we formulate the learning of four types of domainspecific knowledge: e-commerce domain-specific knowledge-bases, aspects of product entities, categories of product entities, and unique selling propositions (USPs) (Reeves, 1961) of product entities. Specifically, e-commerce KB stores standardized product attribute information, product aspects are features that play a crucial role in understanding product information, product categories are the backbones for constructing taxonomies for organization, and USPs are the essence of what differentiates a product from its competitors. K-PLUG learns these types of knowledge into a unified PLM, enhancing performances for various language understanding and generation tasks. To effectively learn these four types of valuable domain-specific knowledge in K-PLUG, we proposed five n"
2021.findings-emnlp.1,D15-1044,0,0.0322804,"UGE-1 (RG-1), ROUGE-2 (RG-2), and ROUGE-L (RG-L) F1 scores (Lin and Hovy, 2003). K-PLUG clearly performs better than other text-based methods. E-commerce knowledge plays a significant role in the abstractive product summarization task, and domain-specific pre-training data and knowledge-injected pre-training objectives both enhance the model. K-PLUG achieves 4.2.2 Abstractive Product Summarization Task Definition. Abstractive product summarization task aims to capture the most attractive information of a product that resonates with potential purchasers. Similar to the text summarization task (Rush et al., 2015; Nallapati et al., 2016; Li 6 Model Home Applications RG-1 RG-2 RG-L RG-1 Clothing RG-2 RG-L Cases&Bags RG-1 RG-2 RG-L LexRank Seq2seq MASS PG 24.06 21.57 28.19 31.11 10.01 7.18 8.02 10.93 18.19 17.61 18.73 21.11 26.87 23.05 26.73 29.11 9.01 6.84 8.03 9.24 17.76 16.82 17.72 19.92 27.09 23.18 27.19 31.31 9.87 6.94 9.03 10.27 18.03 17.29 18.17 21.79 Aspect MMPG* 34.36 12.52 22.35 31.93 11.09 21.54 33.78 12.51 22.43 C-PLUG E-PLUG K-PLUG 32.75 33.11 33.56 11.62 12.07 12.50 21.76 22.01 22.15 31.73 32.61 33.00 10.86 11.03 11.24 20.37 20.98 21.43 32.04 32.37 33.87 10.75 11.14 11.83 21.85 21.98 22.35"
2021.findings-emnlp.1,P19-1139,0,0.291579,"quence-to-sequence models to jointly pretrain for both the encoder and the decoder. While these PLMs can learn rich semantic patterns from raw text data and thereby enhance downstream NLP applications, many of them do not explicitly model domain-specific knowledge. As a result, they may not be as sufficient for capturing human-curated or domain-specific knowledge that is necessary for tasks in a certain domain, such as tasks in e-commerce scenarios. In order to overcome this limitation, several recent studies have proposed to enrich PLMs with explicit knowledge, including knowledge base (KB) (Zhang et al., 2019; Peters et al., 2019; Xiong et al., 2020; Wang et al., 2019, 2020), lexical relation (Lauscher et al., 2019; Wang et al., 2020), word sense (Levine et al., 2020), part-of-speech tag (Ke et al., 2020), and sentiment polarity (Ke et al., 2020; Tian et al., 2020). However, these methods only integrate knowledge into the encoder, and the decoding process in many NLG tasks benefits little from these knowledge. To mitigate this problem, we propose a Knowledge-injected Pre-trained Language model that is suitable for both Natural Language Understanding and Generation (K-PLUG). Different from existing"
2021.findings-emnlp.160,D18-1241,0,0.280843,"he length constraint. For examusually split into multiple chunks that are independently read. It results in the reading ple, each instance in open-domain MRC usually field being limited to individual chunks withconsists of a collection of passages, such as Triviout information collaboration for long docuaQA (Joshi et al., 2017), one of the most popular ment machine reading comprehension. To adopen-domain MRC datasets, containing 6,589 todress this problem, we propose RoR, a readkens on average. In addition, for conversational over-read method, which expands the reading MRC task, such as QuAC (Choi et al., 2018), exfield from chunk to document. Specifically, isting methods incorporate conversation history by RoR includes a chunk reader and a document reader. The former first predicts a set of reprepending the previous utterances to the current gional answers for each chunk, which are then question, which is packed with the document into compacted into a highly-condensed version of a length input (707 tokens on average). the original document, guaranteeing to be enTo handle a long document that exceeds the coded once. The latter further predicts the length constraint, a commonly used approach is globa"
2021.findings-emnlp.160,N19-1423,0,0.0602507,"Missing"
2021.findings-emnlp.160,N18-1202,0,0.0254128,"Missing"
2021.findings-emnlp.160,2020.coling-main.247,0,0.0181694,"l., 2015; Trischler et al., 2017; Rajpurkar et al., 2016, 2018). The best performing models in various MRC tasks are commonly based on the pre-trained language models (PLMs) within the typical encoding limit of 512 tokens. However, the input sequence in some MRC tasks usually exceeds the length limit, such as conversational MRC and open-domain MRC. Conversational MRC, which extends the traditional single-turn MRC, requires the models 3 Approach to additionally understand the conversation his3.1 Task Formulation tory (Reddy et al., 2019; Choi et al., 2018; Gao et al., 2018; Huang et al., 2019; Gupta et al., 2020) Given a document P , a question q, the task of MRC as dialog and conversational recommendation sys- is to predict an answer span y from P based on the tems (Lu et al., 2021). A straightforward but effec- comprehension of P and q. If q is an unanswerable tive approach of modeling the history is to prepend question, the QuAC dataset requires the model to the previous dialogs to the current question, which give an unanswerable tag as the final answer. To will compose a lengthy input sequence with the model the dialog history in QuAC, we prepend relatively long document (Gong et al., 2020). previ"
2021.findings-emnlp.160,P17-1147,0,0.336832,"els consist of a stack of transformer blocks machine reading comprehension. However, that only encode a length-limited sequence (e.g., due to the constraint of encoding length (e.g., 512). However, the input sequences in some MRC 512 WordPiece tokens), a long document is tasks may exceed the length constraint. For examusually split into multiple chunks that are independently read. It results in the reading ple, each instance in open-domain MRC usually field being limited to individual chunks withconsists of a collection of passages, such as Triviout information collaboration for long docuaQA (Joshi et al., 2017), one of the most popular ment machine reading comprehension. To adopen-domain MRC datasets, containing 6,589 todress this problem, we propose RoR, a readkens on average. In addition, for conversational over-read method, which expands the reading MRC task, such as QuAC (Choi et al., 2018), exfield from chunk to document. Specifically, isting methods incorporate conversation history by RoR includes a chunk reader and a document reader. The former first predicts a set of reprepending the previous utterances to the current gional answers for each chunk, which are then question, which is packed wi"
2021.findings-emnlp.160,Q19-1026,0,0.0128756,"cument reading limitation in existing models. • We propose a voting strategy to rerank the answers from regional chunks and a condensed document, overcoming the major drawback in aggregating the answers from different sources. • Extensive experiments on long document benchmarks are conducted to verify the effectiveness of our model. Especially on the QuAC dataset, our model achieves state-of-the-art results over all evaluation metrics on the leaderboard. 2 Related Work Open-domain MRC is a task of answering questions using a large collection of passages (Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019). The main challenge of this task is that the sequence length of multiple passages relevant to each question far exceeds the length limit of 512 tokens. For example, documents in TriviaQA (Joshi et al., 2017) contain 6,589 tokens on average. To enable the PLMs to encode long documents, a common approach is to chunk the document into overlapping chunks of length 512, then process each chunk separately, which inevitably causes the two problems aforementioned. Another intuitive approach is to increase the encoding length of the PLMs. For example, the recently proposed PLMs Longformer (Beltagy et"
2021.findings-emnlp.160,2021.ccl-1.108,0,0.0358678,"Missing"
2021.findings-emnlp.160,2021.findings-acl.99,1,0.825043,"Missing"
2021.findings-emnlp.160,P18-2124,0,0.0189942,".7 73.4 17.8 78.2 65.0 90.0 Table 2: Results on the development set of QuAC. After the operations above, the longest condensed document contains 471 tokens in TriviaQA and 184 tokens in QuAC. When RoR is adapted to other datasets, the length of the condensed documents can be guaranteed to be shorter than 512 as long as the parameters in the above four operations are adjusted correspondingly. In order to improve the model performance, some data augmentations are applied to better train the model. Specifically, ELECTRA is finetuned on other MRC datasets before fine-tuned on QuAC, such as SQuAD (Rajpurkar et al., 2018) and CoQA (Reddy et al., 2019), hoping to transfer the knowledge in other datasets to our model. Experimental results show that CoQA has a much higher lifting effect than SQuAD. This is because both CoQA and QuAC are conversational MRC datasets, while SQuAD is a single-turn MRC dataset. The answers in CoQA are free-form and generally short (average answer length = 2.7), which is quite different from QuAC (average answer length = 15.1). As a result, we choose the rationale sentence of the gold span in CoQA as the prediction target. Model F1 HEQ-Q HEQ-D Human 81.1 100 100 ELECTRA-RoR 74.9 72.2 1"
2021.findings-emnlp.160,D16-1264,0,0.322438,"f the answers are not comparable as they are not 1 Introduction globally normalized over chunks. To address these problems, we propose RoR, a The task of machine reading comprehension read-over-read pipeline, which is able to expand (MRC), which requires machines to answer questhe reading field from chunk-level to documenttions through reading and understanding a given document, has been a growing research field in nat- level. RoR contains a chunk reader and a document ural language understanding (Hermann et al., 2015; reader, both of which are based on the pre-trained Trischler et al., 2017; Rajpurkar et al., 2016, 2018; model. Specifically, the chunk reader first predicts the regional answers from each chunk. These anJoshi et al., 2017; Choi et al., 2018). swers are then compacted into a new document Transformer-based pre-trained models have been widely proven to be effective in a range of natu- through a minimum span coverage algorithm guaranteeing that its sequence length is shorter than the ∗ Corresponding Author: baojunwei001@gmail.com limitation (i.e., 512). By this means, all regional 1 https://quac.ai/ 2 answers can be normalized in one document. This Our code is available at https://github.com"
2021.findings-emnlp.160,Q19-1016,0,0.0732638,"the correct answers to questions after reading a given passage (Hermann et al., 2015; Trischler et al., 2017; Rajpurkar et al., 2016, 2018). The best performing models in various MRC tasks are commonly based on the pre-trained language models (PLMs) within the typical encoding limit of 512 tokens. However, the input sequence in some MRC tasks usually exceeds the length limit, such as conversational MRC and open-domain MRC. Conversational MRC, which extends the traditional single-turn MRC, requires the models 3 Approach to additionally understand the conversation his3.1 Task Formulation tory (Reddy et al., 2019; Choi et al., 2018; Gao et al., 2018; Huang et al., 2019; Gupta et al., 2020) Given a document P , a question q, the task of MRC as dialog and conversational recommendation sys- is to predict an answer span y from P based on the tems (Lu et al., 2021). A straightforward but effec- comprehension of P and q. If q is an unanswerable tive approach of modeling the history is to prepend question, the QuAC dataset requires the model to the previous dialogs to the current question, which give an unanswerable tag as the final answer. To will compose a lengthy input sequence with the model the dialog h"
2021.findings-emnlp.160,W17-2623,0,0.167266,"on (May 17th, 2021)2 . of the answers are not comparable as they are not 1 Introduction globally normalized over chunks. To address these problems, we propose RoR, a The task of machine reading comprehension read-over-read pipeline, which is able to expand (MRC), which requires machines to answer questhe reading field from chunk-level to documenttions through reading and understanding a given document, has been a growing research field in nat- level. RoR contains a chunk reader and a document ural language understanding (Hermann et al., 2015; reader, both of which are based on the pre-trained Trischler et al., 2017; Rajpurkar et al., 2016, 2018; model. Specifically, the chunk reader first predicts the regional answers from each chunk. These anJoshi et al., 2017; Choi et al., 2018). swers are then compacted into a new document Transformer-based pre-trained models have been widely proven to be effective in a range of natu- through a minimum span coverage algorithm guaranteeing that its sequence length is shorter than the ∗ Corresponding Author: baojunwei001@gmail.com limitation (i.e., 512). By this means, all regional 1 https://quac.ai/ 2 answers can be normalized in one document. This Our code is availab"
2021.findings-emnlp.160,P18-1178,0,0.0158503,"Longformer (Beltagy et al., 2020) and Big bird (Zaheer et al., 2021), specifically for long document modeling, have extended the encoding length from 512 to 4,096. However, their encoding length is fixed. The two problems caused by chunking still exist when encoding the sequences longer than 4,096. In contrast, our proposed model RoR is flexible which is able to encode sequences of arbitrary length. Moreover, RoR is assembleable and its encoder can be replaced with any PLMs, such as BERT and Longformer. Theoretically, hierarchical models can be adapted to long document MRC (Yang et al., 2016; Wang et al., 2018; Yang et al., 2020). However, deploying the large transformer-based PLMs as the encoders of hierarchical models can be prohibitively costly. Typically, hierarchical models parallelly encode the splitted chunks of a long document with multiple transformers, which requires extremely large GPU support. In contrast, RoR only needs to read a chunk at each encoding process, then gradually predict all answers from chunk to document. Therefore, RoR is able to deal with a long document without consuming too much computing resources and can be more widely used than hierarchical models. MRC is a fundame"
2021.findings-emnlp.160,N16-1174,1,0.439032,"ntly proposed PLMs Longformer (Beltagy et al., 2020) and Big bird (Zaheer et al., 2021), specifically for long document modeling, have extended the encoding length from 512 to 4,096. However, their encoding length is fixed. The two problems caused by chunking still exist when encoding the sequences longer than 4,096. In contrast, our proposed model RoR is flexible which is able to encode sequences of arbitrary length. Moreover, RoR is assembleable and its encoder can be replaced with any PLMs, such as BERT and Longformer. Theoretically, hierarchical models can be adapted to long document MRC (Yang et al., 2016; Wang et al., 2018; Yang et al., 2020). However, deploying the large transformer-based PLMs as the encoders of hierarchical models can be prohibitively costly. Typically, hierarchical models parallelly encode the splitted chunks of a long document with multiple transformers, which requires extremely large GPU support. In contrast, RoR only needs to read a chunk at each encoding process, then gradually predict all answers from chunk to document. Therefore, RoR is able to deal with a long document without consuming too much computing resources and can be more widely used than hierarchical model"
2021.findings-emnlp.160,D19-5812,0,0.0315881,"Missing"
2021.naacl-main.229,D17-1047,0,0.0227603,"e graph. • Our GraphMerge RGAT model outperforms recent state-of-the-art work on three benchmark datasets (Laptop and Restaurant reviews from SemEval 2014 and the ACL 14 Twitter dataset). It also outperforms its single-parse counterparts as well as other ensemble techniques. 2 Related Work Much recent work on aspect-level sentiment classification has focused on applying attention mechanisms (e.g., co-attention, self attention, and hierarchical attention) to sequence models such recurrent neural networks (RNNs) (Tang et al., 2015, 2016; Liu and Zhang, 2017; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). In a similar vein, pretrained transformer language models such as BERT (Devlin et al., 2018) have also been applied to this task, which operates directly on word sequences (Song et al., 2019; Xu et al., 2019; Rietzler et al., 2019). In parallel, researchers have also found syntactic information to be helpful for this task, and incorporated it into aspect-level sentiment classification models in the form of dependency trees (Dong et al., 2014; He et al., 2018) as well as constituency trees (Nguyen and Shirai, 2015). More recently, r"
2021.naacl-main.229,P14-2009,0,0.248697,"(RNNs) (Tang et al., 2015, 2016; Liu and Zhang, 2017; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). In a similar vein, pretrained transformer language models such as BERT (Devlin et al., 2018) have also been applied to this task, which operates directly on word sequences (Song et al., 2019; Xu et al., 2019; Rietzler et al., 2019). In parallel, researchers have also found syntactic information to be helpful for this task, and incorporated it into aspect-level sentiment classification models in the form of dependency trees (Dong et al., 2014; He et al., 2018) as well as constituency trees (Nguyen and Shirai, 2015). More recently, researchers have developed robust dependency-based models with the help of GNNs that operate either directly on dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019), as well as reshaped dependency trees that center around aspect terms (Wang et al., 2020b). While most recent work stack GNNs on top of BERT models, Tang et al. (2020) have also reported gains by jointly learning the two with a mutual biaffine attention mechanism. Despite the success of these dependency-based models"
2021.naacl-main.229,D18-1380,0,0.0312535,"Missing"
2021.naacl-main.229,N19-1259,0,0.146758,"Missing"
2021.naacl-main.229,P17-1044,0,0.0266605,"which aims to iden- of-the-art dependency parsers usually struggle to predict flawless parse trees especially in out-oftify the sentiment polarity (e.g., positive, negative or neutral) of a specific aspect term in a sentence. domain settings. This poses great challenge to dependency-based methods that rely on these parse For example, in “The exterior, unlike the food, is unwelcoming.”, the polarities of aspect terms “exte- trees—the added benefit from syntactic structure rior” and “food” are negative and positive, respec- does not always prevail the noise introduced by model-predicted parses (He et al., 2017; Sachan tively. This task has many applications, such as et al., 2021). assisting customers to filter online reviews or make purchase decisions on e-commerce websites. In this paper, we propose GraphMerge, a graph Recent studies have shown that syntactic infor- ensemble technique to help dependency-based modmation such as dependency trees is very effec- els mitigate the effect of parsing errors. Our techtive in capturing long-range syntactic relations that nique is based on the observation that different are obscure from the surface form (Zhang et al., parsers, especially ones with different"
2021.naacl-main.229,C18-1096,0,0.0161964,"., 2015, 2016; Liu and Zhang, 2017; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). In a similar vein, pretrained transformer language models such as BERT (Devlin et al., 2018) have also been applied to this task, which operates directly on word sequences (Song et al., 2019; Xu et al., 2019; Rietzler et al., 2019). In parallel, researchers have also found syntactic information to be helpful for this task, and incorporated it into aspect-level sentiment classification models in the form of dependency trees (Dong et al., 2014; He et al., 2018) as well as constituency trees (Nguyen and Shirai, 2015). More recently, researchers have developed robust dependency-based models with the help of GNNs that operate either directly on dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019), as well as reshaped dependency trees that center around aspect terms (Wang et al., 2020b). While most recent work stack GNNs on top of BERT models, Tang et al. (2020) have also reported gains by jointly learning the two with a mutual biaffine attention mechanism. Despite the success of these dependency-based models, they are usually"
2021.naacl-main.229,D19-1549,0,0.177889,"The exterior , unlike the food , is unwelcoming . Figure 1: An example where an incorrect parse (above the sentence) can mislead aspect-level sentiment classification for the term “food” by connecting it to the negative sentiment word “unwelcoming” by mistake. Although having its own issues, the parse below correctly captures the main syntactic structure between the aspect terms “exterior”, “food” and the sentiment word, and is more likely to lead to a correct prediction. graph neural network (GNN) (Kipf and Welling, 2016) model over dependency trees to aspect-level sentiment classification (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020b), which demonstrate that syntactic information is helpful for associating the aspect term with relevant opinion words more directly for increased robustness in sentiment classification. However, existing approaches are vulnerable to parsing errors (Wang et al., 2020b). For example, in Figure 1, the blue parse above the sentence can mislead models to predict negative sentiment for the aspect term “food” with its direct association to 1 Introduction “unwelcoming”. Despite their high edge-wise parsAspect-level sentiment classification is"
2021.naacl-main.229,P18-1249,0,0.0464693,"Missing"
2021.naacl-main.229,K18-1018,0,0.015169,"ng errors does not require any additional computational cost, since we are still applying GNNs to a single graph with the same number of nodes. Last but not least, GraphMerge helps prevent GNNs from overfitting by limiting over-parameterization. Aside from keeping the GNN computation over a single graph to avoid separate parameterization for each parse tree, GraphMerge also introduces more edges in the graph when parses differ, which reduces the diameter of graphs. As a result, fewer layers of GNNs are needed to learn good representations from the graph, alleviating the oversmoothing problem (Li et al., 2018b). To summarize, the main contribution of our work are the following: ensemble graph enables the model to learn from noisy graph and select correct edges among nodes at no additional computational cost. • We retain the syntactic dependency information in the original trees by parameterizing parent-tochildren and children-to-parent edges separately, which improves the performance of the RGAT model on the ensemble graph. • Our GraphMerge RGAT model outperforms recent state-of-the-art work on three benchmark datasets (Laptop and Restaurant reviews from SemEval 2014 and the ACL 14 Twitter dataset"
2021.naacl-main.229,P18-1087,0,0.125201,"Missing"
2021.naacl-main.229,E17-2091,0,0.0221694,"improves the performance of the RGAT model on the ensemble graph. • Our GraphMerge RGAT model outperforms recent state-of-the-art work on three benchmark datasets (Laptop and Restaurant reviews from SemEval 2014 and the ACL 14 Twitter dataset). It also outperforms its single-parse counterparts as well as other ensemble techniques. 2 Related Work Much recent work on aspect-level sentiment classification has focused on applying attention mechanisms (e.g., co-attention, self attention, and hierarchical attention) to sequence models such recurrent neural networks (RNNs) (Tang et al., 2015, 2016; Liu and Zhang, 2017; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). In a similar vein, pretrained transformer language models such as BERT (Devlin et al., 2018) have also been applied to this task, which operates directly on word sequences (Song et al., 2019; Xu et al., 2019; Rietzler et al., 2019). In parallel, researchers have also found syntactic information to be helpful for this task, and incorporated it into aspect-level sentiment classification models in the form of dependency trees (Dong et al., 2014; He et al., 2018) as well as consti"
2021.naacl-main.229,P14-5010,0,0.00438897,"Missing"
2021.naacl-main.229,D15-1298,0,0.0597109,"Missing"
2021.naacl-main.229,2020.acl-demos.14,1,0.889502,"Missing"
2021.naacl-main.229,2021.eacl-main.228,1,0.823451,"Missing"
2021.naacl-main.229,D19-1569,0,0.376313,"ming . Figure 1: An example where an incorrect parse (above the sentence) can mislead aspect-level sentiment classification for the term “food” by connecting it to the negative sentiment word “unwelcoming” by mistake. Although having its own issues, the parse below correctly captures the main syntactic structure between the aspect terms “exterior”, “food” and the sentiment word, and is more likely to lead to a correct prediction. graph neural network (GNN) (Kipf and Welling, 2016) model over dependency trees to aspect-level sentiment classification (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020b), which demonstrate that syntactic information is helpful for associating the aspect term with relevant opinion words more directly for increased robustness in sentiment classification. However, existing approaches are vulnerable to parsing errors (Wang et al., 2020b). For example, in Figure 1, the blue parse above the sentence can mislead models to predict negative sentiment for the aspect term “food” with its direct association to 1 Introduction “unwelcoming”. Despite their high edge-wise parsAspect-level sentiment classification is a fine- ing performance on standard be"
2021.naacl-main.229,C16-1311,0,0.0749951,"Missing"
2021.naacl-main.229,D16-1021,0,0.0445682,"Missing"
2021.naacl-main.229,2020.acl-main.588,0,0.206746,"tactic information to be helpful for this task, and incorporated it into aspect-level sentiment classification models in the form of dependency trees (Dong et al., 2014; He et al., 2018) as well as constituency trees (Nguyen and Shirai, 2015). More recently, researchers have developed robust dependency-based models with the help of GNNs that operate either directly on dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019), as well as reshaped dependency trees that center around aspect terms (Wang et al., 2020b). While most recent work stack GNNs on top of BERT models, Tang et al. (2020) have also reported gains by jointly learning the two with a mutual biaffine attention mechanism. Despite the success of these dependency-based models, they are usually vulnerable to parse errors since they rely on a single parser. Tu et al. • We propose a GraphMerge technique to combine (2012) used a dependency forest to combine muldependency parsing trees from different parsers to tiple dependency trees, however they tackled the improve model robustness to parsing errors. The sentence-level sentiment analysis task instead, and 2885 pos neu neg c Classification d a b e Pooling Edge Union b c"
2021.naacl-main.229,2020.acl-main.295,0,0.814831,"n example where an incorrect parse (above the sentence) can mislead aspect-level sentiment classification for the term “food” by connecting it to the negative sentiment word “unwelcoming” by mistake. Although having its own issues, the parse below correctly captures the main syntactic structure between the aspect terms “exterior”, “food” and the sentiment word, and is more likely to lead to a correct prediction. graph neural network (GNN) (Kipf and Welling, 2016) model over dependency trees to aspect-level sentiment classification (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020b), which demonstrate that syntactic information is helpful for associating the aspect term with relevant opinion words more directly for increased robustness in sentiment classification. However, existing approaches are vulnerable to parsing errors (Wang et al., 2020b). For example, in Figure 1, the blue parse above the sentence can mislead models to predict negative sentiment for the aspect term “food” with its direct association to 1 Introduction “unwelcoming”. Despite their high edge-wise parsAspect-level sentiment classification is a fine- ing performance on standard benchmarks, stategrai"
2021.naacl-main.229,P18-1088,0,0.0126494,"ance of the RGAT model on the ensemble graph. • Our GraphMerge RGAT model outperforms recent state-of-the-art work on three benchmark datasets (Laptop and Restaurant reviews from SemEval 2014 and the ACL 14 Twitter dataset). It also outperforms its single-parse counterparts as well as other ensemble techniques. 2 Related Work Much recent work on aspect-level sentiment classification has focused on applying attention mechanisms (e.g., co-attention, self attention, and hierarchical attention) to sequence models such recurrent neural networks (RNNs) (Tang et al., 2015, 2016; Liu and Zhang, 2017; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). In a similar vein, pretrained transformer language models such as BERT (Devlin et al., 2018) have also been applied to this task, which operates directly on word sequences (Song et al., 2019; Xu et al., 2019; Rietzler et al., 2019). In parallel, researchers have also found syntactic information to be helpful for this task, and incorporated it into aspect-level sentiment classification models in the form of dependency trees (Dong et al., 2014; He et al., 2018) as well as constituency trees (Nguye"
2021.naacl-main.229,2020.emnlp-main.292,0,0.0421324,"Missing"
2021.naacl-main.229,N19-1242,0,0.0399926,"Missing"
2021.naacl-main.229,D19-1464,0,0.411246,"Missing"
2021.naacl-main.229,D18-1244,1,0.828178,"Missing"
2021.naacl-main.455,D09-1027,0,0.316718,"significantly outperforms the strong baselines for both present and absent keyphrases generation. Furthermore, we extend SGG to a title generation task which indicates its extensibility in natural language generation tasks. 1 Select <Present keyphrase&gt; collaborative filtering </Present keyphrase&gt; Selection-Guided Generation <Absent keyphrase&gt; customer relationship management </Absent keyphrase&gt; Figure 1: An example of keyphrase prediction by SGG. Introduction Automatic keyphrase prediction recommends a set of representative phrases that are related to the main topics discussed in a document (Liu et al., 2009). Since keyphrases can provide a high-level topic description of a document, they are beneficial for a wide range of natural language processing (NLP) tasks, such as information extraction (Wan and Xiao, 2008), text summarization (Wang and Cardie, 2013) and question generation (Subramanian et al., 2018). Existing methods for keyphrase prediction can be categorized into extraction and generation approaches. Specifically, keyphrase extraction methods identify important consecutive words from a given document as keyphrases, which means that the extracted keyphrases (denoted as present keyphrases)"
2021.naacl-main.455,D09-1137,0,0.0192803,", and the experiment results indicate the extensibility and effectiveness of our SGG approach on generation tasks. 2 Related Work As mentioned in Section 1, the extraction and generation methods are two different research directions in the field of keyphrase prediction. The existing extraction methods can be broadly classified into supervised and unsupervised approaches. The supervised approaches treat keyphrase extraction as a binary classification task, which train the models with the features of labeled keyphrases to determine whether a candidate phrase is a keyphrase (Witten et al., 1999; Medelyan et al., 2009; Gollapalli et al., 2017). In contrast, the unsupervised approaches treat keyphrase extraction as a ranking task, scoring each candidate using some different ranking metrics, such as clustering (Liu et al., 2009), or graph-based ranking (Mihalcea and Tarau, 2004; Wang et al., 2014; Gollapalli and Caragea, 2014; Zhang et al., 2017). This work is mainly related to keyphrase generation approaches which have demonstrated good performance on keyphrase prediction task. Following CopyRNN (Meng et al., 2017), several extensions have been proposed to boost the generation capability. In CopyRNN, model"
2021.naacl-main.455,P17-1054,0,0.538445,"me from the given document. However, some keyphrases (denoted as absent keyphrases) of a given document do not match any contiguous subsequence but are highly semantically related to the source text. The extraction methods fail to predict these absent keyphrases. Therefore, generation methods have been proposed to produce a keyphrase verbatim from a predefined vocabulary, no matter whether the generated keyphrase appears in the source text. Compared with conventional extraction methods, generation methods have the ability of generating absent keyphrases as well as present keyphrases. CopyRNN (Meng et al., 2017) is the first to employ the sequence-to-sequence (Seq2Seq) framework (Sutskever et al., 2014) with the copying mechanism (Gu et al., 2016) to generate keyphrases for the given documents. Following the CopyRNN, several Seq2Seq-based keyphrase generation approaches have been proposed to improve the generation performance (Chen et al., 2018; Ye and Wang, 2018; Chen et al., 2019; Zhao and Zhang, 2019; Wang et al., 2019; Yuan et al., 2020). All these existing methods generate present and absent keyphrases synchronously without ex5717 Proceedings of the 2021 Conference of the North American Chapter"
2021.naacl-main.455,D18-1439,0,0.450895,"nput, ignoring the leading role of the title. To address this deficiency, Chen et al. (2019) proposed a title-guided Seq2Seq network to sufficiently utilize the already summarized information in title. In addition, some research attempts to introduce external knowledge into keyphrase generation, such as syntactic constraints (Zhao and Zhang, 2019) and latent topics (Wang et al., 2019). These approaches do not consider the one-tomany relationship between the input text and target keyphrases, and thus fail to model the correlation among the multiple target keyphrases. To overcome this drawback, Chen et al. (2018) incorporated the review mechanism into keyphrase generation and proposed a model CorrRNN with correlation constraints. Similarly, SGG separately models one-to-many relationship between the input text and present keyphrases and absent keyphrases. To avoid generating duplicate keyphrases, Chen et al. (2020) proposed an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism. For the same purpose, our method deploys a guider to avoid the generator generating duplicate present keyphrases. Last but most important, all"
2021.naacl-main.455,2020.acl-main.103,0,0.361639,"Missing"
2021.naacl-main.455,P16-1154,0,0.66081,"nce but are highly semantically related to the source text. The extraction methods fail to predict these absent keyphrases. Therefore, generation methods have been proposed to produce a keyphrase verbatim from a predefined vocabulary, no matter whether the generated keyphrase appears in the source text. Compared with conventional extraction methods, generation methods have the ability of generating absent keyphrases as well as present keyphrases. CopyRNN (Meng et al., 2017) is the first to employ the sequence-to-sequence (Seq2Seq) framework (Sutskever et al., 2014) with the copying mechanism (Gu et al., 2016) to generate keyphrases for the given documents. Following the CopyRNN, several Seq2Seq-based keyphrase generation approaches have been proposed to improve the generation performance (Chen et al., 2018; Ye and Wang, 2018; Chen et al., 2019; Zhao and Zhang, 2019; Wang et al., 2019; Yuan et al., 2020). All these existing methods generate present and absent keyphrases synchronously without ex5717 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5717–5726 June 6–11, 2021. ©2021 Association for Comp"
2021.naacl-main.455,P06-1068,0,0.642878,"llowing equations: cat = L X αia,t hi (9) i=1 αa,t = softmax(ua,t ) ua,t i = (10) VaT tanh(Wa [sat ; hi ; r] + ba ) (11) contains a title and an abstract of a scientific publication as source text, and author-assigned keywords as target keyphrases. We randomly select the example which contains at least one present keyphrase to construct the training set. Then, a validation set containing 500 samples will be selected from the remaining examples. In order to evaluate our proposed model comprehensively, we test models on four widely used public datasets from the scientific domain, namely Inspec (Hulth and Megyesi, 2006), Krapivin (Krapivin et al., 2009), SemEval2010 (Kim et al., 2010) and NUS (Nguyen and Kan, 2007), the statistic information of which are summarized in Table 2. Dataset where Va , Wa and ba are learnable parameters. r is a vector produced by the guider. The generation probability pgen at time step t is computed as: Test a pgen = σ(Wgen [cat ; sat ; emb(yt−1 )] + bgen ) (12) where Wgen and bgen are learnable parameters, a ) σ(·) represents a sigmoid function and emb(yt−1 a . In addition, p is the embedding of yt−1 gen in formula (7) is used as a soft switch to choose either generating words ove"
2021.naacl-main.455,S10-1004,0,0.252658,"i = (10) VaT tanh(Wa [sat ; hi ; r] + ba ) (11) contains a title and an abstract of a scientific publication as source text, and author-assigned keywords as target keyphrases. We randomly select the example which contains at least one present keyphrase to construct the training set. Then, a validation set containing 500 samples will be selected from the remaining examples. In order to evaluate our proposed model comprehensively, we test models on four widely used public datasets from the scientific domain, namely Inspec (Hulth and Megyesi, 2006), Krapivin (Krapivin et al., 2009), SemEval2010 (Kim et al., 2010) and NUS (Nguyen and Kan, 2007), the statistic information of which are summarized in Table 2. Dataset where Va , Wa and ba are learnable parameters. r is a vector produced by the guider. The generation probability pgen at time step t is computed as: Test a pgen = σ(Wgen [cat ; sat ; emb(yt−1 )] + bgen ) (12) where Wgen and bgen are learnable parameters, a ) σ(·) represents a sigmoid function and emb(yt−1 a . In addition, p is the embedding of yt−1 gen in formula (7) is used as a soft switch to choose either generating words over vocabulary or copying words from source text based on distributi"
2021.naacl-main.455,W04-3252,0,0.257251,"rediction. The existing extraction methods can be broadly classified into supervised and unsupervised approaches. The supervised approaches treat keyphrase extraction as a binary classification task, which train the models with the features of labeled keyphrases to determine whether a candidate phrase is a keyphrase (Witten et al., 1999; Medelyan et al., 2009; Gollapalli et al., 2017). In contrast, the unsupervised approaches treat keyphrase extraction as a ranking task, scoring each candidate using some different ranking metrics, such as clustering (Liu et al., 2009), or graph-based ranking (Mihalcea and Tarau, 2004; Wang et al., 2014; Gollapalli and Caragea, 2014; Zhang et al., 2017). This work is mainly related to keyphrase generation approaches which have demonstrated good performance on keyphrase prediction task. Following CopyRNN (Meng et al., 2017), several extensions have been proposed to boost the generation capability. In CopyRNN, model training heavily relies on large amount of labeled data, which is often unavailable especially for the new domains. To address this problem, Ye and Wang (2018) proposed 5718 a semi-supervised keyphrase generation model that utilizes both abundant unlabeled data a"
2021.naacl-main.455,P17-1099,0,0.706576,"to guide the generation. Specifically, our SGG is implemented with a hierarchical neural network which performs Seq2Seq learning by applying a multi-task learning strategy. This network consists of a selector at low layer, a generator at high layer, and a guider at middle layer for information transfer. The selector generates present keyphrases through a pointing mechanism (Vinyals et al., 2015), which adopts attention distributions to select a sequence of words from the source text as output. The generator further generates the absent keyphrases through a pointing-generating (PG) mechanism (See et al., 2017). Since present keyphrases have already been generated by the selector, they should not be generated again by the generator. Therefore, a guider is designed to memorize the generated present keyphrases from the selector, and then fed into the attention module of the generator to constrain it to focus on generating absent keyphrases. We summarize our main contributions as follows: • We propose a SGG approach which models present and absent keyphrase generation separately in different stages, i.e., select, guide, and generate, without sacrificing the end-to-end training through back-propagation."
2021.naacl-main.455,W18-2609,0,0.0190146,"e&gt; Selection-Guided Generation <Absent keyphrase&gt; customer relationship management </Absent keyphrase&gt; Figure 1: An example of keyphrase prediction by SGG. Introduction Automatic keyphrase prediction recommends a set of representative phrases that are related to the main topics discussed in a document (Liu et al., 2009). Since keyphrases can provide a high-level topic description of a document, they are beneficial for a wide range of natural language processing (NLP) tasks, such as information extraction (Wan and Xiao, 2008), text summarization (Wang and Cardie, 2013) and question generation (Subramanian et al., 2018). Existing methods for keyphrase prediction can be categorized into extraction and generation approaches. Specifically, keyphrase extraction methods identify important consecutive words from a given document as keyphrases, which means that the extracted keyphrases (denoted as present keyphrases) must exactly come from the given document. However, some keyphrases (denoted as absent keyphrases) of a given document do not match any contiguous subsequence but are highly semantically related to the source text. The extraction methods fail to predict these absent keyphrases. Therefore, generation me"
2021.naacl-main.455,P16-1008,0,0.240195,"roposed SGG which is implemented with a hierarchical neural network. r= M X αp,t (6) t=1 The final hidden representation hi of the i-th source word is the concatenation of forward and backward → − ← − hidden states, i.e., hi = [hi ; hi ]. where M is the length of present keyphrase sequence. r is an unnormalized distribution over the source words. As the attention distribution of selector is equal to the probability distribution over the source words, r represents the possibility that these words have been generated by the selector. The calculation of guider is inspired by the coverage vector (Tu et al., 2016) that is sequentially updated during the decoding process. In contrast to this, the guider here is a static vector which is capable of memorizing a global information. 3.4 3.6 two directions and outputs a sequence of forward → − hidden states {hi }L i=1 and backward hidden states ← − L {hi }i=1 by iterating the following equations: → − hi = LSTM(xi , hi−1 ) ← − hi = LSTM(xi , hi+1 ) (1) (2) Selector A selector is designed to generate present keyphrase sequences through the pointer mechanism (Vinyals et al., 2015), which adopts the attention distribution as a pointer to select words from the so"
2021.naacl-main.455,P13-1137,0,0.111034,"rase&gt; collaborative filtering </Present keyphrase&gt; Selection-Guided Generation <Absent keyphrase&gt; customer relationship management </Absent keyphrase&gt; Figure 1: An example of keyphrase prediction by SGG. Introduction Automatic keyphrase prediction recommends a set of representative phrases that are related to the main topics discussed in a document (Liu et al., 2009). Since keyphrases can provide a high-level topic description of a document, they are beneficial for a wide range of natural language processing (NLP) tasks, such as information extraction (Wan and Xiao, 2008), text summarization (Wang and Cardie, 2013) and question generation (Subramanian et al., 2018). Existing methods for keyphrase prediction can be categorized into extraction and generation approaches. Specifically, keyphrase extraction methods identify important consecutive words from a given document as keyphrases, which means that the extracted keyphrases (denoted as present keyphrases) must exactly come from the given document. However, some keyphrases (denoted as absent keyphrases) of a given document do not match any contiguous subsequence but are highly semantically related to the source text. The extraction methods fail to predic"
2021.naacl-main.455,P19-1240,0,0.0137029,"ress this problem, Ye and Wang (2018) proposed 5718 a semi-supervised keyphrase generation model that utilizes both abundant unlabeled data and limited labeled data. CopyRNN uses the concatenation of article title and abstract as input, ignoring the leading role of the title. To address this deficiency, Chen et al. (2019) proposed a title-guided Seq2Seq network to sufficiently utilize the already summarized information in title. In addition, some research attempts to introduce external knowledge into keyphrase generation, such as syntactic constraints (Zhao and Zhang, 2019) and latent topics (Wang et al., 2019). These approaches do not consider the one-tomany relationship between the input text and target keyphrases, and thus fail to model the correlation among the multiple target keyphrases. To overcome this drawback, Chen et al. (2018) incorporated the review mechanism into keyphrase generation and proposed a model CorrRNN with correlation constraints. Similarly, SGG separately models one-to-many relationship between the input text and present keyphrases and absent keyphrases. To avoid generating duplicate keyphrases, Chen et al. (2020) proposed an exclusive hierarchical decoding framework that in"
2021.naacl-main.455,D18-1447,0,0.0152981,"ng some different ranking metrics, such as clustering (Liu et al., 2009), or graph-based ranking (Mihalcea and Tarau, 2004; Wang et al., 2014; Gollapalli and Caragea, 2014; Zhang et al., 2017). This work is mainly related to keyphrase generation approaches which have demonstrated good performance on keyphrase prediction task. Following CopyRNN (Meng et al., 2017), several extensions have been proposed to boost the generation capability. In CopyRNN, model training heavily relies on large amount of labeled data, which is often unavailable especially for the new domains. To address this problem, Ye and Wang (2018) proposed 5718 a semi-supervised keyphrase generation model that utilizes both abundant unlabeled data and limited labeled data. CopyRNN uses the concatenation of article title and abstract as input, ignoring the leading role of the title. To address this deficiency, Chen et al. (2019) proposed a title-guided Seq2Seq network to sufficiently utilize the already summarized information in title. In addition, some research attempts to introduce external knowledge into keyphrase generation, such as syntactic constraints (Zhao and Zhang, 2019) and latent topics (Wang et al., 2019). These approaches"
2021.naacl-main.455,2020.acl-main.710,0,0.206226,"ne respectively. † indicates that the model is reimplemented. Method Inspec Krapivin NUS SemEval CopyRNN CopyTrans† 10.0 5.6 20.2 16.9 11.6 8.9 6.7 4.1 CorrRNN† CatSeq 8.5 2.9 15.2 7.4 8.0 3.1 3.5 2.5 SGG 11.0 23.5 12.4 4.9 Table 4: Recall@50 results of predicting absent keyphrases of different models on four datasets. The CorrRNN is retrained following the implementation details in Chen et al. (2018) as they did not report the Recall@50 results. • CorrRNN(one-to-many) (Chen et al., 2018) is an extension of CopyRNN incorporating the coverage mechanism (Tu et al., 2016). • CatSeq(one-to-many) (Yuan et al., 2020) has the same model structure as CopyRNN. The difference is CatSeq is trained by one-to-many. The baseline CopyTrans has not been reported in existing papers and thus is retrained. The implementation of Transformer is base on open source tool OpenNMT 1 . For our experiments of absent keyphrase generation, only generation methods are chosen as baselines. The copying mechanism used in all reimplemented generation models is based on the version (See et al., 2017), which is slightly different from the implementations by version (Meng et al., 2017; Gu et al., 2016). SGG indicates the full version o"
2021.naacl-main.455,P19-1515,1,0.652642,"le especially for the new domains. To address this problem, Ye and Wang (2018) proposed 5718 a semi-supervised keyphrase generation model that utilizes both abundant unlabeled data and limited labeled data. CopyRNN uses the concatenation of article title and abstract as input, ignoring the leading role of the title. To address this deficiency, Chen et al. (2019) proposed a title-guided Seq2Seq network to sufficiently utilize the already summarized information in title. In addition, some research attempts to introduce external knowledge into keyphrase generation, such as syntactic constraints (Zhao and Zhang, 2019) and latent topics (Wang et al., 2019). These approaches do not consider the one-tomany relationship between the input text and target keyphrases, and thus fail to model the correlation among the multiple target keyphrases. To overcome this drawback, Chen et al. (2018) incorporated the review mechanism into keyphrase generation and proposed a model CorrRNN with correlation constraints. Similarly, SGG separately models one-to-many relationship between the input text and present keyphrases and absent keyphrases. To avoid generating duplicate keyphrases, Chen et al. (2020) proposed an exclusive h"
2021.textgraphs-1.8,D17-1047,0,0.0223931,"r each head, and concatenate the GCN outputs w.r.t. different heads as the final word representation for sentiment analysis. The main contributions of this work are summarized as the following: 2 Related Work Capturing the interaction between the aspect term and opinion words is essential for predicting the sentiment polarity towards the aspect term. In recent work, various attention mechanisms, such as co-attention, self-attention and hierarchical attention, were utilized to learn this interaction (Tang et al., 2016; Liu and Zhang, 2017; Li et al., 2018c; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). Specifically, they first encoded the context and the aspect term by recurrent neural networks (RNNs), and then stacked several attention layers to learn the aspect term representations from important context words. After the success of the pre-trained BERT model (Devlin et al., 2018), Song et al. (2019) utilized the pre-trained BERT as the encoder. In the study by (Xu et al., 2019), the task was considered as a review reading comprehension (RRC) problem. RRC datasets were post trained on BERT and then fine-tuned to the aspect-level"
2021.textgraphs-1.8,P14-2009,0,0.0262342,"RT model (Devlin et al., 2018), Song et al. (2019) utilized the pre-trained BERT as the encoder. In the study by (Xu et al., 2019), the task was considered as a review reading comprehension (RRC) problem. RRC datasets were post trained on BERT and then fine-tuned to the aspect-level sentiment classification. Rietzler et al. (2019) utilized millions of extra data based on BERT to help sentiment analysis. The above approaches mainly considered the semantic information. Recent approaches attempted to incorporate the syntactic knowledge to learn the syntax-aware representation of the aspect term. Dong et al. (2014) proposed AdaRNN, which adaptively propagated the sentiments of words to target along the dependency tree in a bottom-up manner. Nguyen and Shirai (2015) extended RNN to obtain the representation of the target aspect by aggregating the syntactic information from the dependency and constituent tree of the sentence. He et al. (2018) proposed to use the distance between the context word and the aspect term along the dependency tree as the attention weight. Some re• We propose a selective attention based GCN (SA84 where {wτ , wτ +1 ..., wτ +m−1 } stand for the aspect term containing m words. First"
2021.textgraphs-1.8,P18-1087,0,0.262817,"passing. In some cases, the most important context words, i.e. opinion words, are more than two-hops away from the aspect term words on the dependency tree. As indicated by Figure 1, there are four hops between the target “Mac OS” and the opinion words “easily picked up” on the dependency tree. This type of cases requires more than two layers of GCN to learn interactions between them. However, previous works show that GCN models with two layers often achieve the best performance (Zhang et al., 2018; Xu et al., 2018), deeper GCNs do not bring additional gain due to the over-smoothing problem (Li et al., 2018b), which makes different nodes have similar representations and lose the distinction among nodes. In order to solve the above problem, we propose a novel selective attention based GCN (SA-GCN) model, which combines the GCN model over dependency trees with a self-attention based sequence model over the sentence. On one hand, the selfattention sequence model enables the direct interaction between an aspect term and its context so that it can take care of the situation where the term is far away from the opinion words on the dependency tree. On the other hand, a top-k attention selection module"
2021.textgraphs-1.8,D18-1380,0,0.0467062,"Missing"
2021.textgraphs-1.8,E17-2091,0,0.0226572,"pply a GCN layer again to integrate information from such sparse graph(s) for each head, and concatenate the GCN outputs w.r.t. different heads as the final word representation for sentiment analysis. The main contributions of this work are summarized as the following: 2 Related Work Capturing the interaction between the aspect term and opinion words is essential for predicting the sentiment polarity towards the aspect term. In recent work, various attention mechanisms, such as co-attention, self-attention and hierarchical attention, were utilized to learn this interaction (Tang et al., 2016; Liu and Zhang, 2017; Li et al., 2018c; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). Specifically, they first encoded the context and the aspect term by recurrent neural networks (RNNs), and then stacked several attention layers to learn the aspect term representations from important context words. After the success of the pre-trained BERT model (Devlin et al., 2018), Song et al. (2019) utilized the pre-trained BERT as the encoder. In the study by (Xu et al., 2019), the task was considered as a review reading comprehension (RRC) problem. RRC"
2021.textgraphs-1.8,N19-1259,0,0.0783644,"(16) Finally, the total training loss is: L = Ls + αLo (17) where α ≥ 0 represents the weight of opinion extraction task. 4 Experiments Data Sets. We evaluate our SA-GCN model on four datasets: Laptop reviews from SemEval 2014 Task 4 (14Lap), Restaurant reviews from SemEval 2014 Task 4 (Pontiki et al., 2014), SemEval 2015 Task 12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016) (14Rest, 15Rest and 16Rest). We remove several examples with “conflict” labels. The statistics of these datasets are listed in Table 1. The opinion words labeling for these four datasets come from (Fan et al., 2019). Baselines. Since BERT(Devlin et al., 2018) model shows significant improvements over many NLP tasks, we directly implement SA-GCN based on BERT and compare with following BERT-based baseline models: 1. BERT-SPC (Song et al., 2019) feeds the sentence and term pair into the BERT model and the BERT outputs are used for prediction. 4.1 2. AEN-BERT (Song et al., 2019) uses BERT as the encoder and employs several attention layers. Experimental Results We present results of the SA-GCN model in two aspects: classification performance and qualitative case study. Classification. Table 2 shows comparis"
2021.textgraphs-1.8,D15-1298,0,0.0660808,"Missing"
2021.textgraphs-1.8,P19-1024,0,0.3478,"elects the most important context words, which effectively sparsifies the fully-connected graph from self-attention. Then we apply another GCN layer on top of this new sparsified graph, such that each of those important context words is directly reachable by the aspect term and the interaction between them could be learned. 3 3.1 3.3 With words representations X as node features and dependency tree as the graph, we employ a GCN to capture syntactic relations between the term node and its neighboring nodes. GCNs have shown to be effective for many NLP applications, such as relation extraction (Guo et al., 2019; Zhang et al., 2018), reading comprehension (Kundu et al., 2019; Tu et al., 2019), and aspect-level sentiment analysis (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019). In each GCN layer, a node aggregates the information from its one-hop neighbors and update its representation. In our case, the graph is represented by the dependency tree, where each word is treated as a single node and its representation is denoted as the node feature. The message passing on the graph can be formulated as follows: Proposed Model H (l) = σ(AH (l−1) W (l−1) ) Overview of the Model The goal of our"
2021.textgraphs-1.8,C18-1096,0,0.0237842,"tilized millions of extra data based on BERT to help sentiment analysis. The above approaches mainly considered the semantic information. Recent approaches attempted to incorporate the syntactic knowledge to learn the syntax-aware representation of the aspect term. Dong et al. (2014) proposed AdaRNN, which adaptively propagated the sentiments of words to target along the dependency tree in a bottom-up manner. Nguyen and Shirai (2015) extended RNN to obtain the representation of the target aspect by aggregating the syntactic information from the dependency and constituent tree of the sentence. He et al. (2018) proposed to use the distance between the context word and the aspect term along the dependency tree as the attention weight. Some re• We propose a selective attention based GCN (SA84 where {wτ , wτ +1 ..., wτ +m−1 } stand for the aspect term containing m words. First, we construct the input as “[CLS] + sentence + [SEP] + term + [SEP]” and feed it into BERT. This input format enables explicit interactions between the whole sentence and the term such that the obtained word representations are term-attended. Then, we use average pooling to summarize the information carried by sub-words from BERT"
2021.textgraphs-1.8,D19-1549,0,0.583811,"mm1994@gmail.com Abstract in identifying the sentiment polarity towards the given term. Most approaches consider the semantic information from the context words and utilize the attention mechanism to learn such interactions. However, it has been shown that syntactic information obtained from dependency parsing is very effective in capturing long-range syntactic relations that are obscure from the surface form (Zhang et al., 2018). A recent popular approach to learn syntaxaware representations is employing graph convolutional networks (GCN) (Kipf and Welling, 2017) model over dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020; Tang et al., 2020), which introduces syntactic inductive biases into the message passing. In some cases, the most important context words, i.e. opinion words, are more than two-hops away from the aspect term words on the dependency tree. As indicated by Figure 1, there are four hops between the target “Mac OS” and the opinion words “easily picked up” on the dependency tree. This type of cases requires more than two layers of GCN to learn interactions between them. However, previous works show that GCN models with two layers often achie"
2021.textgraphs-1.8,S15-2082,0,0.0808747,"ew reading comprehension (RRC) problem. RRC datasets were post trained on BERT and then fine-tuned to the aspect-level sentiment classification. Rietzler et al. (2019) utilized millions of extra data based on BERT to help sentiment analysis. The above approaches mainly considered the semantic information. Recent approaches attempted to incorporate the syntactic knowledge to learn the syntax-aware representation of the aspect term. Dong et al. (2014) proposed AdaRNN, which adaptively propagated the sentiments of words to target along the dependency tree in a bottom-up manner. Nguyen and Shirai (2015) extended RNN to obtain the representation of the target aspect by aggregating the syntactic information from the dependency and constituent tree of the sentence. He et al. (2018) proposed to use the distance between the context word and the aspect term along the dependency tree as the attention weight. Some re• We propose a selective attention based GCN (SA84 where {wτ , wτ +1 ..., wτ +m−1 } stand for the aspect term containing m words. First, we construct the input as “[CLS] + sentence + [SEP] + term + [SEP]” and feed it into BERT. This input format enables explicit interactions between the"
2021.textgraphs-1.8,P19-1263,0,0.0269068,"rsifies the fully-connected graph from self-attention. Then we apply another GCN layer on top of this new sparsified graph, such that each of those important context words is directly reachable by the aspect term and the interaction between them could be learned. 3 3.1 3.3 With words representations X as node features and dependency tree as the graph, we employ a GCN to capture syntactic relations between the term node and its neighboring nodes. GCNs have shown to be effective for many NLP applications, such as relation extraction (Guo et al., 2019; Zhang et al., 2018), reading comprehension (Kundu et al., 2019; Tu et al., 2019), and aspect-level sentiment analysis (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019). In each GCN layer, a node aggregates the information from its one-hop neighbors and update its representation. In our case, the graph is represented by the dependency tree, where each word is treated as a single node and its representation is denoted as the node feature. The message passing on the graph can be formulated as follows: Proposed Model H (l) = σ(AH (l−1) W (l−1) ) Overview of the Model The goal of our proposed SA-GCN model is to predict the sentiment polarity of a"
2021.textgraphs-1.8,S14-2004,0,0.11841,"Missing"
2021.textgraphs-1.8,2020.acl-demos.14,1,0.817008,"BERT+SA-GCN is our proposed SA-GCN model with BERT encoder. Joint SA-GCN refers to joint training of sentiment classification and opinion extraction tasks. Evaluation metrics. We train the model on training set, and evaluate the performance on test set in terms of accuracy and macro-F1 scores which are commonly-used in sentiment analysis (Sun et al., 2019; Tang et al., 2016; Wang et al., 2020). Parameter Setting. During training, we set the learning rate to 10−5 . The batch size is 4. We train the model up to 5 epochs with Adam optimizer. We obtain dependency trees using the Stanford Stanza (Qi et al., 2020). The dimension of BERT output dB is 768. The hidden dimensions are selected from {128, 256, 512}. We apply dropout (Srivastava et al., 2014) and the dropout rate range is [0.1, 0.4]. The L2 regularization is set to 10−6 . We use 1 or 2 SA-GCN blocks in our experiments. We choose k in top-k selection module from {2, 3} to achieve the best performance. For joint training, the weight range of opinion extraction loss is [0.05, 0.15].2 algorithm in the decoding phase. And the loss for opinion extraction task is defined as: ˆ o )) Lo = −log(p(yo |H (16) Finally, the total training loss is: L = Ls +"
2021.textgraphs-1.8,P18-1088,0,0.0132798,"ormation from such sparse graph(s) for each head, and concatenate the GCN outputs w.r.t. different heads as the final word representation for sentiment analysis. The main contributions of this work are summarized as the following: 2 Related Work Capturing the interaction between the aspect term and opinion words is essential for predicting the sentiment polarity towards the aspect term. In recent work, various attention mechanisms, such as co-attention, self-attention and hierarchical attention, were utilized to learn this interaction (Tang et al., 2016; Liu and Zhang, 2017; Li et al., 2018c; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). Specifically, they first encoded the context and the aspect term by recurrent neural networks (RNNs), and then stacked several attention layers to learn the aspect term representations from important context words. After the success of the pre-trained BERT model (Devlin et al., 2018), Song et al. (2019) utilized the pre-trained BERT as the encoder. In the study by (Xu et al., 2019), the task was considered as a review reading comprehension (RRC) problem. RRC datasets were post trained on BERT an"
2021.textgraphs-1.8,N19-1242,0,0.0285329,"ical attention, were utilized to learn this interaction (Tang et al., 2016; Liu and Zhang, 2017; Li et al., 2018c; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). Specifically, they first encoded the context and the aspect term by recurrent neural networks (RNNs), and then stacked several attention layers to learn the aspect term representations from important context words. After the success of the pre-trained BERT model (Devlin et al., 2018), Song et al. (2019) utilized the pre-trained BERT as the encoder. In the study by (Xu et al., 2019), the task was considered as a review reading comprehension (RRC) problem. RRC datasets were post trained on BERT and then fine-tuned to the aspect-level sentiment classification. Rietzler et al. (2019) utilized millions of extra data based on BERT to help sentiment analysis. The above approaches mainly considered the semantic information. Recent approaches attempted to incorporate the syntactic knowledge to learn the syntax-aware representation of the aspect term. Dong et al. (2014) proposed AdaRNN, which adaptively propagated the sentiments of words to target along the dependency tree in a b"
2021.textgraphs-1.8,D19-1569,0,0.626666,"sentiment polarity towards the given term. Most approaches consider the semantic information from the context words and utilize the attention mechanism to learn such interactions. However, it has been shown that syntactic information obtained from dependency parsing is very effective in capturing long-range syntactic relations that are obscure from the surface form (Zhang et al., 2018). A recent popular approach to learn syntaxaware representations is employing graph convolutional networks (GCN) (Kipf and Welling, 2017) model over dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020; Tang et al., 2020), which introduces syntactic inductive biases into the message passing. In some cases, the most important context words, i.e. opinion words, are more than two-hops away from the aspect term words on the dependency tree. As indicated by Figure 1, there are four hops between the target “Mac OS” and the opinion words “easily picked up” on the dependency tree. This type of cases requires more than two layers of GCN to learn interactions between them. However, previous works show that GCN models with two layers often achieve the best performance (Zhang et al.,"
2021.textgraphs-1.8,D19-1464,0,0.178173,"Missing"
2021.textgraphs-1.8,D18-1244,1,0.918203,"Graph Convolutional Networks for Aspect-Level Sentiment Classification Xiaochen Hou∗ , Jing Huang, Guangtao Wang, Peng Qi, Xiaodong He, Bowen Zhou JD AI Research, Mountain View, CA ∗ xclmm1994@gmail.com Abstract in identifying the sentiment polarity towards the given term. Most approaches consider the semantic information from the context words and utilize the attention mechanism to learn such interactions. However, it has been shown that syntactic information obtained from dependency parsing is very effective in capturing long-range syntactic relations that are obscure from the surface form (Zhang et al., 2018). A recent popular approach to learn syntaxaware representations is employing graph convolutional networks (GCN) (Kipf and Welling, 2017) model over dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020; Tang et al., 2020), which introduces syntactic inductive biases into the message passing. In some cases, the most important context words, i.e. opinion words, are more than two-hops away from the aspect term words on the dependency tree. As indicated by Figure 1, there are four hops between the target “Mac OS” and the opinion words “easily picked up”"
2021.textgraphs-1.8,C16-1311,0,0.1644,"raph. Finally, we apply a GCN layer again to integrate information from such sparse graph(s) for each head, and concatenate the GCN outputs w.r.t. different heads as the final word representation for sentiment analysis. The main contributions of this work are summarized as the following: 2 Related Work Capturing the interaction between the aspect term and opinion words is essential for predicting the sentiment polarity towards the aspect term. In recent work, various attention mechanisms, such as co-attention, self-attention and hierarchical attention, were utilized to learn this interaction (Tang et al., 2016; Liu and Zhang, 2017; Li et al., 2018c; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). Specifically, they first encoded the context and the aspect term by recurrent neural networks (RNNs), and then stacked several attention layers to learn the aspect term representations from important context words. After the success of the pre-trained BERT model (Devlin et al., 2018), Song et al. (2019) utilized the pre-trained BERT as the encoder. In the study by (Xu et al., 2019), the task was considered as a review reading comprehensio"
2021.textgraphs-1.8,2020.acl-main.588,0,0.321309,"term. Most approaches consider the semantic information from the context words and utilize the attention mechanism to learn such interactions. However, it has been shown that syntactic information obtained from dependency parsing is very effective in capturing long-range syntactic relations that are obscure from the surface form (Zhang et al., 2018). A recent popular approach to learn syntaxaware representations is employing graph convolutional networks (GCN) (Kipf and Welling, 2017) model over dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020; Tang et al., 2020), which introduces syntactic inductive biases into the message passing. In some cases, the most important context words, i.e. opinion words, are more than two-hops away from the aspect term words on the dependency tree. As indicated by Figure 1, there are four hops between the target “Mac OS” and the opinion words “easily picked up” on the dependency tree. This type of cases requires more than two layers of GCN to learn interactions between them. However, previous works show that GCN models with two layers often achieve the best performance (Zhang et al., 2018; Xu et al., 2018), deeper GCNs do"
2021.textgraphs-1.8,2020.acl-main.295,0,0.675681,"y towards the given term. Most approaches consider the semantic information from the context words and utilize the attention mechanism to learn such interactions. However, it has been shown that syntactic information obtained from dependency parsing is very effective in capturing long-range syntactic relations that are obscure from the surface form (Zhang et al., 2018). A recent popular approach to learn syntaxaware representations is employing graph convolutional networks (GCN) (Kipf and Welling, 2017) model over dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020; Tang et al., 2020), which introduces syntactic inductive biases into the message passing. In some cases, the most important context words, i.e. opinion words, are more than two-hops away from the aspect term words on the dependency tree. As indicated by Figure 1, there are four hops between the target “Mac OS” and the opinion words “easily picked up” on the dependency tree. This type of cases requires more than two layers of GCN to learn interactions between them. However, previous works show that GCN models with two layers often achieve the best performance (Zhang et al., 2018; Xu et al., 2"
D08-1011,J07-2003,0,0.127138,"ion effort, the results on the dev set are reported in case insensitive BLEU (ciBLEU) score instead. 5.3 Experimental results In our main experiments, outputs from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data. The bilingual translation models used to compute the semantic similarity are from the wor"
D08-1011,W07-0717,0,0.033654,"s from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data. The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by He (2007), which are trained on two million parallel sentence-pairs selected from the training corpus allowed by the constrained training"
D08-1011,P06-1121,0,0.056997,"Missing"
D08-1011,W07-0711,1,0.77443,"ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data. The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by He (2007), which are trained on two million parallel sentence-pairs selected from the training corpus allowed by the constrained training condition of MT08. 5.3.1 Comparison with TER alignment In the IHMM-based method, the smoothing factor for surface similarity model is set to ρ = 3, the interpolation factor of the overall similarity model is set to α = 0.3, and the controlling factor of the distance-based distortion parameters is set to K=2. These settings are optimized on the dev set. Individual system results and system combination results using both IHMM and TER alignment, on both the dev and test"
D08-1011,P05-3026,0,0.323475,"onnormalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model. There have been other hypothesis alignment methods. Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al. (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005). 5 Evaluation In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008). We compare to the TER-based method used by Rosti et al. (2007). In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as a percentage in the following sections. 5.1 Implementation details In our implementation, the backbone is selected with MBR. Only the top hypothesis from each single system is considered as a backbone"
D08-1011,P08-2021,0,0.424303,"while it is only used implicitly via parameter initialization for IBM Model-1 training by Matusov et al. (2006). On the other hand, the TER-based alignment model is similar to a coarse-grained, nonnormalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model. There have been other hypothesis alignment methods. Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al. (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005). 5 Evaluation In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008). We compare to the TER-based method used by Rosti et al. (2007). In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as"
D08-1011,P07-1091,0,0.038442,"Missing"
D08-1011,N06-1014,0,0.101297,"ld be based on exact match: the surface similarity model, psur (ej |ei ) , would take the value 1.0 if e’= e, and ( f k |ei ) where pt 2s ( fk |ei ) is the translation model from the target-to-source word alignment model. In our method, pt 2 s (null |ei ) for all target words is 100 The other direction, ps 2t (ei |null ) , is available from the source-to-target translation model. 2 Usually a small back-off value is assigned instead of 0. 1 p(a j  i |a j1  i, I ) depend only on the jump distance (i - i') (Vogel et al., 1996): p(i |i, I )  c(i  i) I c(l  i) (5) l 1 As suggested by Liang et al. (2006), we can group the distortion parameters {c(d)}, d= i - i', into a few buckets. In our implementation, 11 buckets are used for c(≤-4), c(-3), ... c(0), ..., c(5), c(≥6). The probability mass for transitions with jump distance larger than 6 and less than -4 is uniformly divided. By doing this, only a handful of c(d) parameters need to be estimated. Although it is possible to estimate them using the EM algorithm on a small development set, we found that a particularly simple model, described below, works surprisingly well in our experiments. Since both the backbone and the hypothesis are in the"
D08-1011,E06-1005,0,0.643756,"significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation. 1 Introduction* System combination has been applied successfully to various machine translation tasks. Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation (MT) systems to form a consensus output (Bangalore, et al. 2001, Matusov et al., 2006, Rosti et al., 2007, Sim et al., 2007). A confusion network comprises a sequence of sets of alternative words, possibly including null’s, with associated scores. The consensus output is then derived by selecting one word from each set of alternatives, to produce the sequence with the best overall score, which could be assigned in various ways such as by voting, by * Mei Yang performed this work when she was an intern with Microsoft Research. using posterior probability estimates, or by using a combination of these measures and other features. Constructing a confusion network requires choosing"
D08-1011,2007.mtsummit-papers.43,1,0.70273,"Missing"
D08-1011,P02-1040,0,0.108033,"sis alignment methods. Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al. (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005). 5 Evaluation In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008). We compare to the TER-based method used by Rosti et al. (2007). In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as a percentage in the following sections. 5.1 Implementation details In our implementation, the backbone is selected with MBR. Only the top hypothesis from each single system is considered as a backbone. A uniform posteriori probability is assigned to all hypotheses. TER is used as loss function in the MBR computation. Similar to (Rosti et al., 2007), each word in the confusion network is associated with a word posterior probability. Given a system S, each of its hypotheses is assigned with a rank-based score of 1/(1+r)η, where r is the rank of the hypothesis, and η is a r"
D08-1011,N03-1017,0,0.0156845,"Missing"
D08-1011,P05-1034,0,0.0651166,"to the system combination, 10-best hypotheses for each source sentence in the dev and test sets are collected from each of the eight single systems. All outputs on the MT08 test set were true-cased before scoring using a log-linear conditional Markov model proposed by Toutanova et al. (2008). However, to save computation effort, the results on the dev set are reported in case insensitive BLEU (ciBLEU) score instead. 5.3 Experimental results In our main experiments, outputs from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All"
D08-1011,N07-1029,0,0.604428,"emented in GIZA++, and heuristically combines results from aligning in both directions. System combination based on this approach gives an improvement over the best single system. However, the number of hypothesis pairs for training is limited by the size of the test corpus. Also, MT hypotheses from the same source sentence are correlated with each other and these hypothesis pairs are not i.i.d. data samples. Therefore, GIZA++ training on such a data set may be unreliable. Bangalore et al. (2001) used a multiple stringmatching algorithm based on Levenshtein edit distance, and later Sim et al. (2007) and Rosti et al. (2007) extended it to a TER-based method for hypothesis alignment. TER (Snover et al., 2006) 3 This only happens if no hypothesis word is aligned to a backbone word but some hypothesis words are aligned to the null associated with that backbone word. 102 measures the minimum number of edits, including substitution, insertion, deletion, and shift of blocks of words, that are needed to modify a hypothesis so that it exactly matches the other hypothesis. The best alignment is the one that gives the minimum number of translation edits. TER-based confusion network construction and"
D08-1011,P07-1040,0,0.668355,"rms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation. 1 Introduction* System combination has been applied successfully to various machine translation tasks. Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation (MT) systems to form a consensus output (Bangalore, et al. 2001, Matusov et al., 2006, Rosti et al., 2007, Sim et al., 2007). A confusion network comprises a sequence of sets of alternative words, possibly including null’s, with associated scores. The consensus output is then derived by selecting one word from each set of alternatives, to produce the sequence with the best overall score, which could be assigned in various ways such as by voting, by * Mei Yang performed this work when she was an intern with Microsoft Research. using posterior probability estimates, or by using a combination of these measures and other features. Constructing a confusion network requires choosing one of the hypothes"
D08-1011,W08-0329,0,0.532485,"1 training by Matusov et al. (2006). On the other hand, the TER-based alignment model is similar to a coarse-grained, nonnormalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model. There have been other hypothesis alignment methods. Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al. (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005). 5 Evaluation In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008). We compare to the TER-based method used by Rosti et al. (2007). In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as a percentage in the following sections. 5.1 Implementation details In our"
D08-1011,2006.amta-papers.25,0,0.112438,"ombination based on this approach gives an improvement over the best single system. However, the number of hypothesis pairs for training is limited by the size of the test corpus. Also, MT hypotheses from the same source sentence are correlated with each other and these hypothesis pairs are not i.i.d. data samples. Therefore, GIZA++ training on such a data set may be unreliable. Bangalore et al. (2001) used a multiple stringmatching algorithm based on Levenshtein edit distance, and later Sim et al. (2007) and Rosti et al. (2007) extended it to a TER-based method for hypothesis alignment. TER (Snover et al., 2006) 3 This only happens if no hypothesis word is aligned to a backbone word but some hypothesis words are aligned to the null associated with that backbone word. 102 measures the minimum number of edits, including substitution, insertion, deletion, and shift of blocks of words, that are needed to modify a hypothesis so that it exactly matches the other hypothesis. The best alignment is the one that gives the minimum number of translation edits. TER-based confusion network construction and system combination has demonstrated superior performance on various large-scale MT tasks (Rosti. et al, 2007)"
D08-1011,C96-2141,0,0.703549,"nice car a sedan he has hypothesis set he have ε good car E4 a ε sedan he has (c) hypothesis alignment EB  argmin TER( E, E) EE EE e.g., EB = E1 (b) backbone selection he have ε good car he has ε nice sedan it ε a nice car he has a ε sedan (d) confusion network Figure 1: Confusion-network-based combination. MT system 3 Indirect-HMM-based Hypothesis Alignment In confusion-network-based system combination for SMT, a major difficulty is aligning hypotheses to the backbone. One possible statistical model for word alignment is the HMM, which has been widely used for bilingual word alignment (Vogel et al., 1996, Och and Ney, 2003). In this paper, we propose an indirect-HMM method for monolingual hypothesis alignment. 99 3.1 IHMM for hypothesis alignment e1I  (e1,..., eI ) denote the backbone, e1J  (e1,..., eJ ) a hypothesis to be aligned to e1I , and a1J  (a1,..., aJ ) the alignment that specifies Let the position of the backbone word aligned to each hypothesis word. We treat each word in the backbone as an HMM state and the words in the hypothesis as the observation sequence. We use a first-order HMM, assuming that the emission probability p(ej |ea j ) depends only on the backbone word, and"
D08-1011,D07-1077,0,0.0163745,"were true-cased before scoring using a log-linear conditional Markov model proposed by Toutanova et al. (2008). However, to save computation effort, the results on the dev set are reported in case insensitive BLEU (ciBLEU) score instead. 5.3 Experimental results In our main experiments, outputs from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different"
D08-1011,P06-1066,0,0.209309,"ve BLEU (ciBLEU) score instead. 5.3 Experimental results In our main experiments, outputs from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data. The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by He (2007), which are trained on two million par"
D08-1011,P08-1059,0,\N,Missing
D08-1011,J03-1002,0,\N,Missing
D08-1011,2005.eamt-1.20,0,\N,Missing
D08-1011,P08-1066,0,\N,Missing
D09-1125,D08-1011,1,0.954325,"d order is determined by the backbone, and the set of possible words at each position is determined by alignment. Since the space of possible alignments is extremely large, approximate and heuristic techniques have been employed to derive them. In pair-wise alignment, each hypothesis is aligned to the backbone in turn, with separate processing to combine the multiple alignments. Several models have been used for pair-wise alignment, starting with TER and proceeding with more sophisticated techniques such as HMM models, ITG, and IHMM (Rosti et. al 2007a, Matusov et al 2008, Krakos et al. 2008, He et al. 2008). A major problem with such methods is that each hypothesis is aligned to the backbone independently, leading to suboptimal behavior. For example, suppose that we use a state-of-the-art word alignment model for pairs of hypotheses, such as the IHMM. Figure 1 shows likely alignment links between every pair of hypotheses. If Hypothesis 1 is aligned to Hypothesis 2 (the backbone), Jeep is likely to align to SUV because they express similar Chinese content. Hypothesis 3 is separately aligned to the backbone and since the alignment is constrained to be one-to-one, SUV is aligned to SUV and Jeep to"
D09-1125,P02-1040,0,0.110034,"Missing"
D09-1125,P05-3026,0,0.0952961,"al., 2006, He et al 08). Some work has made such decisions in a more principled fashion by computing model-based scores (Matusov et al. 2008), but still specialpurpose algorithms and heuristics are needed and a single alignment is fixed. In our approach, no heuristics are used to convert alignments and no concept of a backbone is used. Instead, the globally highest scoring combination of alignment, order, and lexical choice is selected (subject to search error). Other than confusion-network-based algorithms, work most closely related to ours is the method of MT system combination proposed in (Jayaraman and Lavie 2005), which we will refer to as J&L. Like our method, this approach performs word-level system combination and is not limited to following the word order of a single backbone hypothesis; it also allows more flexibility in the selection of correspondence sets during decoding, compared to a confusionnetwork-based approach. Even though their algorithm and ours are broadly similar, there are several important differences. Firstly, the J&L approach is based on pairwise alignments between words in different hypotheses, which are hard and do not have associated probabilities. Every word in every hypothes"
D09-1125,N07-1029,0,0.499518,"Missing"
D09-1125,P08-2021,0,0.141318,"Missing"
D09-1125,W04-3250,0,0.0577903,"Missing"
D09-1125,P07-1040,0,0.182496,"Missing"
D09-1125,koen-2004-pharaoh,0,0.225819,", ??+1 ∈ ? ) ?=1 |?|−1 The joint decoding framework chooses optimal output according to the following log-linear model: ? ?(?) ?(??,? ? = ??,? ? ) ???? ?, ?, ?, ? = ??? ? ?? , ??+1 ? ?=1 Distortion model: Unlike in the conventional CN-based system combination, flexible orders of CS are allowed in this joint decoding framework. In order to model the distortion of different orderings, a distortion model between two CS is defined as follows: First we define the distortion cost between two words at a single hypothesis. Similarly to the distortion penalty in the conventional phrasebased decoder (Koehn 2004b), the distortion cost of jumping from a word at position i to another word at position j, d(i,j), is proportional to the distance between i and j, e.g., |i-j|. Then, the distortion cost of jumping from one CS, which has a position vector recording the original position of each word in that CS, to another CS is a weighted sum of single-hypothesis-based distortion costs: ? Word posterior model: The word posterior feature is the same as the one proposed by Rosti et. al. (2007a). i.e., ? ??? ?, ?, ?, ? = ??? ? ?? ??? ? =1 where the posterior of a single word in a CS is ?(??? , ??? +1 ) = ?(?) ∙"
D09-1125,W08-0329,0,0.101884,"Hypothesis 2 (the backbone), Jeep is likely to align to SUV because they express similar Chinese content. Hypothesis 3 is separately aligned to the backbone and since the alignment is constrained to be one-to-one, SUV is aligned to SUV and Jeep to an empty word which is inserted after SUV. The network in Figure 2a) is the result of this process. An undesirable property of this CN is that the two instances of Jeep are placed in separate columns and cannot vote to reinforce each other. Incremental alignment methods have been proposed to relax the independence assumption of pair-wise alignment (Rosti et al. 2008, Li et al. 2009). Such methods align hypotheses to a partially constructed CN in some order. For example, if in such method, Hypothesis 3 is first aligned to the backbone, followed by Hypothesis 1, we are likely to arrive at the CN in Figure 2b) in which the two instances of Jeep are aligned. However, if Hypothesis 1 is aligned to the backbone first, we would still get the CN in Figure 2a). Notice that the desirable output “She bought the Jeep SUV” cannot be generated from either of the confusion networks because a rereordering of columns would be required. A common characteristic of CN-based"
D09-1125,N09-2052,1,0.685122,"e not collapsed to be one single candidate since they have different original word positions. We need to trace each of them separately during the decoding process. computed based on a weighted voting score: 4 And the global bi-gram voting feature is defined as: A Joint Optimization Framework For System Combination ? ??,? ? ?? = ? ??,? ? ?? ?1 , … , ?? ? = ?=1 and M is the number of CS generated. Note that M may be larger than the length of the output word sequence w since some CS may generate empty words. Bi-gram voting model: The second feature we used is a bi-gram voting feature proposed by Zhao and He (2009), i.e., for each bi-gram ?? , ??+1 , a weighted position-independent voting score is computed: ? ? ?? , ??+1 ? = ?∗ = argmax ??? ? ∈?,?∈?,?∈? ?? ⋅ ?? (?, ?, ?, ?) ?=1 where we denote by C the set of all possible valid arrangements of CS, O the set of all possible orders of CS, W the set of all possible word sequences, consisting of words from the input hypotheses. {?? (?, ?, ?, ?)} are the features and {?? } are the feature weights in the log-linear model, respectively. 4.1 Features A set of features are used in this framework. Each of them models one or more of the alignment, ordering, and le"
D09-1125,E06-1005,0,0.209061,"Missing"
D09-1125,C08-1074,0,0.00875285,"the joint decoding approach. Some of these features are constant across decoding hypotheses and can be ignored. The non-constant features are word posterior, bigram voting, language model score, and word count. They are computed in the same way as for the joint decoding approach. System weights and feature weights are trained together using Powell's search for the IHMM-based approach. Then the same system weights are applied to both IncHMM and Joint Decoding -based approaches, and the feature weights of them are trained using the max-BLEU training method proposed by Och (2003) and refined by Moore and Quirk (2008). Table 1: Performance of individual systems on the dev and test set System ID System A System B System C System D System E 6.2 dev 32.88 32.82 32.16 31.40 27.44 test 31.81 32.03 31.87 31.32 27.67 Comparison against baselines Table 2 lists the BLEU scores achieved by the two baselines and the joint decoding approach. Both baselines surpass the best individual system 1209 significantly. However, the gain of incremental HMM over IHMM is smaller than that reported in Li et al. (2009). One possible reason of such discrepancy could be that fewer hypotheses are used for combination in this experimen"
D09-1125,D07-1029,0,\N,Missing
D09-1125,W09-0408,0,\N,Missing
D09-1125,W05-0801,0,\N,Missing
D09-1125,W07-0711,1,\N,Missing
D09-1125,2005.eamt-1.20,0,\N,Missing
D09-1125,2008.amta-srw.3,0,\N,Missing
D09-1125,P09-1107,1,\N,Missing
D09-1125,P03-1021,0,\N,Missing
D09-1125,N06-1014,0,\N,Missing
D09-1125,P04-1066,0,\N,Missing
D11-1033,eck-etal-2004-language,0,0.0830212,"significant interest in using two translation models, one trained on a larger general-domain corpus and the other on a smaller in-domain corpus, to translate in-domain text. After all, if one has access to an in-domain corpus with which to select data from a general-domain corpus, then one might as well use the in-domain data, too. The expectation is that the larger general-domain model should dominate in regions where the smaller in-domain model lacks coverage due to sparse (or non-existent) ngram counts. In practice, most practical systems also perform target-side language model adaptation (Eck et al., 2004); we eschew this in order to isolate the effects of translation model adaptation alone. Directly concatenating the phrase tables into one larger one isn’t strongly motivated; identical phrase pairs within the resulting table can lead to unpredictable behavior during decoding. Nakov (2008) handled identical phrase pairs by prioritizing the source tables, however in our experience identical entries in phrase tables are not very common when comparing across domains. Foster and Kuhn (2007) interpolated the in- and general-domain phrase tables together, assigning either linear or log-linear weights"
D11-1033,W07-0711,1,0.148887,"baseline single-corpus systems are in Table 1. Corpus IWSLT General Phrases 515k 1,478m Dev 45.43 42.62 Test 37.17 40.51 Table 1: Baseline translation results for in-domain and general-domain systems. 4 357 System Description In order to highlight the data selection work, we used an out-of-the-box Moses framework using GIZA++ (Och and Ney, 2003) and MERT (Och, 2003) to train and tune the machine translation systems. The only exception was the phrase table for the large out-of-domain system trained on 12m sentence pairs, which we trained on a cluster using a word-dependent HMM-based alignment (He, 2007). We used the Moses decoder to produce all the system outputs, and scored them with the NIST mt-eval31a 4 tool used in the IWSLT evalutation. 3.4 We conducted our experiments on the International Workshop on Spoken Language Translation (IWSLT) Chinese-to-English DIALOG task 2 , consisting of transcriptions of conversational speech in a travel setting. Two corpora are needed for the adaptation task. Our in-domain data consisted of the IWSLT corpus of approximately 30,000 sentences in Chinese and English. Our general-domain corpus was 12 million parallel sentences comprising a variety of publicl"
D11-1033,D07-1036,0,0.420476,"Missing"
D11-1033,J03-1002,0,0.0408997,"ed above, on the IWSLT corpus. The resulting model had a phrase table with 515k entries. The general-domain baseline was substantially larger, having been trained on 12 million sentence pairs, and had a phrase table containing 1.5 billion entries. The BLEU scores of the baseline single-corpus systems are in Table 1. Corpus IWSLT General Phrases 515k 1,478m Dev 45.43 42.62 Test 37.17 40.51 Table 1: Baseline translation results for in-domain and general-domain systems. 4 357 System Description In order to highlight the data selection work, we used an out-of-the-box Moses framework using GIZA++ (Och and Ney, 2003) and MERT (Och, 2003) to train and tune the machine translation systems. The only exception was the phrase table for the large out-of-domain system trained on 12m sentence pairs, which we trained on a cluster using a word-dependent HMM-based alignment (He, 2007). We used the Moses decoder to produce all the system outputs, and scored them with the NIST mt-eval31a 4 tool used in the IWSLT evalutation. 3.4 We conducted our experiments on the International Workshop on Spoken Language Translation (IWSLT) Chinese-to-English DIALOG task 2 , consisting of transcriptions of conversational speech in a"
D11-1033,P03-1021,0,0.33736,". The resulting model had a phrase table with 515k entries. The general-domain baseline was substantially larger, having been trained on 12 million sentence pairs, and had a phrase table containing 1.5 billion entries. The BLEU scores of the baseline single-corpus systems are in Table 1. Corpus IWSLT General Phrases 515k 1,478m Dev 45.43 42.62 Test 37.17 40.51 Table 1: Baseline translation results for in-domain and general-domain systems. 4 357 System Description In order to highlight the data selection work, we used an out-of-the-box Moses framework using GIZA++ (Och and Ney, 2003) and MERT (Och, 2003) to train and tune the machine translation systems. The only exception was the phrase table for the large out-of-domain system trained on 12m sentence pairs, which we trained on a cluster using a word-dependent HMM-based alignment (He, 2007). We used the Moses decoder to produce all the system outputs, and scored them with the NIST mt-eval31a 4 tool used in the IWSLT evalutation. 3.4 We conducted our experiments on the International Workshop on Spoken Language Translation (IWSLT) Chinese-to-English DIALOG task 2 , consisting of transcriptions of conversational speech in a travel setting. Two c"
D11-1033,D10-1044,0,\N,Missing
D11-1033,D09-1074,0,\N,Missing
D11-1033,W08-0320,0,\N,Missing
D11-1033,P10-2041,0,\N,Missing
D11-1033,P07-2045,0,\N,Missing
D11-1033,W07-0733,0,\N,Missing
D11-1033,W07-0702,0,\N,Missing
D11-1033,I08-2088,0,\N,Missing
D11-1033,W07-0717,0,\N,Missing
D12-1061,J93-2003,0,0.0377637,"id sparse data problems while the correlation model relies on pure counts of term frequencies. However, the SMT system is used as a black box in their experiments. So the relative contribution of different SMT components is not verified empirically. In this study we break this black box in order to build a better, simpler QE system. We will show that the proposed lexicon models outperform significantly the term correlation model, and that a simpler QE system that incorporates the lexicon models can beat the sophisticated, black-box SMT system. 2.1 The word model takes the form of IBM Model 1 (Brown et al. 1993; Berger and Lafferty 1999). Let be a query, be an expansion term candidate, the translation probability from to is defined as | We view search queries and Web documents as two different languages, and cast QE as a means to bridge the language gap by translating queries to documents, represented by their titles. In this section, we will describe three translation models that are based on terms, triplets, and topics, respectively, and the way these models are learned from query-title pairs extracted from clickthrough data. 667 ∑ ( |) ( |) (1) |is the unsmoothed unigram probawhere bility of word"
D12-1061,C10-1041,1,0.928271,"re composed using different vocabularies and language styles. Query expansion (QE) is an effective strategy to address the problem. It expands a query issued by a user with additional related terms, called expansion terms, so that more relevant documents can be retrieved. In this paper we explore the use of clickthrough data and translation models for QE. We select expansion terms for a query according to how likely it is that the expansion terms occur in the title of a document that is relevant to the query. Assuming that a query is parallel to the titles of documents clicked for that query (Gao et al. 2010a), three lexicon models are trained on query-title pairs extracted from clickthrough data. The first is a word model that learns the translation probability between single words. The second model uses lexicalized triplets to incorporate word dependencies for translation. The third is a bilingual topic model, which represents a query as a distribution of hidden topics and learns the translation between a query and a title term at the semantic level. We will show that the word model provides a rich set of expansion candidates while the triplet and topic models can effectively select good expans"
D12-1061,D08-1039,0,0.0149355,"ating document titles from queries over the entire training corpus: ∏ | (2) where both the titles and the paired queries are viewed as bag of words. The translation probability | takes the form of IBM Model 1 as | ∏∑ ( | ) (3) where is a constant, is the length of , and is the length of . To find the optimal word translation probabilities of IBM Model 1, we used the EM algorithm, where the number of iterations is determined empirically on held-out data. 2.2 2 Lexicon Models Word Model Triplet Model The word model is context independent. The triplet model, which is originally proposed for SMT (Hasan et al. 2008), is intended to capture inter-term dependencies for selecting expansion terms. The model is based on lexicalized triplets ( ) which can be understood as two query terms triggering one expansion term. The translation probability of given for the triplet model is parameterized as | ∑ ∑ ( | ) (4) where Z is a normalization factor based on the corresponding query length, i.e., , and ( | ) is the probability of translating into given another query word . Since can be any word in that is not necessary to be adjacent to , the triple model is able to combine local (i.e. word and phrase level) and glo"
D12-1061,N03-1017,0,0.0589454,"n is that Cui et al. performed the evaluation using documents and search logs collected from the Encarta website, which is much cleaner and more homogenous than the data sets we used. The result suggests that although QE improves the recall of relevant documents, it is also likely to introduce noise that hurts the precision of document retrieval. SMT (Row 3) is a SMT-based QE system. Following Riezler et al. (2008), the system is an implementation of a phrase-based SMT system with a standard set of features for translation model and language model, combined under a log linear model framework (Koehn et al. 2003). Different from Riezler et al.’s system where the translation model is trained on query-snippet pairs and the language model on queries, in our implementation the trans673 lation model is trained on query-title pairs and the language model on titles. To apply the system to QE, expansion terms of a query are taken from those terms in the 10-best translations of the query that have not been seen in the original query string. We see that SMT significantly outperforms TC in NDCG at all levels. The result confirms the conclusion of Riezler et al., demonstrating that context information is crucial"
D12-1061,N04-4024,0,0.255327,") where is the unigram probability computed using a unigram language model trained on the collection of document titles. The fifth type of cliques contains a pair of terms appearing in consecutive order in the expansion. The potential functions are defined as ∑ ∑ ∑ ∑ ∑ ∑ (10) where there are in total 8 ’s to be estimated. Although the MRF is by nature a generative | model, it is not always appropriate to train the parameters using conventional likelihood based apwhere | is the bigram probability com- proaches due to the metric divergence problem puted using a bigram language model trained on (Morgan et al. 2004): i.e., the maximum likelihood the collection of document titles. estimate is unlikely to be the one that optimizes the The sixth type of cliques contains a pair of evaluation metric. In this study the effectiveness of terms appearing unordered within the expansion. a QE method is evaluated by first issuing a set of The potential functions are defined as queries which are expanded using the method to a (11) search engine and then measuring the Web search performance. Better QE methods are supposed to | lead to better Web search results using the correspondingly expanded query set. For this rea"
D12-1061,C08-1093,0,0.273874,"onal Linguistics tomatically from document collections (Jing and Croft 1994), the log-based method is superior in that it explicitly captures the correlation between query terms and document terms, and thus can bridge the lexical gap between them more effectively. Second, since search logs retrain querydocument pairs clicked by millions of users, the term correlations reflect the preference of the majority of users. Third, the term correlations evolve along with the accumulation of user logs, thus can reflect updated user interests at a specific time. However, as pointed out by Riezler et al. (2008), Cui et al.’s correlation-based method suffers low precision of QE partly because the correlation model does not explicitly capture context information and is susceptible to noise. Riezler et al. developed a QE system by retraining a standard phrase-based statistical machine translation (SMT) system using query-snippet pairs extracted from clickthrough data (Riezler et al. 2008; Riezler and Liu 2010). The SMT-based system can produce cleaner, more relevant expansion terms because rich context information useful for filtering noisy expansions is captured by combining language model and phrase"
D12-1061,J10-3010,0,0.0113366,"ence of the majority of users. Third, the term correlations evolve along with the accumulation of user logs, thus can reflect updated user interests at a specific time. However, as pointed out by Riezler et al. (2008), Cui et al.’s correlation-based method suffers low precision of QE partly because the correlation model does not explicitly capture context information and is susceptible to noise. Riezler et al. developed a QE system by retraining a standard phrase-based statistical machine translation (SMT) system using query-snippet pairs extracted from clickthrough data (Riezler et al. 2008; Riezler and Liu 2010). The SMT-based system can produce cleaner, more relevant expansion terms because rich context information useful for filtering noisy expansions is captured by combining language model and phrase translation model in its decoder. Furthermore, in the SMT system all component models are properly smoothed using sophisticated techniques to avoid sparse data problems while the correlation model relies on pure counts of term frequencies. However, the SMT system is used as a black box in their experiments. So the relative contribution of different SMT components is not verified empirically. In this s"
D14-1002,P14-1066,1,0.0924512,"de PLSA (Hofmann 1990) and LDA (Blei et al. 2003). Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., Dumais et al. 1997; Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associated features at different levels of abstraction useful for a variety of tasks (e.g., Collobert et al. 2011; Hinton et al. 2012; Socher et al. 2012; Krizhevsky et al., 2012; Gao et al. 2014). Hinton and Salakhutdinov (2010) propose the most original approach based on an unsupervised version of the deep neural network to discover the hierarchical semantic structure embedded in queries and documents. Huang et al. (2013) significantly extends the approach so that the deep neural network can be trained on large-scale query-document pairs giving much better performance. The use of the convolutional neural network for text processing, central to our DSSM, was also described in Collobert et al. (2011) and Shen et al. (2014) but with very different applications. The DSSM described in Sec"
D14-1002,J93-2003,0,0.0385454,"combination (Row 9) is obtained by incorporating the DSSM feature vectors of source and target documents (i.e., 600 features in total) in the ranker. We thus conclude that on both tasks, automatic highlighting and contextual entity search, features drawn from the output layers of our deep semantic model result in significant gains after being added to a set of non-semantic features, and in comparison to other types of semantic models used in the past. |is the unigram probability of word where | in , and is the probability of translating into , trained on source-target document pairs using EM (Brown et al. 1993). The translation-based approach allows any pair of non-identical but semantically related words to have a nonzero matching score. As a result, it significantly outperforms BM25. BTLM (Row 4) follows the best performing bilingual topic model described in Gao et al. (2011), which is an extension of PLSA (Hofmann 1999). The model is trained on source-target document pairs using the EM algorithm with a constraint enforcing a source document and its target document to not only share the same prior topic distribution, but to also have similar fractions of words assigned to each topic. BLTM defines"
D14-1002,C14-1140,1,0.829571,"Missing"
D14-1002,D10-1025,0,0.0126544,"for interestingness. The task of contextual entity search, which is formulated as an information retrieval problem in this paper, is also related to research on entity resolution (Stefanidis et al. 2013). Latent Semantic Analysis (Deerwester et al. 1990) is arguably the earliest semantic model designed for IR. Generative topic models widely used for IR include PLSA (Hofmann 1990) and LDA (Blei et al. 2003). Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., Dumais et al. 1997; Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associated features at different levels of abstraction useful for a variety of tasks (e.g., Collobert et al. 2011; Hinton et al. 2012; Socher et al. 2012; Krizhevsky et al., 2012; Gao et al. 2014). Hinton and Salakhutdinov (2010) propose the most original approach based on an unsupervised version of the deep neural network to discover the hierarchical semantic structure embedded in queries and documents. Huang et al. (2013) significa"
D14-1002,D12-1110,0,0.00801862,"erative topic models widely used for IR include PLSA (Hofmann 1990) and LDA (Blei et al. 2003). Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., Dumais et al. 1997; Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associated features at different levels of abstraction useful for a variety of tasks (e.g., Collobert et al. 2011; Hinton et al. 2012; Socher et al. 2012; Krizhevsky et al., 2012; Gao et al. 2014). Hinton and Salakhutdinov (2010) propose the most original approach based on an unsupervised version of the deep neural network to discover the hierarchical semantic structure embedded in queries and documents. Huang et al. (2013) significantly extends the approach so that the deep neural network can be trained on large-scale query-document pairs giving much better performance. The use of the convolutional neural network for text processing, central to our DSSM, was also described in Collobert et al. (2011) and Shen et al. (2014) but with very differ"
D14-1002,W11-0329,0,0.0767306,"eywords of local feature vectors, the document. Figure 3 presents a sample of document snippets and their keywords detected by the DSSM according to the procedure elaborated in Figure 2. It is interesting to see that many names are identified as keywords although the DSSM is not designed explicitly for named entity recognition. useful to the corresponding tasks, with a manageable vector size. tanh 1 where ces. 3.2 and tanh (3) tanh (4) are learned linear projection matriTraining the DSSM To optimize the parameters of the DSSM of Figure 1, i.e., , , , we use a pair-wise rank loss as objective (Yih et al. 2011). Consider a source document and two candidate target documents and , where is more interesting than to a user when reading . We construct and , , where two pairs of documents , the former is preferred and should have a higher (2) where the max operation is performed for each dimension of across 1, … , respectively. 5 … the comedy festival formerly known as the us comedy arts festival is a comedy festival held each year in las vegas nevada from its 1985 inception to 2008 . it was held annually at the wheeler opera house and other venues in aspen colorado . the primary sponsor of the festival w"
D14-1002,2011.iwslt-evaluation.19,0,\N,Missing
D14-1002,N09-1054,0,\N,Missing
D16-1166,P14-1133,0,0.0433121,"Missing"
D16-1166,D13-1160,0,0.225736,"Missing"
D16-1166,D14-1067,0,0.112662,"Missing"
D16-1166,P16-1160,0,0.0516014,"Missing"
D16-1166,P16-1076,0,0.528029,"Missing"
D16-1166,C14-1008,0,0.0101052,"Missing"
D16-1166,W15-3904,0,0.0189679,"Missing"
D16-1166,S14-2115,0,0.0332008,"Missing"
D16-1166,P13-1158,0,0.0316285,"Missing"
D16-1166,W03-0428,0,0.147603,"Missing"
D16-1166,N15-1173,0,0.00797439,"Missing"
D16-1166,P14-2105,1,0.92245,"Missing"
D16-1166,P15-1128,1,0.445879,"Missing"
D16-1166,Q15-1023,0,\N,Missing
D16-1166,P11-1060,0,\N,Missing
D16-1166,W12-3016,0,\N,Missing
D16-1189,P09-1010,0,0.0402293,"al., 2012; Sordoni et al., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity"
D16-1189,P11-1028,0,0.0834282,"l., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from t"
D16-1189,P16-1153,1,0.917358,"2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from these other text-based reinforcement learning tasks in that the language in both state and action spaces is unconstrained and"
D16-1189,D15-1239,1,0.645903,"rements) of popularity, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), the rate of posts related to a source rumor (Lukasik et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for in the karma score. Of the prior work on Reddit, the task explored here is most similar to (Jaech et al., 2015) in that it involves choosing relatively high karma comments (or threads) from a time-limited set rather than directly predicting comment (or post) karma. Prior work on popularity prediction used supervised learning; this is the first work that frames tracking hot topics in social media with deep reinforcement learning. 4 4.1 Characterizing a combinatorial action space Notatio"
D16-1189,P15-2085,0,0.0167351,"hus, our work is more closely related to popularity prediction for social media and online news. These studies have explored a variety of definitions (or measurements) of popularity, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), the rate of posts related to a source rumor (Lukasik et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for in the karma score. Of the prior work on Reddit, the task explored here is most similar to (Jaech et al., 2015) in that it involves choosing relatively high karma comments (or threads) from a time-limited set rather than directly predicting comment (or post) karma. Prior work on popularity prediction used supervised learning; this"
D16-1189,D15-1001,0,0.0186737,"gress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from these other text-based reinforcement"
D16-1189,D16-1261,0,0.0321309,"t al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from these other text-based reinforcement learning tasks in that the language in both state and action spaces is unconstrained and quite rich. Dulac-Arnold et al. (2016) also investigated a problem of large discrete action spaces. A Wolpertinger architecture is proposed to reduce computational complexity of evaluating all actions. While a combinatorial action space can be large and discrete, their method"
D16-1189,N15-1020,1,0.920583,"deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from these other text-based reinforcement"
D16-1189,P14-1017,0,0.1357,"l is not to track topics based on frequency, but rather based on reader response. Thus, our work is more closely related to popularity prediction for social media and online news. These studies have explored a variety of definitions (or measurements) of popularity, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), the rate of posts related to a source rumor (Lukasik et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for in the karma score. Of the prior work on Reddit, the task explored here is most similar to (Jaech et al., 2015) in that it involves choosing relatively high karma comments (or threads) from a time-limited set rather than directly predicting comm"
D16-1238,D15-1159,0,0.523057,"y component that stores continuous embeddings for all headwords. In other words, we consider all possible arcs during the parsing. This formulation is adopted by graph-based parsers such as the MSTParser (McDonald et al., 2005). The consideration 2204 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2204–2214, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2 ) for a sentence with n words. This is achieved by feeding the RNN in the query component with a soft headword embedding, which is comput"
D16-1238,P16-1231,0,0.528654,"Missing"
D16-1238,D15-1041,0,0.0751496,"ings for all headwords. In other words, we consider all possible arcs during the parsing. This formulation is adopted by graph-based parsers such as the MSTParser (McDonald et al., 2005). The consideration 2204 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2204–2214, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2 ) for a sentence with n words. This is achieved by feeding the RNN in the query component with a soft headword embedding, which is computed as the probability-weighted sum of all head"
D16-1238,D12-1133,0,0.0746592,"et al., 2013) on Chinese Gigawords (Graff et al., 2005). 2209 † and ∗ are provided in (Alberti et al., 2015) and (Andor et al., 2016), respectively. C&M (2014) Dyer et al. (2015) BiAtt-DP Dev UAS LAS 84.0 82.4 87.2 85.9 87.7 85.3 Test UAS LAS 83.9 82.4 87.2 85.7 88.1 85.7 Table 2: Parsing accuracy on CTB dev and test sets. the transition-based parsers, it achieves better accuracy than Chen and Manning (2014), which uses a feed-forward neural network, and Dyer et al. (2015), which uses three stack LSTM networks. Compared with the integrated parsing and tagging models, the BiAtt-DP outperforms Bohnet and Nivre (2012) but has a small gap to Alberti et al. (2015). On CTB, it achieves best UAS and similar LAS. This may be caused by that the relation vocabulary size is relatively smaller than the average sentence length, which biases the joint objective to be more sensitive to UAS. The parsing speed is around 50–60 sents/sec measured on a desktop with Intel Core i7 CPU @ 3.33GHz using single thread. Next, in Table 3 we show the parsing accuracy of the proposed BiAtt-DP on 12 languages in the CoNLL 2006 shared task, including comparison with state-of-the-art parsers. Specifically, we show UAS of the 3rd-order"
D16-1238,W06-2920,0,0.243172,"AS Scores of MSTParsers Language Arabic Bulgarian Czech Danish Dutch German Japanese Portuguese Slovene Spanish Swedish Turkish Average 1st-order 78.30 (2.02) 90.98 (3.00) 86.18 (4.88) 89.84 (1.80) 82.89 (4.54) 89.54 (3.17) 93.38 (0.14) 89.92 (3.17) 82.09 (4.54) 83.79 (4.59) 88.27 (1.95) 74.81 (3.74) 85.83 (2.85) 2nd-order 78.75 (1.57) 91.56 (2.42) 87.30 (3.76) 90.50 (1.14) 84.11 (3.32) 90.14 (2.57) 92.92 (0.60) 91.08 (2.01) 83.25 (3.38) 84.33 (4.05) 89.05 (1.17) 74.39 (4.16) 86.45 (2.23) Table 5: UAS scores of 1st-order and 2-nd order MSTParsers on 12 languages in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). We use the numbers reported in (Lei et al., 2014). Numbers in brackets indicate the absolute improvement of the proposed BiAtt-DP over the MSTParsers. References Here, we use the following definition of squared Hellinger distance for countable space √ 1X √ ( pi − qi )2 2 i where p, q ∈ ∆k are two k-simplexes. Introducing g ∈ ∆k , the squared Hellinger distance can be upper bounded as H 2 (p, q) ≤ inequality defined for a metric, and the CauchySchwarz’s inequality, respectively. Using the relationship between the KL-divergence and the squared Hellinger distance, (8) can be further bounded by"
D16-1238,D14-1082,0,0.880815,"ar order via sequentially querying the memory component that stores continuous embeddings for all headwords. In other words, we consider all possible arcs during the parsing. This formulation is adopted by graph-based parsers such as the MSTParser (McDonald et al., 2005). The consideration 2204 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2204–2214, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2 ) for a sentence with n words. This is achieved by feeding the RNN in the query component with"
D16-1238,de-marneffe-etal-2006-generating,0,0.168224,"Missing"
D16-1238,P15-1033,0,0.296271,"s continuous embeddings for all headwords. In other words, we consider all possible arcs during the parsing. This formulation is adopted by graph-based parsers such as the MSTParser (McDonald et al., 2005). The consideration 2204 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2204–2214, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2 ) for a sentence with n words. This is achieved by feeding the RNN in the query component with a soft headword embedding, which is computed as the probabili"
D16-1238,D10-1125,0,0.165089,"Missing"
D16-1238,P14-1130,0,0.540772,"berti et al. (2015). On CTB, it achieves best UAS and similar LAS. This may be caused by that the relation vocabulary size is relatively smaller than the average sentence length, which biases the joint objective to be more sensitive to UAS. The parsing speed is around 50–60 sents/sec measured on a desktop with Intel Core i7 CPU @ 3.33GHz using single thread. Next, in Table 3 we show the parsing accuracy of the proposed BiAtt-DP on 12 languages in the CoNLL 2006 shared task, including comparison with state-of-the-art parsers. Specifically, we show UAS of the 3rd-order RBGParser as reported in (Lei et al., 2014) since it also uses low-dimensional continuous embeddings. However, there are several major differences between the RBGParser and the BiAtt-DP. First, in (Lei et al., 2014), the lowdimensional continuous embeddings are derived Language Arabic Bulgarian Czech Danish Dutch German Japanese Portuguese Slovene Spanish Swedish Turkish BiAtt-DP 80.34 [68.58] 93.96 [89.55] 91.16 [85.14] 91.56 [85.53] 87.15 [82.41] 92.71 [89.80] 93.44 [90.67] 92.77 [88.44] 86.01 [75.90] 88.74 [84.03] 90.50 [84.05] 78.43 [66.16] RBGParser 79.95 93.50 90.50 91.39 86.41 91.97 93.71 91.92 86.24 88.00 91.00 76.84 Best Publi"
D16-1238,P14-2050,0,0.0429362,"4 (Czech), 176 (Danish), 220 (Dutch), 200 (German), 128 (Japanese), 168 (Portuguese), 128 (Slovene), 144 (Spanish), 176 (Swedish), and 128 (Turkish). 4.3 Type Results We first compare our parser with state-of-the-art neural transition-based dependency parsers on PTB and CTB. For English, we also compare with stateof-the-art graph-based dependency parsers. The results are shown in Table 1 and Table 2, respectively. It can be seen that the BiAtt-DP outperforms all other graph-based parsers on PTB. Compared with 3 For English, we use the dependency-based word embeddings at https://goo.gl/tWke3I (Levy and Goldberg, 2014). For Chinese, we pre-train 192-dimension skip-gram embeddings (Mikolov et al., 2013) on Chinese Gigawords (Graff et al., 2005). 2209 † and ∗ are provided in (Alberti et al., 2015) and (Andor et al., 2016), respectively. C&M (2014) Dyer et al. (2015) BiAtt-DP Dev UAS LAS 84.0 82.4 87.2 85.9 87.7 85.3 Test UAS LAS 83.9 82.4 87.2 85.7 88.1 85.7 Table 2: Parsing accuracy on CTB dev and test sets. the transition-based parsers, it achieves better accuracy than Chen and Manning (2014), which uses a feed-forward neural network, and Dyer et al. (2015), which uses three stack LSTM networks. Compared wi"
D16-1238,N06-1014,0,0.0587503,"reviously developed uni-directional counterpart, because the former exploits richer contextual information. Intuitively, we can use two separate uni-directional RNNs where each one constructs its respective attended encoder context vectors for computing RNN hidden states. However, the drawback of this approach is that the decoder would often produce different alignments resulting in discrepancies for the forward and backward directions. In this paper, we design a training objective function to enforce attention agreement between both directions, inspired by the alignmentby-agreement idea from Liang et al. (2006). Specifically, we develop a dependency parser (BiAtt-DP) using a bi-directional attention model based on the memory network. Given that the golden alignment is observed for dependency parsing in the training stage, we further derive a simple and interpretable approximation for the agreement objective, which makes a natural connection between the latent and observed alignment cases. The proposed BiAtt-DP parses a sentence in a linear order via sequentially querying the memory component that stores continuous embeddings for all headwords. In other words, we consider all possible arcs during the"
D16-1238,D10-1004,0,0.0192923,"headwords in our model. The model learns to retrieve the most relevant information from the input memory to make decisions on headwords and head-modifier relations. Graph-based Dependency Parsing: In addition to the transition-based parsers, another line of research in dependency parsing uses graph-based models. Graph-based parser usually build a dependency tree from a directed graph and learns to scoring the possible arcs. Due to this nature, nonprojective parsing can be done straightforwardly by most graph-based dependency parsers. The MSTParser (McDonald et al., 2005) and the TurboParser (Martins et al., 2010) are two examples of graphbased parsers. The MSTParser formulates the parsing as searching for the MST, whereas the TurboParser performs approximate variational inference over a factor graph. The RBGParser proposed in (Lei et al., 2014) can also be viewed as a graph-based parser, which scores arcs using low-dimensional continuous features derived from low-rank tensors as well as features used by MSTParser/TurboParser. It also employs a sampler-based algorithm for parsing (Zhang et al., 2014). Neural Attention Model: The proposed BiAttDP is closely related to the memory network (Sukhbaatar et a"
D16-1238,P13-2109,0,0.141805,"Missing"
D16-1238,E06-1011,0,0.745372,"ural Language Processing, pages 2204–2214, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2 ) for a sentence with n words. This is achieved by feeding the RNN in the query component with a soft headword embedding, which is computed as the probability-weighted sum of all headword embeddings in the memory component. To the best of our knowledge, this is the first attempt to apply memory network models to graphbased dependency parsing. Moreover, it is the first extension of neural attention models from unidirection to multi-direction by enforcing"
D16-1238,H05-1066,0,0.657082,"Missing"
D16-1238,W03-3017,0,0.43084,"Missing"
D16-1238,N15-1068,0,0.141622,"Missing"
D16-1238,N12-1054,0,0.0745638,"Missing"
D16-1238,N03-1033,0,0.0797064,"Missing"
D16-1238,P15-1032,0,0.63797,"y querying the memory component that stores continuous embeddings for all headwords. In other words, we consider all possible arcs during the parsing. This formulation is adopted by graph-based parsers such as the MSTParser (McDonald et al., 2005). The consideration 2204 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2204–2214, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2 ) for a sentence with n words. This is achieved by feeding the RNN in the query component with a soft headword embe"
D16-1238,W03-3023,0,0.371894,". Formally, for the forward case, the qt can ˜ t ; xt ]). Albe computed as qt = GRU (qt−1 , [m though the RNN is able to capture long-span context information to some extent, the local context may very easily dominate the hidden state. Therefore, this additional soft headword embedding allows the model to access long-span context information in a different channel. On the other hand, by recursively feeding both the query vector and the soft headword embedding into the RNN, the model implicitly captures high-order parsing history information, which can potentially improve the parsing accuracy (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006). However, for a graph-based dependency parser, utilizing parsing history features is computationally expensive. For example, an k-th order MSTParser (McDonald and Pereira, 2006) has O(nk+1 ) complexity for a sentence of n words. In contrast, the BiAtt-DP implicitly captures high-order parsing history while keeping the complexity in the order of O(n2 ), i.e. for each direction. we compute n(n+1) pair-wise probabilities at,j for t = 1, · · · , n and j = 0, · · · , n. In this paper, we choose to use soft headword embeddings rather than making hard decisions on headwo"
D16-1238,D08-1059,0,0.28016,"Missing"
D16-1238,D12-1030,0,0.0391693,"Missing"
D16-1238,P14-2107,0,0.13558,"Missing"
D16-1238,D13-1093,0,0.0277992,"Missing"
D16-1238,P14-1019,0,0.032224,"ly by most graph-based dependency parsers. The MSTParser (McDonald et al., 2005) and the TurboParser (Martins et al., 2010) are two examples of graphbased parsers. The MSTParser formulates the parsing as searching for the MST, whereas the TurboParser performs approximate variational inference over a factor graph. The RBGParser proposed in (Lei et al., 2014) can also be viewed as a graph-based parser, which scores arcs using low-dimensional continuous features derived from low-rank tensors as well as features used by MSTParser/TurboParser. It also employs a sampler-based algorithm for parsing (Zhang et al., 2014). Neural Attention Model: The proposed BiAttDP is closely related to the memory network (Sukhbaatar et al., 2015) for question answering, as well as the neural attention models for machine translation (Bahdanau et al., 2015) and constituency parsing (Vinyals et al., 2015b). The way we query the memory component and obtain the soft headword embeddings is essentially the attention mechanism. However, different from the above studies where the alignment information is latent, in dependency parsing, the arc between the modifier and headword is known during training. Thus, we can utilize these labe"
D17-1087,P16-1154,0,0.0218917,"Two-Stage SynNet To bootstrap our model fs we use a SynNet (Figure 1), which consists of answer synthesis and question synthesis modules, to generate data on pt . Our SynNet learns the conditional probability of generating answer a = {astart , aend } and question q = {q1 , ...qn } given paragraph p, P (q, a|p). 837 each time step i, a decoder network attends to both h and the previously generated question token qi−1 to produce a hidden representation ri . Since paragraphs may often have named entities and rare words not present during training, we incorporate a copy mechanism into our models (Gu et al., 2016). We use an architecture motivated by latent predictor networks (Ling et al., 2016) to force the model to learn when to copy vs. directly predict the word, without direct supervision of what action to choose. Specifically, at every time step i, two latent predictors generate the probability of generating word wi , a pointer network Cp (Vinyals et al., 2015) which can copy a word from the context paragraph, and a vocabulary predictor Vp which directly generates a probability distribution of choosing a word wi from a predefined vocabulary. The likelihood of choosing predictor k at time step i is"
D17-1087,P16-1002,0,0.0275531,"Missing"
D17-1087,D13-1160,0,0.100668,"Missing"
D17-1087,P14-1133,0,0.0613874,"Missing"
D17-1087,P08-1068,0,0.0649399,"Missing"
D17-1087,P16-1223,0,0.0743711,"Missing"
D17-1087,P15-1086,0,0.0479872,"Missing"
D17-1087,P17-1168,0,0.0376268,"5; Golub and He, 2016; Chen et al., 2016; Hermann et al., 2015). Machine comprehension, a form of extractive question answering where the answer is a snippet or multiple snippets of text within a context paragraph, has recently attracted a lot of attention in the community. The rise of large-scale human annotated datasets with over 100,000 realistic question-answer pairs such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and MSMARCO (Nguyen et al., 2016), has led to a large number of successful deep learning models (Lee et al., 2016; Seo et al., 2017; Xiong et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016). O … w1 Related Work wN Answer Network Figure 1: Illustration of the two-stage SynNet. The SynNet is trained to synthesize the answer and the question, given the paragraph. The first stage of the model, an answer synthesis module, uses a bi-directional LSTM to predict IOB tags on the input paragraph, which mark out key semantic concepts that are likely answers. The second stage, a question synthesis module, uses a uni-directional LSTM to generate the question, while attending on embeddings of the words in the paragraph and IOB ids. Although multiple spans in the paragra"
D17-1087,P16-1057,0,0.0534774,"Missing"
D17-1087,D16-1044,0,0.025425,"a, followed by generating the question q conditioned on the answer and paragraph. chine learning, such as machine translation (Zoph et al., 2016), computer vision, (Sharif Razavian et al., 2014), and speech recognition (Doulaty et al., 2015). Specifically, object recognition models trained on the large-scale ImageNet challenge (Russakovsky et al., 2015) have proven to be excellent feature extractors for diverse tasks such as image captioning (i.e., Lu et al. (2017); Fang et al. (2015); Karpathy and Fei-Fei (2015)) and visual question answering (i.e., Zhou et al. (2015); Xu and Saenko (2016); Fukui et al. (2016); Yang et al. (2016)), among others. In a similar fashion, we use a model pretrained on the SQuAD dataset as a generic feature extractor to bootstrap a QA system on NewsQA. 3 4.1.1 In our answer synthesis module we train a simple IOB tagger to predict whether each word in the paragraph is part of an answer or not. More formally, given a set of words in a paragraph p = {p1 ...pn }, our IOB tagging model learns the conditional probability of labels y1 ...yn , where y1 ∈ IOBSTART , IOBMID , IOBEND if a word pi is marked as an answer by the annotator in our train set, NONE otherwise. We use a bi-d"
D17-1087,D16-1166,1,0.883131,"Missing"
D17-1087,D14-1162,0,0.0890224,"le we train a simple IOB tagger to predict whether each word in the paragraph is part of an answer or not. More formally, given a set of words in a paragraph p = {p1 ...pn }, our IOB tagging model learns the conditional probability of labels y1 ...yn , where y1 ∈ IOBSTART , IOBMID , IOBEND if a word pi is marked as an answer by the annotator in our train set, NONE otherwise. We use a bi-directional Long-Short Term Memory Network (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) for tagging. Specifically, we project each word pi 7→ p∗i into a continuous vector space via pretrained GloVe embeddings (Pennington et al., 2014). We then run a Bi-LSTM over the word embeddings p∗1 , ...p∗n to produce a contextdependent word representation h1 , ...hn , which we feed into two fully connected layers followed by a softmax to produce our tag likelihoods for each word. We select all consecutive spans where y 6= NONE produced by the tagger as our candidate answer chunks, which we feed into our question synthesis module for question generation. The Transfer Learning Task for MC We formalize the task of machine comprehension below. Our MC model takes as input a tokenized question q = {q0 , q1 , ...qn }, a context paragraph p ="
D17-1087,D16-1264,0,0.101666,"B I O 2.1 … … w2 w3 Passage Text Paragraph … Question Answering Question answering is an active area in natural language processing with ongoing research in many directions (Berant et al., 2013; Hill et al., 2015; Golub and He, 2016; Chen et al., 2016; Hermann et al., 2015). Machine comprehension, a form of extractive question answering where the answer is a snippet or multiple snippets of text within a context paragraph, has recently attracted a lot of attention in the community. The rise of large-scale human annotated datasets with over 100,000 realistic question-answer pairs such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and MSMARCO (Nguyen et al., 2016), has led to a large number of successful deep learning models (Lee et al., 2016; Seo et al., 2017; Xiong et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016). O … w1 Related Work wN Answer Network Figure 1: Illustration of the two-stage SynNet. The SynNet is trained to synthesize the answer and the question, given the paragraph. The first stage of the model, an answer synthesis module, uses a bi-directional LSTM to predict IOB tags on the input paragraph, which mark out key semantic concepts that are likely answers. The"
D17-1087,N15-1057,0,0.0321446,"Missing"
D17-1087,P17-1096,0,0.0979199,"end and paragraph p = p1 ...pn , P (q1 , ...qn |p1 ...pn , astart , aend ). We decompose the joint probability distribution of generating all the question words q1 , ...qn into generating Qn the question one word at a time, i.e. i=1 P (qi |p, a, q1...i−1 ). The model is similar to an encoder-decoder network with attention (Bahdanau et al., 2014), which computes the conditional probability P (qi |p1 ...pn , astart , aend , q1...i−1 ). We run a Bi-LSTM over the paragraph to produce contextdependent word representations h = {h1 , ...hn }. To model where the answer is in the paragraph, similar to Yang et al. (2017), we insert answer information by appending a zero/one feature to the paragraph word embeddings. Then, at The Model Two-Stage SynNet To bootstrap our model fs we use a SynNet (Figure 1), which consists of answer synthesis and question synthesis modules, to generate data on pt . Our SynNet learns the conditional probability of generating answer a = {astart , aend } and question q = {q1 , ...qn } given paragraph p, P (q, a|p). 837 each time step i, a decoder network attends to both h and the previously generated question token qi−1 to produce a hidden representation ri . Since paragraphs may oft"
D17-1087,P16-1009,0,0.0307543,"Missing"
D17-1087,W17-2603,0,0.0208719,"Missing"
D17-1087,D16-1163,0,0.0544043,"Missing"
D17-1087,P15-1129,0,0.0588415,"Missing"
D17-1087,P16-1056,0,\N,Missing
D17-1254,C04-1051,0,0.0278951,"VSA∗ m-RNN‡ ρ uni-skip bi-skip† combine-skip† 4 4 4 Our Results hierarchical model+emb. composite model+emb. combine+emb. r † Table 4: Results for image-sentence ranking experiments on the COCO dataset. R@K denotes Recall@K (higher is better) and Med r is the median rank (lower is better). (†) taken from Kiros et al. (2015). (∗) taken from Karpathy and Fei-Fei (2015). (‡) taken from Mao et al. (2015). learning, with the autoencoder on all the data (labeled and unlabled), and the classifier only on the labeled data. Paraphrase detection Now we consider paraphrase detection on the MSRP dataset (Dolan et al., 2004). On this task, one needs to predict whether or not two sentences are paraphrases. The training set consists of 4076 sentence pairs, and the test set has 1725 pairs. As in Tai et al. (2015), given two sentence representations zx and zy , we first compute their element-wise product zx zy and their absolute difference |zx − zy |, and then concatenate them together. A logistic regression model is trained on top of the concatenated features to predict whether two sentences are paraphrases. We present our results on the last column of Table 3. Our best result is better than the other results that u"
D17-1254,D15-1181,0,0.0104769,"Missing"
D17-1254,N16-1162,0,0.63511,"hierarchical CNN-LSTM model. As in Kiros et al. (2015), we first train our proposed models on a large collection of novels. We then evaluate the CNN sentence encoder as a generic feature extractor for 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking and 5 standard classification benchmarks. In these experiments, we train a linear classifier on top of the extracted sentence features, without additional fine-tuning of the CNN. We show that our trained sentence encoder yields generic representations that perform as well as, or better, than those of Kiros et al. (2015); Hill et al. (2016), in all the tasks considered. Summarizing, the main contribution of this paper is a new class of CNN-LSTM encoder-decoder models that is able to leverage the vast quantity of unlabeled text for learning generic sentence representations. Inspired by the skip-thought model (Kiros et al., 2015), we have further explored different variants: (i) CNN is used as the sentence encoder rather than RNN; (ii) larger context windows are considered: we propose the hierarchical CNN-LSTM model to encode a sentence for predicting multiple future sentences. 2 2.1 Model description CNN-LSTM model Consider the s"
D17-1254,P15-1162,0,0.0141126,"M decoder for sentence `, x`,t−1 denotes the (s) word embedding for w`,t−1 , and h`,0 is fixed as a zero vector for all ` = 1, . . . , L. V is a weight matrix used for computing distribution over words. 3 Related work Various methods have been proposed for sentence modeling, which generally fall into two categories. The first consists of models trained specifically for a certain task, typically combined with downstream applications. Several models have been proposed along this line, ranging from simple additional composition of the word vectors (Mitchell and Lapata, 2010; Yu and Dredze, 2015; Iyyer et al., 2015) to those based on complex nonlinear functions like recursive neural networks (Socher et al., 2011, 2013), convolutional neural networks (Kalchbrenner et al., 2014; Hu et al., 2014; Johnson and Zhang, 2015; Zhang et al., 2015; Gan et al., 2017), and recurrent neural networks (Tai et al., 2015; Lin et al., 2017). 2393 The other category consists of methods aiming to learn generic sentence representations that can be used across domains. This includes the paragraph vector (Le and Mikolov, 2014), skip-thought vector (Kiros et al., 2015), and the sequential denoising autoencoders (Hill et al., 201"
D17-1254,N15-1011,0,0.00392349,". 3 Related work Various methods have been proposed for sentence modeling, which generally fall into two categories. The first consists of models trained specifically for a certain task, typically combined with downstream applications. Several models have been proposed along this line, ranging from simple additional composition of the word vectors (Mitchell and Lapata, 2010; Yu and Dredze, 2015; Iyyer et al., 2015) to those based on complex nonlinear functions like recursive neural networks (Socher et al., 2011, 2013), convolutional neural networks (Kalchbrenner et al., 2014; Hu et al., 2014; Johnson and Zhang, 2015; Zhang et al., 2015; Gan et al., 2017), and recurrent neural networks (Tai et al., 2015; Lin et al., 2017). 2393 The other category consists of methods aiming to learn generic sentence representations that can be used across domains. This includes the paragraph vector (Le and Mikolov, 2014), skip-thought vector (Kiros et al., 2015), and the sequential denoising autoencoders (Hill et al., 2016). Hill et al. (2016) also proposed a sentence-level log-linear bag-of-words (BoW) model, where a BoW representation of an input sentence is used to predict adjacent sentences that are also represented as"
D17-1254,D13-1176,0,0.0455306,"ences that are also represented as BoW. Most recently, Wieting et al. (2016); Arora et al. (2017); Pagliardini et al. (2017) proposed methods in which sentences are represented as a weighted average of fixed (pre-trained) word vectors. Our model falls into this category, and is most related to Kiros et al. (2015). However, there are two key aspects that make our model different from Kiros et al. (2015). First, we use CNN as the sentence encoder. The combination of CNN and LSTM has been considered in image captioning (Karpathy and Fei-Fei, 2015), and in some recent work on machine translation (Kalchbrenner and Blunsom, 2013; Meng et al., 2015; Gehring et al., 2016). Our utilization of a CNN is different, and more importantly, the ultimate goal of our model is different. Our work aims to use a CNN to learn generic sentence embeddings. Second, we use the hierarchical CNN-LSTM model to predict multiple future sentences, rather than the surrounding two sentences as in Kiros et al. (2015). Utilizing a larger context window aids our model to learn better sentence representations, capturing longer-term dependencies between sentences. Similar work to this hierarchical language modeling can be found in Li et al. (2015);"
D17-1254,P14-1062,0,0.488571,"d across a broad range of applications, demonstrate the superiority of the proposed model over competing methods. 1 Introduction Learning sentence representations is central to many natural language modeling applications. The aim of a model for this task is to learn fixedlength feature vectors that encode the semantic and syntactic properties of sentences. Deep learning techniques have shown promising performance on sentence modeling, via feedforward neural networks (Huang et al., 2013), recurrent neural networks (RNNs) (Hochreiter and Schmidhuber, 1997), convolutional neural networks (CNNs) (Kalchbrenner et al., 2014; Kim, 2014; Shen et al., 2014), and recursive neural networks (Socher et al., 2013). Most of these models are task-dependent: they are trained specifically for a certain task. However, these methods may become inefficient when we need to repeatedly learn sentence representations for a large number of different tasks, because they may require retraining a new model for each individual task. In this paper, in contrast, we are primarily interested in learning generic sentence representations that can be used across domains. Several approaches have been proposed for learning generic sentence embe"
D17-1254,D14-1181,0,0.146241,"pplications, demonstrate the superiority of the proposed model over competing methods. 1 Introduction Learning sentence representations is central to many natural language modeling applications. The aim of a model for this task is to learn fixedlength feature vectors that encode the semantic and syntactic properties of sentences. Deep learning techniques have shown promising performance on sentence modeling, via feedforward neural networks (Huang et al., 2013), recurrent neural networks (RNNs) (Hochreiter and Schmidhuber, 1997), convolutional neural networks (CNNs) (Kalchbrenner et al., 2014; Kim, 2014; Shen et al., 2014), and recursive neural networks (Socher et al., 2013). Most of these models are task-dependent: they are trained specifically for a certain task. However, these methods may become inefficient when we need to repeatedly learn sentence representations for a large number of different tasks, because they may require retraining a new model for each individual task. In this paper, in contrast, we are primarily interested in learning generic sentence representations that can be used across domains. Several approaches have been proposed for learning generic sentence embeddings. The"
D17-1254,P15-1107,0,0.0153397,"and Blunsom, 2013; Meng et al., 2015; Gehring et al., 2016). Our utilization of a CNN is different, and more importantly, the ultimate goal of our model is different. Our work aims to use a CNN to learn generic sentence embeddings. Second, we use the hierarchical CNN-LSTM model to predict multiple future sentences, rather than the surrounding two sentences as in Kiros et al. (2015). Utilizing a larger context window aids our model to learn better sentence representations, capturing longer-term dependencies between sentences. Similar work to this hierarchical language modeling can be found in Li et al. (2015); Sordoni et al. (2015); Lin et al. (2015); Wang and Cho (2016). Specifically, Li et al. (2015); Sordoni et al. (2015) uses an LSTM for the sentence encoder, while Lin et al. (2015) uses a bag-of-words to represent sentences. 4 Experiments We first provide qualitative analysis of our CNN encoder, and then present experimental results on 8 tasks: 5 classification benchmarks, paraphrase detection, semantic relatedness and image-sentence ranking. As in Kiros et al. (2015), we evaluate the capabilities of our encoder as a generic feature extractor. To further demonstrate the advantage of our learn"
D17-1254,C02-1150,0,0.180082,"else taking the two remaining cars . Table 2: Query-retrieval examples. In each case (block of rows), the first sentence is a query, while the second sentence is the retrieved result from a random subset of 1 million sentences from the BookCorpus dataset. in Theano (Bastien et al., 2012), using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory. capture semantic and syntax of the sentences. 4.1 Classification benchmarks We first study the task of sentence classification on 5 datasets: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005) and TREC (Li and Roth, 2002). On all the datasets, we separately train a logistic regression model on top of the extracted sentence features. We restrict our comparison to methods that also aims to learn generic sentence embeddings for fair comparison. We also provide the state-of-the-art results using task-dependent learning methods for reference. Results are summarized in Table 3. Our CNN encoder provides better results than the combine-skip model of Kiros et al. (2015) on all the 5 datasets. We highlight some observations. First, the autoencoder performs better than the future predictor, indicating that the intra-sent"
D17-1254,D15-1106,0,0.0183887,"hring et al., 2016). Our utilization of a CNN is different, and more importantly, the ultimate goal of our model is different. Our work aims to use a CNN to learn generic sentence embeddings. Second, we use the hierarchical CNN-LSTM model to predict multiple future sentences, rather than the surrounding two sentences as in Kiros et al. (2015). Utilizing a larger context window aids our model to learn better sentence representations, capturing longer-term dependencies between sentences. Similar work to this hierarchical language modeling can be found in Li et al. (2015); Sordoni et al. (2015); Lin et al. (2015); Wang and Cho (2016). Specifically, Li et al. (2015); Sordoni et al. (2015) uses an LSTM for the sentence encoder, while Lin et al. (2015) uses a bag-of-words to represent sentences. 4 Experiments We first provide qualitative analysis of our CNN encoder, and then present experimental results on 8 tasks: 5 classification benchmarks, paraphrase detection, semantic relatedness and image-sentence ranking. As in Kiros et al. (2015), we evaluate the capabilities of our encoder as a generic feature extractor. To further demonstrate the advantage of our learned generic sentence representations, we al"
D17-1254,S14-2001,0,0.0193605,"s can be seen, we obtain the same median rank as in Kiros et al. (2015), indicating that our encoder is as competitive as the skip-thought vectors (Kiros et al., 2015). The performance gain between our encoder and the combine-skip model of Kiros et al. (2015) on the R@1 score is significant, which shows that the CNN encoder has more discriminative power on re2397 trieving the most correct item than the skip-thought vector. Natural language processing (almost) from scratch. In JMLR. Semantic relatedness For our final experiment, we consider the task of semantic relatedness on the SICK dataset (Marelli et al., 2014), consisting of 9927 sentence pairs. Given two sentences, our goal is to produce a real-valued score between [1, 5] to indicate how semantically related two sentences are, based on human generated scores. We compute a feature vector representing the pair of sentences in the same way as on the MSRP dataset. We follow the method in Tai et al. (2015), and use the crossentropy loss for training. Results are summarized in Table 5. Our result is better than the combineskip model of Kiros et al. (2015). This suggests that CNN also provides competitive performance at matching human relatedness judgeme"
D17-1254,P15-1003,0,0.0128212,"as BoW. Most recently, Wieting et al. (2016); Arora et al. (2017); Pagliardini et al. (2017) proposed methods in which sentences are represented as a weighted average of fixed (pre-trained) word vectors. Our model falls into this category, and is most related to Kiros et al. (2015). However, there are two key aspects that make our model different from Kiros et al. (2015). First, we use CNN as the sentence encoder. The combination of CNN and LSTM has been considered in image captioning (Karpathy and Fei-Fei, 2015), and in some recent work on machine translation (Kalchbrenner and Blunsom, 2013; Meng et al., 2015; Gehring et al., 2016). Our utilization of a CNN is different, and more importantly, the ultimate goal of our model is different. Our work aims to use a CNN to learn generic sentence embeddings. Second, we use the hierarchical CNN-LSTM model to predict multiple future sentences, rather than the surrounding two sentences as in Kiros et al. (2015). Utilizing a larger context window aids our model to learn better sentence representations, capturing longer-term dependencies between sentences. Similar work to this hierarchical language modeling can be found in Li et al. (2015); Sordoni et al. (201"
D17-1254,P04-1035,0,0.086325,"e emma ’s hand and lead her to the first taxi , everyone else taking the two remaining cars . Table 2: Query-retrieval examples. In each case (block of rows), the first sentence is a query, while the second sentence is the retrieved result from a random subset of 1 million sentences from the BookCorpus dataset. in Theano (Bastien et al., 2012), using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory. capture semantic and syntax of the sentences. 4.1 Classification benchmarks We first study the task of sentence classification on 5 datasets: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005) and TREC (Li and Roth, 2002). On all the datasets, we separately train a logistic regression model on top of the extracted sentence features. We restrict our comparison to methods that also aims to learn generic sentence embeddings for fair comparison. We also provide the state-of-the-art results using task-dependent learning methods for reference. Results are summarized in Table 3. Our CNN encoder provides better results than the combine-skip model of Kiros et al. (2015) on all the 5 datasets. We highlight some observations. First, the autoencoder performs better t"
D17-1254,P05-1015,0,0.412458,"we can watch the city disappear behind us . i take emma ’s hand and lead her to the first taxi , everyone else taking the two remaining cars . Table 2: Query-retrieval examples. In each case (block of rows), the first sentence is a query, while the second sentence is the retrieved result from a random subset of 1 million sentences from the BookCorpus dataset. in Theano (Bastien et al., 2012), using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory. capture semantic and syntax of the sentences. 4.1 Classification benchmarks We first study the task of sentence classification on 5 datasets: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005) and TREC (Li and Roth, 2002). On all the datasets, we separately train a logistic regression model on top of the extracted sentence features. We restrict our comparison to methods that also aims to learn generic sentence embeddings for fair comparison. We also provide the state-of-the-art results using task-dependent learning methods for reference. Results are summarized in Table 3. Our CNN encoder provides better results than the combine-skip model of Kiros et al. (2015) on all the 5 datasets. We highlight some obse"
D17-1254,D13-1170,0,0.0089657,"over competing methods. 1 Introduction Learning sentence representations is central to many natural language modeling applications. The aim of a model for this task is to learn fixedlength feature vectors that encode the semantic and syntactic properties of sentences. Deep learning techniques have shown promising performance on sentence modeling, via feedforward neural networks (Huang et al., 2013), recurrent neural networks (RNNs) (Hochreiter and Schmidhuber, 1997), convolutional neural networks (CNNs) (Kalchbrenner et al., 2014; Kim, 2014; Shen et al., 2014), and recursive neural networks (Socher et al., 2013). Most of these models are task-dependent: they are trained specifically for a certain task. However, these methods may become inefficient when we need to repeatedly learn sentence representations for a large number of different tasks, because they may require retraining a new model for each individual task. In this paper, in contrast, we are primarily interested in learning generic sentence representations that can be used across domains. Several approaches have been proposed for learning generic sentence embeddings. The paragraphvector model of Le and Mikolov (2014) incorporates a global con"
D17-1254,P15-1150,0,0.655343,"into two categories. The first consists of models trained specifically for a certain task, typically combined with downstream applications. Several models have been proposed along this line, ranging from simple additional composition of the word vectors (Mitchell and Lapata, 2010; Yu and Dredze, 2015; Iyyer et al., 2015) to those based on complex nonlinear functions like recursive neural networks (Socher et al., 2011, 2013), convolutional neural networks (Kalchbrenner et al., 2014; Hu et al., 2014; Johnson and Zhang, 2015; Zhang et al., 2015; Gan et al., 2017), and recurrent neural networks (Tai et al., 2015; Lin et al., 2017). 2393 The other category consists of methods aiming to learn generic sentence representations that can be used across domains. This includes the paragraph vector (Le and Mikolov, 2014), skip-thought vector (Kiros et al., 2015), and the sequential denoising autoencoders (Hill et al., 2016). Hill et al. (2016) also proposed a sentence-level log-linear bag-of-words (BoW) model, where a BoW representation of an input sentence is used to predict adjacent sentences that are also represented as BoW. Most recently, Wieting et al. (2016); Arora et al. (2017); Pagliardini et al. (201"
D17-1254,P16-1125,0,0.0196992,". Our utilization of a CNN is different, and more importantly, the ultimate goal of our model is different. Our work aims to use a CNN to learn generic sentence embeddings. Second, we use the hierarchical CNN-LSTM model to predict multiple future sentences, rather than the surrounding two sentences as in Kiros et al. (2015). Utilizing a larger context window aids our model to learn better sentence representations, capturing longer-term dependencies between sentences. Similar work to this hierarchical language modeling can be found in Li et al. (2015); Sordoni et al. (2015); Lin et al. (2015); Wang and Cho (2016). Specifically, Li et al. (2015); Sordoni et al. (2015) uses an LSTM for the sentence encoder, while Lin et al. (2015) uses a bag-of-words to represent sentences. 4 Experiments We first provide qualitative analysis of our CNN encoder, and then present experimental results on 8 tasks: 5 classification benchmarks, paraphrase detection, semantic relatedness and image-sentence ranking. As in Kiros et al. (2015), we evaluate the capabilities of our encoder as a generic feature extractor. To further demonstrate the advantage of our learned generic sentence representations, we also fine-tune our trai"
D17-1254,N15-1091,0,0.010899,"Missing"
D17-1254,Q15-1017,0,0.0215656,"dden state of the LSTM decoder for sentence `, x`,t−1 denotes the (s) word embedding for w`,t−1 , and h`,0 is fixed as a zero vector for all ` = 1, . . . , L. V is a weight matrix used for computing distribution over words. 3 Related work Various methods have been proposed for sentence modeling, which generally fall into two categories. The first consists of models trained specifically for a certain task, typically combined with downstream applications. Several models have been proposed along this line, ranging from simple additional composition of the word vectors (Mitchell and Lapata, 2010; Yu and Dredze, 2015; Iyyer et al., 2015) to those based on complex nonlinear functions like recursive neural networks (Socher et al., 2011, 2013), convolutional neural networks (Kalchbrenner et al., 2014; Hu et al., 2014; Johnson and Zhang, 2015; Zhang et al., 2015; Gan et al., 2017), and recurrent neural networks (Tai et al., 2015; Lin et al., 2017). 2393 The other category consists of methods aiming to learn generic sentence representations that can be used across domains. This includes the paragraph vector (Le and Mikolov, 2014), skip-thought vector (Kiros et al., 2015), and the sequential denoising autoencod"
D18-1266,Q13-1005,0,0.294332,"er: England Figure 1: An example of semantic parsing from denotations. Given the table environment, map the question to an executable program that evaluates to the answer. Introduction Semantic parsing from denotations (SpFD) is the problem of mapping text to executable formal representations (or program) in a situated environment and executing them to generate denotations (or answer), in the absence of access to correct representations. Several problems have been handled within this framework, including question answering (Berant et al., 2013; Iyyer et al., 2017) and instructions for robots (Artzi and Zettlemoyer, 2013; Misra et al., 2015). Consider the example in Figure 1. Given the question and a table environment, a semantic parser maps the question to an executable program, in this case a SQL query, and then executes the query on the environment to generate the answer England. In the SpFD setting, the training data does not contain the correct programs. Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs and finds suitable candidates, and an update step that uses these programs to update the model. Figure 2 shows"
D18-1266,D13-1160,0,0.690977,"s Wales 25 5 5 8.8 Program: Select Nation Where Points is Maximum Answer: England Figure 1: An example of semantic parsing from denotations. Given the table environment, map the question to an executable program that evaluates to the answer. Introduction Semantic parsing from denotations (SpFD) is the problem of mapping text to executable formal representations (or program) in a situated environment and executing them to generate denotations (or answer), in the absence of access to correct representations. Several problems have been handled within this framework, including question answering (Berant et al., 2013; Iyyer et al., 2017) and instructions for robots (Artzi and Zettlemoyer, 2013; Misra et al., 2015). Consider the example in Figure 1. Given the question and a table environment, a semantic parser maps the question to an executable program, in this case a SQL query, and then executes the query on the environment to generate the answer England. In the SpFD setting, the training data does not contain the correct programs. Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs and finds suitable candidates, a"
D18-1266,D16-1245,0,0.0689966,"Missing"
D18-1266,W10-2903,1,0.86718,"eralization in model shaping. 6 Related Work Semantic Parsing from Denotation Mapping natural language text to formal meaning representation was first studied by Montague (1970). Early work on learning semantic parsers rely on labeled formal representations as the supervision signals (Zettlemoyer and Collins, 2005, 2007; Zelle and Mooney, 1993). However, because getting access to gold formal representation generally requires expensive annotations by an expert, distant supervision approaches, where semantic parsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017). Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates. Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man2449 Question “of these teams, which had m"
D18-1266,P17-1097,0,0.502504,"s approach can suffer from a divergence phenomenon whereby the score of spurious programs picked up by the search in the first epoch increases, making it more likely for the search to pick them up in the future. Such divergence issues are common with latent-variable learning and often require careful initialization to overcome (Rose, 1998). Unfortunately such initialization schemes are not applicable for deep neural networks which form the model of most successful semantic parsers today (Jia and Liang, 2016; Misra and Artzi, 2016; Iyyer et al., 2017). Prior work, such as ✏-greedy exploration (Guu et al., 2017), has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturating the search on high-scoring spurious programs. However, random noise need not bias the search towards the correct program(s). In this paper, we introduce a simple policy-shaping method to guide the search. This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs. 2444 1 This transformation preserves the answer of the question. Algorithm 1 Learning a semantic parser from denotation using generalized updates. Inp"
D18-1266,N12-1015,0,0.0607653,"Missing"
D18-1266,P17-1167,1,0.824952,"Missing"
D18-1266,P16-1002,0,0.0489753,"policy which is based on the score function, for example b✓ (y|x, t, z) / exp{score✓ (y, t)}. However, this approach can suffer from a divergence phenomenon whereby the score of spurious programs picked up by the search in the first epoch increases, making it more likely for the search to pick them up in the future. Such divergence issues are common with latent-variable learning and often require careful initialization to overcome (Rose, 1998). Unfortunately such initialization schemes are not applicable for deep neural networks which form the model of most successful semantic parsers today (Jia and Liang, 2016; Misra and Artzi, 2016; Iyyer et al., 2017). Prior work, such as ✏-greedy exploration (Guu et al., 2017), has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturating the search on high-scoring spurious programs. However, random noise need not bias the search towards the correct program(s). In this paper, we introduce a simple policy-shaping method to guide the search. This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs. 2444 1 This transformation preserves the answ"
D18-1266,D17-1160,0,0.302332,"n frequently co-occurring pairs of tokens in the program and instruction. For example, the token most is highly likely to co-occur with a correct program containing the keyword Max. This happens for the example in Figure 2. Similarly the token not may co-occur with the keyword NotEqual. We assume access to a lexicon ⇤ = {(wj , !j )}kj=1 containing (3) Addressing Update Strategy Selection: Generalized Update Equation Given the set of programs generated by the search step, one can use many objectives to update the parameters. For example, previous work have utilized maximum marginal likelihood (Krishnamurthy et al., 2017; Guu et al., 2017), reinforcement learning (Zhong et al., 2017; Guu et al., 2017) and margin based methods (Iyyer et al., 2017). It could be difficult to choose the suitable algorithm from these options. In this section, we propose a principle and general update equation such that previous update algorithms can be considered as special cases to this equation. Having a general update is important for the following reasons. First, it allows us to understand existing algorithms better by examining their basic properties. Second, the generalized update equation also makes it easy to implement and"
D18-1266,D15-1032,0,0.0173733,"derperform maximum marginal likelihood approaches. Our result on the SQA dataset supports their observation. However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods. Margin-based Learning Margin-based methods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the most violated example. Latent Variable Modeling Learning semantic pars"
D18-1266,D16-1262,0,0.0554985,"sed learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the most violated example. Latent Variable Modeling Learning semantic parsers from denotation can be viewed as a latent variable modeling problem, where the program is the latent variable. Probabilistic latent variable models have been studied using EM-algorithm and its variant (Dempster et al., 1977). The graphical model literature has studied latent variable learning on margin-based methods (Yu and Joachims, 2009) and probabilistic m"
D18-1266,D16-1127,0,0.0263699,"expensive annotations by an expert, distant supervision approaches, where semantic parsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017). Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates. Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man2449 Question “of these teams, which had more than 21 losses?&quot; “of the remaining, which earned the most bronze medals?&quot; “of those competitors from germany, which was not paul sievert?&quot; without policy shaping SELECT Club WHERE Losses = ROW 15 SELECT Nation WHERE Rank = ROW 1 SELECT Name WHERE Time (hand) = ROW 3 with policy shaping SELECT Club WHERE Losses > 21 FollowUp WHERE Bronze is Max FollowUp WHERE Name != ROW 5 Table 4: Training examples and the highest ran"
D18-1266,P11-1060,0,0.4172,"Learning Learning a semantic parser is equivalent to learning the parameters ✓ in the scoring function, which is a structured learning problem, due to the large, structured output space Y. Structured learning algorithms generally consist of two major components: search and update. When the gold programs are available during training, the search procedure finds a set of high-scoring incorrect programs. These programs are used by the update step to derive loss for updating parameters. For example, these programs are used for approximating the partition-function in maximum-likelihood objective (Liang et al., 2011) and finding set of programs causing margin violation in margin based methods (Daumé III and Marcu, 2005). Depending on the exact algorithm being used, these two components are not necessarily separated into isolated steps. For instance, parameters can be updated in the middle of search (e.g., Huang et al., 2012). For learning semantic parsers from denotations, where we assume only answers are available in a training set {(xi , ti , zi )}N i=1 of N examples, the basic construction of the learning algorithms remains the same. However, the problems that search needs to handle in SpFD is more cha"
D18-1266,P05-1012,0,0.147574,"s. ning, 2016), semantic parsing (Guu et al., 2017) and instruction following (Misra et al., 2017). Guu et al. (2017) show that policy gradient methods underperform maximum marginal likelihood approaches. Our result on the SQA dataset supports their observation. However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods. Margin-based Learning Margin-based methods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms"
D18-1266,D17-1106,1,0.8804,"Missing"
D18-1266,D16-1183,1,0.853215,"d on the score function, for example b✓ (y|x, t, z) / exp{score✓ (y, t)}. However, this approach can suffer from a divergence phenomenon whereby the score of spurious programs picked up by the search in the first epoch increases, making it more likely for the search to pick them up in the future. Such divergence issues are common with latent-variable learning and often require careful initialization to overcome (Rose, 1998). Unfortunately such initialization schemes are not applicable for deep neural networks which form the model of most successful semantic parsers today (Jia and Liang, 2016; Misra and Artzi, 2016; Iyyer et al., 2017). Prior work, such as ✏-greedy exploration (Guu et al., 2017), has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturating the search on high-scoring spurious programs. However, random noise need not bias the search towards the correct program(s). In this paper, we introduce a simple policy-shaping method to guide the search. This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs. 2444 1 This transformation preserves the answer of the question. Alg"
D18-1266,P15-1096,1,0.853345,"ple of semantic parsing from denotations. Given the table environment, map the question to an executable program that evaluates to the answer. Introduction Semantic parsing from denotations (SpFD) is the problem of mapping text to executable formal representations (or program) in a situated environment and executing them to generate denotations (or answer), in the absence of access to correct representations. Several problems have been handled within this framework, including question answering (Berant et al., 2013; Iyyer et al., 2017) and instructions for robots (Artzi and Zettlemoyer, 2013; Misra et al., 2015). Consider the example in Figure 1. Given the question and a table environment, a semantic parser maps the question to an executable program, in this case a SQL query, and then executes the query on the environment to generate the answer England. In the SpFD setting, the training data does not contain the correct programs. Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs and finds suitable candidates, and an update step that uses these programs to update the model. Figure 2 shows the two step trainin"
D18-1266,D15-1001,0,0.0445552,"distant supervision approaches, where semantic parsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017). Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates. Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man2449 Question “of these teams, which had more than 21 losses?&quot; “of the remaining, which earned the most bronze medals?&quot; “of those competitors from germany, which was not paul sievert?&quot; without policy shaping SELECT Club WHERE Losses = ROW 15 SELECT Nation WHERE Rank = ROW 1 SELECT Name WHERE Time (hand) = ROW 3 with policy shaping SELECT Club WHERE Losses > 21 FollowUp WHERE Bronze is Max FollowUp WHERE Name != ROW 5 Table 4: Training examples and the highest ranked program in the beam search, scored accor"
D18-1266,D16-1261,0,0.0160404,"rsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017). Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates. Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man2449 Question “of these teams, which had more than 21 losses?&quot; “of the remaining, which earned the most bronze medals?&quot; “of those competitors from germany, which was not paul sievert?&quot; without policy shaping SELECT Club WHERE Losses = ROW 15 SELECT Nation WHERE Rank = ROW 1 SELECT Name WHERE Time (hand) = ROW 3 with policy shaping SELECT Club WHERE Losses > 21 FollowUp WHERE Bronze is Max FollowUp WHERE Name != ROW 5 Table 4: Training examples and the highest ranked program in the beam search, scored according to the shaped policy, after training with MAV"
D18-1266,N12-1087,1,0.808266,"of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the most violated example. Latent Variable Modeling Learning semantic parsers from denotation can be viewed as a latent variable modeling problem, where the program is the latent variable. Probabilistic latent variable models have been studied using EM-algorithm and its variant (Dempster et al., 1977). The graphical model literature has studied latent variable learning on margin-based methods (Yu and Joachims, 2009) and probabilistic models (Quattoni et al., 2007). Samdani et al. (2012) studied various variants of EM algorithm and showed that all of them are special cases of a unified framework. Our generalized update framework is similar in spirit. 7 Conclusion In this paper, we propose a general update equation from semantic parsing from denotation and propose a policy shaping method for addressing the spurious program challenge. For the future, we plan to apply the proposed learning framework to more semantic parsing tasks and consider new methods for policy shaping. 8 Acknowledgements We thank Ryan Benmalek, Alane Suhr, Yoav Artzi, Claire Cardie, Chris Quirk, Michel Gall"
D18-1266,W04-3201,0,0.0982967,"tep for these examples. ning, 2016), semantic parsing (Guu et al., 2017) and instruction following (Misra et al., 2017). Guu et al. (2017) show that policy gradient methods underperform maximum marginal likelihood approaches. Our result on the SQA dataset supports their observation. However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods. Margin-based Learning Margin-based methods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning,"
D18-1266,D07-1080,0,0.0177084,"al., 2017). Guu et al. (2017) show that policy gradient methods underperform maximum marginal likelihood approaches. Our result on the SQA dataset supports their observation. However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods. Margin-based Learning Margin-based methods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the mo"
D18-1266,D07-1071,0,0.197773,"Missing"
K19-1070,D17-1215,0,0.0366705,"ecision. For example, “Northridge earthquake” is mistakenly taken as the answer to the question about what earthquake caused $20 million in damage. Because “$20 billon” is positioned far away from “Northridge earthquake”, it is hard for a model to link these two concepts together and recognize the mismatch of “$20 million” in the question and “$20 billion” in the context. Introduction Ever since the release of many challenging large scale datasets for machine reading comprehension (MRC) (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2016; Yang et al., 2018; Reddy et al., 2018; Jia and Liang, 2017), there have been correspondingly many models for these datasets (Yu et al., 2018; Seo et al., 2017; Liu et al., 2018b; Hu et al., 2017; Xiong et al., 2017; Wang et al., 2018; Liu et al., 2018c; Tay et al., 2018). Knowing what you don’t know (Rajpurkar et al., 2018) is important in real applications of reading comprehension. Unanswerable questions are commonplace in the real world, and SQuAD 2.0 was released specifically to target this problem (see Figure 1 for an example of non-answerable questions). Motivated by exploiting high level semantic relationships in the context, our first step is t"
K19-1070,P17-1147,0,0.0333028,"el semantics in the context are helpful to make better answerable or unanswerable decision. For example, “Northridge earthquake” is mistakenly taken as the answer to the question about what earthquake caused $20 million in damage. Because “$20 billon” is positioned far away from “Northridge earthquake”, it is hard for a model to link these two concepts together and recognize the mismatch of “$20 million” in the question and “$20 billion” in the context. Introduction Ever since the release of many challenging large scale datasets for machine reading comprehension (MRC) (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2016; Yang et al., 2018; Reddy et al., 2018; Jia and Liang, 2017), there have been correspondingly many models for these datasets (Yu et al., 2018; Seo et al., 2017; Liu et al., 2018b; Hu et al., 2017; Xiong et al., 2017; Wang et al., 2018; Liu et al., 2018c; Tay et al., 2018). Knowing what you don’t know (Rajpurkar et al., 2018) is important in real applications of reading comprehension. Unanswerable questions are commonplace in the real world, and SQuAD 2.0 was released specifically to target this problem (see Figure 1 for an example of non-answerable questions). Motivate"
K19-1070,P18-1157,0,0.0301885,"Missing"
K19-1070,P18-1158,0,0.0149095,"ioned far away from “Northridge earthquake”, it is hard for a model to link these two concepts together and recognize the mismatch of “$20 million” in the question and “$20 billion” in the context. Introduction Ever since the release of many challenging large scale datasets for machine reading comprehension (MRC) (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2016; Yang et al., 2018; Reddy et al., 2018; Jia and Liang, 2017), there have been correspondingly many models for these datasets (Yu et al., 2018; Seo et al., 2017; Liu et al., 2018b; Hu et al., 2017; Xiong et al., 2017; Wang et al., 2018; Liu et al., 2018c; Tay et al., 2018). Knowing what you don’t know (Rajpurkar et al., 2018) is important in real applications of reading comprehension. Unanswerable questions are commonplace in the real world, and SQuAD 2.0 was released specifically to target this problem (see Figure 1 for an example of non-answerable questions). Motivated by exploiting high level semantic relationships in the context, our first step is to extract meaningful high-level semantics from question/context. Multi-head self-attentive pooling 747 Proceedings of the 23rd Conference on Computational Natural Language Le"
K19-1070,N18-1202,0,0.0311034,"ule on top of the base reader BERT. In addition to the original startend prediction layers trained from true answers in the base reader, we include a separate start-end prediction layer, with separate parameters, trained specifically on plausible and true answers available in SQuAD 2.0. The context output C from BERT is projected into two hidden state layers S and E, where C, S and E ∈ RL×h , L is the context length and h is the hidden size. The S and E layers are then projected down to a hidden Pretraining embeddings on large unlabelled corpus has been shown to improve many downstream tasks (Peters et al., 2018; Howard and Ruder, 2018; Alec et al., 2018). The recently released 749 dimension of 1, and trained with Cross-Entropy Loss against the plausible and true answer starts and ends. The hidden states S and E of this layer are concatenated with the last context layer output C and projected back to the original dimension to obtain the augmented context vector X, which is fused with start-end span information. S = tanh(CW1 + b1 ) (1) E = tanh(CW2 + b2 ) (2) X = [C; S; E]W (3) Figure 3: Illustration of a Relation Network. The gθ is a MLP to score relationships between pairs term also helps prevent th"
K19-1070,P18-2124,0,0.0457322,"Missing"
K19-1070,D18-1348,0,0.0293522,"herefore our relation module takes all pairs of context objects to score, and use the question objects to guide the scoring function. We use 2 question heads q0 , q1 , so our scoring function is: where W3 ∈ Rh×h , and W4 ∈ Rn×h ; σ is an activation function, such as tanh; n is the number of heads, and h is the hidden dimension. The output O ∈ Rn×h contains the n objects with hidden dimension h that are passed to the next layer. ri = 3.2.1 Object Extraction Regularization In order to help encourage the multiple heads to extract different meaningful semantics in the text, a regularization loss (Xia et al., 2018) is introduced to encourage each head to attend to slightly different sections of the context. Overlapping objects centered on the answer span are expected, due to information fused from S and E, but we do not want the entire weight distribution of the head to be solely focused on the answer span. As we show in later figures, many heads heavily weight the answer span, but also weight information relevant to the answer span needed to make a better non-answerable prediction. Our regularization n X ωi,j ∗ gθ (oi , oj , q0 , q1 ) (7) γi ∗ fφ (ri ) (8) j=0 z= n X i=0 where the outputs ri is the wei"
K19-1070,D18-1259,0,0.016881,"ake better answerable or unanswerable decision. For example, “Northridge earthquake” is mistakenly taken as the answer to the question about what earthquake caused $20 million in damage. Because “$20 billon” is positioned far away from “Northridge earthquake”, it is hard for a model to link these two concepts together and recognize the mismatch of “$20 million” in the question and “$20 billion” in the context. Introduction Ever since the release of many challenging large scale datasets for machine reading comprehension (MRC) (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2016; Yang et al., 2018; Reddy et al., 2018; Jia and Liang, 2017), there have been correspondingly many models for these datasets (Yu et al., 2018; Seo et al., 2017; Liu et al., 2018b; Hu et al., 2017; Xiong et al., 2017; Wang et al., 2018; Liu et al., 2018c; Tay et al., 2018). Knowing what you don’t know (Rajpurkar et al., 2018) is important in real applications of reading comprehension. Unanswerable questions are commonplace in the real world, and SQuAD 2.0 was released specifically to target this problem (see Figure 1 for an example of non-answerable questions). Motivated by exploiting high level semantic relatio"
N09-2052,E06-1005,0,0.359913,"Missing"
N09-2052,P07-1040,0,0.324134,"Missing"
N09-2052,D08-1011,1,0.901309,"Missing"
N09-2052,P02-1040,0,0.0781015,"Missing"
N09-2052,P08-1059,0,0.0695454,"Missing"
N09-2052,W06-3110,0,\N,Missing
N13-1048,J93-2003,0,0.0320478,"e easily incorporated into a standard phrase-based SMT system, requiring no code change in the runtime engine. In the rest of the paper, Section 2 presents the MRF model for phrase translation. Section 3 describes the way the model parameters are estimated. Section 4 presents the experimental results on two Europarl translation tasks. Section 5 reviews previous work that lays the foundation of this study. Section 6 concludes the paper. 2 Model The traditional translation models are directional models that are based on conditional probabilities. As suggested by the noisy-channel model for SMT (Brown et al. 1993):  ∗ = argmax | = argmax ()|  (1)  The Bayes rule leads us to invert the conditioning of translation probability from a foreign (source) sentence  to an English (target) translation . However, in practice, the implementation of state-of-the-art phrase-based SMT systems uses a weighted log-linear combination of several models ℎ(, , ) including the logarithm of the phrase probability (and the lexical weight) in source-totarget and target-to-source directions (Och and Ney 2004)  ∗ = argmax ∑   ℎ (, , ) (2) = argmax  (, )  where  in ℎ(, , ) is a hidden struct"
N13-1048,P05-1033,0,0.244434,"Missing"
N13-1048,N09-1025,0,0.0657147,"riminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoret"
N13-1048,D08-1024,0,0.0811986,"n probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, bu"
N13-1048,W06-3105,0,0.129937,"Missing"
N13-1048,P08-2010,0,0.0584937,"learn phrase translation probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze obj"
N13-1048,N04-1035,0,0.20719,"Missing"
N13-1048,D12-1061,1,0.847411,"y recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansion (Metzler et al. 2007; Gao et al. 2012) and POS tagging (Haghighi and Klein 2006). 3 For comparison, the method of He and Deng (2012) also achieved very similar results to ours using the same experimental setting, as described in Section 4. Another undirected graphical model that has been more widely used for NLP is a CRF (Lafferty et al. 2001). An MRF differs from a CRF in that its partition function is no longer observation dependent. As a result, learning an MRF is harder than learning a CRF using maximum likelihood estimation (Haghighi and Klein 2006). Our work provides an alternative learning method that is based on discrimina"
N13-1048,N12-1023,0,0.175958,"Missing"
N13-1048,N06-1041,0,0.0235448,"tart to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansion (Metzler et al. 2007; Gao et al. 2012) and POS tagging (Haghighi and Klein 2006). 3 For comparison, the method of He and Deng (2012) also achieved very similar results to ours using the same experimental setting, as described in Section 4. Another undirected graphical model that has been more widely used for NLP is a CRF (Lafferty et al. 2001). An MRF differs from a CRF in that its partition function is no longer observation dependent. As a result, learning an MRF is harder than learning a CRF using maximum likelihood estimation (Haghighi and Klein 2006). Our work provides an alternative learning method that is based on discriminative training. 6 Conclusions The contribut"
N13-1048,D08-1039,0,0.0402308,"g in phrase-based SMT systems (Koehn et al. 2003). The potential function is defined as  ,  =   (, ), where the feature  (, ), called the word-pair feature, is an indicator function whose value is 1 if  is a word in target phrase  and f is a word in source phrase , and 0 otherwise. The third type of cliques contains three word nodes. Two of them are in one language and the third in the other language. A potential over such a clique is intended to capture inter-word dependencies for selecting word translations. The potential function is inspired by the triplet lexicon model (Hasan et al. 2008) which is based on lexicalized triplets (, , ’) . It can be understood as two source (or target) words triggering one target (or source) word. The potential function is defined as  ,   ,  =   (,   , ), where the feature  (,   , ), called the triplet feature, is an indicator function whose value is 1 if  is a word in target phrase  and  and ’ are two different words in source phrase , and 0 otherwise. For any clique that contains nodes in only one language we assume that   = 1 for all setting of the clique, which has no impact on scoring a phrase pair. One m"
N13-1048,W07-0711,1,0.812388,"EN, the training set contains 688K sentence pairs, with 21 words per sentence on average. The development set contains 2000 sentences. We used 2000 sentences from the WMT05 shared task as TEST1, and the 2000 sentences from the WMT06 shared task as TEST2. Two baseline phrase-based SMT systems, each for one language pair, are developed as follows. These baseline systems are used in our experiments both for comparison purpose and for generating N-best lists for discriminative training. First, we performed word alignment on the training set using a hidden Markov model with lexicalized distortion (He 2007), then extracted the phrase table from the word aligned bilingual texts (Koehn et al. 2003). The maximum phrase length is set to four. Other models used in a baseline system include a lexicalized reordering model, word count and phrase count, and a trigram language model trained on the English training data provided by the WMT06 shared task. A fast beam-search phrasebased decoder (Moore and Quirk 2007) is used and the distortion limit is set to four. The decoder is modified so as to output the Viterbi derivation for each translation hypothesis. The metric used for evaluation is case insensitiv"
N13-1048,P12-1031,1,0.925646,". The most common method of constructing the phrase table takes a two-phase approach. First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data. The second phase is parameter estimation, where each phrase pair is assigned with some scores that are estimated based on counting of words or phrases on the same word-aligned training data. There has been a lot of research on improving the quality of the phrase table using more principled methods for phrase extraction (e.g., Lamber and Banchs 2005), parameter estimation (e.g., Wuebker et al. 2010; He and Deng 2012), or both (e.g., Marcu and Wong 2002; Denero et al. 2006). The focus of this paper is on the parameter estimation phase. We revisit the problem of scoring a phrase translation pair by developing a new phrase translation model based on Markov random fields (MRFs) and large-scale discriminative training. We strive to address the following three primary concerns. First of all, instead of parameterizing a phrase translation pair using a set of scoring functions that are learned independently (e.g., phrase translation probabilities and lexical weights) we use a general, statistical framework in whi"
N13-1048,D11-1125,0,0.211763,"evious work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansi"
N13-1048,J10-4005,0,0.0159991,"jointly with other component models with respect to an objective function that is closely related to the evaluation metric under consideration, i.e., BLEU in this paper. To this end, we resort to a large-scale discriminative training approach, following the pioneering work of Liang et al. (2006). Although there are established methods of tuning a handful of features on small training sets, such as the MERT method (Och 2003), the development of discriminative training methods for millions of features on millions of sentence pairs is still an ongoing area of research. A recent survey is due to Koehn (2010). In this paper we show that by using stochastic gradient ascent and an N-best list based 450 Proceedings of NAACL-HLT 2013, pages 450–459, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics expected BLEU as the objective function, largescale discriminative training can lead to significant improvements. The third primary concern is the ease of adoption of the proposed method. To this end, we use a simple and well-established learning method, ensuring that the results can be easily reproduced. We also develop the features for the MRF model in such a way that the"
N13-1048,W06-3114,0,0.0158381,"es on the training set are of artificially high quality with the derivations containing artificially long phrase pairs. The discrepancy between the translations on training and test sets could hurt the training performance. However, we found in our experiments that the impact of over-fitting on the quality of the trained MRF models is negligible1. 4 Experiments We conducted our experiments on two Europarl translation tasks, German-to-English (DE-EN) and French-to-English (FR-EN). The data sets are published for the shared task in NAACL 2006 Workshop on Statistical Machine Translation (WMT06) (Koehn and Monz 2006). For DE-EN, the training set contains 751K sentence pairs, with 21 words per sentence on average. The official development set used for the shared 1 As pointed out by one of the reviewers, the fact that our training works fine without leave-one-out is probably due to the small phrase length limit (i.e., 4) we used. If a longer phrase limit (e.g., 7) is used the result might be different. We leave it to future work. 455 DE-EN (TEST2) FR-EN (TEST2) 27.3 26.0 25.6 26.0 30.8 30.7 30.5 31.4 Table 1: Baseline results in BLEU. The results of top ranked systems are reported in Koehn and Monz (2006)2."
N13-1048,N03-1017,0,0.239642,"and  is source phrase, and 0 otherwise. While the conditional probabilities in a directional translation model are estimated using relative frequencies of phrase pairs extracted from word-aligned parallel sentences, the parameter of the phrase-pair function  is learned discriminatively, as we will describe in Section 3. Second, we consider cliques that contain two word nodes, one in source phrase and the other in target phrase. A potential over such a clique captures word-to-word translation dependencies similar to the use the IBM Model 1 for lexical weighting in phrase-based SMT systems (Koehn et al. 2003). The potential function is defined as  ,  =   (, ), where the feature  (, ), called the word-pair feature, is an indicator function whose value is 1 if  is a word in target phrase  and f is a word in source phrase , and 0 otherwise. The third type of cliques contains three word nodes. Two of them are in one language and the third in the other language. A potential over such a clique is intended to capture inter-word dependencies for selecting word translations. The potential function is inspired by the triplet lexicon model (Hasan et al. 2008) which is based on lexicalized"
N13-1048,N04-1023,0,0.113554,"ties based on this alignment. The latter learn phrase translation probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective funct"
N13-1048,2005.mtsummit-posters.11,0,0.0840419,"Missing"
N13-1048,P06-1096,0,0.533593,"lation in a unified way. To this end, we propose the use of a MRF model. Second, because the phrase model has to work with other component models in an SMT system in order to produce good translations and the quality of translation is measured via BLEU score, it is desirable to optimize the parameters of the phrase model jointly with other component models with respect to an objective function that is closely related to the evaluation metric under consideration, i.e., BLEU in this paper. To this end, we resort to a large-scale discriminative training approach, following the pioneering work of Liang et al. (2006). Although there are established methods of tuning a handful of features on small training sets, such as the MERT method (Och 2003), the development of discriminative training methods for millions of features on millions of sentence pairs is still an ongoing area of research. A recent survey is due to Koehn (2010). In this paper we show that by using stochastic gradient ascent and an N-best list based 450 Proceedings of NAACL-HLT 2013, pages 450–459, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics expected BLEU as the objective function, largescale discrimina"
N13-1048,W02-1018,0,0.0604325,"ucting the phrase table takes a two-phase approach. First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data. The second phase is parameter estimation, where each phrase pair is assigned with some scores that are estimated based on counting of words or phrases on the same word-aligned training data. There has been a lot of research on improving the quality of the phrase table using more principled methods for phrase extraction (e.g., Lamber and Banchs 2005), parameter estimation (e.g., Wuebker et al. 2010; He and Deng 2012), or both (e.g., Marcu and Wong 2002; Denero et al. 2006). The focus of this paper is on the parameter estimation phase. We revisit the problem of scoring a phrase translation pair by developing a new phrase translation model based on Markov random fields (MRFs) and large-scale discriminative training. We strive to address the following three primary concerns. First of all, instead of parameterizing a phrase translation pair using a set of scoring functions that are learned independently (e.g., phrase translation probabilities and lexical weights) we use a general, statistical framework in which arbitrary features extracted from"
N13-1048,N04-4024,0,0.0291741,"among different language pairs and depend to a large degree upon the amount and quality of training data. We leave a comprehensive study of features to future work. 3 Training This section describes the way the parameters of the MRF model are estimated. Although MRFs are by nature generative models, it is not always appropriate to train the parameters using conventional likelihood based approaches mainly for two reasons. The first is due to the difficulty in computing the partition function in Equation (4), especially in a task of our scale. The second is due to the metric divergence problem (Morgan et al. 2004). That is, the maximum likelihood estimation is unlikely to be optimal for the evaluation metric under consideration, as demonstrated on a variety of tasks including machine translation (Och 2003) and information retrieval (Metzler and Croft 2005; Gao et al. 2005). Therefore, we propose a large-scale discriminative training approach that uses stochastic gradient ascent and an N-best list based expected BLEU as the objective function. We cast machine translation as a structured classification task (Liang et al. 2006). It maps an input source sentence  to an output pair (, ) where  is the ou"
N13-1048,2007.mtsummit-papers.43,0,0.0181846,"iments both for comparison purpose and for generating N-best lists for discriminative training. First, we performed word alignment on the training set using a hidden Markov model with lexicalized distortion (He 2007), then extracted the phrase table from the word aligned bilingual texts (Koehn et al. 2003). The maximum phrase length is set to four. Other models used in a baseline system include a lexicalized reordering model, word count and phrase count, and a trigram language model trained on the English training data provided by the WMT06 shared task. A fast beam-search phrasebased decoder (Moore and Quirk 2007) is used and the distortion limit is set to four. The decoder is modified so as to output the Viterbi derivation for each translation hypothesis. The metric used for evaluation is case insensitive BLEU score (Papineni et al. 2002). We also performed a significance test using the paired ttest. Differences are considered statistically significant when the p-value is less than 0.05. Table 1 2 The official results are accessible http://www.statmt.org/wmt06/shared-task/results.html at # Systems 1 2 3 4 5 DE-EN 27.0 FR-EN TEST1 TEST2 TEST1 TEST2 26.8 26.0 27.3 α 27.2 α 26.8 αβ 26.8 αβ 26.0 27.1 α 26"
N13-1048,J04-4002,0,0.595513,"Missing"
N13-1048,P03-1021,0,0.807988,"dels in an SMT system in order to produce good translations and the quality of translation is measured via BLEU score, it is desirable to optimize the parameters of the phrase model jointly with other component models with respect to an objective function that is closely related to the evaluation metric under consideration, i.e., BLEU in this paper. To this end, we resort to a large-scale discriminative training approach, following the pioneering work of Liang et al. (2006). Although there are established methods of tuning a handful of features on small training sets, such as the MERT method (Och 2003), the development of discriminative training methods for millions of features on millions of sentence pairs is still an ongoing area of research. A recent survey is due to Koehn (2010). In this paper we show that by using stochastic gradient ascent and an N-best list based 450 Proceedings of NAACL-HLT 2013, pages 450–459, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics expected BLEU as the objective function, largescale discriminative training can lead to significant improvements. The third primary concern is the ease of adoption of the proposed method. To th"
N13-1048,P02-1040,0,0.0895129,"Missing"
N13-1048,W11-2119,0,0.20822,"the 1-best translation hypotheses In our experiments we find that using sBLEU defined above leads to a small but consistent improvement over other variations of sentence-level BLEU proposed previously (e.g., Liang et al. 2006). In particular, the use of the scaling factor ) in computing  makes  of the baseline’s 1best output close to perfect on training data, and has an effect of forcing the discriminative training to improve BLEU by improving n-gram precisions rather than by improving brevity penalty. 3.2 Parameter Estimation We use an N-best list based expected BLEU, a variant of that in Rosti et al. (2011), as the objective function for parameter optimization. Given the current model  , the expected BLEU, denoted by xBLEU(), over one training sample i.e., a labeled N-best list GEN() generated from a pair of source and target sentences (,   ), is defined as 454 1 Initialize , assuming  is fixed during training 2 For t = 1…T (T = the total number of iterations) 3 For each training sample (labeled 100-best list) 4 Compute ી | for each translation hypothesis  based on the current model = (, ) 5 Update the model via  =  + ∙ (), where is the learning rate and the gradient computed"
N13-1048,P12-1002,0,0.698607,"defined in Equation (11). Let . = ,  ∗ , ∗  − ,   ,   . The hinge loss under the N-best re-ranking framework is defined as max (0,1 −  .) . It is easy to verify that to train a model using this version of hinge loss, the update rule of Equation (12) can be rewritten as  &*+ , if 0 =  ∗ 3 (14)  ') = / &*+  + ,., $ℎ 12 where 0 is the highest scored candidate in GEN. Following Shalev-Shwartz (2012), by setting = 1 , we reach the Perceptron-based training algorithm that has been widely used in previous studies of discriminative training for SMT (e.g., Liang et al. 2006; Simianer et al. 2012). The logistic loss log(1 + exp(− .)) leads to an update rule similar to that of hinge loss  ') = /  &*+ ,  &*+ + , (.)., if 0 =  ∗ 3 (15) $ℎ 12 where ી   = 1/(1 + exp(  )). The log loss is widely used when a probabilistic interpretation of the trained model is desired, as in conditional random fields (CRFs) (Lafferty et al. 2001). Given a training sample, log loss is defined as log   ∗ |, where  ∗ is the oracle translation hypothesis with respect to its reference translation.   ∗ | is computed as Equation (10). So, unlike hinge loss and logistic loss, log loss ta"
N13-1048,P06-1091,0,0.0442089,"ward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrie"
N13-1048,D07-1080,0,0.377455,"alignment. The latter learn phrase translation probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith"
N13-1048,P10-1049,0,0.566799,"nslation (SMT) system. The most common method of constructing the phrase table takes a two-phase approach. First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data. The second phase is parameter estimation, where each phrase pair is assigned with some scores that are estimated based on counting of words or phrases on the same word-aligned training data. There has been a lot of research on improving the quality of the phrase table using more principled methods for phrase extraction (e.g., Lamber and Banchs 2005), parameter estimation (e.g., Wuebker et al. 2010; He and Deng 2012), or both (e.g., Marcu and Wong 2002; Denero et al. 2006). The focus of this paper is on the parameter estimation phase. We revisit the problem of scoring a phrase translation pair by developing a new phrase translation model based on Markov random fields (MRFs) and large-scale discriminative training. We strive to address the following three primary concerns. First of all, instead of parameterizing a phrase translation pair using a set of scoring functions that are learned independently (e.g., phrase translation probabilities and lexical weights) we use a general, statistic"
N13-1048,P08-1024,0,\N,Missing
N15-1092,P14-1023,0,0.0130009,"milar concepts; e.g., feature generation, dimensionality reduction, and vector space models. The main motivation is similar: to abstract away from surface forms in words, sentences, or documents, in order to alleviate sparsity and approximate semantics. Traditional techniques include LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007), PCA (Karhunen, 1998), and non-linear kernel variants (Sch¨olkopf et al., 1998). Recently, learningbased approaches inspired by neural networks, especially DNNs, have gained in prominence, due to their favorable performance (Huang et al., 2013; Baroni et al., 2014; Milajevs et al., 2014). Popular methods for learning word representations include (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec4 The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised o"
N15-1092,D14-1082,0,0.226541,"odong Liu†∗, Jianfeng Gao‡ , Xiaodong He‡ , Li Deng‡ , Kevin Duh† and Ye-yi Wang‡ Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara 630-0192, Japan ‡ Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA xiaodong-l@is.naist.jp, {jfgao,xiaohe,deng}@microsoft.com kevinduh@is.naist.jp, yeyiwang@microsoft.com † Abstract representations learned from large corpora. Further, since these representations are usually in a lowdimensional vector space, they result in more compact models than those built from surface-form features. A recent successful example is the parser by (Chen and Manning, 2014), which is not only accurate but also fast. Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect"
N15-1092,P14-1066,1,0.29944,"n. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 201"
N15-1092,D14-1002,1,0.736534,"n. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 201"
N15-1092,D14-1070,0,0.0132056,"Missing"
N15-1092,P14-1062,0,0.00783704,"yyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 2014b), parse tree structure (Irsoy and Cardie, 2014), and run-time inference (Le and Mikolov, 2014). The synergy between multi-task learning and neural nets is quite natural; the general idea dates back to (Caruana, 1997). The main challenge is in designing the tasks and the network structure. For example, (Collobert et al., 2011) defined part-of-speech tagging, chunking, and named entity recognition as multiple tasks in a single sequence labeler; (Bordes et al., 2012) defined multiple data sources as tasks in their relation extraction system. While conceptual"
N15-1092,N13-1090,0,0.587375,"g large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. 1 However, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations uni"
N15-1092,D14-1079,0,0.0296496,"feature generation, dimensionality reduction, and vector space models. The main motivation is similar: to abstract away from surface forms in words, sentences, or documents, in order to alleviate sparsity and approximate semantics. Traditional techniques include LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007), PCA (Karhunen, 1998), and non-linear kernel variants (Sch¨olkopf et al., 1998). Recently, learningbased approaches inspired by neural networks, especially DNNs, have gained in prominence, due to their favorable performance (Huang et al., 2013; Baroni et al., 2014; Milajevs et al., 2014). Popular methods for learning word representations include (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec4 The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing a"
N15-1092,D14-1162,0,0.118051,"s-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. 1 However, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks. Introd"
N15-1092,D13-1170,0,0.00800131,"tions to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. 1 However, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks. Introduction Recent advances in deep neural networks (DNNs) have demonstrated the importance of learn"
N15-1092,P10-1040,0,0.0115602,"et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec4 The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other meth"
N15-1092,2014.lilt-9.5,0,\N,Missing
N16-1098,W08-2227,1,0.249101,"Missing"
N16-1098,D13-1178,0,0.046906,"ured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics son, 2009; Manshadi et al., 2008), but teasing out useful info"
N16-1098,P13-1035,0,0.0363528,"esentations. Chambers and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the ‘protagonist’. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques per"
N16-1098,D15-1075,0,0.0394456,"ressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation. There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a r"
N16-1098,P08-1090,1,0.58165,"used on learning scripts (Schank and Abelson, 1977). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computation"
N16-1098,P09-1068,1,0.635097,"77). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics son, 2009; Manshadi et al., 2008"
N16-1098,D13-1185,1,0.266302,"and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the ‘protagonist’. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Picho"
N16-1098,N13-1104,1,0.900888,"ical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics son, 2009; Manshadi et al., 2008), but teasing out useful information from noisy bl"
N16-1098,W07-1401,0,0.0338919,"Test’. 4.1 Story Cloze Test The cloze task (Taylor, 1953) is used to evaluate a human (or a system) for language understanding by deleting a random word from a sentence and having a human fill in the blank. We introduce ‘Story Cloze Test’, in which a system is given a four-sentence ‘context’ and two alternative endings to the story, called ‘right ending’ and ‘wrong ending’. Hence, in this test the fifth sentence is blank. Then the system’s task is to choose the right ending. The ‘right ending’ can be viewed as ‘entailing’ hypothesis in a classic Recognizing Textual Entailment (RTE) framework (Giampiccolo et al., 2007), and ‘wrong’ ending can be seen as the ’contradicting’ hypothesis. Table 4 shows three example Story Cloze Test cases. Story Cloze Test will serve as a generic story understanding evaluation framework, also applicable to evaluation of story generation models (for instance by computing the log-likelihoods assigned to the two ending alternatives by the story generation model), which does not necessarily imply requirement for explicit narrative knowledge learning. However, it is safe to say that any model that performs well on Story Cloze Test is demonstrating some level of deeper story understa"
N16-1098,E12-1034,0,0.0756634,"Missing"
N16-1098,P04-1077,0,0.0329659,"orkers participated Average # cases written by one worker Max # cases written by one worker Average payment per test case (cents) Size of the final set (verified by human) 13,500 282 47.8 1461 10 3,744 Table 5: Statistics for crowd-sourcing Story Cloze Test instances. search engine8 hits of the main event (verb) together with its semantic roles (e.g., ‘I*poison*flowers’ vs ‘I*nourish*flowers’). We extract the main verb and its corresponding roles using TRIPS semantic parser. 2. N-gram Overlap: Simply chooses the alternative which shares more n-grams with the context. We compute Smoothed-BLEU (Lin and Och, 2004) score for measuring up to four-gram overlap of an alternative and the context. 3. GenSim: Average Word2Vec: Choose the hypothesis with closer average word2vec (Mikolov et al., 2013) embedding to the average word2vec embedding of the context. This is basically an enhanced word overlap baseline, which accounts for semantic similarity. 4. Sentiment-Full: Choose the hypothesis that matches the average sentiment of the context. We use the state-of-the-art sentiment analysis model (Manning et al., 2014) which assigns a numerical value from 1 to 5 to a sentence. 5. Sentiment-Last: Choose the hypothe"
N16-1098,P14-5010,0,0.00272135,"imply chooses the alternative which shares more n-grams with the context. We compute Smoothed-BLEU (Lin and Och, 2004) score for measuring up to four-gram overlap of an alternative and the context. 3. GenSim: Average Word2Vec: Choose the hypothesis with closer average word2vec (Mikolov et al., 2013) embedding to the average word2vec embedding of the context. This is basically an enhanced word overlap baseline, which accounts for semantic similarity. 4. Sentiment-Full: Choose the hypothesis that matches the average sentiment of the context. We use the state-of-the-art sentiment analysis model (Manning et al., 2014) which assigns a numerical value from 1 to 5 to a sentence. 5. Sentiment-Last: Choose the hypothesis that matches the sentiment of the last context sentence. 8 https://developers.google.com/ custom-search/ 846 6. Skip-thoughts Model: This model uses Skipthoughts’ Sentence2Vec embedding (Kiros et al., 2015) which models the semantic space of novels. This model is trained on the ‘BookCorpus’ (Zhu et al., 2015) (containing 16 different genres) of over 11,000 books. We use the skip-thoughts embedding of the alternatives and contexts for making decision the same way as with GenSim model. 7. Narrati"
N16-1098,P09-1025,0,0.00927628,"ing the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009). Finally, there is a rich body of work on story plot generation and creative or artistic story telling (M´endez et al., 2014; Riedl and Le´on, 2008). This paper is unique to these in its corpus of short, simple stories with a wide variety of commonsense events. We show these to be useful for learning, but also for enabling a rich evaluation framework for narrative understanding. 3 A Corpus of Short Commonsense Stories We aimed to build a corpus with two goals in mind: 1. The corpus contains a variety of commonsense causal and temporal relations between everyday events. This enables learning n"
N16-1098,W16-1007,1,0.12213,"Missing"
N16-1098,P15-1019,0,0.0707931,"Missing"
N16-1098,E14-1024,0,0.538376,"2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation. There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward child"
N16-1098,P10-1100,0,0.0173048,"g can help direct the field to a new direction of deeper language understanding. 2 Related Work Several lines of research have recently focused on learning narrative/event representations. Chambers and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the ‘protagonist’. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a"
N16-1098,D13-1020,0,0.0780145,"vents, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation. There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009). Finally, the"
N16-1098,D15-1195,0,0.170589,"Missing"
N16-1098,H89-1033,0,0.710176,"onsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding. 1 Introduction Story understanding is an extremely challenging task in natural language understanding with a longrunning history in AI (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced"
N16-1147,N15-1053,0,0.0541033,"31) visit (321) market (311) outdoor activity (267) Table 1: The number of albums in our tiered dataset for 1 the 15 most frequent Story kinds of stories. Re-telling Storytelling Motivation and Related Work 3 Dataset Construction Extracting Photos We begin by generating a list of “storyable” event types. We leverage the idea that “storyable” events tend to involve some form of posStory 4 Preferred Photo Sequence Flickr Album Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015). Such work focuses on direct, literal description of image content. While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for naturalistic interactions. There is a significant difference, yet unexplored, between"
N16-1147,P15-2017,1,0.822682,"Baseline Experiments We report baseline experiments on the storytelling task in Table 7, training on the SIS tier and testing on half the SIS validation set (valtest). Example output from each system is presented in Table 5. To highlight some differences between story and caption generation, we also train on the DII tier in isolation, and produce captions per-image, rather than in sequence. These results are shown in Table 7. To train the story generation model, we use a sequence-to-sequence recurrent neural net (RNN) approach, which naturally extends the single-image captioning technique of Devlin et al. (2015) and Vinyals et al. (2014) to multiple images. Here, we encode an image sequence by running an RNN over the fc7 vectors of each image, in reverse order. This is used as the initial hidden state to the story decoder model, which learns to produce the story one word at a time using softmax loss over the training data vocabulary. We use Gated Recurrent Units (GRUs) (Cho et al., 2014) for both the image encoder and story decoder. In the baseline system, we generate the story using a simple beam search (size=10), which has been successful in image captioning previously (Devlin et al., 2015). Howeve"
N16-1147,D13-1128,0,0.0674328,"tivity (267) Table 1: The number of albums in our tiered dataset for 1 the 15 most frequent Story kinds of stories. Re-telling Storytelling Motivation and Related Work 3 Dataset Construction Extracting Photos We begin by generating a list of “storyable” event types. We leverage the idea that “storyable” events tend to involve some form of posStory 4 Preferred Photo Sequence Flickr Album Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015). Such work focuses on direct, literal description of image content. While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for naturalistic interactions. There is a significant difference, yet unexplored, between remarking that a visual scene shows “sitting i"
N16-1147,D15-1021,1,0.889893,"Missing"
N16-1147,N16-1014,1,0.758927,"The dog was happy to be in the water. The family gathered together for a meal. The food was delicious. The dog was excited to be there. The kids were playing in the water. The boat was a little too much to drink. The family got together for a cookout. They had a lot of delicious food. The dog was happy to be there. They had a great time on the beach. They even had a swim in the water. Table 5: Example stories generated by baselines. This is a predictable result given the label bias problem inherent in maximum likelihood training; recent work has looked at ways to address this issue directly (Li et al., 2016). To establish a stronger baseline, we explore several decode-time heuristics to improve the quality of the generated story. The first heuristic is to lower the decoder beam size substantially. We find that using a beam size of 1 (greedy search) significantly increases the story quality, resulting in a 4.6 gain in METEOR score. However, the same effect is not seen for caption generation, with the greedy caption model obtaining worse quality than the beam search model. This highlights a key difference in generating stories versus generating captions. Although the stories produced using a greedy"
N16-1147,P04-1077,0,0.227891,"tter understand which metric could serve as a proxy for human evaluation, we compute pairwise correlation coefficients between automatic metrics and human judgments on 3,000 stories sampled from the SIS training set. For the human judgements, we again use crowdsourcing on MTurk, asking five judges per story to rate how strongly they agreed with the statement “If these were my photos, I would like using a story like this to share my experience with my friends”.7 We take the average of the five judgments as the final score for the story. For the automatic metrics, we use METEOR,8 smoothed-BLEU (Lin and Och, 2004), and Skip-Thoughts (Kiros et al., 2015) to compute similarity between each story for a given sequence. Skip-thoughts provide a Sentence2Vec embedding which models the semantic space of novels. As Table 4 shows, METEOR correlates best with human judgment according to all the correlation coefficients. This signals that a metric such as METEOR which incorporates paraphrasing correlates best with human judgement on this task. A more 7 Scale presented ranged from “Strongly disagree” to “Strongly agree”, which we convert to a scale of 1 to 5. 8 We use METEOR version 1.5 with hter weights. r ρ τ MET"
N16-1147,P14-5010,0,0.00748671,"instill morals, and share advice; focusing AI research towards this task therefore has the potential to bring about more humanlike intelligence and understanding. Re-telling Story 3 Story 1 2 easter (259) church (243) graduation ceremony (236) office (226) father’s day (221) Story 2 Story 5 Description for Images in Isolation & in Sequences Figure 2: Dataset crowdsourcing workflow. Caption in Sequence session, e.g., “John’s birthday party,” or “Shabnam’s visit.” Using the Flickr data release (Thomee et al., 2015), we aggregate 5-grams of photo titles and descriptions, using Stanford CoreNLP (Manning et al., 2014) to extract possessive dependency patterns. We keep the heads of possessive phrases if they can be classified as an EVENT in WordNet3.0, relying on manual winnowing to target our collection efforts.2 These terms are then used to collect albums using the Flickr API.3 We only include albums with 10 to 50 photos where all album photos are taken within a 48-hour span and CC-licensed. See Table 1 for the query terms with the most albums returned. The photos returned from this stage are then presented to crowd workers using Amazon’s Mechanical Turk to collect the corresponding stories and descriptio"
N16-1147,Q14-1006,0,0.0517771,"ket (311) outdoor activity (267) Table 1: The number of albums in our tiered dataset for 1 the 15 most frequent Story kinds of stories. Re-telling Storytelling Motivation and Related Work 3 Dataset Construction Extracting Photos We begin by generating a list of “storyable” event types. We leverage the idea that “storyable” events tend to involve some form of posStory 4 Preferred Photo Sequence Flickr Album Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015). Such work focuses on direct, literal description of image content. While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for naturalistic interactions. There is a significant difference, yet unexplored, between remarking that a vi"
N16-1174,P14-1062,0,0.152187,"ication is one of the fundamental task in Natural Language Processing. The goal is to assign labels to text. It has broad applications including topic labeling (Wang and Manning, 2012), sentiment classification (Maas et al., 2011; Pang and Lee, 2008), and spam detection (Sahami et al., 1998). Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–based approaches to text classification have been quite effective (Kim, 2014; Zhang et al., 2015; Johnson and Zhang, 2014; Tang et al., 2015), in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the r"
N16-1174,D14-1002,1,0.0625345,"ig. 1, which is a short Yelp review where the task is to predict the rating on a scale from 1–5. Intuitively, the first and third sentence have stronger information in assisting the prediction of the rating; within these sentences, the word delicious, a-m-a-z-i-n-g contributes more in implying the positive attitude contained in this review. Attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis (Shen et al., 2014; Gao et al., 2014). The key difference to previous work is that our system uses context to discover when a sequence of tokens is relevant rather than simply filtering for (sequences of) tokens, taken out of context. To evaluate the performance of our model in comparison to other common classification architectures, we look at six data sets (§3). Our model outperforms previous approaches by a significant margin. 2 Hierarchical Attention Networks The overall architecture of the Hierarchical Attention Network (HAN) is shown in Fig. 2. It consists of several parts: a word sequence encoder, a word-level attention la"
N16-1174,D14-1181,0,0.347192,"8), and spam detection (Sahami et al., 1998). Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–based approaches to text classification have been quite effective (Kim, 2014; Zhang et al., 2015; Johnson and Zhang, 2014; Tang et al., 2015), in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the relevant sections involves modeling the interactions of the words, not just their presence in isolation. Our primary contribution is a new neural architecture (§2), the Hierarchical Attention Network (HAN) that is designed to capture two"
N16-1174,P15-1107,0,0.642353,"Missing"
N16-1174,D15-1106,0,0.0757692,"nd Internet. explore the structure of a sentence and use a treestructured LSTMs for classification. There are also some works that combine LSTM and CNN structure to for sentence classification (Lai et al., 2015; Zhou et al., 2015). Tang et al. (2015) use hierarchical structure in sentiment classification. They first use a CNN or LSTM to get a sentence vector and then a bi-directional gated recurrent neural network to compose the sentence vectors to get a document vectors. There are some other works that use hierarchical structure in sequence generation (Li et al., 2015) and language modeling (Lin et al., 2015). The attention mechanism was proposed by (Bahdanau et al., 2014) in machine translation. The encoder decoder framework is used and an attention mechanism is used to select the reference words in original language for words in foreign language before translation. Xu et al. (2015) uses the attention mechanism in image caption generation to select the relevant image regions when generating words in the captions. Further uses of the attention mechanism include parsing (Vinyals et al., 2014), natural language question answering (Sukhbaatar et al., 2015; 1487 Kumar et al., 2015; Hermann et al., 201"
N16-1174,P11-1015,0,0.312116,"cocktails. ||next time I in Phoenix, I will go back here. ||Highly recommend. Figure 1: A simple example review from Yelp 2013 that consists of five sentences, delimited by period, question mark. The first and third sentence delivers stronger meaning and inside, the word delicious, a-m-a-z-i-n-g contributes the most in defining sentiment of the two sentences. Introduction Text classification is one of the fundamental task in Natural Language Processing. The goal is to assign labels to text. It has broad applications including topic labeling (Wang and Manning, 2012), sentiment classification (Maas et al., 2011; Pang and Lee, 2008), and spam detection (Sahami et al., 1998). Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–based approaches to text classification have bee"
N16-1174,P14-5010,0,0.0519213,"words (average and maximum per document). LSTM takes the whole document as a single sequence and the average of the hidden states of all words is used as feature for classification. Conv-GRNN and LSTM-GRNN were proposed by (Tang et al., 2015). They also explore the hierarchical structure: a CNN or LSTM provides a sentence vector, and then a gated recurrent neural network (GRNN) combines the sentence vectors from a document level vector representation for classification. 3.3 Model configuration and training We split documents into sentences and tokenize each sentence using Stanford’s CoreNLP (Manning et al., 2014). We only retain words appearing more than 5 times in building the vocabulary and replace the words that appear 5 times with a special UNK token. We obtain the word embedding by training an unsupervised word2vec (Mikolov et al., 2013) model on the training and validation splits and then use the word embedding to initialize We . The hyper parameters of the models are tuned on the validation set. In our experiments, we set the word embedding dimension to be 200 and the GRU dimension to be 50. In this case a combination of forward and backward GRU gives us 100 dimensions for word/sentence annotat"
N16-1174,D13-1170,0,0.0565051,"ing sentences. Note that this happens in a multiclass setting, that is, detection happens before the selection of the topic! 4 Related Work Kim (2014) use neural networks for text classification. The architecture is a direct application of CNNs, as used in computer vision (LeCun et al., 1998), albeit with NLP interpretations. Johnson and Zhang (2014) explores the case of directly using a high-dimensional one hot vector as input. They find that it performs well. Unlike word level modelings, Zhang et al. (2015) apply a character-level CNN for text classification and achieve competitive results. Socher et al. (2013) use recursive neural networks for text classification. Tai et al. (2015) GT: 4 Prediction: 4 pork belly = delicious . scallops ? i do n’t . even . like . scallops , and these were a-m-a-z-i-n-g . fun and tasty cocktails . next time i ’m in phoenix , i will go back here . highly recommend . GT: 0 Prediction: 0 terrible value . ordered pasta entree . . $ 16.95 good taste but size was an appetizer size . . no salad , no bread no vegetable . this was . our and tasty cocktails . our second visit . i will not go back . Figure 5: Documents from Yelp 2013. Label 4 means star 5, label 0 means star 1."
N16-1174,P15-1150,0,0.146221,"ction happens before the selection of the topic! 4 Related Work Kim (2014) use neural networks for text classification. The architecture is a direct application of CNNs, as used in computer vision (LeCun et al., 1998), albeit with NLP interpretations. Johnson and Zhang (2014) explores the case of directly using a high-dimensional one hot vector as input. They find that it performs well. Unlike word level modelings, Zhang et al. (2015) apply a character-level CNN for text classification and achieve competitive results. Socher et al. (2013) use recursive neural networks for text classification. Tai et al. (2015) GT: 4 Prediction: 4 pork belly = delicious . scallops ? i do n’t . even . like . scallops , and these were a-m-a-z-i-n-g . fun and tasty cocktails . next time i ’m in phoenix , i will go back here . highly recommend . GT: 0 Prediction: 0 terrible value . ordered pasta entree . . $ 16.95 good taste but size was an appetizer size . . no salad , no bread no vegetable . this was . our and tasty cocktails . our second visit . i will not go back . Figure 5: Documents from Yelp 2013. Label 4 means star 5, label 0 means star 1. GT: 1 Prediction: 1 why does zebras have stripes ? what is the purpose or"
N16-1174,P14-1146,0,0.12484,"(Mikolov et al., 2013) is used as feature set. 3.2.2 SVMs SVMs-based methods are reported in (Tang et al., 2015), including SVM+Unigrams, Bigrams, Text Features, AverageSG, SSWE. In detail, Unigrams and Bigrams uses bag-of-unigrams and bagof-bigrams as features respectively. Text Features are constructed according to (Kiritchenko et al., 2014), including word and character n-grams, sentiment lexicon features etc. AverageSG constructs 200-dimensional word vectors using word2vec and the average word embeddings of each document are used. SSWE uses sentiment specific word embeddings according to (Tang et al., 2014). 3.2.3 Neural Network methods The neural network based methods are reported in (Tang et al., 2015) and (Zhang et al., 2015). CNN-word Word based CNN models like that of (Kim, 2014) are used. CNN-char Character level CNN models are reported in (Zhang et al., 2015). Data set Yelp 2013 Yelp 2014 Yelp 2015 IMDB review Yahoo Answer Amazon review classes documents average #s max #s average #w max #w vocabulary 5 5 5 10 10 5 335,018 1,125,457 1,569,264 348,415 1,450,000 3,650,000 8.9 9.2 9.0 14.0 6.4 4.9 151 151 151 148 515 99 151.6 156.9 151.9 325.6 108.4 91.9 1184 1199 1199 2802 4002 596 211,245 4"
N16-1174,D15-1167,0,0.706504,"l approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–based approaches to text classification have been quite effective (Kim, 2014; Zhang et al., 2015; Johnson and Zhang, 2014; Tang et al., 2015), in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the relevant sections involves modeling the interactions of the words, not just their presence in isolation. Our primary contribution is a new neural architecture (§2), the Hierarchical Attention Network (HAN) that is designed to capture two basic insights about document structure. First, since documents"
N16-1174,P12-2018,0,0.260108,"ops, and these were a-m-a-z-i-n-g . ||fun and tasty cocktails. ||next time I in Phoenix, I will go back here. ||Highly recommend. Figure 1: A simple example review from Yelp 2013 that consists of five sentences, delimited by period, question mark. The first and third sentence delivers stronger meaning and inside, the word delicious, a-m-a-z-i-n-g contributes the most in defining sentiment of the two sentences. Introduction Text classification is one of the fundamental task in Natural Language Processing. The goal is to assign labels to text. It has broad applications including topic labeling (Wang and Manning, 2012), sentiment classification (Maas et al., 2011; Pang and Lee, 2008), and spam detection (Sahami et al., 1998). Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–bas"
N16-1174,D15-1176,1,\N,Missing
N16-1174,N15-1011,0,\N,Missing
N18-1016,P05-1018,0,0.054586,"ds to fine-tune neural generation models using automatic measures such as CIDEr as the reward. However, because most existing automatic measures focus on local n-gram patterns, fine-tuning on those measures may yield deteriorated text despite increased automatic scores, especially for tasks that require long coherent generation (§6.1). Since writing out a scoring term that quantifies the quality of discourse coherence is an open research question, we take inspiration from previous research that learns the overall ordering structure of a document as an approximation of the discourse structure (Barzilay and Lapata, 2005, 2008; Barzilay and Lee, 2004; Li and Hovy, 2014), and propose two neural teachers that can learn to score an ordered sequence of sentences. The scores from these neural teachers are then used to formulate rewards (§4.2) that guide coherent long text generation systems in a policy gradient reinforcement learning setup. Notably, the neural teachers are trained offline on gold sequences in an unsupervised manner prior to training the generator. They are not trained jointly with the generator and their parameters are fixed during policy learning. 2.1 … GRU sj = Lj X xij (1) i=1 where xij is a wo"
N18-1016,D16-1127,1,0.931436,"gure 1: The generator is rewarded for imitating the discourse structure of the gold sequence. Importantly, most automatic measures are based on local n-gram patterns, providing only a limited and myopic perspective of overall text quality. As a result, while models trained to directly optimize these measures can yield improvements on the same measures, they may not lead to better quality in terms of overall coherence or discourse structure. Indeed, recent studies have reported cases where commonly used measures do not align well with desired aspects of generation quality (Rennie et al., 2017; Li et al., 2016). The challenge, however, is to define a global score that can measure the complex aspects of text quality beyond local n-gram patterns. In this paper, we investigate learning neural rewards and their use in a reinforcement learning regime with a specific focus on learning more discourse-aware and coherent text generation. Our approach shares the spirit of the work of Lowe et al. (2017), where neural scores were learned to approximate human judgments of dialogue quality. The key difference is that our rewards can be fully automatically constructed without requiring human judgments and can be t"
N18-1016,J08-1001,0,0.328444,"Missing"
N18-1016,N04-1015,0,0.102263,"models using automatic measures such as CIDEr as the reward. However, because most existing automatic measures focus on local n-gram patterns, fine-tuning on those measures may yield deteriorated text despite increased automatic scores, especially for tasks that require long coherent generation (§6.1). Since writing out a scoring term that quantifies the quality of discourse coherence is an open research question, we take inspiration from previous research that learns the overall ordering structure of a document as an approximation of the discourse structure (Barzilay and Lapata, 2005, 2008; Barzilay and Lee, 2004; Li and Hovy, 2014), and propose two neural teachers that can learn to score an ordered sequence of sentences. The scores from these neural teachers are then used to formulate rewards (§4.2) that guide coherent long text generation systems in a policy gradient reinforcement learning setup. Notably, the neural teachers are trained offline on gold sequences in an unsupervised manner prior to training the generator. They are not trained jointly with the generator and their parameters are fixed during policy learning. 2.1 … GRU sj = Lj X xij (1) i=1 where xij is a word embedding and sj is a sente"
N18-1016,W04-1013,0,0.0121079,"al loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft Research 173 Proceedings of NAACL-HLT 2018, pages 173–184 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics underlying text generator (see"
N18-1016,P09-1068,0,0.0299016,"ilar to our work is work on using neural and embedding rewards to improve dialogue (Li et al., 2016), image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical resu"
N18-1016,P17-1103,0,0.0265649,"ity in terms of overall coherence or discourse structure. Indeed, recent studies have reported cases where commonly used measures do not align well with desired aspects of generation quality (Rennie et al., 2017; Li et al., 2016). The challenge, however, is to define a global score that can measure the complex aspects of text quality beyond local n-gram patterns. In this paper, we investigate learning neural rewards and their use in a reinforcement learning regime with a specific focus on learning more discourse-aware and coherent text generation. Our approach shares the spirit of the work of Lowe et al. (2017), where neural scores were learned to approximate human judgments of dialogue quality. The key difference is that our rewards can be fully automatically constructed without requiring human judgments and can be trained in an unsupervised manner. More specifically, we propose a neural reward learning scheme that is trained to capture crosssentence ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many exis"
N18-1016,mori-etal-2014-flow,0,0.0430861,"n et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical results demonstrate that our teacher-trained generator better models the latent event sequences of cooking recipes,"
N18-1016,D14-1179,0,0.018117,"Missing"
N18-1016,P02-1040,0,0.101245,"ng text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft Research 173 Proceedings of NAACL-HLT 2018, pages 173–184 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics underlying text generator (see Figure 1), which is trained u"
N18-1016,D17-1103,0,0.0163234,"del for the mixed reward was chosen as the one that achieved the highest average geometric mean of BLEU-4 reward and average relative ordering reward for each generated sequence y in the development set: Mixed Training As the model learns parameters to optimize the amount of reward it receives from the teacher, it is not explicity encouraged to produce fluent generations. The model quickly learns to generate simple sequences that exploit the teacher for high rewards despite being incoherent recipes (e.g., Figure 4). Consequently, it is possible that generated sequences are no longer readable (Pasunuru and Bansal, 2017; Paulus et al., 2018). Title: Chili Grits Ingredients: boiling water, butter, shredded cheddar cheese, jalapenos, eggs, chicken cream of soup, salt Generated Recipe: Here . T rb4 (y) X r¯ = rRO (yt ) T (17) t=1 Figure 4: Recipe generated from a self-critical model with no mixed training where rb4 is the BLEU-4 score of the whole generated sequence, and rRO is computed using Equa177 Model Cross-entropy (MLE) BLEU-4 (Rennie et al., 2017) CIDEr (Rennie et al., 2017) ROUGE-L (Paulus et al., 2018) BLEU-1 (γ = 0.97) BLEU-4 (γ = 0.99) CIDEr (γ = 0.97) ROUGE-L (γ = 0.97) Absolute Ordering (AO) Relati"
N18-1016,D15-1114,1,0.861518,"image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical results demonstrate that our teacher-trained generator better models the latent event sequences o"
N18-1016,D16-1032,1,0.893981,"to capture crosssentence ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft"
N18-1016,P16-1027,0,0.0227005,"sing neural and embedding rewards to improve dialogue (Li et al., 2016), image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical results demonstrate that our tea"
N18-1016,D14-1218,0,0.0229299,"measures such as CIDEr as the reward. However, because most existing automatic measures focus on local n-gram patterns, fine-tuning on those measures may yield deteriorated text despite increased automatic scores, especially for tasks that require long coherent generation (§6.1). Since writing out a scoring term that quantifies the quality of discourse coherence is an open research question, we take inspiration from previous research that learns the overall ordering structure of a document as an approximation of the discourse structure (Barzilay and Lapata, 2005, 2008; Barzilay and Lee, 2004; Li and Hovy, 2014), and propose two neural teachers that can learn to score an ordered sequence of sentences. The scores from these neural teachers are then used to formulate rewards (§4.2) that guide coherent long text generation systems in a policy gradient reinforcement learning setup. Notably, the neural teachers are trained offline on gold sequences in an unsupervised manner prior to training the generator. They are not trained jointly with the generator and their parameters are fixed during policy learning. 2.1 … GRU sj = Lj X xij (1) i=1 where xij is a word embedding and sj is a sentence embedding. Each"
N18-1016,D17-1197,0,0.0277131,"nce ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft Research 173 Procee"
N18-1016,D17-1062,0,0.0267216,"0.056 0.054 0.052 0.050 0.048 State Change BLEU-4 0.95 0.97 0.98 0.300 0.295 0.290 0.285 0.280 0.275 Figure 5: Action and State Change BLEU Metrics for different initializations of `max and γ guage model to deteriorate. Interestingly, a higher `max leads to better performance on global coherence scores, implying that relative order rewards conditioned on more sentences allow the model to learn longer-range context co-occurrences. 7 Most similar to our work is work on using neural and embedding rewards to improve dialogue (Li et al., 2016), image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn"
N18-1016,D15-1044,0,0.164506,"d in an unsupervised manner. More specifically, we propose a neural reward learning scheme that is trained to capture crosssentence ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning met"
N18-1016,D17-1239,0,0.0387937,"Missing"
N18-1016,1983.tc-1.13,0,0.67195,"Missing"
N18-1114,W05-0909,0,0.0579862,"or v has 2048 dimensions. Word embedding vectors in We are downloaded from the web (Pennington et al., 2017). The model is implemented in TensorFlow (Abadi et al., 2015) with the default settings for random initialization and optimization by backpropagation. In our experiments, we choose d = 25 (where d is the dimension of vector pt ). The dimension of ˆ t is 25 × 25); the vocabSt is 625 × 625 (while S ulary size V = 8, 791; the dimension of ut and ft is d2 = 625. The main evaluation results on the MS COCO dataset are reported in Table 5.2. The widelyused BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015) metrics are reported in our quantitative evaluation of the performance of the proposed model. In evaluation, our baseline is the widely used CNN-LSTM captioning method originally proposed in (Vinyals et al., 2015). For comparison, we include results in that paper in the first line of Table 5.2. We also re-implemented the model using the latest ResNet features and report the results in the second line of Table 5.2. Our re-implementation of the CNN-LSTM method matches the performance reported in (Gan et al., 2017), showing that the baseline is a state-of-thear"
N18-1114,P15-2017,1,0.839834,"pretable patterns. (More comprehensive results will be discussed elsewhere.) The clusters can be interpreted as falling into 3 groups (see Table 5.3.1). Clusters 2 and 3 are clearly positional roles: every initial word is generated by a role-unbinding vector from Cluster 2, Related work This work follows a great deal of recent captiongeneration literature in exploiting end-to-end deep learning with a CNN image-analysis front end producing a distributed representation that is then used to drive a natural-language generation process, typically using RNNs (Mao et al., 2015; Vinyals et al., 2015; Devlin et al., 2015; Chen and Zitnick, 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015; Kiros et al., 2014a,b; Xu et al., 2017; Rennie et al., 2017; Yao et al., 2017; Lu et al., 2017). Our grammatical interpretation of the structural roles of words in sentences makes contact with other work that incorporates deep learning into grammatically-structured networks (Tai et al., 2015; Kumar et al., 2016; Kong et al., 2017; Andreas et al., 2015; Yogatama et al., 2016; Maillard et al., 2017; Socher et al., 2010; Pollack, 1990). Here, the network is not itself structured to match the grammatical structure of sente"
N18-1114,D16-1044,0,0.0367547,"Socher et al., 2010; Pollack, 1990). Here, the network is not itself structured to match the grammatical structure of sentences being processed; the structure is fixed, but is designed to support the learning of distributed representations that incorporate structure internal to the representations themselves — filler/role structure. TPRs are also used in NLP in (Palangi et al., 2017) but there the representation of each individual input word is constrained to be a literal TPR filler/role binding. (The idea of using the outer product to construct internal representations was also explored in (Fukui et al., 2016).) Here, by contrast, the learned representations are not themselves constrained, but the global structure of the network is designed to display the somewhat abstract property of being TPR-capable: the archi1270 tecture uses the TPR unbinding operation of the matrix-vector product to extract individual words for sequential output. 7 Conclusion Tensor Product Representation (TPR) (Smolensky, 1990) is a general technique for constructing vector embeddings of complex symbol structures in such a way that powerful symbolic functions can be computed using hand-designed neural network computation. In"
N18-1114,P02-1040,0,0.106016,"ageNet dataset. The feature vector v has 2048 dimensions. Word embedding vectors in We are downloaded from the web (Pennington et al., 2017). The model is implemented in TensorFlow (Abadi et al., 2015) with the default settings for random initialization and optimization by backpropagation. In our experiments, we choose d = 25 (where d is the dimension of vector pt ). The dimension of ˆ t is 25 × 25); the vocabSt is 625 × 625 (while S ulary size V = 8, 791; the dimension of ut and ft is d2 = 625. The main evaluation results on the MS COCO dataset are reported in Table 5.2. The widelyused BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015) metrics are reported in our quantitative evaluation of the performance of the proposed model. In evaluation, our baseline is the widely used CNN-LSTM captioning method originally proposed in (Vinyals et al., 2015). For comparison, we include results in that paper in the first line of Table 5.2. We also re-implemented the model using the latest ResNet features and report the results in the second line of Table 5.2. Our re-implementation of the CNN-LSTM method matches the performance reported in (Gan et al., 2017), showing th"
N18-1114,P15-1150,0,0.0429963,"end-to-end deep learning with a CNN image-analysis front end producing a distributed representation that is then used to drive a natural-language generation process, typically using RNNs (Mao et al., 2015; Vinyals et al., 2015; Devlin et al., 2015; Chen and Zitnick, 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015; Kiros et al., 2014a,b; Xu et al., 2017; Rennie et al., 2017; Yao et al., 2017; Lu et al., 2017). Our grammatical interpretation of the structural roles of words in sentences makes contact with other work that incorporates deep learning into grammatically-structured networks (Tai et al., 2015; Kumar et al., 2016; Kong et al., 2017; Andreas et al., 2015; Yogatama et al., 2016; Maillard et al., 2017; Socher et al., 2010; Pollack, 1990). Here, the network is not itself structured to match the grammatical structure of sentences being processed; the structure is fixed, but is designed to support the learning of distributed representations that incorporate structure internal to the representations themselves — filler/role structure. TPRs are also used in NLP in (Palangi et al., 2017) but there the representation of each individual input word is constrained to be a literal TPR filler/rol"
N18-1150,N16-1012,0,0.139479,"e capable of generating fluent language, variants of encoder-decoder RNNs (Sutskever et al., 2014; Bahdanau et al., 2015) have shown promising results on the abstractive summarization task (Rush et al., 2015; Nallapati et al., 2017). The fundamental challenge, however, is that the strong performance of neural models at encoding short text does not generalize well to long text. The motivation behind our approach is to be able to dynamically attend to different parts of the input to capture salient facts. While recent work in summarization addresses these issues using improved attention models (Chopra et al., 2016), pointer networks with coverage mechanisms (See et al., 2017), and coherence-focused training objectives (Paulus et al., 2018; Jaques et al., 2017), an effective mechanism for representing a long document remains a challenge. Simultaneous work has investigated the use of deep communicating agents (Sukhbaatar et al., 2016) for collaborative tasks such as logic puzzles (Foerster et al., 2016), visual dialog (Das et al., 2017), and reference games (Lazaridou et al., 2016). Our work builds on these approaches to propose the first study on using communicating agents to encode long text for summari"
N18-1150,P16-1188,0,0.222848,"Missing"
N18-1150,W16-3617,0,0.0592334,"Missing"
N18-1150,D15-1166,0,0.153582,"Missing"
N18-1150,P14-5010,0,0.00914565,"Missing"
N18-1150,1983.tc-1.13,0,0.463783,"Missing"
N18-1150,N16-1174,1,0.51722,"capture all the facts in the human summary, while (m7) is able to include all the facts with few extra details, generating more relevant and diverse summaries. 6 Related Work Several recent works investigate attention mechanisms for encoder-decoder models to sharpen the 1669 context that the decoder should focus on within the input encoding (Luong et al., 2015; Vinyals et al., 2015b; Bahdanau et al., 2015). For example, Luong et al. (2015) proposes global and local attention networks for machine translation, while others investigate hierarchical attention networks for document classification (Yang et al., 2016), sentiment classification (Chen et al., 2016), and dialog response selection (Zhou et al., 2016). Attention mechanisms have shown to be crucial for summarization as well (Rush et al., 2015; Zeng et al., 2016; Nallapati et al., 2017), and pointer networks (Vinyals et al., 2015a), in particular, help address redundancy and saliency in generated summaries (Cheng and Lapata, 2016; See et al., 2017; Paulus et al., 2018; Fan et al., 2017). While we share the same motivation as these works, our work uniquely presents an approach based on CommNet, the deep communicating agent framework (Sukhbaatar et"
N18-1150,D17-1103,0,0.0367234,"h sentence s0q , q=1. . . Q, where s0q ∈{st :yt =‘·’, 1≤t≤T }, are used to compute the cosine similarity between two consecutively generated sentences. To minimize the similarity between end-of-sentence hidden states we define a semantic cohesion loss: 1665 LSEM = PQ 0 0 q=2 cos(sq , sq−1 ) (18) The final training objective is then: LMLE-SEM = LMLE + λLSEM (19) where λ is a tunable hyperparameter. Reinforcement Learning (RL) Loss Policy gradient methods can directly optimize discrete target evaluation metrics such as ROUGE that are non-differentiable (Paulus et al., 2018; Jaques et al., 2017; Pasunuru and Bansal, 2017; Wu et al., 2016). At each time step, the word generated by the model can be viewed as an action taken by an RL agent. Once the full sequence yˆ is generated, it is compared against the ground truth sequence y ∗ to compute the reward r(ˆ y ). Our model learns using a self-critical training approach (Rennie et al., 2016), which learns by exploring new sequences and comparing them to the best greedily decoded sequence. For each training example d, two output sequences are generated: yˆ, which is sampled from the probability distribution at each time step, p(ˆ yt |ˆ y1 . . . yˆt−1 , d), and y˜,"
N18-1150,D14-1162,0,0.0795994,"Missing"
N18-1150,D16-1036,0,0.0187609,"extra details, generating more relevant and diverse summaries. 6 Related Work Several recent works investigate attention mechanisms for encoder-decoder models to sharpen the 1669 context that the decoder should focus on within the input encoding (Luong et al., 2015; Vinyals et al., 2015b; Bahdanau et al., 2015). For example, Luong et al. (2015) proposes global and local attention networks for machine translation, while others investigate hierarchical attention networks for document classification (Yang et al., 2016), sentiment classification (Chen et al., 2016), and dialog response selection (Zhou et al., 2016). Attention mechanisms have shown to be crucial for summarization as well (Rush et al., 2015; Zeng et al., 2016; Nallapati et al., 2017), and pointer networks (Vinyals et al., 2015a), in particular, help address redundancy and saliency in generated summaries (Cheng and Lapata, 2016; See et al., 2017; Paulus et al., 2018; Fan et al., 2017). While we share the same motivation as these works, our work uniquely presents an approach based on CommNet, the deep communicating agent framework (Sukhbaatar et al., 2016). Compared to prior multi-agent works on logic puzzles (Foerster et al., 2017), langua"
N18-1150,D15-1044,0,0.839936,"input text. Introduction We focus on the task of abstractive summarization of a long document. In contrast to extractive summarization, where a summary is composed of a subset of sentences or words lifted from the input text as is, abstractive summarization requires the generative ability to rephrase and restructure sentences to compose a coherent and concise summary. As recurrent neural networks (RNNs) are capable of generating fluent language, variants of encoder-decoder RNNs (Sutskever et al., 2014; Bahdanau et al., 2015) have shown promising results on the abstractive summarization task (Rush et al., 2015; Nallapati et al., 2017). The fundamental challenge, however, is that the strong performance of neural models at encoding short text does not generalize well to long text. The motivation behind our approach is to be able to dynamically attend to different parts of the input to capture salient facts. While recent work in summarization addresses these issues using improved attention models (Chopra et al., 2016), pointer networks with coverage mechanisms (See et al., 2017), and coherence-focused training objectives (Paulus et al., 2018; Jaques et al., 2017), an effective mechanism for representi"
N18-1150,P17-1108,0,0.594329,"formance gains. We fix γ = 0.97 for the RL term in Equation (21) and λ = 0.1 for the SEM term in MLE and MIXED training. Additional details are provided in Appendix A.2. Evaluation We evaluate our system using ROUGE-1 (unigram recall), ROUGE-2 (bigram recall) and ROUGE-L (longest common sequence).1 We select the MLE models with the lowest negative log-likelihood and the MLE+RL models with the highest ROUGE-L scores on a sample of validation data to evaluate on the test 1666 1 We use pyrouge (pypi.python.org/pypi/pyrouge/0.1.3). Model SummaRuNNer (Nallapati et al., 2017) graph-based attention (Tan et al., 2017) pointer generator (See et al., 2017) pointer generator + coverage (See et al., 2017) controlled summarization with fixed values (Fan et al., 2017) RL, with intra-attention (Paulus et al., 2018) ML+RL, with intra-attention(Paulus et al., 2018) (m1) MLE, pgen, no-comm (1-agent) (our baseline-1) (m2) MLE+SEM, pgen, no-comm (1-agent) (our baseline-2) (m3) MLE+RL, pgen, no-comm (1-agent) (our baseline-3) (m4) DCA MLE+SEM, pgen, no-comm (3-agents) (m5) DCA MLE+SEM, mpgen, with-comm (3-agents) (m6) DCA MLE+SEM, mpgen, with-comm, with caa (3-agents) (m7) DCA MLE+SEM+RL, mpgen, with-comm, with caa (3-"
N18-1150,D15-1011,0,\N,Missing
N18-1150,P16-1046,0,\N,Missing
N18-2115,D11-1039,0,0.0113816,"dings by Hashimoto et al. (2017) (100 dimension) and the GloVe word embedding (100 dimension) by Pennington et al. (2014); each token is embedded into a 200 dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer la"
N18-2115,D17-1206,0,0.0607491,"Missing"
N18-2115,P17-1089,0,0.0991901,"Missing"
N18-2115,D13-1160,0,0.0744986,"17) (100 dimension) and the GloVe word embedding (100 dimension) by Pennington et al. (2014); each token is embedded into a 200 dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer latent programs given i"
N18-2115,D17-1160,0,0.0288339,"dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer latent programs given input/output examples, while program synthesis models (Zhong et al., 2017; Parisotto et al., 2017) aim to generate explicit programs and"
N18-2115,J84-3009,0,0.113713,"Missing"
N18-2115,P14-5010,0,0.00279152,"arameters with gradient descent: θi0 = θ − α∇θ LTi (fθ ) 9: end for P 10: Update θ ← θ − β∇θ Ti ∼p(T ) LTi (fθi0 ) using each Di0 from Ti and LTi for the metaupdate 11: end while In this section, we introduce the WikiSQL dataset and preprocessing steps, the learner model in our meta-learning setup, and the experimental results. {x(j) , y(j) } 4.1 Dataset We evaluate our model on the WikiSQL dataset (Zhong et al., 2017). We follow the data preprocessing in (Wang et al., 2017). Specifically, we first preprocess the dataset by running both tables and question-query pairs through Stanford Stanza (Manning et al., 2014) using the script included with the WikiSQL dataset, which normalizes punctuations and cases of the dataset. We further normalize each question based on its corresponding table: for table entries and columns occurring in questions or queries, we normalize their format to be consistent with the table. After preprocessing, we filter the training set by removing pairs whose ground truth solution contains constants not mentioned in the question, as our model requires the constants to be copied from the question. We train and tune our model only on the filtered training and filtered development set"
N18-2115,D16-1166,1,0.852119,"14); each token is embedded into a 200 dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer latent programs given input/output examples, while program synthesis models (Zhong et al., 2017; Parisotto et a"
N18-2115,P15-1129,0,0.0351406,"nnington et al. (2014); each token is embedded into a 200 dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer latent programs given input/output examples, while program synthesis models (Zhong et al.,"
N18-2115,P15-1128,1,0.891606,"Missing"
N18-2115,P14-2105,1,0.83,"ng (100 dimension) by Pennington et al. (2014); each token is embedded into a 200 dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer latent programs given input/output examples, while program synthes"
N18-2115,Q14-1042,0,0.0446038,"nd the GloVe word embedding (100 dimension) by Pennington et al. (2014); each token is embedded into a 200 dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer latent programs given input/output examples, whi"
P09-1107,P08-2021,0,0.100227,"Missing"
P09-1107,E06-1005,0,0.132743,"d ngrams between two spans; 2) treat CN spans as HMM states and define state transition as distortion over words in component translations in the CN; and 3) use a consensus decoding algorithm over one hypothesis and multiple IHMMs, each of which corresponds to a component translation in the CN. All these three approaches of incremental alignment based on IHMM are shown to be superior to both incremental TER alignment and conventional IHMM alignment in the setting of the Chinese-to-English track of the 2008 NIST Open MT evaluation. 1 Introduction Word-level combination using confusion network (Matusov et al. (2006) and Rosti et al. (2007)) is a widely adopted approach for combining Machine Translation (MT) systems’ output. Word alignment between a backbone (or skeleton) translation and a hypothesis translation is a key problem in this approach. Translation Edit Rate (TER, Snover et al. (2006)) based alignment proposed in Sim 1 Note that this CN may generate an incomplete sentence “he bought a”, which is nevertheless unlikely to be selected as it leads to low language model score. 949 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 949–957, c Suntec, Singapore, 2-"
P09-1107,W08-0329,0,0.785695,"Xidazhi Street, Harbin, China ypliu@mtlab.hit.edu.cn Ning Xi Nanjing University 8 Hankou Road, Nanjing, China xin@nlp.nju.edu.cn Abstract et al. (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al. (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. With an alignment method, each hypothesis is aligned against the backbone and all the alignments are then used to build a confusion network (CN) for generating a better translation. However, as pointed out by Rosti et al. (2008), such a pair-wise alignment strategy will produce a low-quality CN if there are errors in the alignment of any of the hypotheses, no matter how good the alignments of other hypotheses are. For example, suppose we have the backbone “he buys a computer” and two hypotheses “he bought a laptop computer” and “he buys a laptop”. It will be natural for most alignment methods to produce the alignments in Figure 1a. The alignment of hypothesis 2 against the backbone cannot be considered an error if we consider only these two translations; nevertheless, when added with the alignment of another hypothes"
P09-1107,P02-1040,0,\N,Missing
P09-1107,P07-1040,0,\N,Missing
P09-1107,J03-1002,0,\N,Missing
P09-1107,D08-1011,1,\N,Missing
P12-1031,P06-1002,0,0.0130679,"Missing"
P12-1031,P08-1024,0,0.0175924,"raining, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks. 1. Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation m"
P12-1031,J93-2003,0,0.0675454,"slation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood, which may not correspond closely to the translation measure, e.g., bilingual evaluation understudy (BLEU) (Papineni et al., 2002). Therefore, it is desirable to train all these parameters to directly maximize an objective that directly links to translation quality. However, there are a large number of parameters in these models, making discriminative training for them non-trivial (e.g., Liang et al., 2006, Chiang et al., 2009). Liang et al. (2006) proposed"
P12-1031,N03-1017,0,0.161351,"istical machine translation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood, which may not correspond closely to the translation measure, e.g., bilingual evaluation understudy (BLEU) (Papineni et al., 2002). Therefore, it is desirable to train all these parameters to directly maximize an objective that directly links to translation quality. However, there are a large number of parameters in these models, making discriminative training for them non-trivial (e.g., Liang et al., 2006, Chiang et al., 2009). Liang e"
P12-1031,W04-3250,0,0.126121,"Missing"
P12-1031,P06-1096,0,0.680805,"Missing"
P12-1031,D08-1076,0,0.0287972,"ng MIRA. In our work, we have many more parameters to train, and the training is conducted on the entire training corpora. Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training. As a further related work, Rosti et al. (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al., 2008) and lattice-based MERT (Macherey et al., 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al. (2010) proposed a method to train the phrase translation model using ExpectationMaximization algorithm with a leave-one-out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities were estimated based on the phrase alignments. To prevent overfitting,"
P12-1031,D08-1064,0,0.0102403,"?-‐ ???? ????ℎ??) + ? ∙ ?!! #(?-‐ ????) + ? (19) where ?!! is the prior value of ?! , ? is a smoothing factor usually takes a value of 5 and ?!! can be set by ?!! = ?!!! ∙ ?!!! ?!!! , for n = 3, 4. ?! and ?! are estimated empirically. Brevity penalty (BP) also plays a key role. Instead of clip it at 1, we use ! a non-clipped BP, ?? = ? (!!!) , for sentence-level BLEU1. We further scale the reference length, r, by a factor such that the total length of references on the training set equals that of the baseline output2. 1 This is to better approximate corpus-level BLEU, i.e., as discussed in (Chiang, et al., 2008), the per-sentence BP might effectively exceed unity in corpus-level BLEU computation. 2 This is to focus the training on improving BLEU by improving n-gram match instead of by improving BP, e.g., this makes the BP of the baseline output already being perfect. 4.3.3. Training procedure The parameter set θ is optimized on the training set while the feature weights λ are tuned on a small tuning set3. Since θ and λ affect the training of each other, we train them in alternation. I.e., at each iteration, we first fix λ and update θ, then we re-tune λ given the new θ. Due to mismatch between traini"
P12-1031,2007.mtsummit-papers.43,0,0.0669302,"Missing"
P12-1031,N09-1025,0,0.629175,"wth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks. 1. Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated b"
P12-1031,P02-1038,0,0.0908163,"Missing"
P12-1031,P03-1021,0,0.879863,"bilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks. 1. Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood"
P12-1031,D10-1044,0,0.00792926,"or phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks. 1. Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated by relative frequency"
P12-1031,P02-1040,0,0.103309,"ror rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood, which may not correspond closely to the translation measure, e.g., bilingual evaluation understudy (BLEU) (Papineni et al., 2002). Therefore, it is desirable to train all these parameters to directly maximize an objective that directly links to translation quality. However, there are a large number of parameters in these models, making discriminative training for them non-trivial (e.g., Liang et al., 2006, Chiang et al., 2009). Liang et al. (2006) proposed a large set of lexical and Part-of-Speech features and trained the model weights associated with these features using perceptron. Since many of the reference translations are non-reachable, an empirical local updating strategy had to be devised to fix this problem by"
P12-1031,W07-0711,1,0.799951,"Missing"
P12-1031,P05-1034,0,0.0790501,"Missing"
P12-1031,W11-2119,0,0.223392,"g the updating reference and therefore gives a more principal way of setting the training objective. As another closely related study, Chiang et al. (2009) incorporated about ten thousand syntactic features in addition to the baseline features. The feature weights are trained on a tuning set with 2010 sentences using MIRA. In our work, we have many more parameters to train, and the training is conducted on the entire training corpora. Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training. As a further related work, Rosti et al. (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al., 2008) and lattice-based MERT (Macherey et al., 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al. (2010) proposed a method to train the phrase translation model u"
P12-1031,P06-2101,0,0.0247608,"e feature weights are trained on a tuning set with 2010 sentences using MIRA. In our work, we have many more parameters to train, and the training is conducted on the entire training corpora. Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training. As a further related work, Rosti et al. (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al., 2008) and lattice-based MERT (Macherey et al., 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al. (2010) proposed a method to train the phrase translation model using ExpectationMaximization algorithm with a leave-one-out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities"
P12-1031,P10-1049,0,0.0941317,"e scale discriminative training. As a further related work, Rosti et al. (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al., 2008) and lattice-based MERT (Macherey et al., 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al. (2010) proposed a method to train the phrase translation model using ExpectationMaximization algorithm with a leave-one-out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities were estimated based on the phrase alignments. To prevent overfitting, the statistics of phrase pairs from a particular sentence was excluded from the phrase table when aligning that sentence. However, as pointed out by Liang et al (2006), the same problem as in the bold updating existed, i.e"
P12-1031,D08-1065,0,0.0408372,"rained on a tuning set with 2010 sentences using MIRA. In our work, we have many more parameters to train, and the training is conducted on the entire training corpora. Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training. As a further related work, Rosti et al. (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al., 2008) and lattice-based MERT (Macherey et al., 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al. (2010) proposed a method to train the phrase translation model using ExpectationMaximization algorithm with a leave-one-out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities were estimated based on"
P12-1031,D11-1033,1,\N,Missing
P12-1031,federico-etal-2012-iwslt,0,\N,Missing
P12-1031,2011.iwslt-evaluation.1,0,\N,Missing
P12-1031,D11-1081,0,\N,Missing
P14-1066,D13-1176,0,0.177152,"lation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 20"
P14-1066,N03-1017,0,0.177482,"continuous representation of a phrase. Finally, the translation score of a 1 Introduction source-target phrase pair is computed by the disThe phrase translation model, also known as the tance between their feature vectors. The main motivation behind the CPTM is to phrase table, is one of the core components of alleviate the data sparseness problem associated phrase-based statistical machine translation (SMT) with the traditional counting-based methods by systems. The most common method of constructing the phrase table takes a two-phase approach grouping phrases with a similar meaning across (Koehn et al. 2003). First, the bilingual phrase different languages. This style of grouping is pairs are extracted heuristically from an automat- made possible because of the distributed nature of ically word-aligned training data. The second the continuous-space representations for phrases. phase, which is the focus of this paper, is parame- No such sharing was possible in the original symter estimation where each phrase pair is assigned bolic space for representing words or phrases. In with some scores that are estimated based on this model, semantically or grammatically related counting these phrases or thei"
P14-1066,2005.mtsummit-posters.11,0,0.0297066,"Missing"
P14-1066,P06-1096,0,0.0100054,"ns a real-valued weight to each feature. The components GEN(. ), ? and ? define a loglinear model that maps ?? to an output sentence as follows: ? ∗ = argmax ?T ?(?? , ?, ?) (1) (?,?)∈GEN(?? ) which states that given ? and ?, argmax returns the highest scoring translation ? ∗ , maximizing over correspondences ?. In phrase-based SMT, ? consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases. Since computing the argmax exactly is intractable, it is commonly performed approximatedly by beam search (Och and Ney 2004). Following Liang et al. (2006), we assume that every translation candidate is always coupled with a corresponding ?, called the Viterbi derivation, generated by (1). 4 A Continuous-Space Phrase Translation Model (CPTM) The architecture of the CPTM is shown in Figures 1 and 2, where for each pair of source and target phrases (?? , ?? ) in a source-target sentence pair, we first project them into feature vectors ??? and ??? in a latent, continuous space via a neural network with one hidden layer (as shown in Figure 2), and then compute the translation score, score(?? , ?? ), by the distance of their feature vectors in that s"
P14-1066,N13-1048,1,0.931372,"proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires learning a mapping between input ? ∈ ℱ to output ? ∈ ℰ. We are given  Training samples (?? , ?? ) for ? = 1 … ?, where eac"
P14-1066,P12-1031,1,0.819542,"ured in selecting translations. However, longer phrases occur less often in trainThis paper tackles the sparsity problem in ing data, leading to a severe data sparseness probestimating phrase translation probabilities lem in parameter estimation. There has been a by learning continuous phrase representaplethora of research reported in the literature on tions, whose distributed nature enables the improving parameter estimation for the phrase sharing of related phrases in their representranslation model (e.g., DeNero et al. 2006; tations. A pair of source and target phrases Wuebker et al. 2010; He and Deng 2012; Gao and are projected into continuous-valued vecHe 2013). tor representations in a low-dimensional This paper revisits the problem of scoring a latent space, where their translation score phrase translation pair by developing a Continuis computed by the distance between the ous-space Phrase Translation Model (CPTM). pair in this new space. The projection is The translation score of a phrase pair in this model performed by a neural network whose is computed as follows. First, we represent each weights are learned on parallel training phrase as a bag-of-words vector, called word vecdata. Exper"
P14-1066,P07-2045,0,0.0641989,"us representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires learning a mapping between input ? ∈ ℱ to output ? ∈ ℰ. We are given  Training samples (?? , ?? ) for ? = 1 … ?, where each source sentence ?? is paired with a reference translation in target language ?? ;  A procedure GEN to generate a list of N-best candidates GEN(?? ) for an input ?? , where GEN in this study is the baseline phrasebased SMT system, i.e., an in-house implementation of the Moses system (Koehn et al. 2007) that does not use the CPTM, and each ? ∈ GEN(?? ) is labeled by the sentence-level BLEU score (He and Deng 2012), denoted by sBleu(?? , ?) , which measures the quality of ? with respect to its reference translation ?? ;  A vector of features ? ∈ ℝ? that maps each (?? , ?) to a vector of feature values2; and  A parameter vector ? ∈ ℝ? , which assigns a real-valued weight to each feature. The components GEN(. ), ? and ? define a loglinear model that maps ?? to an output sentence as follows: ? ∗ = argmax ?T ?(?? , ?, ?) (1) (?,?)∈GEN(?? ) which states that given ? and ?, argmax returns the hig"
P14-1066,W06-3114,0,0.0110009,"es of source given target phrase mappings ???? (?|?) and vice versa ???? (?|?), as well as lexical weighting estimates ??? (?|?) and ??? (?|?), word and phrase penalties, a linear distortion feature, and a lexicalized reordering feature. The baseline includes a standard 5-gram modified Kneser-Ney language model trained on the target side of the parallel corpora described below. Log-linear weights are estimated with the MERT algorithm (Och 2003). 704 Evaluation. We test our models on two different data sets. First, we train an English to French system based on the data of WMT 2006 shared task (Koehn and Monz 2006). The parallel corpus includes 688K sentence pairs of parliamentary proceedings for training. The development set contains 2000 sentences, and the test set contains other 2000 sentences, all from the official WMT 2006 shared task. Second, we experiment with a French to English system developed using 2.1M sentence pairs of training data, which amounts to 102M words, from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for 5M words which are newswire. We use the 2009 newswire data set, comprising 2525 sentences, as the development set. We evaluate"
P14-1066,D13-1054,0,0.0498113,"00 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier"
P14-1066,W02-1018,0,0.0501549,"phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires le"
P14-1066,N13-1090,1,0.0323834,"e translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the st"
P14-1066,D09-1092,0,0.0576492,"Missing"
P14-1066,W11-2124,0,0.0188708,"similarity between a source phrase and its paired target phrase by projecting them into a common, continuous space that is language independent. The rest of the paper is organized as follows. Section 2 reviews previous work. Section 3 reviews the log-linear model for phrase-based SMT and Sections 4 presents the CPTM. Section 5 describes the way the model parameters are estimated, followed by the experimental results in Section 6. Finally, Section 7 concludes the paper. 1 version of such a model can be trained efficiently because the factor models used by Son et al. cannot be applied directly. Niehues et al. (2011) use different translation units in order to integrate the n-gram translation model into the phrasebased approach. However, it is not clear how a continuous 2 Related Work Representations of words or documents as continuous vectors have a long history. Most of the earlier latent semantic models for learning such vectors are designed for information retrieval (Deerwester et al. 1990; Hofmann 1999; Blei et al. 2003). In contrast, recent work on continuous space language models, which estimate the probability of a word sequence in a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have"
P14-1066,P03-1021,0,0.658932,"CPTM is incorporated, is parameterized by (?, ?), where ? is a vector of a handful of parameters used in the log-linear model of (1), with one weight for each feature; and ? is the projection matrices used in the CPTM defined by (2) and (3). In our experiments we take three steps to learn (?, ?): 1. We use a baseline phrase-based SMT system to generate for each source sentence in training data an N-best list of translation hypotheses4. 2. We set ? to that of the baseline system and let ??+1 = 1, and optimize ? w.r.t. a loss function on training data5. 3. We fix ? , and optimize ? using MERT (Och 2003) to maximize BLEU on dev data. The translation score of a source phrase f and a target phrase e can be measured as the similarity (or distance) between their feature vectors. We choose the dot product as the similarity function3: score(?, ?) ≡ sim? (?? , ?? ) = ??T ?? (3) According to (2), we see that the value of the scoring function is determined by the projection matrices ? = {?1 , ?2 }. The CPTM of (2) and (3) can be incorporated into the log-linear model for SMT (1) by 3 In our experiments, we compare dot product and the cosine similarity functions and find that the former works better fo"
P14-1066,J04-4002,0,0.135511,"r vector ? ∈ ℝ? , which assigns a real-valued weight to each feature. The components GEN(. ), ? and ? define a loglinear model that maps ?? to an output sentence as follows: ? ∗ = argmax ?T ?(?? , ?, ?) (1) (?,?)∈GEN(?? ) which states that given ? and ?, argmax returns the highest scoring translation ? ∗ , maximizing over correspondences ?. In phrase-based SMT, ? consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases. Since computing the argmax exactly is intractable, it is commonly performed approximatedly by beam search (Och and Ney 2004). Following Liang et al. (2006), we assume that every translation candidate is always coupled with a corresponding ?, called the Viterbi derivation, generated by (1). 4 A Continuous-Space Phrase Translation Model (CPTM) The architecture of the CPTM is shown in Figures 1 and 2, where for each pair of source and target phrases (?? , ?? ) in a source-target sentence pair, we first project them into feature vectors ??? and ??? in a latent, continuous space via a neural network with one hidden layer (as shown in Figure 2), and then compute the translation score, score(?? , ?? ), by the distance of"
P14-1066,P02-1040,0,0.0988536,"from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for 5M words which are newswire. We use the 2009 newswire data set, comprising 2525 sentences, as the development set. We evaluate on four newswire domain test sets from 2008, 2010 and 2011 as well as the 2010 system combination test set, containing 2034 to 3003 sentences. In this study we perform a detailed empirical comparison using the WMT 2006 data set, and verify our best models and results using the larger WMT 2012 data set. The metric used for evaluation is case insensitive BLEU score (Papineni et al. 2002). We also perform a significance test using the Wilcoxon signed rank test. Differences are considered statistically significant when the p-value is less than 0.05. 6.2 Results of the CPTM Table 1 shows the results measured in BLEU evaluated on the WMT 2006 data set, where Row 1 is the baseline system. Rows 2 to 4 are the systems enhanced by integrating different versions of the CPTM. Rows 5 to 7 present the results of previous models. Row 8 is our best system. Table 2 shows the main results on the WMT 2012 data set. CPTM is the model described in Sections 4. As illustrated in Figure 2, the num"
P14-1066,D10-1025,1,0.710259,"(Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural net"
P14-1066,P10-1049,0,0.0665678,"formation can be captured in selecting translations. However, longer phrases occur less often in trainThis paper tackles the sparsity problem in ing data, leading to a severe data sparseness probestimating phrase translation probabilities lem in parameter estimation. There has been a by learning continuous phrase representaplethora of research reported in the literature on tions, whose distributed nature enables the improving parameter estimation for the phrase sharing of related phrases in their representranslation model (e.g., DeNero et al. 2006; tations. A pair of source and target phrases Wuebker et al. 2010; He and Deng 2012; Gao and are projected into continuous-valued vecHe 2013). tor representations in a low-dimensional This paper revisits the problem of scoring a latent space, where their translation score phrase translation pair by developing a Continuis computed by the distance between the ous-space Phrase Translation Model (CPTM). pair in this new space. The projection is The translation score of a phrase pair in this model performed by a neural network whose is computed as follows. First, we represent each weights are learned on parallel training phrase as a bag-of-words vector, called w"
P14-1066,W11-0329,1,0.895525,"have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to repres"
P14-1066,W11-2119,0,0.0732387,"Missing"
P14-1066,D07-1045,0,0.0166222,"e projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source"
P14-1066,C12-2104,0,0.199051,"shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as w"
P14-1066,W12-2702,0,0.016591,"gle words, are smooth function of these feature vectors, a small Abstract 699 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 699–709, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics change in the features should only lead to a small change in the translation score. The primary research task in developing the CPTM is learning the continuous representation of a phrase that is effective for SMT. Motivated by recent studies on continuous-space language models (e.g., Bengio et al. 2003; Mikolov et al. 2011; Schwenk et al., 2012), we use a neural network to project a word vector to a feature vector. Ideally, the projection would discover those latent features that are useful to differentiate good translations from bad ones, for a given source phrase. However, there is no training data with explicit annotation on the quality of phrase translations. The phrase translation pairs are hidden in the parallel source-target sentence pairs, which are used to train the traditional translation models. The quality of a phrase translation can only be judged implicitly through the translation quality of the sentences, as measured b"
P14-1066,N13-1120,1,0.781116,"Missing"
P14-1066,D13-1141,0,0.0690135,"ing, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier"
P14-1066,N13-1034,0,0.0171881,"rity of the phrases in the project space. As we see from Table 1, both latent semantic models, although leading to some slight improvement over Baseline, are much less effective than CPTM. Finally, we compare the CPTM with the Markov Random Field model using phrase features (MRFP in Tables 1 and 2), proposed by Gao and He (2013)7, on both the WMT 2006 and WMT 2012 datasets. MRFp is a state-of-the-art large scale discriminative training model that uses the same expected BLEU training criterion, which has proven to give superior performance across a range of MT tasks recently (He and Deng 2012, Setiawan and Zhou 2013, Gao and He 2013). Unlike CPTM, MRFp is a linear model that simply treats each phrase pair as a single feature. Therefore, although both are trained using the 7 Gao and He (2013) reported results of MRF models with different feature sets. We picked the MRF using phrase features only (MRFP) for comparison since we are mainly interested in phrase representation. 706 same expected BLEU based objective function, CPTM and MRFp model the translation relationship between two phrases from different angles. MRFp estimates one translation score for each phrase pair explicitly without parameter sharing,"
P14-1066,D12-1110,0,0.0585792,"Missing"
P14-1066,N12-1005,0,0.34284,"vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its transla"
P14-1066,W06-3105,0,\N,Missing
P14-1066,D13-1106,0,\N,Missing
P14-2105,D13-1160,0,0.0869,"graphy-related questions, learned using inductive logic programming (Zelle and Mooney, 1996). Research in this line originally used small, domain-specific databases, such as GeoQuery (Tang and Mooney, 2001; Liang et 643 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 643–648, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics al., 2013). Very recently, researchers have started developing semantic parsers for large, generaldomain knowledge bases like Freebase and DBpedia (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). Despite significant progress, the problem remains challenging. Most methods have not yet been scaled to large KBs that can support general open-domain QA. In contrast, Fader et al. (2013) proposed the PARALEX system, which targets answering single-relation questions using an automatically created knowledge base, ReVerb (Fader et al., 2011). By applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers, they successfully demonstrated a high-quality lexicon of patternmatching rules can be learned for this r"
P14-2105,P13-1042,0,0.0635643,"ser for answering geography-related questions, learned using inductive logic programming (Zelle and Mooney, 1996). Research in this line originally used small, domain-specific databases, such as GeoQuery (Tang and Mooney, 2001; Liang et 643 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 643–648, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics al., 2013). Very recently, researchers have started developing semantic parsers for large, generaldomain knowledge bases like Freebase and DBpedia (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). Despite significant progress, the problem remains challenging. Most methods have not yet been scaled to large KBs that can support general open-domain QA. In contrast, Fader et al. (2013) proposed the PARALEX system, which targets answering single-relation questions using an automatically created knowledge base, ReVerb (Fader et al., 2011). By applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers, they successfully demonstrated a high-quality lexicon of patternmatching rules can"
P14-2105,D11-1142,0,0.0744022,"e 23-25 2014. 2014 Association for Computational Linguistics al., 2013). Very recently, researchers have started developing semantic parsers for large, generaldomain knowledge bases like Freebase and DBpedia (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). Despite significant progress, the problem remains challenging. Most methods have not yet been scaled to large KBs that can support general open-domain QA. In contrast, Fader et al. (2013) proposed the PARALEX system, which targets answering single-relation questions using an automatically created knowledge base, ReVerb (Fader et al., 2011). By applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers, they successfully demonstrated a high-quality lexicon of patternmatching rules can be learned for this restricted form of semantic parsing. The other line of work related to our approach is continuous representations for semantic similarity, which has a long history and is still an active research topic. In information retrieval, TF-IDF vectors (Salton and McGill, 1983), latent semantic analysis (Deerwester et al., 1990) and topic models (Blei et al., 2003) take the bag"
P14-2105,P13-1158,0,0.789814,"rks. Leveraging the question paraphrase data mined from the WikiAnswers corpus by Fader et al. (2013), we train two semantic similarity models: one links a mention from the question to an entity in the KB and the other maps a relation pattern to a relation. The answer to the question can thus be derived by finding the relation–entity triple r(e1 , e2 ) in the KB and returning the entity not mentioned in the question. By using a general semantic similarity model to match patterns and relations, as well as mentions and entities, our system outperforms the existing rule learning system, PARALEX (Fader et al., 2013), with higher precision at all the recall points when answering the questions in the same test set. The highest achievable F1 score of our system is 0.61, versus 0.54 of PARALEX. The rest of the paper is structured as follows. We first survey related work in Sec. 2, followed by the problem definition and the high-level description of our approach in Sec. 3. Sec. 4 details our semantic models and Sec. 5 shows the experimental results. Finally, Sec. 6 concludes the paper. We develop a semantic parsing framework based on semantic similarity for open domain question answering (QA). We focus on sin"
P14-2105,D13-1161,0,0.0181989,"ons, learned using inductive logic programming (Zelle and Mooney, 1996). Research in this line originally used small, domain-specific databases, such as GeoQuery (Tang and Mooney, 2001; Liang et 643 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 643–648, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics al., 2013). Very recently, researchers have started developing semantic parsers for large, generaldomain knowledge bases like Freebase and DBpedia (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). Despite significant progress, the problem remains challenging. Most methods have not yet been scaled to large KBs that can support general open-domain QA. In contrast, Fader et al. (2013) proposed the PARALEX system, which targets answering single-relation questions using an automatically created knowledge base, ReVerb (Fader et al., 2011). By applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers, they successfully demonstrated a high-quality lexicon of patternmatching rules can be learned for this restricted form of semantic"
P14-2105,J13-2005,0,0.0208691,"Missing"
P14-2105,P11-1060,0,\N,Missing
P15-1128,P14-1091,0,0.396713,"gely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (Kwiatkowski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The representation power of the semantic parse is thus controlled by t"
P15-1128,D14-1002,1,0.473482,"etworks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine. This continuous-space representation approach has been proposed recently for semantic parsing and question answering (Bordes et al., 2014a; Yih et al., 2014) and has shown better results compared to lexical matching approaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; Gao et al., 2014) to this matching problem. The network architecture is illustrated in Fig. 6. The CNN model first applies a word hashing technique (Huang et al., 2013) that breaks a word into a vector of letter-trigrams (xt → ft in Fig. 6). For example, the bag of letter-trigrams of the word “who” are #-w-h, w-h-o, h-o-# after adding the actor x Meg Griffin Convolution matrix: Wc Word hashing layer: ft x y x Figure 7: Extending an inferential chain with constraints and aggregation functions. word boundary symbol #. Then, it uses a convolutional layer to project the letter-trigram vectors of words within a con"
P15-1128,P14-1133,0,0.16292,"wever, most traditional approaches for semantic parsing are largely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (Kwiatkowski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The represent"
P15-1128,D13-1160,0,0.888293,"et al., 2014), but with some key differences. The nodes and edges in our query graph closely resemble the exact entities and predicates from the knowledge base. As a result, the graph can be straightforwardly translated to a logical form query that is directly executable. In contrast, the query graph in (Reddy et al., 2014) is mapped from the CCG parse of the question, and needs further transformations before mapping to subgraphs 2 y should be grounded to a CVT entity in this case. of the target knowledge base to retrieve answers. Semantically, our query graph is more related to simple λ-DCS (Berant et al., 2013; Liang, 2013), which is a syntactic simplification of λ-calculus when applied to graph databases. A query graph can be viewed as the tree-like graph pattern of a logical form in λ-DCS. For instance, the path from the answer node to an entity node can be described using a series of join operations in λ-DCS. Different paths of the tree graph are combined via the intersection operators. 3 Staged Query Graph Generation We focus on generating query graphs with the following properties. First, the tree graph consists of one entity node as the root, referred as the topic entity. Second, there exists"
P15-1128,D13-1161,0,0.0148159,"eved simply by executing the query. The semantic parse also provides a deeper understanding of the question, which can be used to justify the answer to users, as well as to provide easily interpretable information to developers for error analysis. However, most traditional approaches for semantic parsing are largely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (Kwiatkowski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Sem"
P15-1128,J13-2005,0,0.417175,"ski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The representation power of the semantic parse is thus controlled by the set of legitimate actions applicable to each state. In particular, we stage the actions into three main steps: locating the topic entity in the question, finding the main relationship between the answer and the topic entity, and expanding the query graph with additional constraints that"
P15-1128,D14-1067,0,0.894208,"ce, one of our constructions maps the question to a pattern by replacing the entity mention with a generic symbol <e&gt; and then compares it with a candidate chain, such as “who first voiced meg on <e&gt;” vs. cast-actor. The model consists of two neural networks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine. This continuous-space representation approach has been proposed recently for semantic parsing and question answering (Bordes et al., 2014a; Yih et al., 2014) and has shown better results compared to lexical matching approaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; Gao et al., 2014) to this matching problem. The network architecture is illustrated in Fig. 6. The CNN model first applies a word hashing technique (Huang et al., 2013) that breaks a word into a vector of letter-trigrams (xt → ft in Fig. 6). For example, the bag of letter-trigrams of the word “who” are #-w-h, w-h-o, h-o-# after adding the actor x Meg Griffin Convolu"
P15-1128,P13-1042,0,0.321803,"Missing"
P15-1128,Q14-1030,0,0.0642215,"to map entities retrieved by the query. The diamond node arg min constrains that the answer needs to be the earliest actor for this role. Equivalently, the logical form query in λ-calculus without the aggregation function is: λx.∃y.cast(FamilyGuy, y) ∧ actor(y, x) ∧ character(y, MegGriffin) Running this query graph against K as in Fig. 1 will match both LaceyChabert and MilaKunis before applying the aggregation function, but only LaceyChabert is the correct answer as she started this role earlier (by checking the from property of the grounded CVT node). Our query graph design is inspired by (Reddy et al., 2014), but with some key differences. The nodes and edges in our query graph closely resemble the exact entities and predicates from the knowledge base. As a result, the graph can be straightforwardly translated to a logical form query that is directly executable. In contrast, the query graph in (Reddy et al., 2014) is mapped from the CCG parse of the question, and needs further transformations before mapping to subgraphs 2 y should be grounded to a CVT entity in this case. of the target knowledge base to retrieve answers. Semantically, our query graph is more related to simple λ-DCS (Berant et al."
P15-1128,P15-1049,1,0.237944,"iority queue, which is formally defined in Appendix A. In the following subsections, we use a running example of finding the semantic parse of question qex = “Who first voiced Meg of Family Guy?” to describe the sequence of actions. 3.1 Family Guy Linking Topic Entity Starting from the initial state s0 , the valid actions are to create a single-node graph that corresponds to the topic entity found in the given question. For instance, possible topic entities in qex can either be FamilyGuy or MegGriffin, shown in Fig. 4. We use an entity linking system that is designed for short and noisy text (Yang and Chang, 2015). For each entity e in the knowledge base, the system first prepares a surface-form lexicon that lists all possible ways that e can be mentioned in text. This lexicon is created using various data sources, such as names and aliases of the entities, the anchor text in Web documents and the Wikipedia redirect table. Given a question, it considers all the y Family Guy cast Family Guy writer y Family Guy genre x Family Guy actor start x x Figure 5: Candidate core inferential chains start from the entity FamilyGuy. consecutive word sequences that have occurred in the lexicon as possible mentions, p"
P15-1128,D14-1071,0,0.379928,"t nodes indicating that certain conditions cannot be satisfied. Our graph generation method is inspired by (Yao and Van Durme, 2014; Bao et al., 2014). Unlike traditional semantic parsing approaches, it uses the knowledge base to help prune the search space when forming the parse. Similar ideas have also been explored in (Poon, 2013). Empirically, our results suggest that it is crucial to identify the core inferential chain, which matches the relationship between the topic entity in the question and the answer. Our CNN models can be analogous to the embedding approaches (Bordes et al., 2014a; Yang et al., 2014), but are more sophisticated. By allowing parameter sharing among different question-pattern and KB predicate pairs, the matching score of a rare or even unseen pair in the training data can still be predicted precisely. This is due to the fact that the prediction is based on the shared model parameters (i.e., projection matrices) that are estimated using all training pairs. 6 Conclusion In this paper, we present a semantic parsing framework for question answering using a knowledge base. We define a query graph as the meaning representation that can be directly mapped to a logical form. Semant"
P15-1128,P14-1090,0,0.722517,"Missing"
P15-1128,P14-2105,1,0.725471,"ctions maps the question to a pattern by replacing the entity mention with a generic symbol <e&gt; and then compares it with a candidate chain, such as “who first voiced meg on <e&gt;” vs. cast-actor. The model consists of two neural networks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine. This continuous-space representation approach has been proposed recently for semantic parsing and question answering (Bordes et al., 2014a; Yih et al., 2014) and has shown better results compared to lexical matching approaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; Gao et al., 2014) to this matching problem. The network architecture is illustrated in Fig. 6. The CNN model first applies a word hashing technique (Huang et al., 2013) that breaks a word into a vector of letter-trigrams (xt → ft in Fig. 6). For example, the bag of letter-trigrams of the word “who” are #-w-h, w-h-o, h-o-# after adding the actor x Meg Griffin Convolution matrix: Wc Word"
P15-1128,P13-1092,0,\N,Missing
P15-2017,D13-1106,1,0.509649,"et al., 2002) and METEOR (Denkowski and Lavie, 2014). BLEU roughly measures the fraction of N -grams (up to 4 grams) that are in common between a hypothesis and one or more references, and penalizes short hypotheses by a brevity penalty term.7 METEOR (Denkowski and Lavie, 2014) measures unigram precision and recall, extending exact word matches to include similar words based on WordNet synonyms and stemmed tokens. We also report the perplexity (PPLX) of studied detectionconditioned LMs. The PPLX is in many ways the natural measure of a statistical LM, but can be loosely correlated with BLEU (Auli et al., 2013). 3.3 Model Comparison In Table 1, we summarize the generation performance of our different models. The discrete detection based models are prefixed with “D”. Some example generated results are show in Table 2. We see that the detection-conditioned LSTM LM produces much lower PPLX than the detection-conditioned ME LM, but its BLEU score is no better. The MRNN has the lowest PPLX, and highest BLEU among all LMs studThe Microsoft COCO Dataset We work with the Microsoft COCO dataset (Lin et al., 2014), with 82,783 training images, and the validation set split into 20,243 validation images and 20,"
P15-2017,E06-1032,0,0.0151544,"maintains a local context window. In contrast, the MRNN approach tends to generate such anaphoric relationships correctly. However, the D-ME LM maintains an explicit coverage state vector tracking which attributes have already been emitted. The MRNN implicitly maintains the full state using its recurrent layer, which sometimes results in multiple emission mistakes, where the same attribute is emitted more than once. This is particularly evident when coordination (“and”) is present (examples (4) and (5)). Human Evaluation Because automatic metrics do not always correlate with human judgments (Callison-Burch et al., 2006; Hodosh et al., 2013), we also performed human evaluations using the same procedure as in Fang et al. (2015). Here, human judges were presented with an image, a system generated caption, and a human generated caption, and were asked which caption was “better”.9 For each condition, 5 judgments were obtained for 1000 images from the testval set. 4.1 Repeated Captions All of our models produce a large number of captions seen in the training and repeated for different images in the test set, as shown in Table 6 (also observed by Vinyals et al. (2014) for their LSTM-based model). There are at leas"
P15-2017,W14-1602,0,0.0190758,"14). In this paper, we study the relative merits of these approaches. By using an identical state-ofthe-art CNN as the input to RNN-based and MEbased models, we are able to empirically compare the strengths and weaknesses of the language modeling components. We find that the approach of directly generating the text with an MRNN1 outperforms the ME LM when measured by BLEU on the COCO dataset (Lin et al., 2014),2 but this recurrent model tends to reproduce captions in the training set. In fact, a simple k-nearest neighbor approach, which is common in earlier related work (Farhadi et al., 2010; Mason and Charniak, 2014), performs similarly to the MRNN. In contrast, the ME LM generates the most novel captions, and does the best at captioning images for which there is no close match in the training data. With a Deep Multimodal Similarity Model (DMSM) incorporated,3 the ME LM significantly outperforms other methods according to human judgments. In sum, the contributions of this paper are as follows: Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on ima"
P15-2017,P03-1021,0,0.0582718,"Missing"
P15-2017,W14-3348,0,0.00627796,"sure incorrect information is not mentioned. Further details on retrieval-based methods are available in, e.g., (Ordonez et al., 2011; Hodosh et al., 2013). 3 D-ME+DMSM+MRNN k-NN a gray and white cat sitting on top of it a cat sitting in front of a mirror a close up of a cat looking at the camera a cat sitting on top of a wooden table annotated captions. The images create a challenging testbed for image captioning and are widely used in recent automatic image captioning work. 3.2 Metrics The quality of generated captions is measured automatically using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). BLEU roughly measures the fraction of N -grams (up to 4 grams) that are in common between a hypothesis and one or more references, and penalizes short hypotheses by a brevity penalty term.7 METEOR (Denkowski and Lavie, 2014) measures unigram precision and recall, extending exact word matches to include similar words based on WordNet synonyms and stemmed tokens. We also report the perplexity (PPLX) of studied detectionconditioned LMs. The PPLX is in many ways the natural measure of a statistical LM, but can be loosely correlated with BLEU (Auli et al., 2013). 3.3 Model Comparison In Table 1,"
P15-2017,P02-1040,0,0.116646,"thod for captioning, as it helps ensure incorrect information is not mentioned. Further details on retrieval-based methods are available in, e.g., (Ordonez et al., 2011; Hodosh et al., 2013). 3 D-ME+DMSM+MRNN k-NN a gray and white cat sitting on top of it a cat sitting in front of a mirror a close up of a cat looking at the camera a cat sitting on top of a wooden table annotated captions. The images create a challenging testbed for image captioning and are widely used in recent automatic image captioning work. 3.2 Metrics The quality of generated captions is measured automatically using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). BLEU roughly measures the fraction of N -grams (up to 4 grams) that are in common between a hypothesis and one or more references, and penalizes short hypotheses by a brevity penalty term.7 METEOR (Denkowski and Lavie, 2014) measures unigram precision and recall, extending exact word matches to include similar words based on WordNet synonyms and stemmed tokens. We also report the perplexity (PPLX) of studied detectionconditioned LMs. The PPLX is in many ways the natural measure of a statistical LM, but can be loosely correlated with BLEU (Auli et al., 2"
P15-2017,D14-1179,0,\N,Missing
P16-1153,P09-1010,0,0.0211587,"There has been increasing interest in applying deep reinforcement learning to a variety problems, but only a few studies address problems with natural language state or action spaces. In language processing, reinforcement learning has been applied to a dialogue management system that converses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al., 2013). There has also been interest in extracting textual knowledge to improve game control performance (Branavan et al., 2011), and mapping text instructions to sequences of executable actions (Branavan et al., 2009). In some applications, it is possible to manually design features for state-action pairs, which are then used in reinforcement learning to learn a near-optimal policy (Li et al., 2009). Designing such features, however, require substantial domain knowledge. The work most closely related to our study inolves application of deep reinforcement to learning decision policies for parser-based text games. Narasimhan et al. (2015) applied a Long ShortTerm Memory DQN framework, which achieves higher average reward than the random and Bagof-Words DQN baselines. In this work, actions are constrained to"
P16-1153,P11-1028,0,0.0158818,"N can generalize well to “unseen” natural language descriptions of actions. 4 Related Work There has been increasing interest in applying deep reinforcement learning to a variety problems, but only a few studies address problems with natural language state or action spaces. In language processing, reinforcement learning has been applied to a dialogue management system that converses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al., 2013). There has also been interest in extracting textual knowledge to improve game control performance (Branavan et al., 2011), and mapping text instructions to sequences of executable actions (Branavan et al., 2009). In some applications, it is possible to manually design features for state-action pairs, which are then used in reinforcement learning to learn a near-optimal policy (Li et al., 2009). Designing such features, however, require substantial domain knowledge. The work most closely related to our study inolves application of deep reinforcement to learning decision policies for parser-based text games. Narasimhan et al. (2015) applied a Long ShortTerm Memory DQN framework, which achieves higher average rewar"
P16-1153,D15-1001,0,0.52424,"Missing"
P16-1153,D15-1166,0,0.00887504,"Missing"
P16-1170,N12-1092,1,0.637005,"captures, however, the majority of the questions in Visual7w are designed to be answerable by only the image, making them unnatural for asking a human. Thus, learning to generate the questions in VQA task is not a useful sub-task, as the intersection between VQG and any VQA questions is by definition minimal. Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.e., “what to ask about”. For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank questions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content. To our knowledge, neither a retrieval model nor a deep representation of textual input, presented in our work, have yet been used to generate questions. Data Collection Methodology Task Definition: Given an image, the task is to generate a natural question which can potentially engage a human in starting a con"
P16-1170,N15-1053,0,0.0320703,"a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics. Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language. 1 Generated Caption: - A man standing next to a motorcycle. Figure 1: Example image along with its natural questions and automatically generated caption. Introduction We are witnessing a renewed interest in interdisciplinary AI research in vision & language, from descriptions of the visual input such as image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) and video transcription (Rohrbach et al., 2012; Venugopalan et al., 2015), to testing computer understanding of an image through question answering (Antol et al., 2015; Malinowski and Fritz, 2014). The most established work in the vision & language community is ‘image captioning’, where the task is to produce a literal description of the image. It has been shown (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) that a reasonable language modeling paired with deep visual features trained on large enough datasets promise"
P16-1170,W14-3348,0,0.0246802,"S COCO dataset. Human evaluation results of a recent work (Tran et al., 2016) further confirms the significant image captioning quality degradation on out-of-domain data. To further explore this difference, we crowdsourced 5 captions for each image in the V QGBing−5000 dataset using the same prompt as used to source the MS COCO captions. We call this new dataset CaptionsBing−5000 . Table 3 shows the results of testing the state-of-the-art MSR captioning system on the CaptionsBing−5000 dataset as compared to the MS COCO dataset, measured by the standard BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. The wide gap in the results further confirms that indeed the V QGBing−5000 dataset covers a new class of images; we hope the availability of this new dataset will encourage including more diverse domains for image captioning. Bing 0.101 BLEU M S COCO 0.291 Bing 0.151 METEOR M S COCO 0.247 Table 3: Image captioning results Together with this paper we are releasing an extended set of VQG dataset to the community. We hope that the availability of this dataset will encourage the research community to tackle more end-goal oriented vision & language tasks. 4 Models In this Section we prese"
P16-1170,P15-2017,1,0.165044,"e are witnessing a renewed interest in interdisciplinary AI research in vision & language, from descriptions of the visual input such as image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) and video transcription (Rohrbach et al., 2012; Venugopalan et al., 2015), to testing computer understanding of an image through question answering (Antol et al., 2015; Malinowski and Fritz, 2014). The most established work in the vision & language community is ‘image captioning’, where the task is to produce a literal description of the image. It has been shown (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) that a reasonable language modeling paired with deep visual features trained on large enough datasets promise a good performance on image captioning, making it a less challenging task from language learning perspective. Furthermore, although this task has a great value for communities of people who are low-sighted or cannot see in all or some environments, for others, the description does not add anything to what a person has already perceived. The popularity of the image sharing applications in social media and user engagement around images is eviden"
P16-1170,D15-1021,1,0.579546,"s naturally contain objects less frequently. Hence, we see that VQG questions include the mention of more of those literal objects. Figure 3(b) shows that COCO captions have a larger vocabulary size, which reflects their longer and more descriptive sentences. VQG shows a relatively large vocabulary size as well, indicating greater diversity in question formulation than VQA and CQA. Moreover, Figure 3(c) shows that the verb part of speech is represented with high frequency in our dataset. Figure 3(d) depicts the percentage of abstract terms such as ‘think’ or ‘win’ in the vocabulary. Following Ferraro et al. (2015), we use a list of most common abstract terms in English (Vanderwende et al., 2015), and count all the other words except a set of function words as concrete. This figure supports our expectation that VQG covers more abstract concepts. Furthermore, Figure 3(e) shows inter-annotation textual similarity according to the BLEU metric (Papineni et al., 2002). Interestingly, VQG shows the highest interannotator textual similarity, which reflects on the existence of consensus among human for asking Figure 3: Comparison of various annotations on the MS COCO dataset. (a) Percentage of gold objects used"
P16-1170,P15-2073,1,0.456361,"n on AMT, asking three crowd workers to each rate the quality of candidate questions on a three-point semantic scale. 5.2 Automatic Evaluation The goal of automatic evaluation is to measure the similarity of system-generated question hypotheses and the crowdsourced question references. To capture n-gram overlap and textual similarity between hypotheses and references, we use standard Machine Translation metrics, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). We use BLEU with equal weights up to 4-grams and default setting of METEOR version 1.5. Additionally we use ∆BLEU (Galley et al., 2015) which is specifically tailored towards generation tasks with diverse references, such as conversations. ∆BLEU requires rating per reference, distinguishing between the quality of the references. For this purpose, we crowd-sourced three human ratings (on a scale of 1-3) per reference and used the majority rating. The pairwise correlational analysis of human and automatic metrics is presented in Table 6, where we report on Pearson’s r, Spearman’s ρ and Kendall’s τ correlation coefficients. As this table reveals, ∆BLEU strongly correlates with human judgment and we suggest it as the main evaluat"
P16-1170,N10-1086,0,0.624,"ommonsense reasoning, going beyond spatial reasoning required for ‘which’ or ‘who’ questions. This is more in line with the type of questions that VQG captures, however, the majority of the questions in Visual7w are designed to be answerable by only the image, making them unnatural for asking a human. Thus, learning to generate the questions in VQA task is not a useful sub-task, as the intersection between VQG and any VQA questions is by definition minimal. Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.e., “what to ask about”. For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank questions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content. To our knowledge, neither a retrieval model nor a deep representation of textual input, presented in our work, have yet been used to generate questions. Data Collec"
P16-1170,P15-1086,1,0.454133,"ng to generate the questions in VQA task is not a useful sub-task, as the intersection between VQG and any VQA questions is by definition minimal. Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.e., “what to ask about”. For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank questions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content. To our knowledge, neither a retrieval model nor a deep representation of textual input, presented in our work, have yet been used to generate questions. Data Collection Methodology Task Definition: Given an image, the task is to generate a natural question which can potentially engage a human in starting a conversation. Questions that are visually verifiable, i.e., that can be answered by looking at only the image, are outside the scope of this task. For instance, in Figu"
P16-1170,P16-1094,0,0.00667955,"question can naturally start a conversation. To continue progress on this task, it is possible to increase the size of the training data, but we also expect to develop models that will learn to generalize to unseen concepts. For instance, consider the examples of system errors in Table 7, where visual features can be enough for detecting the specific set of objects in each image, but the system cannot make sense of the combination of previously unseen concepts. Another natural future extension of this work is to include question generation within a conversational system (Sordoni et al., 2015; Li et al., 2016), where the context and conversation history affect the types of questions being asked. Human BLEU 0.915 (4.6e-27) 0.67 (7.0e-10) 0.51 (7.9e-10) - How long did it take to make that ice sculpture? GRNN METEOR 0.916 (4.8e-27) 0.628 (1.5e-08) 0.476 (1.6e-08) - How long has he been hiking? - Is this in a hotel room? KNN r ρ τ - How deep was the snow? - Do you enjoy the light in this bathroom? - Is the dog looking to take a shower? Table 7: Examples of errors in generation. The rows are Humanconsensus , GRNNall , and KNN+minbleu−all . Acknowledgment We would like to thank the anonymous reviewers fo"
P16-1170,P04-1077,0,0.0126356,"Missing"
P16-1170,W13-2114,0,0.0187415,"e by only the image, making them unnatural for asking a human. Thus, learning to generate the questions in VQA task is not a useful sub-task, as the intersection between VQG and any VQA questions is by definition minimal. Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.e., “what to ask about”. For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank questions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content. To our knowledge, neither a retrieval model nor a deep representation of textual input, presented in our work, have yet been used to generate questions. Data Collection Methodology Task Definition: Given an image, the task is to generate a natural question which can potentially engage a human in starting a conversation. Questions that are visually verifiable, i.e., that can be answered by looking at"
P16-1170,W03-0203,0,0.279042,"require high-level commonsense reasoning, going beyond spatial reasoning required for ‘which’ or ‘who’ questions. This is more in line with the type of questions that VQG captures, however, the majority of the questions in Visual7w are designed to be answerable by only the image, making them unnatural for asking a human. Thus, learning to generate the questions in VQA task is not a useful sub-task, as the intersection between VQG and any VQA questions is by definition minimal. Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.e., “what to ask about”. For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank questions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content. To our knowledge, neither a retrieval model nor a deep representation of textual input, presented in our work, have yet been used to gener"
P16-1170,P02-1040,0,0.11831,"formulation than VQA and CQA. Moreover, Figure 3(c) shows that the verb part of speech is represented with high frequency in our dataset. Figure 3(d) depicts the percentage of abstract terms such as ‘think’ or ‘win’ in the vocabulary. Following Ferraro et al. (2015), we use a list of most common abstract terms in English (Vanderwende et al., 2015), and count all the other words except a set of function words as concrete. This figure supports our expectation that VQG covers more abstract concepts. Furthermore, Figure 3(e) shows inter-annotation textual similarity according to the BLEU metric (Papineni et al., 2002). Interestingly, VQG shows the highest interannotator textual similarity, which reflects on the existence of consensus among human for asking Figure 3: Comparison of various annotations on the MS COCO dataset. (a) Percentage of gold objects used in annotations. (b) Vocabulary size (c) Percentage of verb POS (d) Percentage of abstract terms (e) Inter-annotation textual similarity score. The MS COCO dataset is limited in terms of the concepts it covers, due to its pre-specified set of object categories. Word frequency in V QGcoco−5000 dataset, as demonstrated in Figure 4, bears this out, with th"
P16-1170,P14-2053,0,0.0135128,"7w are designed to be answerable by only the image, making them unnatural for asking a human. Thus, learning to generate the questions in VQA task is not a useful sub-task, as the intersection between VQG and any VQA questions is by definition minimal. Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.e., “what to ask about”. For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank questions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content. To our knowledge, neither a retrieval model nor a deep representation of textual input, presented in our work, have yet been used to generate questions. Data Collection Methodology Task Definition: Given an image, the task is to generate a natural question which can potentially engage a human in starting a conversation. Questions that are visually verifiable, i.e., that c"
P16-1170,N15-1020,1,0.0955277,"Missing"
P16-1170,N15-3006,1,0.341314,"nclude the mention of more of those literal objects. Figure 3(b) shows that COCO captions have a larger vocabulary size, which reflects their longer and more descriptive sentences. VQG shows a relatively large vocabulary size as well, indicating greater diversity in question formulation than VQA and CQA. Moreover, Figure 3(c) shows that the verb part of speech is represented with high frequency in our dataset. Figure 3(d) depicts the percentage of abstract terms such as ‘think’ or ‘win’ in the vocabulary. Following Ferraro et al. (2015), we use a list of most common abstract terms in English (Vanderwende et al., 2015), and count all the other words except a set of function words as concrete. This figure supports our expectation that VQG covers more abstract concepts. Furthermore, Figure 3(e) shows inter-annotation textual similarity according to the BLEU metric (Papineni et al., 2002). Interestingly, VQG shows the highest interannotator textual similarity, which reflects on the existence of consensus among human for asking Figure 3: Comparison of various annotations on the MS COCO dataset. (a) Percentage of gold objects used in annotations. (b) Vocabulary size (c) Percentage of verb POS (d) Percentage of a"
P16-1170,N15-1173,0,0.0354635,"Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language. 1 Generated Caption: - A man standing next to a motorcycle. Figure 1: Example image along with its natural questions and automatically generated caption. Introduction We are witnessing a renewed interest in interdisciplinary AI research in vision & language, from descriptions of the visual input such as image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) and video transcription (Rohrbach et al., 2012; Venugopalan et al., 2015), to testing computer understanding of an image through question answering (Antol et al., 2015; Malinowski and Fritz, 2014). The most established work in the vision & language community is ‘image captioning’, where the task is to produce a literal description of the image. It has been shown (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) that a reasonable language modeling paired with deep visual features trained on large enough datasets promise a good performance on image captioning, making it a less challenging task from language learning perspective. Furthermore, although thi"
P18-5007,D17-1106,0,0.0203824,", Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (Wang et al., 2018b),"
P18-5007,N18-1113,1,0.878952,"Missing"
P18-5007,D17-1060,1,0.831141,"2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (Wang et al., 2018b), discussing the techniques of leveraging hierarchies in DRL for NLP generation problems. This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in reinforcement learning. The intended length of the tutorial is 3 hours, including a coffee break. Many Natural Language Processing (NLP) tasks (including generation, language grounding, reasoning, information extraction, coreferenc"
P18-5007,D15-1001,0,0.0312245,"forcement Learning for NLP William Yang Wang UC Santa Barbara william@cs.ucsb.edu Jiwei Li Shannon.ai jiwei li@shannonai.com Abstract their modern deep learning extensions such as Deep QNetworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we d"
P18-5007,P18-1053,1,0.885681,"Missing"
P18-5007,D16-1261,0,0.0216518,"om Abstract their modern deep learning extensions such as Deep QNetworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study"
P18-5007,P18-1104,1,0.822012,"b.edu Jiwei Li Shannon.ai jiwei li@shannonai.com Abstract their modern deep learning extensions such as Deep QNetworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong"
P18-5007,D17-1103,0,0.0282946,"etworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study of hierarchical deep reinforcement learning for video captioning ("
P18-5007,P18-1199,1,0.823639,"deep learning extensions such as Deep QNetworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study of hierarchical de"
P19-1260,D17-1209,0,0.0222336,"a sets (Sun et al., 2018; Wu et al., 2019). However, these data sets require multi-hop reasoning over multiple sentences or multiple common knowledge while the problem 1 By May 30th 2019, http://qangaroo.cs.ucl. ac.uk/leaderboard.html 2705 Final scores we want to solve in this paper requires collecting evidences across multiple documents. GNN for NLP: Recently, there is considerable amount of interest in applying GNN to NLP tasks and great success has been achieved. For example, in neural machine translation, GNN has been employed to integrate syntactic and semantic information into encoders (Bastings et al., 2017; Marcheggiani et al., 2018); Zhang et al. (2018) applied GNN to relation extraction over pruned dependency trees; the study by Yao et al. (2018) employed GNN over a heterogeneous graph to do text classification, which inspires our idea of the HDE graph; Liu et al. (2018) proposed a new contextualized neural network for sequence learning by leveraging various types of non-local contextual information in the form of information passing over GNN. These studies are related to our work in the sense that we both use GNN to improve the information interaction over long context or across documents. 3"
P19-1260,D14-1179,0,0.0273333,"Missing"
P19-1260,N19-1240,0,0.276926,"Missing"
P19-1260,N18-2007,0,0.0324694,"t is not enough to find the correct answer. To promote the study for multi-hop RC over multiple documents, two data sets are recently proposed: W IKI H OP (Welbl et al., 2018) and HotpotQA (Yang et al., 2018). These two data sets require multi-hop reasoning over multiple supporting documents to find the answer. In Figure 1, we show an excerpt from one sample in W IKI H OP development set to illustrate the need for multi-hop reasoning. Two types of approaches have been proposed on the multi-hop multi-document RC problem. The first is based on previous neural RC models. The earliest attempt in (Dhingra et al., 2018) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener2704 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2704–2713 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ated coreference annotations. Adding this layer to the neural RC models improved performance on multi-hop tasks. Recently, an attention based system (Zhong et al., 2019) utilizing both documentlevel and entity-level information achieved stateo"
P19-1260,D17-1206,0,0.032038,"-GCN(De Cao et al., 2018) DynSAN* Proposed 68.5 70.9 Single models Model Full model - HDE graph - different edge types - candidate nodes scores - entity nodes scores - candidate nodes - document nodes - entity nodes 71.2 73.8 74.3 Table 2: Ablation results on the W IKI H OP dev set. Table 1: Performance comparison among different models on W IKI H OP development and test set. The results of “BiDAF” are presented in the paper by Welbl et al. (2018). Models annotated with “*” are unpublished but available on W IKI H OP leaderboard. “-” indicates unavailable numbers. character n-gram embeddings (Hashimoto et al., 2017) are used to convert words into 400dimensional vector representations. Out of vocabulary words are initialized with random vectors. The embedding matrices are not updated during training. The proposed model is implemented with PyTorch (Paszke et al., 2017). More details about experimental and hyperparameter settings can be found in supplementary materials. The performance on development set is measured after each training epoch, and the model with the highest accuracy is saved and submitted to be evaluated on the blind test set. We will make our code publicly available after the review process"
P19-1260,P16-1145,0,0.0240712,"tput dimension is 1. We directly sum the scores from candidate nodes and entity nodes as the final scores over multiple candidates. Thus, the output score vector a ∈ RC×1 gives a distribution over all candidates. Since the task is multi-class classification, we use cross-entropy loss as training objective which takes a and the labels as input. 4 4.1 Experiments Dataset We use W IKI H OP (Welbl et al., 2018) to validate the effectiveness of our proposed model. The query of W IKI H OP is constructed with entities and relations from W IKI DATA, while supporting documents are from W IKI R EADING (Hewlett et al., 2016). A bipartite graph connecting entities and documents is first built and the answer for each query is located by traversal on this graph. Candidates that are type-consistent with the answer and share the same relation in query with the answer are included, resulting in a set of candidates. Thus, W IKI H OP is a multi-choice style reading comprehension data set. There are totally about 43K samples in training set, 5K samples in development set and 2.5K samples in test set. The test set is not provided and can only be evaluated on blindly. The task is to predict the correct answer given a query"
P19-1260,N18-1023,0,0.0313722,"how the effectiveness of attention mechanisms. Our model is very different from the other two studies (Dhingra et al., 2018; Kundu et al., 2018): these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs. Besides these studies, our work is also related to the following research directions. Multi-hop RC: There exist several different data sets that require reasoning in multiple steps in literature, for example bAbI (Weston et al., 2015), MultiRC (Khashabi et al., 2018) and OpenBookQA (Mihaylov et al., 2018). A lot of systems have been proposed to solve the multi-hop RC problem with these data sets (Sun et al., 2018; Wu et al., 2019). However, these data sets require multi-hop reasoning over multiple sentences or multiple common knowledge while the problem 1 By May 30th 2019, http://qangaroo.cs.ucl. ac.uk/leaderboard.html 2705 Final scores we want to solve in this paper requires collecting evidences across multiple documents. GNN for NLP: Recently, there is considerable amount of interest in applying GNN to NLP tasks and great success has been achieved. For"
P19-1260,Q18-1023,0,0.0477443,"Missing"
P19-1260,W02-0109,0,0.394884,"Missing"
P19-1260,N18-2078,0,0.0298225,"8; Wu et al., 2019). However, these data sets require multi-hop reasoning over multiple sentences or multiple common knowledge while the problem 1 By May 30th 2019, http://qangaroo.cs.ucl. ac.uk/leaderboard.html 2705 Final scores we want to solve in this paper requires collecting evidences across multiple documents. GNN for NLP: Recently, there is considerable amount of interest in applying GNN to NLP tasks and great success has been achieved. For example, in neural machine translation, GNN has been employed to integrate syntactic and semantic information into encoders (Bastings et al., 2017; Marcheggiani et al., 2018); Zhang et al. (2018) applied GNN to relation extraction over pruned dependency trees; the study by Yao et al. (2018) employed GNN over a heterogeneous graph to do text classification, which inspires our idea of the HDE graph; Liu et al. (2018) proposed a new contextualized neural network for sequence learning by leveraging various types of non-local contextual information in the form of information passing over GNN. These studies are related to our work in the sense that we both use GNN to improve the information interaction over long context or across documents. 3 Cand score 1 Context encodi"
P19-1260,D18-1260,0,0.0128476,"anisms. Our model is very different from the other two studies (Dhingra et al., 2018; Kundu et al., 2018): these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs. Besides these studies, our work is also related to the following research directions. Multi-hop RC: There exist several different data sets that require reasoning in multiple steps in literature, for example bAbI (Weston et al., 2015), MultiRC (Khashabi et al., 2018) and OpenBookQA (Mihaylov et al., 2018). A lot of systems have been proposed to solve the multi-hop RC problem with these data sets (Sun et al., 2018; Wu et al., 2019). However, these data sets require multi-hop reasoning over multiple sentences or multiple common knowledge while the problem 1 By May 30th 2019, http://qangaroo.cs.ucl. ac.uk/leaderboard.html 2705 Final scores we want to solve in this paper requires collecting evidences across multiple documents. GNN for NLP: Recently, there is considerable amount of interest in applying GNN to NLP tasks and great success has been achieved. For example, in neural machine translation,"
P19-1260,D14-1162,0,0.0843767,"xtual information in the form of information passing over GNN. These studies are related to our work in the sense that we both use GNN to improve the information interaction over long context or across documents. 3 Cand score 1 Context encoding Given a query q with the form of (s, r, ?) which represents subject, relation and unknown object respectively, a set of support documents Sq and a set of candidates Cq , the task is to predict the correct answer a∗ to the query. To encode information including in the text of query, candidates and support documents, we use a pretrained embedding matrix (Pennington et al., 2014) to convert word sequences to sequences of vectors. Let Xq ∈ j i Rlq ×d , Xis ∈ Rls ×d and Xjc ∈ Rlc ×d represent the embedding matrices of query, i-th supporting document and j-th candidate of a sample, where lq , lsi and lcj are the numbers of words in query, i-th supporting document and j-th candidate respectively. d is the dimension of the word embedding. We use bidirectional recurrent neural networks (RNN) Entity scores FC Cand nodes Self-attn FC Doc nodes Entity nodes Self-attn Self-attn coattn Methodology In this section, we describe different modules of the proposed Heterogeneous Docum"
P19-1260,N18-1202,0,0.10753,"Missing"
P19-1260,P18-2124,0,0.0525314,"Missing"
P19-1260,D16-1264,0,0.0530475,"ph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance. 1 Figure 1: A W IKI H OP example. Words with different colors indicate the evidences across documents. Introduction Being able to comprehend a document and output correct answer given a query/question about content in the document, often referred as machine reading comprehension (RC) or question answering (QA), is an important and challenging task in natural language processing (NLP). Plenty of data sets have been constructed to facilitate research on this topic, such as SQuAD (Rajpurkar et al., 2016, 2018), NarrativeQA (Koˇcisk`y et al., 2018) and CoQA (Reddy et al., 2018). Many neural models have been proposed to tackle the machine RC/QA problem (Seo et al., 2016; Xiong et al., 2016; Tay et al., 2018), and great success has been achieved, especially after the release of the BERT (Devlin et al., 2018). However, current research mainly focuses on machine RC/QA on a single document or paragraph, and still lacks the ability to do reasoning across multiple documents when a single document is not enough to find the correct answer. To promote the study for multi-hop RC over multiple documents,"
P19-1260,Q18-1021,0,0.106312,"CoQA (Reddy et al., 2018). Many neural models have been proposed to tackle the machine RC/QA problem (Seo et al., 2016; Xiong et al., 2016; Tay et al., 2018), and great success has been achieved, especially after the release of the BERT (Devlin et al., 2018). However, current research mainly focuses on machine RC/QA on a single document or paragraph, and still lacks the ability to do reasoning across multiple documents when a single document is not enough to find the correct answer. To promote the study for multi-hop RC over multiple documents, two data sets are recently proposed: W IKI H OP (Welbl et al., 2018) and HotpotQA (Yang et al., 2018). These two data sets require multi-hop reasoning over multiple supporting documents to find the answer. In Figure 1, we show an excerpt from one sample in W IKI H OP development set to illustrate the need for multi-hop reasoning. Two types of approaches have been proposed on the multi-hop multi-document RC problem. The first is based on previous neural RC models. The earliest attempt in (Dhingra et al., 2018) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gene"
P19-1260,D18-1259,0,0.0327061,"ural models have been proposed to tackle the machine RC/QA problem (Seo et al., 2016; Xiong et al., 2016; Tay et al., 2018), and great success has been achieved, especially after the release of the BERT (Devlin et al., 2018). However, current research mainly focuses on machine RC/QA on a single document or paragraph, and still lacks the ability to do reasoning across multiple documents when a single document is not enough to find the correct answer. To promote the study for multi-hop RC over multiple documents, two data sets are recently proposed: W IKI H OP (Welbl et al., 2018) and HotpotQA (Yang et al., 2018). These two data sets require multi-hop reasoning over multiple supporting documents to find the answer. In Figure 1, we show an excerpt from one sample in W IKI H OP development set to illustrate the need for multi-hop reasoning. Two types of approaches have been proposed on the multi-hop multi-document RC problem. The first is based on previous neural RC models. The earliest attempt in (Dhingra et al., 2018) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener2704 Proceedings of the 57th Ann"
P19-1260,D18-1244,0,0.0426428,", these data sets require multi-hop reasoning over multiple sentences or multiple common knowledge while the problem 1 By May 30th 2019, http://qangaroo.cs.ucl. ac.uk/leaderboard.html 2705 Final scores we want to solve in this paper requires collecting evidences across multiple documents. GNN for NLP: Recently, there is considerable amount of interest in applying GNN to NLP tasks and great success has been achieved. For example, in neural machine translation, GNN has been employed to integrate syntactic and semantic information into encoders (Bastings et al., 2017; Marcheggiani et al., 2018); Zhang et al. (2018) applied GNN to relation extraction over pruned dependency trees; the study by Yao et al. (2018) employed GNN over a heterogeneous graph to do text classification, which inspires our idea of the HDE graph; Liu et al. (2018) proposed a new contextualized neural network for sequence learning by leveraging various types of non-local contextual information in the form of information passing over GNN. These studies are related to our work in the sense that we both use GNN to improve the information interaction over long context or across documents. 3 Cand score 1 Context encoding Given a query q wi"
W07-0710,P02-1040,0,0.0733993,"aints on the combination, and hence relaxing constraints of monotonicity and independence of feature functions. We expand features into a non-parametric, non-linear, and high-dimensional space. We extend empirical Bayes reward training of model parameters to meta parameters of feature generation. In effect, this allows us to trade away some human expert feature design for data. Preliminary results on a standard task show an encouraging improvement. 2 Model 1 Introduction In recent years, statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation (Papineni et al., 2002) and errorbased optimization (Och, 2003). The conditional log-linear feature combination framework (Berger, Della Pietra and Della Pietra, 1996) is remarkably simple and effective in practice. Therefore, recent efforts (Och et al., 2004) have concentrated on feature design – wherein more intelligent features may be added. Because of their simplicity, however, log-linear models impose some constraints on how new information may be inserted into the system to achieve the best results. In other words, In this section, we describe the general log-linear model used for statistical machine translati"
W07-0710,J96-1002,0,0.0404411,"Missing"
W07-0710,N04-1021,0,0.1113,"el parameters to meta parameters of feature generation. In effect, this allows us to trade away some human expert feature design for data. Preliminary results on a standard task show an encouraging improvement. 2 Model 1 Introduction In recent years, statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation (Papineni et al., 2002) and errorbased optimization (Och, 2003). The conditional log-linear feature combination framework (Berger, Della Pietra and Della Pietra, 1996) is remarkably simple and effective in practice. Therefore, recent efforts (Och et al., 2004) have concentrated on feature design – wherein more intelligent features may be added. Because of their simplicity, however, log-linear models impose some constraints on how new information may be inserted into the system to achieve the best results. In other words, In this section, we describe the general log-linear model used for statistical machine translation, as well as a training objective function and algorithm. The goal is to translate a French (source) sentence indexed by t, with surface string ft . Among a set of Kt outcomes, we denote an English (target) a hy(t) pothesis with surfac"
W07-0710,P02-1038,0,0.204645,"e quickly review how to adjust λ to get better translation results. First, let us define the figure of merit used for evaluation of translation quality. 2.2.1 BLEU Evaluation The BLEU score (Papineni et al., 2002) was defined to measure overlap between a hypothesized translation and a set of human references. n-gram overlap counts {cn }4n=1 are computed over the test set sentences, and compared to the total counts of n-grams in the hypothesis: n,(t) ck n,(t) ak , max. # of matching n-grams for hyp. (t) ek , (t) , # of n-grams in hypothesis ek . 2.2.2 Maximum Likelihood Used in earlier models (Och and Ney, 2002), the likelihood criterion is defined as the likelihood of an (t) oracle hypothesis ek∗ , typically a single reference translation, or alternatively the closest match which was decoded. When the model is correct and infinite amounts of data are available, this method will converge to the Bayes error (minimum achievable error), where we define a classification task of selecting k∗ against all others. 2.2.3 Regularization Schemes One can convert a maximum likelihood problem into maximum a posteriori using Bayes’ rule: arg max λ Those quantities are abbreviated ck and ak to simplify the notation."
W07-0710,P06-2101,0,0.0596042,"ient ascent method over an approximation of the empirical Bayes reward function. 2.3.1 Approximation Because the empirical Bayes reward is defined over a set of sentences, it may not be decomposed sentence by sentence. This is computationally burP densome. Its sufficient statistics are r, t ck and P a . The function may be reconstructed in a firstk t order approximation with respect to each of these statistics. In practice this has the effect of commuting the expectation inside of the functional, and for that reason we call this criterion BLEU soft. This approximation is called linearization (Smith and Eisner, 2006). We used a first-order approximation for speed, and ease of interpretation of the derivations. The new objective function is: Maximum Empirical Bayes Reward The algorithm may be improved by giving partial credit for confidence pk of the model to partially correct hypotheses outside of the most likely hypothesis (Smith and Eisner, 2006): ¯ + J , log BP 4 X 1 n=1 4 log where the average bleu penalty is: ¯ , min{0; 1 − log BP T Kt 1 XX pk log B({ek (t)}). T t=1 k=1 Instead of the BLEU score, we use its logrithm, because we think it is exponentially hard to improve BLEU. This model is equivalent"
W07-0710,P03-1021,0,\N,Missing
W07-0710,P05-1034,0,\N,Missing
W07-0711,P02-1040,0,0.108043,"86.2 91.6 Table 2: Comparison of test set Precision between various models trained on 500K sentence pairs. All numbers are in percentage. model baseline HMM WDHMM (τ = 1000) IBM model 4 (GIZA++) E→F 90.6 91.9 F→E 91.4 92.6 combined 88.3 89.1 91.1 90.8 88.4 Table 3: Comparison of test set Recall between various models trained on 500K sentence pairs. All numbers are in percentage. 5.2 Machine translation on Europarl corpus We further tested our WDHMM on a phrase-based machine translation system to see whether our improvement on word alignment can also improve MT accuracy measured by BLEU score (Papineni et al., 2002). The machine translation experiment was conducted on the English-to-French track of NAACL 2006 Europarl evaluation workshop. The supplied training corpus contains 688K sentence pairs. Text data are already tokenized. In our experiment, we first lower-cased all text, then word clustering was performed to cluster words of English and French into 32 word classes respectively using the tool provided by (J. Goodman). Then word alignment was performed. Both baseline HMM and IBM model 4 use word-class based transition models, and in WDHMM the word-class based transition model was used for prior dist"
W07-0711,N03-1017,0,0.0467843,"Missing"
W07-0711,W04-3250,0,0.0836423,"Missing"
W07-0711,koen-2004-pharaoh,0,0.0656493,"el weight vector is trained on a dev set with 2000 English sentences, each of which has one French translation reference. In the experiment, only the first 500 sentences were used to train the log-linear model weight vector, where minimum error rate (MER) training was used (Och, 2003). After MER training, the weight vector that gives the best accuracy on the development set was selected. We then applied it to tests. There are 2000 sentences in the development-test set devtest, 2000 sentences in a test set test, and 1064 out-of-domain sentences called nc-test. The Pharaoh phrase-based decoder (Koehn 2004b) was used for decoding. The maximum re-ordering limit for decoding was 85 set to 7. We used default settings for all other parameters. We present BLEU scores of MT systems using different word alignments on all three test sets, where Fig 2 shows BLEU scores of the two indomain tests, and Fig 3 shows MT results on the out-of-domain test set. In testing, the prior parameter τ of WDHMM was varied in the range of [20, 5000]. In Fig. 2, the horizontal dash line and the horizontal dot line represent BLEU scores of the baseline HMM on devtest set and test set, respectively. The dash-line curve and"
W07-0711,N06-1014,0,0.0444083,"more details. These transition probabilities { p (i |i ′, I )} is a multinomial distribution estimated according to (2), where at each iteration the distortion set {c(i - i')} is the fractional count of transitions with jump width d = i - i', i.e., J −1 I c( d ) = ∑∑ Pr(a j = i, a j +1 = i + d |f1J , e1I , Λ ′) (4) j =1 i =1 where Λ' is the model obtained from the immediate previous iteration and these terms in (4) can be efficiently computed by using the ForwardBackward algorithm (Rabiner 1989). In practice, we can bucket the distortion parameters {c(d)} into a few buckets as implemented in (Liang et al., 2006). In our implementation, 15 buckets are used for c(≤-7), c(-6), ... c(0), ..., c(≥7). The probability mass for transitions with jump width larger than 6 is uniformly divided. As suggested in (Liang et al., 2006), we also use two separate sets of distortion parameters for transitioning into the first state, and for transitioning out of the last state, respectively. Finally, we further smooth transition probabilities with a uniform distribution as described in (Och and Ney, 2000a), 1 p′(a j |a j _ , I ) = α ⋅ + (1 − α ) ⋅ p (a j |a j _ , I ) . I After training, Viterbi decoding is used to find t"
W07-0711,P06-1065,0,0.0230336,"Missing"
W07-0711,C00-2163,0,0.609915,"severe. Moreover, since the sparsity of different words are very different, it is difficult to find a one-size-fits-all interpolation weight, and therefore simple linear interpolation is not optimal. In order to address this problem, we use Bayesian learning so that the transition model parameters are estimated by maximum a posteriori (MAP) training. With the help of the prior distribution of the model, the training is regularized and results in robust models. In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model (Vogel et al., 1996, Och and Ney, 2000a). Then we describe the equations of MAP training for word dependent transition models. In section 5, we present word alignment results that show significant alignment error rate reductions compared to the baseline HMM and IBM model 4. We also conducted phrase-based machine translation experiments on the Europarl corpus, English – French track, and shown that the proposed method can lead to significant BLEU score improvement compared to the HMM and IBM model 4. 2 Baseline HMM alignment model We briefly review the HMM based word alignment models (Vogel, 1996, Och and Ney, 2000a). Let’s denote"
W07-0711,P00-1056,0,0.795235,"severe. Moreover, since the sparsity of different words are very different, it is difficult to find a one-size-fits-all interpolation weight, and therefore simple linear interpolation is not optimal. In order to address this problem, we use Bayesian learning so that the transition model parameters are estimated by maximum a posteriori (MAP) training. With the help of the prior distribution of the model, the training is regularized and results in robust models. In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model (Vogel et al., 1996, Och and Ney, 2000a). Then we describe the equations of MAP training for word dependent transition models. In section 5, we present word alignment results that show significant alignment error rate reductions compared to the baseline HMM and IBM model 4. We also conducted phrase-based machine translation experiments on the Europarl corpus, English – French track, and shown that the proposed method can lead to significant BLEU score improvement compared to the HMM and IBM model 4. 2 Baseline HMM alignment model We briefly review the HMM based word alignment models (Vogel, 1996, Och and Ney, 2000a). Let’s denote"
W07-0711,P02-1038,0,0.0269461,"e table was extracted from the word aligned bilingual texts. The maximum phrase length was set to 7. In the phrase-based MT system, there are four channel models. They are direct maximum likelihood estimate of the probability of target phrase given source phrase, and the same estimate of source given target; we also compute the lexicon weighting features for source given target and target given source, respectively. Other models include word count and phrase count, and a 3-gram language model provided by the workshop. These models are combined in a log-linear framework with different weights (Och and Ney, 2002). The model weight vector is trained on a dev set with 2000 English sentences, each of which has one French translation reference. In the experiment, only the first 500 sentences were used to train the log-linear model weight vector, where minimum error rate (MER) training was used (Och, 2003). After MER training, the weight vector that gives the best accuracy on the development set was selected. We then applied it to tests. There are 2000 sentences in the development-test set devtest, 2000 sentences in a test set test, and 1064 out-of-domain sentences called nc-test. The Pharaoh phrase-based"
W07-0711,P03-1021,0,0.00706078,"target; we also compute the lexicon weighting features for source given target and target given source, respectively. Other models include word count and phrase count, and a 3-gram language model provided by the workshop. These models are combined in a log-linear framework with different weights (Och and Ney, 2002). The model weight vector is trained on a dev set with 2000 English sentences, each of which has one French translation reference. In the experiment, only the first 500 sentences were used to train the log-linear model weight vector, where minimum error rate (MER) training was used (Och, 2003). After MER training, the weight vector that gives the best accuracy on the development set was selected. We then applied it to tests. There are 2000 sentences in the development-test set devtest, 2000 sentences in a test set test, and 1064 out-of-domain sentences called nc-test. The Pharaoh phrase-based decoder (Koehn 2004b) was used for decoding. The maximum re-ordering limit for decoding was 85 set to 7. We used default settings for all other parameters. We present BLEU scores of MT systems using different word alignments on all three test sets, where Fig 2 shows BLEU scores of the two indo"
W07-0711,W02-1012,0,0.0225149,"Machine Translation, pages 80–87, c Prague, June 2007. 2007 Association for Computational Linguistics forward (monotonic alignment) or jumping backward (non-monotonic alignment), is not modeled. In this paper, we present a method to further improve the transition models for HMM alignment model. For each source word e, we not only model its self-transition probability, but also the probability of jumping from word e to a different word. For this purpose, we estimate a full transition model for each source word. A key problem for detailed word-dependent transition modeling is data sparsity. In (Toutanova et al., 2002), the word dependent self-transition probability P(stay|e) is interpolated with the global HMM self-transition probability to alleviate the data sparsity problem, where an interpolation weight is used for all words and that weight is tuned on a hold-out set. In the proposed word dependent transition model, because there are a large number of parameters to estimate, the data sparsity problem is even more severe. Moreover, since the sparsity of different words are very different, it is difficult to find a one-size-fits-all interpolation weight, and therefore simple linear interpolation is not op"
W07-0711,C96-2141,0,0.742638,"problem is even more severe. Moreover, since the sparsity of different words are very different, it is difficult to find a one-size-fits-all interpolation weight, and therefore simple linear interpolation is not optimal. In order to address this problem, we use Bayesian learning so that the transition model parameters are estimated by maximum a posteriori (MAP) training. With the help of the prior distribution of the model, the training is regularized and results in robust models. In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model (Vogel et al., 1996, Och and Ney, 2000a). Then we describe the equations of MAP training for word dependent transition models. In section 5, we present word alignment results that show significant alignment error rate reductions compared to the baseline HMM and IBM model 4. We also conducted phrase-based machine translation experiments on the Europarl corpus, English – French track, and shown that the proposed method can lead to significant BLEU score improvement compared to the HMM and IBM model 4. 2 Baseline HMM alignment model We briefly review the HMM based word alignment models (Vogel, 1996, Och and Ney, 20"
W07-0711,H05-1022,0,0.0596434,"orks have been done to improve transition models in HMM based word alignment. Och and Ney (2000a) have suggested estimating word-class based transition models so as to provide more detailed transition probabilities. However, due to the sparse data problem, only a small number of word classes are usually used and the many words in the same class still have to share the same transition model. Toutanova et al. (2002) has proposed to estimate a word-dependent self-transition model P(stay|e) so that each word can have its own probability to decide whether to stay or jump to a different word. Later Deng and Byrne (2005) proposed a word dependent phrase length model to better model state occupancy. However, these model can only model the probability of selfjumping. Important knowledge of jumping from e to a different position should also be word dependent but is not modeled. Another interesting comparison is between WDHMM and the fertility-based models, e.g., IBM model 3-5. Compared to these models, a major disadvantage of HMM is the absence of a model of source word fertility. However, as discussed in (Toutanova et al. 2002),the word dependent selftransition model can be viewed as an approximation of fertili"
W07-0711,P05-1059,0,0.0165468,"Missing"
W07-0711,J93-2003,0,\N,Missing
W07-0711,P06-1067,0,\N,Missing
W07-0711,H05-1021,0,\N,Missing
W07-0711,D07-1090,0,\N,Missing
W07-0711,P03-1012,0,\N,Missing
W07-0711,P07-1003,0,\N,Missing
W07-0711,D07-1007,0,\N,Missing
W07-0711,H05-1023,0,\N,Missing
W07-0711,C02-1032,0,\N,Missing
W07-0711,P06-1009,0,\N,Missing
W12-3124,C08-1005,0,0.0415003,"hms Two different methods have been proposed for building confusion networks: pairwise and incremental alignment. In pairwise alignment, each hypothesis corresponding to a source sentence is aligned independently with the skeleton hypothesis. This set of alignments is consolidated using the skeleton words as anchors to form the confusion network (Matusov et al., 2006; Sim et al., 2007). The same word in two hypotheses may be aligned with a different word in the skeleton resulting in repetition in the network. A two-pass alignment algorithm to improve pairwise TER alignments was introduced in (Ayan et al., 2008). In incremental alignment (Rosti et al., 2008), the confusion network is initialized by forming a simple graph with one word per link from the skeleton hypothesis. Each remaining hypothesis is aligned with the partial confusion network, which allows words from all previous hypotheses be considered as matches. The order in which the hypotheses are aligned may influence the alignment quality. Rosti et al. (2009) proposed a sentence specific alignment order by choosing the unaligned hypothesis closest to the partial confusion network according to TER. The following five alignment algorithms were"
W12-3124,J93-2003,0,0.0425621,"uence the alignment quality. Rosti et al. (2009) proposed a sentence specific alignment order by choosing the unaligned hypothesis closest to the partial confusion network according to TER. The following five alignment algorithms were used in this study. 3.1 Pairwise GIZA++ Enhanced Hypothesis Alignment Matusov et al. (2006) proposed using the GIZA++ Toolkit (Och and Ney, 2003) to align a set of target language translations. A parallel corpus where each system output acting as a skeleton appears as a translation of all system outputs corresponding to the same source sentence. The IBM Model 1 (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) are used to estimate the alignment. Alignments from both “translation” directions are used to obtain symmetrized alignments by interpolating the HMM occupation statistics (Matusov et al., 2004). The algorithm may benefit from the fact that it considers the entire test set when estimating the alignment model parameters; i.e., word alignment links from all output sentences influence the estimation, whereas other alignment algorithms only consider words within a pair of sentences (pairwise alignment) or all outputs corresponding to a single sour"
W12-3124,W11-2103,0,0.0445044,"based on GIZA++ Toolkit which introduced word reordering not present in MSA, and Sim et al. (2007) used the alignments produced by the translation edit rate (TER) (Snover et al., 2006) scoring. Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al., 1996) and inversion transduction grammars (ITG) (Wu, 1997). System combinations produced via confusion network decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation (Callison-Burch et al., 2011). However, there has not been a comparison of the most popular hypothesis alignment algorithms using the same sets of MT system outputs and otherwise identical combination pipelines. This paper attempts to systematically compare the quality of five hypothesis alignment algorithms. Alignments were produced for the same system outputs from three common test sets used in the 2009 NIST Open MT Evaluation and the 2011 Workshop on Statistical Machine Translation. Identical pre-processing, decoding, and weight tuning algorithms were used to quantitatively evaluate the alignment quality. Case insensit"
W12-3124,W98-1115,0,0.0726946,"used to infer the alignment. The pairwise IHMM was extended to operate incrementally in (Li et al., 2009). Sentence specific alignment order is not used by this aligner, which is referred to as iIHMM later in this paper. 3.3 alignment of system outputs. ITGs form an edit distance, invWER (Leusch et al., 2003), that permits properly nested block movements of substrings. For well-formed sentences, this may be more natural than allowing arbitrary shifts. The ITG algorithm is very expensive due to its O(n6 ) complexity. The search algorithm for the best ITG alignment, a best-first chart parsing (Charniak et al., 1998), was augmented with an A∗ search heuristic of quadratic complexity (Klein and Manning, 2003), resulting in significant reduction in computational complexity. The finite state-machine heuristic computes a lower bound to the alignment cost of two strings by allowing arbitrary word re-orderings. The ITG hypothesis alignment algorithm was extended to operate incrementally in (Karakos et al., 2010) and a novel version where the cost function is computed based on the stem/synonym similarity of (Snover et al., 2009) was used in this work. Also, a sentence specific alignment order was used. This alig"
W12-3124,I11-1075,1,0.871525,"combination. Availability of multiple system outputs within the DARPA GALE program as well as NIST Open MT and Workshop on Statistical Machine Translation evaluations has led to extensive research in combining the strengths of diverse MT systems, resulting in significant gains in translation quality. System combination methods proposed in the literature can be roughly divided into three categories: (i) hypothesis selection (Rosti et al., 2007b; Hildebrand and Vogel, 2008), (ii) re-decoding (Frederking and Nirenburg, 1994; Jayaraman and Lavie, 2005; Rosti et al., 2007b; He and Toutanova, 2009; Devlin et al., 2011), and (iii) confusion network decoding. Confusion network decoding has proven to be the most popular as it does not require deep N best lists1 and operates on the surface strings. It has Confusion network decoding has proven to be one of the most successful approaches to machine translation system combination. The hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have been proposed in the literature. This paper describes a systematic comparison of five well known hypothesis alignment algorithms for MT system combination via confusion netw"
W12-3124,A94-1016,0,0.294452,"ntary. The complementary information in the outputs from multiple MT systems may be exploited by system combination. Availability of multiple system outputs within the DARPA GALE program as well as NIST Open MT and Workshop on Statistical Machine Translation evaluations has led to extensive research in combining the strengths of diverse MT systems, resulting in significant gains in translation quality. System combination methods proposed in the literature can be roughly divided into three categories: (i) hypothesis selection (Rosti et al., 2007b; Hildebrand and Vogel, 2008), (ii) re-decoding (Frederking and Nirenburg, 1994; Jayaraman and Lavie, 2005; Rosti et al., 2007b; He and Toutanova, 2009; Devlin et al., 2011), and (iii) confusion network decoding. Confusion network decoding has proven to be the most popular as it does not require deep N best lists1 and operates on the surface strings. It has Confusion network decoding has proven to be one of the most successful approaches to machine translation system combination. The hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have been proposed in the literature. This paper describes a systematic comparison o"
W12-3124,D09-1125,1,0.91068,"be exploited by system combination. Availability of multiple system outputs within the DARPA GALE program as well as NIST Open MT and Workshop on Statistical Machine Translation evaluations has led to extensive research in combining the strengths of diverse MT systems, resulting in significant gains in translation quality. System combination methods proposed in the literature can be roughly divided into three categories: (i) hypothesis selection (Rosti et al., 2007b; Hildebrand and Vogel, 2008), (ii) re-decoding (Frederking and Nirenburg, 1994; Jayaraman and Lavie, 2005; Rosti et al., 2007b; He and Toutanova, 2009; Devlin et al., 2011), and (iii) confusion network decoding. Confusion network decoding has proven to be the most popular as it does not require deep N best lists1 and operates on the surface strings. It has Confusion network decoding has proven to be one of the most successful approaches to machine translation system combination. The hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have been proposed in the literature. This paper describes a systematic comparison of five well known hypothesis alignment algorithms for MT system combinat"
W12-3124,D08-1011,1,0.923232,"considers the entire test set when estimating the alignment model parameters; i.e., word alignment links from all output sentences influence the estimation, whereas other alignment algorithms only consider words within a pair of sentences (pairwise alignment) or all outputs corresponding to a single source sentence (incremental alignment). However, it does not naturally extend to incremental alignment. The monotone one-to-one alignments are then transformed into a confusion network. This aligner is referred to as GIZA later in this paper. 3.2 Incremental Indirect Hidden Markov Model Alignment He et al. (2008) proposed using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs. The parameters of the IHMM are estimated indirectly from a variety of sources including semantic word similarity, surface word similarity, and a distance-based distortion penalty. The alignment between two target language outputs are treated as the hidden states. A standard Viterbi algorithm is used to infer the alignment. The pairwise IHMM was extended to operate incrementally in (Li et al., 2009). Sentence specific alignment order is not used by this aligner, which is referred to as iIHMM later i"
W12-3124,2008.amta-srw.3,0,0.130672,"hese assumptions may be suboptimal and complementary. The complementary information in the outputs from multiple MT systems may be exploited by system combination. Availability of multiple system outputs within the DARPA GALE program as well as NIST Open MT and Workshop on Statistical Machine Translation evaluations has led to extensive research in combining the strengths of diverse MT systems, resulting in significant gains in translation quality. System combination methods proposed in the literature can be roughly divided into three categories: (i) hypothesis selection (Rosti et al., 2007b; Hildebrand and Vogel, 2008), (ii) re-decoding (Frederking and Nirenburg, 1994; Jayaraman and Lavie, 2005; Rosti et al., 2007b; He and Toutanova, 2009; Devlin et al., 2011), and (iii) confusion network decoding. Confusion network decoding has proven to be the most popular as it does not require deep N best lists1 and operates on the surface strings. It has Confusion network decoding has proven to be one of the most successful approaches to machine translation system combination. The hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have been proposed in the literatu"
W12-3124,P05-3026,0,0.0671933,"tion in the outputs from multiple MT systems may be exploited by system combination. Availability of multiple system outputs within the DARPA GALE program as well as NIST Open MT and Workshop on Statistical Machine Translation evaluations has led to extensive research in combining the strengths of diverse MT systems, resulting in significant gains in translation quality. System combination methods proposed in the literature can be roughly divided into three categories: (i) hypothesis selection (Rosti et al., 2007b; Hildebrand and Vogel, 2008), (ii) re-decoding (Frederking and Nirenburg, 1994; Jayaraman and Lavie, 2005; Rosti et al., 2007b; He and Toutanova, 2009; Devlin et al., 2011), and (iii) confusion network decoding. Confusion network decoding has proven to be the most popular as it does not require deep N best lists1 and operates on the surface strings. It has Confusion network decoding has proven to be one of the most successful approaches to machine translation system combination. The hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have been proposed in the literature. This paper describes a systematic comparison of five well known hypothesi"
W12-3124,P08-2021,1,0.889809,"Missing"
W12-3124,N03-1016,0,0.0741193,"i et al., 2009). Sentence specific alignment order is not used by this aligner, which is referred to as iIHMM later in this paper. 3.3 alignment of system outputs. ITGs form an edit distance, invWER (Leusch et al., 2003), that permits properly nested block movements of substrings. For well-formed sentences, this may be more natural than allowing arbitrary shifts. The ITG algorithm is very expensive due to its O(n6 ) complexity. The search algorithm for the best ITG alignment, a best-first chart parsing (Charniak et al., 1998), was augmented with an A∗ search heuristic of quadratic complexity (Klein and Manning, 2003), resulting in significant reduction in computational complexity. The finite state-machine heuristic computes a lower bound to the alignment cost of two strings by allowing arbitrary word re-orderings. The ITG hypothesis alignment algorithm was extended to operate incrementally in (Karakos et al., 2010) and a novel version where the cost function is computed based on the stem/synonym similarity of (Snover et al., 2009) was used in this work. Also, a sentence specific alignment order was used. This aligner is referred to as iITGp later in this paper. 3.4 Sim et al. (2007) proposed using transla"
W12-3124,W04-3250,0,0.20963,"ed for the NIST and WMT experiments to minimize perplexity on the English reference translations of the previous evaluations, NIST MT08 and WMT10. The system combination weights, both bi-gram lattice decoding and 5-gram 300-best list re-scoring weights, were tuned separately for lattices build with each hypothesis alignment algorithm. The final re-scoring 4 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ResultsRelease/indexISC.html 195 outputs were detokenized before computing case insensitive BLEU scores. Statistical significance was computed for each pairwise comparison using bootstrapping (Koehn, 2004). Aligner GIZA iTER iTERp iIHMM iITGp Decode tune test 60.06 57.95 59.74 58.63† 60.18 59.05† 60.51 59.27†‡ 60.65 59.37†‡ Oracle tune test 75.06 74.47 73.84 73.20 76.43 75.58 76.50 76.17 76.53 76.05 Table 2: Case insensitive BLEU scores for NIST MT09 Arabic-English system combination outputs. Note, four reference translations were available. Decode corresponds to results after weight tuning and Oracle corresponds to graph TER oracle. Dagger (†) denotes statistically significant difference compared to GIZA and double dagger (‡) compared to iTERp and the aligners above it. The BLEU scores for Ara"
W12-3124,2003.mtsummit-papers.32,1,0.749201,"Missing"
W12-3124,P09-1107,1,0.925933,"er is referred to as GIZA later in this paper. 3.2 Incremental Indirect Hidden Markov Model Alignment He et al. (2008) proposed using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs. The parameters of the IHMM are estimated indirectly from a variety of sources including semantic word similarity, surface word similarity, and a distance-based distortion penalty. The alignment between two target language outputs are treated as the hidden states. A standard Viterbi algorithm is used to infer the alignment. The pairwise IHMM was extended to operate incrementally in (Li et al., 2009). Sentence specific alignment order is not used by this aligner, which is referred to as iIHMM later in this paper. 3.3 alignment of system outputs. ITGs form an edit distance, invWER (Leusch et al., 2003), that permits properly nested block movements of substrings. For well-formed sentences, this may be more natural than allowing arbitrary shifts. The ITG algorithm is very expensive due to its O(n6 ) complexity. The search algorithm for the best ITG alignment, a best-first chart parsing (Charniak et al., 1998), was augmented with an A∗ search heuristic of quadratic complexity (Klein and Manni"
W12-3124,C04-1032,1,0.864957,"this study. 3.1 Pairwise GIZA++ Enhanced Hypothesis Alignment Matusov et al. (2006) proposed using the GIZA++ Toolkit (Och and Ney, 2003) to align a set of target language translations. A parallel corpus where each system output acting as a skeleton appears as a translation of all system outputs corresponding to the same source sentence. The IBM Model 1 (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) are used to estimate the alignment. Alignments from both “translation” directions are used to obtain symmetrized alignments by interpolating the HMM occupation statistics (Matusov et al., 2004). The algorithm may benefit from the fact that it considers the entire test set when estimating the alignment model parameters; i.e., word alignment links from all output sentences influence the estimation, whereas other alignment algorithms only consider words within a pair of sentences (pairwise alignment) or all outputs corresponding to a single source sentence (incremental alignment). However, it does not naturally extend to incremental alignment. The monotone one-to-one alignments are then transformed into a confusion network. This aligner is referred to as GIZA later in this paper. 3.2 I"
W12-3124,E06-1005,1,0.924758,"n used in confusion network decoding yielding small gains over using 1-best 191 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 191–199, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics also been shown to be very successful in combining speech recognition outputs (Fiscus, 1997; Mangu et al., 2000). The first application of confusion network decoding in MT system combination appeared in (Bangalore et al., 2001) where a multiple string alignment (MSA), made popular in biological sequence analysis, was applied to the MT system outputs. Matusov et al. (2006) proposed an alignment based on GIZA++ Toolkit which introduced word reordering not present in MSA, and Sim et al. (2007) used the alignments produced by the translation edit rate (TER) (Snover et al., 2006) scoring. Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al., 1996) and inversion transduction grammars (ITG) (Wu, 1997). System combinations produced via confusion network decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical"
W12-3124,J03-1002,1,0.0317226,"r link from the skeleton hypothesis. Each remaining hypothesis is aligned with the partial confusion network, which allows words from all previous hypotheses be considered as matches. The order in which the hypotheses are aligned may influence the alignment quality. Rosti et al. (2009) proposed a sentence specific alignment order by choosing the unaligned hypothesis closest to the partial confusion network according to TER. The following five alignment algorithms were used in this study. 3.1 Pairwise GIZA++ Enhanced Hypothesis Alignment Matusov et al. (2006) proposed using the GIZA++ Toolkit (Och and Ney, 2003) to align a set of target language translations. A parallel corpus where each system output acting as a skeleton appears as a translation of all system outputs corresponding to the same source sentence. The IBM Model 1 (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) are used to estimate the alignment. Alignments from both “translation” directions are used to obtain symmetrized alignments by interpolating the HMM occupation statistics (Matusov et al., 2004). The algorithm may benefit from the fact that it considers the entire test set when estimating the alignment model"
W12-3124,P03-1021,0,0.0272462,"sion network structure’s resemblance to a sausage. 193 unique n-gram contexts before LM scores can be assigned the arcs. Using long n-gram context may require pruning to reduce memory usage. Given uniform initial system weights, pruning may remove desirable paths. In this work, the lattices were expanded to bi-gram context and no pruning was performed. A set of bi-gram decoding weights were tuned directly on the expanded lattices using a distributed optimizer (Rosti et al., 2010). Since the score in Equation 2 is not a simple log-linear interpolation, the standard minimum error rate training (Och, 2003) with exact line search cannot be used. Instead, downhill simplex (Press et al., 2007) was used in the optimizer client. After bi-gram decoding weight optimization, another set of 5-gram rescoring weights were tuned on 300-best lists generated from the bi-gram expanded lattices. 3 Hypothesis Alignment Algorithms Two different methods have been proposed for building confusion networks: pairwise and incremental alignment. In pairwise alignment, each hypothesis corresponding to a source sentence is aligned independently with the skeleton hypothesis. This set of alignments is consolidated using th"
W12-3124,P02-1040,0,0.0950013,"has not been a comparison of the most popular hypothesis alignment algorithms using the same sets of MT system outputs and otherwise identical combination pipelines. This paper attempts to systematically compare the quality of five hypothesis alignment algorithms. Alignments were produced for the same system outputs from three common test sets used in the 2009 NIST Open MT Evaluation and the 2011 Workshop on Statistical Machine Translation. Identical pre-processing, decoding, and weight tuning algorithms were used to quantitatively evaluate the alignment quality. Case insensitive BLEU score (Papineni et al., 2002) was used as the translation quality metric. 2 Confusion Network Decoding A confusion network is a linear graph where all paths visit all nodes. Two consecutive nodes may be connected by one or more arcs. Given the arcs represent words in hypotheses, multiple arcs connecting two consecutive nodes can be viewed as alternative words in that position of a set of hypotheses encoded by the network. A special NULL token represents a skipped word and will not appear in the system combination output. For example, three hypotheses outputs (Rosti et al., 2011). 192 “twelve big cars”, “twelve cars”, and"
W12-3124,P07-1040,1,0.88807,"d modeling. Many of these assumptions may be suboptimal and complementary. The complementary information in the outputs from multiple MT systems may be exploited by system combination. Availability of multiple system outputs within the DARPA GALE program as well as NIST Open MT and Workshop on Statistical Machine Translation evaluations has led to extensive research in combining the strengths of diverse MT systems, resulting in significant gains in translation quality. System combination methods proposed in the literature can be roughly divided into three categories: (i) hypothesis selection (Rosti et al., 2007b; Hildebrand and Vogel, 2008), (ii) re-decoding (Frederking and Nirenburg, 1994; Jayaraman and Lavie, 2005; Rosti et al., 2007b; He and Toutanova, 2009; Devlin et al., 2011), and (iii) confusion network decoding. Confusion network decoding has proven to be the most popular as it does not require deep N best lists1 and operates on the surface strings. It has Confusion network decoding has proven to be one of the most successful approaches to machine translation system combination. The hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have"
W12-3124,N07-1029,1,0.875478,"ranslation, pages 191–199, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics also been shown to be very successful in combining speech recognition outputs (Fiscus, 1997; Mangu et al., 2000). The first application of confusion network decoding in MT system combination appeared in (Bangalore et al., 2001) where a multiple string alignment (MSA), made popular in biological sequence analysis, was applied to the MT system outputs. Matusov et al. (2006) proposed an alignment based on GIZA++ Toolkit which introduced word reordering not present in MSA, and Sim et al. (2007) used the alignments produced by the translation edit rate (TER) (Snover et al., 2006) scoring. Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al., 1996) and inversion transduction grammars (ITG) (Wu, 1997). System combinations produced via confusion network decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation (Callison-Burch et al., 2011). However, there has not been a comparison of the most popular hypothes"
W12-3124,W08-0329,1,0.848494,"or building confusion networks: pairwise and incremental alignment. In pairwise alignment, each hypothesis corresponding to a source sentence is aligned independently with the skeleton hypothesis. This set of alignments is consolidated using the skeleton words as anchors to form the confusion network (Matusov et al., 2006; Sim et al., 2007). The same word in two hypotheses may be aligned with a different word in the skeleton resulting in repetition in the network. A two-pass alignment algorithm to improve pairwise TER alignments was introduced in (Ayan et al., 2008). In incremental alignment (Rosti et al., 2008), the confusion network is initialized by forming a simple graph with one word per link from the skeleton hypothesis. Each remaining hypothesis is aligned with the partial confusion network, which allows words from all previous hypotheses be considered as matches. The order in which the hypotheses are aligned may influence the alignment quality. Rosti et al. (2009) proposed a sentence specific alignment order by choosing the unaligned hypothesis closest to the partial confusion network according to TER. The following five alignment algorithms were used in this study. 3.1 Pairwise GIZA++ Enhanc"
W12-3124,W09-0409,1,0.870846,"potheses may be aligned with a different word in the skeleton resulting in repetition in the network. A two-pass alignment algorithm to improve pairwise TER alignments was introduced in (Ayan et al., 2008). In incremental alignment (Rosti et al., 2008), the confusion network is initialized by forming a simple graph with one word per link from the skeleton hypothesis. Each remaining hypothesis is aligned with the partial confusion network, which allows words from all previous hypotheses be considered as matches. The order in which the hypotheses are aligned may influence the alignment quality. Rosti et al. (2009) proposed a sentence specific alignment order by choosing the unaligned hypothesis closest to the partial confusion network according to TER. The following five alignment algorithms were used in this study. 3.1 Pairwise GIZA++ Enhanced Hypothesis Alignment Matusov et al. (2006) proposed using the GIZA++ Toolkit (Och and Ney, 2003) to align a set of target language translations. A parallel corpus where each system output acting as a skeleton appears as a translation of all system outputs corresponding to the same source sentence. The IBM Model 1 (Brown et al., 1993) and hidden Markov model (HMM"
W12-3124,W10-1748,1,0.890146,"to distinguish paths with 2 A link is used as a synonym to the set of arcs between two consecutive nodes. The name refers to the confusion network structure’s resemblance to a sausage. 193 unique n-gram contexts before LM scores can be assigned the arcs. Using long n-gram context may require pruning to reduce memory usage. Given uniform initial system weights, pruning may remove desirable paths. In this work, the lattices were expanded to bi-gram context and no pruning was performed. A set of bi-gram decoding weights were tuned directly on the expanded lattices using a distributed optimizer (Rosti et al., 2010). Since the score in Equation 2 is not a simple log-linear interpolation, the standard minimum error rate training (Och, 2003) with exact line search cannot be used. Instead, downhill simplex (Press et al., 2007) was used in the optimizer client. After bi-gram decoding weight optimization, another set of 5-gram rescoring weights were tuned on 300-best lists generated from the bi-gram expanded lattices. 3 Hypothesis Alignment Algorithms Two different methods have been proposed for building confusion networks: pairwise and incremental alignment. In pairwise alignment, each hypothesis correspondi"
W12-3124,2006.amta-papers.25,0,0.0415224,"ociation for Computational Linguistics also been shown to be very successful in combining speech recognition outputs (Fiscus, 1997; Mangu et al., 2000). The first application of confusion network decoding in MT system combination appeared in (Bangalore et al., 2001) where a multiple string alignment (MSA), made popular in biological sequence analysis, was applied to the MT system outputs. Matusov et al. (2006) proposed an alignment based on GIZA++ Toolkit which introduced word reordering not present in MSA, and Sim et al. (2007) used the alignments produced by the translation edit rate (TER) (Snover et al., 2006) scoring. Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al., 1996) and inversion transduction grammars (ITG) (Wu, 1997). System combinations produced via confusion network decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation (Callison-Burch et al., 2011). However, there has not been a comparison of the most popular hypothesis alignment algorithms using the same sets of MT system outputs and otherwise identic"
W12-3124,W09-0441,0,0.0293967,"xity. The search algorithm for the best ITG alignment, a best-first chart parsing (Charniak et al., 1998), was augmented with an A∗ search heuristic of quadratic complexity (Klein and Manning, 2003), resulting in significant reduction in computational complexity. The finite state-machine heuristic computes a lower bound to the alignment cost of two strings by allowing arbitrary word re-orderings. The ITG hypothesis alignment algorithm was extended to operate incrementally in (Karakos et al., 2010) and a novel version where the cost function is computed based on the stem/synonym similarity of (Snover et al., 2009) was used in this work. Also, a sentence specific alignment order was used. This aligner is referred to as iITGp later in this paper. 3.4 Sim et al. (2007) proposed using translation edit rate scorer3 to obtain pairwise alignment of system outputs. The TER scorer tries to find shifts of blocks of words that minimize the edit distance between the shifted reference and a hypothesis. Due to the computational complexity, a set of heuristics is used to reduce the run time (Snover et al., 2006). The pairwise TER hypothesis alignment algorithm was extended to operate incrementally in (Rosti et al., 2"
W12-3124,C96-2141,1,0.668959,"2000). The first application of confusion network decoding in MT system combination appeared in (Bangalore et al., 2001) where a multiple string alignment (MSA), made popular in biological sequence analysis, was applied to the MT system outputs. Matusov et al. (2006) proposed an alignment based on GIZA++ Toolkit which introduced word reordering not present in MSA, and Sim et al. (2007) used the alignments produced by the translation edit rate (TER) (Snover et al., 2006) scoring. Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al., 1996) and inversion transduction grammars (ITG) (Wu, 1997). System combinations produced via confusion network decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation (Callison-Burch et al., 2011). However, there has not been a comparison of the most popular hypothesis alignment algorithms using the same sets of MT system outputs and otherwise identical combination pipelines. This paper attempts to systematically compare the quality of five hypothesis alignment algorithms. Alignments were pro"
W12-3124,J97-3002,0,0.0170128,"MT system combination appeared in (Bangalore et al., 2001) where a multiple string alignment (MSA), made popular in biological sequence analysis, was applied to the MT system outputs. Matusov et al. (2006) proposed an alignment based on GIZA++ Toolkit which introduced word reordering not present in MSA, and Sim et al. (2007) used the alignments produced by the translation edit rate (TER) (Snover et al., 2006) scoring. Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al., 1996) and inversion transduction grammars (ITG) (Wu, 1997). System combinations produced via confusion network decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation (Callison-Burch et al., 2011). However, there has not been a comparison of the most popular hypothesis alignment algorithms using the same sets of MT system outputs and otherwise identical combination pipelines. This paper attempts to systematically compare the quality of five hypothesis alignment algorithms. Alignments were produced for the same system outputs from three common t"
W12-3124,W11-2121,1,0.876089,"proposed using inversion transduction grammars (ITG) (Wu, 1997) for pairwise 194 Incremental Translation Edit Rate Alignment with Flexible Matching Incremental Translation Edit Rate Plus Alignment Snover et al. (2009) extended TER scoring to consider synonyms and paraphrase matches, called 3 http://www.cs.umd.edu/˜snover/tercom/ TER-plus (TERp). The shift heuristics in TERp were also relaxed relative to TER. Shifts are allowed if the words being shifted are: (i) exactly the same, (ii) synonyms, stems or paraphrases of the corresponding reference words, or (iii) any such combination. Xu et al. (2011) proposed using an incremental version of TERp for building consensus networks. A sentence specific alignment order was used by this aligner, which is referred to as iTERp later in this paper. 4 Experimental Evaluation Combination experiments were performed on (i) Arabic-English, from the informal system combination track of the 2009 NIST Open MT Evaluation4 ; (ii) German-English from the system combination evaluation of the 2011 Workshop on Statistical Machine Translation (Callison-Burch et al., 2011) (WMT11) and (iii) Spanish-English, again from WMT11. Eight top-performing systems (as evalua"
W12-3124,2005.eamt-1.20,0,\N,Missing
W15-3003,W11-2123,0,0.0158756,"n the WIT3 TED Chinese-English corpus (Cettolo et al., 2012), a good example of a subdomain with little available training data. We used the IWSLT dev2010 and test2010 sets (also from WIT3 ) for tuning and evaluation. The larger pool from which we selected data was constructed from an aggregation of 47 LDC Chinese-English parallel datasets.1 Table 1 contains the corpus statistics for the task and pool bilingual corpora. Corpus TED (task) LDC (pool) Sentences 145,901 6,025,295 Vocab (En) 49,323 458,570 Vocab (Zh) 64,616 714,628 Table 1: Chinese-English Parallel Data. We used the KenLM toolkit (Heafield, 2011) to build all language models used in this work (i.e., both for data selection and for the MT systems used for extrinsic evaluation). In all cases the models were 4-gram LMs. We used the Stanford part-of-speech tagger (Toutanova et al., 2003) when constructing our hybrid representations, to generate the POS tags for each of the English and Chinese sides of the corpora.2 We consider three ways of applying data selection using the standard (fully lexicalized) corpus representation and our hybrid representation. The first two use the monolingual MooreLewis method (Equation 1) to respectively comp"
W15-3003,2014.eamt-1.7,0,0.0214638,"LDC2013E125 LDC2013E132 LDC2013E83 LDC2013T03 LDC2013T05 LDC2013T07 LDC2013T11 LDC2013T16 LDC2014E08 LDC2014E111 LDC2014E50 LDC2014E69 LDC2014E99 LDC2014T04 LDC2014T11. 2 The Stanford NLP tools use the Penn tagsets, which comprise 43 tags for English and 35 for Chinese. 60 TED vocab LDC vocab Joint vocab LDC minus singletons Baseline selection vocab English 49,323 458,570 470,154 243,882 257,744 Chinese 64,616 714,628 729,283 373,381 388,927 Joint vocab Vocab with count ≥ 10 POS tags Hybrid vocab previously found that setting the threshold to 10 is slightly better than a minimum count of 20 (Axelrod, 2014), and varying the threshold further is a topic for future work; see Section 5. (most domain-like) to highest score (least domainlike). For each of those ranked pools, we consider increasingly larger subsets of the data: the best n = 50K, the best n = 100K, and so on. The largest subset we consider consists of the best n = 4M sentence pairs out of the 6M available. 4.2 4.2.1 Results Intrinsic Evaluation As noted, each of the bilingual Moore-Lewis method and our hybrid word/POS variation produces a version of the additional training pool in which sentences are ranked by relevance. We then select"
W15-3003,W13-2233,0,0.0717906,"Missing"
W15-3003,D11-1033,1,0.879115,"the undue effects of infrequent words. The proposal can be realized straightforwardly: after part-of-speech tagging the in-domain and pool corpora, we identify all words that appear infrequently in either one of the two corpora, and replace each of their word tokens with its POS tag. (1) where Hm (s) is the per-word cross entropy of s according to language model m. Lower scores for cross-entropy difference indicate more relevant sentences, i.e. those that are most like the target domain and unlike the full pool average. In bilingual settings, the bilingual Moore-Lewis criterion, introduced by Axelrod et al. (2011), combines the 59 cal machine translation as a downstream task. Relevance computation, sentence ranking and subset selection then proceed as usual according to the Moore-Lewis or bilingual Moore-Lewis criterion. As an example, consider again the phrases “an earthquake in Port-au-Prince” and “an earthquake in Kodari”, and suppose that the words an, in, and earthquake are above-threshold in frequency. Our hybrid word/POS representation for both sentences would be the same: “an earthquake in NNP”. Our approach differs from the standard data selection method most significantly in its handling of r"
W15-3003,N03-2003,1,0.666232,"Missing"
W15-3003,P10-2041,0,0.202258,"system. The catch, of course, is that any large data pool can be expected to contain sentences that are at best irrelevant to the domain, and at worst detrimental: the goals of fidelity (matching in-domain data as closely as possible) and broad coverage are often at odds (Gasc´o et al., 2012). As a result, much work has focused on fidelity. Mirkin and Besacier (2014) survey the difficulties of increasing coverage while minimizing impact on model performance. We build on the standard approach for data selection in language modeling, which uses crossentropy difference as the similarity metric (Moore and Lewis, 2010). The Moore-Lewis procedure first trains an in-domain language model (LM) on the in-domain data, and another LM on the full pool of general data. It assigns to each full-pool sentence s a cross-entropy difference score, HLMIN (s) − HLMP OOL (s), (2) 3 Our Approach: Abstracting Away Words in the Long Tail Our approach is motivated by the observation that domain mismatches can have a strong register component, and this comprises both lexical and syntactic differences. We are inspired, as well, by work in stylometry, observing that attempts to quantify differences between text datasets try to lea"
W15-3003,P02-1040,0,0.0918678,"e, particularly for Chinese. Similarly, Figure 1 shows that our hybrid method also increases more rapidly to asymptotically approach full in-domain vocabulary coverage as well. 4.2.2 Extrinsic Evaluation Improved vocabulary coverage is a positive result, but we are also interested in downstream application performance. Accordingly, we trained SMT systems using cdec (Dyer et al., 2010) on subsets of selected data. All SMT systems were tuned using MIRA (Chiang et al., 2008) on the dev2010 data from IWSLT (Federico et al., 2011), and then evaluated on the test2010 IWSLT test set using both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). To isolate the impact of the data selection method, we present results just using the selected data, without the combining with the in-domain data into a multi-model sys4.2.3 Selection Model Size The resulting translation system sizes conform with prior work: selecting smaller subsets yields smaller downstream MT systems. For example, an MT system trained on 1M selected sentences is ∼2.3GB in size, a factor of 5 smaller than the 11.3GB baseline MT system trained on all 6M sentences. In addition, we observe a healthy re62 Figure 1: Percentage of TED vocabulary co"
W15-3003,D08-1024,1,0.706303,"inearly with the amount of selection data. By contrast, our proposed method appears to asymptotically approach full in-domain vocabulary coverage, particularly for Chinese. Similarly, Figure 1 shows that our hybrid method also increases more rapidly to asymptotically approach full in-domain vocabulary coverage as well. 4.2.2 Extrinsic Evaluation Improved vocabulary coverage is a positive result, but we are also interested in downstream application performance. Accordingly, we trained SMT systems using cdec (Dyer et al., 2010) on subsets of selected data. All SMT systems were tuned using MIRA (Chiang et al., 2008) on the dev2010 data from IWSLT (Federico et al., 2011), and then evaluated on the test2010 IWSLT test set using both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). To isolate the impact of the data selection method, we present results just using the selected data, without the combining with the in-domain data into a multi-model sys4.2.3 Selection Model Size The resulting translation system sizes conform with prior work: selecting smaller subsets yields smaller downstream MT systems. For example, an MT system trained on 1M selected sentences is ∼2.3GB in size, a factor of 5 smalle"
W15-3003,2006.amta-papers.25,0,0.0215726,"ilarly, Figure 1 shows that our hybrid method also increases more rapidly to asymptotically approach full in-domain vocabulary coverage as well. 4.2.2 Extrinsic Evaluation Improved vocabulary coverage is a positive result, but we are also interested in downstream application performance. Accordingly, we trained SMT systems using cdec (Dyer et al., 2010) on subsets of selected data. All SMT systems were tuned using MIRA (Chiang et al., 2008) on the dev2010 data from IWSLT (Federico et al., 2011), and then evaluated on the test2010 IWSLT test set using both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). To isolate the impact of the data selection method, we present results just using the selected data, without the combining with the in-domain data into a multi-model sys4.2.3 Selection Model Size The resulting translation system sizes conform with prior work: selecting smaller subsets yields smaller downstream MT systems. For example, an MT system trained on 1M selected sentences is ∼2.3GB in size, a factor of 5 smaller than the 11.3GB baseline MT system trained on all 6M sentences. In addition, we observe a healthy re62 Figure 1: Percentage of TED vocabulary covered vs. number of selected s"
W15-3003,W13-2803,0,0.0193944,"cally a range of values for n is considered, selecting the n that performs best on held-out indomain data. While shown to be effective, however, wordbased scores may not capture all facets of relevance. The strategy of a hybrid word/POS representation was first explored by Bulyko et al. (2003), who used class-dependent weights for mixing multi-source language models. The classes were a combination of the 100 most frequent words and POS tags. Bisazza and Federico (2012) target in-domain coverage by using a hybrid word/POS representation to train an additional LM for decoding in an MT pipeline. Toral (2013) uses a hybrid word/class representation for data selection for language modeling; he replaces all named entities with their type (e.g. ’person’, ’organization’), and experiments with also lemmatizing the remaining words. Related Work Data selection is a widely-used variant of domain adaptation that requires quantifying the relevance to the domain of the sentences in a pooled corpus of additional data. The pool is sorted by relevance score, the highest ranked portion is kept, and the rest discarded.This process – also known as “rank-and-select” in language modeling (Sethy et al., 2009) – ident"
W15-3003,N03-1033,0,0.0161681,"from which we selected data was constructed from an aggregation of 47 LDC Chinese-English parallel datasets.1 Table 1 contains the corpus statistics for the task and pool bilingual corpora. Corpus TED (task) LDC (pool) Sentences 145,901 6,025,295 Vocab (En) 49,323 458,570 Vocab (Zh) 64,616 714,628 Table 1: Chinese-English Parallel Data. We used the KenLM toolkit (Heafield, 2011) to build all language models used in this work (i.e., both for data selection and for the MT systems used for extrinsic evaluation). In all cases the models were 4-gram LMs. We used the Stanford part-of-speech tagger (Toutanova et al., 2003) when constructing our hybrid representations, to generate the POS tags for each of the English and Chinese sides of the corpora.2 We consider three ways of applying data selection using the standard (fully lexicalized) corpus representation and our hybrid representation. The first two use the monolingual MooreLewis method (Equation 1) to respectively compute relevance scores using the English (output) side and the Chinese (input) side of the parallel corpora. The third uses bilingual Moore-Lewis (Equation 2) to compute the bilingual score over both sides. Each of these three variants produces"
W15-3003,E12-1045,0,\N,Missing
W15-3003,E12-1016,0,\N,Missing
W15-3003,P10-4002,1,\N,Missing
W15-3003,2014.amta-researchers.23,0,\N,Missing
W15-3003,2011.iwslt-evaluation.1,0,\N,Missing
W15-3003,2012.eamt-1.60,0,\N,Missing
