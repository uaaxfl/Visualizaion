2020.coling-main.117,W18-0528,0,0.0158119,"int the lexical-semantic relationship between nouns that have changed. Our study differs from previous work in that we investigate the effectiveness of the edits through human annotations and that we work with instructional texts instead of Wikipedia articles. 2.2 Revisions in Persuasive Texts Another genre that has been used to examine revision histories are essays, such as those in the ArgRewrite corpus (Zhang et al., 2017). Similar as in Wikipedia, there is a body of research available devoted to the classification and analysis of edit types (Zhang and Litman, 2015; Zhang and Litman, 2016; Afrin and Litman, 2018). A closely related study to ours is the one from Afrin and Litman (2018), which aimed to develop a computational model that could predict, given the original and revised sentence, if the revised sentence is better than the original. For this purpose, Afrin and Litman (2018) showed the annotators the original and revised sentence, and asked them to label the revised sentence as Better or Not Better. The authors developed a small set of guidelines that the annotators had to use, and relied on the annotators’ judgement for cases not covered by those guidelines. They let the annotators know the i"
2020.coling-main.117,2020.lrec-1.702,1,0.835784,"ns about the required level of specificity. 1 Introduction Instructional texts often go through multiple revisions. For their final form, the author has to ensure that it is clear in which sequence described actions need to be performed and in what manner they have to be executed. A collection of revisions of instructional texts can be found, for instance, on the community-edited platform wikiHow. In previous work using wikiHow revision histories, we showed that improvements in texts can be modeled computationally under the assumption that revised sentences are better than their predecessors (Anthonio et al., 2020). This assumption seems intuitive for edits that correct or clarify parts of a sentence (e.g., in terms of grammar/factual mistakes). In the analysis, however, we also found a number of edits that involve paraphrases or more subtle differences in terms of specificity/genericity. In these cases, it remains unclear in how far edits actually represent improvements. This problem notably exists also in other work on textual revisions which aim to develop a computational model, including studies on Wikipedia (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012; Faruqui et al., 2018) and ArgRewrit"
2020.coling-main.117,Q17-1010,0,0.00532511,"ntext can improve the identification of revised versions by means of modeling factors such as cohesive ties, which are known to contribute to the readability of a text (Mahlberg, 2006; Pitler and Nenkova, 2008). 4.1 Experimental Setup Model. In both settings, we use a bidirectional long short-term memory network (BiLSTM) as our classification model (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997). The input layer of the BiLSTM is initialized with pre-trained word embeddings. In the sentence in isolation setting, we experiment with various types of embeddings, including FastText (Bojanowski et al., 2017), GloVe (Pennington et al., 2014), BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). Furthermore, we use two stacked BiLSTMS and experiment with dimensions for each BiLSTM of 256 and 512. We concatenate the last forward and backward hidden layers to obtain a final representation, which is fed into a linear layer that outputs a single classification score. Between the stacked BiLSTMs and on the final representation, we apply Dropout (Srivastava et al., 2014) with a probability of 0.5. At training time, we use the labels 0 and 1 for Sb and Sr , respectively, and we compute the loss usin"
2020.coling-main.117,E12-1036,0,0.121074,"ssumption that revised sentences are better than their predecessors (Anthonio et al., 2020). This assumption seems intuitive for edits that correct or clarify parts of a sentence (e.g., in terms of grammar/factual mistakes). In the analysis, however, we also found a number of edits that involve paraphrases or more subtle differences in terms of specificity/genericity. In these cases, it remains unclear in how far edits actually represent improvements. This problem notably exists also in other work on textual revisions which aim to develop a computational model, including studies on Wikipedia (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012; Faruqui et al., 2018) and ArgRewrite (Zhang and Litman, 2015). In particular, these studies neglect two important aspects: First, they merely categorize edits by how they change a sentence (e.g., information deletion/addition, link creation/deletion), without specifying if or in how far the change represents an actual improvement. Secondly, they provide limited or no insight about how the content of a revised sentence is semantically related to the content of the original sentence. Such information could facilitate the understanding of why a change is perceive"
2020.coling-main.117,W12-4006,0,0.0162296,"two related lines of research in this section: studies on revisions in Wikipedia (Section 2.1) and on revisions in persuasive texts (Section 2.2). 1360 2.1 Revisions in Wikipedia Within NLP, various applications have been developed that build upon Wikipedia revision history. Examples are preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008) simplification (Woodsend and Lapata, 2011; Yatskar et al., 2010), bias detection (Recasens et al., 2013), sentence paraphrasing (Max and Wisniewski, 2010), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012) and information retrieval (Aji et al., 2010; Nunes et al., 2011). Revisions in Wikipedia have also been used to gain more theoretical knowledge on why and how texts are edited collaboratively (Bronner and Monz, 2012; Liu and Ram, 2011; Yang et al., 2017; Daxenberger and Gurevych, 2012; Faruqui et al., 2018). The majority of these studies categorize edits based on two aspects. First, whether the edits are domain-specific or not. Second, whether the edits affect the meaning of the text (text-base) or not (surface-changes). These edit categories were introduced in Faigley and Witte (1981) and ex"
2020.coling-main.117,N13-1055,0,0.0162097,"how the performances vary in relation to the lexicalsemantic differences between the original and revised sentence (§6). 2 Related Work There is only one related study using wikiHow (Anthonio et al., 2020), which we built upon (see Section 3). Therefore, we discuss studies within two related lines of research in this section: studies on revisions in Wikipedia (Section 2.1) and on revisions in persuasive texts (Section 2.2). 1360 2.1 Revisions in Wikipedia Within NLP, various applications have been developed that build upon Wikipedia revision history. Examples are preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008) simplification (Woodsend and Lapata, 2011; Yatskar et al., 2010), bias detection (Recasens et al., 2013), sentence paraphrasing (Max and Wisniewski, 2010), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012) and information retrieval (Aji et al., 2010; Nunes et al., 2011). Revisions in Wikipedia have also been used to gain more theoretical knowledge on why and how texts are edited collaboratively (Bronner and Monz, 2012; Liu and Ram, 2011; Yang et al., 2017; Daxenberger and Gurevych, 2012; Faruqui et al., 2018). The majo"
2020.coling-main.117,C12-1044,0,0.0867186,"entences are better than their predecessors (Anthonio et al., 2020). This assumption seems intuitive for edits that correct or clarify parts of a sentence (e.g., in terms of grammar/factual mistakes). In the analysis, however, we also found a number of edits that involve paraphrases or more subtle differences in terms of specificity/genericity. In these cases, it remains unclear in how far edits actually represent improvements. This problem notably exists also in other work on textual revisions which aim to develop a computational model, including studies on Wikipedia (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012; Faruqui et al., 2018) and ArgRewrite (Zhang and Litman, 2015). In particular, these studies neglect two important aspects: First, they merely categorize edits by how they change a sentence (e.g., information deletion/addition, link creation/deletion), without specifying if or in how far the change represents an actual improvement. Secondly, they provide limited or no insight about how the content of a revised sentence is semantically related to the content of the original sentence. Such information could facilitate the understanding of why a change is perceived as a potential improvement. Fo"
2020.coling-main.117,N19-1423,0,0.0105257,"of modeling factors such as cohesive ties, which are known to contribute to the readability of a text (Mahlberg, 2006; Pitler and Nenkova, 2008). 4.1 Experimental Setup Model. In both settings, we use a bidirectional long short-term memory network (BiLSTM) as our classification model (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997). The input layer of the BiLSTM is initialized with pre-trained word embeddings. In the sentence in isolation setting, we experiment with various types of embeddings, including FastText (Bojanowski et al., 2017), GloVe (Pennington et al., 2014), BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). Furthermore, we use two stacked BiLSTMS and experiment with dimensions for each BiLSTM of 256 and 512. We concatenate the last forward and backward hidden layers to obtain a final representation, which is fed into a linear layer that outputs a single classification score. Between the stacked BiLSTMs and on the final representation, we apply Dropout (Srivastava et al., 2014) with a probability of 0.5. At training time, we use the labels 0 and 1 for Sb and Sr , respectively, and we compute the loss using Binary Cross Entropy Loss4 . At evaluation time, we use the"
2020.coling-main.117,D18-1028,0,0.0232484,"Missing"
2020.coling-main.117,max-wisniewski-2010-mining,0,0.0299385,"2020), which we built upon (see Section 3). Therefore, we discuss studies within two related lines of research in this section: studies on revisions in Wikipedia (Section 2.1) and on revisions in persuasive texts (Section 2.2). 1360 2.1 Revisions in Wikipedia Within NLP, various applications have been developed that build upon Wikipedia revision history. Examples are preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008) simplification (Woodsend and Lapata, 2011; Yatskar et al., 2010), bias detection (Recasens et al., 2013), sentence paraphrasing (Max and Wisniewski, 2010), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012) and information retrieval (Aji et al., 2010; Nunes et al., 2011). Revisions in Wikipedia have also been used to gain more theoretical knowledge on why and how texts are edited collaboratively (Bronner and Monz, 2012; Liu and Ram, 2011; Yang et al., 2017; Daxenberger and Gurevych, 2012; Faruqui et al., 2018). The majority of these studies categorize edits based on two aspects. First, whether the edits are domain-specific or not. Second, whether the edits affect the meaning of the text (text-base) or not (surface-change"
2020.coling-main.117,P15-1146,0,0.0715559,"Missing"
2020.coling-main.117,D14-1162,0,0.0852387,"ion of revised versions by means of modeling factors such as cohesive ties, which are known to contribute to the readability of a text (Mahlberg, 2006; Pitler and Nenkova, 2008). 4.1 Experimental Setup Model. In both settings, we use a bidirectional long short-term memory network (BiLSTM) as our classification model (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997). The input layer of the BiLSTM is initialized with pre-trained word embeddings. In the sentence in isolation setting, we experiment with various types of embeddings, including FastText (Bojanowski et al., 2017), GloVe (Pennington et al., 2014), BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). Furthermore, we use two stacked BiLSTMS and experiment with dimensions for each BiLSTM of 256 and 512. We concatenate the last forward and backward hidden layers to obtain a final representation, which is fed into a linear layer that outputs a single classification score. Between the stacked BiLSTMs and on the final representation, we apply Dropout (Srivastava et al., 2014) with a probability of 0.5. At training time, we use the labels 0 and 1 for Sb and Sr , respectively, and we compute the loss using Binary Cross Entropy Loss4 . At"
2020.coling-main.117,D08-1020,0,0.058726,"Missing"
2020.coling-main.117,prasad-etal-2008-penn,0,0.096837,"Missing"
2020.coling-main.117,P13-1162,0,0.0196178,"related study using wikiHow (Anthonio et al., 2020), which we built upon (see Section 3). Therefore, we discuss studies within two related lines of research in this section: studies on revisions in Wikipedia (Section 2.1) and on revisions in persuasive texts (Section 2.2). 1360 2.1 Revisions in Wikipedia Within NLP, various applications have been developed that build upon Wikipedia revision history. Examples are preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008) simplification (Woodsend and Lapata, 2011; Yatskar et al., 2010), bias detection (Recasens et al., 2013), sentence paraphrasing (Max and Wisniewski, 2010), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012) and information retrieval (Aji et al., 2010; Nunes et al., 2011). Revisions in Wikipedia have also been used to gain more theoretical knowledge on why and how texts are edited collaboratively (Bronner and Monz, 2012; Liu and Ram, 2011; Yang et al., 2017; Daxenberger and Gurevych, 2012; Faruqui et al., 2018). The majority of these studies categorize edits based on two aspects. First, whether the edits are domain-specific or not. Second, whether the edits affect the mean"
2020.coling-main.117,D11-1038,0,0.0246798,"iginal and revised sentence (§6). 2 Related Work There is only one related study using wikiHow (Anthonio et al., 2020), which we built upon (see Section 3). Therefore, we discuss studies within two related lines of research in this section: studies on revisions in Wikipedia (Section 2.1) and on revisions in persuasive texts (Section 2.2). 1360 2.1 Revisions in Wikipedia Within NLP, various applications have been developed that build upon Wikipedia revision history. Examples are preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008) simplification (Woodsend and Lapata, 2011; Yatskar et al., 2010), bias detection (Recasens et al., 2013), sentence paraphrasing (Max and Wisniewski, 2010), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012) and information retrieval (Aji et al., 2010; Nunes et al., 2011). Revisions in Wikipedia have also been used to gain more theoretical knowledge on why and how texts are edited collaboratively (Bronner and Monz, 2012; Liu and Ram, 2011; Yang et al., 2017; Daxenberger and Gurevych, 2012; Faruqui et al., 2018). The majority of these studies categorize edits based on two aspects. First, whether the edits are do"
2020.coling-main.117,D17-1213,0,0.0160098,"revision history. Examples are preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008) simplification (Woodsend and Lapata, 2011; Yatskar et al., 2010), bias detection (Recasens et al., 2013), sentence paraphrasing (Max and Wisniewski, 2010), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012) and information retrieval (Aji et al., 2010; Nunes et al., 2011). Revisions in Wikipedia have also been used to gain more theoretical knowledge on why and how texts are edited collaboratively (Bronner and Monz, 2012; Liu and Ram, 2011; Yang et al., 2017; Daxenberger and Gurevych, 2012; Faruqui et al., 2018). The majority of these studies categorize edits based on two aspects. First, whether the edits are domain-specific or not. Second, whether the edits affect the meaning of the text (text-base) or not (surface-changes). These edit categories were introduced in Faigley and Witte (1981) and extended in Jones (2008). In general, text-base edits are further subcategorized by their syntactic operation, such as deletion or modification. In this categorization, questions such as “why did authors decide to change the sentence” and “what is the effe"
2020.coling-main.117,N10-1056,0,0.0425608,"(§6). 2 Related Work There is only one related study using wikiHow (Anthonio et al., 2020), which we built upon (see Section 3). Therefore, we discuss studies within two related lines of research in this section: studies on revisions in Wikipedia (Section 2.1) and on revisions in persuasive texts (Section 2.2). 1360 2.1 Revisions in Wikipedia Within NLP, various applications have been developed that build upon Wikipedia revision history. Examples are preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008) simplification (Woodsend and Lapata, 2011; Yatskar et al., 2010), bias detection (Recasens et al., 2013), sentence paraphrasing (Max and Wisniewski, 2010), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012) and information retrieval (Aji et al., 2010; Nunes et al., 2011). Revisions in Wikipedia have also been used to gain more theoretical knowledge on why and how texts are edited collaboratively (Bronner and Monz, 2012; Liu and Ram, 2011; Yang et al., 2017; Daxenberger and Gurevych, 2012; Faruqui et al., 2018). The majority of these studies categorize edits based on two aspects. First, whether the edits are domain-specific or not. S"
2020.coling-main.117,W10-3504,0,0.0273091,"refore, we discuss studies within two related lines of research in this section: studies on revisions in Wikipedia (Section 2.1) and on revisions in persuasive texts (Section 2.2). 1360 2.1 Revisions in Wikipedia Within NLP, various applications have been developed that build upon Wikipedia revision history. Examples are preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008) simplification (Woodsend and Lapata, 2011; Yatskar et al., 2010), bias detection (Recasens et al., 2013), sentence paraphrasing (Max and Wisniewski, 2010), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012) and information retrieval (Aji et al., 2010; Nunes et al., 2011). Revisions in Wikipedia have also been used to gain more theoretical knowledge on why and how texts are edited collaboratively (Bronner and Monz, 2012; Liu and Ram, 2011; Yang et al., 2017; Daxenberger and Gurevych, 2012; Faruqui et al., 2018). The majority of these studies categorize edits based on two aspects. First, whether the edits are domain-specific or not. Second, whether the edits affect the meaning of the text (text-base) or not (surface-changes). These edit categories were introduced in Faigley a"
2020.coling-main.117,W15-0616,0,0.273737,"his assumption seems intuitive for edits that correct or clarify parts of a sentence (e.g., in terms of grammar/factual mistakes). In the analysis, however, we also found a number of edits that involve paraphrases or more subtle differences in terms of specificity/genericity. In these cases, it remains unclear in how far edits actually represent improvements. This problem notably exists also in other work on textual revisions which aim to develop a computational model, including studies on Wikipedia (Bronner and Monz, 2012; Daxenberger and Gurevych, 2012; Faruqui et al., 2018) and ArgRewrite (Zhang and Litman, 2015). In particular, these studies neglect two important aspects: First, they merely categorize edits by how they change a sentence (e.g., information deletion/addition, link creation/deletion), without specifying if or in how far the change represents an actual improvement. Secondly, they provide limited or no insight about how the content of a revised sentence is semantically related to the content of the original sentence. Such information could facilitate the understanding of why a change is perceived as a potential improvement. For instance, it might be that a sentence is perceived as better"
2020.coling-main.117,N16-1168,0,0.324544,"In particular, we pinpoint the lexical-semantic relationship between nouns that have changed. Our study differs from previous work in that we investigate the effectiveness of the edits through human annotations and that we work with instructional texts instead of Wikipedia articles. 2.2 Revisions in Persuasive Texts Another genre that has been used to examine revision histories are essays, such as those in the ArgRewrite corpus (Zhang et al., 2017). Similar as in Wikipedia, there is a body of research available devoted to the classification and analysis of edit types (Zhang and Litman, 2015; Zhang and Litman, 2016; Afrin and Litman, 2018). A closely related study to ours is the one from Afrin and Litman (2018), which aimed to develop a computational model that could predict, given the original and revised sentence, if the revised sentence is better than the original. For this purpose, Afrin and Litman (2018) showed the annotators the original and revised sentence, and asked them to label the revised sentence as Better or Not Better. The authors developed a small set of guidelines that the annotators had to use, and relied on the annotators’ judgement for cases not covered by those guidelines. They let"
2020.coling-main.117,P17-1144,0,0.0809883,"on model. Our work is similar to Yang et al. (2017) in that we shed more light on those sentences that have been revised for other reasons than improving the fluency. In particular, we pinpoint the lexical-semantic relationship between nouns that have changed. Our study differs from previous work in that we investigate the effectiveness of the edits through human annotations and that we work with instructional texts instead of Wikipedia articles. 2.2 Revisions in Persuasive Texts Another genre that has been used to examine revision histories are essays, such as those in the ArgRewrite corpus (Zhang et al., 2017). Similar as in Wikipedia, there is a body of research available devoted to the classification and analysis of edit types (Zhang and Litman, 2015; Zhang and Litman, 2016; Afrin and Litman, 2018). A closely related study to ours is the one from Afrin and Litman (2018), which aimed to develop a computational model that could predict, given the original and revised sentence, if the revised sentence is better than the original. For this purpose, Afrin and Litman (2018) showed the annotators the original and revised sentence, and asked them to label the revised sentence as Better or Not Better. The"
2020.crac-1.4,W13-2322,0,0.0775248,"chael Roth Institute for NLP University of Stuttgart {tatianak|koller}@coli.uni-saarland.de michael.roth@ims.uni-stuttgart.de Abstract This work addresses coreference resolution in Abstract Meaning Representation (AMR) graphs, a popular formalism for semantic parsing. We evaluate several current coreference resolution techniques on a recently published AMR coreference corpus, establishing baselines for future work. We also demonstrate that coreference resolution can improve the accuracy of a state-ofthe-art semantic parser on this corpus. 1 Introduction Abstract Meaning Representations (AMRs, Banarescu et al. (2013)) are a popular type of symbolic semantic representation for semantic parsing. AMRs are labeled directed graphs whose nodes represent entities, events, properties, and states; the edges represent semantic relations between the nodes. For instance, in the example AMRs of Fig. 2, the predicate node c describes a come-back relation between the ARG1 “I” and the ARG3 “this”. AMR is designed to abstract over the way in which a certain piece of meaning was expressed in language; thus “the destruction of the room by the boy” and “the boy destroyed the room” are represented by the same graph. In the ex"
2020.crac-1.4,P13-2131,0,0.0349698,"tokens. We collapsed the coreferent nodes by replacing all edges into a node for a coreferent token by edges into the first node of the coreference chain; see O’Gorman et al. (2018) for details. For example, in Fig. 2 (a) there are three coreferent nodes i:i, i2:i and i3:i. Since all three nodes represent the same entity the corresponding edges can be rearranged to point to the same node i:i as shown in Fig. 2 (b). We evaluated the performance of Lindemann’s parser, with and without the added coreference information, on the complete MS-AMR test data. To this end, we computed the Smatch score (Cai and Knight, 2013) for the predicted vs. gold document-level graphs. Table 3 shows the micro- and macro-average Smatch precision, recall and f-score for the documents from the test set. The left column indicates the scores obtained by comparing the gold AMRs with coreference to the ones generated by the parser without coreference. The middle column shows the scores for the gold MS-AMR graphs versus the parser output augmented with coreference predictions. The overall improvement in f-score is around three points Smatch f-score. The right column shows the scores obtained by augmenting Lindemann’s parser output w"
2020.crac-1.4,P16-1061,0,0.0629468,"Missing"
2020.crac-1.4,2020.tacl-1.5,0,0.118877,"mentions are not pieces of text as in other coreference annotation schemes, but nodes in the AMR graphs. The annotation also specifies what implicit roles of predicate nodes the entity fills. In this paper, we make two contributions. First, we evaluate the performance of different coreference resolution tools on the MS-AMR annotations. We evaluate these on the token level (by projecting the coreference annotations from the nodes to the sentences) and on the node level (by projecting the tools’ coreference predictions to the nodes of the graphs) and find that AllenNLP with SpanBERT embeddings (Joshi et al., 2020) generally performs best. Second, we show for the first time how the output of a coreference system can be integrated into the predictions of a state-of-the-art AMR parser. We use the neural semantic parser of Lindemann et al. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 33 Proceedings of the 3rd Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2020), pages 33–38, Barcelona, Spain (online), December 12, 2020. a) k: know-01 :ARG0 a3: amr-unknown m: multi-sentence"
2020.crac-1.4,J13-4004,0,0.0793353,"Missing"
2020.crac-1.4,D17-1018,0,0.0951134,"Missing"
2020.crac-1.4,P19-1450,1,0.921324,"the SpanBERT version of AllenNLP achieves the best results in all metrics. 4 AMR parsing with coreference Coreference is not an isolated task in MS-AMR parsing; in order to predict the gold annotations, coreference information needs to be incorporated into AMR graphs predicted by a semantic parser. We thus 35 macro-average: micro-average: AMR parser P R F 0.57 0.52 0.54 0.57 0.50 0.53 AMR parser + AllenNLP P R F 0.61 0.54 0.57 0.60 0.53 0.56 AMR parser + oracle P R F 0.63 0.56 0.59 0.63 0.55 0.58 Table 3: Smatch evaluation of document-level coreference annotations. extended the AMR parser of Lindemann et al. (2019) with coreference information. First, we prepared gold annotations at the document level. For this, we combined the individual AMRs from each document into a single graph to represent document-level annotations. The coreference chains were extracted from the gold annotations of the MS-AMR corpus, and coreferent nodes in the document graph were merged following the procedure described in (O’Gorman et al., 2018). Second, we ran Lindemann’s parser on each sentence separately and combined the predicted AMR graphs into a document-level graph. Then we ran SpanBERT AllenNLP (henceforth just AllenNLP)"
2020.crac-1.4,C18-1313,0,0.171972,"Missing"
2020.crac-1.4,D14-1162,0,0.0832495,"Missing"
2020.emnlp-main.675,W09-0628,0,0.0440762,"Missing"
2020.emnlp-main.675,N13-1055,0,0.0341949,"Missing"
2020.emnlp-main.675,C12-1044,0,0.0698123,"Missing"
2020.emnlp-main.675,D13-1055,0,0.0204727,"Wikipedia Revisions. Revisions in Wikipedia have been leveraged for various NLP tasks, such as spelling error correction (Ehsan and Faili, 2013; Grundkiewicz and Junczys-Dowmunt, 2014; Zesch, 2012), preposition error correction (Cahill et al., 2013), paraphrasing (Max and Wisniewski, 2010), sentence simplification and compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), textual entailment recognition (Zanzotto and Pennacchiotti, 2010) and lexical simplification (Yatskar et al., 2010). Within this framework, a number of studies have analyzed the type of edits that authors made (Daxenberger and Gurevych, 2013, 2012; Faruqui et al., 2018; Pfeil et al., 2006; Bronner and Monz, 2012; Liu and Ram, 2011) and their intentions (Yang et al., 2017; Zhang and Litman, 2016). These studies built further upon Faigley and Witte (1981) and Jones (2008). Daxenberger and Gurevych (2013) and Yang et al. (2017) performed multi-class classification to automati4 We used the error types defined in the CoNLL-2013 shared task (Ng et al., 2013) cally detect edit types and edit intentions respectively. Other text classification studies focused on a smaller set of revision intentions in Wikipedia, such as Recasens et al. (2"
2020.emnlp-main.675,D19-1457,0,0.0282662,"ast et al., 2008) and between factual and fluency edits (Fong and Biuk-Aghai, 2010) wikiHow Revisions. Compared to Wikipedia revisions, wikiHow has received less attention in NLP. Apart from (Anthonio et al., 2020), there is no other work that leveraged the revision history of wikiHow articles. However, wikiHow has been used for summarization (Koupaee and Wang, 2018) and knowledge acquisition (Chu et al., 2017; Zhou et al., 2019). Others have also employed it to model procedure-specific relationships in sentences (Park and Motahari Nezhad, 2018) and underlying reasons for these relationships (Mishra et al., 2019). Related Tasks. Afrin and Litman (2018), in a related task, worked with revisions in argumentative essays from ArgRewrite (Zhang et al., 2017). The authors trained a RandomForest classifier to predict, given an original sentence and a revised one, if the revised sentence is better than the original. Tan and Lee (2014) conducted a related study, which analyzed potential strength differences in original– revised sentence pairs in academic writing using a qualitative approach. 6 Conclusions We demonstrated in an experimental comparison that it is easier to distinguish sentence versions computati"
2020.emnlp-main.675,W13-3601,0,0.0310065,"Pennacchiotti, 2010) and lexical simplification (Yatskar et al., 2010). Within this framework, a number of studies have analyzed the type of edits that authors made (Daxenberger and Gurevych, 2013, 2012; Faruqui et al., 2018; Pfeil et al., 2006; Bronner and Monz, 2012; Liu and Ram, 2011) and their intentions (Yang et al., 2017; Zhang and Litman, 2016). These studies built further upon Faigley and Witte (1981) and Jones (2008). Daxenberger and Gurevych (2013) and Yang et al. (2017) performed multi-class classification to automati4 We used the error types defined in the CoNLL-2013 shared task (Ng et al., 2013) cally detect edit types and edit intentions respectively. Other text classification studies focused on a smaller set of revision intentions in Wikipedia, such as Recasens et al. (2013) who worked on bias/nonbias detection. Attention has also been given to distinguish between vandalism and non-vandalism (Adler et al., 2011; Harpalani et al., 2011; Potthast et al., 2008) and between factual and fluency edits (Fong and Biuk-Aghai, 2010) wikiHow Revisions. Compared to Wikipedia revisions, wikiHow has received less attention in NLP. Apart from (Anthonio et al., 2020), there is no other work that l"
2020.emnlp-main.675,P13-1162,0,0.0115562,"and Gurevych, 2013, 2012; Faruqui et al., 2018; Pfeil et al., 2006; Bronner and Monz, 2012; Liu and Ram, 2011) and their intentions (Yang et al., 2017; Zhang and Litman, 2016). These studies built further upon Faigley and Witte (1981) and Jones (2008). Daxenberger and Gurevych (2013) and Yang et al. (2017) performed multi-class classification to automati4 We used the error types defined in the CoNLL-2013 shared task (Ng et al., 2013) cally detect edit types and edit intentions respectively. Other text classification studies focused on a smaller set of revision intentions in Wikipedia, such as Recasens et al. (2013) who worked on bias/nonbias detection. Attention has also been given to distinguish between vandalism and non-vandalism (Adler et al., 2011; Harpalani et al., 2011; Potthast et al., 2008) and between factual and fluency edits (Fong and Biuk-Aghai, 2010) wikiHow Revisions. Compared to Wikipedia revisions, wikiHow has received less attention in NLP. Apart from (Anthonio et al., 2020), there is no other work that leveraged the revision history of wikiHow articles. However, wikiHow has been used for summarization (Koupaee and Wang, 2018) and knowledge acquisition (Chu et al., 2017; Zhou et al., 20"
2020.emnlp-main.675,W11-2845,0,0.0897034,"Missing"
2020.emnlp-main.675,P14-2066,0,0.371515,"used for summarization (Koupaee and Wang, 2018) and knowledge acquisition (Chu et al., 2017; Zhou et al., 2019). Others have also employed it to model procedure-specific relationships in sentences (Park and Motahari Nezhad, 2018) and underlying reasons for these relationships (Mishra et al., 2019). Related Tasks. Afrin and Litman (2018), in a related task, worked with revisions in argumentative essays from ArgRewrite (Zhang et al., 2017). The authors trained a RandomForest classifier to predict, given an original sentence and a revised one, if the revised sentence is better than the original. Tan and Lee (2014) conducted a related study, which analyzed potential strength differences in original– revised sentence pairs in academic writing using a qualitative approach. 6 Conclusions We demonstrated in an experimental comparison that it is easier to distinguish sentence versions computationally in wikiHowToImprove than in WikiAtomicEdits. We further introduced a new task of predicting whether a sentence requires revision and showed promising first results on specific types of revisions. As next steps, we plan to address further types of revisions and extend our experiments to document-level settings. A"
2020.emnlp-main.675,W18-5446,0,0.079669,"Missing"
2020.emnlp-main.675,P08-2035,0,0.122599,"Missing"
2020.emnlp-main.675,D17-1213,0,0.0187857,"Grundkiewicz and Junczys-Dowmunt, 2014; Zesch, 2012), preposition error correction (Cahill et al., 2013), paraphrasing (Max and Wisniewski, 2010), sentence simplification and compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), textual entailment recognition (Zanzotto and Pennacchiotti, 2010) and lexical simplification (Yatskar et al., 2010). Within this framework, a number of studies have analyzed the type of edits that authors made (Daxenberger and Gurevych, 2013, 2012; Faruqui et al., 2018; Pfeil et al., 2006; Bronner and Monz, 2012; Liu and Ram, 2011) and their intentions (Yang et al., 2017; Zhang and Litman, 2016). These studies built further upon Faigley and Witte (1981) and Jones (2008). Daxenberger and Gurevych (2013) and Yang et al. (2017) performed multi-class classification to automati4 We used the error types defined in the CoNLL-2013 shared task (Ng et al., 2013) cally detect edit types and edit intentions respectively. Other text classification studies focused on a smaller set of revision intentions in Wikipedia, such as Recasens et al. (2013) who worked on bias/nonbias detection. Attention has also been given to distinguish between vandalism and non-vandalism (Adler e"
2020.emnlp-main.675,C10-1137,0,0.0447428,"Missing"
2020.emnlp-main.675,N10-1056,0,0.107811,"Missing"
2020.emnlp-main.675,N16-1042,0,0.0421349,"Missing"
2020.emnlp-main.675,W10-3504,0,0.05187,"Missing"
2020.emnlp-main.675,E12-1054,0,0.0514377,"Missing"
2020.emnlp-main.675,P17-1144,0,0.106334,"has received less attention in NLP. Apart from (Anthonio et al., 2020), there is no other work that leveraged the revision history of wikiHow articles. However, wikiHow has been used for summarization (Koupaee and Wang, 2018) and knowledge acquisition (Chu et al., 2017; Zhou et al., 2019). Others have also employed it to model procedure-specific relationships in sentences (Park and Motahari Nezhad, 2018) and underlying reasons for these relationships (Mishra et al., 2019). Related Tasks. Afrin and Litman (2018), in a related task, worked with revisions in argumentative essays from ArgRewrite (Zhang et al., 2017). The authors trained a RandomForest classifier to predict, given an original sentence and a revised one, if the revised sentence is better than the original. Tan and Lee (2014) conducted a related study, which analyzed potential strength differences in original– revised sentence pairs in academic writing using a qualitative approach. 6 Conclusions We demonstrated in an experimental comparison that it is easier to distinguish sentence versions computationally in wikiHowToImprove than in WikiAtomicEdits. We further introduced a new task of predicting whether a sentence requires revision and sho"
2020.emnlp-main.675,N16-1168,0,0.0217238,"unczys-Dowmunt, 2014; Zesch, 2012), preposition error correction (Cahill et al., 2013), paraphrasing (Max and Wisniewski, 2010), sentence simplification and compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), textual entailment recognition (Zanzotto and Pennacchiotti, 2010) and lexical simplification (Yatskar et al., 2010). Within this framework, a number of studies have analyzed the type of edits that authors made (Daxenberger and Gurevych, 2013, 2012; Faruqui et al., 2018; Pfeil et al., 2006; Bronner and Monz, 2012; Liu and Ram, 2011) and their intentions (Yang et al., 2017; Zhang and Litman, 2016). These studies built further upon Faigley and Witte (1981) and Jones (2008). Daxenberger and Gurevych (2013) and Yang et al. (2017) performed multi-class classification to automati4 We used the error types defined in the CoNLL-2013 shared task (Ng et al., 2013) cally detect edit types and edit intentions respectively. Other text classification studies focused on a smaller set of revision intentions in Wikipedia, such as Recasens et al. (2013) who worked on bias/nonbias detection. Attention has also been given to distinguish between vandalism and non-vandalism (Adler et al., 2011; Harpalani et"
2020.emnlp-main.675,P16-2034,0,0.0173995,"Missing"
2020.emnlp-main.675,W19-5808,0,0.0211121,"s et al. (2013) who worked on bias/nonbias detection. Attention has also been given to distinguish between vandalism and non-vandalism (Adler et al., 2011; Harpalani et al., 2011; Potthast et al., 2008) and between factual and fluency edits (Fong and Biuk-Aghai, 2010) wikiHow Revisions. Compared to Wikipedia revisions, wikiHow has received less attention in NLP. Apart from (Anthonio et al., 2020), there is no other work that leveraged the revision history of wikiHow articles. However, wikiHow has been used for summarization (Koupaee and Wang, 2018) and knowledge acquisition (Chu et al., 2017; Zhou et al., 2019). Others have also employed it to model procedure-specific relationships in sentences (Park and Motahari Nezhad, 2018) and underlying reasons for these relationships (Mishra et al., 2019). Related Tasks. Afrin and Litman (2018), in a related task, worked with revisions in argumentative essays from ArgRewrite (Zhang et al., 2017). The authors trained a RandomForest classifier to predict, given an original sentence and a revised one, if the revised sentence is better than the original. Tan and Lee (2014) conducted a related study, which analyzed potential strength differences in original– revise"
2020.lrec-1.702,C12-1044,0,0.0592838,"r own WikiEdits corpus. The task of these models is to generate a phrase which would be appropriate to insert into a sentence at a specific position. Their results show that a language model trained on article edits is more successful in proposing phrases that capture the same discourse function as human insertions than a language model trained on Wikipedia more generally. Faruqui et al. (2018) concluded that the supervision provided by article edits encodes aspects of language distinct from non-edited text. Another study which uses the revision history of the English Wikipedia is the work of Daxenberger and Gurevych (2012). They build a corpus of 1,995 edits from 891 article revisions from English Wikipedia texts and propose a 21category classification scheme of edit types. The categories are classified into three top layers: • Wikipedia Policy: invalid edits as defined by internal Wikipedia Policies and respective defense mechanisms (e.g. VANDALISM). • Surface Edits: edits not affecting the meaning of the text (e.g. S PELLING /G RAMMAR). • Text-Base: edits affecting the meaning of the text (e.g. I NFORMATION -I NSERT). Three annotators annotated the data and obtained an agreement in terms of Krippendorff’s alp"
2020.lrec-1.702,D13-1055,0,0.0209072,"one of the revision types. We describe the former in Section 5.1 and the latter in Section 5.2. In Section 5.3, we summarize our findings and provide a brief discussion. 5.1. Revision Type Categorization Our initial categorization consists of four categories, based on a manual inspection of the sample of 100 pairs of base and revised versions of a sentence. We provide examples for each category in Table 5. Since we aim to explore the proportion of edits that have likely led to improved instructions, we are mainly interested in the samples categorized as Information Modification/Insertion. In Daxenberger and Gurevych (2013), such edits are called text-base edits. They are of particular interest in this work because they may clarify information from the base version of a sentence. Task. Since we are mainly interested in instances categorized as Information Modification/Insertion, we designed an intuitive task which should help participants to differentiate between Information Modification/Insertion and the Figure 1: A screenshot of the interface, showing the base version (Text A) and the revised version of a sentence (Text B). The button ‘show changes’ highlights differences between both versions. The lower half"
2020.lrec-1.702,D18-1028,0,0.0751101,"this section, we present studies conducted within two related lines of research. In Section 2.1, we discuss previous work on revisions in the English Wikipedia. Currently available wikiHow corpora are described in Section 2.2. 3 Data and code are available here: https://github. com/irshadbhat/wikiHowToImprove 5721 2.1. Revisions in English Wikipedia There are a number of studies on revision histories from Wikipedia articles for various NLP tasks, such as sentence simplification and linguistic bias detection (Recasens et al., 2013). A study particularly similar to ours has been carried out by Faruqui et al. (2018) on Wikipedia articles. Faruqui et al. investigate differences between phrases inserted during a revision from the general language observed in Wikipedia texts. They approach this task through annotation experiments and linguistic analyses. The latter revealed that nouns, adjectives and adverbs occur considerably more often in edited, inserted text than non-edited text. In their computational experiments, Faruqui et al. (2018) model and analyze edits that insert information through language models based on sequence-to-sequence methods: one trained on Wikipedia texts and one trained on their ow"
2020.lrec-1.702,L18-1550,0,0.0213974,"that uses simple n-gram (n = 1, 2) features. The other type of model is based on long short-term memory (LSTM) networks, which make it possible to model sequential dependencies. LSTM Details. We implement each LSTM-based model as a bidirectional LSTM network with an additional attention layer (Zhou et al., 2016), as illustrated in Figure 2. We use two 256-dimensional stacked BiLSTMs with a 128dimensional attention layer on top to encode contextual information spread across the sentence. The input layer of the BiLSTM network is initialized with 300-dimensional pretrained FastText word vectors (Grave et al., 2018). The attention layer takes the BiLSTM hidden representations as input and returns their weighted sum as a embedding vector Seek out of the advice of older relatives. Seek out of the advice from older relatives. Depress the clutch fully. Release the clutch fully. Keep water and food away from your laptop. Keep drinks and food away from your laptop. Table 9: Example pairs of versions where the baseline fails, but the LSTM-based models assigns labels correctly. for the full sentence. For the classification model, the output layer uses a 128-dimensional feed-forward neural network with a softmax"
2020.lrec-1.702,C02-1150,0,0.100861,"asked one annotator to label 100 additional base–revised sentence pairs. This way, we collected 51 additional questions, for a total of N =111. We first provide an analysis of these questions in Section 5.2.1. We then investigate in Section 5.2.2 in how far differences in potential answers to a question reflect improvements between the base version of a sentence and its revised version. 5.2.1. Question Categorization In a first step, we categorized the 111 collected questions into semantic classes. For this purpose, one of the authors annotated the questions using the classification scheme of Li and Roth (2002). This classification scheme categorizes questions according to the type of answer required (i.e., the information added or modified in a revised version). The different classes and their frequencies are shown in Table 7. As indicated by the numbers, most questions are clas8 If both annotators provided questions, we pick one randomly. Main Class Subclass DESCRIPTION (N =72) Manner (N =55), Reason (N =8) Description (N =9) ENTITY (N =23) Other (N =8) Substance (N =1) Creative (N =1), Product (N =4) Food (N =1), Event (N =6) Disease and Medicine (N =2) NUMERIC (N =7) Period (N =2), Other (N =2)"
2020.lrec-1.702,P02-1040,0,0.106999,"Missing"
2020.lrec-1.702,P13-1162,0,0.0592697,"ark models that distinguish different versions of a sentence (§6). 2. Related Work In this section, we present studies conducted within two related lines of research. In Section 2.1, we discuss previous work on revisions in the English Wikipedia. Currently available wikiHow corpora are described in Section 2.2. 3 Data and code are available here: https://github. com/irshadbhat/wikiHowToImprove 5721 2.1. Revisions in English Wikipedia There are a number of studies on revision histories from Wikipedia articles for various NLP tasks, such as sentence simplification and linguistic bias detection (Recasens et al., 2013). A study particularly similar to ours has been carried out by Faruqui et al. (2018) on Wikipedia articles. Faruqui et al. investigate differences between phrases inserted during a revision from the general language observed in Wikipedia texts. They approach this task through annotation experiments and linguistic analyses. The latter revealed that nouns, adjectives and adverbs occur considerably more often in edited, inserted text than non-edited text. In their computational experiments, Faruqui et al. (2018) model and analyze edits that insert information through language models based on sequ"
2020.lrec-1.702,P16-2034,0,0.0150935,"as incorrect. In line with this evaluation setup, we train a pairwise ranking model on pairs of versions hsa , sb i with the objective of learning to rank sb higher than sa . We train two types of classification models: As a baseline for this task, we train a Multinomial Naive Bayes classification model that uses simple n-gram (n = 1, 2) features. The other type of model is based on long short-term memory (LSTM) networks, which make it possible to model sequential dependencies. LSTM Details. We implement each LSTM-based model as a bidirectional LSTM network with an additional attention layer (Zhou et al., 2016), as illustrated in Figure 2. We use two 256-dimensional stacked BiLSTMs with a 128dimensional attention layer on top to encode contextual information spread across the sentence. The input layer of the BiLSTM network is initialized with 300-dimensional pretrained FastText word vectors (Grave et al., 2018). The attention layer takes the BiLSTM hidden representations as input and returns their weighted sum as a embedding vector Seek out of the advice of older relatives. Seek out of the advice from older relatives. Depress the clutch fully. Release the clutch fully. Keep water and food away from"
2021.codi-main.6,2020.lrec-1.702,1,0.735956,"Workshop on Computational Approaches to Discourse, pages 58–71 November 10–11, 2021. ©2021 Association for Computational Linguistics 3. The follow-up context Cf , which contains the remaining tokens of the original/revised sentence to ensure that the reference fits into the sentence grammatically (in the example, within 1 or 2 days needs to fit after Use ). 6,014 instances of implicit references, which we extracted automatically by comparing different versions of articles in wikiHow1 . In practice, we make use of an existing resource of wikiHow sentences and revisions called wikiHowToImprove (Anthonio et al., 2020), from which we select specifically those cases in which a referring expression was inserted that refers to an entity mentioned in the preceding context. Based on this dataset, we set up a cloze task in which we evaluate the ability of computational models to generate references for insertions that occur naturally in publicly available texts. Finally, we analyze predictions of different modeling approaches as well as differences between model-generated and human-inserted references, which provide useful insights regarding potential weaknesses of existing models and potential causes of human mi"
2021.codi-main.6,J12-4003,0,0.0184568,"example, references play a crucial role in instructional texts as they provide answers to questions such as Which objects need to be used? If such references are not made explicitly, they might be clear to readers who have task-specific knowledge, but for others they might cause problems or misunderstandings. Resolving such implicit references could improve clarity and prevent problems in discourse processing when multiple interpretations exist. In natural language processing, implicit references have been handled as part of existing tasks such as semantic role labeling of implicit arguments (Gerber and Chai, 2012, cf. §3). Implicit arguments are generally hard to model computationally because they do not show up in easy to 58 Proceedings of the 2nd Workshop on Computational Approaches to Discourse, pages 58–71 November 10–11, 2021. ©2021 Association for Computational Linguistics 3. The follow-up context Cf , which contains the remaining tokens of the original/revised sentence to ensure that the reference fits into the sentence grammatically (in the example, within 1 or 2 days needs to fit after Use ). 6,014 instances of implicit references, which we extracted automatically by comparing different versi"
2021.codi-main.6,P10-1160,0,0.0506149,"by first sampling candidate reference tokens r1 . . . rL from a conditional probability distribution P (ri |Cp , r1 . . . ri−1 ) and then re-ranking the highest scoring candidates according to the full sequence probability P (Cp , r1 . . . rL , Cf ). Formulating the task in this way enables a direct application of language models and we demonstrate suitable baselines based on an auto-regressive language model in Section 5. 3 The task of resolving implicit references can be viewed as a modified version of implicit argument labeling. First studies on implicit argument labeling were conducted by Gerber and Chai (2010) and Ruppenhofer et al. (2009). Gerber and Chai (2010) collected a dataset by manually labeling implicit arguments of 10 different nominal predicates in NomBank (Meyers et al., 2004), yielding about 1,000 instances. Ruppenhofer et al. (2009) created a dataset through manual annotation of fictional text. Their dataset contains more different predicates than previous studies, but is smaller in size. More recent studies make use of the two datasets and attempted to create additional training data artificially (Silberer and Frank, 2012; Roth and Frank, 2013; Laparra and Rigau, 2013a,b; Chiarcos an"
2021.codi-main.6,N18-1076,0,0.0299977,"Missing"
2021.codi-main.6,2020.aacl-main.82,0,0.0379224,", 2015). Many of them are based on co-reference and discourse salience, which we also use for our baselines. Schenk and Chiarcos (2016) propose an unsupervised approach by aligning implicit arguments to semantic role labeling annotated data. Cheng and Erk (2019, 2018) generated large amounts of training data automatically using co-reference resolution. They also build a neural model based on argument fillers that occur multiple times in a narrative event chain. Finally, there are also datasets with domain-specific annotations such as geographic-event roles (Ebner et al., 2020) and on recipes (Jiang et al., 2020). Another closely related task is zero anaphora resolution, which has been extensively studied in pro-drop languages such as Chinese (Yeh and Chen, • We propose a new task that requires NLP models to generate explicit references to resolve cases of implicit language (§2). • We provide a dataset of 6,014 texts that involve the insertion of an explicit reference according to the text’s revision history (§4). • We show that methods based on the Generative Pre-trained Transformer model (GPT) present a strong baseline for this task (§5). • We conduct two analyses that shed light on the strengths of"
2021.codi-main.6,W15-4626,0,0.0149016,"Chai (2010) and Ruppenhofer et al. (2009). Gerber and Chai (2010) collected a dataset by manually labeling implicit arguments of 10 different nominal predicates in NomBank (Meyers et al., 2004), yielding about 1,000 instances. Ruppenhofer et al. (2009) created a dataset through manual annotation of fictional text. Their dataset contains more different predicates than previous studies, but is smaller in size. More recent studies make use of the two datasets and attempted to create additional training data artificially (Silberer and Frank, 2012; Roth and Frank, 2013; Laparra and Rigau, 2013a,b; Chiarcos and Schenk, 2015). Many of them are based on co-reference and discourse salience, which we also use for our baselines. Schenk and Chiarcos (2016) propose an unsupervised approach by aligning implicit arguments to semantic role labeling annotated data. Cheng and Erk (2019, 2018) generated large amounts of training data automatically using co-reference resolution. They also build a neural model based on argument fillers that occur multiple times in a narrative event chain. Finally, there are also datasets with domain-specific annotations such as geographic-event roles (Ebner et al., 2020) and on recipes (Jiang e"
2021.codi-main.6,N16-1173,0,0.018262,"10 different nominal predicates in NomBank (Meyers et al., 2004), yielding about 1,000 instances. Ruppenhofer et al. (2009) created a dataset through manual annotation of fictional text. Their dataset contains more different predicates than previous studies, but is smaller in size. More recent studies make use of the two datasets and attempted to create additional training data artificially (Silberer and Frank, 2012; Roth and Frank, 2013; Laparra and Rigau, 2013a,b; Chiarcos and Schenk, 2015). Many of them are based on co-reference and discourse salience, which we also use for our baselines. Schenk and Chiarcos (2016) propose an unsupervised approach by aligning implicit arguments to semantic role labeling annotated data. Cheng and Erk (2019, 2018) generated large amounts of training data automatically using co-reference resolution. They also build a neural model based on argument fillers that occur multiple times in a narrative event chain. Finally, there are also datasets with domain-specific annotations such as geographic-event roles (Ebner et al., 2020) and on recipes (Jiang et al., 2020). Another closely related task is zero anaphora resolution, which has been extensively studied in pro-drop languages"
2021.codi-main.6,W04-2705,0,0.12861,"rding to the full sequence probability P (Cp , r1 . . . rL , Cf ). Formulating the task in this way enables a direct application of language models and we demonstrate suitable baselines based on an auto-regressive language model in Section 5. 3 The task of resolving implicit references can be viewed as a modified version of implicit argument labeling. First studies on implicit argument labeling were conducted by Gerber and Chai (2010) and Ruppenhofer et al. (2009). Gerber and Chai (2010) collected a dataset by manually labeling implicit arguments of 10 different nominal predicates in NomBank (Meyers et al., 2004), yielding about 1,000 instances. Ruppenhofer et al. (2009) created a dataset through manual annotation of fictional text. Their dataset contains more different predicates than previous studies, but is smaller in size. More recent studies make use of the two datasets and attempted to create additional training data artificially (Silberer and Frank, 2012; Roth and Frank, 2013; Laparra and Rigau, 2013a,b; Chiarcos and Schenk, 2015). Many of them are based on co-reference and discourse salience, which we also use for our baselines. Schenk and Chiarcos (2016) propose an unsupervised approach by al"
2021.codi-main.6,D08-1055,0,0.0112126,"definition. We formally define the task of resolving implicit references as a generation task that requires the prediction of a reference S, given: 1. The original/revised sentence and its preceding context Cp , which includes at least one mention that co-refers to the correct reference (for the example shown in Table 1: Place the ground beef in a microwave. Microwave until it finishes thawing. Use ). 2. The number of tokens L of the reference to be generated according to the final version of a sentence (in case of the example: 2). 1 Related Work http://www.wikihow.org 59 2003) and Japanese (Taira et al., 2008; Isozaki and Hirao, 2003; Seki et al., 2002; Nakaiwa, 1997; Imamura et al., 2009). A closely related study to ours is Imamura et al. (2009), who used language model probabilities as features. As a commonality, previous work addresses semantic arguments of predicates that are realized outside a local syntactic scope. Our definition of implicit references subsumes such arguments, with the main difference that our task does not require the type of an argument or its semantic role to be specified. As a consequence, references in our task can fill one, none or multiple roles of different predicate"
2021.codi-main.6,N03-2024,0,0.167354,"ized outside a local syntactic scope. Our definition of implicit references subsumes such arguments, with the main difference that our task does not require the type of an argument or its semantic role to be specified. As a consequence, references in our task can fill one, none or multiple roles of different predicates. Once the correct reference has been identified, our task additionally requires the generation of a referring expression. This task has been addressed separately in previous work, for instance, using rule-based approaches (Reiter and Dale, 2000), feature-based machine learning (Nenkova and McKeown, 2003; Greenbacker and McCoy, 2009; Same and van Deemter, 2020; Kibrik et al., 2016), and deep neural networks (Castro Ferreira et al., 2016; Cao and Cheung, 2019). 4 sentence was created by inserting a word or contiguous set of words in the original sentence. In other words, eliminating the insertion from the revised sentence yields the original sentence. This is a logical starting point, as the implicit reference in the original sentence can be verbalized through insertion. We find cases with contiguous insertions in wikiHowToImprove by computing the differences between the original and revised s"
2021.codi-main.6,Y03-1047,0,0.129156,"Missing"
2021.codi-main.6,S13-1043,1,0.716956,"it argument labeling were conducted by Gerber and Chai (2010) and Ruppenhofer et al. (2009). Gerber and Chai (2010) collected a dataset by manually labeling implicit arguments of 10 different nominal predicates in NomBank (Meyers et al., 2004), yielding about 1,000 instances. Ruppenhofer et al. (2009) created a dataset through manual annotation of fictional text. Their dataset contains more different predicates than previous studies, but is smaller in size. More recent studies make use of the two datasets and attempted to create additional training data artificially (Silberer and Frank, 2012; Roth and Frank, 2013; Laparra and Rigau, 2013a,b; Chiarcos and Schenk, 2015). Many of them are based on co-reference and discourse salience, which we also use for our baselines. Schenk and Chiarcos (2016) propose an unsupervised approach by aligning implicit arguments to semantic role labeling annotated data. Cheng and Erk (2019, 2018) generated large amounts of training data automatically using co-reference resolution. They also build a neural model based on argument fillers that occur multiple times in a narrative event chain. Finally, there are also datasets with domain-specific annotations such as geographic-"
2021.eacl-srw.5,E14-4007,0,0.0193631,". More recently, Anthonio et al. (2020) performed a similar categorization on the revisions in WikiHow. Traditional computational analyses of vague statements have been based on logical representations (DeVault and Stone, 2004; Tang, 2008). In contrast, our focus is on vagueness in terms of lexical changes in revisions, which is more similar to previous analyses that considered the contextdependent resolution of vague expressions such as colour references (Meo et al., 2014). Other computational approaches to vagueness include, the detection of vague sense definitions in ontological resources (Alexopoulos and Pavlopoulos, 2014) and website privacy policies (Lebanoff and Liu, 2018) as well as the verification of historical documents (Vertan, 2019). Our approach to identifying and classifying vagueness is analyzed using FrameNet frames which provide specialized relations among conceptual categories, in a manner similar to recent advances in neural models that use sentence-level information to perform hyponymy–hypernymy classification. Roller et al. (2018) analyzes lexicosyntactic pattern-based instances of word-specific hypernymy-hyponymy constructions. Snow et al. (2004) explores the extraction of predefined patterns"
2021.eacl-srw.5,2020.lrec-1.702,1,0.764211,"h the two versions. We create and analyze the dataset by extracting relevant instances from wikiHowToImIntroduction Instructional texts aim to describe the actions necessary to accomplish a task or goal, in as clear and concise a manner as possible. WikiHow1 is an extensive compendium of instructional guides for various topics and domains. Any user may edit the articles, and WikiHow collates these revision histories. The edit history of such informal instructional articles is a source of user-generated data that can help identify possible reasons and necessities for editing. wikiHowToImprove (Anthonio et al., 2020) is a dataset that compiles revision histories for the analysis of linguistic phenomena that occur in edits of instructional texts, ranging from the correction of typos and grammatical errors to the clarification of ambiguity and vagueness. In this paper, we focus on cases of lexical vagueness, defined as “lexeme[s] with a single but nonspecific meaning” (Tuggy, 1993), which can potentially cause misunderstandings in instructional texts. Specifically, we study vagueness based on the change in the main verb in the original and revised version of an instruction. We say that an instruction was va"
2021.eacl-srw.5,P98-1013,0,0.308268,"are little to no additional edits (often just vandalism or spam). Data Creation, Preprocessing, and Analysis WikiHow articles mostly contain instructions, but also include descriptions, explanations, and other 2 3 31 https://pyenchant.github.io/ https://stanfordnlp.github.io/stanza/ 3.2 Verb Frame Analysis Total Train Test Val Usage Inheritance Subframe Other 15,243 13,166 9,481 3,925 11,084 9,179 6,835 2,833 2,194 2,008 1,720 649 1,965 1,793 926 443 Total 41,615 30,044 6,237 5,334 Relation We perform an analysis of verb frame relations from this extracted corpus using the FrameNet hierarchy (Baker et al., 1998). In order to identify evoked frames from the data, we use the INCEpTION Project’s neural FrameNet Tools parser4 (Klie et al., 2018; Markard and Klie, 2020). FrameNet Tools identifies the frame-evoking elements, the evoked frames, and the context elements’ roles in these frame for a given sentence. In this work, we ignore role assignments and only consider predictions of evoked frames, which we found to be generally reliable in our data.5 We extract the frame of the root verb in the original and revised sentences. For each pair, we identify the frame relation, if any, using the NLTK FrameNet A"
2021.eacl-srw.5,N18-1134,0,0.0217777,"Missing"
2021.eacl-srw.5,C04-1181,0,0.0203035,"tracted corpus consists of 41,615 sentences. Related Work Our paper focuses on revisions in wikiHow for a specific linguistic phenomenon, namely vagueness. The motivation to use revision histories as corpora for NLP tasks was introduced by Ferschke et al. (2013). The task of defining and categorizing edit intentions has been explored well for the Wikipedia edits corpus (Yang et al., 2016, 2017). More recently, Anthonio et al. (2020) performed a similar categorization on the revisions in WikiHow. Traditional computational analyses of vague statements have been based on logical representations (DeVault and Stone, 2004; Tang, 2008). In contrast, our focus is on vagueness in terms of lexical changes in revisions, which is more similar to previous analyses that considered the contextdependent resolution of vague expressions such as colour references (Meo et al., 2014). Other computational approaches to vagueness include, the detection of vague sense definitions in ontological resources (Alexopoulos and Pavlopoulos, 2014) and website privacy policies (Lebanoff and Liu, 2018) as well as the verification of historical documents (Vertan, 2019). Our approach to identifying and classifying vagueness is analyzed usi"
2021.eacl-srw.5,N19-1423,0,0.0193787,"Missing"
2021.eacl-srw.5,L18-1550,0,0.0269568,"Missing"
2021.eacl-srw.5,C18-2002,0,0.0654024,"Missing"
2021.eacl-srw.5,W19-9007,0,0.0154421,"vague statements have been based on logical representations (DeVault and Stone, 2004; Tang, 2008). In contrast, our focus is on vagueness in terms of lexical changes in revisions, which is more similar to previous analyses that considered the contextdependent resolution of vague expressions such as colour references (Meo et al., 2014). Other computational approaches to vagueness include, the detection of vague sense definitions in ontological resources (Alexopoulos and Pavlopoulos, 2014) and website privacy policies (Lebanoff and Liu, 2018) as well as the verification of historical documents (Vertan, 2019). Our approach to identifying and classifying vagueness is analyzed using FrameNet frames which provide specialized relations among conceptual categories, in a manner similar to recent advances in neural models that use sentence-level information to perform hyponymy–hypernymy classification. Roller et al. (2018) analyzes lexicosyntactic pattern-based instances of word-specific hypernymy-hyponymy constructions. Snow et al. (2004) explores the extraction of predefined patterns for hypernyms and hyponyms in the same sentence, while Shwartz et al. (2016) incorporates distributional methods for the"
2021.eacl-srw.5,D18-1387,0,0.0280148,"orization on the revisions in WikiHow. Traditional computational analyses of vague statements have been based on logical representations (DeVault and Stone, 2004; Tang, 2008). In contrast, our focus is on vagueness in terms of lexical changes in revisions, which is more similar to previous analyses that considered the contextdependent resolution of vague expressions such as colour references (Meo et al., 2014). Other computational approaches to vagueness include, the detection of vague sense definitions in ontological resources (Alexopoulos and Pavlopoulos, 2014) and website privacy policies (Lebanoff and Liu, 2018) as well as the verification of historical documents (Vertan, 2019). Our approach to identifying and classifying vagueness is analyzed using FrameNet frames which provide specialized relations among conceptual categories, in a manner similar to recent advances in neural models that use sentence-level information to perform hyponymy–hypernymy classification. Roller et al. (2018) analyzes lexicosyntactic pattern-based instances of word-specific hypernymy-hyponymy constructions. Snow et al. (2004) explores the extraction of predefined patterns for hypernyms and hyponyms in the same sentence, whil"
2021.eacl-srw.5,N18-1057,0,0.0265157,"ion. Some examples of vague and clarified instructions are provided in Table 1. As indicated by the examples, the revised verb is usually more specific in that it provides additional information on how or why an action needs to be taken. The classification of vague and clarified instructions is a first step towards automatic text editing for clarification based on linguistic criteria such as ambiguity and vagueness at a sentence level. Existing tools for text editing focus on text simplification and fact editing (Malmi et al., 2019), while others are designed for grammatical error correction (Xie et al., 2018). Our work acts as the first step towards automated editing based on linguistic criteria by identifying vague instructions and differentiating them from “clarified” ones. Our use of the wikiHowToImprove corpus also utilizes a resource of edit pairs, therefore introducing a new dataset for the linguistic study of vagueness as well as exploring the general versatility of such corpora. Our contributions are to create a dataset of vague and clarified instructions, provide an analysis based on semantic frames, and demonstrate the first results of a neural model’s ability to distinguish the two vers"
2021.eacl-srw.5,D19-1510,0,0.0253155,"wToImprove Dataset main verb is contextually more specific than the original version. Some examples of vague and clarified instructions are provided in Table 1. As indicated by the examples, the revised verb is usually more specific in that it provides additional information on how or why an action needs to be taken. The classification of vague and clarified instructions is a first step towards automatic text editing for clarification based on linguistic criteria such as ambiguity and vagueness at a sentence level. Existing tools for text editing focus on text simplification and fact editing (Malmi et al., 2019), while others are designed for grammatical error correction (Xie et al., 2018). Our work acts as the first step towards automated editing based on linguistic criteria by identifying vague instructions and differentiating them from “clarified” ones. Our use of the wikiHowToImprove corpus also utilizes a resource of edit pairs, therefore introducing a new dataset for the linguistic study of vagueness as well as exploring the general versatility of such corpora. Our contributions are to create a dataset of vague and clarified instructions, provide an analysis based on semantic frames, and demons"
2021.eacl-srw.5,L16-1206,0,0.018889,"evoked by the root verb in the original and revised version of an instruction (§3.2). The final extracted data consists of only those revisions where the root verb has been modified to be more specific to the sentence. This extracted corpus consists of 41,615 sentences. Related Work Our paper focuses on revisions in wikiHow for a specific linguistic phenomenon, namely vagueness. The motivation to use revision histories as corpora for NLP tasks was introduced by Ferschke et al. (2013). The task of defining and categorizing edit intentions has been explored well for the Wikipedia edits corpus (Yang et al., 2016, 2017). More recently, Anthonio et al. (2020) performed a similar categorization on the revisions in WikiHow. Traditional computational analyses of vague statements have been based on logical representations (DeVault and Stone, 2004; Tang, 2008). In contrast, our focus is on vagueness in terms of lexical changes in revisions, which is more similar to previous analyses that considered the contextdependent resolution of vague expressions such as colour references (Meo et al., 2014). Other computational approaches to vagueness include, the detection of vague sense definitions in ontological reso"
2021.eacl-srw.5,D17-1213,0,0.0479613,"Missing"
2021.eacl-srw.5,2020.acl-demos.14,0,0.0284451,"2016) incorporates distributional methods for their classification using sentence-level features. 3 3.1 Data Extraction and Cleaning wikiHowToImprove is a noisy source of data with misspellings, non-standard abbreviations, grammatical errors, emoticons, etc. In order to use the data for our task, we first perform some cleaning and preprocessing. We filter the typos and misspellings in the dataset by comparing all the vocabulary words to words in the English dictionary using the Enchant python API2 . After filtering the typos, we POS tag and dependency parse the data using the Stanza library3 (Qi et al., 2020). We discard all sentence pairs where the sentences are shorter than four or longer than 50 words. We then create a sub-corpus of instructional sentneces by extracting those edit pairs in which both the original and revised version of a sentence fulfill at least one of the following criteria: • imperative form—the root verb has no nominal subject (e.g. “Please finish the task”); • instructional indicative form—the nominal subject of the root verb is ‘you,’ ‘it’ or ‘one’ (e.g. “You should finish the task”); • passive form with ‘let’—the sentence is in passive voice, and the root verb is ‘let’ ("
2021.eacl-srw.5,P18-2057,0,0.0122659,"references (Meo et al., 2014). Other computational approaches to vagueness include, the detection of vague sense definitions in ontological resources (Alexopoulos and Pavlopoulos, 2014) and website privacy policies (Lebanoff and Liu, 2018) as well as the verification of historical documents (Vertan, 2019). Our approach to identifying and classifying vagueness is analyzed using FrameNet frames which provide specialized relations among conceptual categories, in a manner similar to recent advances in neural models that use sentence-level information to perform hyponymy–hypernymy classification. Roller et al. (2018) analyzes lexicosyntactic pattern-based instances of word-specific hypernymy-hyponymy constructions. Snow et al. (2004) explores the extraction of predefined patterns for hypernyms and hyponyms in the same sentence, while Shwartz et al. (2016) incorporates distributional methods for their classification using sentence-level features. 3 3.1 Data Extraction and Cleaning wikiHowToImprove is a noisy source of data with misspellings, non-standard abbreviations, grammatical errors, emoticons, etc. In order to use the data for our task, we first perform some cleaning and preprocessing. We filter the"
2021.eacl-srw.5,D17-2001,0,0.0173817,"order to identify evoked frames from the data, we use the INCEpTION Project’s neural FrameNet Tools parser4 (Klie et al., 2018; Markard and Klie, 2020). FrameNet Tools identifies the frame-evoking elements, the evoked frames, and the context elements’ roles in these frame for a given sentence. In this work, we ignore role assignments and only consider predictions of evoked frames, which we found to be generally reliable in our data.5 We extract the frame of the root verb in the original and revised sentences. For each pair, we identify the frame relation, if any, using the NLTK FrameNet API6 (Schneider and Wooters, 2017). We found that most edits could be categorized into one of the following frame relations between the frames evoked by the original and revised verb frames: Table 2: Number of sentences in the extracted dataset and distribution of FrameNet relations between original and revised verbs. We also show the distribution of train, test and validation for each frame relation. verb or could not assign a frame. For instance, the verb compel as in “you may feel compelled . . . ” is not in FrameNet. We categorize these instances, which are fewer in number than the other categories, under a single Other ca"
2021.eacl-srw.5,P16-1226,0,0.0490119,"Missing"
2021.unimplicit-1.4,2021.unimplicit-1.8,0,0.0426411,"Missing"
2021.unimplicit-1.4,2020.lrec-1.702,1,0.549696,"tences that remained unchanged over multiple article-level revisions (until the final available version) to not require clarification. Based on this assumption, we create a class-balanced data set for our task by selecting for each sentence that requires clarification exactly one sentence that does not require clarification. In the following, we provide details on the collection procedure and an annotation-based verification thereof as well as statistics of the final data set. 2.1 Data Collection We extract instances of clarifications from a resource of revision edits called wikiHowToImprove (Anthonio et al., 2020). Specifically, we used a state-of-the-art a constituency parser (Mrini et al., 2020) to preprocess all revisions from wikiHow28 Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language (UnImplicit 2021), pages 28–32 Bangkok, Thailand (online), August 5, 2021. ©2021 Association for Computational Linguistics Edit type Description Modifiers Insertion of an adverbial/adjectival modifier Example 7 Try watching one game to see if you like it. (→ Try watching one game alone to see if you like it.) 3 Learn about some teams. Pronouns Complements Replacement of a pronoun wi"
2021.unimplicit-1.4,2020.coling-main.117,1,0.702393,"Missing"
2021.unimplicit-1.4,2020.emnlp-main.675,1,0.399365,"Missing"
2021.unimplicit-1.4,2020.findings-emnlp.65,0,0.0183492,"ailable version) to not require clarification. Based on this assumption, we create a class-balanced data set for our task by selecting for each sentence that requires clarification exactly one sentence that does not require clarification. In the following, we provide details on the collection procedure and an annotation-based verification thereof as well as statistics of the final data set. 2.1 Data Collection We extract instances of clarifications from a resource of revision edits called wikiHowToImprove (Anthonio et al., 2020). Specifically, we used a state-of-the-art a constituency parser (Mrini et al., 2020) to preprocess all revisions from wikiHow28 Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language (UnImplicit 2021), pages 28–32 Bangkok, Thailand (online), August 5, 2021. ©2021 Association for Computational Linguistics Edit type Description Modifiers Insertion of an adverbial/adjectival modifier Example 7 Try watching one game to see if you like it. (→ Try watching one game alone to see if you like it.) 3 Learn about some teams. Pronouns Complements Replacement of a pronoun with a noun phrase Insertion of an optional verb complement Quantifier/ Modals Insertio"
C10-1108,H05-1042,0,0.0321602,"s automatically from corpus data. Previous work by Dale et al. (2005) and Roth and Frank (2009) on generating NL directions used hand-crafted heuristics. Duboue and McKeown (2003) were the first to model content selection as a machine learning task, in which selection rules are induced from pairs of human-written text and associated sets of database entries. They induce baseline selection rules from exact matches of NL expressions with database entries; in addition, classbased rules are computed by matching database entry types against NL expressions, using statistical co-occurrence clusters. Barzilay and Lapata (2005) incorporate the interplay between multiple events and entities when learning content selection rules using a special link function. Recent work by Liang et al. (2009) focuses on modeling grounded language, by aligning realworld representations with NL text that references corresponding world states. They show how a generative model can be used to segment text into utterances and to identify relevant facts with minimal supervision. Both tasks are handled jointly in a unified framework by training a hierarchical semi-Markov model on pairs of text and world states, thereby modeling sequencing ef"
C10-1108,W02-1022,0,0.0749619,"Missing"
C10-1108,J93-2003,0,0.0128356,"each component and each attribute: p(f, w|a) = p(f |a)λ(dist(f, a)) p(w|a) (1) The individual models are described in more detail in the following subsections. 4.1 Frame Alignment Model This basic frame alignment model specifies the probabilities p(f |a) for aligning an attribute a of type at (i.e., one of the types listed in Section 3.1) to a frame element f labeled as type ft . This alignment model is initialized as a uniform distribution over f and trained using a straight-forward implementation of the EM algorithm, following the well-known IBM Model 1 for alignment in machine translation (Brown et al., 1993). The expectation step (E-step) computes expected counts given occurrences of ft and at under the assumption that all alignments are independent 1:1 correspondences: P 0 0 {hf 0 ,a0 i|ft0 =ft ∧a0t =at } p(f |a ) P count(ft , at ) = 0 {hf 0 ,yi|ft0 =ft } p(f |y) (2) The probabilities are re-estimated to maximize the overall alignment probability by normalizing 962 the estimated counts (M-step): 4.2 count(ft , at ) p(f |a) = P x count(xt , at ) (3) Distance Model We hypothesize that the order of route directions tends to be consistent with the order of maneuvers encoded by the route representati"
C10-1108,burchardt-etal-2006-salto,1,0.836714,"cation that guided them along a route by means of a 2D animation. Subsequently they had to write NL route directions in German for the shown routes. The subjects were allowed to use all information displayed by the web application: named places, buildings, bridges and street names, etc. The resulting directions were POS-tagged with TreeTagger (Schmid, 1997), dependency-parsed with XLE (Maxwell and Kaplan, 1993), and manually revised. Additionally, we annotated framesemantic markup (Fillmore et al., 2003) and gold standard alignments to the route representation using the SALTO annotation tool (Burchardt et al., 2006). Frame semantic markup. The texts are annotated with an inventory of 4 frames relevant for directions (S ELF MOTION, P ERCEPTION, B E ING LOCATED , L OCATIVE RELATION ), with semantic roles (frame elements) such as DIREC TION , GOAL , PATH , LOCATION . Figure 2 illustrates a typical example for the use of the S ELF M OTION frame, once with the elements SOURCE and DIRECTION , and once with the elements DIRECTION and GOAL. Our alignment model uses the frame semantic annotation as structuring information. Gold standard alignments. For evaluation we constructed gold alignments. We asked two annot"
C10-1108,P09-1011,0,0.02701,"Missing"
C10-1108,J93-4001,0,0.0294045,"Directions The corpus of route directions used in this work is a subset of the data collected by Schuldes et al. (2009) in a desk-based experiment. To elicit NL route directions, subjects were shown a web application that guided them along a route by means of a 2D animation. Subsequently they had to write NL route directions in German for the shown routes. The subjects were allowed to use all information displayed by the web application: named places, buildings, bridges and street names, etc. The resulting directions were POS-tagged with TreeTagger (Schmid, 1997), dependency-parsed with XLE (Maxwell and Kaplan, 1993), and manually revised. Additionally, we annotated framesemantic markup (Fillmore et al., 2003) and gold standard alignments to the route representation using the SALTO annotation tool (Burchardt et al., 2006). Frame semantic markup. The texts are annotated with an inventory of 4 frames relevant for directions (S ELF MOTION, P ERCEPTION, B E ING LOCATED , L OCATIVE RELATION ), with semantic roles (frame elements) such as DIREC TION , GOAL , PATH , LOCATION . Figure 2 illustrates a typical example for the use of the S ELF M OTION frame, once with the elements SOURCE and DIRECTION , and once wit"
C10-1108,P09-4010,1,0.926595,"hat specify, for a given navigational task and an automatically computed route representation, a sequence of actions to be followed by the user to reach his or her goal. A corpusbased approach to generate route directions involves (i) the selection of elements along the route that need to be mentioned, and (ii) the induction of a mapping from route elements to linguistic structures that can be used as a basis for NL generation. Previously developed natural language generation (NLG) systems make use of simple heuristics for the task of content selection for route directions (Dale et al., 2005; Roth and Frank, 2009). In our work, we aim for a corpus-based approach that can be flexibly modeled after natural, humanproduced directions for varying subtasks (e.g., indoor vs. outdoor navigation), and that facilitates multilingual extensions. By employing salient landmarks and allowing for variation in NL realization, such a system is expected to generate natural sounding directions that are easier to memorize and easier to follow than directions given by a classical route planner or navigation system. This paper presents an Expectation-Maximization (EM) based algorithm that aligns geographical route representa"
C10-1108,W09-2814,1,0.921155,"t al., 1977) to learn correspondences between (segments of) the geographical representation of a route and linguistic instructions of how to follow this route in order to arrive at a designated goal. We are specifically interested in identifying which parts of a route are realized in natural language and which kinds of semantic constructions are used to express them. As a data source for inducing such correspondences we use a parallel corpus of route representations and corresponding route directions that were collected in a controlled experiment for navigation in an urban street network (cf. Schuldes et al. (2009)). For the alignment task, the routes were compiled to a specification format that has been realized in an internal version of an online route planner. Figure 1 displays the route rep959 Figure 1: A (partial) route representation of the route segment displayed on the right. resentation for a small route segment (a junction connecting ’Hauptstraße’ and ’Leyergasse’). The corresponding part of a NL route direction is displayed in Figure 2. The route representation and the NL direction share some common concepts: For example, both contain references to a landmark called “Sudpfanne” (marked as [1]"
C10-1108,E03-1026,0,0.0256458,"nt. As route directions are typically presented in a linear order with respect to the route, we incorporate an additional distance model λ in our alignment. We further account for word choice within a frame element as an additional factor. The word choice model p(w|a) will exploit attribute type and value information in the route representations that are reflected in word choice in the linguistic instructions. Both extensions are inspired by and share similarities with models that have been successfully applied in work on text alignment for the task of machine translation (Vogel et al., 1996; Tiedemann, 2003). Our full model is a distribution over frame elements f and words w that factorizes the three above mentioned parts under the assumption of independence between each component and each attribute: p(f, w|a) = p(f |a)λ(dist(f, a)) p(w|a) (1) The individual models are described in more detail in the following subsections. 4.1 Frame Alignment Model This basic frame alignment model specifies the probabilities p(f |a) for aligning an attribute a of type at (i.e., one of the types listed in Section 3.1) to a frame element f labeled as type ft . This alignment model is initialized as a uniform distri"
C10-1108,P10-1083,0,0.0273985,"ed as follows: In Section 2 we discuss related work. Section 3 introduces the task, and the representation formats and resources we use. Section 4 introduces a basic Expectation-Maximization model and two extensions for the alignment task. Section 5 outlines the experiments and presents the evaluation results. In Section 6 we conclude and discuss future work. 2 Related Work Various aspects of route directions have been subject of research in computational linguistics, ranging from instructional dialogues in MapTask (Anderson et al., 1991) to recent work on learning to follow route directions (Vogel and Jurafsky, 2010). However, little work has been done on generating NL directions based on data from Geographical Information Systems (Dale et al., 2005; Roth and Frank, 2009). NLG systems are typically realized as pipeline architectures (Reiter and Dale, 2000). As a first step, they compute a set of messages that represent the information to be conveyed to a user, given a specific communicative task (Content Selection). Selecting appropriate content for a task can be defined heuristically, by manually crafted rules or by learning content selection rules automatically from corpus data. Previous work by Dale et"
C10-1108,C96-2141,0,0.166103,"of direction alignment. As route directions are typically presented in a linear order with respect to the route, we incorporate an additional distance model λ in our alignment. We further account for word choice within a frame element as an additional factor. The word choice model p(w|a) will exploit attribute type and value information in the route representations that are reflected in word choice in the linguistic instructions. Both extensions are inspired by and share similarities with models that have been successfully applied in work on text alignment for the task of machine translation (Vogel et al., 1996; Tiedemann, 2003). Our full model is a distribution over frame elements f and words w that factorizes the three above mentioned parts under the assumption of independence between each component and each attribute: p(f, w|a) = p(f |a)λ(dist(f, a)) p(w|a) (1) The individual models are described in more detail in the following subsections. 4.1 Frame Alignment Model This basic frame alignment model specifies the probabilities p(f |a) for aligning an attribute a of type at (i.e., one of the types listed in Section 3.1) to a frame element f labeled as type ft . This alignment model is initialized a"
C10-1108,W03-1016,0,\N,Missing
C10-1108,W01-0100,0,\N,Missing
D08-1048,C96-1005,0,0.0284295,"W N is a WordNet-based similarity measure. In the following subsections we will describe how we build sub-graphs and model the similarity measure for the different part of speech. Figure 1 reports an excerpt of the noun subgraph for the frame P EOPLE BY AGE, covering the suitable senses of its nominal LUs {adult, baby, boy, kid, youngster, youth}. The relevant senses (e.g. sense 1 of youth out of the 6 potential ones) are generally selected, as they share the most specific generalizations in WordNet with the other words. Nouns. To compute similarity for nouns we adopt conceptual density (cd) (Agirre and Rigau, 1996), a semantic similarity model previously applied to word sense disambiguation tasks. Given a frame f and its set of nominal lexical units Fn , the nominal subgraph Sfn is built as follows. All senses of all words in Fn are activated in WordNet. All hypernyms Hfn of these senses are then retrieved. Every synset σ ∈ Hfn is given a cd score, representing the density of the WordNet subhierarchy rooted at σ in representing the set of nouns Fn . The intuition behind this model is that the larger the number of LUs in Fn that are generalized by σ is, the better it captures the lexical semantics intend"
D08-1048,P98-1013,0,0.460895,"lexical units. Experimental results show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined. 1 Introduction Most inference-based NLP tasks require a large amount of semantic knowledge at the predicateargument level. This type of knowledge allows to identify meaning-preserving transformations, such as active/passive, verb alternations and nominalizations, which are crucial in several linguistic inferences. Recently, the integration of NLP systems with manually-built resources at the predicate argument-level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) has received growing interest. For example, Shen and Lapata (2007) show the potential improvement that FrameNet can bring on the performance of a Question Answering (QA) system. Similarly, several other studies (e.g. (Bar-Haim et al., 2005; Garoufi, 2007)) indicate that frame semantics plays a central role in Recognizing Textual Entailment (RTE). Unfortunately, most attempts to integrate FrameNet or similar resources in QA and RTE systems have so far failed, as reviewed respectively in (Shen and Lapata, 2007) and (Burchardt and Frank, 2006). These studies in"
D08-1048,S07-1018,0,0.262501,"Missing"
D08-1048,W05-1210,0,0.0161983,"ment level. This type of knowledge allows to identify meaning-preserving transformations, such as active/passive, verb alternations and nominalizations, which are crucial in several linguistic inferences. Recently, the integration of NLP systems with manually-built resources at the predicate argument-level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) has received growing interest. For example, Shen and Lapata (2007) show the potential improvement that FrameNet can bring on the performance of a Question Answering (QA) system. Similarly, several other studies (e.g. (Bar-Haim et al., 2005; Garoufi, 2007)) indicate that frame semantics plays a central role in Recognizing Textual Entailment (RTE). Unfortunately, most attempts to integrate FrameNet or similar resources in QA and RTE systems have so far failed, as reviewed respectively in (Shen and Lapata, 2007) and (Burchardt and Frank, 2006). These studies indicate limited coverage as the main reason of insuccess. Indeed, the FrameNet database only contains 10,000 lexical units (LUs), far less than the 210,000 entries in WordNet 3.0. Also, frames are based on more complex information than word senses, so that their manual develo"
D08-1048,basili-etal-2004-similarity,1,0.858067,"Missing"
D08-1048,burchardt-etal-2006-salsa,0,0.0409171,"Missing"
D08-1048,J93-1003,0,0.0753841,"esent results for the following models and parametrizations (further parametrizations have revealed comparable performance). Dist-word : the word-based space described in Section 3. Contextual features correspond to the set of the 4,000 most frequent words in the BNC.4 The association measure between LUs and contexts is the pointwise mutual information. Valid contexts for LUs are fixed to a 20-window. Dist-syntax : the syntax-based space described in Section 3. Context features are the 10,000 most frequent syntactic relations in the BNC5 . As association measure we apply log-likelihood ratio (Dunning, 1993) to normalized frequency. Syntactic relations are extracted using the Minipar parser. Dist-mixed : the mixed space described in Sec3 In the distributional model, we recompute the centroids for each frame f in which the LU appeared, applying Eq. 2 to the set F − {l}. 4 We didn’t use the FrameNet corpus directly, as it is too small to obtain reliable statistics. 5 Specifically, we use the minimum context selection function and the plain path value function described in Pado (2007). tion 3. As for the Dist-word model, contextual features are 4,000 and pointwise mutual information is the associati"
D08-1048,J05-1004,0,0.0799121,"ts show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined. 1 Introduction Most inference-based NLP tasks require a large amount of semantic knowledge at the predicateargument level. This type of knowledge allows to identify meaning-preserving transformations, such as active/passive, verb alternations and nominalizations, which are crucial in several linguistic inferences. Recently, the integration of NLP systems with manually-built resources at the predicate argument-level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) has received growing interest. For example, Shen and Lapata (2007) show the potential improvement that FrameNet can bring on the performance of a Question Answering (QA) system. Similarly, several other studies (e.g. (Bar-Haim et al., 2005; Garoufi, 2007)) indicate that frame semantics plays a central role in Recognizing Textual Entailment (RTE). Unfortunately, most attempts to integrate FrameNet or similar resources in QA and RTE systems have so far failed, as reviewed respectively in (Shen and Lapata, 2007) and (Burchardt and Frank, 2006). These studies indicate limited coverage as the main"
D08-1048,J98-1004,0,0.354058,"Missing"
D08-1048,D07-1002,0,0.198823,"good level of accuracy and coverage, especially when combined. 1 Introduction Most inference-based NLP tasks require a large amount of semantic knowledge at the predicateargument level. This type of knowledge allows to identify meaning-preserving transformations, such as active/passive, verb alternations and nominalizations, which are crucial in several linguistic inferences. Recently, the integration of NLP systems with manually-built resources at the predicate argument-level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) has received growing interest. For example, Shen and Lapata (2007) show the potential improvement that FrameNet can bring on the performance of a Question Answering (QA) system. Similarly, several other studies (e.g. (Bar-Haim et al., 2005; Garoufi, 2007)) indicate that frame semantics plays a central role in Recognizing Textual Entailment (RTE). Unfortunately, most attempts to integrate FrameNet or similar resources in QA and RTE systems have so far failed, as reviewed respectively in (Shen and Lapata, 2007) and (Burchardt and Frank, 2006). These studies indicate limited coverage as the main reason of insuccess. Indeed, the FrameNet database only contains 1"
D08-1048,C98-1013,0,\N,Missing
D08-1048,J06-1003,0,\N,Missing
D08-1048,N04-3012,0,\N,Missing
D08-1048,P98-2127,0,\N,Missing
D08-1048,C98-2122,0,\N,Missing
D12-1016,S12-1051,0,0.039481,"Missing"
D12-1016,J08-1001,0,0.0596153,"rning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). Introduction Discourse coherence is an important aspect in natural language generation (NLG) applications. A number of theories have investigated coherence inducing factors. A prominent example is Centering Theory (Grosz et al., 1995), which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse. A data-driven model based on this theory is the entity-based approach by Barzilay and Lapata (2008), which models coherence phenomena by observing sentence-to-sentence transitions of entity occurrences. For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved arguments. These include the specific forms of argument realization, as a pronoun or a specific type of referential expression, as studied in prior"
D12-1016,N04-1015,0,0.0367931,"sed clustering with Mincuts. Our method significantly outperforms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in F1 -score. 1 The main hypothesis of our work is that we can automatically learn context-specific realization patterns for predicate argument structures (PAS) from a semantically parsed corpus of comparable text pairs. Our assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). Introduction Discourse coherence is an important aspect in natural language generation (NLG) applications. A number of theories have investigated coherence inducing factors. A prominent example is Centering Theory (Grosz et al., 1995), which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse. A data-driven model based on this theory is the entity-based approach by Barzilay and Lapata (2008), which models coherence phenomena by observing sente"
D12-1016,W09-2816,0,0.0210245,"coherence phenomena by observing sentence-to-sentence transitions of entity occurrences. For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved arguments. These include the specific forms of argument realization, as a pronoun or a specific type of referential expression, as studied in prior work in NLG (Belz et al., 2009, inter alia). The specific set-up we examine, however, allows us to further investi171 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 171–182, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics gate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. Example (1), extracted from our corpus of aligned texts,illustrates this point: Both texts report on the same event of locating victims i"
D12-1016,C10-3009,0,0.121121,"Missing"
D12-1016,C10-1011,0,0.131038,"Missing"
D12-1016,J93-2003,0,0.0355543,"Missing"
D12-1016,J08-4005,0,0.580087,"or the alignment of predicates. Section 5 outlines the experiments and presents evaluation results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work The task of aligning words in general has been studied extensively in previous work, for example as part of research in statistical machine translation (SMT). Typically, alignment models in SMT are trained by observing and (re-)estimating co-occurrence counts of word pairs in parallel sentences (Brown et al., 1993). The same methods have also been applied in monolingual settings, for example to align words in paraphrases (Cohn et al., 2008). In contrast to traditional word alignment tasks, our focus is not on pairs of isolated sentences but on aligning predicates within the discourse contexts in which they are situated. Furthermore, text pairs for our task should not be strictly parallel as we are specifically interested in the impact of different discourse contexts. In Section 5, we will show that this particular setting indeed constitutes a more challenging task compared to traditional word alignment in parallel or paraphrasing sentences. Another set of related tasks is found in the area of textual inference. Since 2006, there"
D12-1016,I05-5002,0,0.0397043,"ncebased word alignment setting and our novel alignment task that operates on full texts. 5.1.1 Sentence-level Alignment Setting For sentence-based predicate alignment we make use of the following three corpora that are wordaligned subsets of the paraphrase collections described in (Cohn et al., 2008): MTC consists of 100 177 sentence pairs from the Multiple-Translation Chinese Corpus (Huang et al., 2002), Leagues contains 100 sentential paraphrases from two translations of Jules Verne’s “Twenty Thousand Leagues Under the Sea”, and MSR is a sub-set of the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), consisting of 130 sentence pairs. All three paraphrase collections are in English. Results for these experiments are reported in Section 5.3.1. Note that in order to determine alignment candidates, we apply the same pre-processing steps as used for the annotation of our corpus. The semantic parser identified an average number of 3.8, 5.1 and 4.7 predicates per text (i.e., per paraphrase sentence) in MTC, Leagues and MSR, respectively. All models are evaluated against the subset of gold standard alignments (cf. Cohn et al. (2008)) between pairs of words marked as predicates. 5.1.2 Text-level"
D12-1016,J95-2003,0,0.422835,"predicate argument structures (PAS) from a semantically parsed corpus of comparable text pairs. Our assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). Introduction Discourse coherence is an important aspect in natural language generation (NLG) applications. A number of theories have investigated coherence inducing factors. A prominent example is Centering Theory (Grosz et al., 1995), which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse. A data-driven model based on this theory is the entity-based approach by Barzilay and Lapata (2008), which models coherence phenomena by observing sentence-to-sentence transitions of entity occurrences. For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investi"
D12-1016,D11-1051,0,0.0596534,"larity function that defines fixed similarity scores between 0 and 1 for pairs of predicates p1 , p2 depending on their relatedness within the VerbNet class hierarchy:  1.0     0.8 simVN (p1 , p2 ) =     0.0 if ∃C : p1 , p2 ∈ C if ∃C, Cs : Cs ∈ sub(C) ∧ p1 , p2 ∈ C ∪ Cs (4) else Distributional similarity. As some predicates may not be covered by the WordNet and VerbNet hierarchies, we additionally calculate similarity based on distributional meaning in a semantic space (Landauer and Dumais, 1997). Following the traditional bag-of-words approach that has been applied in related tasks (Guo and Diab, 2011; Mitchell and Lapata, 2010), we consider the 2,000 most frequent context words c1 , . . . , c2000 ∈ C as dimensions of a vector space and define predicates as vectors using their Pointwise Mutual Information (PMI): p~ = (PMI(p, c1 ), . . . , PMI(p, c2000 ) (5) freq(x, y) freq(x) ∗ freq(y) Given the vector representations of two predicates, we calculate their similarity as the cosine of the angle between the two vectors: with PMI(x, y) = simDist (p1 , p2 ) = p~1 · p~2 |p~1 |∗ |p~2 | (6) Argument similarity. While the previous similarity measures are purely type-based, argument similarity integ"
D12-1016,N06-1014,0,0.0847026,"tead, it greedily computes as many 1-to-1 alignments as possible, starting from the highest similarity to the learned threshold (Greedy). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). For the three sentence-based paraphrase settings MTC, Leagues and MSR, Cohn et al. (2008) readily provide GIZA++ (Och and Ney, 2003) alignments as part of their word-aligned paraphrase corpus. For the experiments in the GigaPairs setting, we train our own word alignment model using the state-of-theart word alignment tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases in the latter setting using a re-implementation of the paraphrase detection system by Wan et al. (2006).8 In the following section, we abbreviate both baselines using SMT alignment tools as WordAlign. 8 Note that the performance of this system lies slightly below the state-of-the-art results reported by Socher et al. (2011) However, we were not able to reproduce the results of Socher et al. using the publicly available release of their software. 178 5.3 Results We measure precision as the number of pred"
D12-1016,D08-1084,0,0.350652,"Missing"
D12-1016,C10-1087,0,0.0314354,"Missing"
D12-1016,P10-1123,0,0.0347533,"Missing"
D12-1016,J03-1002,0,0.00933824,"settings. In order to assess the benefits of the clustering step, we propose a second baseline that uses the same similarity measures and thresholds as our Full model, but omits the clustering step described in Section 4.3. Instead, it greedily computes as many 1-to-1 alignments as possible, starting from the highest similarity to the learned threshold (Greedy). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). For the three sentence-based paraphrase settings MTC, Leagues and MSR, Cohn et al. (2008) readily provide GIZA++ (Och and Ney, 2003) alignments as part of their word-aligned paraphrase corpus. For the experiments in the GigaPairs setting, we train our own word alignment model using the state-of-theart word alignment tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases in the latter setting using a re-implementation of the paraphrase detection system by Wan et al. (2006).8 In the following section, we abbreviate both baselines using SMT alignment tools as WordAlign. 8 Note that the performance of this system lies slightly below the state-of-the"
D12-1016,J05-1004,0,0.118418,"Missing"
D12-1016,N04-3012,0,0.0569774,"= args(p2 ), we define the following measures. WordNet similarity. Given all pairs of synsets s1 , s2 that contain the predicates p1 , p2 , respectively, we compute the maximal similarity using the information theoretic measure described in Lin (1998). Our implementation exploits the WordNet hierarchy 6 All token-based frequency counts (i.e., f req() and idf ()) are computed over all documents from the AFP and APW parts of the English Gigaword Fifth Edition. 175 (Fellbaum, 1998) to find the synset of the least common subsumer (lcs) and uses the pre-computed Information Content (IC) files from Pedersen et al. (2004) to compute Lin’s measure: simWN (p1 , p2 ) = IC(lcs(s1 , s2 )) IC(s1 ) ∗ IC(s2 ) (3) In order to compute similarities between verbal and nominal predicates, we further use derivation information from NomBank (Meyers et al., 2008): if a noun represents a nominalization of a verbal predicate, we resort to the corresponding verb synset. If no relation can be found between two predicates, we set a default value of simWN = 0. This applies in particular to all cases that involve a predicate not present in WordNet. VerbNet similarity. To overcome systematic problems with the WordNet verb hierarchy ("
D12-1016,C08-1092,0,0.0642078,"compute Lin’s measure: simWN (p1 , p2 ) = IC(lcs(s1 , s2 )) IC(s1 ) ∗ IC(s2 ) (3) In order to compute similarities between verbal and nominal predicates, we further use derivation information from NomBank (Meyers et al., 2008): if a noun represents a nominalization of a verbal predicate, we resort to the corresponding verb synset. If no relation can be found between two predicates, we set a default value of simWN = 0. This applies in particular to all cases that involve a predicate not present in WordNet. VerbNet similarity. To overcome systematic problems with the WordNet verb hierarchy (cf. Richens (2008)), we further compute similarity between verbal predicates using VerbNet (Kipper et al., 2008). Verbs in VerbNet are categorized into semantic classes according to their syntactic behavior. A class C can recursively embed sub-classes Cs ∈ sub(C) that represent finer semantic and syntactic distinctions. We define a simple similarity function that defines fixed similarity scores between 0 and 1 for pairs of predicates p1 , p2 depending on their relatedness within the VerbNet class hierarchy:  1.0     0.8 simVN (p1 , p2 ) =     0.0 if ∃C : p1 , p2 ∈ C if ∃C, Cs : Cs ∈ sub(C) ∧ p1 , p2 ∈"
D12-1016,S12-1030,1,0.855971,"le corpus of paired texts. The induced alignments will (i) serve to identify events described in both comparable texts, and (ii) provide information about the underlying argument structures and how they are realized in each context to establish a coherent discourse. We investigate a graph-based clustering method for induc1 See the recent SemEval 2010 task: Linking Events and their Participants in Discourse, (Ruppenhofer et al., 2010). 2 Note that we provide details regarding the construction of a suitable data set and further examples involving non-realized arguments in a complementary paper (Roth and Frank, 2012). 172 ing such alignments as clustering provides a suitable framework to implicitly relate alignment decisions to one another, by exploiting global information encoded in a graph. The remainder of this paper is structured as follows: In Section 2, we discuss previous work in related tasks. Section 3 describes our task and a suitable data set. Section 4 introduces a graph-based clustering model using Mincuts for the alignment of predicates. Section 5 outlines the experiments and presents evaluation results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work The task of al"
D12-1016,S10-1008,0,0.0877217,"dications. This paper focuses on the first of these tasks, henceforth called predicate alignment.2 In line with data-driven approaches in NLP, we automatically align predicates in a suitable corpus of paired texts. The induced alignments will (i) serve to identify events described in both comparable texts, and (ii) provide information about the underlying argument structures and how they are realized in each context to establish a coherent discourse. We investigate a graph-based clustering method for induc1 See the recent SemEval 2010 task: Linking Events and their Participants in Discourse, (Ruppenhofer et al., 2010). 2 Note that we provide details regarding the construction of a suitable data set and further examples involving non-realized arguments in a complementary paper (Roth and Frank, 2012). 172 ing such alignments as clustering provides a suitable framework to implicitly relate alignment decisions to one another, by exploiting global information encoded in a graph. The remainder of this paper is structured as follows: In Section 2, we discuss previous work in related tasks. Section 3 describes our task and a suitable data set. Section 4 introduces a graph-based clustering model using Mincuts for t"
D12-1016,P10-1098,0,0.0295023,"rms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in F1 -score. 1 The main hypothesis of our work is that we can automatically learn context-specific realization patterns for predicate argument structures (PAS) from a semantically parsed corpus of comparable text pairs. Our assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). Introduction Discourse coherence is an important aspect in natural language generation (NLG) applications. A number of theories have investigated coherence inducing factors. A prominent example is Centering Theory (Grosz et al., 1995), which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse. A data-driven model based on this theory is the entity-based approach by Barzilay and Lapata (2008), which models coherence phenomena by observing sentence-to-sentence transitions of entity occurrences. For our purposes"
D12-1016,U06-1019,0,0.10726,"tools commonly used in statistical machine translation (SMT). For the three sentence-based paraphrase settings MTC, Leagues and MSR, Cohn et al. (2008) readily provide GIZA++ (Och and Ney, 2003) alignments as part of their word-aligned paraphrase corpus. For the experiments in the GigaPairs setting, we train our own word alignment model using the state-of-theart word alignment tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases in the latter setting using a re-implementation of the paraphrase detection system by Wan et al. (2006).8 In the following section, we abbreviate both baselines using SMT alignment tools as WordAlign. 8 Note that the performance of this system lies slightly below the state-of-the-art results reported by Socher et al. (2011) However, we were not able to reproduce the results of Socher et al. using the publicly available release of their software. 178 5.3 Results We measure precision as the number of predicted alignments that are annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments divided by the total n"
D12-1016,W09-0621,0,0.0512409,"Missing"
D12-1016,W07-1401,0,\N,Missing
D14-1045,P09-1004,0,0.0549445,"Missing"
D14-1045,J12-1005,0,0.0485022,"Missing"
D14-1045,D10-1115,0,0.0449887,"ity) has to be chosen in advance. The popularity of distributional methods for word representation has been a motivation for developing representations of larger constructions such as phrases and sentences, and there have been several proposals for computing the meaning of word combinations in vector spaces. Mitchell and Lapata (2010) introduced a general framework where composition is formulated as a function f of two vectors u and v. Depending on how f is chosen, different composition models arise, the simplest being an additive model where f (u, v) = u + v. To capture relational functions, Baroni and Zamparelli (2010) expanded on this approach by representing verbs, adjectives and adverbs by matrices which can modify the properties of nouns (represented by vectors). Socher et al. (2012) combined word representations with syntactic structure information, through a recursive neural network that learns vector space representations for multi-word phrases and sentences. An empirical comparison of these composition methods was provided in (Blacoe and Lapata, 2012). In this work, we use type-based continuous representations of words to compose representations of multiple word sequences and spans, which can then b"
D14-1045,J12-4003,0,0.0392427,"Missing"
D14-1045,J02-3001,0,0.121239,"al manually-build semantic resources, including FrameNet (Ruppenhofer et al., 2010) and PropBank (Palmer et al., 2005), have been developed with the goal of documenting and providing examples of such transformations and how they preserve semantic role information. Given that labelled corpora are inevitably restricted in size and coverage, and that syntactic cues are not by themselves unambiguous or sufficient, the success of systems that automatically provide corresponding analyses has been limited in practice. 2 Related Work Research into using distributional information in SRL dates back to Gildea and Jurafsky (2002), who used distributions over verb-object co-occurrence clusters to improve coverage in argument classification. The distribution of a word over these soft clusters assignments was added as 407 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 407–413, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics features to their classifier. The SRL system by Croce et al. (2010) combines argument clustering based on co-occurrence frequencies with a language model. Collobert et al. (2011) used distributional word represen"
D14-1045,W09-1206,0,0.324161,"Missing"
D14-1045,C10-3009,0,0.201516,"Missing"
D14-1045,D12-1050,0,0.0156066,"how f is chosen, different composition models arise, the simplest being an additive model where f (u, v) = u + v. To capture relational functions, Baroni and Zamparelli (2010) expanded on this approach by representing verbs, adjectives and adverbs by matrices which can modify the properties of nouns (represented by vectors). Socher et al. (2012) combined word representations with syntactic structure information, through a recursive neural network that learns vector space representations for multi-word phrases and sentences. An empirical comparison of these composition methods was provided in (Blacoe and Lapata, 2012). In this work, we use type-based continuous representations of words to compose representations of multiple word sequences and spans, which can then be incorporated directly as features into SRL systems. 408 further includes representations for the combination of p and ai , the set of words in the dependency path between p and ai , and the set of words in the full span of ai . We compute additive compositional representations of multiple words, using the simplest method of Mitchell and Lapata (2010) where the composed representation is the uniformly weighted sum of each single representation."
D14-1045,P14-1136,0,0.0550767,"pages 407–413, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics features to their classifier. The SRL system by Croce et al. (2010) combines argument clustering based on co-occurrence frequencies with a language model. Collobert et al. (2011) used distributional word representations in a neural network model that can update representations during training. Zapirain et al. (2013) suggested distributional information as a basis for a selectional preference model that can be used as a single additional feature for classifying potential arguments. Most recently, Hermann et al. (2014) used distributional word representations within pre-defined syntactic contexts as input to a classifier which learns to distinguish different predicate senses. Distributional Feature Computation Argument a Predicate p Predicate-argument Interaction Argument Span w1 . . . wn Dependency Path from a to p ~a p~ ~a + p~ Σi w ~i Σw∈path(a,p) w ~ Table 1: Features based on distributional word representations and additive composition. Vector w ~ denotes the representation of word w. 3 Method Following the set-up of the CoNLL shared task in 2009, we consider predicate-argument structures that consist"
D14-1045,C10-1011,0,0.0333875,"to perform all follow-up experiments using the 50-dimensional embeddings induced by Turian et al. (2010), using the method by Collobert et al., as they led to slightly better results in F1 -score than other representations. No significant differences were observed, however, using other types of representations or vector sizes. Semantic Role Labeller. In all our experiments, we use the publicly available system by Bj¨orkelund et al. (2010).1 This system combines the first-ranked SRL system and the firstranked syntactic parser in the CoNLL 2009 shared task for English (Bj¨orkelund et al., 2009; Bohnet, 2010). To the best of our knowledge, this combination represents the current state-of-the-art for semantic role labelling following the PropBank/NomBank paradigm (Palmer et al., 2005; Meyers et al., 2004). To re-train and evaluate models with different feature sets, we use the same training, development and test sets as provided in the CoNLL shared task (Hajiˇc et al., 2009). Although the employed system features a full syntactic-semantic parsing pipeline, we only modify the feature sets of the two components directly related to the actual role labelling task, namely argument identification and arg"
D14-1045,P10-1099,0,0.0858515,"to a classifier can lead to immediate improvements in SRL performance, this observation seems in part counterintuitive. Just because one word has a specific representation does not mean that it should be assigned a specific argument label. In fact, one would expect a more complex interplay between the representation of an argument ai and the context it appears in. To model aspects of this interplay, we define an extended set of features that A complementary line of research explores the representation of sequence information. Prominent examples are the works by Deschacht and Moens (2009) and Huang and Yates (2010) who learned and applied Hidden Markov Models to assign state variables to words and word spans, which serve as supplementary features for classification. One drawback of this approach is that state variables are discrete and the number of states (i.e., their granularity) has to be chosen in advance. The popularity of distributional methods for word representation has been a motivation for developing representations of larger constructions such as phrases and sentences, and there have been several proposals for computing the meaning of word combinations in vector spaces. Mitchell and Lapata (2"
D14-1045,J92-4003,0,0.309583,"resentations for predicates and argument as additional features. Performance numbers in percent. Experimental Setup We evaluate the impact of different types of features by performing experiments on a benchmark dataset for semantic role labelling. To assess the gains of distributional representations realistically, we incorporate the features described in Section 3 into a state-of-the-art SRL system. The following paragraphs summarize the details of our experimental setup. (Mnih and Hinton, 2009; Collobert et al., 2011; Huang et al., 2012), eigenvectors (Dhillon et al., 2011), Brown clusters (Brown et al., 1992), and post-processed co-occurrence counts (Lebret and Collobert, 2014). Results on the development set for various off-the-shelf representations are shown in Table 2. The numbers reveal that any kind of word representation can be employed to improve results. We choose to perform all follow-up experiments using the 50-dimensional embeddings induced by Turian et al. (2010), using the method by Collobert et al., as they led to slightly better results in F1 -score than other representations. No significant differences were observed, however, using other types of representations or vector sizes. Se"
D14-1045,P12-1092,0,0.0127745,"2: Results on the CoNLL-2009 development set, using off-the-shelf word representations for predicates and argument as additional features. Performance numbers in percent. Experimental Setup We evaluate the impact of different types of features by performing experiments on a benchmark dataset for semantic role labelling. To assess the gains of distributional representations realistically, we incorporate the features described in Section 3 into a state-of-the-art SRL system. The following paragraphs summarize the details of our experimental setup. (Mnih and Hinton, 2009; Collobert et al., 2011; Huang et al., 2012), eigenvectors (Dhillon et al., 2011), Brown clusters (Brown et al., 1992), and post-processed co-occurrence counts (Lebret and Collobert, 2014). Results on the development set for various off-the-shelf representations are shown in Table 2. The numbers reveal that any kind of word representation can be employed to improve results. We choose to perform all follow-up experiments using the 50-dimensional embeddings induced by Turian et al. (2010), using the method by Collobert et al., as they led to slightly better results in F1 -score than other representations. No significant differences were o"
D14-1045,E14-1051,0,0.00867917,"s. Performance numbers in percent. Experimental Setup We evaluate the impact of different types of features by performing experiments on a benchmark dataset for semantic role labelling. To assess the gains of distributional representations realistically, we incorporate the features described in Section 3 into a state-of-the-art SRL system. The following paragraphs summarize the details of our experimental setup. (Mnih and Hinton, 2009; Collobert et al., 2011; Huang et al., 2012), eigenvectors (Dhillon et al., 2011), Brown clusters (Brown et al., 1992), and post-processed co-occurrence counts (Lebret and Collobert, 2014). Results on the development set for various off-the-shelf representations are shown in Table 2. The numbers reveal that any kind of word representation can be employed to improve results. We choose to perform all follow-up experiments using the 50-dimensional embeddings induced by Turian et al. (2010), using the method by Collobert et al., as they led to slightly better results in F1 -score than other representations. No significant differences were observed, however, using other types of representations or vector sizes. Semantic Role Labeller. In all our experiments, we use the publicly avai"
D14-1045,P10-1025,0,0.0448428,"e corresponding analyses has been limited in practice. 2 Related Work Research into using distributional information in SRL dates back to Gildea and Jurafsky (2002), who used distributions over verb-object co-occurrence clusters to improve coverage in argument classification. The distribution of a word over these soft clusters assignments was added as 407 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 407–413, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics features to their classifier. The SRL system by Croce et al. (2010) combines argument clustering based on co-occurrence frequencies with a language model. Collobert et al. (2011) used distributional word representations in a neural network model that can update representations during training. Zapirain et al. (2013) suggested distributional information as a basis for a selectional preference model that can be used as a single additional feature for classifying potential arguments. Most recently, Hermann et al. (2014) used distributional word representations within pre-defined syntactic contexts as input to a classifier which learns to distinguish different pr"
D14-1045,W04-2705,0,0.0990535,"Missing"
D14-1045,D09-1003,0,0.111061,"Missing"
D14-1045,J05-1004,0,0.905112,"redicate and argument, and capture full argument spans. 1 Introduction The goal of semantic role labelling (SRL) is to discover the relations that hold between a predicate and its arguments in a given input sentence (e.g., “who” did “what” to “whom”, “when”, “where”, and “how”). This semantic knowledge at the predicate-argument level is required by inference-based NLP tasks in order to identify meaning-preserving transformations, such as active/passive, verb alternations and nominalizations. Several manually-build semantic resources, including FrameNet (Ruppenhofer et al., 2010) and PropBank (Palmer et al., 2005), have been developed with the goal of documenting and providing examples of such transformations and how they preserve semantic role information. Given that labelled corpora are inevitably restricted in size and coverage, and that syntactic cues are not by themselves unambiguous or sufficient, the success of systems that automatically provide corresponding analyses has been limited in practice. 2 Related Work Research into using distributional information in SRL dates back to Gildea and Jurafsky (2002), who used distributions over verb-object co-occurrence clusters to improve coverage in argu"
D14-1045,S12-1030,1,0.846833,"Missing"
D14-1045,D12-1110,0,0.0224402,"as phrases and sentences, and there have been several proposals for computing the meaning of word combinations in vector spaces. Mitchell and Lapata (2010) introduced a general framework where composition is formulated as a function f of two vectors u and v. Depending on how f is chosen, different composition models arise, the simplest being an additive model where f (u, v) = u + v. To capture relational functions, Baroni and Zamparelli (2010) expanded on this approach by representing verbs, adjectives and adverbs by matrices which can modify the properties of nouns (represented by vectors). Socher et al. (2012) combined word representations with syntactic structure information, through a recursive neural network that learns vector space representations for multi-word phrases and sentences. An empirical comparison of these composition methods was provided in (Blacoe and Lapata, 2012). In this work, we use type-based continuous representations of words to compose representations of multiple word sequences and spans, which can then be incorporated directly as features into SRL systems. 408 further includes representations for the combination of p and ai , the set of words in the dependency path between"
D14-1045,P11-1145,0,0.0154704,"Missing"
D14-1045,C12-1161,0,0.0366499,"Missing"
D14-1045,P10-1040,0,0.0749009,"a state-of-the-art SRL system. The following paragraphs summarize the details of our experimental setup. (Mnih and Hinton, 2009; Collobert et al., 2011; Huang et al., 2012), eigenvectors (Dhillon et al., 2011), Brown clusters (Brown et al., 1992), and post-processed co-occurrence counts (Lebret and Collobert, 2014). Results on the development set for various off-the-shelf representations are shown in Table 2. The numbers reveal that any kind of word representation can be employed to improve results. We choose to perform all follow-up experiments using the 50-dimensional embeddings induced by Turian et al. (2010), using the method by Collobert et al., as they led to slightly better results in F1 -score than other representations. No significant differences were observed, however, using other types of representations or vector sizes. Semantic Role Labeller. In all our experiments, we use the publicly available system by Bj¨orkelund et al. (2010).1 This system combines the first-ranked SRL system and the firstranked syntactic parser in the CoNLL 2009 shared task for English (Bj¨orkelund et al., 2009; Bohnet, 2010). To the best of our knowledge, this combination represents the current state-of-the-art fo"
D14-1045,C00-2137,0,0.200715,"Missing"
D14-1045,J13-3006,0,\N,Missing
D14-1045,W09-1201,0,\N,Missing
D14-1045,W09-1209,0,\N,Missing
D17-3004,D07-1002,0,\N,Missing
D17-3004,lo-wu-2010-evaluating,0,\N,Missing
D17-3004,W05-0635,0,\N,Missing
D17-3004,W05-0628,0,\N,Missing
D17-3004,W04-3212,0,\N,Missing
D17-3004,W05-0634,0,\N,Missing
D17-3004,P10-1040,0,\N,Missing
D17-3004,J05-1004,0,\N,Missing
D17-3004,D14-1162,0,\N,Missing
D17-3004,W04-2705,0,\N,Missing
D17-3004,N07-1069,0,\N,Missing
D17-3004,W13-3520,0,\N,Missing
D17-3004,W08-0804,0,\N,Missing
D17-3004,J93-2004,0,\N,Missing
D17-3004,D12-1074,0,\N,Missing
D17-3004,de-marneffe-etal-2014-universal,0,\N,Missing
D17-3004,W08-2121,0,\N,Missing
D17-3004,D09-1059,0,\N,Missing
D17-3004,W09-1205,1,\N,Missing
D17-3004,J92-4003,0,\N,Missing
D17-3004,W09-1201,0,\N,Missing
D17-3004,petrov-etal-2012-universal,0,\N,Missing
D17-3004,P08-1063,0,\N,Missing
D17-3004,W09-1208,0,\N,Missing
D17-3004,Q15-1034,1,\N,Missing
D17-3004,P14-1111,1,\N,Missing
D17-3004,W13-5503,0,\N,Missing
D17-3004,D16-1177,1,\N,Missing
D19-6007,P16-1223,0,0.0623605,"Missing"
D19-6007,D19-6008,0,0.0508671,"odel for the actual machine comprehension task. Participants • JDA (Da, 2019) use three different knowledge bases, namely ConceptNet (Speer et al., 2017), ATOMIC (Sap et al., 2019) and WebChild (Tandon et al., 2017). They extract In total, five teams submitted systems in task 1, and one team participated in task 2. All submitted models were neural networks, and all made use of pretrained Transformer language models 69 relevant edges from the knowledge bases and compute relation embeddings, which are combined with BERT-based word representations with a diadic multiplication operation. • KARNA (Jain and Singh, 2019) use a BERT model, but they enhance the text representation with edges that are extracted from ConceptNet. Following Wang et al. (2018), they extract relations between words in the text and the question/answer, and append them to the text representation. Instead of computing relational embeddings, they append a specific string that describes the relation. 5 # Team Name acc acccs accOOD 1 2 3 4 5 PSH–SJTU IIT-KGP BLCU-NLP JDA KARNA 0.906 0.905∗ 0.842∗ 0.807∗ 0.733∗ 0.903 0.894 0.812 0.775 0.697 0.915 0.931 0.838 0.796 0.729 - 0.715 0.651 0.666 0.634 0.673 0.619 - TriAN Attentive Reader Logistic"
D19-6007,P17-1147,0,0.0248495,"blation tests indicating the importance of including commonsense knowledge. In comparison to ATOMIC and WebChild, Da (2019) report that ConceptNet is most beneficial for performance on the task 1 data, which can be explained with its domain: The OMCS (Singh et al., 2002) data are part of the ConceptNet database, and OMCS scenarios were also used to collect texts for the task 1 data. All in all, powerful pretrained models such as XLNet still outperform approaches that make use of structured knowledge bases, which indicates that they are (at least to some extent) capable of Web texts. TriviaQA (Joshi et al., 2017) is a corpus of webcrawled trivia and quiz-league websites together with evidence documents from the web. A large part of questions requires a system to make use of factual commonsense knowledge for finding an answer. CommonsenseQA (Talmor et al., 2018) consists of 9,000 crowdsourced multiplechoice questions with a focus on relations between entities that appear in ConceptNet (Speer et al., 2017). Evidence documents were webcrawled based on the question and added after the crowdsourcing step. Pretraining and finetuning on other data. Several participants reported effects of pretraining/finetun"
D19-6007,Q18-1023,0,0.0444937,"Missing"
D19-6007,L18-1564,1,0.883216,"Missing"
D19-6007,D17-1082,0,0.233399,"Webchild ConceptNet - 1 Table 2: Overview of participating systems DocQA (Clark and Gardner, 2018) is a strong baseline model for extractive QA. It consists of components such as bi-directional attention flow (Seo et al., 2016) and self-attention, both of which are widely used in MC models. We also evaluated a variant of DocQA with ELMo (Peters et al., 2018) to analyze the impact of ELMo on this task. such as BERT (Devlin et al., 2019). The participants used a wide range of external corpora and resources to augment their models, ranging from other machine comprehension data sets such as RACE (Lai et al., 2017) or MCScript (Ostermann et al., 2018a), up to commonsense knowledge databases such as ConceptNet (Speer et al., 2017), WebChild (Tandon et al., 2017) or ATOMIC (Sap et al., 2019). Table 2 gives a summary of the participating systems. Random Guess acts as the lower bound of the evaluated models, which randomly picks a named entity from the passage as the answer. The reported results are averaged over 5 runs. 3.3 • PSH–SJTU (Li et al., 2019) participated in both tasks with a Transformer model based on XLNet (Yang et al., 2019b). For task 1, they pretrain the model in several steps, first on the"
D19-6007,S19-1012,1,0.863993,"kers were then asked to write questions about noun or verb phrases that were highlighted in the texts. After collecting questions, the sentences containing the noun or verb phrases were deleted from the texts. During the answer collection, crowd workers thus had to infer the information required for finding an answer from background knowledge. Five turkers wrote correct and incorrect answer candidates for each question, and the most difficult incorrect candidates were selected via adversarial filtering (Zellers et al., 2018). For our shared task, we use the same data split as Ostermann et al. (2019): 14,191 questions on 2,500 texts for the training set, 2,020 questions on 355 texts for the development set and 3,610 questions on 632 texts for the test set. All texts for five scenarios were reserved for the test set only to increase difficulty. Data and Tasks Text understanding systems are often evaluated by means of a reading comprehension task, which is also referred to as machine (reading) comprehension (MC). The central idea is that a system has to process a text and then find a correct answer to a question that is asked on the text. Our shared tasks follow this paradigm and use machin"
D19-6007,D19-6011,0,0.246941,"participants used a wide range of external corpora and resources to augment their models, ranging from other machine comprehension data sets such as RACE (Lai et al., 2017) or MCScript (Ostermann et al., 2018a), up to commonsense knowledge databases such as ConceptNet (Speer et al., 2017), WebChild (Tandon et al., 2017) or ATOMIC (Sap et al., 2019). Table 2 gives a summary of the participating systems. Random Guess acts as the lower bound of the evaluated models, which randomly picks a named entity from the passage as the answer. The reported results are averaged over 5 runs. 3.3 • PSH–SJTU (Li et al., 2019) participated in both tasks with a Transformer model based on XLNet (Yang et al., 2019b). For task 1, they pretrain the model in several steps, first on the RACE data (Lai et al., 2017) and then on SWAG (Zellers et al., 2018). For task 2, they do not conduct specific pretraining steps, but implement a range of simple rulebased answer verification strategies to verify the output of the model. Evaluation Task 1. The evaluation measure for task 1 is accuracy, computed as the number of correctly answered questions divided by the number of all questions. We also report accuracy values on questions"
D19-6007,N18-1202,0,0.0255822,"ure Commonsense Other Resources Tasks 1 2 PSH–SJTU IIT-KGP - RACE, SWAG RACE 1, 2 1 3 4 BLCU-NLP JDA Transformer (XLNet) Transformer (BERT + XLNet) Transformer (BERT) Transformer (BERT) ReCoRD, RACE Wikipedia 1 1 5 KARNA Transformer (BERT) ConceptNet, Atomic, Webchild ConceptNet - 1 Table 2: Overview of participating systems DocQA (Clark and Gardner, 2018) is a strong baseline model for extractive QA. It consists of components such as bi-directional attention flow (Seo et al., 2016) and self-attention, both of which are widely used in MC models. We also evaluated a variant of DocQA with ELMo (Peters et al., 2018) to analyze the impact of ELMo on this task. such as BERT (Devlin et al., 2019). The participants used a wide range of external corpora and resources to augment their models, ranging from other machine comprehension data sets such as RACE (Lai et al., 2017) or MCScript (Ostermann et al., 2018a), up to commonsense knowledge databases such as ConceptNet (Speer et al., 2017), WebChild (Tandon et al., 2017) or ATOMIC (Sap et al., 2019). Table 2 gives a summary of the participating systems. Random Guess acts as the lower bound of the evaluated models, which randomly picks a named entity from the pa"
D19-6007,D19-6012,0,0.0641609,"pretrained on the RACE data (Lai et al., 2017), and their output is averaged for a final prediction. Task 2. We use two evaluation metrics, EM and F1, similar to those used by SQuAD (Rajpurkar et al., 2016b). Exact Match (EM) measures the percentage of predictions that match a reference answer exactly. (Macro-averaged) F1 measures the average overlap between model predictions and reference answers. For computing F1 , we treat prediction and reference answers as bags of tokens. We take the maximum F1 over all reference answers for a given query, and then average over all queries. 4 • BLCU-NLP (Liu et al., 2019) use a Transformer model based on BERT, which is finetuned in two stages: they first tune the BERTbased language model on the RACE and ReCoRD datasets and then (further) train the model for the actual machine comprehension task. Participants • JDA (Da, 2019) use three different knowledge bases, namely ConceptNet (Speer et al., 2017), ATOMIC (Sap et al., 2019) and WebChild (Tandon et al., 2017). They extract In total, five teams submitted systems in task 1, and one team participated in task 2. All submitted models were neural networks, and all made use of pretrained Transformer language models"
D19-6007,P18-1157,0,0.0138699,"of three baseline models. KT-NET (Yang et al., 2019a) employs an attention mechanism to adaptively select desired knowledge from knowledge bases, and then fuses selected knowledge with BERT to enable contextand knowledge-aware predictions for machine reading comprehension. Logistic Regression Model. Merkhofer et al. (2018) presented a logistic regression classifier for the SemEval 2018 shared task 11, which used simple overlap features and word patterns on MCScript, a predecessor of the dataset used for this task. Their model outperformed many neural networks in spite of its simplicity. SAN (Liu et al., 2018) is a top-ranked MC model. It shares many components with other MC models, and employs a stochastic answer module. As we used SAN to filter out queries in the data collection, it is necessary to verify that the collected queries are hard for not only SAN but also other MC architectures. 3.1 Task 1 Baselines 68 Rank Team Name Architecture Commonsense Other Resources Tasks 1 2 PSH–SJTU IIT-KGP - RACE, SWAG RACE 1, 2 1 3 4 BLCU-NLP JDA Transformer (XLNet) Transformer (BERT + XLNet) Transformer (BERT) Transformer (BERT) ReCoRD, RACE Wikipedia 1 1 5 KARNA Transformer (BERT) ConceptNet, Atomic, Webc"
D19-6007,D16-1264,0,0.384056,"1 is accuracy, computed as the number of correctly answered questions divided by the number of all questions. We also report accuracy values on questions that crowd workers explicitly annotated as requiring commonsense as well as performance on the five held-out scenarios. • IIT-KGP (Sharma and Roychowdhury, 2019) present an ensemble of different pretrained language models, namely BERT and XLNet. Both models are pretrained on the RACE data (Lai et al., 2017), and their output is averaged for a final prediction. Task 2. We use two evaluation metrics, EM and F1, similar to those used by SQuAD (Rajpurkar et al., 2016b). Exact Match (EM) measures the percentage of predictions that match a reference answer exactly. (Macro-averaged) F1 measures the average overlap between model predictions and reference answers. For computing F1 , we treat prediction and reference answers as bags of tokens. We take the maximum F1 over all reference answers for a given query, and then average over all queries. 4 • BLCU-NLP (Liu et al., 2019) use a Transformer model based on BERT, which is finetuned in two stages: they first tune the BERTbased language model on the RACE and ReCoRD datasets and then (further) train the model fo"
D19-6007,S18-1181,0,0.0538854,"Missing"
D19-6007,L16-1555,1,0.846494,"stions has two answer candidates, one of which is correct. Questions in the data were annotated for reasoning types, i.e. according to whether the answer to a question can be found in the text or needs to be inferred from commonsense knowledge. Roughly half of the questions do require inferences over commonsense knowledge. The texts in MCScript2.0 are short narrations (164.4 tokens on average) on a total of 200 different everyday activities. All texts were crowdsourced on Amazon Mechanical Turk1 , by asking crowd workers to tell a story about one of the 200 scenarios as if talking to a child (Modi et al., 2016; Ostermann et al., 2018a), resulting in simple texts which explicitly mention many details of a scenario. In the question collection, which was also conducted via crowdsourcing, turkers were then asked to write questions about noun or verb phrases that were highlighted in the texts. After collecting questions, the sentences containing the noun or verb phrases were deleted from the texts. During the answer collection, crowd workers thus had to infer the information required for finding an answer from background knowledge. Five turkers wrote correct and incorrect answer candidates for each ques"
D19-6007,N16-1098,0,0.0788234,"Missing"
D19-6007,D18-1009,0,0.17611,"of a scenario. In the question collection, which was also conducted via crowdsourcing, turkers were then asked to write questions about noun or verb phrases that were highlighted in the texts. After collecting questions, the sentences containing the noun or verb phrases were deleted from the texts. During the answer collection, crowd workers thus had to infer the information required for finding an answer from background knowledge. Five turkers wrote correct and incorrect answer candidates for each question, and the most difficult incorrect candidates were selected via adversarial filtering (Zellers et al., 2018). For our shared task, we use the same data split as Ostermann et al. (2019): 14,191 questions on 2,500 texts for the training set, 2,020 questions on 355 texts for the development set and 3,610 questions on 632 texts for the test set. All texts for five scenarios were reserved for the test set only to increase difficulty. Data and Tasks Text understanding systems are often evaluated by means of a reading comprehension task, which is also referred to as machine (reading) comprehension (MC). The central idea is that a system has to process a text and then find a correct answer to a question tha"
D19-6007,P19-1472,0,0.035309,"Missing"
D19-6007,P17-4020,0,0.0453697,"Missing"
D19-6007,W17-2623,0,0.0520537,"Missing"
D19-6007,P19-1226,0,0.0546566,"resent five baselines for ReCoRD: BERT (Devlin et al., 2019) is a new language representation model. Recently fine-tuning the pre-trained BERT with an additional output layer has created state-of-the-art models on a wide range of NLP tasks. We formalized ReCoRD as an extractive QA task like SQuAD, and then reused the fine-tuning script for SQuAD to fine-tune BERT for ReCoRD. Shared Task Setup The baselines for our shared tasks were adapted from Ostermann et al. (2019) and Zhang et al. (2018), respectively. Following Ostermann et al. (2019), we present results of three baseline models. KT-NET (Yang et al., 2019a) employs an attention mechanism to adaptively select desired knowledge from knowledge bases, and then fuses selected knowledge with BERT to enable contextand knowledge-aware predictions for machine reading comprehension. Logistic Regression Model. Merkhofer et al. (2018) presented a logistic regression classifier for the SemEval 2018 shared task 11, which used simple overlap features and word patterns on MCScript, a predecessor of the dataset used for this task. Their model outperformed many neural networks in spite of its simplicity. SAN (Liu et al., 2018) is a top-ranked MC model. It share"
D19-6007,C00-2137,0,0.398033,"Missing"
J15-4003,J10-4006,0,0.0256897,"the similarity of contexts of two given predicates over all their instances in a corpus. To compute this measure, we first calculate the Pointwise Mutual Information (PMI) for each predicate p ∈ {p1 , p2 } and the n most frequent context words c ∈ C following Equation (11). pmi(p, c) = freq(p, c) freq(p) ∗ freq(c) (11) As we are dealing with predicates of different parts-of-speech, we calculate joint frequencies in terms of context windows instead of relying on syntactic dependencies as proposed in more recent approaches to distributional semantics (Pado´ and Lapata 2007; Erk and Pado´ 2008; Baroni and Lenci 2010). More precisely, we extract context windows of five words to the left and to the right from the Gigaword corpus (Parker et al. 2011), and compute the PMI for the 2,000 most frequent context words c1 . . . c2,000 ∈ C. The same setting has been successfully applied in related tasks, including word sense disambiguation (Guo and Diab 2011) and measuring phrase similarity (Mitchell and Lapata 2010). Vector representations are computed following Equation (12), and similarities are calculated as the cosine function of the angle between two vectors, as defined in Equation (13). p~ = (pmi(p, c1 ), pmi"
J15-4003,P05-1018,0,0.224995,"ved in related work on classifying discourse relations, information learned from artificial training data might not always generalize well to naturally occurring examples (Sporleder and Lascarides 2008). To automatically create data that is linguistically more similar to manually labeled implicit arguments, we introduce an alternative method that induces instances of implicit arguments from a raw corpus of comparable texts. Coherence Modeling. In the context of coherence modeling, much previous work has focused on entity-based approaches, with the most prominent model being the entity grid by Barzilay and Lapata (2005). This model has originally been proposed for automatic sentence ordering but has since also been applied in coherence evaluation and readability assessment (Barzilay and Lapata 2008; Pitler and Nenkova 2008), story generation (McIntyre and Lapata 2010), and authorship attribution (Feng and Hirst 2014). Based on the original model, several extensions have been proposed: For example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-b"
J15-4003,J08-1001,0,0.0279457,"d Lascarides 2008). To automatically create data that is linguistically more similar to manually labeled implicit arguments, we introduce an alternative method that induces instances of implicit arguments from a raw corpus of comparable texts. Coherence Modeling. In the context of coherence modeling, much previous work has focused on entity-based approaches, with the most prominent model being the entity grid by Barzilay and Lapata (2005). This model has originally been proposed for automatic sentence ordering but has since also been applied in coherence evaluation and readability assessment (Barzilay and Lapata 2008; Pitler and Nenkova 2008), story generation (McIntyre and Lapata 2010), and authorship attribution (Feng and Hirst 2014). Based on the original model, several extensions have been proposed: For example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are,"
J15-4003,P11-2040,0,0.0138763,"th identical predicate form and constituent order. We found that this restriction constrains affected arguments to be modifiers, prepositional phrases, and direct objects. We argue that this is actually a desirable property because more complicated alternations could affect coherence by themselves. In other words, resulting interplays would make it difficult to distinguish between the isolated effect of argument realization itself and other effects, triggered for example by sentence order (Gordon, Grosz, and Gilliom 1993). Annotation. We set up a Web experiment using the evaluation toolkit by Belz and Kow (2011) to collect ratings of local coherence for implicit and explicit arguments. For this experiment, we compiled a data set of 150 document pairs. Each text in such a pair consists of the same content, with the only difference being one argument realization. We presented all 150 pairs to two annotators12 and asked them to indicate their preference for one alternative over the other using a continuous slider scale. The annotators got to see the full texts, with the alternatives presented next to each other. To make texts easier to read and differences easier to spot, we collapsed all identical sent"
J15-4003,C10-3009,0,0.19257,"et of comparable texts used in this work contains 167,728 pairs of articles that were extracted by matching the headlines of texts published within the same time frame (Roth and Frank 2012a). A set of such document headlines is given in Example 4: Example 4 India fires tested anti-ship cruise missile (Xinhua News Agency, 29 October 2003) India tests supersonic cruise anti-ship missile (Agence France Presse, 29 October 2003) URGENT: India tests anti-ship cruise missile (Associated Press Worldstream, 29 October 2003) ¨ We preprocess each article in the set of 167,728 pairs using the MATE tools (Bjorkelund et al. 2010; Bohnet 2010), including a state-of-the-art semantic role labeler that identifies PropBank/NomBank-style predicate–argument structures (Palmer, Gildea, and Kingsbury 2005; Meyers, Reeves, and Macleod 2008). Based on the acquired PAS, we perform manual alignments. In Section 4.1.1, we summarize the annotation guidelines for this step. An overview of the resulting development and evaluation data set is provided in Section 4.1.2. 4.1.1 Manual Annotation. We selected 70 pairs of comparable texts and asked two annotators to manually align predicate–argument structures obtained from preprocessing."
J15-4003,C10-1011,0,0.146645,"sed in this work contains 167,728 pairs of articles that were extracted by matching the headlines of texts published within the same time frame (Roth and Frank 2012a). A set of such document headlines is given in Example 4: Example 4 India fires tested anti-ship cruise missile (Xinhua News Agency, 29 October 2003) India tests supersonic cruise anti-ship missile (Agence France Presse, 29 October 2003) URGENT: India tests anti-ship cruise missile (Associated Press Worldstream, 29 October 2003) ¨ We preprocess each article in the set of 167,728 pairs using the MATE tools (Bjorkelund et al. 2010; Bohnet 2010), including a state-of-the-art semantic role labeler that identifies PropBank/NomBank-style predicate–argument structures (Palmer, Gildea, and Kingsbury 2005; Meyers, Reeves, and Macleod 2008). Based on the acquired PAS, we perform manual alignments. In Section 4.1.1, we summarize the annotation guidelines for this step. An overview of the resulting development and evaluation data set is provided in Section 4.1.2. 4.1.1 Manual Annotation. We selected 70 pairs of comparable texts and asked two annotators to manually align predicate–argument structures obtained from preprocessing. Both annotator"
J15-4003,C10-1017,0,0.0228962,"airs of comparable texts as graphs and aim to find those edges in a graph that represent connections between predicates that need to be aligned. Although our aim is to find edges between nodes, we note that the majority of predicates (nodes) in our data set are not aligned and hence a crucial prerequisite to generate precise alignments is to filter out those nodes that are unlikely to be good alignment candidates. To achieve the filtering and alignment goals at the same time, we rely on graph clustering techniques that have successfully been applied in the NLP literature (Su and Markert 2009; Cai and Strube 2010; Chen and Ji 2010, inter alia) and that can be used to partition a graph into singleton nodes and smaller subgraphs. The clustering method applied in our model relies on so-called minimum cuts (henceforth also called mincuts) in order to partition a bipartite graph, representing pairs of texts, into clusters of alignable predicate–argument structures. A mincut operation divides a given graph into two disjoint subgraphs. Each cut is performed between some source node s and some target node t, such that (1) each of the two nodes will be in a different subgraph and (2) the sum of weights of all"
J15-4003,E09-1018,0,0.0456256,"Missing"
J15-4003,S10-1059,0,0.0603408,"annotations for NomBank (Meyers, Reeves, and Macleod 2008), and Moor, Roth, and Frank (2013) provided annotations for parts of the OntoNotes corpus (Weischedel et al. 2011). All three resources are, however, severely limited: Annotations in the latter two studies are restricted to 10 and 5 predicate types, respectively; the training set of the SemEval task, in contrast, consists of full-text annotations for all occurring predicates but contains only 245 instances of resolved implicit arguments in total. All groups working on the shared task identified data sparsity as one of the main issues (Chen et al. 2010; Ruppenhofer et al. 2012; Laparra and Rigau 2013). Silberer and Frank (2012) point out that additional training data can be heuristically created by treating anaphoric pronoun mentions as implicit arguments. Their experimental results confirmed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. As observed in related work on classifying discourse relations, information learned from artificial training data might not always generalize well to naturally occurring"
J15-4003,W10-2301,0,0.0129356,"xts as graphs and aim to find those edges in a graph that represent connections between predicates that need to be aligned. Although our aim is to find edges between nodes, we note that the majority of predicates (nodes) in our data set are not aligned and hence a crucial prerequisite to generate precise alignments is to filter out those nodes that are unlikely to be good alignment candidates. To achieve the filtering and alignment goals at the same time, we rely on graph clustering techniques that have successfully been applied in the NLP literature (Su and Markert 2009; Cai and Strube 2010; Chen and Ji 2010, inter alia) and that can be used to partition a graph into singleton nodes and smaller subgraphs. The clustering method applied in our model relies on so-called minimum cuts (henceforth also called mincuts) in order to partition a bipartite graph, representing pairs of texts, into clusters of alignable predicate–argument structures. A mincut operation divides a given graph into two disjoint subgraphs. Each cut is performed between some source node s and some target node t, such that (1) each of the two nodes will be in a different subgraph and (2) the sum of weights of all removed edges will"
J15-4003,J08-4005,0,0.06079,"Missing"
J15-4003,C90-2047,0,0.147538,"Missing"
J15-4003,P11-1118,0,0.0428116,"Missing"
J15-4003,P11-2022,0,0.0743087,"ng. In the context of coherence modeling, much previous work has focused on entity-based approaches, with the most prominent model being the entity grid by Barzilay and Lapata (2005). This model has originally been proposed for automatic sentence ordering but has since also been applied in coherence evaluation and readability assessment (Barzilay and Lapata 2008; Pitler and Nenkova 2008), story generation (McIntyre and Lapata 2010), and authorship attribution (Feng and Hirst 2014). Based on the original model, several extensions have been proposed: For example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, solely based on explicit entity mentions, resulting in insufficient representations when dealing with inferable references. Example 3 illustrates this shortcoming. Example 3 (a) 27 tons of cigarettes were picked up in Le Havre. (b) The containers had a"
J15-4003,D08-1094,0,0.10509,"Missing"
J15-4003,W07-2321,0,0.0332981,"mparable texts. Coherence Modeling. In the context of coherence modeling, much previous work has focused on entity-based approaches, with the most prominent model being the entity grid by Barzilay and Lapata (2005). This model has originally been proposed for automatic sentence ordering but has since also been applied in coherence evaluation and readability assessment (Barzilay and Lapata 2008; Pitler and Nenkova 2008), story generation (McIntyre and Lapata 2010), and authorship attribution (Feng and Hirst 2014). Based on the original model, several extensions have been proposed: For example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, solely based on explicit entity mentions, resulting in insufficient representations when dealing with inferable references. Example 3 illustrates this shortcoming. Example 3 (a) 27 tons of cigarettes were picked up in Le H"
J15-4003,J12-1005,0,0.0255035,"nomena such as coreference (Postolache, Cristea, and Orasan 2006). All of the aforementioned instances of the projection approach make use of the same underlying technique: Firstly, words are 630 Roth and Frank Inducing Implicit Arguments from Comparable Texts aligned in a parallel corpus using statistical word alignment; secondly, annotations on a single word or between multiple words in one text are transferred to the corresponding aligned word(s) in the parallel text. This procedure typically assumes that two parallel ¨ sentences express the same meaning. A notable exception is the work by Furstenau and Lapata (2012), which utilizes alignments between syntactic structures to “project” semantic role information from a role-annotated corpus to unseen sentences that are selected from a corpus in the same language. In our work, we apply annotation projection to monolingual comparable texts. In comparison to parallel texts, we have to account for potential differences in perspective and detail that make our task—in particular, the alignment sub-task—considerably ¨ more difficult. In contrast to Furstenau and Lapata’s setting, which involves incomparable texts, we assume that text pairs in our setting still con"
J15-4003,J12-4003,0,0.341067,"withdrew its troops last month.1 Applying a semantic role labeling system on sentence (1b) produces a representation that consists of the predicate withdraw, a temporal modifier (last month) and two associated arguments: the entity withdrawing (Nicaragua) and the thing being withdrawn (its troops). From the previous sentence (1a), we can additionally infer a third argument: namely, the source from which Nicaragua withdrew its troops (Iraq). By leaving this piece of information implicit, the text fragment in sentence (1b) illustrates a typical case of non-local, or implicit, role realization (Gerber and Chai 2012). In this article, we view implicit arguments as a discourse-level phenomenon and treat corresponding instances as implicit references to discourse entities. Taking this perspective, we build upon previous work on discourse analysis. Following Sidner (1979) and Joshi and Kuhn (1979), utterances in discourse typically focus on a set of salient entities, which are also called the foci or centers. Using the notion of centers, Grosz, Joshi, and Weinstein (1995) defined the Centering framework, which relates the salience of an entity in discourse to linguistic factors such as choice of referring ex"
J15-4003,W13-0111,0,0.713011,"Missing"
J15-4003,J95-2003,0,0.809679,"Missing"
J15-4003,D11-1051,0,0.019409,"th predicates of different parts-of-speech, we calculate joint frequencies in terms of context windows instead of relying on syntactic dependencies as proposed in more recent approaches to distributional semantics (Pado´ and Lapata 2007; Erk and Pado´ 2008; Baroni and Lenci 2010). More precisely, we extract context windows of five words to the left and to the right from the Gigaword corpus (Parker et al. 2011), and compute the PMI for the 2,000 most frequent context words c1 . . . c2,000 ∈ C. The same setting has been successfully applied in related tasks, including word sense disambiguation (Guo and Diab 2011) and measuring phrase similarity (Mitchell and Lapata 2010). Vector representations are computed following Equation (12), and similarities are calculated as the cosine function of the angle between two vectors, as defined in Equation (13). p~ = (pmi(p, c1 ), pmi(p, c2 ), . . . , pmi(p, c2,000 )) simDist (p1 , p2 ) = p~1 · p~2 ||p~1 ||∗ ||p~2 || (12) (13) 6 Note that the weight of 0.8 was set in an ad hoc manner (instead of being optimized) in order to avoid overfitting on our small development corpus. 636 Roth and Frank Inducing Implicit Arguments from Comparable Texts Bag-of-Words Similarity."
J15-4003,N13-1111,0,0.0440643,"Missing"
J15-4003,J09-1003,0,0.0195639,"lience, that is, contexts of referential continuity and irrelevance, can also be reflected by the non-realization of an entity (Brown 1983). Specific instances of this phenomenon, so-called zero anaphora, have been well-studied in pro-drop languages such as Japanese (Kameyama 1985), Turkish (Turan 1995), and Italian (Di Eugenio 1990). For English, only a few studies exist that explicitly investigated the effect of non-realizations on coherence. Existing work suggests, however, that indirect references and non-realizations are important for modeling and measuring coherence (Poesio et al. 2004; Karamanis et al. 2009), respectively, and that such phenomena need to be taken into consideration to explain local coherence where adjacent sentences are neither connected by discourse relations nor in terms of coreference (Louis and Nenkova 2010). In this work, we propose a new model to predict whether realizing an argument contributes to local coherence in a given position in discourse. Example 1 illustrated a text fragment, in which argument realization is necessary in the first sentence but redundant in the second. That is, mentioning Iraq in the second sentence is not necessary (for a human being) to understan"
J15-4003,J93-1006,0,0.698565,"Missing"
J15-4003,W13-0114,0,0.0321226,"Missing"
J15-4003,J13-4004,0,0.0229695,"Missing"
J15-4003,D12-1045,0,0.173169,"st to work on paraphrasing, we are specifically interested in pairs of text fragments that involve implicit arguments, which can only be resolved in context. In line with our goal of inducing implicit arguments, we define the units or expressions to be aligned in our task as the predicate–argument structures that can (automatically) be identified in text. This task definition further makes our task distinct from event coreference, where coreference is established based on a pre-specified set of events, reference types, or definitions of event identity (Walker et al. 2006; Pradhan et al. 2007; Lee et al. 2012, inter alia). Although corresponding annotations can certainly overlap with those in our task, we emphasize that the focus of our work is not to find all occurrences of co-referring events. Instead, our goal is to align predicate–argument structures that have a common meaning in context across different discourses. Hence, we neither consider intra-document coreference nor pronominal event references here. As alignable units in our work are not restricted to a pre-specified definition of event or event identity, the task addressed here involves any kind of event, state, or entity that is lingu"
J15-4003,N06-1014,0,0.0822724,"Missing"
J15-4003,P14-5010,0,0.00753894,"uments implicit in one structure can straightforwardly be induced based on this information by looking for co-referring mentions of the argument explicit in the aligned structure. In practice, we make use of additional checks and filters to ensure that only reliable information is being used. We describe the preprocessing steps in the following paragraphs and provide additional details on our implementation of the induction procedure in Section 5.2. Single Document Preprocessing. We apply several preprocessing steps to each document in our data set. First, we use the Stanford CoreNLP package (Manning et al. 2014) ¨ for tokenization and sentence splitting. We then apply the MATE tools (Bjorkelund et al. 2010; Bohnet 2010), including the integrated PropBank/NomBank-based semantic parser, to determine local PAS. Finally, we resolve pronouns that occur in a PAS using the coreference resolution system by Martschat et al. (2012), which placed second for English in the CoNLL-2012 Shared Task (Pradhan et al. 2012). High Precision Alignments. Once all single documents are preprocessed, we align PAS across pairs of comparable texts. We want to induce reliable instances of implicit arguments based on aligned PAS"
J15-4003,W12-4511,0,0.0246843,"Missing"
J15-4003,P10-1158,0,0.0128194,"y more similar to manually labeled implicit arguments, we introduce an alternative method that induces instances of implicit arguments from a raw corpus of comparable texts. Coherence Modeling. In the context of coherence modeling, much previous work has focused on entity-based approaches, with the most prominent model being the entity grid by Barzilay and Lapata (2005). This model has originally been proposed for automatic sentence ordering but has since also been applied in coherence evaluation and readability assessment (Barzilay and Lapata 2008; Pitler and Nenkova 2008), story generation (McIntyre and Lapata 2010), and authorship attribution (Feng and Hirst 2014). Based on the original model, several extensions have been proposed: For example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, solely based on explicit entity mentions, resulting in insuffi"
J15-4003,W13-0211,1,0.915473,"Missing"
J15-4003,J07-2002,0,0.153484,"Missing"
J15-4003,J05-1004,0,0.0421712,"Missing"
J15-4003,P86-1004,0,0.212552,"in paraphrasing and event coreference. 3.1 Implicit Arguments and Coherence Modeling The goal of this work is to induce instances of implicit arguments, together with their discourse antecedents, and to utilize them in semantic processing and coherence modeling. This section summarizes previous work on implicit arguments and coherence modeling, and provides an outlook on how instances of implicit arguments can be of use in a novel entity-based model of local coherence. Implicit Arguments. The role of implicit arguments was studied early on in the context of semantic processing (Fillmore 1986; Palmer et al. 1986), although most semantic 628 Roth and Frank Inducing Implicit Arguments from Comparable Texts role labeling systems nowadays operate solely within local syntactic structures and do not perform any additional inference regarding missing information. First data sets that focus on implicit arguments have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse,” Gerber and Chai (2012) made available implicit argument annotations for NomBank (Meyers, Reeves, and Macleod 2008), and Moor, Roth, and Frank (2013) provide"
J15-4003,N10-1047,0,0.0257012,"k by considering discourse-level information and an additional argument-specific measure that takes into account argument labels. We demonstrate the benefits of these measures in practice in Section 4.3. Similarity in WordNet. Given all synsets that contain the two predicates p1 , p2 , we compute their similarity in WordNet (Fellbaum 1998) as the maximal pairwise score calculated using the information content based measure proposed by Lin (1998). We rely on the WordNet hierarchy to find the least common subsumer (lcs) of two synsets and use the pre-computed Information Content (IC) files from Pedersen (2010) to compute this measure as defined in Equation (9). simWN (p1 , p2 ) = max hs1 ,s2 i:si ∈synsets(pi ) IC(lcs(s1 , s2 )) IC(s1 ) ∗ IC(s2 ) (9) 635 Computational Linguistics Volume 41, Number 4 Similarity in VerbNet. We additionally make use of VerbNet (Kipper et al. 2008) to compute similarities between verb pairs that cannot be captured by WordNet relations. Verbs in VerbNet are categorized into classes according to their meaning as well as syntactic behavior. A verb class C can recursively embed sub-classes Cs ∈ sub(C) that represent finer semantic and syntactic distinctions. In Equation (10"
J15-4003,D08-1020,0,0.0699801,"Missing"
J15-4003,J04-3003,0,0.0474076,"Missing"
J15-4003,postolache-etal-2006-transferring,0,0.0745251,"Missing"
J15-4003,W12-4501,0,0.0280163,"mentation of the induction procedure in Section 5.2. Single Document Preprocessing. We apply several preprocessing steps to each document in our data set. First, we use the Stanford CoreNLP package (Manning et al. 2014) ¨ for tokenization and sentence splitting. We then apply the MATE tools (Bjorkelund et al. 2010; Bohnet 2010), including the integrated PropBank/NomBank-based semantic parser, to determine local PAS. Finally, we resolve pronouns that occur in a PAS using the coreference resolution system by Martschat et al. (2012), which placed second for English in the CoNLL-2012 Shared Task (Pradhan et al. 2012). High Precision Alignments. Once all single documents are preprocessed, we align PAS across pairs of comparable texts. We want to induce reliable instances of implicit arguments based on aligned PASs pairs and hence apply our graph-based clustering technique using the high-precision tuning step described in Section 4.3. We run Figure 2 Illustration of the induction approach: Texts consist of PAS (represented by overlapping rounded rectangles); we exploit alignments between corresponding predicates across texts (solid lines) and co-referring entity mentions (dashed lines) to infer implicit arg"
J15-4003,J10-4004,0,0.0174609,"ument). Because our task is based in a monolingual setting, we can make use of the same preprocessing tools across texts. Paraphrasing and Event Coreference. We overcome the difficulty of inducing word alignments across comparable texts by computing alignments on the basis of predicate– argument structures. Using predicate–argument structures as targets makes our setting related to previous work on paraphrase detection and coreference resolution of event mentions. Each of these tasks focuses, however, on a different level of linguistic analysis from ours: Following the definitions embraced by Recasens and Vila (2010), “paraphrasing” is a relation between two lexical units that have the same meaning, whereas “coreference” indicates that two referential expressions point to the same referent in discourse.3 In contrast to work on paraphrasing, we are specifically interested in pairs of text fragments that involve implicit arguments, which can only be resolved in context. In line with our goal of inducing implicit arguments, we define the units or expressions to be aligned in our task as the predicate–argument structures that can (automatically) be identified in text. This task definition further makes our ta"
J15-4003,S12-1030,1,0.823849,"Section 4.3. 4.1 Corpus and Annotation As a basis for aligning predicate–argument structures across texts, we make use of a data set of comparable texts extracted from the English Gigaword corpus (Parker et al. 2011). The Gigaword corpus is one of the largest English corpora available in the news domain and contains over 9.8 million articles from seven newswire agencies that report on (the same) real-world incidents. The data set of comparable texts used in this work contains 167,728 pairs of articles that were extracted by matching the headlines of texts published within the same time frame (Roth and Frank 2012a). A set of such document headlines is given in Example 4: Example 4 India fires tested anti-ship cruise missile (Xinhua News Agency, 29 October 2003) India tests supersonic cruise anti-ship missile (Agence France Presse, 29 October 2003) URGENT: India tests anti-ship cruise missile (Associated Press Worldstream, 29 October 2003) ¨ We preprocess each article in the set of 167,728 pairs using the MATE tools (Bjorkelund et al. 2010; Bohnet 2010), including a state-of-the-art semantic role labeler that identifies PropBank/NomBank-style predicate–argument structures (Palmer, Gildea, and Kingsbury"
J15-4003,D12-1016,1,0.892284,"Missing"
J15-4003,S10-1008,0,0.477606,"vides an outlook on how instances of implicit arguments can be of use in a novel entity-based model of local coherence. Implicit Arguments. The role of implicit arguments was studied early on in the context of semantic processing (Fillmore 1986; Palmer et al. 1986), although most semantic 628 Roth and Frank Inducing Implicit Arguments from Comparable Texts role labeling systems nowadays operate solely within local syntactic structures and do not perform any additional inference regarding missing information. First data sets that focus on implicit arguments have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse,” Gerber and Chai (2012) made available implicit argument annotations for NomBank (Meyers, Reeves, and Macleod 2008), and Moor, Roth, and Frank (2013) provided annotations for parts of the OntoNotes corpus (Weischedel et al. 2011). All three resources are, however, severely limited: Annotations in the latter two studies are restricted to 10 and 5 predicate types, respectively; the training set of the SemEval task, in contrast, consists of full-text annotations for all occurring predicates but contains only 245 in"
J15-4003,S12-1001,1,0.928839,"Missing"
J15-4003,I08-1064,1,0.816176,"rationale of this approach is to induce annotated data in one language, given already-annotated instances in another language. As an example, semantic role annotations of a text in English can be transferred to a parallel text in order to induce annotated instances for a lexicon in another language (Pado´ and Lapata 2009). In previous work, this method has been applied on various levels of linguistic analysis: from syntactic information in the form of part-of-speech tags and dependencies (Yarowsky and Ngai 2001; Hwa et al. 2005), through annotation of temporal expressions and semantic roles (Spreyer and Frank 2008; van der Plas, Merlo, and Henderson 2011), to discourse-level phenomena such as coreference (Postolache, Cristea, and Orasan 2006). All of the aforementioned instances of the projection approach make use of the same underlying technique: Firstly, words are 630 Roth and Frank Inducing Implicit Arguments from Comparable Texts aligned in a parallel corpus using statistical word alignment; secondly, annotations on a single word or between multiple words in one text are transferred to the corresponding aligned word(s) in the parallel text. This procedure typically assumes that two parallel ¨ sente"
J15-4003,N09-1001,0,0.0344601,"Missing"
J15-4003,S10-1065,0,0.0943488,"Missing"
J15-4003,W11-0908,0,0.0403508,"Missing"
J15-4003,P11-2052,0,0.0564057,"Missing"
J15-4003,U06-1019,0,0.012047,"ing described in Section 4.2.3. Instead, it greedily merges as many 1-to-1 alignments as possible, starting with the highest similarity (Greedy). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation. We train our own word alignment model using the state-of-the-art word alignment tool Berkeley Aligner (Liang, Taskar, and Klein 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases using a re-implementation of a previously proposed paraphrase detection system based on lemma and n-gram overlap (Wan et al. 2006). In the following section, we abbreviate the alignment based model as WordAlign. 4.3.2 Results. The results for the alignment task are presented in Table 2. From all approaches, Greedy and WordAlign yield the lowest performance. For WordAlign, we observe two main reasons. On the one hand, sentence paraphrase detection does not perform perfectly. Hence, the extracted sentence pairs do not always contain gold 8 We also performed an evaluation on sentence-level predicate alignment, but skipped the discussion here as this task is not relevant for our induction framework. As the additional discour"
J15-4003,P13-2012,0,0.0505633,"Missing"
J15-4003,N01-1026,0,0.0744396,"n to ensure that necessary references are explicit and that redundant repetitions are avoided. 3.2 Semantic Resource Induction The methods applied in this article are based on ideas from previous work on inducing semantic resources from parallel and comparable texts. Most work in this direction has been done in the context of cross-lingual settings, including the learning of transla¨ tions of words and phrases using statistical word alignments (Kay and Roscheisen ˆ e, and Klein 2008, inter alia) and approaches to pro1993; DeNero, Bouchard-Cot´ jecting annotations from one language to another (Yarowsky and Ngai 2001; Kozhevnikov and Titov 2013, inter alia). In the following, we discuss previous approaches to annotation projection as well as related work in paraphrasing and event coreference. Annotation Projection. A widespread method for the induction of semantic resources is the so-called annotation projection approach. The rationale of this approach is to induce annotated data in one language, given already-annotated instances in another language. As an example, semantic role annotations of a text in English can be transferred to a parallel text in order to induce annotated instances for a lexicon in a"
J15-4003,C00-2137,0,0.0403076,"Missing"
J15-4003,N10-1043,0,\N,Missing
J15-4003,N10-1138,0,\N,Missing
J15-4003,H86-1011,0,\N,Missing
J15-4003,D08-1033,0,\N,Missing
J15-4003,P08-2011,0,\N,Missing
L18-1564,P09-1068,0,0.0612194,"spects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each ques"
L18-1564,P16-1223,0,0.0634014,"computed in the same way. We use different weight matrices for a, t and q, respectively. A combined representation p for the text–question pair is then constructed using a bilinear transformation matrix W: p = t> Wq (1) We compute a score for each answer by using the dot product and pass the scores for both answers through a softmax layer for prediction. The probability p for an answer a to be correct is thus defined as: Attentive Reader The attentive reader is a well-established machine comprehension model that reaches good performance e.g. on the CNN/Daily Mail corpus (Hermann et al., 2015; Chen et al., 2016). We use the model formulation by Chen et al. (2016) and Lai et al. (2017), who employ bilinear weight functions to compute both attention and answer-text fit. Bidirectional GRUs are used to encode questions, texts and answers into hidden representations. For a question q and an answer a, the last state of the GRUs, q and a, are used as representations, while the text is encoded as a sequence of hidden states t1 ...tn . We then compute an attention score sj for each hidden state tj using the question representation q, a weight matrix Wa , and an attention bias b. Last, a text representation t"
L18-1564,P17-1147,0,0.0871477,"Missing"
L18-1564,D17-1082,0,0.139289,"Missing"
L18-1564,W14-1606,1,0.892066,"to tell who ate the food: Rachel or the waitress. In contrast, if we utilize commonsense knowledge, in particular, script knowledge about the EATING IN A RESTAURANT scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel. Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney,"
L18-1564,L16-1555,1,0.938737,"scribe our data collection for these 3 components. We first describe a series of pilot studies that we conducted in order to collect commonsense inference questions (Section 2.1.). In Section 2.2., we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk1 (henceforth MTurk). Section 2.3. gives information about some necessary postprocessing steps and the dataset validation. Lastly, Section 2.4. gives statistics about the final dataset. 2.1. Pilot Study As a starting point for our pilots, we made use of texts from the InScript corpus (Modi et al., 2016), which provides stories centered around everyday situations (see Section 2.2.2.). We conducted three different pilot studies to determine the best way of collecting questions that require inference over commonsense knowledge: The most intuitive way of collecting reading comprehension questions is to show texts to workers and let them formulate questions and answers on the texts, which is what we tried internally in a first pilot. Since our focus is to provide an evaluation framework for inference over commonsense knowledge, we manually assessed the number of questions that indeed require comm"
L18-1564,K16-1008,1,0.87344,"i and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx. 2,100 texts and a total of approx. 14,00"
L18-1564,D14-1162,0,0.0796453,"Missing"
L18-1564,E14-1024,0,0.0215786,"ly, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx."
L18-1564,P16-1027,0,0.0137069,"(Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx. 2,100 texts and a total of"
L18-1564,D16-1264,0,0.0964981,"Missing"
L18-1564,P10-1100,1,0.940718,"e utilize commonsense knowledge, in particular, script knowledge about the EATING IN A RESTAURANT scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel. Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connectio"
L18-1564,D13-1020,0,0.338565,"Missing"
L18-1564,W17-2623,0,0.0482171,"rdson et al., 2013), BAbI (Weston et al., 2015), the Children’s Book Test (CBT, Hill et al. (2015)), CNN/Daily Mail (Hermann et al., 2015), the Stanford Question Answering Dataset (SQuAD, Rajpurkar et al. (2016)), and RACE (Lai et al., 2017). These datasets differ with respect to text type (Wikipedia texts, examination texts, etc.), mode of answer selection (span-based, multiple choice, etc.) and test systems regarding different aspects of language understand3572 ing, but they do not explicitly address commonsense knowledge. Two notable exceptions are the NewsQA and TriviaQA datasets. NewsQA (Trischler et al., 2017) is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. NewsQA closely resembles our own data collection with respect to the method of data acquisition. As for our data collection, full texts were not shown to workers as a basis for question formulation, but only the text’s title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background knowledge. The NewsQA text collection differs from ours in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge re"
L18-1564,L16-1556,1,0.888375,"rio and that can be answered from different texts (about that scenario), but for which a text does not need to provide the answer explicitly. The next section will describe the mode of collection chosen for the final dataset, based on the third pilot, in more detail. 2.2. Data Collection 2.2.1. Scenario Selection As mentioned in the previous section, we decided to base the question collection on script scenarios rather than specific texts. As a starting point for our data collection, we use 1 www.mturk.com scenarios from three script data collections (Regneri et al., 2010; Singh et al., 2002; Wanzare et al., 2016). Together, these resources contain more than 200 scenarios. To make sure that scenarios have different complexity and content, we selected 80 of them and came up with 20 new scenarios. Together with the 10 scenarios from InScript, we end up with a total of 110 scenarios. 2.2.2. Texts For the collection of texts, we followed Modi et al. (2016), where workers were asked to write a story about a given activity “as if explaining it to a child”. This results in elaborate and explicit texts that are centered around a single scenario. Consequently, the texts are syntactically simple, facilitating ma"
L18-1564,W17-0901,1,0.858709,"knowledge, in particular, script knowledge about the EATING IN A RESTAURANT scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel. Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks."
L18-1564,P08-1090,0,\N,Missing
N18-1023,P99-1042,0,0.522474,"espite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills. 1 Introduction Machine Comprehension of natural language text is a fundamental challenge in AI and it has received significant attention throughout the history of AI (Greene, 1959; McCarthy, 1976; Reiter, 1976; Winograd, 1980). In particular, in natural language processing (NLP) it has been studied under various settings, such as multiplechoice Question-Answering (QA) (Green Jr. et al., 1961), Reading Comprehension (RC) (Hirschman et al., 1999), Recognizing Textual Entailment (RTE) (Dagan et al., 2013) etc. The area has seen rapidly increasing interest, thanks to the existence of sizable datasets and standard benchmarks. CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016) to name a few, are some of the datasets that were released recently with the goal of facilitating research in machine comprehension. Despite all the excitement 252 Proceedings of NAACL-HLT 2018, pages 252–262 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics S3: Hearing n"
N18-1023,P13-1035,0,0.0648697,"Missing"
N18-1023,ide-etal-2008-masc,0,0.0213793,"ically collecting paragraphs, composing questions and answer-options through crowdsourcing platform, and manually curating the collected data. We also summarize a pilot study that helped us design this process, and end with a summary of statistics of the collected corpus. 3.1 Sources of documents The paragraphs used in our dataset are extracted from various sources. Here is the complete list of the text types and sources used in our dataset, and the number of paragraphs extracted from each category (indicated in square brackets on the right): 1. News: [121] • CNN (Hermann et al., 2015) • WSJ (Ide et al., 2008) • NYT (Ide et al., 2008) 2. Wikipedia articles [92] 3. Articles on society, law and justice (Ide and Suderman, 2006) [91] 4. Articles on history and anthropology (Ide et al., 2008) [65] 2 5. Elementary school science textbooks [153] 6. 9/11 reports (Ide and Suderman, 2006) [72] 7. Fiction: [277] • Stories from the Gutenberg project • Children stories from MCTest (Richardson et al., 2013) • Movie plots from CMU Movie Summary corpus (Bamman et al., 2013) Principles of design Questions and answers in our dataset are designed based on the following key principles: Multi-sentenceness. Questions in"
N18-1023,ide-suderman-2006-integrating,0,0.093396,"ly curating the collected data. We also summarize a pilot study that helped us design this process, and end with a summary of statistics of the collected corpus. 3.1 Sources of documents The paragraphs used in our dataset are extracted from various sources. Here is the complete list of the text types and sources used in our dataset, and the number of paragraphs extracted from each category (indicated in square brackets on the right): 1. News: [121] • CNN (Hermann et al., 2015) • WSJ (Ide et al., 2008) • NYT (Ide et al., 2008) 2. Wikipedia articles [92] 3. Articles on society, law and justice (Ide and Suderman, 2006) [91] 4. Articles on history and anthropology (Ide et al., 2008) [65] 2 5. Elementary school science textbooks [153] 6. 9/11 reports (Ide and Suderman, 2006) [72] 7. Fiction: [277] • Stories from the Gutenberg project • Children stories from MCTest (Richardson et al., 2013) • Movie plots from CMU Movie Summary corpus (Bamman et al., 2013) Principles of design Questions and answers in our dataset are designed based on the following key principles: Multi-sentenceness. Questions in our challenge require models to use information from multiple sentences of a paragraph. This is ensured through expl"
N18-1023,P16-1223,0,0.115073,"Missing"
N18-1023,D17-1215,0,0.0884839,"Missing"
N18-1023,P17-1147,0,0.118006,"Missing"
N18-1023,D17-1082,0,0.161802,"Missing"
N18-1023,Q15-1025,0,0.0598859,"Missing"
N18-1023,S18-1181,0,0.0231449,"Q (Rec(q)) with Q as the set of all questions. Since by design, each answer-option can be judged independently, we consider another metric, F1a , evaluating binary decisions on all the answer-options in the dataset. We define F1a to be the harmonic mean of Pre(Q) and Rec(Q), with S ˆ A(Q)| Pre(Q) = |A(Q)∩ ; A(Q) = q∈Q A(q); and ˆ |A(Q)| ˆ similar definitions for A(Q) and Rec(Q). 4.1 SurfaceLR (logistic regression baseline). As a simple baseline that makes use of our small training set, we reimplemented and trained a logistic regression model using word-based overlap features. As described in (Merkhofer et al., 2018), this baseline takes into account the lengths of a text, question and each answer candidate, as well as indicator features regarding the (co)occurrences of any words in them. Baselines Human. Human performance provides us with an estimate of the best achievable results on datasets. Using mechanical turk, we ask 4 people (limited to native speakers) to solve our data. We evaluate score of each label by averaging the decision of the individuals. SemanticILP (semi-structured baseline). This state-of-the-art solver, originally proposed for science questions and biology tests, uses a semistructure"
N18-1023,D15-1237,0,0.0582703,"Missing"
N18-1023,P17-2081,0,0.0584787,"Missing"
N18-1023,P15-1121,0,0.0803149,"Missing"
N18-1023,D13-1020,0,0.374141,"plete list of the text types and sources used in our dataset, and the number of paragraphs extracted from each category (indicated in square brackets on the right): 1. News: [121] • CNN (Hermann et al., 2015) • WSJ (Ide et al., 2008) • NYT (Ide et al., 2008) 2. Wikipedia articles [92] 3. Articles on society, law and justice (Ide and Suderman, 2006) [91] 4. Articles on history and anthropology (Ide et al., 2008) [65] 2 5. Elementary school science textbooks [153] 6. 9/11 reports (Ide and Suderman, 2006) [72] 7. Fiction: [277] • Stories from the Gutenberg project • Children stories from MCTest (Richardson et al., 2013) • Movie plots from CMU Movie Summary corpus (Bamman et al., 2013) Principles of design Questions and answers in our dataset are designed based on the following key principles: Multi-sentenceness. Questions in our challenge require models to use information from multiple sentences of a paragraph. This is ensured through explicit validation. We exclude any question that can be answered based on a single sentence from a paragraph. Open-endedness. Our dataset is not restricted to questions whose answer can be found verbatim in a paragraph. Instead, we provide a set of handcrafted answer-options f"
N19-1390,P18-1073,0,0.42452,"t’) Figure 1: Supervision for distinguishing antonyms from synonyms can be derived using discourse markers. Here, antonyms available in English (denoted by solid edge between hot and cold) are translated to German via cross-lingual word embeddings. Using co-occurrences with discourse markers indicative of antonymy (shown in box), we can identify pairs of words in the n-best translations (clouds) that are also antonymous (e.g., dashed edge between kalt and heiß). Introduction Recent work has shown that monolingual word embeddings in different languages can be aligned in an unsupervised manner (Artetxe et al., 2018; Kementchedjhieva et al., 2018). The resulting cross-lingual embeddings can be used to share supervision for lexical classification tasks across languages, when annotated data is not available in one language. For instance, a model for distinguishing lexical relations such as hypernymy and meronymy can be transferred to other languages (Glavaš and Vulić, 2018). However, this kind of transfer, using only cross-lingual embeddings, misses useful monolingual information available in the target language. In this paper, we consider one lexical classification task, namely the distinction between syn"
N19-1390,Q17-1010,0,0.0755618,"ins. 5 Analysis Table 2: Macro-averaged F1 -scores for crosslingual synonymy/antonymy distinction in German (de), Hindi (hi), and Vietnamese (vi). Significant differences from NoTrans are marked by asterisk(s) (* p&lt;0.1, ** p&lt;0.01). Monolingual results in English (en) are only shown for comparison. word tokens in text to those of the word forms that actually appear in the synonymy/antonymy datasets. Note that, while we use an unsupervised morphological analyzer, a stemmer can also be used, if available for that language. Next, we created monolingual embeddings for each language using fastText (Bojanowski et al., 2017). Finally, we applied the unsupervised variant of vecmap (Artetxe et al., 2018) to compute alignments and cross-lingual mappings. The word and discourse marker co-occurrence counts required for our approach are computed on the same monolingual corpora used for training monolingual embeddings. Results. Table 2 shows macro-averaged F1 scores for crosslingual synonymy/antonymy distinction (top part) as well as monolingual results in English for comparison (bottom part). The monolingual results show that word embeddings provide appropriate information for classification in most instances, achievin"
N19-1390,bojar-etal-2014-hindencorp,0,0.0238818,"ish training data in a target language, as described in Section 3.1. Our own approach, henceforth called LingTrans, is based on n-best translations and exploits word co-occurrences in contrast relations for reranking, as described in Section 3.2. All three models take cross-lingual word embeddings as input. We created these as follows. First, the unsupervised morphological analyzer of Xu et al. (2018) is used to lemmatize the monolingual corpora for the respective languages—the German Wikipedia, the Vietnamese portion of the LORELEI pack (Strassel and Tracey, 2016), and the HindEnCorp corpus (Bojar et al., 2014). This step ensures that we can compare the (stems of) 1 Since we do not assume access to syntactic paths in the target languages, we only experimented with lexical surface paths and found them to consistently degrade performance. 2 To verify the correctness of the crawled data, we asked a native speaker to validate all instances. Since synonym/antonym labels were derived automatically, validation was only performed to remove noisy instances. 3901 LexNet-crosslingual de (dev) NoTrans BestTrans LingTrans LexNet-monolingual (en) + path embeddings 59.9 62.2 64.2* hi vi 54.6 42.2 59.2 46.9 63.5* 5"
N19-1390,N18-2029,0,0.268247,"ds in the n-best translations (clouds) that are also antonymous (e.g., dashed edge between kalt and heiß). Introduction Recent work has shown that monolingual word embeddings in different languages can be aligned in an unsupervised manner (Artetxe et al., 2018; Kementchedjhieva et al., 2018). The resulting cross-lingual embeddings can be used to share supervision for lexical classification tasks across languages, when annotated data is not available in one language. For instance, a model for distinguishing lexical relations such as hypernymy and meronymy can be transferred to other languages (Glavaš and Vulić, 2018). However, this kind of transfer, using only cross-lingual embeddings, misses useful monolingual information available in the target language. In this paper, we consider one lexical classification task, namely the distinction between synonyms and antonyms, which is important for downstream applications such as contradiction detection (Harabagiu et al., 2006; Marneffe et al., 2008; Voorhees, 2008) and machine translation (Marton et al., 2011). To facilitate better transfer, we propose to use monolingual information in the form of word co-occurrences in contrast relations, in addition to cross-l"
N19-1390,C92-2082,0,0.489458,"uch information can be recovered (at least partially) by relying on linguistic intuitions about contrast relations in discourse. 3899 Proceedings of NAACL-HLT 2019, pages 3899–3905 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2 Related Work English The identification of paradigmatic relations such as synonymy and antonymy has been a task of interest for more than a decade. Early work focused on the identification of instances of a single relation: for example, Charles and Miller (1989) investigated co-occurrences as an indicator for antonymy, Hearst (1992) introduced a patternbased approach to identify hypernymy, and Edmonds and Hirst (2002) applied distributional statistics to identify synonymy. Beyond the identification of instances of a particular relation, more recent approaches attempt to distinguish relations such as synonymy and antonymy, based on lexico-syntactic patterns, distributional information, or combinations of both (Lin et al., 2003; Shwartz and Dagan, 2016). As supervision for weighting different features, most recent work makes use of lexical resources and/or lists of affix patterns that indicate contrast morphologically (Yih"
N19-1390,K18-1021,0,0.0253557,"ion for distinguishing antonyms from synonyms can be derived using discourse markers. Here, antonyms available in English (denoted by solid edge between hot and cold) are translated to German via cross-lingual word embeddings. Using co-occurrences with discourse markers indicative of antonymy (shown in box), we can identify pairs of words in the n-best translations (clouds) that are also antonymous (e.g., dashed edge between kalt and heiß). Introduction Recent work has shown that monolingual word embeddings in different languages can be aligned in an unsupervised manner (Artetxe et al., 2018; Kementchedjhieva et al., 2018). The resulting cross-lingual embeddings can be used to share supervision for lexical classification tasks across languages, when annotated data is not available in one language. For instance, a model for distinguishing lexical relations such as hypernymy and meronymy can be transferred to other languages (Glavaš and Vulić, 2018). However, this kind of transfer, using only cross-lingual embeddings, misses useful monolingual information available in the target language. In this paper, we consider one lexical classification task, namely the distinction between synonyms and antonyms, which is imp"
N19-1390,N15-1098,0,0.192558,"Missing"
N19-1390,P08-1118,0,0.0123589,"fication tasks across languages, when annotated data is not available in one language. For instance, a model for distinguishing lexical relations such as hypernymy and meronymy can be transferred to other languages (Glavaš and Vulić, 2018). However, this kind of transfer, using only cross-lingual embeddings, misses useful monolingual information available in the target language. In this paper, we consider one lexical classification task, namely the distinction between synonyms and antonyms, which is important for downstream applications such as contradiction detection (Harabagiu et al., 2006; Marneffe et al., 2008; Voorhees, 2008) and machine translation (Marton et al., 2011). To facilitate better transfer, we propose to use monolingual information in the form of word co-occurrences in contrast relations, in addition to cross-lingual embeddings (see Figure 1). In particular, we utilize the fact that discourse markers conveying contrast are more likely to be surrounded by antonyms than synonyms (e.g., hot…while…cold), as shown by Roth and Schulte im Walde (2014). Our analysis reveals that (1) fine-grained semantic information that is required to distinguish synonyms from antonyms is insufficiently prese"
N19-1390,W11-2128,0,0.0713667,"Missing"
N19-1390,E17-1008,0,0.0150844,"rker m in the corpus, n(w|m) is the number of times term w occurs with m, and n(w1 , w2 |m) is the number of co-occurrences w1 …m…w2 . For simplicity, we count all joint occurrences within a sentence, regardless of sentence length and distance between words. In practice, the set M is constructed by manually translating eight discourse markers that frequently indicate contrast relations according to the Penn Discourse Treebank 2.0 (Prasad et al., 2007): although, by comparison, by contrast, however, nevertheless, nonetheless, though, and thus. 4 Vietnamese. The antonymy and synonymy dataset by Nguyen et al. (2017) is used for training and estimating an upper bound in English. For crosslingual development and hyper-parameter selection, we use the German dataset by Glavaš and Vulić (2018). Specifically, we select the number of hidden layers {0, 1}, learning rate {0.001,…,0.1}, dropout rate {0.0, 0.5}, and whether to include information on paths between words {yes, no}.1 As examples of under-resourced languages, we evaluate on the Vietnamese dataset ViCon by Nguyen et al. (2018) and on a new Hindi dataset, which we crawled from hindi2dictionary.com.2 Note that annotated data in these languages is held out"
N19-1390,N18-2032,0,0.0916831,"omparison, by contrast, however, nevertheless, nonetheless, though, and thus. 4 Vietnamese. The antonymy and synonymy dataset by Nguyen et al. (2017) is used for training and estimating an upper bound in English. For crosslingual development and hyper-parameter selection, we use the German dataset by Glavaš and Vulić (2018). Specifically, we select the number of hidden layers {0, 1}, learning rate {0.001,…,0.1}, dropout rate {0.0, 0.5}, and whether to include information on paths between words {yes, no}.1 As examples of under-resourced languages, we evaluate on the Vietnamese dataset ViCon by Nguyen et al. (2018) and on a new Hindi dataset, which we crawled from hindi2dictionary.com.2 Note that annotated data in these languages is held out exclusively for testing. The task addressed here is to distinguish synonymous from antonymous words. Accordingly, we consider only word pairs for training and testing that are marked as antonyms or synonyms, even if the original dataset also contains unrelated words or other relations. Statistics of all considered word pairs, the ratio between synonyms and antonyms, as well as sizes of the text corpora used in our experiments are given in Table 1. Experiments Our ex"
N19-1390,N15-1100,0,0.13629,"identify hypernymy, and Edmonds and Hirst (2002) applied distributional statistics to identify synonymy. Beyond the identification of instances of a particular relation, more recent approaches attempt to distinguish relations such as synonymy and antonymy, based on lexico-syntactic patterns, distributional information, or combinations of both (Lin et al., 2003; Shwartz and Dagan, 2016). As supervision for weighting different features, most recent work makes use of lexical resources and/or lists of affix patterns that indicate contrast morphologically (Yih et al., 2012; Mohammad et al., 2013; Ono et al., 2015). Supervision for lexical classification tasks is not available in all languages. To overcome this difficulty, some approaches (Mrkšić et al., 2017; Glavaš and Vulić, 2018) combine resources for English and cross-lingual word embeddings to distinguish lexical relations in other languages. That is, they train and test one model across different languages. In contrast, we propose to first use unsupervised translation techniques to transfer supervision into a target language, given resources available in English. More specifically, our approach transfers supervision using a combination of unsuper"
N19-1390,P03-1017,0,0.0674076,"Missing"
N19-1390,P14-2086,1,0.896313,"Missing"
N19-1390,W16-5304,0,0.318302,"Missing"
N19-1390,L16-1521,0,0.014683,"ually. BestTrans uses 1st best translations of the English training data in a target language, as described in Section 3.1. Our own approach, henceforth called LingTrans, is based on n-best translations and exploits word co-occurrences in contrast relations for reranking, as described in Section 3.2. All three models take cross-lingual word embeddings as input. We created these as follows. First, the unsupervised morphological analyzer of Xu et al. (2018) is used to lemmatize the monolingual corpora for the respective languages—the German Wikipedia, the Vietnamese portion of the LORELEI pack (Strassel and Tracey, 2016), and the HindEnCorp corpus (Bojar et al., 2014). This step ensures that we can compare the (stems of) 1 Since we do not assume access to syntactic paths in the target languages, we only experimented with lexical surface paths and found them to consistently degrade performance. 2 To verify the correctness of the crawled data, we asked a native speaker to validate all instances. Since synonym/antonym labels were derived automatically, validation was only performed to remove noisy instances. 3901 LexNet-crosslingual de (dev) NoTrans BestTrans LingTrans LexNet-monolingual (en) + path embeddings 5"
N19-1390,P08-1008,0,0.0236891,"anguages, when annotated data is not available in one language. For instance, a model for distinguishing lexical relations such as hypernymy and meronymy can be transferred to other languages (Glavaš and Vulić, 2018). However, this kind of transfer, using only cross-lingual embeddings, misses useful monolingual information available in the target language. In this paper, we consider one lexical classification task, namely the distinction between synonyms and antonyms, which is important for downstream applications such as contradiction detection (Harabagiu et al., 2006; Marneffe et al., 2008; Voorhees, 2008) and machine translation (Marton et al., 2011). To facilitate better transfer, we propose to use monolingual information in the form of word co-occurrences in contrast relations, in addition to cross-lingual embeddings (see Figure 1). In particular, we utilize the fact that discourse markers conveying contrast are more likely to be surrounded by antonyms than synonyms (e.g., hot…while…cold), as shown by Roth and Schulte im Walde (2014). Our analysis reveals that (1) fine-grained semantic information that is required to distinguish synonyms from antonyms is insufficiently preserved cross-lingua"
N19-1390,C18-1005,0,0.0205438,"aining data in English and allows us to test in how far semantic properties related to the distinction between synonymy and antonymy are preserved cross-lingually. BestTrans uses 1st best translations of the English training data in a target language, as described in Section 3.1. Our own approach, henceforth called LingTrans, is based on n-best translations and exploits word co-occurrences in contrast relations for reranking, as described in Section 3.2. All three models take cross-lingual word embeddings as input. We created these as follows. First, the unsupervised morphological analyzer of Xu et al. (2018) is used to lemmatize the monolingual corpora for the respective languages—the German Wikipedia, the Vietnamese portion of the LORELEI pack (Strassel and Tracey, 2016), and the HindEnCorp corpus (Bojar et al., 2014). This step ensures that we can compare the (stems of) 1 Since we do not assume access to syntactic paths in the target languages, we only experimented with lexical surface paths and found them to consistently degrade performance. 2 To verify the correctness of the crawled data, we asked a native speaker to validate all instances. Since synonym/antonym labels were derived automatica"
N19-1390,C00-2137,0,0.0302111,"ross-lingual results by NoTrans indicate that aligning and mapping embedding spaces does not preserve all semantic properties relevant to the distinction between synonymy and antonymy. The use of first-best translations in BestTrans alleviates this issue partially and improves F1 by up to 4.6 points. Our intuition regarding discourse cues in LingTrans leads to further improvements of up to 9.9 additional points in F1 and considerably closes the gap between monolingual and cross-lingual results (56.8−64.2 vs. 70.1 F1 ). Based on an approximate randomization test over the respective test items (Yeh, 2000), we find the improvements of our proposed approach LingTrans over NoTrans to be significant in VietWe examine how far our proposed approach affects performance on the task of synonymy–antonymy distinction and discuss remaining shortcomings. A first observation concerns the general variance of results across languages. In addition to differences in terms of available corpus sizes, we observe different challenges in each language and dataset. Notably, each dataset is created with its own linguistically motivated definitions of antonymy and synonymy, some of which are more relaxed than others. F"
N19-1390,D12-1111,0,0.470252,"Missing"
P09-4010,W09-2814,1,\N,Missing
P09-4010,A00-2026,0,\N,Missing
P14-2086,J06-1005,0,0.25431,"524 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–530, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics based relation extraction to verbs, distinguishing five non-disjoint relations (similarity, strength, antonymy, enablement, happens-before). Pantel and Pennacchiotti (2006) developed Espresso, a weakly-supervised system that exploits patterns in large-scale web data to distinguish between five noun-noun relations (hypernymy, meronymy, succession, reaction, production). Similarly to Girju et al. (2006), they used generic patterns, but relied on a bootstrapping cycle combined with reliability measures, rather than manual resources. Whereas each of the aforementioned approaches considers only one word class and clearly disjoint categories, we distinguish between paradigmatic relations that can be distributionally very similar and propose a unified framework for nouns, verbs and adjectives. Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability"
P14-2086,W97-0802,0,0.0962232,"instances (cf. Yeh, 2000) revealed that only two differences in results are significant. We hypothesize that one reason for this outcome might be that both models cover complementary sets of instances. To verify this hypothesis, we apply a combined model, which is based on a weighted linear combination of distances computed by the two individual models.5 As displayed in Table 3, this combined model yields further improvements Development Set and Hyperparameters We select the hyperparameters of our model using an independent development set, which we extract from the lexical resource GermaNet (Hamp and Feldweg, 1997). For each considered word category, we extract instances of synonymy, antonymy and hypernymy. In total, 1502 instances are identified, with 64 of them overlapping with the evaluation data set described in Section 3. Note though that the development set is not used for evaluation but only to select the following hyperparameters. We experimented with different vector values (absolute frequency, log frequency, pointwise mutual information (PMI)), distance measures (cosine, euclidean) and normalization schemes. In contrast to S&K, who did not observe any improvements using PMI, we found it to per"
P14-2086,P13-2013,0,0.0208346,"les of discourse relations/markers. information to distinguish between paradigmatic relations. Our approach is motivated by linguistic studies that indicated a connection between discourse relations and lexical relations of words occurring in the respective discourse segments: Murphy et al. (2009) have shown, for example, that antonyms frequently serve as indicators for contrast relations in English and Swedish. More generally, pairs of word tokens have been identified as strong features for classifying discourse relations when no explicit discourse markers are available (Pitler et al., 2009; Biran and McKeown, 2013). Whereas word pairs have frequently been used as features for disambiguating discourse relations, to the best of our knowledge, our approach is novel in that we are the first to apply discourse relations as features for classifying lexical relations. One reason for this might be that discourse relations in general are only available in manually annotated corpora. Previous work has shown, however, that such relations can be classified reliably given the presence of explicit discourse markers.1 We hence rely on such markers as proxies for discourse relations (for examples, cf. Table 2). 4.1 Mod"
P14-2086,D13-1167,0,0.0316553,"Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) extended this approach to induce vector representations that can capture multiple relations. Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) proposed to distinguish between the three relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations i"
P14-2086,C92-2082,0,0.372261,"than manual resources. Whereas each of the aforementioned approaches considers only one word class and clearly disjoint categories, we distinguish between paradigmatic relations that can be distributionally very similar and propose a unified framework for nouns, verbs and adjectives. Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used pattern"
P14-2086,P04-1087,0,0.0350868,"acted from text. Each option comes with its own shortcomings: knowledge bases, on the one hand, are typically developed for a single language or domain, meaning that they might not generalize well; word patterns, on the other hand, are noisy and can be sparse for infrequent word pairs. In this paper, we propose to strike a balance between availability and restrictedness by making use of discourse markers. This approach has several advantages: markers are frequently found across genres (Webber, 2009), they exist in many languages (Jucker and Yiv, 1998), and capture various semantic properties (Hutchinson, 2004). We implement discourse markers within a vector space model that aims to distinguish between the three paradigmatic relations synonymy, antonymy and hypernymy in German and in English, across the three word classes of nouns, verbs, adjectives. We examine the performance of discourse markers as vector space dimensions in isolation and also explore their contribution in combination with lexical patterns. Distinguishing between paradigmatic relations such as synonymy, antonymy and hypernymy is an important prerequisite in a range of NLP applications. In this paper, we explore discourse relations"
P14-2086,W04-3205,0,0.0359404,"e relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Girju et al. (2003) applied a single pattern to distinguish pairs of nouns that are in a causal relationship from those that are not, and Girju et al. (2006) extended the work towards part–whole relations, applying a supervised, knowledge-intensive approach. Chklovski and Pantel (2004) were the first to apply pattern3 Baseline Model and Data Set The task addressed in this work is to distinguish between synonymy, antonymy and hypernymy. As a starting point, we build on the approach and data set used by Schulte im Walde and K¨oper (2013, henceforth just S&K). In their work, frequency statistics over automatically acquired co-occurrence patterns were found to be good indicators for the paradigmatic relation that holds between two given words of the same word class. They further experimented with refinements of the vector space model, for example, by only considering patterns o"
P14-2086,S12-1012,0,0.0231113,"y disjoint categories, we distinguish between paradigmatic relations that can be distributionally very similar and propose a unified framework for nouns, verbs and adjectives. Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘e"
P14-2086,P08-1118,0,0.173762,"Missing"
P14-2086,J02-2001,0,0.0169077,"erms of distributional similarity (Sch¨utze, 1992; Turney and Pantel, 2010), hence perform below their potential when inferring the type of relation that holds between two words. This distinction is crucial, however, in a range of tasks: in sentiment analysis, for example, words of the same and opposing polarity need to be distinguished; in textual entailment, systems further need to identify hypernymy because of directional inference requirements. 2 Related Work As mentioned above, there is a rich tradition of research on identifying a single paradigmatic relations. Work on synonyms includes Edmonds and Hirst (2002), who employed a co-occurrence network and second-order co-occurrence, and Curran (2003), who explored word-based and syntaxbased co-occurrence for thesaurus construction. 524 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–530, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics based relation extraction to verbs, distinguishing five non-disjoint relations (similarity, strength, antonymy, enablement, happens-before). Pantel and Pennacchiotti (2006) developed Espresso, a weakly-supervised"
P14-2086,D08-1103,0,0.158947,"identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) e"
P14-2086,D08-1094,0,0.0233038,"Missing"
P14-2086,J13-3004,0,0.106162,"on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) extended this approach to induce vector representations that can capture multiple relations. Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) pr"
P14-2086,N03-1011,0,0.0556205,"ge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) proposed to distinguish between the three relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Girju et al. (2003) applied a single pattern to distinguish pairs of nouns that are in a causal relationship from those that are not, and Girju et al. (2006) extended the work towards part–whole relations, applying a supervised, knowledge-intensive approach. Chklovski and Pantel (2004) were the first to apply pattern3 Baseline Model and Data Set The task addressed in this work is to distinguish between synonymy, antonymy and hypernymy. As a starting point, we build on the approach and data set used by Schulte im Walde and K¨oper (2013, henceforth just S&K). In their work, frequency statistics over automatically"
P14-2086,J06-3003,0,0.0247675,"ies to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) extended this approach to induce vector representations that can capture multiple relations. Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) proposed to distinguish between the three relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Girju et al. (2003) appli"
P14-2086,P06-1015,0,0.391085,"k on synonyms includes Edmonds and Hirst (2002), who employed a co-occurrence network and second-order co-occurrence, and Curran (2003), who explored word-based and syntaxbased co-occurrence for thesaurus construction. 524 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–530, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics based relation extraction to verbs, distinguishing five non-disjoint relations (similarity, strength, antonymy, enablement, happens-before). Pantel and Pennacchiotti (2006) developed Espresso, a weakly-supervised system that exploits patterns in large-scale web data to distinguish between five noun-noun relations (hypernymy, meronymy, succession, reaction, production). Similarly to Girju et al. (2006), they used generic patterns, but relied on a bootstrapping cycle combined with reliability measures, rather than manual resources. Whereas each of the aforementioned approaches considers only one word class and clearly disjoint categories, we distinguish between paradigmatic relations that can be distributionally very similar and propose a unified framework for nou"
P14-2086,P06-2111,0,0.0319444,"Missing"
P14-2086,P09-1076,0,0.125534,"tly relied on manually created knowledge sources, or lexico-syntactic patterns that can be automatically extracted from text. Each option comes with its own shortcomings: knowledge bases, on the one hand, are typically developed for a single language or domain, meaning that they might not generalize well; word patterns, on the other hand, are noisy and can be sparse for infrequent word pairs. In this paper, we propose to strike a balance between availability and restrictedness by making use of discourse markers. This approach has several advantages: markers are frequently found across genres (Webber, 2009), they exist in many languages (Jucker and Yiv, 1998), and capture various semantic properties (Hutchinson, 2004). We implement discourse markers within a vector space model that aims to distinguish between the three paradigmatic relations synonymy, antonymy and hypernymy in German and in English, across the three word classes of nouns, verbs, adjectives. We examine the performance of discourse markers as vector space dimensions in isolation and also explore their contribution in combination with lexical patterns. Distinguishing between paradigmatic relations such as synonymy, antonymy and hyp"
P14-2086,D08-1020,0,0.0942524,"Missing"
P14-2086,C04-1146,0,0.106838,"Missing"
P14-2086,P09-1077,0,0.0256296,". . . Table 2: Examples of discourse relations/markers. information to distinguish between paradigmatic relations. Our approach is motivated by linguistic studies that indicated a connection between discourse relations and lexical relations of words occurring in the respective discourse segments: Murphy et al. (2009) have shown, for example, that antonyms frequently serve as indicators for contrast relations in English and Swedish. More generally, pairs of word tokens have been identified as strong features for classifying discourse relations when no explicit discourse markers are available (Pitler et al., 2009; Biran and McKeown, 2013). Whereas word pairs have frequently been used as features for disambiguating discourse relations, to the best of our knowledge, our approach is novel in that we are the first to apply discourse relations as features for classifying lexical relations. One reason for this might be that discourse relations in general are only available in manually annotated corpora. Previous work has shown, however, that such relations can be classified reliably given the presence of explicit discourse markers.1 We hence rely on such markers as proxies for discourse relations (for examp"
P14-2086,prasad-etal-2008-penn,0,0.1444,"Missing"
P14-2086,C00-2137,0,0.196532,"Missing"
P14-2086,D12-1111,0,0.301787,"Missing"
P14-2086,E14-4008,1,0.771115,"tinguish between paradigmatic relations that can be distributionally very similar and propose a unified framework for nouns, verbs and adjectives. Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-p"
P14-2086,schafer-bildhauer-2012-building,0,0.0413909,"Missing"
P16-1113,W11-2136,0,0.0533615,"goal of semantic role labeling (SRL) is to identify and label the arguments of semantic predicates in a sentence according to a set of predefined relations (e.g., “who” did “what” to “whom”). Semantic roles provide a layer of abstraction beyond syntactic dependency relations, such as subject and object, in that the provided labels are insensitive to syntactic alternations and can also be applied to nominal predicates. Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to argu"
P16-1113,W09-1206,0,0.0259266,"Missing"
P16-1113,C10-3009,0,0.0561063,"Missing"
P16-1113,C10-1011,0,0.0217723,"Missing"
P16-1113,W09-1207,0,0.0285503,"Missing"
P16-1113,D09-1003,0,0.0204523,"Missing"
P16-1113,D15-1112,0,0.594303,"Missing"
P16-1113,S15-1033,0,0.194129,"pectively. 6 Related Work Neural Networks for SRL Collobert et al. (2011) pioneered neural networks for the task of P R F1 83.2 82.4 80.4 75.9 75.1 75.2 79.4 78.6 77.7 P R F1 81.8 81.2 82.1 78.5 78.3 75.4 80.1 79.7 78.6 P R F1 83.1 83.2 78.9 78.0 77.4 74.3 80.5 80.2 76.5 Table 7: Results (in percentage) on the CoNLL2009 test sets for Chinese, German and Spanish. semantic role labeling. They developed a feedforward network that uses a convolution function over windows of words to assign SRL labels. Apart from constituency boundaries, their system does not make use of any syntactic information. Foland and Martin (2015) extended their model and showcased significant improvements when including binary indicator features for dependency paths. Similar features were used by FitzGerald et al. (2015), who include role labeling predictions by neural networks as factors in a global model. These approaches all make use of binary features derived from syntactic parses either to indicate constituency boundaries or to represent full dependency paths. An extreme alternative has been recently proposed in Zhou and Xu (2015), who model SRL decisions with a multi-layered LSTM network that takes word sequences as input but no"
P16-1113,J02-3001,0,0.0697656,"traction beyond syntactic dependency relations, such as subject and object, in that the provided labels are insensitive to syntactic alternations and can also be applied to nominal predicates. Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008). Most semantic role labeling approaches to date rely heavily on lexical and syntactic indicator features. Through the availability of large annotated resources, such as PropBank (Palmer et al., 2005), st"
P16-1113,N15-1121,0,0.606604,"Missing"
P16-1113,D15-1169,0,0.0286861,"edicates. The difficulty lies in that simple lexical and syntactic indicator features are not able to model interactions triggered by such phenomena. For instance, con1192 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1192–1202, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics sider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bj¨orkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al. (2015); and easySRL, Lewis et al. (2015)). Despite all systems claiming stateof-the-art or competitive performance, none of them is able to correctly identify He as the agent argument of the predicate raise. Given the complex dependency path relation between the predicate and its argument, none of the systems actually identifies He as an argument at all. In this paper, we develop a new neural network model that can be applied to the task of semantic role labeling. The goal of this model is to better handle control predicates and other phenomena that can be observed from the dependency structure of a sentence. In particular, we aim t"
P16-1113,P15-2047,0,0.027825,"model the influence of discourse on role labeling decisions. Rather than coming up with completely new features, in this work we proposed to revisit some well-known features and represent them in a novel way that generalizes better. Our proposed model is inspired both by the necessity to overcome the problems of sparse lexico-syntactic features and by the recent success of SRL models based on neural networks. Dependency-based embeddings The idea of embedding dependency structures has previously been applied to tasks such as relation classification and sentiment analysis. Xu et al. (2015) and Liu et al. (2015) use neural networks to embed dependency paths between entity pairs. To identify the relation that holds between two entities, their approaches make use of pooling layers that detect parts of a path that indicate a specific relation. In contrast, our work aims at modeling an individual path as a complete sequence, in which every item is of relevance. Tai et al. (2015) and Ma et al. (2015) learn embeddings of dependency structures representing full sentences, in a sentiment classification task. In our model, embeddings are learned jointly with other features, and as a result problems that may r"
P16-1113,P15-2029,0,0.0133036,"d on neural networks. Dependency-based embeddings The idea of embedding dependency structures has previously been applied to tasks such as relation classification and sentiment analysis. Xu et al. (2015) and Liu et al. (2015) use neural networks to embed dependency paths between entity pairs. To identify the relation that holds between two entities, their approaches make use of pooling layers that detect parts of a path that indicate a specific relation. In contrast, our work aims at modeling an individual path as a complete sequence, in which every item is of relevance. Tai et al. (2015) and Ma et al. (2015) learn embeddings of dependency structures representing full sentences, in a sentiment classification task. In our model, embeddings are learned jointly with other features, and as a result problems that may result from erroneous parse trees are mitigated. 7 Conclusions We introduced a neural network architecture for semantic role labeling that jointly learns embeddings for dependency paths and feature combinations. Our experimental results indicate that our model substantially increases classification performance, leading to new state-of-the-art results. In a qualitive analysis, we found that"
P16-1113,J13-4006,0,0.043061,"to represent full dependency paths. An extreme alternative has been recently proposed in Zhou and Xu (2015), who model SRL decisions with a multi-layered LSTM network that takes word sequences as input but no syntactic parse information at all. Our approach falls in between the two extremes: we rely on syntactic parse information but rather than solely making using of sparse binary features, we explicitly model dependency paths in a neural network architecture. Other SRL approaches Within the SRL literature, recent alternatives to neural network architectures include sigmoid belief networks (Henderson et al., 2013) as well as low-rank tensor models (Lei et al., 2015). Whereas Lei et al. only make use of dependency paths as binary indicator features, Henderson et al. propose a joint model for syntactic and semantic parsing that learns and ap1199 plies incremental dependency path representations to perform SRL decisions. The latter form of representation is closest to ours, however, we do not build syntactic parses incrementally. Instead, we take syntactically preprocessed text as input and focus on the SRL task only. Apart from more powerful models, most recent progress in SRL can be attributed to novel"
P16-1113,P82-1020,0,0.651084,"Missing"
P16-1113,J05-1004,0,0.206555,"ldea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008). Most semantic role labeling approaches to date rely heavily on lexical and syntactic indicator features. Through the availability of large annotated resources, such as PropBank (Palmer et al., 2005), statistical models based on such features achieve high accuracy. However, results often fall short when the input to be labeled involves instances of linguistic phenomena that are relevant for the labeling decision but appear infrequently at training time. Examples include control and raising verbs, nested conjunctions or other recursive structures, as well as rare nominal predicates. The difficulty lies in that simple lexical and syntactic indicator features are not able to model interactions triggered by such phenomena. For instance, con1192 Proceedings of the 54th Annual Meeting of the As"
P16-1113,P10-1099,0,0.0169377,"Whereas Lei et al. only make use of dependency paths as binary indicator features, Henderson et al. propose a joint model for syntactic and semantic parsing that learns and ap1199 plies incremental dependency path representations to perform SRL decisions. The latter form of representation is closest to ours, however, we do not build syntactic parses incrementally. Instead, we take syntactically preprocessed text as input and focus on the SRL task only. Apart from more powerful models, most recent progress in SRL can be attributed to novel features. For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences. Zapirain et al. (2013) propose different selection preference models in order to deal with the sparseness of lexical features. Roth and Woodsend (2014) address the same problem with word embeddings and compositions thereof. Roth and Lapata (2015) recently introduced features that model the influence of discourse on role labeling decisions. Rather than coming up with completely new features, in this work we proposed to revisit some well-known features and represent them in a novel w"
P16-1113,C08-1050,0,0.100724,"et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008). Most semantic role labeling approaches to date rely heavily on lexical and syntactic indicator features. Through the availability of large annotated resources, such as PropBank (Palmer et al., 2005), statistical models based on such features achieve high accuracy. However, results often fall short when the input to be labeled involves instances of linguistic phenomena that are relevant for the labeling decision but appear infrequently at training time. Examples include control and raising verbs, nested conjunctions or other recursive structures, as well as rare nominal predicates. The diffic"
P16-1113,P14-2095,0,0.0292695,"Missing"
P16-1113,W05-0634,0,0.0866458,"es are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008). Most semantic role labeling approaches to date rely heavily on lexical and syntactic indicator features. Through the availability of large annotated resources, such as PropBank (Palmer et al., 2005), statistical models based on such features achieve high accuracy. However, results often fall short when the input to be labeled involves instances of linguistic phenomena that are relevant for the labeling decision but appear infr"
P16-1113,J08-2005,0,0.789326,"de range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008). Most semantic role labeling approaches to date rely heavily on lexical and syntactic indicator features. Through the availability of large annotated resources, such as PropBank (Palmer et al., 2005), statistical models based on such features achieve high accuracy. However, results often fall short when the input to be labeled involves instances of linguistic phenomena that are relevant for the labeling decision but appear infrequently at training time."
P16-1113,Q15-1032,1,0.825178,"tally. Instead, we take syntactically preprocessed text as input and focus on the SRL task only. Apart from more powerful models, most recent progress in SRL can be attributed to novel features. For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences. Zapirain et al. (2013) propose different selection preference models in order to deal with the sparseness of lexical features. Roth and Woodsend (2014) address the same problem with word embeddings and compositions thereof. Roth and Lapata (2015) recently introduced features that model the influence of discourse on role labeling decisions. Rather than coming up with completely new features, in this work we proposed to revisit some well-known features and represent them in a novel way that generalizes better. Our proposed model is inspired both by the necessity to overcome the problems of sparse lexico-syntactic features and by the recent success of SRL models based on neural networks. Dependency-based embeddings The idea of embedding dependency structures has previously been applied to tasks such as relation classification and sentime"
P16-1113,D14-1045,1,0.86505,"onjunctions or other recursive structures, as well as rare nominal predicates. The difficulty lies in that simple lexical and syntactic indicator features are not able to model interactions triggered by such phenomena. For instance, con1192 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1192–1202, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics sider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bj¨orkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al. (2015); and easySRL, Lewis et al. (2015)). Despite all systems claiming stateof-the-art or competitive performance, none of them is able to correctly identify He as the agent argument of the predicate raise. Given the complex dependency path relation between the predicate and its argument, none of the systems actually identifies He as an argument at all. In this paper, we develop a new neural network model that can be applied to the task of semantic role labeling. The goal of this model is to better handle control predicates and other phenomena that can be observed from"
P16-1113,P15-1109,0,0.203621,"art from constituency boundaries, their system does not make use of any syntactic information. Foland and Martin (2015) extended their model and showcased significant improvements when including binary indicator features for dependency paths. Similar features were used by FitzGerald et al. (2015), who include role labeling predictions by neural networks as factors in a global model. These approaches all make use of binary features derived from syntactic parses either to indicate constituency boundaries or to represent full dependency paths. An extreme alternative has been recently proposed in Zhou and Xu (2015), who model SRL decisions with a multi-layered LSTM network that takes word sequences as input but no syntactic parse information at all. Our approach falls in between the two extremes: we rely on syntactic parse information but rather than solely making using of sparse binary features, we explicitly model dependency paths in a neural network architecture. Other SRL approaches Within the SRL literature, recent alternatives to neural network architectures include sigmoid belief networks (Henderson et al., 2013) as well as low-rank tensor models (Lei et al., 2015). Whereas Lei et al. only make u"
P16-1113,Q13-1019,0,0.0083313,"ument identification, argument classification, and re-ranking. The neural-network components introduced in Section 2 are used in the last three steps. The following sub-sections describe all components in more detail. 3.1 Predicate Identification and Disambiguation Given a syntactically analyzed sentence, the first two steps in an end-to-end SRL system are to identify and disambiguate the semantic predicates in the sentence. Here, we focus on verbal and nominal predicates but note that other syntactic categories have also been construed as predicates in the NLP literature (e.g., prepositions; Srikumar and Roth (2013)). For both identification and disambiguation steps, we apply the same logistic reraise.01 1st best arg 2nd best arg ARG? ARG? best label 2nd best label best A0 A1 A0 raise.01 raise.01 R ERANKER he funds funds A0 A1 score raise.01 funds score best overall scoring structure step n and formalize the state of the hidden layer h and softmax output sc for each class category c as follows: h = max(0, WBh B + Weh en + bh ) sense P REDICATE class s label c h He had trouble raising funds. O UTPUT HeA0 had trouble raising fundsA1 . Figure 4: Pipeline architecture of our SRL system. gression classifiers"
P16-1113,P15-1150,0,0.0109569,"Missing"
P16-1113,J08-2002,0,0.0292913,"CATION ... o1 e1 en Vector of binary indicator features B (2) (3) D ISAMBIGUATION A RGUMENT (4) I DENTIFICATION Figure 3: Neural model for joint learning of path embeddings and higher-order features: The path sequence x1 . . . xn is fed into a LSTM layer, a hidden layer h combines the final embedding en and binary input features B, and an output layer s assigns the highest probable class label c. A RGUMENT C LASSIFICATION sc = 3 hs s Wes c en + Wc h + bc es hs Σi (Wi en + Wi h + bsi ) (6) (7) System Architecture The overall architecture of our SRL system closely follows that of previous work (Toutanova et al., 2008; Bj¨orkelund et al., 2009) and is depicted in Figure 4. We use a pipeline that consists of the following steps: predicate identification and disambiguation, argument identification, argument classification, and re-ranking. The neural-network components introduced in Section 2 are used in the last three steps. The following sub-sections describe all components in more detail. 3.1 Predicate Identification and Disambiguation Given a syntactically analyzed sentence, the first two steps in an end-to-end SRL system are to identify and disambiguate the semantic predicates in the sentence. Here, we f"
P16-1113,P12-1095,0,0.015937,"le labeling (SRL) is to identify and label the arguments of semantic predicates in a sentence according to a set of predefined relations (e.g., “who” did “what” to “whom”). Semantic roles provide a layer of abstraction beyond syntactic dependency relations, such as subject and object, in that the provided labels are insensitive to syntactic alternations and can also be applied to nominal predicates. Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and"
P16-1113,D15-1206,0,0.0113031,"roduced features that model the influence of discourse on role labeling decisions. Rather than coming up with completely new features, in this work we proposed to revisit some well-known features and represent them in a novel way that generalizes better. Our proposed model is inspired both by the necessity to overcome the problems of sparse lexico-syntactic features and by the recent success of SRL models based on neural networks. Dependency-based embeddings The idea of embedding dependency structures has previously been applied to tasks such as relation classification and sentiment analysis. Xu et al. (2015) and Liu et al. (2015) use neural networks to embed dependency paths between entity pairs. To identify the relation that holds between two entities, their approaches make use of pooling layers that detect parts of a path that indicate a specific relation. In contrast, our work aims at modeling an individual path as a complete sequence, in which every item is of relevance. Tai et al. (2015) and Ma et al. (2015) learn embeddings of dependency structures representing full sentences, in a sentiment classification task. In our model, embeddings are learned jointly with other features, and as a resu"
P16-1113,W04-3212,0,0.0228742,"word form, its predicted part-of-speech tag as well as dependency relations to all syntactic children. 3.2 Argument Identification and Classification Given a sentence and a set of sense-disambiguated predicates in it, the next two steps of our SRL system are to identify all arguments of each predicate and to assign suitable role labels to them. For both steps, we train several LSTM-based neural network models as described in Section 2. In particular, we train separate networks for nominal and verbal predicates and for identification and classification. Following the findings of earlier work (Xue and Palmer, 2004), we assume that different feature sets are relevant for the respective tasks and hence different embedding representations should be learned. As binary input features, we use the following sets from the SRL literature (Bj¨orkelund et al., 2010). 1195 Argument labeling step forget gate memory→gates |e| |h| alpha dropout rate Identification (verb) Identification (noun) Classification (verb) Classification (noun) − − + − + + − − 25 16 5 88 90 125 300 500 0.0006 0.0009 0.0155 0.0055 0.42 0.25 0.50 0.46 Table 2: Hyperparameters selected for best models and training procedures Lexico-syntactic feat"
P16-1113,W09-1209,0,0.592956,"Missing"
P16-1113,J13-3006,0,\N,Missing
P16-1113,W09-1201,0,\N,Missing
Q15-1032,E14-1023,0,0.0771782,"word in context can influence correct role assignment. While concepts such as polysemy, homonymy and metonymy are all relevant here, the scarce training data available for FrameNet-based SRL calls for a light-weight model that can be applied without large amounts of labeled data. We therefore employ distributional word representations which we critically adapt based on document content. We describe our contribution in Section 5.1. Entities that fill semantic roles are sometimes mentioned in discourse. Given a specific mention 3 We note that better results have been reported in Hermann et al. (2014) and T¨ackstr¨om et al. (2015). However, both of these more recent approaches rely on a custom frame identification component as well as proprietary tools and models for tagging and parsing which are not publicly available. Argument identification and classification Lemma form of f POS tag of f Any syntactic dependents of f * Subcat frame of f * Voice of a* Any lemma in a* Number of words in a First word and POS tag in a Second word and POS tag in a Last word and POS tag in a Relation from first word in a to its parent Relation from second word in a to its parent Relation from last word in a t"
Q15-1032,C10-3009,0,0.18338,"Missing"
Q15-1032,N10-1030,0,0.0483915,"a given sentence. Most subsequent work in SRL builds on Gildea and Jurafsky’s feature set, often with the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and tensor approximations to represent lexical and syntactic features as well as combinations thereof. To the best of our knowledge, there exists no prior work where features based on discourse context are used to assign roles on the sentence level."
Q15-1032,C12-1042,0,0.10028,"Missing"
Q15-1032,P10-1025,0,0.0644011,"roperties (typically represented by co-occurrence counts) of linguistic entities such as words and phrases (Sahlgren, 2008). Although the absolute meaning of distributional representations remains unclear, they have proven highly successful for modeling relative aspects of meaning, as required for instance in word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Given their ability to model lexical similarity, it is not surprising that such representations are also successful at representing similar words in semantic tasks related to role labeling (Pennacchiotti et al., 2008; Croce et al., 2010; Zapirain et al., 2013). Although distributional representations can be used directly as features for role labeling (Pad´o et al., 2008; Gorinski et al., 2013; Roth and Woodsend, 2014, inter alia), further gains should be possible when considering document-specific properties such as genre and context. This is particularly true in the context of FrameNet, where different senses are observed across a diverse range of texts including spoken dialogue and debate transcripts as well Country Frame Frame Element Iran Supply Commerce buy R ECIPIENT B UYER China Supply Commerce sell S UPPLIER S ELLER"
Q15-1032,D11-1096,0,0.0162915,"irst to model role assignment to verb arguments based on FrameNet. Their model makes use of lexical and syntactic features, including binary indicators for the words involved, syntactic categories, dependency paths as well as position and voice in a given sentence. Most subsequent work in SRL builds on Gildea and Jurafsky’s feature set, often with the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and t"
Q15-1032,P11-1144,0,0.144737,"s the baseline system described in Section 4. The second system, henceforth Framat+context , is an enhanced version of the baseline that additionally uses all extensions described in Section 5. Finally, we also consider the output of SEMAFOR (Das et al., 2014), a state-of-the-art model for frame-semantic role labeling. Although all systems are provided with entire documents as input, SEMAFOR and Framat process each document sentence-by-sentence whereas Framat+context also uses features over all sentences. For evaluation, we use the same FrameNet training and evaluation texts as established in Das and Smith (2011). We compute precision, recall and F1 -score using the modified SemEval-2007 scorer from the SEMAFOR website.6 6 7 http://www.ark.cs.cmu.edu/SEMAFOR/eval/ Results produced by running SEMAFOR on the exact same 455 Table 5: Full structure prediction results using gold frames, Framat and different sets of context features. All numbers are percentages. Results Table 4 summarizes our results with Framat, Framat+context , and SEMAFOR using gold and predicted frames (see the upper and lower half of the table, respectively). Although differences in system architecture lead to different precision/recal"
Q15-1032,J14-1002,0,0.401776,"ame elements which are instantiated within the same sentence (i.e., a given predicate’s arguments). The adopted SRL system has been developed for PropBank/NomBank-style role labeling and we make several changes to adapt it to FrameNet. Specifically, we change the argument labeling procedure from predicate-specific to frame-specific 2 Version 1.5, released September 2010. 451 roles and implement I/O methods to read and generate FrameNet XML files. For direct comparison with the previous state-of-the-art for FrameNetbased SRL, we further implement additional features used in the SEMAFOR system (Das et al., 2014) and combine the role labeling components of mate-tools with SEMAFOR’s preprocessing toolchain.3 All features used in our system are listed in Table 1. The main differences between our adaptation of mate-tools and SEMAFOR are as follows: whereas the latter implements identification and labeling of role fillers in one step, mate-tools follow the insight that these two steps are conceptually different (Xue and Palmer, 2004) and should be modeled separately. Accordingly, mate-tools contain a global reranking component which takes into account identification and labeling decisions while SEMAFOR on"
Q15-1032,D09-1003,0,0.046518,"often with the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and tensor approximations to represent lexical and syntactic features as well as combinations thereof. To the best of our knowledge, there exists no prior work where features based on discourse context are used to assign roles on the sentence level. Discourse-like features have been previously applied in models that deal with so"
Q15-1032,D10-1113,1,0.820547,"ticle about a politician’s visit to the country. as travel guides and newspaper articles. Country names, for example, can be observed as fillers for different roles depending on the text genre and its perspective. Whereas some text may talk about a country as an interesting holiday destination (e.g., “Berlitz Intro to Jamaica”), others may discuss what a country is good at or interested in (e.g., “Iran [Nuclear] Introduction”). A list of the most frequent roles assigned to different country names are displayed in Table 2. Previous approaches model word meaning in context (Thater et al., 2010; Dinu and Lapata, 2010, inter alia) using sentence-level information which is already available in traditional SRL systems in the form of explicit features. Here, we go one step further and define a simple model in which word meaning representations are adapted to each document. As a starting point, we use the GloVe toolkit (Pennington et al., 2014) for learning representations4 and apply it to the Wikipedia corpus made available by the Westbury Lab.5 The learned representations can be seen as word vectors whose components encode basic bits of related encyclopaedic knowledge. We adapt these general representations"
Q15-1032,S15-1033,0,0.0297529,"the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and tensor approximations to represent lexical and syntactic features as well as combinations thereof. To the best of our knowledge, there exists no prior work where features based on discourse context are used to assign roles on the sentence level. Discourse-like features have been previously applied in models that deal with so-called implicit arguments, i.e., roles which are not locally realized but resolvable within the greater discourse context (Ruppenhofer et al., 2010; Gerber and Chai, 2012). Successful features for resolving implicit arguments incl"
Q15-1032,J12-4003,0,0.0296284,"proaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and tensor approximations to represent lexical and syntactic features as well as combinations thereof. To the best of our knowledge, there exists no prior work where features based on discourse context are used to assign roles on the sentence level. Discourse-like features have been previously applied in models that deal with so-called implicit arguments, i.e., roles which are not locally realized but resolvable within the greater discourse context (Ruppenhofer et al., 2010; Gerber and Chai, 2012). Successful features for resolving implicit arguments include the distance between mentions and any discourse relations occurring between them (Gerber and Chai, 2012), roles assigned to mentions in the previous context, the discourse prominence of the denoted entity (Silberer and Frank, 2012), and its centering status (Laparra and Rigau, 2013). None of these features have been used in a standard SRL system to date (and trivially, not all of them will be helpful as, for example, the number of sentences between a predicate and an argument is always zero within a sentence). In this paper, we ext"
Q15-1032,J02-3001,0,0.423636,"the usefulness of such additional information. The remainder of this paper is structured as follows. In Section 2, we present related work on semantic role labeling and the various features applied in traditional SRL systems. In Section 3, we provide additional background on the FrameNet resource. Sections 4 and 5 describe our baseline system and contextual extensions, respectively, and Section 6 presents our experimental results. We conclude the paper by discussing in more detail the output of our system and highlighting avenues for future work. 2 Related Work Early work in SRL dates back to Gildea and Jurafsky (2002), who were the first to model role assignment to verb arguments based on FrameNet. Their model makes use of lexical and syntactic features, including binary indicators for the words involved, syntactic categories, dependency paths as well as position and voice in a given sentence. Most subsequent work in SRL builds on Gildea and Jurafsky’s feature set, often with the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (M"
Q15-1032,W13-0111,0,0.0283497,"f distributional representations remains unclear, they have proven highly successful for modeling relative aspects of meaning, as required for instance in word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Given their ability to model lexical similarity, it is not surprising that such representations are also successful at representing similar words in semantic tasks related to role labeling (Pennacchiotti et al., 2008; Croce et al., 2010; Zapirain et al., 2013). Although distributional representations can be used directly as features for role labeling (Pad´o et al., 2008; Gorinski et al., 2013; Roth and Woodsend, 2014, inter alia), further gains should be possible when considering document-specific properties such as genre and context. This is particularly true in the context of FrameNet, where different senses are observed across a diverse range of texts including spoken dialogue and debate transcripts as well Country Frame Frame Element Iran Supply Commerce buy R ECIPIENT B UYER China Supply Commerce sell S UPPLIER S ELLER Iraq Locative relation Arriving G ROUND G OAL Table 2: Most frequent roles assigned to country names appearing FrameNet texts: whereas Iran and China are mostl"
Q15-1032,P14-1136,0,0.379277,"h EH8 9AB {mroth,mlap}@inf.ed.ac.uk Abstract including question answering (Shen and Lapata, 2007), text-to-scene generation (Coyne et al., 2012), stock price prediction (Xie et al., 2013), and social network extraction (Agarwal et al., 2014). Whereas some tasks directly utilize information encoded in the FrameNet resource, others make use of FrameNet indirectly through the output of SRL systems that are trained on data annotated with frame-semantic representations. While advances in machine learning have recently given rise to increasingly powerful SRL systems following the FrameNet paradigm (Hermann et al., 2014; T¨ackstr¨om et al., 2015), little effort has been devoted to improve such models from a linguistic perspective. Frame semantic representations have been useful in several applications ranging from text-to-scene generation, to question answering and social network analysis. Predicting such representations from raw text is, however, a challenging task and corresponding models are typically only trained on a small set of sentence-level annotations. In this paper, we present a semantic role labeling system that takes into account sentence and discourse context. We introduce several new features"
Q15-1032,P10-1099,0,0.140601,"the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and tensor approximations to represent lexical and syntactic features as well as combinations thereof. To the best of our knowledge, there exists no prior work where features based on discourse context are used to assign roles on the sentence level. Discourse-like features have been previously applied in models that deal with so-called implicit arguments,"
Q15-1032,C08-1050,0,0.0833314,"avenues for future work. 2 Related Work Early work in SRL dates back to Gildea and Jurafsky (2002), who were the first to model role assignment to verb arguments based on FrameNet. Their model makes use of lexical and syntactic features, including binary indicators for the words involved, syntactic categories, dependency paths as well as position and voice in a given sentence. Most subsequent work in SRL builds on Gildea and Jurafsky’s feature set, often with the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth"
Q15-1032,W13-0114,0,0.0139823,"es on the sentence level. Discourse-like features have been previously applied in models that deal with so-called implicit arguments, i.e., roles which are not locally realized but resolvable within the greater discourse context (Ruppenhofer et al., 2010; Gerber and Chai, 2012). Successful features for resolving implicit arguments include the distance between mentions and any discourse relations occurring between them (Gerber and Chai, 2012), roles assigned to mentions in the previous context, the discourse prominence of the denoted entity (Silberer and Frank, 2012), and its centering status (Laparra and Rigau, 2013). None of these features have been used in a standard SRL system to date (and trivially, not all of them will be helpful as, for example, the number of sentences between a predicate and an argument is always zero within a sentence). In this paper, we extend the contextual features used for resolving implicit arguments to the SRL task and show how a set of discourse-level enhancements can be added to a traditional sentence-level SRL model. 3 FrameNet The Berkeley FrameNet project (Ruppenhofer et al., 2010) develops a semantic lexicon and an annotated example corpus based on Fillmore’s (1976) th"
Q15-1032,J13-4004,0,0.0120809,"efined frame elements) to just 27 (number of semantic types observed for frame elements in the training data). In practice, we define one binary indicator feature fs for each semantic type s observed at training time. Given a potential filler, we set the feature value of fs to 1 (otherwise 0) if and only if there exists a co-referent entity mention annotated as a frame element filler with semantic type s. Since texts in FrameNet do not contain any manual mark-up of coreference relations, we rely on entity mentions and coreference chains predicted by the Stanford Coreference Resolution system (Lee et al., 2013). 5.3 Discourse Newness Our third contextual feature type is based on the observation that the salience of a discourse entity and its semantic prominence are interrelated. Previous work (Rose, 2011) showed that semantic prominence, as signal-led by semantic roles, can better explain subsequent phenomena related to discourse salience (such as pronominalization) than syntactic indicators. Our question here is whether this insight can be also applied in reverse. Can information on discourse salience be useful as an indicator for semantic roles? For this feature, we make use of the same coreferenc"
Q15-1032,N15-1121,0,0.040238,"Missing"
Q15-1032,N13-1090,0,0.0211214,"computed coreference chains. We describe the motivation and actual implementation of this feature in Section 5.3. 452 Modeling Word Meaning in Context The underlying idea of distributional models of semantics is that meaning can be acquired based on distributional properties (typically represented by co-occurrence counts) of linguistic entities such as words and phrases (Sahlgren, 2008). Although the absolute meaning of distributional representations remains unclear, they have proven highly successful for modeling relative aspects of meaning, as required for instance in word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Given their ability to model lexical similarity, it is not surprising that such representations are also successful at representing similar words in semantic tasks related to role labeling (Pennacchiotti et al., 2008; Croce et al., 2010; Zapirain et al., 2013). Although distributional representations can be used directly as features for role labeling (Pad´o et al., 2008; Gorinski et al., 2013; Roth and Woodsend, 2014, inter alia), further gains should be possible when considering document-specific properties such as genre and context. This is particularly true in th"
Q15-1032,P04-1043,0,0.035425,"), who were the first to model role assignment to verb arguments based on FrameNet. Their model makes use of lexical and syntactic features, including binary indicators for the words involved, syntactic categories, dependency paths as well as position and voice in a given sentence. Most subsequent work in SRL builds on Gildea and Jurafsky’s feature set, often with the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using"
Q15-1032,C08-1084,0,0.0306773,"Missing"
Q15-1032,D08-1048,1,0.901343,"Missing"
Q15-1032,D14-1162,0,0.100984,"hains. We describe the motivation and actual implementation of this feature in Section 5.3. 452 Modeling Word Meaning in Context The underlying idea of distributional models of semantics is that meaning can be acquired based on distributional properties (typically represented by co-occurrence counts) of linguistic entities such as words and phrases (Sahlgren, 2008). Although the absolute meaning of distributional representations remains unclear, they have proven highly successful for modeling relative aspects of meaning, as required for instance in word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Given their ability to model lexical similarity, it is not surprising that such representations are also successful at representing similar words in semantic tasks related to role labeling (Pennacchiotti et al., 2008; Croce et al., 2010; Zapirain et al., 2013). Although distributional representations can be used directly as features for role labeling (Pad´o et al., 2008; Gorinski et al., 2013; Roth and Woodsend, 2014, inter alia), further gains should be possible when considering document-specific properties such as genre and context. This is particularly true in the context of FrameNet, whe"
Q15-1032,D14-1045,1,0.93992,"2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and tensor approximations to represent lexical and syntactic features as well as combinations thereof. To the best of our knowledge, there exists no prior work where features based on discourse context are used to assign roles on the sentence level. Discourse-like features have been previously applied in models that deal with so-called implicit arguments, i.e., roles which are not locally realized but resolvable within the greater discourse context (Ruppenhofer et al., 2010; Gerber and Chai, 2012). Successful fe"
Q15-1032,R11-1046,0,0.0186717,"ents suitable for an entity given co-occurring words, we can also can explicitly consider previous role assignments to the same entity. As shown in Table 2, a country that fills the S UPPLIER role is more likely to also fill the role of a S ELLER than that of a B UYER. Given the high number of different frame elements in FrameNet, only a small fraction of pairs can be found in the training data, which entails that directly utilizing role co-occurrences might not be helpful. In order to benefit from previous role assignments in discourse, we follow related work on resolving implicit arguments (Ruppenhofer et al., 2011; Silberer and Frank, 2012) and consider the semantic types of role assignments (see Section 3) as features instead of the role labels themselves. This tremendously reduces the feature space from more than 8,000 options (number of defined frame elements) to just 27 (number of semantic types observed for frame elements in the training data). In practice, we define one binary indicator feature fs for each semantic type s observed at training time. Given a potential filler, we set the feature value of fs to 1 (otherwise 0) if and only if there exists a co-referent entity mention annotated as a fr"
Q15-1032,D07-1002,1,0.911714,"Missing"
Q15-1032,S12-1001,0,0.134012,"res based on discourse context are used to assign roles on the sentence level. Discourse-like features have been previously applied in models that deal with so-called implicit arguments, i.e., roles which are not locally realized but resolvable within the greater discourse context (Ruppenhofer et al., 2010; Gerber and Chai, 2012). Successful features for resolving implicit arguments include the distance between mentions and any discourse relations occurring between them (Gerber and Chai, 2012), roles assigned to mentions in the previous context, the discourse prominence of the denoted entity (Silberer and Frank, 2012), and its centering status (Laparra and Rigau, 2013). None of these features have been used in a standard SRL system to date (and trivially, not all of them will be helpful as, for example, the number of sentences between a predicate and an argument is always zero within a sentence). In this paper, we extend the contextual features used for resolving implicit arguments to the SRL task and show how a set of discourse-level enhancements can be added to a traditional sentence-level SRL model. 3 FrameNet The Berkeley FrameNet project (Ruppenhofer et al., 2010) develops a semantic lexicon and an an"
Q15-1032,Q15-1003,0,0.179993,"Missing"
Q15-1032,P10-1097,0,0.0614322,"Missing"
Q15-1032,P05-1073,0,0.167102,"Missing"
Q15-1032,P13-1086,0,0.0608974,"Missing"
Q15-1032,W04-3212,0,0.108074,"generate FrameNet XML files. For direct comparison with the previous state-of-the-art for FrameNetbased SRL, we further implement additional features used in the SEMAFOR system (Das et al., 2014) and combine the role labeling components of mate-tools with SEMAFOR’s preprocessing toolchain.3 All features used in our system are listed in Table 1. The main differences between our adaptation of mate-tools and SEMAFOR are as follows: whereas the latter implements identification and labeling of role fillers in one step, mate-tools follow the insight that these two steps are conceptually different (Xue and Palmer, 2004) and should be modeled separately. Accordingly, mate-tools contain a global reranking component which takes into account identification and labeling decisions while SEMAFOR only uses reranking techniques to filter overlapping argument predictions and other constraints (see Das et al., 2014 for details). We discuss the advantage of a global reranker for our setting in Section 5. 5 Extensions based on Context Context can be relevant for semantic role labeling in various different ways. In this section, we motivate and describe four extensions over previous approaches. The first extension is a se"
Q15-1032,C00-2137,0,0.19467,"Missing"
Q15-1032,J13-3006,0,\N,Missing
roth-schulte-im-walde-2008-corpus,poesio-etal-2002-acquiring,1,\N,Missing
roth-schulte-im-walde-2008-corpus,N07-1025,0,\N,Missing
roth-schulte-im-walde-2008-corpus,J07-2002,0,\N,Missing
roth-schulte-im-walde-2008-corpus,J06-2001,1,\N,Missing
roth-schulte-im-walde-2008-corpus,J98-1004,0,\N,Missing
roth-schulte-im-walde-2008-corpus,P98-2127,0,\N,Missing
roth-schulte-im-walde-2008-corpus,C98-2122,0,\N,Missing
S12-1030,J08-1001,0,0.0161348,"lly or globally. For example, Centering Theory (Grosz et al., 1995) provides a framework to model local coherence by relating the choice of referring expressions to the salience of an entity at certain stages of a discourse. An example for a global coherence model would be Rhetorical Structure Theory (Mann and Thompson, 1988), which addresses overall text structure by means of coherence relations between the parts of a text. In addition to such theories, computational approaches have been proposed to capture corresponding phenomena empirically. A prominent example is the entity-based model by Barzilay and Lapata (2008). In their approach, local coherence is modeled by the observation of sentence-to-sentence realization patterns of individual entities. The learned model reflects a key idea from Centering Theory, namely that adjacent sentences in a coherent discourse are likely to involve the same entities. One shortcoming of Barzilay and Lapata’s model (and extensions of it) is that it only investigates overt realization patterns in terms of grammatical functions. These functions reflect explicit realizations of predicate argument structures (PAS), but they do not capture the full range of salience factors."
S12-1030,N04-1015,0,0.0429036,"rse. We henceforth refer to such cases as non-realized arguments. Our main hypothesis is that context specific realization patterns for PAS can be automatically 218 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 218–227, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics learned from a semantically parsed corpus of comparable text pairs. This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved participants. As a first step towards this overall goal, we describe the construction of a resource that contains more than 160,000 document pairs that are known to talk about the sa"
S12-1030,P01-1008,0,0.103415,"e potential benefits of the created resource in more detail. Section 5 presents experiments on predicate alignment using this new data set and outlines first results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work Data sets comprising parallel texts have been released for various different tasks, including paraphrase extraction and statistical machine translation (SMT). While corpora for SMT are typically multilingual (e.g. Europarl, Koehn (2005)), there also exist monolingual parallel corpora that consist of multiple translations of one text into the same language (Barzilay and McKeown, 2001; Huang et al., 2002, inter alia). Each translation can provide alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same. A higher degree of variation can be found in the Microsoft Research Paraphrase Corpus (e.g. MSRPC, Dolan and Brockett (2005)), which consists of paraphrases automatically extracted from different sources. In the MSRPC, however, original discourse contexts are not provided for each sentence. In contrast to truly parallel monolingual corpora, there also exist a range of comparable corpora that hav"
S12-1030,W09-2816,0,0.0612606,"n and Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish a coherent discourse. Different aspects related to this setting have been studied in previous work. For example, Filippova and Strube (2007) and Cahill and Riester (2009) examine factors that determine constituent order and Belz et al. (2009) study the conditions for the use of different types of referring expressions. The specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. As in the aforementioned work, we are specifically interested in the generation of coherent discourses (e.g. for summarization). Yet, our work also complements research in discourse analysis. A recent example for such work is the Semeval 2010 Task 10 (Ruppenhofer et al., 2010), which aims at linking events and their particip"
S12-1030,C10-3009,0,0.0564182,"Missing"
S12-1030,C10-1011,0,0.0228425,"tal of 9.8 million newswire articles from seven distinct sources. For construction of our corpus we make use of all combinations of agency pairs in Gigaword. 3.1 Corpus Creation In order to extract pairs of articles describing the same news event, we implemented the pairwise similarity method presented by Wubben et al. (2009). The method is based on measuring word overlap in news headlines, weighting each word by its TF*IDF score to give a higher impact to words occurring with lower frequency. As our focus is to provide 220 Gold Standard Annotation We pre-processed all texts using MATE tools (Bohnet, 2010; Bj¨orkelund et al., 2010), a pipeline of natural language processing modules including a state-of-the-art semantic role labeler that computes Prop/NomBank annotations (Palmer et al., 2005; Meyers et al., 2008). The output was used to provide pre-labeled verbal and nominal predicates for annotation. We asked two students1 to tag alignments of corresponding predicates in 70 text pairs derived from the created corpus. All document pairs were randomly chosen from the AFP and APW sections of Gigaword with the constraint that each text consists of 100 to 300 words2 . We chose this constraint as lo"
S12-1030,P09-1092,0,0.0209954,"ra that have been used for tasks such as (multi-document) summarization (McKeown and Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish a coherent discourse. Different aspects related to this setting have been studied in previous work. For example, Filippova and Strube (2007) and Cahill and Riester (2009) examine factors that determine constituent order and Belz et al. (2009) study the conditions for the use of different types of referring expressions. The specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. As in the aforementioned work, we are specifically interested in the generation of coherent discourses (e.g. for summarization). Yet, our work also complements research in discourse analysis. A recent example for such work is the Semeval 2010 Task 10 (Ru"
S12-1030,J08-4005,0,0.142242,"baseline for this task is to align all predicates whose lemmas are identical (SameLemma). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). We train our own word alignment model using the state-of-the-art tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases for this baseline using a re-implementation of the paraphrase detection system by Wan et al. (2006). In the following sections, we abbreviate this model as WordAlign. 5.3 Results Following Cohn et al. (2008) we measure precision as the number of predicted alignments also annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments devided by the total number of sure alignments in the gold standard. We subsequently compute the F1 -score as the harmonic mean between precision and recall. Table 2 presents the results for our model and the two baselines. From all four approaches, WordAlign performs worst. We identify two main reasons for this: On the one hand, the paraphrase detection does not perform perfectly. Hen"
S12-1030,I05-5002,0,0.0603853,"cluding paraphrase extraction and statistical machine translation (SMT). While corpora for SMT are typically multilingual (e.g. Europarl, Koehn (2005)), there also exist monolingual parallel corpora that consist of multiple translations of one text into the same language (Barzilay and McKeown, 2001; Huang et al., 2002, inter alia). Each translation can provide alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same. A higher degree of variation can be found in the Microsoft Research Paraphrase Corpus (e.g. MSRPC, Dolan and Brockett (2005)), which consists of paraphrases automatically extracted from different sources. In the MSRPC, however, original discourse contexts are not provided for each sentence. In contrast to truly parallel monolingual corpora, there also exist a range of comparable corpora that have been used for tasks such as (multi-document) summarization (McKeown and Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identi"
S12-1030,P07-1041,0,0.054786,"xist a range of comparable corpora that have been used for tasks such as (multi-document) summarization (McKeown and Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish a coherent discourse. Different aspects related to this setting have been studied in previous work. For example, Filippova and Strube (2007) and Cahill and Riester (2009) examine factors that determine constituent order and Belz et al. (2009) study the conditions for the use of different types of referring expressions. The specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. As in the aforementioned work, we are specifically interested in the generation of coherent discourses (e.g. for summarization). Yet, our work also complements research in discourse analysis. A recent example for such work i"
S12-1030,J95-2003,0,0.521808,"investigation of discourse coherence phenomena. Initial experiments on the task of predicting predicate alignments across text pairs show promising results. Our findings establish that manual and automatic predicate alignments across texts are feasible and that our data set holds potential for empirical research into a variety of discourse-related tasks. 1 Introduction Research in the fields of discourse and pragmatics has led to a number of theories that try to explain and formalize the effect of discourse coherence inducing elements either locally or globally. For example, Centering Theory (Grosz et al., 1995) provides a framework to model local coherence by relating the choice of referring expressions to the salience of an entity at certain stages of a discourse. An example for a global coherence model would be Rhetorical Structure Theory (Mann and Thompson, 1988), which addresses overall text structure by means of coherence relations between the parts of a text. In addition to such theories, computational approaches have been proposed to capture corresponding phenomena empirically. A prominent example is the entity-based model by Barzilay and Lapata (2008). In their approach, local coherence is m"
S12-1030,2005.mtsummit-papers.11,0,0.0413496,"ted tasks. Section 3 introduces the new task together with a description of how we prepared a suitable data set. Section 4 discusses the potential benefits of the created resource in more detail. Section 5 presents experiments on predicate alignment using this new data set and outlines first results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work Data sets comprising parallel texts have been released for various different tasks, including paraphrase extraction and statistical machine translation (SMT). While corpora for SMT are typically multilingual (e.g. Europarl, Koehn (2005)), there also exist monolingual parallel corpora that consist of multiple translations of one text into the same language (Barzilay and McKeown, 2001; Huang et al., 2002, inter alia). Each translation can provide alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same. A higher degree of variation can be found in the Microsoft Research Paraphrase Corpus (e.g. MSRPC, Dolan and Brockett (2005)), which consists of paraphrases automatically extracted from different sources. In the MSRPC, however, original discourse co"
S12-1030,N06-1014,0,0.0742809,"ation of four different similarity measures. Subsequently, we also tune the weighting scheme for similarity measures on the development set. We found the best performing combination of weights to be 0.09, 0.19, 0.48 and 0.24 for simWN , simVN , simDist and simArgs , respectively. Baselines. A simple baseline for this task is to align all predicates whose lemmas are identical (SameLemma). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). We train our own word alignment model using the state-of-the-art tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases for this baseline using a re-implementation of the paraphrase detection system by Wan et al. (2006). In the following sections, we abbreviate this model as WordAlign. 5.3 Results Following Cohn et al. (2008) we measure precision as the number of predicted alignments also annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments devided by the total number of sure alignments in the gold standard. We subsequently comp"
S12-1030,J05-1004,0,0.00661811,"rder to extract pairs of articles describing the same news event, we implemented the pairwise similarity method presented by Wubben et al. (2009). The method is based on measuring word overlap in news headlines, weighting each word by its TF*IDF score to give a higher impact to words occurring with lower frequency. As our focus is to provide 220 Gold Standard Annotation We pre-processed all texts using MATE tools (Bohnet, 2010; Bj¨orkelund et al., 2010), a pipeline of natural language processing modules including a state-of-the-art semantic role labeler that computes Prop/NomBank annotations (Palmer et al., 2005; Meyers et al., 2008). The output was used to provide pre-labeled verbal and nominal predicates for annotation. We asked two students1 to tag alignments of corresponding predicates in 70 text pairs derived from the created corpus. All document pairs were randomly chosen from the AFP and APW sections of Gigaword with the constraint that each text consists of 100 to 300 words2 . We chose this constraint as longer text pairs contain a high number of unrelated predicates, making this task difficult to manage for the annotators. Sure and possible links. Following standard practice in word alignmen"
S12-1030,S10-1008,0,0.286229,"9) examine factors that determine constituent order and Belz et al. (2009) study the conditions for the use of different types of referring expressions. The specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. As in the aforementioned work, we are specifically interested in the generation of coherent discourses (e.g. for summarization). Yet, our work also complements research in discourse analysis. A recent example for such work is the Semeval 2010 Task 10 (Ruppenhofer et al., 2010), which aims at linking events and their participants in discourse. The provided data sets for this task, however, are critically small (438 train and 525 test sentences). Eventually, the corpus we present in this paper could also be beneficial for data-driven approaches to role linking in discourse. 3 A Corpus for Aligning Predications across Comparable Texts a high-quality data set for predicate alignment and follow-up tasks, we impose an additional date constraint to favor precision over recall. We apply this constraint by requiring a pair of articles to be published within a two-day time f"
S12-1030,P10-1098,0,0.0570476,"ts. Our main hypothesis is that context specific realization patterns for PAS can be automatically 218 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 218–227, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics learned from a semantically parsed corpus of comparable text pairs. This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved participants. As a first step towards this overall goal, we describe the construction of a resource that contains more than 160,000 document pairs that are known to talk about the same events and participants. Example (1), extracted from our corpus"
S12-1030,U06-1019,0,0.0377495,"to be 0.09, 0.19, 0.48 and 0.24 for simWN , simVN , simDist and simArgs , respectively. Baselines. A simple baseline for this task is to align all predicates whose lemmas are identical (SameLemma). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). We train our own word alignment model using the state-of-the-art tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases for this baseline using a re-implementation of the paraphrase detection system by Wan et al. (2006). In the following sections, we abbreviate this model as WordAlign. 5.3 Results Following Cohn et al. (2008) we measure precision as the number of predicted alignments also annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments devided by the total number of sure alignments in the gold standard. We subsequently compute the F1 -score as the harmonic mean between precision and recall. Table 2 presents the results for our model and the two baselines. From all four approaches, WordAlign performs worst. We i"
S12-1030,W09-0621,0,0.0313421,"Missing"
S13-1043,P05-1018,0,0.0218793,"SemEval task contains only 245 resolved implicit arguments in total. As pointed out by Silberer and Frank (2012), additional training data can be heuristically created by treating anaphoric mentions as implicit arguments. Their experimental results showed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun m"
S13-1043,J08-1001,0,0.0283184,"y treating anaphoric mentions as implicit arguments. Their experimental results showed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity m"
S13-1043,P11-2040,0,0.01551,"tically by only considering PAS with identical predicate form and constituent order. We found that this restriction constrains affected arguments to be modifiers, prepositional phrases and direct objects. We argue that this is actually a desirable property because more complicated alternations could affect coherence by themselves; resulting interplays would make it difficult to distinguish between the isolated effect of argument realization itself and other effects, triggered for example by sentence order (Gordon et al., 1993). 5.2 Annotation We set up a web experiment using the NLTK package (Belz and Kow, 2011) to collect (local) coherence ratings for implicit and explicit arguments. For this experiment, we compiled a data set of 150 document pairs. As described in Section 5.1, each text pair consists of mostly the same text, with the only difference being one argument realization. We presented all 150 pairs to two annotators7 and asked them to indicate their preference for one alternative over the other using a continuous slider scale. The annotators got to see the full texts, with the alternatives presented next to each other. To make texts easier to read and differences easier to spot, we collaps"
S13-1043,C10-3009,0,0.04757,"Missing"
S13-1043,C10-1011,0,0.0896888,"Missing"
S13-1043,E09-1018,0,0.125959,"originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared 307 The aim of this work is to automatically construct a data set"
S13-1043,S10-1059,0,0.300227,"Missing"
S13-1043,P08-2011,0,0.153737,"but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared 307 The aim of this work is to automatically construct a data set of implicit arguments and their discourse antecedents. We"
S13-1043,P11-1118,0,0.0841378,"ons (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mo"
S13-1043,P11-2022,0,0.0792339,"ons (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mo"
S13-1043,W07-2321,0,0.024618,"th manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 198"
S13-1043,J12-4003,0,0.062769,"reasons for this are the scarcity of annotated data, and the inherent difficulty of inferring discourse antecedents automatically. In this paper, we propose to induce implicit arguments and discourse antecedents by exploiting complementary (explicit) information obtained from monolingual comparable texts (Section 3). We apply the empirically acquired data in argument linking (Section 4) and coherence modeling (Section 5). We conclude with a discussion on the advantages of our data set and outline directions for future work (Section 6). 2 task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations for the NomBank corpus (Meyers et al., 2008) and Moor et al. (2013) provide annotations for parts of the OntoNotes corpus (Weischedel et al., 2011). However, these resources are very limited: The annotations by Moor et al. and Gerber and Chai are restricted to 5 and 10 predicate types, respectively. The training set of the SemEval task contains only 245 resolved implicit arguments in total. As pointed out by Silberer and Frank (2012), additional training data can be heuristically created by treating anaphoric mentions as implicit arguments. Their e"
S13-1043,W13-0111,0,0.642617,"Missing"
S13-1043,J95-2003,0,0.684005,"Missing"
S13-1043,W13-0114,0,0.107611,"Missing"
S13-1043,D12-1045,0,0.0299107,"set of implicit arguments and their discourse antecedents. We propose an induction approach that exploits complementary information obtained from pairs of comparable texts. As a basis for this approach, we rely on several preparatory steps proposed in the literature that first identify information two documents have in common (cf. Figure 1). In particular, we align corresponding predicateargument structures (PAS) using graph-based clustering (Roth and Frank, 2012b). We then determine co-referring entities across the texts using coreference resolution techniques on concatenated document pairs (Lee et al., 2012). These preprocessing steps are described in more detail in Section 3.1. Given the preprocessed comparable texts and aligned PAS, we propose to heuristically identify implicit arguments and link them to their antecedents via the cross-document coreference chains. We describe the details of this approach in Section 3.2. 3.1 Data preparation The starting point for our approach is the data set of automatically aligned predicate pairs that has been released by Roth and Frank (2012a).1 This data 1 cf. http://www.cl.uni-heidelberg.de/%7Emroth/ Sentence that comprises a PAS with an (correctly predict"
S13-1043,J13-4004,0,0.0192441,"t induced antecedent The [∅A0 ] [operatingA3 ] loss, as measured by . . . widened to 189 million euros . . . It was handed over to Mozambican control . . . 33 years after [∅A0 ] independence. . . . [local officials A0 ] failed to immediately report [the accident A1 ] [∅A2 ] . . . T-Online[’s] Mozambique[’s] [to] the government Table 1: Three positive examples of automatically induced implicit argument and antecedent pairs. Cross-document coreference. We apply crossdocument coreference resolution to induce antecedents for implicit arguments. In practice, we use the Stanford Coreference System (Lee et al., 2013) and run it on pairs of texts by simply providing a single document as input, comprising of a concatenation of the two texts. To perform this step with high precision, we only use the most precise resolution sieves: “String Match”, “Relaxed String Match”, “Precise Constructs”, “Strict Head Match [A-C]”, and “Proper Head Noun Match”. Figure 1: Illustration of the induction approach: texts consist of PAS (represented by overlapping circles); we exploit alignments between corresponding predicates across texts (marked by solid lines) and co-referring entities (marked by dotted lines) to infer impl"
S13-1043,N10-1043,0,0.0210972,"ession and syntactic form. Both extremes of salience, i.e., contexts of referential continuity (Brown, 1983) and irrelevance, can also be reflected by the non-realization of an entity. Altough specific instances of non-realization, so-called zero anaphora, have been well-studied in discourse analysis (Sag and Hankamer, 1984; Tanenhaus and Carlson, 1990, inter alia), this phenomenon has widely been ignored in computational approaches to entitybased coherence modeling. It could, however, provide an explanation for local coherence in cases that are not covered by current models of Centering (cf. Louis and Nenkova (2010)). In this work, we propose a new model to predict whether realizing an argument contributes to local coherence in a given position in discourse. Example (1) shows a text fragment, in which argument realization is necessary in the first sentence but redundant in the second. Implicit arguments are a discourse-level phenomenon that has not been extensively studied in semantic processing. One reason for this lies in the scarce amount of annotated data sets available. We argue that more data of this kind would be helpful to improve existing approaches to linking implicit arguments in discourse and"
S13-1043,W12-4511,0,0.0428612,"Missing"
S13-1043,P09-1025,0,0.0180409,"lts showed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit a"
S13-1043,W13-0211,1,0.806288,"Missing"
S13-1043,P86-1004,0,0.716489,"uments and their antecedents by exploiting comparable texts; we show how the induced data can be used as training data for improving existing argument linking models; finally, we present a novel approach to modeling local coherence that extends previous approaches by taking into account non-explicit entity references. 1 (1) Introduction Semantic role labeling systems traditionally process text in a sentence-by-sentence fashion, constructing local structures of semantic meaning (Palmer et al., 2010). Information relevant to these structures, however, can be non-local in natural language texts (Palmer et al., 1986; Fillmore, 1986, inter alia). In this paper, we view instances of this phenomenon, also referred to as implicit arguments, as elements of discourse. In a coherent discourse, each utterance focuses on a salient set of entities, also called “foci” (Sidner, 1979) or “centers” (Joshi and Kuhn, 1979). According to the theory of Centering (Grosz El Salvador is now the only Latin American country which still has troops in [Iraq]. Nicaragua, Honduras and the Dominican Republic have withdrawn their troops [∅]. From a semantic processing perspective, a human reader can easily infer that “Iraq”, the mar"
S13-1043,D08-1020,0,0.126358,"ns as implicit arguments. Their experimental results showed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring refe"
S13-1043,S12-1030,1,0.826033,"have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared 307 The aim of this work is to automatically construct a data set of implicit arguments and their discourse antecedents. We propose an induction approach that exploits complementary information obtained from pairs of comparable texts. As a basis for this approach, we rely on several preparatory steps proposed in the literature that first identify information two documents have in common (cf. Figure 1). In particular, we align corresponding predicateargument structures (PAS) using graph-based clustering (Roth and Frank, 2012b). We then determine co-referring entities across the texts using coreference resolution techniques on concatenated document pairs (Lee et al., 2012). These preprocessing steps are described in more detail in Section 3.1. Given the preprocessed comparable texts and aligned PAS, we propose to heuristically identify implicit arguments and link them to their antecedents via the cross-document coreference chains. We describe the details of this approach in Section 3.2. 3.1 Data preparation The starting point for our approach is the data set of automatically aligned predicate pairs that has been r"
S13-1043,D12-1016,1,0.850005,"have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared 307 The aim of this work is to automatically construct a data set of implicit arguments and their discourse antecedents. We propose an induction approach that exploits complementary information obtained from pairs of comparable texts. As a basis for this approach, we rely on several preparatory steps proposed in the literature that first identify information two documents have in common (cf. Figure 1). In particular, we align corresponding predicateargument structures (PAS) using graph-based clustering (Roth and Frank, 2012b). We then determine co-referring entities across the texts using coreference resolution techniques on concatenated document pairs (Lee et al., 2012). These preprocessing steps are described in more detail in Section 3.1. Given the preprocessed comparable texts and aligned PAS, we propose to heuristically identify implicit arguments and link them to their antecedents via the cross-document coreference chains. We describe the details of this approach in Section 3.2. 3.1 Data preparation The starting point for our approach is the data set of automatically aligned predicate pairs that has been r"
S13-1043,S10-1008,0,0.158797,"spectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared 307 The aim of this work is to automatically construct a data set of implicit arguments and their discourse antecedents. We propose an induction approach that exploits complementary information obtained from pairs of comparable texts. As a basis for this approach, we rely on several preparatory steps proposed in the literature that first identify information two documents have in common (cf. Figure 1). In particular, we align corresponding predicateargument structures (PAS) using graph-based clustering (Roth and Frank, 2012b). We then determine co-referring entities"
S13-1043,S12-1001,1,0.832791,"data set and outline directions for future work (Section 6). 2 task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations for the NomBank corpus (Meyers et al., 2008) and Moor et al. (2013) provide annotations for parts of the OntoNotes corpus (Weischedel et al., 2011). However, these resources are very limited: The annotations by Moor et al. and Gerber and Chai are restricted to 5 and 10 predicate types, respectively. The training set of the SemEval task contains only 245 resolved implicit arguments in total. As pointed out by Silberer and Frank (2012), additional training data can be heuristically created by treating anaphoric mentions as implicit arguments. Their experimental results showed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been app"
S13-1043,S10-1065,0,0.305685,"Missing"
S13-1043,W11-0908,0,0.242624,"mple of an original and modified (marked by an asterik) sentence: (2) [The Dalai Lama’sA0 ] visit [to FranceA1 ] ends on Tuesday. * [The Dalai Lama’sA0 ] visit ends on Tuesday. Note that adding and removing arguments at random can lead to structures that are semantically implausible. Hence, we restrict this procedure to predicate-argument structures (PAS) that actually occur and are aligned across two texts, and create modifications by replacing a single argument position in one text with the corresponding argument position in the comparable text. Examples (2) and (3) 5 Results as reported in Tonelli and Delmonte (2011) Results computed as an average over the scores given for both test files; rounded towards the number given for the test file that contained more instances. 6 311 show two such comparable texts. The original PAS in Example (2) contains an explicit argument that is implicit in the aligned PAS and hence removed in the modified version. Vice versa, the original text in (3) involves an implicit argument, which is made explicit in the modified version. (3) [The Dalai Lama’sA0 ] visit coincides with the Beijing Olympics. * [The Dalai Lama’sA0 ] visit [to FranceA1 ] coincides with the Beijing Olympic"
S13-1043,C00-2137,0,0.084284,"Missing"
S13-1043,H86-1011,0,\N,Missing
S17-1016,P10-1143,0,0.026874,"narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible. 1 Introduction Event structure is a prominent topic in NLP. While semantic role labelers (Gildea and Jurafsky, 2002; Palmer et al., 2010) are well-established tools for the analysis of the internal structure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities"
S17-1016,P09-1068,0,0.160534,"the analysis of the internal structure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et"
S17-1016,L16-1555,1,0.896046,"veryday activities, so called event sequence descriptions (ESDs). ESDs consist of short telegram-style descriptions of single events (event descriptions, ED). The textual order of EDs corresponds to the temporal order of respective events, i.e. temporal information is explicitly encoded. DeScript contains 50 ESDs for each of 40 different scenarios. Alongside the ESDs, it also provides gold event paraphrase sets, i.e. clusters of all event descriptions denoting the same event type, labeled with the respective type. While DeScript is a source of structured script knowledge, the InScript corpus (Modi et al., 2016) provides us with the appropriate kind of narrative texts. InScript is a collection of 910 stories centered around some specific scenario, for 10 of the 40 scenarios in DeScript, e.g. BAKING A CAKE, RIDING A BUS , TAKING A SHOWER . All verbs occurring in the texts are annotated with an event type if they are relevant to the script instantiated by the story; as non-script event otherwise. In the upper part of Fig. 1, you see the initial fragment of a story about baking a cake; together with a script excerpt in the lower part, depicted by labeled event paraphrase sets. I looked up the recipe and"
S17-1016,W14-1606,0,0.175518,"75); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address the task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect explicit script knowledge via crowdsourcing, by asking people to describe everyday activities. These crowdsourced descriptions form a basis for high-quality automatic extraction of script structure without any h"
S17-1016,E14-1006,1,0.837053,"fsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address the task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect explicit script knowledge via crowdsourcing, by asking people to describe everyday activities. These crowdsourced descriptions form a basis for high-quality automatic extraction of script structure without any human intervention (Regne"
S17-1016,Q17-1003,1,0.848171,"n that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address the task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect expli"
S17-1016,J02-3001,0,0.0278061,"derstanding and is relevant for a variety of downstream tasks. In this paper, we consider two recent datasets which provide a rich and general representation of script events in terms of paraphrase sets. We introduce the task of mapping event mentions in narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible. 1 Introduction Event structure is a prominent topic in NLP. While semantic role labelers (Gildea and Jurafsky, 2002; Palmer et al., 2010) are well-established tools for the analysis of the internal structure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long"
S17-1016,E12-1034,0,0.415153,"Missing"
S17-1016,D12-1045,0,0.0269025,"ipt event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible. 1 Introduction Event structure is a prominent topic in NLP. While semantic role labelers (Gildea and Jurafsky, 2002; Palmer et al., 2010) are well-established tools for the analysis of the internal structure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to"
S17-1016,P10-1100,1,0.84377,"tructure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address"
S17-1016,P16-1113,1,0.812832,"h the current script scenario, we employ two features: a binary feature indicating whether the verb is used in the ESDs for the given scenario; and a scenariospecific tf–idf score that is computed by treating all ESDs from a scenario as one document, summed over the verb and its dependents. In Section 4.2, we evaluate models with and without script features, to test the impact of scenario-specific information. Frame Feature. We further employ framesemantic information because we expect script events to typically evoke certain frames.We use a state-of-the-art semantic role labeler (Roth, 2016; Roth and Lapata, 2016) based on FrameNet (Rup2 For EDs, we use all mentioned head nouns. To emphasize the importance of the verb, we double its weight when averaging. 4 Because our CRF model only supports nominal features, we discretize embeddings from code.google.com/ archive/p/word2vec/ by binning the component values into three intervals [−∞, −], [−, ], [, ∞]. The hyperparameter  is determined on a held-out development set. 3 P Lemma Our model Our model (scen. indep.) R F1 0.365 0.949 0.526 0.628 0.817 0.709 0.513 0.877 0.645 Table 1: Identification of script-relevant verbs within a scenario and independent"
S17-1016,S15-1024,1,0.873865,"Missing"
S17-1016,L16-1556,1,0.905957,"ssed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address the task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect explicit script knowledge via crowdsourcing, by asking people to describe everyday activities. These crowdsourced descriptions form a basis for high-quality automatic extraction of script structure without any human intervention (Regneri et al., 2010; Wanzare et al., 2017). The events of the resulting structure are defined as sets of alternative realizations, which cover lexical variation and provide paraphrase information. To the best of our knowledge, these advantages have not been explicitly used elsewhere. Aligning script structures with texts is a complex task. In a first at"
S17-1016,W17-0901,1,0.781511,"task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect explicit script knowledge via crowdsourcing, by asking people to describe everyday activities. These crowdsourced descriptions form a basis for high-quality automatic extraction of script structure without any human intervention (Regneri et al., 2010; Wanzare et al., 2017). The events of the resulting structure are defined as sets of alternative realizations, which cover lexical variation and provide paraphrase information. To the best of our knowledge, these advantages have not been explicitly used elsewhere. Aligning script structures with texts is a complex task. In a first attempt, we assume that three steps are necessary to solve it, although in the long run, an integrated approach will be preferable: First, the script which is addressed by the event mention must be identified. Second, it has to be decided whether a verb denotes a script event at all. Fina"
S17-1016,S15-2132,0,\N,Missing
S17-1016,J82-1004,0,\N,Missing
S18-1119,D15-1075,0,0.0806933,"Missing"
S18-1119,P08-1090,0,0.586286,"corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text. The paper is organized as follows: In Section 2, we give an overview of other machine compreh"
S18-1119,P09-1068,0,0.104067,"Missing"
S18-1119,P16-1223,0,0.0969874,"Missing"
S18-1119,P17-1168,0,0.024165,"are employed to enrich word token representations. ZMU (Li and Zhou, 2018) consider a wide variety of neural models, ranging from CNNs, LSTMs and BiLSTMs with attention, together with pretrained Word2Vec and GloVe embeddings. They also employ data augmentation methods typically used in image processing. Their best performing model is a BiLSTM with attention mechanism and combined GloVe and Word2Vec embeddings. ECNU (Sheng et al., 2018) use BiGRUs and BiLSTMs to encode questions, answers and texts. They implement a multi-hop attention mechanism from question to text (a Gated Attention Reader (Dhingra et al., 2017)). ´ ELiRF-UPV (Jos´e -Angel Gonz´alez et al., 2018) employs a BiLSTM with attention to find similarities between texts, questions, and answers. Each word is represented based on Numberbatch embeddings, which encode information from ConceptNet. 752 YNU AI1799 (Liu et al., 2018) submitted an ensemble of neural network models based on LSTMs, RNNs, and BiLSTM/CNN combinations, with attention mechanisms. In addition to word2vec embeddings, positional embeddings are used that are generated based on word embeddings. Rank Team name y/n what why who where when 1 2 3 4 5 6 7 8 9 10 11 Yuanfudao MITRE J"
S18-1119,S18-1174,0,0.0156272,"2: The accuracy of participating systems and the two baselines in total, on commonsense-based questions (CS), text-based questions (TXT) and on out-of-domain questions (from the 5 held-out testing scenarios). The best performance for each column is marked in bold print. Significant differences in results between two adjacent lines are marked by an asterisk (* p&lt;0.05) in the upper line. The last line shows the human upper bound (Ostermann et al., 2018) as comparison. (based on ConceptNet). The model is pretrained on another large machine comprehension dataset, namely the RACE corpus. YNU Deep (Ding and Zhou, 2018) test different LSTMs and BiLSTMs variants to encode questions, answers and texts. A simple attention mechanism is applied between question–answer and text–answer pairs. The final submission is an ensemble of five model instances. MITRE (Merkhofer et al., 2018) use a combination of 3 systems - two LSTMs with attention mechanisms, and one logistic regression model using patterns based on the vocabulary of the training set. The two neural models use different word embeddings - one trained on GoogleNews, another one trained on Twitter, which were enriched with word overlap features. Interestingly"
S18-1119,S18-1176,0,0.0287908,"Missing"
S18-1119,S18-1172,0,0.0613083,"Missing"
S18-1119,P17-1147,0,0.0637147,"Missing"
S18-1119,S18-1180,0,0.0405537,"other one trained on Twitter, which were enriched with word overlap features. Interestingly, the simple logistic regression model achieves competitive performance and would have ranked 4th as an individual system. Jiangnan (Xia, 2018) applies a BiLSTM over GloVe and CoVe embeddings (McCann et al., 2017) with an additional attention mechanism. The attention mechanism computes soft word alignment between words in the question and the text or answer. Manual features, including part-of-speech tags, named entitity types, and term frequencies, are employed to enrich word token representations. ZMU (Li and Zhou, 2018) consider a wide variety of neural models, ranging from CNNs, LSTMs and BiLSTMs with attention, together with pretrained Word2Vec and GloVe embeddings. They also employ data augmentation methods typically used in image processing. Their best performing model is a BiLSTM with attention mechanism and combined GloVe and Word2Vec embeddings. ECNU (Sheng et al., 2018) use BiGRUs and BiLSTMs to encode questions, answers and texts. They implement a multi-hop attention mechanism from question to text (a Gated Attention Reader (Dhingra et al., 2017)). ´ ELiRF-UPV (Jos´e -Angel Gonz´alez et al., 2018) e"
S18-1119,S18-1120,0,0.229149,"Missing"
S18-1119,S18-1173,0,0.0325421,"Missing"
S18-1119,S18-1181,0,0.0662618,"Missing"
S18-1119,K16-1008,1,0.821428,"ate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text. The paper is organized as follows: In Section 2, we give an overview of other machine comprehension datasets. In Section 3, we describe"
S18-1119,L16-1555,1,0.854976,"riety of tasks, including story understanding (Schank, 1990), information extraction (Rau et al., 1989), and drawing inferences from texts (Miikkulainen, 1993). Factual knowledge is mentioned explicitly in texts from sources such as Wikipedia and news papers. On the contrary, script knowledge is often implicit in the texts as it is assumed to be known to the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proce"
S18-1119,W14-1606,1,0.907601,"the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the durat"
S18-1119,Q17-1003,1,0.839109,"she thus refers to Rachel. This shared task assesses how the inclusion of commonsense knowledge benefits natural language understanding systems. In particular, we focus on commonsense knowledge about everyday activities, referred to as scripts. Scripts are sequences of events describing stereotypical human activities (also called scenarios), for example baking a cake, taking a bus, etc. (Schank and Abelson, 1975). The concept of scripts has its underpinnings in cognitive psychology and has been shown to be an important component of the human cognitive system (Bower et al., 1979; Schank, 1982; Modi et al., 2017). From an application perspective, scripts have been shown to be useful for a variety of tasks, including story understanding (Schank, 1990), information extraction (Rau et al., 1989), and drawing inferences from texts (Miikkulainen, 1993). Factual knowledge is mentioned explicitly in texts from sources such as Wikipedia and news papers. On the contrary, script knowledge is often implicit in the texts as it is assumed to be known to the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative"
S18-1119,N16-1098,0,0.0711233,"mind think about nothing but peaceful, happy thoughts. I stayed in there for only about ten minutes because it was so hot and steamy. When I got out, I turned the sauna off to save energy and took a cool shower. I got out of the shower and dried off. After that, I put on my extra set of clean clothes I brought with me, and got in my car and drove home. 2 Q1 Where did they sit inside the sauna? a. on the floor b. on a bench Q2 How long did they stay in the sauna? a. about ten min- b. over thirty utes minutes Figure 1: An example for a text from MCScript with 2 reading comprehension questions. (Mostafazadeh et al., 2016)). These tasks test a system’s ability to learn script knowledge from a text but they do not provide a mechanism to evaluate how useful script knowledge is in natural language understanding tasks. Our shared task bridges this gap by directly relating commonsense knowledge and language comprehension. The task has a machine comprehension setting: A machine is given a text document and asked questions based on the text. In addition to what is mentioned in the text, answering the questions requires knowledge beyond the facts mentioned in the text. In particular, a substantial subset of questions r"
S18-1119,L18-1564,1,0.88915,"ms with regard to specific question types and based on whether a question is directly answerable, or only inferable from the text. 4.2 Baselines We provide results of two baseline systems as lower bounds for comparison: a rule-based baseline (Sliding Window) and a neural end-to-end system (Attentive Reader). Both baselines are described in 2 IUCM cluster MCScript texts and try to find answers also in other texts, that are topically similar. In that sense, MCScript itself is used to represent commonsense knowledge. more detail below. For details about the tuning of hyperparameters, we refer to Ostermann et al. (2018). Sliding Window The sliding window baseline is a simple rule-based method that answers a question on a text by predicting the answer option with the highest similarity to the text. The intuition underlying this method is that answers similar to a text should be more plausible than answer options that are different from the text (independent of the question). In our baseline implementation, we compute similarity using a sliding window that compares each answer option to any possible “window” of w tokens of the text. For comparison, each window and each answer is represented by an average vecto"
S18-1119,D14-1162,0,0.0806018,"Missing"
S18-1119,D16-1264,0,0.12393,"atasets have been proposed for machine comprehension. One example is MCTest (Richardson et al., 2013), a small curated dataset of 660 stories, with 4 multiple choice questions per story. The stories are crowdsourced and not limited to a domain. Answering questions in MCTest requires drawing inferences from multiple sentences from the text passage. In our dataset, in contrast, answering requires drawing inferences using knowledge not explicit in the text. Another recently published multiple choice dataset is RACE (Lai et al., 2017), which contains 100,000 questions on reading examination data. Rajpurkar et al. (2016) have proposed the Stanford Question Answering Dataset (SQuAD), a data set of 100,000 questions on Wikipedia articles collected via crowdsourcing. In that dataset, the answer to a question corresponds to a segment/span from the reading passage. Since Wikipedia articles mostly contain factual knowledge, SQuAD does not assess how in practice, language comprehension relies on implicit and underrepresented knowledge about everyday activities i.e. script knowledge. Weston et al. (2015) have created the BAbI dataset. BAbI is a synthetic reading comprehension data set testing different types of reaso"
S18-1119,P10-1100,1,0.886473,"plicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally i"
S18-1119,S18-1179,0,0.0225614,"ion of the softmax function over both answer options for the question: p(a|t, q) = sof tmax(t> Ws a) 5 embeddings (Speer et al., 2017). One participating system made use of script knowledge in the form of event sequence descriptions. Resources commonly used by participants include pretrained word embeddings such as GloVe (Pennington et al., 2014) or word2vec (Mikolov et al., 2013), and preprocessing pipelines such as NLTK4 . In the following, we provide short summaries of the participants’ systems and we give an overview of models and resources used by them (Table 1). Non-neural methods IUCM (Reznikova and Derczynski, 2018) applied an unsupervised approach that assigns the correct answer to a question based on text overlap. Text overlap is computed based on the given passage and text sources of the same topic. Different clustering and topic modeling techniques are used to identify such text sources in MCScript and DeScript. (2) Participants We ran our shared task through the CodaLab platform3 . 24 teams submitted results during the evaluation period, out of which 11 teams provided system descriptions: 8 teams from China, and one team each from Spain, Russia and the US. The full leader board containing all 24 sub"
S18-1119,D13-1020,0,0.150799,"ned in the text, answering the questions requires knowledge beyond the facts mentioned in the text. In particular, a substantial subset of questions requires inference over commonsense knowledge via scripts. For example, consider the short narrative in (1). For the first question, the correct choice for an answer requires commonsense knowledge about the activity of going to the sauna, which goes beyond what is mentioned in the text: Usually, people sit on benches inside a sauna, an Related Work Recently, a number of datasets have been proposed for machine comprehension. One example is MCTest (Richardson et al., 2013), a small curated dataset of 660 stories, with 4 multiple choice questions per story. The stories are crowdsourced and not limited to a domain. Answering questions in MCTest requires drawing inferences from multiple sentences from the text passage. In our dataset, in contrast, answering requires drawing inferences using knowledge not explicit in the text. Another recently published multiple choice dataset is RACE (Lai et al., 2017), which contains 100,000 questions on reading examination data. Rajpurkar et al. (2016) have proposed the Stanford Question Answering Dataset (SQuAD), a data set of"
S18-1119,S15-1024,1,0.874914,"Missing"
S18-1119,D15-1195,0,0.0608887,"Missing"
S18-1119,S18-1175,0,0.0341503,"Missing"
S18-1119,W17-2623,0,0.0302808,"s mostly contain factual knowledge, SQuAD does not assess how in practice, language comprehension relies on implicit and underrepresented knowledge about everyday activities i.e. script knowledge. Weston et al. (2015) have created the BAbI dataset. BAbI is a synthetic reading comprehension data set testing different types of reasoning to solve different tasks. In contrast to our dataset, the artificial texts in BAbI are not reflective of a typically occurring narrative text. Two recently published datasets that also have a larger focus on commonsense reasoning are NewsQA and TriviaQA. NewsQA (Trischler et al., 2017) contains newswire texts from CNN with crowdsourced questions and answers. During the question collection, workers were only presented with the title of the text, and a short summary. This 748 method ensures that literal repetitions of the text are avoided and the generation of non-trivial questions requiring background knowledge is supported. The NewsQA text collection differs from MCScript in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge required to answer the questions is mostly factual knowledge and script knowledge is only marginally relevant. Tr"
S18-1119,W17-0901,1,0.841966,"cript knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text. The paper i"
S18-1119,C00-2137,0,0.237567,"Missing"
S18-1119,S18-1177,0,0.0482581,"Missing"
S19-1012,P17-1147,0,0.0906084,"Missing"
S19-1012,Q18-1023,0,0.0576173,"Missing"
S19-1012,D17-1082,0,0.122087,"Missing"
S19-1012,P14-5010,0,0.00332435,"lot studies, we then showed the texts with highlighted target sentences to workers and asked them to write questions about these sentences. We however found, that in many cases, the written questions were too general or nonsensical. We concluded that an even more structured task was required and decided to concentrate on questions of two types: (1) questions that ask about participants, and (2) questions about the temporal event structure of a scenario. Participants are usually instantiated by noun phrases (NPs), while events are described by verb phrases (VPs). We thus used Stanford CoreNLP (Manning et al., 2014) to extract both NPs and VPs in the target sentences and split up the experiment into two parts: In the first part, workers were required to write questions that ask about the given noun phrase. Figure 2 shows a screenshot of an item from the first part. The first column shows the reading text with the target sentence highlighted. The second columns shows all extracted phrases with a field for one question per phrase.3 Full details of the experiment instructions are given in the Supplemental Material. In the second part, we then asked workers to write a temporal question (when, how long, etc.)"
S19-1012,S18-1181,0,0.109565,"Missing"
S19-1012,L16-1555,1,0.896473,"ton et al., 2009), a large corpus of narrative blog stories and identified additional scenarios in these stories. Third, we added new scenarios that are related to existing ones or that extend them. We collected 20 texts per new scenario, using the same text collection method as Ostermann et al. (2018a): We asked workers to tell a story about a certain everyday scenario “as if talking to a child”. This instruction ensures that the resulting stories are simple in language and clearly structured. Texts collected this way have been found to explicitly mention many script events and participants (Modi et al., 2016; Ostermann et al., 2018a). They are thus ideal to evaluate script-based inference. 3.2 Question Collection For the question collection, we followed Ostermann et al. (2018a) in telling workers that the data are collected for a reading comprehension task for children, in order to get linguistically simple and explicit questions. However, as mentioned above, we guide workers towards asking questions about target sentences rather than a complete text. As target sentences, we selected every fourth sentence in a text. In order to avoid selecting target sentences with too much or too little content,"
S19-1012,W14-1606,0,0.0173416,"ts. In the example, script knowledge subsumes the fact that the paying event is a part of the eating in a restaurant scenario. Recent years have seen different approaches to learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections (Chambers and Jurafsky, 2008, 2009), and the manual induction of script knowledge via crowdsourcing (Regneri et al., 2010; Wanzare et al., 2016). Script knowledge has been represented both symbolically (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) and with neural models (Modi and Titov, 2014; Pichotta and Mooney, 2016). Scripts have been evaluated mostly intrinsically (Wanzare et al., 2017; Ostermann et al., 2017). An exception is MCScript (Ostermann et al., 2018a), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work. Previous work has shown, however, that script knowledge is not required for performing well on the data set (Ostermann et al., 2018b). Hence, to date, there exists no evaluation method that allows one to systematically assess the contribution of models of script knowledge to the task of automated"
S19-1012,L18-1564,1,0.343517,"learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections (Chambers and Jurafsky, 2008, 2009), and the manual induction of script knowledge via crowdsourcing (Regneri et al., 2010; Wanzare et al., 2016). Script knowledge has been represented both symbolically (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) and with neural models (Modi and Titov, 2014; Pichotta and Mooney, 2016). Scripts have been evaluated mostly intrinsically (Wanzare et al., 2017; Ostermann et al., 2017). An exception is MCScript (Ostermann et al., 2018a), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work. Previous work has shown, however, that script knowledge is not required for performing well on the data set (Ostermann et al., 2018b). Hence, to date, there exists no evaluation method that allows one to systematically assess the contribution of models of script knowledge to the task of automated text understanding. Our work closes this gap: We present MCScript2.0, a reading comprehension corpus focused on script events and participants. It contains more than 3,400 tex"
S19-1012,S17-1016,1,0.849017,"rio. Recent years have seen different approaches to learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections (Chambers and Jurafsky, 2008, 2009), and the manual induction of script knowledge via crowdsourcing (Regneri et al., 2010; Wanzare et al., 2016). Script knowledge has been represented both symbolically (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) and with neural models (Modi and Titov, 2014; Pichotta and Mooney, 2016). Scripts have been evaluated mostly intrinsically (Wanzare et al., 2017; Ostermann et al., 2017). An exception is MCScript (Ostermann et al., 2018a), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work. Previous work has shown, however, that script knowledge is not required for performing well on the data set (Ostermann et al., 2018b). Hence, to date, there exists no evaluation method that allows one to systematically assess the contribution of models of script knowledge to the task of automated text understanding. Our work closes this gap: We present MCScript2.0, a reading comprehension corpus focused on script events"
S19-1012,E14-1024,0,0.0350256,"cts that typically play a role in the situation, referred to as participants. In the example, script knowledge subsumes the fact that the paying event is a part of the eating in a restaurant scenario. Recent years have seen different approaches to learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections (Chambers and Jurafsky, 2008, 2009), and the manual induction of script knowledge via crowdsourcing (Regneri et al., 2010; Wanzare et al., 2016). Script knowledge has been represented both symbolically (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) and with neural models (Modi and Titov, 2014; Pichotta and Mooney, 2016). Scripts have been evaluated mostly intrinsically (Wanzare et al., 2017; Ostermann et al., 2017). An exception is MCScript (Ostermann et al., 2018a), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work. Previous work has shown, however, that script knowledge is not required for performing well on the data set (Ostermann et al., 2018b). Hence, to date, there exists no evaluation method that allows one to systematically assess the"
S19-1012,P10-1100,1,0.928525,"ke place during such situations, and their typical temporal order, referred to as events, as well as the persons and objects that typically play a role in the situation, referred to as participants. In the example, script knowledge subsumes the fact that the paying event is a part of the eating in a restaurant scenario. Recent years have seen different approaches to learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections (Chambers and Jurafsky, 2008, 2009), and the manual induction of script knowledge via crowdsourcing (Regneri et al., 2010; Wanzare et al., 2016). Script knowledge has been represented both symbolically (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) and with neural models (Modi and Titov, 2014; Pichotta and Mooney, 2016). Scripts have been evaluated mostly intrinsically (Wanzare et al., 2017; Ostermann et al., 2017). An exception is MCScript (Ostermann et al., 2018a), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work. Previous work has shown, however, that script knowledge is not required for performing well on the data"
S19-1012,D15-1195,0,0.133014,"Missing"
S19-1012,W17-2623,0,0.0200773,"evaluate a very broad notion of commonsense, including e.g. physical knowledge (for trivia texts) and knowledge about political facts (for newswire texts). However, none of them explicitly tackle script knowledge. 7 6 Related Work Recent years have seen a number of datasets that evaluate commonsense inference. Like our corpus, most of these data sets choose a machine comprehension setting. The data sets can be roughly classified along their text domain: News Texts. Two recently published machine comprehension data sets that require commonsense inference are based on news texts. First, NewsQA (Trischler et al., 2017) is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. During data collection, full texts were not shown to workers as a basis for question formulation, but only the text’s title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background knowledge. Second, ReCoRD (Zhang et al., 2018) contains cloze-style questions on newswire texts that were not crowdsourced, but automatically extracted by pruning a named entity in a larger passage from the text. Web Texts. Other corpora use web docum"
S19-1012,S18-1120,0,0.0123581,"text, question and answers are represented with word embeddings. Bidirectional gated recurrent units (GRUs, Cho et al. (2014)) process the text, question and answers and transform them into sequences of contextualized hidden states. The text is represented as a weighted average of the hidden states with a bilinear attention formulation, and another bilinear weight matrix is used to compute a scalar as score for each answer. For a formalization, we refer to Ostermann et al. (2018a) and Chen et al. (2016). Three-way Attentive Network (TriAN) As third model, we use a three-way attentive network (Wang et al., 2018), the best-scoring model of the shared task on MCScript6 . Various types of in6 Code available at https://github.com/ intfloat/commonsense-rc formation are employed to represent tokens: Word embeddings, part of speech tags, named entity embeddings, and word count/overlap features, similar to the logistic classifier. Three bidirectional LSTM (Hochreiter and Schmidhuber, 1997) modules are used to encode text, question and answers. The resulting hidden representations are reweighted with three attention matrices and then summed into vectors using three self-attention layers. Additionally, token r"
S19-1012,W17-0901,1,0.871616,"in a restaurant scenario. Recent years have seen different approaches to learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections (Chambers and Jurafsky, 2008, 2009), and the manual induction of script knowledge via crowdsourcing (Regneri et al., 2010; Wanzare et al., 2016). Script knowledge has been represented both symbolically (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) and with neural models (Modi and Titov, 2014; Pichotta and Mooney, 2016). Scripts have been evaluated mostly intrinsically (Wanzare et al., 2017; Ostermann et al., 2017). An exception is MCScript (Ostermann et al., 2018a), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work. Previous work has shown, however, that script knowledge is not required for performing well on the data set (Ostermann et al., 2018b). Hence, to date, there exists no evaluation method that allows one to systematically assess the contribution of models of script knowledge to the task of automated text understanding. Our work closes this gap: We present MCScript2.0, a reading comprehension corpus"
S19-1012,D18-1009,0,0.0729801,"didates per question for the data set. To choose the most plausible correct answer candidate, we adapt the procedure from Ostermann et al. (2018a): We normalize all correct answers (lowercasing, normalizing numbers4 , deleting stopwords5 ) and then merge candidates that are contained in another candidate, and candidate pairs with a Levenshtein (1966) distance of less than 3. The most frequent candidate is then selected as correct answer. If there was no clear majority, we selected a candidate at random. To select an incorrect answer candidate, we adapt the adversarial filtering algorithm from Zellers et al. (2018). Our implementation uses a simple classifier that utilizes shallow surface features. The algorithm selects the incorrect answer candidate from the set of possible candidates that is most difficult for the classifier, i.e. an incorrect answer that is hard to tell apart from the correct 4 answer (e.g. the incorrect answers in Figure 1: eating and utensils are also mentioned in the text). By picking incorrect answers with the adversarial filtering method, the dataset becomes robust against surface-oriented methods. Practically, the algorithm starts with a random assignment, i.e. a random incorre"
S19-1012,E12-1034,0,\N,Missing
S19-1012,P08-1090,0,\N,Missing
S19-1012,P09-1068,0,\N,Missing
S19-1012,S18-1119,1,\N,Missing
W09-2814,W08-1109,0,0.0246511,"Missing"
W09-2814,P98-1013,0,0.0420771,"Missing"
W09-2814,W09-0628,0,0.0304965,"learned graphs, we will generate directions for a wide range of possible routes. Dale et al. (2005) developed a system that takes GIS data as input and uses a pipeline architecture to generate verbal route directions. In contrast to their approach, our approach will be based on an integrated architecture allowing for more interaction between the different stages of generation. The idea of combining verbal directions with scenes from a virtual 3D environment has recently lead to a new framework for evaluating NLG systems: The Challenge on Generating Instructions in Virtual Environments (GIVE) (Byron et al., 2009) is planned to become a regular event for the NLG community. This work describes first steps towards building a system that synchronously generates multimodal (textual and visual) route directions for pedestrians. We pursue a corpus-based approach for building a generation model that produces natural instructions in multiple languages. We conducted an empirical study to collect verbal route directions, and annotated the acquired texts on different levels. Here we describe the experimental setting and an analysis of the collected data. 1 Michael Strube† Introduction Route directions guide a per"
W09-2814,W07-1203,0,0.0433801,"Missing"
W09-2814,J09-2005,0,0.0246238,"Missing"
W09-2814,kunze-lemnitzer-2002-germanet,0,0.0204592,"Missing"
W09-2814,W05-0618,1,0.78264,"Missing"
W09-2814,J93-4001,0,0.0302683,"Missing"
W09-2814,P09-4010,1,0.701736,"surface generation model. We observed a variety of coherence-inducing elements that are generic in nature and thus seem well-suited for a corpusbased generation model. As other languages are known to exhibit differences in verbal realization of directions (von Stutterheim et al., 2002), we have to extend our data collection in order to generate systematic linguistic variations from a single underlying semantic structure for all languages. The linguistic annotation levels of frames and roles, syntactic dependencies, and basic word categories have been tested successfully with a similar corpus (Roth & Frank, 2009). The next steps will consist in the alignment of physical routes and landmarks with semantic representations in an integrated generation architecture. Table 3: Frequencies of temporal adverbs indicating linear (&gt; t ) and reversed linear order (&lt; t ) the following action or situation is not supposed to take place (e.g. Gehen Sie vorher rechts ‘beforehand turn right’). Backward-looking event anaphors and references to result states: We also found explicit references to past events (e.g. Nach dem Durchqueren ‘after traversing’) and result states of events, e.g. the adverbial phrase unten angekom"
W09-2814,burchardt-etal-2006-salto,1,\N,Missing
W09-2814,J03-1003,0,\N,Missing
W09-2814,C98-1013,0,\N,Missing
W13-0211,S10-1059,0,0.517109,"Missing"
W13-0211,P10-1160,0,0.197188,"Missing"
W13-0211,J12-4003,0,0.28181,"ssing this task, using supervised and recently also semi- and unsupervised methods (Palmer et al., 2010). Traditional SRL is restricted to the local syntactic domain. In discourse interpretation, however, we typically find locally unrealized argument roles that are contextually bound to antecedents beyond their local structure. Thus, by using strictly local methods, we are far from capturing the full potential offered by semantic argument structure (Fillmore and Baker, 2001; Burchardt et al., 2005). The task of resolving the reference of implicit arguments has been addressed in previous work: Gerber and Chai (2012) address the task in the nominal domain by learning a model from manually annotated data following the NomBank paradigm (Meyers et al., 2004). In contrast, Ruppenhofer et al. (2010) follow the FrameNet paradigm, which is not restricted to nominal predicates. However, their data set suffers from considerable sparsity with respect to annotation instances per predicate (cf. Section 2). Our contribution addresses the problem of sparse training resources for implicit role binding by providing a higher volume of predicate-specific annotations for non-local role binding, using OntoNotes (Weischedel e"
W13-0211,meyers-etal-2004-annotating,0,0.0850802,"local syntactic domain. In discourse interpretation, however, we typically find locally unrealized argument roles that are contextually bound to antecedents beyond their local structure. Thus, by using strictly local methods, we are far from capturing the full potential offered by semantic argument structure (Fillmore and Baker, 2001; Burchardt et al., 2005). The task of resolving the reference of implicit arguments has been addressed in previous work: Gerber and Chai (2012) address the task in the nominal domain by learning a model from manually annotated data following the NomBank paradigm (Meyers et al., 2004). In contrast, Ruppenhofer et al. (2010) follow the FrameNet paradigm, which is not restricted to nominal predicates. However, their data set suffers from considerable sparsity with respect to annotation instances per predicate (cf. Section 2). Our contribution addresses the problem of sparse training resources for implicit role binding by providing a higher volume of predicate-specific annotations for non-local role binding, using OntoNotes (Weischedel et al., 2011) as underlying corpus. A qualitative analysis of the produced annotations leads to a number of hypotheses on implicit role realiz"
W13-0211,J05-1004,0,0.50929,"Missing"
W13-0211,R11-1046,0,0.714775,"Missing"
W13-0211,S10-1008,0,0.44858,"e interpretation, however, we typically find locally unrealized argument roles that are contextually bound to antecedents beyond their local structure. Thus, by using strictly local methods, we are far from capturing the full potential offered by semantic argument structure (Fillmore and Baker, 2001; Burchardt et al., 2005). The task of resolving the reference of implicit arguments has been addressed in previous work: Gerber and Chai (2012) address the task in the nominal domain by learning a model from manually annotated data following the NomBank paradigm (Meyers et al., 2004). In contrast, Ruppenhofer et al. (2010) follow the FrameNet paradigm, which is not restricted to nominal predicates. However, their data set suffers from considerable sparsity with respect to annotation instances per predicate (cf. Section 2). Our contribution addresses the problem of sparse training resources for implicit role binding by providing a higher volume of predicate-specific annotations for non-local role binding, using OntoNotes (Weischedel et al., 2011) as underlying corpus. A qualitative analysis of the produced annotations leads to a number of hypotheses on implicit role realization. Using the extended set of annotat"
W13-0211,S12-1001,1,0.836729,"and the influence of constructional licensing. 5 Evaluation Experiments We evaluate the impact of predicate-specific annotations for classification using two scenarios: (CV) we examine the linking performance of models trained and tested on the same predicate by adopting the 10-fold Cross-Validation scenario used by Gerber and Chai (2012) (G&C).11 (SemEval) Secondly, we examine the direct effect of using our annotations as additional training data for linking NIs in the SemEval 2010 task on implicit role binding. We use the state-of-the-art system and best performing feature set described in Silberer and Frank (2012) (S&F) to make direct comparisons to previous results. CV. As positive training and test samples for this scenario, we use all annotated (and resolvable) NIs and randomly split them into 10 folds. Negative training samples (i.e., incorrect NI fillers) are automat11 Note that this is not a direct comparison as both the annotation paradigm and the data sets are different. verb give put leave bring pay — average Cross-Validation precision recall 48.8 33.3 48.3 72.7 35.4 — 47.7 33.3 20.0 56.0 27.6 20.0 — 31.4 F1 39.6 25.0 51.9 40.0 25.6 — 36.4 training data S&F’12 no additional data + best heurist"
W13-0211,S10-1065,0,0.434924,"Missing"
W13-0211,W11-0908,0,0.861322,"Missing"
W14-2410,P13-1042,0,0.0357614,"ten provided with explicit guidelines on how to concisely express requirements in natural language. As a consequence, we also find their requirement texts to be more regimented and stylised than those written by senior software engineers. Examples (2) and (3) show examples of a student-written and developer-written requirement, respectively. (2) The user must be able to vote on polls. (3) For each user contact, back-end must perform a check to determine whether the contact is a registered user or not. Data Set In comparison to two extant data sets, namely GeoQuery880 (Tang, 2003) and Free917 (Cai and Yates, 2013), we find that our collection is still relatively small in terms of example sentences. The Given our conviction that mapping natural language software requirements to formal representations provides an attractive challenge for semantic parsing research, we believe that there is a more general benefit in building a corpus of annotated requirements. One immediate obstacle is that software requirements can drastically differ in quality, style and granularity. To cover a range of possible 2 The majority of collected requirements are from a software development course organized jointly by several E"
W15-0403,C10-3009,0,0.0280736,"Missing"
W15-0403,C10-1011,0,0.0239405,"ns that 3 We thank the authors for making their data available to us. the pipeline separates the sentence into word tokens, identifies the grammatical category of each word (e.g., “user” → noun, “upload” → verb) and determines their uninflected base forms (e.g., “users” → “user”). Finally, the pipeline identifies syntactic dependents of each word and their respective grammatical relation (e.g., h“user”,“must”i → subject-of, h“upload”,“photos”i → object-of). For all syntactic analysis steps, we rely on components and pre-trained models from a system called Mate tools (Bj¨orkelund et al., 2010; Bohnet, 2010), which is freely available online.4 This choice is based on two criteria. First, the system achieves state-of-the-art performance on a benchmark data set for syntactic analysis (Hajiˇc et al., 2009). Second, the output of the syntactic analysis steps has successfully been used as input for the related task of PropBank/NomBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2008). 4.2 Semantic Analysis Our actual semantic role labeling pipeline consists of four main steps to extract instances of ontology concepts and relations from requirements written in natural language tex"
W15-0403,J05-1004,0,0.0443901,"ammatical relation (e.g., h“user”,“must”i → subject-of, h“upload”,“photos”i → object-of). For all syntactic analysis steps, we rely on components and pre-trained models from a system called Mate tools (Bj¨orkelund et al., 2010; Bohnet, 2010), which is freely available online.4 This choice is based on two criteria. First, the system achieves state-of-the-art performance on a benchmark data set for syntactic analysis (Hajiˇc et al., 2009). Second, the output of the syntactic analysis steps has successfully been used as input for the related task of PropBank/NomBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2008). 4.2 Semantic Analysis Our actual semantic role labeling pipeline consists of four main steps to extract instances of ontology concepts and relations from requirements written in natural language text: (1) identifying instances of the concepts Action and Object, (2) assigning the respective concept type, (3) determining instances of related concepts, and (4) labeling relationships between pairs of concept instances. Our implementation is based on the semantic role labeler from Mate tools and uses the built-in re-ranker to find the best joint output of steps (3) and (4)."
W15-0403,W14-2410,1,0.395444,"istic post-processing rules. Despite this seemingly long tradition, methods for processing software requirements have tended to depend on domain-specific heuristics and knowledge bases or have required additional user intervention. In contrast, we propose to utilize annotated data to learn how to perform semantic parsing of requirements automatically. 3 Ontology and Dataset As training and testing material for our semantic role labeling approach, we use a high-level ontology of static software functionalities and an existing data set of software requirements with annotated ontology instances (Roth et al., 2014). The ontology by Roth et al. covers general concepts for describing static software functionalities. The main concepts and their associated relations are as follows: Action An Action describes an operation that is performed by an Actor on any number of Object(s).2 The participants of an Action are indicated by the relations HAS ACTOR and ACTS ON, respectively. Actor (HAS ACTOR) A Actor is an active participant of an Action and can be the user of a system or a software system itself. Object (ACTS ON) A Object is any kind of entity involved in an Action other than the Actor. Property (HAS PROPE"
W15-0403,D14-1045,1,0.870065,"Missing"
W15-0403,C00-2137,0,0.0143607,"Missing"
W15-0403,W09-1201,0,\N,Missing
W17-0906,W17-0908,0,0.124042,"Missing"
W17-0906,W17-0909,0,0.387095,"d. Submissions The Shared Task was conducted through CodaLab competitions4 . We received a total of 18 registrations, out of which eight teams participated: four teams from the US, three teams from Germany and one team from India. In the following, we provide short paragraphs summarizing our baseline and approaches of the submissions. More details can be found in the respective system description papers. mflor (Educational Testing Service). Rulebased combination of two systems that score possible endings in terms of how well they lexically cohere with and fit the sentiment of the given story (Flor and Somasundaran, 2017). Sentiment is given priority, and the model backs off to lexical coherence based on pointwise mutual information scores. Pranav Goel (IIT Varanasi). Ensemble model that takes into account scores from two systems that measure overlap in sentiment and sentence similarity between the story and the two possible endings (Goel and Singh, 2017). msap (University of Washington). Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences (Schwartz et al., 2017). These ending “style” features include sentence length as well as wo"
W17-0906,W17-0912,0,0.164607,"ls can be found in the respective system description papers. mflor (Educational Testing Service). Rulebased combination of two systems that score possible endings in terms of how well they lexically cohere with and fit the sentiment of the given story (Flor and Somasundaran, 2017). Sentiment is given priority, and the model backs off to lexical coherence based on pointwise mutual information scores. Pranav Goel (IIT Varanasi). Ensemble model that takes into account scores from two systems that measure overlap in sentiment and sentence similarity between the story and the two possible endings (Goel and Singh, 2017). msap (University of Washington). Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences (Schwartz et al., 2017). These ending “style” features include sentence length as well as word and character n-gram in each candidate ending (independent of story). These style features have been shown useful in other tasks such as age, gender, or native language detection. ROCNLP (baseline) Two feed-forward neural networks trained jointly on ROCStories to project the four-sentences context and the right fifth sentence into the"
W17-0906,H05-1044,0,0.0509919,"Missing"
W17-0906,H89-1033,0,0.679051,"Missing"
W17-0906,N16-1098,1,0.310191,"esident Marco was excited to be a registered voter. He thought long and hard about who to vote for. Finally he had decided on his favorite candidate. He placed his vote for that candidate. Marco was proud that he had finally voted. Spaghetti Sauce Tina made spaghetti for her boyfriend. It took a lot of work, but she was very proud. Her boyfriend ate the whole plate and said it was good. Tina tried it herself, and realized it was disgusting. She was touched that he pretended it was good to spare her feelings. Table 2: Example ROCStories instances from the Winter 2017 release. 3 As described in Mostafazadeh et al. (2016), the SCT cases are collected through Amazon Mechanical Turk (Mturk) on the basis of the ROCStories corpus, a collection of five-sentence everyday life stories which are full of stereotypical sequence of events. To construct SCT cases, they randomly sampled complete five-sentence stories from the ROCStories corpus and presented only the first four sentences of each story to the Mturk workers. Then, for each story, a worker was asked to write a ‘right ending’ and a ‘wrong ending’. This resulting set was further filtered by human verification: they compile each SCT case into two independent five"
W17-0906,W17-0911,0,0.0613484,"2017). Linguistic features include aspects of sentiment, negation, pronominalization and n-gram overlap between the story and possible endings. 98,159 1,871 1,871 Table 3: The size of the provided shared task datasets. uate the systems in terms of accuracy, which we #correct measure as #test Any other details recases . garding our shared task can be accessed via our shared task page http://cs.rochester. edu/nlp/rocstories/LSDSem17/. 4 roemmele (University of Southern California). Binary classifier based on a recurrent neural network that operates over (sentence-level) Skipthought embeddings (Roemmele et al., 2017). For training, different data augmentation methods are explored. Submissions The Shared Task was conducted through CodaLab competitions4 . We received a total of 18 registrations, out of which eight teams participated: four teams from the US, three teams from Germany and one team from India. In the following, we provide short paragraphs summarizing our baseline and approaches of the submissions. More details can be found in the respective system description papers. mflor (Educational Testing Service). Rulebased combination of two systems that score possible endings in terms of how well they l"
W17-0906,W17-0910,0,0.073945,"Missing"
W17-0906,W17-0907,0,0.273047,"exically cohere with and fit the sentiment of the given story (Flor and Somasundaran, 2017). Sentiment is given priority, and the model backs off to lexical coherence based on pointwise mutual information scores. Pranav Goel (IIT Varanasi). Ensemble model that takes into account scores from two systems that measure overlap in sentiment and sentence similarity between the story and the two possible endings (Goel and Singh, 2017). msap (University of Washington). Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences (Schwartz et al., 2017). These ending “style” features include sentence length as well as word and character n-gram in each candidate ending (independent of story). These style features have been shown useful in other tasks such as age, gender, or native language detection. ROCNLP (baseline) Two feed-forward neural networks trained jointly on ROCStories to project the four-sentences context and the right fifth sentence into the same vector space. This model is called Deep Structured Semantic Model (DSSM) (Huang et al., 2013) and had outperformed all the other baselines reported in Mostafazadeh et al. (2016). cogcomp"
W17-6934,P14-1065,0,0.0717136,"Missing"
W17-6934,P14-1002,0,0.0129937,"Naive Bayes model with a range of binary features. Follow-up work examined different methods for feature selection (Lin et al., 2009; Park and Cardie, 2012) as well as novel feature types based on pairs of word classes/clusters, entity mentions, and word embeddings (Biran and McKeown, 2013; Louis et al., 2010; Braud and Denis, 2015). Further improvements were made via multi-task learning (Lan et al., 2013) and training data expansion (Rutherford and Xue, 2015). In recent years, a myriad of neural-network based models have been proposed for the task of recognizing implicit discourse relations (Ji and Eisenstein, 2014; Zhang et al., 2015; Liu and Li, 2016; Qin et al., 2017, inter alia). Models of this kind have a high expressive power and generally outperform methods that rely on manual feature engineering. However, being able to trace back improvements to individual features was key to my discussion in Section 3. Recent results in downstream NLP tasks indicate that neural network models can perform better when incorporating binary features (Cheng et al., 2016; Sennrich and Haddow, 2016, inter alia). 5 Conclusions I proposed a simple model combination for discourse relation classification that aggregates o"
W17-6934,P13-1047,0,0.0145772,"was first introduced in the context of implicit and explicit relation classification (Marcu and Echihabi, 2002). Pitler et al. (2009) were the first to address implicit relations specifically. They applied a Naive Bayes model with a range of binary features. Follow-up work examined different methods for feature selection (Lin et al., 2009; Park and Cardie, 2012) as well as novel feature types based on pairs of word classes/clusters, entity mentions, and word embeddings (Biran and McKeown, 2013; Louis et al., 2010; Braud and Denis, 2015). Further improvements were made via multi-task learning (Lan et al., 2013) and training data expansion (Rutherford and Xue, 2015). In recent years, a myriad of neural-network based models have been proposed for the task of recognizing implicit discourse relations (Ji and Eisenstein, 2014; Zhang et al., 2015; Liu and Li, 2016; Qin et al., 2017, inter alia). Models of this kind have a high expressive power and generally outperform methods that rely on manual feature engineering. However, being able to trace back improvements to individual features was key to my discussion in Section 3. Recent results in downstream NLP tasks indicate that neural network models can perf"
W17-6934,J13-4004,0,0.0147369,"00 features Dates and number. Indicator features for the number of date and number expressions in each discourse segment (e.g. 1:DATE:0; see Pitler et al., 2009). ca. 10 000 Production rules. Features on production rules used to construct each discourse segment’s constituency tree (e.g. 1:S NP VP; see Lin et al., 2009). ca. 78 000 Verb features. Indicators for the main verb, its tense/modality and average verb phrase length (e.g. 1:VERB:phone, 2:TENSE:past; see Park and Cardie, 2012). ca. 20 000 Coreference. Set of features that indicate coreferring mentions, as predicted by Stanford CoreNLP (Lee et al., 2013), across two related discourse segments (see Rutherford and Xue, 2014). ca. 10 000 Brown clusters. Feature sets indicating precomputed Brown cluster IDs (Turian et al., 2010) of words occurring in each discourse segment (e.g. 2:11100110; see Braud and Denis, 2015). 200–6 400 Pairwise Brown clusters. Pairwise Brown cluster IDs indicating word pairs across two related discourse segments (e.g. 11110110x11000100; see Braud and Denis, 2015). up to 10 million 2 Sum/averaging is used here because of its simplicity and robustness (Kittler et al., 1998). Due to the small development set size, methods w"
W17-6934,D09-1036,0,0.432736,"ns has further proven a useful preprocessing step for a range of downstream tasks (Louis et al., 2010; Guzm´an et al., 2014; Narasimhan and Barzilay, 2015; Chandrasekaran et al., 2017, inter alia). From a computational perspective, it has been shown that recognizing discourse relations can be performed with high accuracy when explicit discourse markers are available (Pitler et al., 2008). However, classifying relations without explicit markers, so-called implicit discourse relations, has persisted as a difficult task to date (cf. Xue et al., 2016). One of the main challenges, as identified in Lin et al. (2009), is the need to perform inference over two discourse segments. In this paper, I propose a new set of features based on semantic roles to address this challenge. These features are meant to provide a shallow form of semantic representation, which might help a classifier to make better informed classification decisions. I argue that role semantic representations are particularly well-suited for this task because different types of discourse relations are defined over the propositions that they connect. For example, definitions in the Penn Discourse TreeBank 2.0 annotation manual (Prasad et al.,"
W17-6934,D16-1130,0,0.0521096,"model, AllFeats, is a baseline logistic regression classifier that uses all best development feature sets at the same time. For comparison, I consider a range of current state-of-the-art models. The best feature-rich models (Rutherford and Xue, 2014; Braud and Denis, 2015) use a range of binary indicator features largely identical to the features described in Section 2.1. The most notable difference to this work is that Rutherford and Xue use a small list of coreference patterns in addition to features that simply indicate coreferring mention counts. Neural-network models (Zhang et al., 2015; Liu and Li, 2016; Qin et al., 2016) use attention or convolution mechanisms to identify important words and word spans in each discourse segment. They then predict the discourse relation based on a composition function applied over representations of important words. All of the comparison models use the same training and test instances as this work and are directly comparable. 3 I also experimented with feature conjunctions in order to explicitly model semantic interactions between two discourse segments. However, such conjuctions consistently reduced development performance, probably due to sparsity. comp co"
W17-6934,W10-4327,0,0.0623274,"eatures, providing insights into why and when role semantics is helpful. 1 Introduction Understanding natural language texts involves, inter alia, correctly identifying coherent segments and the relations that hold between them. Recognizing discourse relations is an important part of this process because such relations not only conceptualize which parts of a text belong together but also how they are related. Apart from direct applications in text analysis (e.g., discourse parsing), recognizing discourse relations has further proven a useful preprocessing step for a range of downstream tasks (Louis et al., 2010; Guzm´an et al., 2014; Narasimhan and Barzilay, 2015; Chandrasekaran et al., 2017, inter alia). From a computational perspective, it has been shown that recognizing discourse relations can be performed with high accuracy when explicit discourse markers are available (Pitler et al., 2008). However, classifying relations without explicit markers, so-called implicit discourse relations, has persisted as a difficult task to date (cf. Xue et al., 2016). One of the main challenges, as identified in Lin et al. (2009), is the need to perform inference over two discourse segments. In this paper, I pro"
W17-6934,W10-4310,0,0.109727,"eatures, providing insights into why and when role semantics is helpful. 1 Introduction Understanding natural language texts involves, inter alia, correctly identifying coherent segments and the relations that hold between them. Recognizing discourse relations is an important part of this process because such relations not only conceptualize which parts of a text belong together but also how they are related. Apart from direct applications in text analysis (e.g., discourse parsing), recognizing discourse relations has further proven a useful preprocessing step for a range of downstream tasks (Louis et al., 2010; Guzm´an et al., 2014; Narasimhan and Barzilay, 2015; Chandrasekaran et al., 2017, inter alia). From a computational perspective, it has been shown that recognizing discourse relations can be performed with high accuracy when explicit discourse markers are available (Pitler et al., 2008). However, classifying relations without explicit markers, so-called implicit discourse relations, has persisted as a difficult task to date (cf. Xue et al., 2016). One of the main challenges, as identified in Lin et al. (2009), is the need to perform inference over two discourse segments. In this paper, I pro"
W17-6934,P02-1047,0,0.0769444,"linear temporal order (“before” vs. “after”). Higher gains could be achieved if training and evaluation was performed on more specific relation annotations but such instances are too rare in practice for the feature-rich classifiers to learn robust generalizations: For example, the current version of the Penn Discourse Treebank contains a total of only 151 implicit relation instances of the discourse relation Temporal.Asynchronous.Succession. 4 Related Work The task of predicting implicit discourse relations was first introduced in the context of implicit and explicit relation classification (Marcu and Echihabi, 2002). Pitler et al. (2009) were the first to address implicit relations specifically. They applied a Naive Bayes model with a range of binary features. Follow-up work examined different methods for feature selection (Lin et al., 2009; Park and Cardie, 2012) as well as novel feature types based on pairs of word classes/clusters, entity mentions, and word embeddings (Biran and McKeown, 2013; Louis et al., 2010; Braud and Denis, 2015). Further improvements were made via multi-task learning (Lan et al., 2013) and training data expansion (Rutherford and Xue, 2015). In recent years, a myriad of neural-n"
W17-6934,P15-1121,0,0.0149176,"hen role semantics is helpful. 1 Introduction Understanding natural language texts involves, inter alia, correctly identifying coherent segments and the relations that hold between them. Recognizing discourse relations is an important part of this process because such relations not only conceptualize which parts of a text belong together but also how they are related. Apart from direct applications in text analysis (e.g., discourse parsing), recognizing discourse relations has further proven a useful preprocessing step for a range of downstream tasks (Louis et al., 2010; Guzm´an et al., 2014; Narasimhan and Barzilay, 2015; Chandrasekaran et al., 2017, inter alia). From a computational perspective, it has been shown that recognizing discourse relations can be performed with high accuracy when explicit discourse markers are available (Pitler et al., 2008). However, classifying relations without explicit markers, so-called implicit discourse relations, has persisted as a difficult task to date (cf. Xue et al., 2016). One of the main challenges, as identified in Lin et al. (2009), is the need to perform inference over two discourse segments. In this paper, I propose a new set of features based on semantic roles to"
W17-6934,J05-1004,0,0.0533488,"wise Brown cluster IDs indicating word pairs across two related discourse segments (e.g. 11110110x11000100; see Braud and Denis, 2015). up to 10 million 2 Sum/averaging is used here because of its simplicity and robustness (Kittler et al., 1998). Due to the small development set size, methods with additional parameters may tend to overfit. 2.2 Features based on Semantic Roles As new features, I propose to utilize the semantic roles identified in a pair of discourse segments. I define two variants of this feature type: one based on FrameNet (Ruppenhofer et al., 2010) and one based on PropBank (Palmer et al., 2005). All features are computed automatically using a state-of-the-art semantic role labeler (Roth, 2016; Roth and Lapata, 2016). Each variant includes both raw labels as well as a combination of the label and the filler word to which the label is assigned. To reduce sparsity, filler words are always represented by pre-computed Brown cluster IDs (Turian et al., 2010). The list below provides additional details as well as example instances based on the sentences shown in Example (1). FrameNet roles. This feature set indicates all frame elements that are identified in a pair of related discourse seg"
W17-6934,W12-1614,0,0.0754255,"e of Example (1), instances of this feature set include 1:FIRST:Mr., 2:FIRST:He, etc. (for details, see Pitler et al., 2009). ca. 74 000 features Dates and number. Indicator features for the number of date and number expressions in each discourse segment (e.g. 1:DATE:0; see Pitler et al., 2009). ca. 10 000 Production rules. Features on production rules used to construct each discourse segment’s constituency tree (e.g. 1:S NP VP; see Lin et al., 2009). ca. 78 000 Verb features. Indicators for the main verb, its tense/modality and average verb phrase length (e.g. 1:VERB:phone, 2:TENSE:past; see Park and Cardie, 2012). ca. 20 000 Coreference. Set of features that indicate coreferring mentions, as predicted by Stanford CoreNLP (Lee et al., 2013), across two related discourse segments (see Rutherford and Xue, 2014). ca. 10 000 Brown clusters. Feature sets indicating precomputed Brown cluster IDs (Turian et al., 2010) of words occurring in each discourse segment (e.g. 2:11100110; see Braud and Denis, 2015). 200–6 400 Pairwise Brown clusters. Pairwise Brown cluster IDs indicating word pairs across two related discourse segments (e.g. 11110110x11000100; see Braud and Denis, 2015). up to 10 million 2 Sum/averagi"
W17-6934,P09-1077,0,0.096563,", I use logistic regression models with L2 loss, as implemented in the LIBLINEAR toolkit (Mu-Chu et al., 2015). Accordingly, the aggregated model predicts a relation r for instance i iff n1 Σj=1...n score cr,j (i) > 0.5. The following list provides an overview of all feature sets from the literature that I reimplemented for the described approach, and gives the total number of features for each type. First/Last. Set of indicators for the first and last words in each discourse segment. In case of Example (1), instances of this feature set include 1:FIRST:Mr., 2:FIRST:He, etc. (for details, see Pitler et al., 2009). ca. 74 000 features Dates and number. Indicator features for the number of date and number expressions in each discourse segment (e.g. 1:DATE:0; see Pitler et al., 2009). ca. 10 000 Production rules. Features on production rules used to construct each discourse segment’s constituency tree (e.g. 1:S NP VP; see Lin et al., 2009). ca. 78 000 Verb features. Indicators for the main verb, its tense/modality and average verb phrase length (e.g. 1:VERB:phone, 2:TENSE:past; see Park and Cardie, 2012). ca. 20 000 Coreference. Set of features that indicate coreferring mentions, as predicted by Stanford"
W17-6934,C08-2022,0,0.0323281,"is process because such relations not only conceptualize which parts of a text belong together but also how they are related. Apart from direct applications in text analysis (e.g., discourse parsing), recognizing discourse relations has further proven a useful preprocessing step for a range of downstream tasks (Louis et al., 2010; Guzm´an et al., 2014; Narasimhan and Barzilay, 2015; Chandrasekaran et al., 2017, inter alia). From a computational perspective, it has been shown that recognizing discourse relations can be performed with high accuracy when explicit discourse markers are available (Pitler et al., 2008). However, classifying relations without explicit markers, so-called implicit discourse relations, has persisted as a difficult task to date (cf. Xue et al., 2016). One of the main challenges, as identified in Lin et al. (2009), is the need to perform inference over two discourse segments. In this paper, I propose a new set of features based on semantic roles to address this challenge. These features are meant to provide a shallow form of semantic representation, which might help a classifier to make better informed classification decisions. I argue that role semantic representations are parti"
W17-6934,prasad-etal-2008-penn,0,0.0159879,"for PropBank labels. Because argument labels in PropBank (A0. . . A5) are only meaningful with respect to a given predicate, I define two conjoined versions of this feature type: one takes into account the predicate’s class in VerbNet (Kipper et al., 2008) and one the predicate lemma itself (e.g., 2:work-73.2 A0 and 2:work A0:11100110, resp.). In each variant, predicate-independent labels (modifiers such as time and location) are optionally considered in the same representation format. ca. 560 000 3 Experiments I evaluate the proposed model on version 2.0 of the Penn Discourse Treebank (PDTB, Prasad et al., 2008). To ensure a fair comparison, I use the same preprocessing and weighting techniques as well as the same data instances as previous work (Rutherford and Xue, 2014; Braud and Denis, 2015). That is, each instance is a pair of implicitly related discourse segments as annotated in the PDTB corpus. Sections 2–20 of the corpus are used for training, 21–22 for testing, and all other sections for development. Baseline and comparison models. I use three variants of the proposed model to directly examine the utility of semantic roles and combining classifiers. The first two models are instances of the f"
W17-6934,D16-1246,0,0.0178706,"s a baseline logistic regression classifier that uses all best development feature sets at the same time. For comparison, I consider a range of current state-of-the-art models. The best feature-rich models (Rutherford and Xue, 2014; Braud and Denis, 2015) use a range of binary indicator features largely identical to the features described in Section 2.1. The most notable difference to this work is that Rutherford and Xue use a small list of coreference patterns in addition to features that simply indicate coreferring mention counts. Neural-network models (Zhang et al., 2015; Liu and Li, 2016; Qin et al., 2016) use attention or convolution mechanisms to identify important words and word spans in each discourse segment. They then predict the discourse relation based on a composition function applied over representations of important words. All of the comparison models use the same training and test instances as this work and are directly comparable. 3 I also experimented with feature conjunctions in order to explicitly model semantic interactions between two discourse segments. However, such conjuctions consistently reduced development performance, probably due to sparsity. comp cont exp temp Neural"
W17-6934,P17-1093,0,0.0117425,"rk examined different methods for feature selection (Lin et al., 2009; Park and Cardie, 2012) as well as novel feature types based on pairs of word classes/clusters, entity mentions, and word embeddings (Biran and McKeown, 2013; Louis et al., 2010; Braud and Denis, 2015). Further improvements were made via multi-task learning (Lan et al., 2013) and training data expansion (Rutherford and Xue, 2015). In recent years, a myriad of neural-network based models have been proposed for the task of recognizing implicit discourse relations (Ji and Eisenstein, 2014; Zhang et al., 2015; Liu and Li, 2016; Qin et al., 2017, inter alia). Models of this kind have a high expressive power and generally outperform methods that rely on manual feature engineering. However, being able to trace back improvements to individual features was key to my discussion in Section 3. Recent results in downstream NLP tasks indicate that neural network models can perform better when incorporating binary features (Cheng et al., 2016; Sennrich and Haddow, 2016, inter alia). 5 Conclusions I proposed a simple model combination for discourse relation classification that aggregates outputs from multiple classifiers. Several classifiers us"
W17-6934,P16-1113,1,0.793826,"Denis, 2015). up to 10 million 2 Sum/averaging is used here because of its simplicity and robustness (Kittler et al., 1998). Due to the small development set size, methods with additional parameters may tend to overfit. 2.2 Features based on Semantic Roles As new features, I propose to utilize the semantic roles identified in a pair of discourse segments. I define two variants of this feature type: one based on FrameNet (Ruppenhofer et al., 2010) and one based on PropBank (Palmer et al., 2005). All features are computed automatically using a state-of-the-art semantic role labeler (Roth, 2016; Roth and Lapata, 2016). Each variant includes both raw labels as well as a combination of the label and the filler word to which the label is assigned. To reduce sparsity, filler words are always represented by pre-computed Brown cluster IDs (Turian et al., 2010). The list below provides additional details as well as example instances based on the sentences shown in Example (1). FrameNet roles. This feature set indicates all frame elements that are identified in a pair of related discourse segments. For instance, two frame element fillers are identified in the phrase he continued to work: he is the Agent of the fra"
W17-6934,E14-1068,0,0.0624852,"r of date and number expressions in each discourse segment (e.g. 1:DATE:0; see Pitler et al., 2009). ca. 10 000 Production rules. Features on production rules used to construct each discourse segment’s constituency tree (e.g. 1:S NP VP; see Lin et al., 2009). ca. 78 000 Verb features. Indicators for the main verb, its tense/modality and average verb phrase length (e.g. 1:VERB:phone, 2:TENSE:past; see Park and Cardie, 2012). ca. 20 000 Coreference. Set of features that indicate coreferring mentions, as predicted by Stanford CoreNLP (Lee et al., 2013), across two related discourse segments (see Rutherford and Xue, 2014). ca. 10 000 Brown clusters. Feature sets indicating precomputed Brown cluster IDs (Turian et al., 2010) of words occurring in each discourse segment (e.g. 2:11100110; see Braud and Denis, 2015). 200–6 400 Pairwise Brown clusters. Pairwise Brown cluster IDs indicating word pairs across two related discourse segments (e.g. 11110110x11000100; see Braud and Denis, 2015). up to 10 million 2 Sum/averaging is used here because of its simplicity and robustness (Kittler et al., 1998). Due to the small development set size, methods with additional parameters may tend to overfit. 2.2 Features based on S"
W17-6934,N15-1081,0,0.0141539,"and explicit relation classification (Marcu and Echihabi, 2002). Pitler et al. (2009) were the first to address implicit relations specifically. They applied a Naive Bayes model with a range of binary features. Follow-up work examined different methods for feature selection (Lin et al., 2009; Park and Cardie, 2012) as well as novel feature types based on pairs of word classes/clusters, entity mentions, and word embeddings (Biran and McKeown, 2013; Louis et al., 2010; Braud and Denis, 2015). Further improvements were made via multi-task learning (Lan et al., 2013) and training data expansion (Rutherford and Xue, 2015). In recent years, a myriad of neural-network based models have been proposed for the task of recognizing implicit discourse relations (Ji and Eisenstein, 2014; Zhang et al., 2015; Liu and Li, 2016; Qin et al., 2017, inter alia). Models of this kind have a high expressive power and generally outperform methods that rely on manual feature engineering. However, being able to trace back improvements to individual features was key to my discussion in Section 3. Recent results in downstream NLP tasks indicate that neural network models can perform better when incorporating binary features (Cheng et"
W17-6934,W16-2209,0,0.0243371,"years, a myriad of neural-network based models have been proposed for the task of recognizing implicit discourse relations (Ji and Eisenstein, 2014; Zhang et al., 2015; Liu and Li, 2016; Qin et al., 2017, inter alia). Models of this kind have a high expressive power and generally outperform methods that rely on manual feature engineering. However, being able to trace back improvements to individual features was key to my discussion in Section 3. Recent results in downstream NLP tasks indicate that neural network models can perform better when incorporating binary features (Cheng et al., 2016; Sennrich and Haddow, 2016, inter alia). 5 Conclusions I proposed a simple model combination for discourse relation classification that aggregates outputs from multiple classifiers. Several classifiers use novel features based on automatic semantic role labeling. I have shown that such features improve classification performance and provide shallow insights into relationships between role semantics and discourse semantics. In the future, I plan to apply more sophisticated methods of model ensembling. I would like to investigate whether neural network approaches to discourse relation classification can also benefit from"
W17-6934,P10-1040,0,0.0434397,"00 Production rules. Features on production rules used to construct each discourse segment’s constituency tree (e.g. 1:S NP VP; see Lin et al., 2009). ca. 78 000 Verb features. Indicators for the main verb, its tense/modality and average verb phrase length (e.g. 1:VERB:phone, 2:TENSE:past; see Park and Cardie, 2012). ca. 20 000 Coreference. Set of features that indicate coreferring mentions, as predicted by Stanford CoreNLP (Lee et al., 2013), across two related discourse segments (see Rutherford and Xue, 2014). ca. 10 000 Brown clusters. Feature sets indicating precomputed Brown cluster IDs (Turian et al., 2010) of words occurring in each discourse segment (e.g. 2:11100110; see Braud and Denis, 2015). 200–6 400 Pairwise Brown clusters. Pairwise Brown cluster IDs indicating word pairs across two related discourse segments (e.g. 11110110x11000100; see Braud and Denis, 2015). up to 10 million 2 Sum/averaging is used here because of its simplicity and robustness (Kittler et al., 1998). Due to the small development set size, methods with additional parameters may tend to overfit. 2.2 Features based on Semantic Roles As new features, I propose to utilize the semantic roles identified in a pair of discourse"
W17-6934,D15-1266,0,0.0610763,"ment set. The third model, AllFeats, is a baseline logistic regression classifier that uses all best development feature sets at the same time. For comparison, I consider a range of current state-of-the-art models. The best feature-rich models (Rutherford and Xue, 2014; Braud and Denis, 2015) use a range of binary indicator features largely identical to the features described in Section 2.1. The most notable difference to this work is that Rutherford and Xue use a small list of coreference patterns in addition to features that simply indicate coreferring mention counts. Neural-network models (Zhang et al., 2015; Liu and Li, 2016; Qin et al., 2016) use attention or convolution mechanisms to identify important words and word spans in each discourse segment. They then predict the discourse relation based on a composition function applied over representations of important words. All of the comparison models use the same training and test instances as this work and are directly comparable. 3 I also experimented with feature conjunctions in order to explicitly model semantic interactions between two discourse segments. However, such conjuctions consistently reduced development performance, probably due to"
W19-3410,N15-1122,0,0.0183463,"esent a two-stage model that combines established methods from topic segmentation and text classification (Section 4). • Finally, we show that the proposed model achieves promising results but also reveals some of the difficulties underlying the task of scenario detection (Section 5). 2 Motivation and Background about the topics represented in these corpora. For instance, Chambers and Jurafsky (2008, 2009); Pichotta and Mooney (2014) leverage newswire texts, Manshadi et al. (2008); Gordon (2010); Rudinger et al. (2015); Tandon et al. (2014, 2017) leverage web articles while Ryu et al. (2010); Abend et al. (2015); Chu et al. (2017) leverage organized procedural knowledge (e.g. from eHow.com, wikiHow.com). The top part of Table 1 summarizes various script knowledge-bases. Our work lies in between both lines of research and may help to connect them: we take an extended set of specific scenarios as a starting point and attempt to identify instances of those scenarios in a large-scale collection of narrative texts. Textual resources. Previous work created scriptrelated resources by crowdsourcing stories that instantiate script knowledge of specific scenarios. For example, Modi et al. (2016) and Ostermann"
W19-3410,P10-1100,1,0.868215,"ant previous work in these areas in more detail. Script knowledge. Scripts are descriptions of prototypical everyday activities such as eating in a restaurant or riding a bus (Schank and Abelson, 1977). Different lines of research attempt to acquire script knowledge. Early researchers attempted to handcraft script knowledge (Mueller, 1999; Gordon, 2001). Another line of research focuses on the collection of scenario-specific script knowledge in form of event sequence descriptions (ESDs) via crowdsourcing, (Singh et al., 2002; Gupta and Kochenderfer, 2004; Li et al., 2012; Raisig et al., 2009; Regneri et al., 2010; Wanzare et al., 2016)). ESDs are sequences of short sentences, in bullet style, describing how a given scenario is typically realized. The top part of Table 1 summarizes various script knowledgebases (ESDs). While datasets like OMICS seem large, they focus only on mundane indoor scenarios (e.g. open door, switch off lights). A third line of research tries to leverage existing large text corpora to induce script-like knowledge Script-related tasks. Several tasks have been proposed that require or test computational models of script knowledge. For example, Kasch and Oates (2010) and Rahimtorog"
W19-3410,P17-4020,0,0.0586831,"Missing"
W19-3410,W10-0905,0,\N,Missing
W19-3410,P94-1002,0,\N,Missing
W19-3410,E14-1024,0,\N,Missing
W19-3410,P08-1090,0,\N,Missing
W19-3410,P09-1068,0,\N,Missing
W19-3410,D15-1195,0,\N,Missing
W19-3410,D14-1082,0,\N,Missing
W19-3410,N16-1098,0,\N,Missing
W19-3410,W16-3644,0,\N,Missing
W19-3410,W16-4011,0,\N,Missing
W19-3410,S18-1119,1,\N,Missing
W19-3410,L18-1512,1,\N,Missing
W19-3410,L16-1555,1,\N,Missing
W19-3410,W12-3307,0,\N,Missing
