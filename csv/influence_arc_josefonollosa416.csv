2005.iwslt-1.23,2002.tmi-tutorials.2,0,0.0759388,"ared under monotone conditions. We finally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phras"
2005.iwslt-1.23,P01-1067,0,0.0561396,"under monotone conditions. We finally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, w"
2005.iwslt-1.23,W02-1018,0,0.0292349,"monotone conditions. We finally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where"
2005.iwslt-1.23,N04-1033,0,0.415123,"tuples methods. As can be seen,to produce the source sentence, the extracted unfolded tuples must be reordered. It is not the case of the target sentence, as it can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any gi"
2005.iwslt-1.23,J04-4002,0,0.0562728,"t is not the case of the target sentence, as it can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provid"
2005.iwslt-1.23,W05-0827,1,0.833763,"t can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [16] as the"
2005.iwslt-1.23,P05-1032,0,0.0181694,"he target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [16] as the probability of reappearence of larger phrases decreases. In our system we considered two length limits. We first extract all the phrases of length X or less (usually X equal to 3 or 4). Then, we also add phrases up to length Y (Y greater than X) if they cannot be generated by smaller phrases. Basically, we select additional phrases with source words that otherwise would be missed because"
2005.iwslt-1.23,N03-1017,0,0.0564353,"gth in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [16] as the probability of reappearence of larger phrases decreases. In our system we considered two length limits. We first extract all the phrases of length X or less (usually X equal to 3 or 4). Then, we also add phrases up to length Y (Y greater than X) if they cannot be generated by smaller phrases. Basically, we select additional phrases with source words that otherwise would be missed because of cross or long alignments [14]. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frecuency. N (f, e) P (f |e) = N (e) (3) where N(f,e) means t"
2005.iwslt-1.23,N04-1021,0,0.198833,"s is approximated by the product of word 3-gram probabilities: p(Tk ) ≈ k Y p(wn |wn−2 , wn−1 ) (4) n=1 where Tk refers to the partial translation hypothesis and wn to the nth word in it. As default language model feature, we use a standard word-based trigram language model generated with smoothing Kneser-Ney and interpolation of higher and lower order ngrams (by using SRILM [17]). • The following two feature functions correspond to a forward and backwards lexicon models. These models provides lexicon translation probabilities for each tuple based on the word-to-word IBM model 1 probabilities [18]. These lexicon models are computed according to the following equation: p((t, s)n ) = J X I Y 1 pIBM 1 (tin |sjn ) (I + 1)J j=1 i=0 (5) where sjn and tin are the j th and ith words in the source and target sides of tuple (t, s)n , being J and I the corresponding total number words in each side of it. For computing the forward lexicon model, IBM model 1 probabilities from GIZA++ [19] source-to-target alignments are used. In the case of the backwards lexicon model, GIZA++ target-to-source alignments are used instead. • The last feature in common we consider corresponds to a word penalty model."
2005.iwslt-1.23,2005.mtsummit-papers.37,1,0.848127,"model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where the sequential composition of the phrases source words corresponds to the source sentence reordered. This procedure poses additional difficulties when applied to the ngram-based approach, because the characteristics of the ngram-based translation model. Despite of this, recent works ( [7], [8]) have shown how applying a reordering schema in the training process the ngram-based approach can also take advantage of the distortion capabilities. In this paper we study the differences and similarities of both approaches (ngram-based and phrase-based), focusing on the translation model, where the translation context is differently taken into account. We also investigate the differences in the translation (bilingual) units (tuples and phrases) and show efficiency results in terms of computation time and memory size for both systems. We have extended the comparison in [6] to a Chinese"
2005.iwslt-1.23,W05-0831,0,0.0547079,"l, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where the sequential composition of the phrases source words corresponds to the source sentence reordered. This procedure poses additional difficulties when applied to the ngram-based approach, because the characteristics of the ngram-based translation model. Despite of this, recent works ( [7], [8]) have shown how applying a reordering schema in the training process the ngram-based approach can also take advantage of the distortion capabilities. In this paper we study the differences and similarities of both approaches (ngram-based and phrase-based), focusing on the translation model, where the translation context is differently taken into account. We also investigate the differences in the translation (bilingual) units (tuples and phrases) and show efficiency results in terms of computation time and memory size for both systems. We have extended the comparison in [6] to a Chinese to En"
2005.iwslt-1.23,P02-1038,0,0.234488,"els taken into account in the log-linear combination of features (see equation 1), and the bilingual units extraction methods (namely tuples and phrases). In section 3 is discussed the decoder used in both systems (MARIE) [9], giving details of pruning and reordering techniques. The comparison framework, experiments and results are shown in section 4, while conclusions are detailed in section 5. 2. Modeling Alternatively to the classical source channel approach, statistical machine translation models directly the posterior probability p(eI1 |f1J ) as a log-linear combination of feature models [10], based on the maximum entropy framework, as shown in [11]. This simplifies the introduction of several additional models explaining the translation process, as the search becomes: arg max{exp( eI1 X λi hi (e, f ))} (1) i where the feature functions hi are the system models (translation model, language model, reordering model, ...), and the λi weights are typically optimized to maximize a scoring function on a development set. The Translation Model is based on bilingual units (here called tuples and phrases). A bilingual unit consists of two monolingual fragments, where each one is supposed to"
2005.iwslt-1.23,J96-1002,0,0.0712527,"atures (see equation 1), and the bilingual units extraction methods (namely tuples and phrases). In section 3 is discussed the decoder used in both systems (MARIE) [9], giving details of pruning and reordering techniques. The comparison framework, experiments and results are shown in section 4, while conclusions are detailed in section 5. 2. Modeling Alternatively to the classical source channel approach, statistical machine translation models directly the posterior probability p(eI1 |f1J ) as a log-linear combination of feature models [10], based on the maximum entropy framework, as shown in [11]. This simplifies the introduction of several additional models explaining the translation process, as the search becomes: arg max{exp( eI1 X λi hi (e, f ))} (1) i where the feature functions hi are the system models (translation model, language model, reordering model, ...), and the λi weights are typically optimized to maximize a scoring function on a development set. The Translation Model is based on bilingual units (here called tuples and phrases). A bilingual unit consists of two monolingual fragments, where each one is supposed to be the translation of its counterpart. During training, t"
2005.iwslt-1.23,takezawa-etal-2002-toward,0,0.0468191,"sary trade-off between quality and efficiency. 4. Comparison 4.1. Evaluation Framework Figure 2: Search graph corresponding to a source sentence with four words. Details of constraints are given in following sections. The search loops expanding available hypotheses. The expansion proceeds incrementally starting in the group of lists covering 1 source word, ending with the group of lists covering J − 1 source words (J is the size in words of the source sentence). See [9] for further details. Experiments have been carried out using two databases: the EPPS database (Spanish-English) and the BTEC [20] database (Chinese-English). The BTEC is a small corpus translation task, used in the IWSLT’04 spoken language campaign1. Table 1 shows the main statistics of the used data, namely number of sentences, words, vocabulary, and mean sentence lengths for each language. The EPPS data set corresponds to the parliamentary session transcriptions of the European Parliament and is currently available at the Parliament’s website (http://www.euro parl.eu.int/). In the case of the results presented here, we have used the version of the EPPS data that was made available by RWTH Aachen University through the"
2005.iwslt-1.23,2005.iwslt-1.24,1,0.680304,"ated to the corpus size). Similar accuracy results in all tasks are reached for the baseline configurations. When upgrading the systems with additional features, slight differences appear. Although improvements added by each feature depends on the task and system, similar performances are reached in the best system’s configurations. Under reordering conditions, the ngram-based system seems to take advantage of the unfolding method applied in training, outperforming the phrase-based system. However, last results obtained for the IWSLT’05 show an opposite behaviour of both systems, see [22] and [23]. We can conclude that both approaches have a similar performance in terms of translation quality. The slight differences seen in the experiments are related to how the systems take advantage of each feature model and to the current system’s implementation. In terms of the memory size and computation time, the ngram-based system has obtained consistently better results. This indicates how even though using a smaller vocabulary of bilingual units, it has been more efficiently built and managed. The last characteristic becomes of great importance when working with large databases. 6. Acknowledgm"
2005.iwslt-1.24,J96-1002,0,0.0402617,"e translation model P (f |e) [6]. In the last few years, new systems tend to use sequences of words, commonly called phrases [7], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework.   M  λm hm (e, f ) (3) e˜ = argmax e m=1 The features functions, h m , are the system models (translation model, language model and others) and weights, λi , are typically optimized to maximize a scoring function [12]. It is derived from the Maximum Entropy approach as shown in [1] and has the advantage that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phraseextraction algorithm in [13] and results in Chinese to English and Arabic to English tasks are reported. It also combines several alignments before extracting phrases and interesting features. It is organized as follows. Section 2 explains the SMT system: the phrase extraction, its modification and shows the different features which have been taken into account and, briefly, the decoding; section 3 presents the evaluation framework and the r"
2005.iwslt-1.24,W05-0827,1,0.783429,"-gram probabilities: 2.1.1. Word alignment Given a sentence pair, we use GIZA++ [10] to align each of them word-to-word. We can train in both translation directions and we obtain: (1) the alignment in the source to target direction (s2t); and (2) the alignment in the target to source direction. If we compose the union of both alignments (sU t), we get a higher recall and a lower precision of the combined alignment. 2.1.2. Phrase-extraction Phrases are extracted from sentence pairs and theirs corespondents word alignments following the criterion in [13] and the modification in phrase length in [4]. A phrase is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is unfeasible to build a dictionary with all the phrases. That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [7] as the probability of reappearance of larger phrases decreases. In our system we considered two"
2005.iwslt-1.24,2005.iwslt-1.23,1,0.726606,"ts are pruned out according to the accumulated probabilities of their hypotheses. Worst hypotheses with minor probabilities are discarded to make the search feasible. Also the decoder allows reordering. The use of the reordering strategies suppose a necessary trade-off between quality and efficiency. That is why two reordering strategies are used: • A distortion limit (m). A source word (phrase or tuple) is only allowed to be reordered if it does not exceed a distortion limit, measured in words. • A reorderings limit (j). Any translation path is only allowed to perform j reordering jumps. See [5] for further details. 3. Evaluation Framework 3.1. Corpus Statistics Experiments have been carried out in two tasks of the IWSLT’05 evaluation 1: Chinese to English (BTEC Corpus [15]) and Arabic to English. The BTEC is a small corpus translation task. Table 1 shows the main statistics of the used data, namely number of sentences, words, vocabulary, and mean sentence lengths for each language. 1 www.slt.atr.jp/IWSLT2005 BTEC Training Sentences Words Vocabulary Development Sentences Words Vocabulary Test Sentences Words Vocabulary Chinese 20 k 176.2 k 8.7 k 1006 7.3 k 1.4 k 506 3.7 k 963 English"
2005.iwslt-1.24,N03-1017,0,0.0943422,"to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P (e|f ), e˜ = argmax P (e|f ) e (1) If we use Bayes rule to reformulate the translation probability, we obtain, e˜ = argmax P (f |e)P (e) e (2) This translation model is known as the source-channel approach [2] and it consists on a language model P (e) and a separate translation model P (f |e) [6]. In the last few years, new systems tend to use sequences of words, commonly called phrases [7], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework.   M  λm hm (e, f ) (3) e˜ = argmax e m=1 The features functions, h m , are the system models (translation model, language model and others) and weights, λi , are typically optimized to maximize a scoring function [12]. It is derived from the Maximum Entropy approach as shown in [1] and has the advantage that additional features functions can be easily integrated in the overall system. This paper addresse"
2005.iwslt-1.24,W02-1018,0,0.0257822,"used which lead to a clear improvement in the performance of translation. Finally, the system manages to do reordering. We report results in terms of translation accuracy by using the BTEC corpus in the tasks of Chinese to English and Arabic to English, in the framework of IWSLT’05 evaluation. 1. Introduction From the initial word-based translation models [3], research on statistical machine translation has been strongly improved. At the end of the last decade the use of context in the translation model (phrase-based approach) supposed a clear improvement in translation quality ( [17], [16], [8]). Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P (e|f ), e˜ = argmax P (e|f ) e (1) If we use Bayes rule to reformulate the translation probability, we ob"
2005.iwslt-1.24,N04-1021,0,0.0284561,"n |...wn−3 , wn−2 , wn−1 ) (5) n=1 where Tk refers to the partial translation hypothesis and wn to the nth word in it. • As translation model we use the conditional probability. Note that no smoothing is performed, which may cause an overestimation of the probability of rare phrases. This is specially harmful given a bilingual phrase where the source part has a big frequency of appearance but the target part appears rarely. That is why we use the posterior phrase probability, we compute again the relative frequency but replacing the count of the target phrase by the count of the source phrase [11]. P (e|f ) = N  (f, e) N (f ) (6) where N’(f,e) means the number of times the phrase e is translated by f. If a phrase f has N &gt; 1 possible translations, then each one contributes as 1/N. Adding this feature function we reduce the number of cases in which the overall probability is overestimated. • The following two feature functions correspond to a forward and backward lexicon models. These models provides lexicon translation probabilities for each tuple based on the word-to-word IBM model 1 probabilities [11]. These lexicon models are computed according to the following equation: J  I  1"
2005.iwslt-1.24,P02-1038,0,0.0539036,"proach [2] and it consists on a language model P (e) and a separate translation model P (f |e) [6]. In the last few years, new systems tend to use sequences of words, commonly called phrases [7], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework.   M  λm hm (e, f ) (3) e˜ = argmax e m=1 The features functions, h m , are the system models (translation model, language model and others) and weights, λi , are typically optimized to maximize a scoring function [12]. It is derived from the Maximum Entropy approach as shown in [1] and has the advantage that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phraseextraction algorithm in [13] and results in Chinese to English and Arabic to English tasks are reported. It also combines several alignments before extracting phrases and interesting features. It is organized as follows. Section 2 explains the SMT system: the phrase extraction, its modification and shows the different features which have been taken into account and, briefly, th"
2005.iwslt-1.24,J04-4002,0,0.320048,"n model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework.   M  λm hm (e, f ) (3) e˜ = argmax e m=1 The features functions, h m , are the system models (translation model, language model and others) and weights, λi , are typically optimized to maximize a scoring function [12]. It is derived from the Maximum Entropy approach as shown in [1] and has the advantage that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phraseextraction algorithm in [13] and results in Chinese to English and Arabic to English tasks are reported. It also combines several alignments before extracting phrases and interesting features. It is organized as follows. Section 2 explains the SMT system: the phrase extraction, its modification and shows the different features which have been taken into account and, briefly, the decoding; section 3 presents the evaluation framework and the results in Chinese to English and Arabic to English tasks are reported; and the final section shows some conclusions on the experiments and in the evaluation of IWSLT’05. 2. SMT system"
2005.iwslt-1.24,takezawa-etal-2002-toward,0,0.0301298,"er allows reordering. The use of the reordering strategies suppose a necessary trade-off between quality and efficiency. That is why two reordering strategies are used: • A distortion limit (m). A source word (phrase or tuple) is only allowed to be reordered if it does not exceed a distortion limit, measured in words. • A reorderings limit (j). Any translation path is only allowed to perform j reordering jumps. See [5] for further details. 3. Evaluation Framework 3.1. Corpus Statistics Experiments have been carried out in two tasks of the IWSLT’05 evaluation 1: Chinese to English (BTEC Corpus [15]) and Arabic to English. The BTEC is a small corpus translation task. Table 1 shows the main statistics of the used data, namely number of sentences, words, vocabulary, and mean sentence lengths for each language. 1 www.slt.atr.jp/IWSLT2005 BTEC Training Sentences Words Vocabulary Development Sentences Words Vocabulary Test Sentences Words Vocabulary Chinese 20 k 176.2 k 8.7 k 1006 7.3 k 1.4 k 506 3.7 k 963 English 20 k 182.3 k 7.3 k 1006 6k 1.3 k 506 - Table 1: Chinese to English task. BTEC Corpus: Training, Development and Test data sets. The Development data set has 16 references, (k stands"
2005.iwslt-1.24,P01-1067,0,0.0695767,"es are used which lead to a clear improvement in the performance of translation. Finally, the system manages to do reordering. We report results in terms of translation accuracy by using the BTEC corpus in the tasks of Chinese to English and Arabic to English, in the framework of IWSLT’05 evaluation. 1. Introduction From the initial word-based translation models [3], research on statistical machine translation has been strongly improved. At the end of the last decade the use of context in the translation model (phrase-based approach) supposed a clear improvement in translation quality ( [17], [16], [8]). Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P (e|f ), e˜ = argmax P (e|f ) e (1) If we use Bayes rule to reformulate the translation probability,"
2005.iwslt-1.24,2002.tmi-tutorials.2,0,0.0513375,"features are used which lead to a clear improvement in the performance of translation. Finally, the system manages to do reordering. We report results in terms of translation accuracy by using the BTEC corpus in the tasks of Chinese to English and Arabic to English, in the framework of IWSLT’05 evaluation. 1. Introduction From the initial word-based translation models [3], research on statistical machine translation has been strongly improved. At the end of the last decade the use of context in the translation model (phrase-based approach) supposed a clear improvement in translation quality ( [17], [16], [8]). Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P (e|f ), e˜ = argmax P (e|f ) e (1) If we use Bayes rule to reformulate the translation probabi"
2005.iwslt-1.24,N04-1033,0,0.557737,"ecoding that build this system. The Translation Model is based on bilingual phrase (or phrases). A bilingual unit consists of two monolingual fragments, where each one is supposed to be the translation of its counterpart. During training, the system learns a dictionary of these bilingual fragments, the actual core of the translation systems. 2.1. Phrase-based Translation Model 2.2. Additional features The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [18]. • Firstly, we consider the target language model. It actually consists of an n-gram model, in which the probability of a translation hypothesis is approximated by the product of word n-gram probabilities: 2.1.1. Word alignment Given a sentence pair, we use GIZA++ [10] to align each of them word-to-word. We can train in both translation directions and we obtain: (1) the alignment in the source to target direction (s2t); and (2) the alignment in the target to source direction. If we compose the union of both alignments (sU t), we get a higher recall and a lower precision of the combined alignm"
2005.iwslt-1.24,J90-2002,0,\N,Missing
2005.mtsummit-papers.36,N01-1018,0,0.0681459,"ation of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper implements a translation model based on the ﬁnite-state perspective, (de Gispert and Mari˜ no, 2002) and (de Gispert et al., 2004), which is used along with a log-linear combination of four additional feature functions (Crego et al., 2005). The implemented translation model, which is referred to as tuple n-gram model, diﬀers from the well known phrase-model approach (Koehn et al., 2003) in two basic issues."
2005.mtsummit-papers.36,J96-1002,0,0.115652,"entioned in the introduction, the translation system presented here implements a log-linear combination of feature functions 1 In the present version of the system, target words aligned to NULL are always attached to the following word. Further work in this area is proposed in the last section. along with the tuple n-gram model. This section describes the log-linear model and each of the four speciﬁc feature functions that are used. Finally, a brief description of the customized decoding tool that is used is presented. 3.1 Log-Linear Model Framework According to the maximum entropy framework (Berger et al., 1996), the corresponding translation hypothesis T , for a given source sentence S, is deﬁned by the target sentence that maximizes a log-linear combination of feature functions hi (S, T ), as described in the following equation:  argmax λi hi (S, T ) (2) T i where the λi ’s constitute the weighting coeﬃcients of the log-linear combination and the feature function hi (S, T ) corresponds to a logarithmic scaling of the ith -model probabilities. These weights are computed via an optimization procedure which maximizes the translation BLEU (Papineni et al., 2002) over a given development set. This opti"
2005.mtsummit-papers.36,J90-2002,0,0.523079,"Missing"
2005.mtsummit-papers.36,J93-2003,0,0.0465928,"ementation of translation algorithms based on statistical methods (Brown et al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is l"
2005.mtsummit-papers.36,N04-1033,0,0.157603,"Missing"
2005.mtsummit-papers.36,2005.iwslt-1.23,1,0.857305,"Missing"
2005.mtsummit-papers.36,2004.iwslt-evaluation.14,1,0.687498,"Missing"
2005.mtsummit-papers.36,P05-2012,1,0.78658,"Missing"
2005.mtsummit-papers.36,N03-1017,0,0.067546,"The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper implements a translation"
2005.mtsummit-papers.36,P02-1038,0,0.221853,"otivated by the development of computer resources needed to allow the implementation of translation algorithms based on statistical methods (Brown et al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of"
2005.mtsummit-papers.36,N04-1021,0,0.0290838,"length penalization in order to compensate the system preference for short target sentences caused by the presence of the previous target language model. This penalization depends on the total number of words contained in the partial translation hypothesis, and it is computed as follows: wp(Tk ) = exp(number of words in Tk ) (4) where, again, Tk refers to the partial translation hypothesis. The fourth and ﬁfth feature functions correspond to a forward and backward lexicon models. These models provide IBM 1 translation probabilities for each tuple based on the IBM 1 lexical parameters p(t|s) (Och et al., 2004). These lexicon models are computed according to the following equation: pIBM 1 ((t, s)n ) = I J   1 p(tin |sjn ) (5) (I + 1)J j=1 i=0 (3) where sjn and tin are the j th and ith words in the source and target sides of tuple (t, s)n , being J and I the corresponding total number words in each side of it. For computing the forward lexicon model, IBM model 1 lexical parameters from GIZA++ source-to-target alignments are used. In the case of the backward lexicon model, GIZA++ target-to-source alignments are used instead. where Tk refers to the partial translation hypothesis and wn to the nth wor"
2005.mtsummit-papers.36,P02-1040,0,0.115781,"cording to the maximum entropy framework (Berger et al., 1996), the corresponding translation hypothesis T , for a given source sentence S, is deﬁned by the target sentence that maximizes a log-linear combination of feature functions hi (S, T ), as described in the following equation:  argmax λi hi (S, T ) (2) T i where the λi ’s constitute the weighting coeﬃcients of the log-linear combination and the feature function hi (S, T ) corresponds to a logarithmic scaling of the ith -model probabilities. These weights are computed via an optimization procedure which maximizes the translation BLEU (Papineni et al., 2002) over a given development set. This optimization is performed by using an in-house developed optimization algorithm, which is based on a simplex method (Press et al., 2002). 3.2 Implemented Feature Functions The proposed translation system implements a total of ﬁve feature functions: • tuple 3-gram model, • target language model, • word penalty model, • source-to-target lexicon model, and • target-to-source lexicon model. The ﬁrst of these functions is the tuple 3-gram model, which was already described in the previous section. The second feature function implemented is a target language model"
2005.mtsummit-papers.36,2002.tmi-tutorials.2,0,0.0433398,"al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper"
2005.mtsummit-papers.36,P00-1056,0,\N,Missing
2006.iwslt-evaluation.17,W05-0820,0,0.0116848,", namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes. 1. Introduction Rooted in the Finite-State Transducers approach to SMT [1, 2] and estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternative to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3]. This is specially true when dealing with pairs of languages with a relatively similar word order [4, 5]. Given the language pairs involved in this year’s evaluation, efforts have been focused on improving the word reordering strategies for Ngram-based SMT. Specifically, a novel reordering strategy based on extending the search graph with automatically-extracted reordering patterns is explored. Results are very promising while keeping computational expenses at a similar level of monotone search. Additionally, a novel tuple segmentation strategy based on the entropy of Part-Of-Speech distributions was used with slight improvements in model estimation. This paper is organized as follows. Section 2"
2006.iwslt-evaluation.17,W06-3114,0,0.0159274,", namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes. 1. Introduction Rooted in the Finite-State Transducers approach to SMT [1, 2] and estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternative to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3]. This is specially true when dealing with pairs of languages with a relatively similar word order [4, 5]. Given the language pairs involved in this year’s evaluation, efforts have been focused on improving the word reordering strategies for Ngram-based SMT. Specifically, a novel reordering strategy based on extending the search graph with automatically-extracted reordering patterns is explored. Results are very promising while keeping computational expenses at a similar level of monotone search. Additionally, a novel tuple segmentation strategy based on the entropy of Part-Of-Speech distributions was used with slight improvements in model estimation. This paper is organized as follows. Section 2"
2006.iwslt-evaluation.17,2005.mtsummit-papers.36,1,0.472143,"g patterns. Section 4 focuses on tuple segmentation strategies, and contrasts the criterion on IBM model 1 probabilities from 2005 with a novel criterion based on Part-Of-Speech entropy distributions. Later on, Section 5 reports on all experiments carried out from Arabic, Chinese, Italian and Japanese into English for IWSLT 2006. Finally, Section 6 sums up the main conclusions from the paper and discusses future research lines. 2. 2005 system review The TALP Ngram-based SMT system performs a log-linear combination of a translation model and additional feature functions (see further details in [6, 7]). In contrast to phrasebased models, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel"
2006.iwslt-evaluation.17,2005.iwslt-1.23,1,0.868702,"l and additional feature functions (see further details in [6, 7]). In contrast to phrasebased models, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints 116 3. Word ordering strategies 2.2. Feature functions As additional feature functions to better guide the translation process, the system incorporates a target language model, a word bonus model and two lexicon models. The target language model is estimated"
2006.iwslt-evaluation.17,N04-1033,0,0.120187,"ated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints 116 3. Word ordering strategies 2.2. Feature functions As additional feature functions to better guide the translation process, the system incorporates a target language model, a word bonus model and two lexicon models. The target language model is estimated as a standard ngram over the target words, as follows: pLM (tk ) ≈ k Y p(wn |wn−N +1 , ..., wn−1 ) (2) n=1 where tk refers to the par"
2006.iwslt-evaluation.17,2005.mtsummit-papers.37,1,0.788658,"n |wn−N +1 , ..., wn−1 ) (2) n=1 where tk refers to the partial hypothesis and wn to the nth word in it. Usually, this feature is accompanied by a word bonus model based on sentence length, compensating the target language model preference for short sentences (in number of target words). This bonus depends on the number of target words in the partial hypothesis, denoted as: pW P (tk ) = exp(number of words in tk ) When dealing with pairs of languages with non-monotonic word order, a certain reordering strategy is required. Apart from that, tuples need to be extracted by an unfolding technique [11]. This means that the tuples are broken into smaller tuples, and these are sequenced in the order of the target words. In order not to lose the information on the correct order, the decoder performs then a reordered search (or a monotone search extended with reordering paths), which is guided by the n-gram model of the unfolded tuples and the additional feature models. Figure 1 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual n-gram language model with reordered source words. (3) where tk refers to the partial hypo"
2006.iwslt-evaluation.17,A00-1031,0,0.0103336,"Language-dependent preprocessing For all language pairs, training sentences were split by using final dots on both sides of the bilingual text (when the number of dots was equal), increasing the number of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. English Table 1: Arabic→English corpus statistics. sent. it en it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronom"
2006.iwslt-evaluation.17,N06-2013,0,0.0357882,"mber of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. English Table 1: Arabic→English corpus statistics. sent. it en it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. The Arabic Treebank tag set used contains 20 different tags. Corpora statistics for all language pairs can be found in 2 Ver"
2006.iwslt-evaluation.17,P05-1071,0,0.0218458,"n it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. The Arabic Treebank tag set used contains 20 different tags. Corpora statistics for all language pairs can be found in 2 Version 119 2.0. Linguistic Data Consortium Catalog: LDC2004L02. 5.2.3. Chinese official Chinese preprocessing included resegmentation and POStagging. These tasks were done by using ICTCLAS [19]. Resultan"
2006.iwslt-evaluation.17,W03-1730,0,0.0274114,"Missing"
2006.iwslt-evaluation.17,2004.iwslt-evaluation.14,1,\N,Missing
2006.iwslt-evaluation.17,E99-1010,0,\N,Missing
2006.iwslt-evaluation.17,J96-1002,0,\N,Missing
2006.iwslt-evaluation.17,W05-0823,1,\N,Missing
2006.iwslt-evaluation.17,N07-2022,1,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.1,0,\N,Missing
2006.iwslt-evaluation.17,J06-4004,1,\N,Missing
2006.iwslt-evaluation.17,N03-1017,0,\N,Missing
2006.iwslt-evaluation.17,J03-1002,0,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-evaluation.1,0,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.24,1,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-papers.2,1,\N,Missing
2006.iwslt-evaluation.17,atserias-etal-2006-freeling,0,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.11,0,\N,Missing
2006.iwslt-evaluation.17,P00-1056,0,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-papers.5,1,\N,Missing
2006.iwslt-evaluation.18,2005.iwslt-1.24,1,0.787344,"TALP phrase-based statistical machine translation system, enriched with the statistical machine reordering technique. We also report the combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report th"
2006.iwslt-evaluation.18,W06-1609,1,0.921033,"e combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to Eng"
2006.iwslt-evaluation.18,W06-3125,1,0.836838,"ional Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from th"
2006.iwslt-evaluation.18,P02-1038,0,0.0577834,"re consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. (1) The feature functions, hm , and weights, λi , are typically optimized to maximize the scoring function [4]. Two basic issues differentiate the n-gram-based system from the phrase-based system: the bilingual units are extracted from a monotonic segmentation of the training data; the unit probabilities are based on a standard back-off language model rather than directly on relative frequencies. In both systems, the introduction of reordering capabilities is crucial for certain language pairs. This paper is organized as follows. Section 2 describes the TALP-phrase system, with particular emphasis on a new reordering technique: the statistical machine reordering approach. In Section 3, we combine the"
2006.iwslt-evaluation.18,J04-4002,0,0.0302302,"X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from these phrase translations. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [5]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequen"
2006.iwslt-evaluation.18,2005.iwslt-1.6,0,0.0322379,"using the GIZA++ tool [7]. During word alignment, we used 50 classes per language. We aligned both translation directions and combined the two alignments with the union operation. train dev4 dev123 test ASRtest • Word classes (which were used to help the aligner and to perform the SMR process) were determined using “mkcls”, a tool freely-available with GIZA++. • The language model was estimated using the SRILM toolkit [8]. • The decoder was MARIE [9]. • The optimization tool used for computing log-linear weights was based on the simplex method [6]. Following the consensus strategy proposed in [10], the objective function was set to 100 · BLEU + 4 · N IST . 489 500 500 500 voc. 9.7k 9.6k 1,096 909 1,292 1,311 slen. 6.7 7.0 11.2 6.0 11.7 11.6 refs. 1 7 16 7 7 Corpus statistics for all language pairs can be found in Tables 1, 2, 3 and 4, respectively, where number of sentences, running words, vocabulary, sentence length and human references are shown. sent. train dev4 dev123 test ASRtest Experiments were carried out for all tasks of the IWSLT06 evaluation (Zh2En, Jp2En, Ar2En and It2En) using the BTEC Corpus provided for the open data track1 . it en it it it it 24.6k 489 500 500 500 wrds"
2006.iwslt-evaluation.18,A00-1031,0,0.0229282,"while randomly selecting 500 sentences from developments 1, 2 and 3 (around 160 sentences from each) to build an internal test set (dev123). zh en zh zh zh zh 4.4. Language-dependent preprocessing For all language pairs, training sentences were split by using full stops on both sides of the bilingual text (when the number of stops was equal), increasing the number of sentences and reducing their length. Specific preprocessing for each language is detailed in the respective section below. 4.4.1. English English preprocessing includes part-of-speech tagging using the freely-available TnT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly p"
2006.iwslt-evaluation.18,N06-2013,0,0.0411805,"ing their length. Specific preprocessing for each language is detailed in the respective section below. 4.4.1. English English preprocessing includes part-of-speech tagging using the freely-available TnT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. search (with m = 5 and j = 3) for all tasks and for all systems (with or without SMR technique); except for the Italian to E"
2006.iwslt-evaluation.18,P05-1071,0,0.0382995,"nT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. search (with m = 5 and j = 3) for all tasks and for all systems (with or without SMR technique); except for the Italian to English task where a monotonic search was used. The primary system of each task is that which had the best performance in the internal test. In all tasks, the SMR improved the results in the internal te"
2006.iwslt-evaluation.18,W03-1730,0,0.0516051,"Missing"
2006.iwslt-evaluation.18,atserias-etal-2006-freeling,0,0.0138517,"for the internal test set (specially, for the Arabic and Japanese tasks). The higher the number of unknown words, the worse the SMR output and, consequently, the quality of translation. Here, a possible solution would be to predict word classes for unknown words in order to avoid their bad influence in the SMR output. 4.4.3. Chinese Set development test evaluation Chinese preprocessing included re-segmentation and POStagging. These tasks were performed using ICTCLAS [15]. 4.4.4. Italian Italian was POS-tagged and lemmatized using the freelyavailable FreeLing morpho-syntactic analysis package [16]. Additionally, Italian contracted prepositions were separated into preposition + article, for example ’alla’→’a la’, ’degli’→’di gli’ or ’dallo’→’da lo’. 4.4.5. Japanese When dealing with Japanese, one has to come up with new methods for overcoming the absence of delimiters between words. We addressed this issue by word segmentation using the freely available JUMAN tool [17] version 5.1. This tool was also used for POS-tagging of the Japanese text. 4.5. Results In Table 6 we show the results for all the TALP systems that participated in the IWSLT 2006: the TALP-phrase, the TALP-tuple and the"
2006.iwslt-evaluation.18,2004.iwslt-evaluation.8,0,\N,Missing
2006.iwslt-papers.2,2005.iwslt-1.5,0,0.0293593,"ent speeches (T C -S TAR evaluations). Therefore, new techniques must be deployed to take the best advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator"
2006.iwslt-papers.2,2005.iwslt-1.12,0,0.0293042,"ent speeches (T C -S TAR evaluations). Therefore, new techniques must be deployed to take the best advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator"
2006.iwslt-papers.2,2005.iwslt-1.15,0,0.0276075,"ore, new techniques must be deployed to take the best advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the re"
2006.iwslt-papers.2,W05-0821,0,0.0268435,", it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the resulting probability functions are smooth functions of the word representation, better generalization t"
2006.iwslt-papers.2,2003.mtsummit-papers.6,0,0.224734,"on lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the resulting probability functions are smooth functions of the word representation, better generalization to unknown n-grams can be expected. A"
2006.iwslt-papers.2,P06-2093,1,0.833984,"epresentation, better generalization to unknown n-grams can be expected. A neural network can be used to simultaneously learn the projection of the words onto the continuous space and to estimate the ngram probabilities. This is still a n-gram approach, but the LM posterior probabilities are ”interpolated” for any possible context of length n-1 instead of backing-off to shorter contexts. This approach was successfully used in large vocabulary continuous speech recognition [9], and initial experiments have shown that it can be used to improve a word-based statistical machine translation system [10]. Here, the continuous space LM is applied the first time to a state-of-the-art phrase-based SMT system. Translation of four different languages is considered: Mandarin, Japanese, Arabic and Italian to English. These languages exhibit very different characteristics, e.g. with respect to word order, which may affect the role of the target LM, although a reordering model is used in the SMT systems. We also investigate the use of the continuous space LM in a SMT system based on bilingual n-grams. This paper is organized as follows. In the next section we first describe the baseline statistical ma"
2006.iwslt-papers.2,2005.iwslt-1.23,1,0.849141,"ine statistical machine translation systems. Section 3 presents the architecture and training algorithms of the continuous space LM and section 4 summarizes the experimental evaluation. The paper concludes with a discussion of future research directions. 166 2. Baseline systems During the last few years, the use of context in SMT systems has provided great improvements in translation. SMT has evolved from the original word-based approach to phrasebased translation systems. In parallel to the phrase-based approach, the use of bilingual n-grams gives comparable results, as shown by Crego et al. [11]. Two basic issues differentiate the n-gram-based system from the phrase-based: training data are monotonically segmented into bilingual units; and the model considers n-gram probabilities rather than relative frequencies. This translation approach is described in detail by Mari˜no et al. [12]. Both systems follow a maximum entropy approach, in which a log-linear combination of multiple models is implemented, as an alternative to the source-channel approach: This simplifies the introduction of several additional models explaining the translation process, as the search becomes: e∗ = arg max p(e"
2006.iwslt-papers.2,2005.mtsummit-papers.36,1,0.808081,"few years, the use of context in SMT systems has provided great improvements in translation. SMT has evolved from the original word-based approach to phrasebased translation systems. In parallel to the phrase-based approach, the use of bilingual n-grams gives comparable results, as shown by Crego et al. [11]. Two basic issues differentiate the n-gram-based system from the phrase-based: training data are monotonically segmented into bilingual units; and the model considers n-gram probabilities rather than relative frequencies. This translation approach is described in detail by Mari˜no et al. [12]. Both systems follow a maximum entropy approach, in which a log-linear combination of multiple models is implemented, as an alternative to the source-channel approach: This simplifies the introduction of several additional models explaining the translation process, as the search becomes: e∗ = arg max p(e|f ) X λi hi (e, f ))} = arg max{exp( e (1) i where f and e are sentences in the source and target language respectively. The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set. Both the n-grambased and the"
2006.iwslt-papers.2,2006.iwslt-evaluation.18,1,0.876569,"e λi weights are typically optimized to maximize a scoring function on a development set. Both the n-grambased and the phrase-based system use a language model on the target language as feature function, i.e. P (e), but they differ in the translation model. In both cases, it is based on bilingual units. A bilingual unit consists of two monolingual fragments, where each one is supposed to be the translation of its counterpart. During training, each system learns its dictionary of bilingual fragments. Both SMT approaches were evaluated in I WSLT’06 evaluation and they are described in detail in [13, 14]. Therefore, we only give a short summary in the following two sections. • no smaller tuples can be extracted without violating the previous constraints. As a consequence of these constraints, only one segmentation is possible for a given sentence pair. Two important issues regarding this translation model must be considered. First, it often occurs that a large number of single-word translation probabilities are left out of the model. This happens for all words that are always embedded in tuples containing two or more words, then no translation probability for an independent occurrence of thes"
2006.iwslt-papers.2,takezawa-etal-2002-toward,0,0.0402314,"corresponds basically to a table look-up using hashing techniques, while a forward pass through the neural network is necessary for the continuous space LM. Very efficient optimizations are possible, in particular when n-grams with the same context can be grouped together, but a reorganization of the decoder may be necessary. More details on optimizing the neural network LM can be found in [9]. 4. Experimental Evaluation In this work we report results on the Basic Traveling Expression Corpus (B TEC). This corpus consists of typical sentences from phrase books for tourists in several languages [16]. Translation to English from four languages is considered: Mandarin, Japanese, Arabic and Italian. The reference phrase- and n-gram-based SMT systems participated in the open data track of the 2006 I WSLT evaluation [13, 14], i.e. only the supplied subset of the full B TEC corpus was used to train all the statistical models. Details on the data preprocessed as in [13, 14] are summarized in Table 1. We report results on the supplied development corpus of 489 sentences (less than 6k words) using the BLEU score with seven references translations. The scoring is case insensitive and punctuations"
2006.iwslt-papers.2,2005.iwslt-1.6,0,0.0223383,"st advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the resulting probability functions are smooth functions"
2006.iwslt-papers.2,2005.iwslt-1.11,0,\N,Missing
2007.iwslt-1.26,2006.iwslt-papers.2,1,0.882609,"4]. Efforts have been focused on improving translation according to human evaluation by further developing different stages of the SMT system: alignment and rescoring. As in previous years, we aligned the training corpus using Giza++ software. However, instead of keeping the default parameters, we performed a minimum translation error training procedure to adjust Giza++ smoothing parameters to the task. This procedure had been successful with an alignment system based on discriminative training [5]. For the rescoring we incorporate a neural network language model as previously experienced in [6]. The neural network language model mainly is able to produce a better generalization in the translation system. This paper is organized as follows. Section 2 briefly reviews last year’s system, including tuple definition and extraction, translation model and feature functions, decoding tool and reordering and optimization criterion. Section 3 describes the alignment translation-minimum-error training procedure. Section 4 focuses on rescoring using a neural language model (NNLM). Next, Section 5 reports on all experiments carried out from Arabic and Chinese into English for IWSLT 2007. Finally"
2007.iwslt-1.26,N04-1033,0,0.0623584,"ls, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. In this way, it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(S, T ) = K Y p((˜ s, t˜)k |(˜ s, t˜)k−N +1 , ..., (˜ s, t˜)k−1 ) (1) k=1 where s refers to source, t to target, and (˜ s, t˜)k to the k th tuple of a given bilingual sentence pair segmented in K tuples. 2.2. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints However, when dealing with pairs of languages with nonmonotonic word order, a certain reordering strategy is required to extract more reusable units (less sparse). Hence, we allow the source words to be reordered before extracting translation units from training sentence pairs by following the word-to-word alignments. The unfolding technique is fully described in [10]. Figure 1 shows an ex"
2007.iwslt-1.26,2005.mtsummit-papers.37,1,0.856996,"following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints However, when dealing with pairs of languages with nonmonotonic word order, a certain reordering strategy is required to extract more reusable units (less sparse). Hence, we allow the source words to be reordered before extracting translation units from training sentence pairs by following the word-to-word alignments. The unfolding technique is fully described in [10]. Figure 1 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual n-gram language model with reordered source words. where tn refers to the nth word in the partial translation hypothesis T . Usually, this feature is accompanied by a word bonus model based on sentence length, compensating the target language model preference for short sentences (in number of target words). This bonus depends on the number of target words in the partial hypothesis, denoted as: pW P (T ) = exp(number of words in T ). The third and fourth fe"
2007.iwslt-1.26,2006.iwslt-papers.5,1,0.867108,"ChineseEnglish task, a secondary run was performed with a rescoring module, as described in Sections 4 and 5.3.2. 2.5. Feature Weights Optimization To tune the weight of each feature function in the SMT system, we used the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm [12]. SPSA is a stochastic implementation of the conjugate gradient method which requires only two evaluations of the objective function in each iteration, regardless of the dimension of the optimization problem. It was observed to be more robust than the Downhill Simplex method when tuning SMT coefficients [13]. The SPSA procedure is in the general recursive stochastic approximation form: ˆ k+1 = λ ˆ k − ak g ˆk ) ˆk (λ λ (5) lation tuples (as no word within a tuple can be linked to a word out of it [9]). Starting from the monotonic graph, each sequence of input POS tags fulfilling a source-side rewrite rule implies the addition of a reordering arc (which encodes the reordering detailed in the target-side of the rule). Figure 2 shows how three rewrite rules applied over an input sentence extend the search graph given the reordering patterns that match the source POS tag sequence 1 . ˆ k ) is the esˆ"
2007.iwslt-1.26,N07-2022,1,0.823419,"o phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3, 4]. Efforts have been focused on improving translation according to human evaluation by further developing different stages of the SMT system: alignment and rescoring. As in previous years, we aligned the training corpus using Giza++ software. However, instead of keeping the default parameters, we performed a minimum translation error training procedure to adjust Giza++ smoothing parameters to the task. This procedure had been successful with an alignment system based on discriminative training [5]. For the rescoring we incorporate a neural network language model as previously experienced in [6]. The neural network language model mainly is able to produce a better generalization in the translation system. This paper is organized as follows. Section 2 briefly reviews last year’s system, including tuple definition and extraction, translation model and feature functions, decoding tool and reordering and optimization criterion. Section 3 describes the alignment translation-minimum-error training procedure. Section 4 focuses on rescoring using a neural language model (NNLM). Next, Section 5"
2007.iwslt-1.26,N06-2013,0,0.076004,"fluency and METEOR is well correlated to adequacy [4], we supposed that adding all references was beneficial to monolingual language models but not to the bilingual language model. Table 2: Chinese→English corpus statistics. 5.2. Data Preprocessing For all language pairs, training sentences were split by using final dots on both sides of the bilingual text (when the number of dots was equal), increasing the number of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. Arabic Following a similar approach to that in [16], we used the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available"
2007.iwslt-1.26,W03-1730,0,0.018621,"ts length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. Arabic Following a similar approach to that in [16], we used the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal"
2007.iwslt-1.26,A00-1031,0,0.0277074,"he MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results i"
2007.iwslt-1.26,E99-1010,0,0.0816899,"ion produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results in all cases, and the best combination for the system with pattern-based reordering was the union6 . At the end, the best alignment configuration for our baseline system was obtained with Giza++ software, running respectively 5, 5, 3 and 3 iterations of models 1, HMM, 3 and 4, using English stems and 50"
2007.iwslt-1.26,J03-1002,0,0.00791937,"y available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results in all cases, and the best combination for the system with pattern-based reordering was the union6 . At the end, the best alignment configuration for our baseline system was obtained with Giza++ software, running respectively 5, 5, 3 and 3 iterations of models 1, HMM, 3 and 4, using English stems and 50 classes and taking the union of source-target and target-source alignments. Table 3 show results for the new features of this year’s system. We optimized the foll"
2008.amta-papers.6,J96-1002,0,0.0194792,"volved from the initial word-based translation models to more advanced models that take the context surrounding the words into account, i.e. the so-called phrase-based system (Koehn et al., 2003). The phrase-based model is usually the main feature in a log-linear framework, reminiscent of the maximum entropy modeling approach. One of the best known reordering approach is permitting arbitrary word-reorderings. However, the exact decoding problem was shown to be NPhard (Knight, 1999). To solve this problem, several approaches have defined different kinds of constraints as for example heuristic (Berger et al., 1996) (Crego et al., 2005) or linguistic (Wu, 1996). Other approaches try to reorder the source language in a way that better matches the target language (Popovic and Ney, 2006) (Collins et al., 2005). 82 A natural evolution of the source reordering strategies consists in using a word graph, containing the N -best reordering decisions, instead of the single-best used in the above strategies. The reordering problem is equally approached by alleviating the difficulty of needing highly accurate reordering decisions in preprocessing. The final decision is delayed, to be subsequently in the global searc"
2008.amta-papers.6,P05-1066,0,0.201459,"Missing"
2008.amta-papers.6,W06-1609,1,0.929925,"Missing"
2008.amta-papers.6,2005.iwslt-1.23,1,0.903582,"Missing"
2008.amta-papers.6,W05-0831,0,0.155057,"005). 82 A natural evolution of the source reordering strategies consists in using a word graph, containing the N -best reordering decisions, instead of the single-best used in the above strategies. The reordering problem is equally approached by alleviating the difficulty of needing highly accurate reordering decisions in preprocessing. The final decision is delayed, to be subsequently in the global search, where all the information is then available. Inspired by (Knight and Al-Onaizan, 1998), they permute the source sentence to provide a source input graph that extends the search graph. In (Kanthak et al., 2005), they train the system using a monotonized source corpora and they translate the test set allowing source reorderings which are limited by constraints such as IBM or ITG. Similarly in (Crego and Mari˜no, 2007; Zhang et al., 2007), reordering is addressed through a source input graph. In this case, the reordering hypotheses are defined from a set of linguistically motivated rules (either using Part of Speech; chunks; or parse trees). Previous work (Costa-juss`a and Fonollosa, 2006) presents the SMR approach which is based on using the powerful SMT techniques to generate a reordered source inpu"
2008.amta-papers.6,knight-al-onaizan-1998-translation,0,0.0262969,"hes try to reorder the source language in a way that better matches the target language (Popovic and Ney, 2006) (Collins et al., 2005). 82 A natural evolution of the source reordering strategies consists in using a word graph, containing the N -best reordering decisions, instead of the single-best used in the above strategies. The reordering problem is equally approached by alleviating the difficulty of needing highly accurate reordering decisions in preprocessing. The final decision is delayed, to be subsequently in the global search, where all the information is then available. Inspired by (Knight and Al-Onaizan, 1998), they permute the source sentence to provide a source input graph that extends the search graph. In (Kanthak et al., 2005), they train the system using a monotonized source corpora and they translate the test set allowing source reorderings which are limited by constraints such as IBM or ITG. Similarly in (Crego and Mari˜no, 2007; Zhang et al., 2007), reordering is addressed through a source input graph. In this case, the reordering hypotheses are defined from a set of linguistically motivated rules (either using Part of Speech; chunks; or parse trees). Previous work (Costa-juss`a and Fonollo"
2008.amta-papers.6,J99-4005,0,0.0460704,"in the TC-STAR task (Es/En) at a relatively low computational cost. 1 Introduction Statistical machine translation (SMT) has evolved from the initial word-based translation models to more advanced models that take the context surrounding the words into account, i.e. the so-called phrase-based system (Koehn et al., 2003). The phrase-based model is usually the main feature in a log-linear framework, reminiscent of the maximum entropy modeling approach. One of the best known reordering approach is permitting arbitrary word-reorderings. However, the exact decoding problem was shown to be NPhard (Knight, 1999). To solve this problem, several approaches have defined different kinds of constraints as for example heuristic (Berger et al., 1996) (Crego et al., 2005) or linguistic (Wu, 1996). Other approaches try to reorder the source language in a way that better matches the target language (Popovic and Ney, 2006) (Collins et al., 2005). 82 A natural evolution of the source reordering strategies consists in using a word graph, containing the N -best reordering decisions, instead of the single-best used in the above strategies. The reordering problem is equally approached by alleviating the difficulty o"
2008.amta-papers.6,N03-1017,0,0.00898152,"erful techniques of the SMT systems to solve reordering problems. Here, the novelties yield in: (1) using the SMR approach in a SMT phrase-based system, (2) adding a feature function in the SMR step, and (3) analyzing the reordering hypotheses at several stages. Coherent improvements are reported in the TC-STAR task (Es/En) at a relatively low computational cost. 1 Introduction Statistical machine translation (SMT) has evolved from the initial word-based translation models to more advanced models that take the context surrounding the words into account, i.e. the so-called phrase-based system (Koehn et al., 2003). The phrase-based model is usually the main feature in a log-linear framework, reminiscent of the maximum entropy modeling approach. One of the best known reordering approach is permitting arbitrary word-reorderings. However, the exact decoding problem was shown to be NPhard (Knight, 1999). To solve this problem, several approaches have defined different kinds of constraints as for example heuristic (Berger et al., 1996) (Crego et al., 2005) or linguistic (Wu, 1996). Other approaches try to reorder the source language in a way that better matches the target language (Popovic and Ney, 2006) (C"
2008.amta-papers.6,popovic-ney-2006-pos,0,0.0136571,"stem (Koehn et al., 2003). The phrase-based model is usually the main feature in a log-linear framework, reminiscent of the maximum entropy modeling approach. One of the best known reordering approach is permitting arbitrary word-reorderings. However, the exact decoding problem was shown to be NPhard (Knight, 1999). To solve this problem, several approaches have defined different kinds of constraints as for example heuristic (Berger et al., 1996) (Crego et al., 2005) or linguistic (Wu, 1996). Other approaches try to reorder the source language in a way that better matches the target language (Popovic and Ney, 2006) (Collins et al., 2005). 82 A natural evolution of the source reordering strategies consists in using a word graph, containing the N -best reordering decisions, instead of the single-best used in the above strategies. The reordering problem is equally approached by alleviating the difficulty of needing highly accurate reordering decisions in preprocessing. The final decision is delayed, to be subsequently in the global search, where all the information is then available. Inspired by (Knight and Al-Onaizan, 1998), they permute the source sentence to provide a source input graph that extends the"
2008.amta-papers.6,W07-0721,1,0.785303,"Missing"
2008.amta-papers.6,P96-1021,0,0.0368004,"more advanced models that take the context surrounding the words into account, i.e. the so-called phrase-based system (Koehn et al., 2003). The phrase-based model is usually the main feature in a log-linear framework, reminiscent of the maximum entropy modeling approach. One of the best known reordering approach is permitting arbitrary word-reorderings. However, the exact decoding problem was shown to be NPhard (Knight, 1999). To solve this problem, several approaches have defined different kinds of constraints as for example heuristic (Berger et al., 1996) (Crego et al., 2005) or linguistic (Wu, 1996). Other approaches try to reorder the source language in a way that better matches the target language (Popovic and Ney, 2006) (Collins et al., 2005). 82 A natural evolution of the source reordering strategies consists in using a word graph, containing the N -best reordering decisions, instead of the single-best used in the above strategies. The reordering problem is equally approached by alleviating the difficulty of needing highly accurate reordering decisions in preprocessing. The final decision is delayed, to be subsequently in the global search, where all the information is then available"
2008.amta-papers.6,W07-0401,0,0.0165022,"approached by alleviating the difficulty of needing highly accurate reordering decisions in preprocessing. The final decision is delayed, to be subsequently in the global search, where all the information is then available. Inspired by (Knight and Al-Onaizan, 1998), they permute the source sentence to provide a source input graph that extends the search graph. In (Kanthak et al., 2005), they train the system using a monotonized source corpora and they translate the test set allowing source reorderings which are limited by constraints such as IBM or ITG. Similarly in (Crego and Mari˜no, 2007; Zhang et al., 2007), reordering is addressed through a source input graph. In this case, the reordering hypotheses are defined from a set of linguistically motivated rules (either using Part of Speech; chunks; or parse trees). Previous work (Costa-juss`a and Fonollosa, 2006) presents the SMR approach which is based on using the powerful SMT techniques to generate a reordered source input for an Ngram-based SMT system both in training and decoding steps. One step further, (R. Costa-juss`a and R. Fonollosa, 2007) shows how the SMR system generates a weighted reordering graph, allowing the SMT decoder to make the r"
2008.iwslt-evaluation.17,J03-1002,0,0.00397375,"I1 maximizing a loglinear combination of several feature models [4]: - 116 - ( eˆI1 = arg max eI1 M X ) λm hm (eI1 , f1J ) m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. Proceedings of IWSLT 2008, Hawaii - U.S.A. The N gram-based approach regards translation as a stochastic process maximizing the joint probability p(f, e), leading to a decomposition based on bilingual n-grams, socalled tuples, that are extracted from a word-to-word alignment (performed with GIZA++ tool1 and generated by growdiag-final method [5]). Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [6]: languages capturing bilingual context, as described by the following equation: p(S, T ) = K Y p((˜ s, t˜)k |(˜ s, t˜)k−N +1 , ..., (˜ s, t˜)k−1 ) (1) k=1 where s refers to source, t to target, and (˜ s, t˜)k to the k tuple of a given bilingual sentence pair segmented in K tuples. The bilingual TM actually constitutes an n-gram-based language model (LM) of tuples, which approximates the joint probability between the languages under consideration and can be seen here as a LM, where t"
2008.iwslt-evaluation.17,N04-1033,0,0.0316825,"I1 , f1J ) m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. Proceedings of IWSLT 2008, Hawaii - U.S.A. The N gram-based approach regards translation as a stochastic process maximizing the joint probability p(f, e), leading to a decomposition based on bilingual n-grams, socalled tuples, that are extracted from a word-to-word alignment (performed with GIZA++ tool1 and generated by growdiag-final method [5]). Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [6]: languages capturing bilingual context, as described by the following equation: p(S, T ) = K Y p((˜ s, t˜)k |(˜ s, t˜)k−N +1 , ..., (˜ s, t˜)k−1 ) (1) k=1 where s refers to source, t to target, and (˜ s, t˜)k to the k tuple of a given bilingual sentence pair segmented in K tuples. The bilingual TM actually constitutes an n-gram-based language model (LM) of tuples, which approximates the joint probability between the languages under consideration and can be seen here as a LM, where the language is composed by tuples. th • a monotonic segmentation of each bilingual sentence pair is produced • n"
2008.iwslt-evaluation.17,W06-1609,1,0.710642,"2008, Hawaii - U.S.A. 2.5. Statistical Machine Reordering 3.1. Punctuation restoration The conception of the Statistical Machine Reordering (SMR) stems from the idea of using the powerful techniques developed for SMT and to translate the source language (S) into a reordered source language (S’), which more closely matches the order of the target language. To infer more reorderings, it makes use of word classes. To correctly integrate the SMT and SMR systems, both are concatenated by using a word graph which offers weighted reordering hypotheses to the SMT system. The details are described in [8] and [9]. We decided to embed punctuation restoration in the main translation step. For this purpose we preprocessed the training corpus as follows: 2.6. Translation models interpolation The resulting preprocessed training corpus is used to train a standard SMT system (wi stands for the i-th word). During the post-evaluation period we have implemented a TM interpolation strategy following the ideas proposed in [3], where the authors present a promising technique of target LMs linear interpolation. These findings open the way to involve additional monolingual information into the translation pr"
2008.iwslt-evaluation.17,W07-0721,1,0.843062,"awaii - U.S.A. 2.5. Statistical Machine Reordering 3.1. Punctuation restoration The conception of the Statistical Machine Reordering (SMR) stems from the idea of using the powerful techniques developed for SMT and to translate the source language (S) into a reordered source language (S’), which more closely matches the order of the target language. To infer more reorderings, it makes use of word classes. To correctly integrate the SMT and SMR systems, both are concatenated by using a word graph which offers weighted reordering hypotheses to the SMT system. The details are described in [8] and [9]. We decided to embed punctuation restoration in the main translation step. For this purpose we preprocessed the training corpus as follows: 2.6. Translation models interpolation The resulting preprocessed training corpus is used to train a standard SMT system (wi stands for the i-th word). During the post-evaluation period we have implemented a TM interpolation strategy following the ideas proposed in [3], where the authors present a promising technique of target LMs linear interpolation. These findings open the way to involve additional monolingual information into the translation process, a"
2008.iwslt-evaluation.17,2007.mtsummit-papers.29,0,0.0263997,"iod we have implemented a TM interpolation strategy following the ideas proposed in [3], where the authors present a promising technique of target LMs linear interpolation. These findings open the way to involve additional monolingual information into the translation process, and also gives a motivation to interpolate the translation and reordering tables in a linear way. Due to a small amount of available in-domain data (IWSLT training material), we have used an out-of-domain 130K-line subset from the Arabic News, English Translation of Arabic Treebank and Ummah LDC parallel corpora (VIOLIN) [10] to increase the final translation and reordering tables. Both corpus statistics can be found in table 1. Instead of time-consuming iterative TM reconstruction and using the highest BLEU score as an maximization criteria, we adjust the weights as a function of the lowest perplexity estimated by the corresponding interpolated combination of the target-side LMs and generalize the optimization results on the interpolated translation and reordering models. The word-to-word alignment was obtained from the joint database (IWSLT + VIOLIN). Then, we separately computed the translation and reordering t"
2008.iwslt-evaluation.17,P07-2045,0,0.0159161,"ssion Corpus (BTEC) Arabic to English translation task. The model weights were tuned with the 2006 development corpus (Dev6), containing 489 sentences and 6 reference translations and the 2002 development set (500 sentences and 16 reference translations) was used as an internal test, according to which we take a decision about better or worse system performance. 4.1.1. Arabic data preprocessing In this section we present a phrase-based MT system that was used in the evaluation. This system is based on the well-known MOSES2 toolkit, which is nowadays considered as a state-of-the-art SMT system [11]. The training and weights tuning procedures are explained in details in the above-mentioned publication, as well as, on the MOSES web page: http://www.statmt.org/moses/. 2 www.statmt.org/moses/ We used a similar approach to that shown in [12], namely the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of enclitics: w+, f+, b+, k+, l+, Al+ and pronominal enclitics. The -TAGBIES option produces Bies POS tags on all taggable t"
2008.iwslt-evaluation.17,N06-2013,0,0.0588409,"anslations) was used as an internal test, according to which we take a decision about better or worse system performance. 4.1.1. Arabic data preprocessing In this section we present a phrase-based MT system that was used in the evaluation. This system is based on the well-known MOSES2 toolkit, which is nowadays considered as a state-of-the-art SMT system [11]. The training and weights tuning procedures are explained in details in the above-mentioned publication, as well as, on the MOSES web page: http://www.statmt.org/moses/. 2 www.statmt.org/moses/ We used a similar approach to that shown in [12], namely the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of enclitics: w+, f+, b+, k+, l+, Al+ and pronominal enclitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 3 http://www.slc.atr.jp/IWSLT2008/ - 118 - Proceedings of IWSLT 2008, Hawaii - U.S.A. Sentences Words Average sentence length Vocabulary IWSLT Arabic 24.45 K 170.24 K 6.96 10.89 K English 24.45 K 188.54 K 7.71 6.92 K VIOLIN Arabic 130.5"
2008.iwslt-evaluation.17,2005.iwslt-1.8,0,0.0320169,"n”) outperforms BTEC-only system by 1.8 BLEU points and 1.2 METEOR points for the CRR track and by 2.1 BLEU points and by about 1 METEOR points for the ASR track measured on the official evaluation test set. ”Supplied 2” line stands for the results obtained with the TALPtuples system as described in sub-section 4.1.3. - 119 - • TM(s), direct and inverse phrase/word based TM. • Distortion model, which assigns a cost linear to the reordering distance, while the cost is based on the number of source words which are skipped when translating a new source phrase. • Lexicalized word reordering model [15]. • Word and phrase penalties, which count the number of words and phrases in the target string. • Target-side LM. The TM and reordering model were trained using the standard MOSES tools. Weights of feature functions were tuned by using the optimization tools from the MOSES package. The search operation was accomplished by MOSES decoder. The experiments with the Chinese to English MT were carried out on the BTEC Chinese-English data [16] augmented with HIT-corpus4 , Olympic-corpus5 and PKUcorpus6 from Chinese LDC. 20K BTEC sentence pairs were supplied for the IWSLT 2008 evaluation campaign. HI"
2008.iwslt-evaluation.17,takezawa-etal-2002-toward,0,0.0149266,"ng distance, while the cost is based on the number of source words which are skipped when translating a new source phrase. • Lexicalized word reordering model [15]. • Word and phrase penalties, which count the number of words and phrases in the target string. • Target-side LM. The TM and reordering model were trained using the standard MOSES tools. Weights of feature functions were tuned by using the optimization tools from the MOSES package. The search operation was accomplished by MOSES decoder. The experiments with the Chinese to English MT were carried out on the BTEC Chinese-English data [16] augmented with HIT-corpus4 , Olympic-corpus5 and PKUcorpus6 from Chinese LDC. 20K BTEC sentence pairs were supplied for the IWSLT 2008 evaluation campaign. HIT corpus contains 132K sentence pairs in total, and is known as a multi-source ChineseEnglish parallel corpus; Olympic corpus has 54K bilingual sentences mainly from sport and travelling domains; while PKU-corpus has about 200K parallel phrases and is considered as a domain-balanced corpus. Besides, the English part of the Tanaka corpus7 was used as a complementary training 4 http://mitlab.hit.edu.cn/index.php/resources 5 http://www.chin"
2008.iwslt-evaluation.17,W03-1730,0,0.0212439,"OR)/2 0.6016 0.6055 0.6210 0.5892 0.5320 0.5320 0.5473 0.5296 NIST 8.5253 8.5940 8.8772 8.7421 7.2878 7.2808 7.6113 7.5862 Table 2: Official and post-evaluation results for Arabic-English translation. Sentences Words Vocabulary Chinese 19,972 164K 8,506 IWSLT’08 English Spanish 19,972 19,972 182K 147K 8,301 16,953 All additional data Chinese English 379,065 379,065 4,834K 5,036K 57,055 75,156 Table 3: Corpus used during the Chinese-English training material for the target-side LM. The I2R research group performed word segmentation for the Chinese part using ICTCLAS tools8 developed in the ICT [17]. Table 3 reports the basic statistics of the principal and additional corpora that were used to build the Chinese-toEnglish SMT system. Regarding English-to-Spanish translation, no extra corpora were used. “you ’re”, and negations like “don’t”, “wouldn’t” or “can’t” were split as “do n’t”, “would n’t” and “ca n’t”. The output of this system was performed in accordance with the official evaluation specification, without any postprocessing needed. Table 5 shows the results of the EnglishSpanish system trained with the BTEC corpus. 4.2.1. Chinese-English independent results The union of the BTEC"
2008.iwslt-evaluation.17,N04-1022,0,0.0541414,"and “you’re” were split as “we ’ll” and 8 http://www.nlp.org.cn/project/project.php?proj BLEU NIST METEOR id=6 - 120 - Our primary approach to the pivot task was a system cascade. Using the 50-best list of translation hypotheses generated by the decoder for the Chinese-to-English system, a 4-best list was made for each of the first list instances, totally representing a 200-best of possible Spanish translations for each Chinese sentence. From that 200-best list, which is allowed for repetitions, the single-best translation was computed using a Minimum Bayes Risk (MBR) strategy as described in [18]. We used the MOSES implementation of the MBR algorithm. This strategy of 200-best list rescoring performed better than a single-best list selection for both systems, gaining 2.5 BLEU points in the development set. Proceedings of IWSLT 2008, Hawaii - U.S.A. 4.2.4. Secondary submission As an alternative approach to the system cascade, we followed a different strategy for the secondary submission combining the phrase translation probabilities of the two language pairs (Chinese-English and English-Spanish translations) with the strategy proposed in [19] to obtain the translation probabilities for"
2008.iwslt-evaluation.17,P07-1108,0,0.0348074,"m Bayes Risk (MBR) strategy as described in [18]. We used the MOSES implementation of the MBR algorithm. This strategy of 200-best list rescoring performed better than a single-best list selection for both systems, gaining 2.5 BLEU points in the development set. Proceedings of IWSLT 2008, Hawaii - U.S.A. 4.2.4. Secondary submission As an alternative approach to the system cascade, we followed a different strategy for the secondary submission combining the phrase translation probabilities of the two language pairs (Chinese-English and English-Spanish translations) with the strategy proposed in [19] to obtain the translation probabilities for each Chinese-Spanish phrase. The final phrase probabilities are calculated as followed: φ(fi |ei ) = X φ(fi |pi )φ(pi |ei ) (2) pi where φ(fi |ei ) corresponds to the translation probability of the Chinese phrase fi given the Spanish phrase ei , φ(fi |pi ) stands for the translation probability of the Chinese phrase fi given the English phrase pi and φ(pi |ei ) stands for the translation probability of the English phrase pi given the Spanish phrase ei . It is important to mention that the English and Spanish phrases are lowercased in this system and"
2008.iwslt-evaluation.17,carreras-etal-2004-freeling,0,0.029059,"6 sentences with 16 Spanish references for tuning the system. The basic statistics of this corpus can be seen in table 7. 4.3.1. Data preprocessing The Chinese corpus was not preprocessed before translation: the corpus was tokenized by words and the punctuation marks were separated. Note that the TM, as well as the LM and reordering model, was trained with punctuation marks and the official test set that did not contain this information, therefore it was preprocessed with the hidden-ngram tool to restore it. The Spanish part of the corpus was lowercased and tokenized using the Freeling toolkit[20], an open source tool for language analysis. It splitted the enclitics from the Spanish verbs (d´amelo → da +me +lo) and also generated the POS tags that were lately used to estimate a target-side POS LM and in postprocessing. 4.3.2. Data postprocessing Once the decoding process had finished, the output of the system was still lowercased and splitted with the enclitics and the POS tags were generated. Afterwards, a postprocess including two steps was performed: firstly, the original morphological verbs form was restored using the enclitics and POS tags information; on the next step, the case i"
2008.iwslt-evaluation.17,2007.iwslt-1.1,0,\N,Missing
2008.iwslt-evaluation.17,2006.iwslt-evaluation.1,0,\N,Missing
2009.eamt-1.27,W08-0406,0,0.0142705,"lternative to the phrase-based state-of-the-art Moses1 system. 2 Related work In practice, a reordering model operates on a sentence level and is carried out based on word reordering rules derived from the training corpus. Reordering patterns can be purely statistical (see Costa-jussà and Fonollosa (2006), for example), use language-based syntactic information (Collins et al., 2005); the reordering can be driven by a lat1 www.statmt.org/moses/ Proceedings of the 13th Annual Conference of the EAMT, pages 197–204, Barcelona, May 2009 197 tice of syntactically motivated alternative translations (Elming, 2008) or be based on automatically extracted patterns driven by syntactical structure of the languages (see Crego and Mariño (2007b) as an example). Another recent implementation of the preprocessing approach to syntax-based reordering though an n-best list generation can be found in Li et al. (2007). Word class-based reordering patterns were part of Och’s Alignment Template system (Och et al., 2004). The modern state-of-the-art phrase-based translation system Moses, along with a distance based distortion model (Koehn et al., 2003), implements the phrase-based reordering (Tillmann and Zhang, 2005)."
2009.eamt-1.27,P05-1066,0,0.140942,"Missing"
2009.eamt-1.27,W06-1609,1,0.941672,"t from the reordering rules representing the order of child nodes, a set of additional rewrite rules based on a deep topdown subtree analysis is considered, which is another novel aspect of the paper. We used the N -gram-based SMT system of Mariño et al. (2006) to test the proposed syntaxbased reordering model, which is an alternative to the phrase-based state-of-the-art Moses1 system. 2 Related work In practice, a reordering model operates on a sentence level and is carried out based on word reordering rules derived from the training corpus. Reordering patterns can be purely statistical (see Costa-jussà and Fonollosa (2006), for example), use language-based syntactic information (Collins et al., 2005); the reordering can be driven by a lat1 www.statmt.org/moses/ Proceedings of the 13th Annual Conference of the EAMT, pages 197–204, Barcelona, May 2009 197 tice of syntactically motivated alternative translations (Elming, 2008) or be based on automatically extracted patterns driven by syntactical structure of the languages (see Crego and Mariño (2007b) as an example). Another recent implementation of the preprocessing approach to syntax-based reordering though an n-best list generation can be found in Li et al. (20"
2009.eamt-1.27,2006.iwslt-evaluation.18,1,0.868743,"Missing"
2009.eamt-1.27,2007.mtsummit-papers.16,0,0.0319675,"es on a sentence level and is carried out based on word reordering rules derived from the training corpus. Reordering patterns can be purely statistical (see Costa-jussà and Fonollosa (2006), for example), use language-based syntactic information (Collins et al., 2005); the reordering can be driven by a lat1 www.statmt.org/moses/ Proceedings of the 13th Annual Conference of the EAMT, pages 197–204, Barcelona, May 2009 197 tice of syntactically motivated alternative translations (Elming, 2008) or be based on automatically extracted patterns driven by syntactical structure of the languages (see Crego and Mariño (2007b) as an example). Another recent implementation of the preprocessing approach to syntax-based reordering though an n-best list generation can be found in Li et al. (2007). Word class-based reordering patterns were part of Och’s Alignment Template system (Och et al., 2004). The modern state-of-the-art phrase-based translation system Moses, along with a distance based distortion model (Koehn et al., 2003), implements the phrase-based reordering (Tillmann and Zhang, 2005). Reordering algorithms specifically developed for an N -gram system include a constrained distance-based distortion model (Co"
2009.eamt-1.27,2005.iwslt-1.23,1,0.909742,"Missing"
2009.eamt-1.27,2005.mtsummit-papers.35,0,0.195464,"system performance. In experiments, we show significant improvement for the Chinese-to-English translation task. 1 Introduction One of the most challenging problems facing machine translation (MT) is how to place the translated words in the natural order of the target language. A monotone SMT system suffers from weakness in the distortion model, even if it is able to generate correct word-by-word translation. In this study we propose a reordering model that involves both source- and target-side syntax information in the word reordering process. Our work is inspired by the approach proposed in Imamura et al. (2005), where a complete syntaxdriven SMT system based on a two-side subtree transfer is described. In their approach they construct a probabilistic non-isomorphic tree mapping model based on a context-free breakdown of the source and target parse trees; extract alignment templates that incorporate the constraints of the parse trees; and apply syntax-based decoding. We c 2009 European Association for Machine Translation. ° Mark Dras Macquarie University North Ryde NSW 2109, Sydney, Australia madras@ics.mq.edu.au propose to use a similar non-isomorphic subtree mapping to extract reordering rules, but"
2009.eamt-1.27,W08-0315,1,0.851424,"rds is translated into the reordered sequence using SMT techniques. In Xia and Mccord (2004) the authors present a hybrid system for French-English translation, based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. This method differs from the one presented in this paper, among other distinctions, by a lexical model underlying the subtree syntax transfer and a different statistical model used for translation. 3 Baseline SMT system N -gram-based SMT has proved to be competitive with the state-of-the-art systems in recent evaluation campaigns (Khalilov et al., 2008; Lambert et al., 2007). According to the N -gram-based approach, the translation process is considered as an arg max searching for the translation hypothesis eˆI1 maximizing a log-linear combination of a translation model (TM) and a set of feature models: ( M ) X λm hm (eI1 , f1J ) (1) eˆI1 = arg max eI1 m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. A detailed description of the N -gram-based approach can be found in Mariño et al. (2006). As decoder, we used MARIE2 (Crego et al., 2005), a beam-search decode"
2009.eamt-1.27,P03-1054,0,0.0379698,"t al. (2006). As decoder, we used MARIE2 (Crego et al., 2005), a beam-search decoder implementing a distance-based constrained distortion model, limited by two parameters: m - a maximum distance measured number in words that a phrase can be reordered and j - a maximum number of ""jumps"" within a sentence (Costa-jussà et al., 2006). 4 Syntax-based reordering Our syntax-based reordering (SBR) system requires access to source and target language parse trees, along with the source-to-target and targetto-source word alignments intersection. In the framework of the study we used the Stanford Parser (Klein and Manning, 2003) for both languages, however the system permits using any other natural language parser allowing for different formal grammars for the source and the target languages. 4.1 Notation SBR operates with source and target parse trees that represent the syntactic structure of a string in source and target languages in a Context-Free Grammar (CFG) fashion. This representation is called ""CFG form"", and is formally defined in the usual way as G = hN, T, R, Si, where N is a set of nonterminal symbols (corresponding to source-side phrase and partof-speech tags); T is a set of source-side terminals (the l"
2009.eamt-1.27,N03-1017,0,0.00437198,"a, May 2009 197 tice of syntactically motivated alternative translations (Elming, 2008) or be based on automatically extracted patterns driven by syntactical structure of the languages (see Crego and Mariño (2007b) as an example). Another recent implementation of the preprocessing approach to syntax-based reordering though an n-best list generation can be found in Li et al. (2007). Word class-based reordering patterns were part of Och’s Alignment Template system (Och et al., 2004). The modern state-of-the-art phrase-based translation system Moses, along with a distance based distortion model (Koehn et al., 2003), implements the phrase-based reordering (Tillmann and Zhang, 2005). Reordering algorithms specifically developed for an N -gram system include a constrained distance-based distortion model (Costa-jussà et al., 2006) and a linguistically motivated reordering model employing monotonic search graph extension (Crego and Mariño, 2007a). An example of a word order monotonization strategy can be found in Costa-jussà and Fonollosa (2006), where a monotone sequence of source words is translated into the reordered sequence using SMT techniques. In Xia and Mccord (2004) the authors present a hybrid syst"
2009.eamt-1.27,W04-3250,0,0.10282,"Missing"
2009.eamt-1.27,J06-4004,1,0.897523,"Missing"
2009.eamt-1.27,J03-1002,0,0.00299387,"on|p1 (2) where ηi ∈ N for all 0 ≤ i ≤ k; (do . . . dk ) is a permutation of (0 . . . k); Lexicon includes the source-side set of words for each ηi ; and p1 is a probability associated with the rule. Figure 1 gives two examples of the rule format. 4.2 Rule extraction Concept. Inspired by the ideas presented in Imamura et al. (2005), where monolingual correspon2 198 http://gps-tsc.upc.es/veu/soft/soft/marie/ dences of syntactic nodes are used during decoding, we extract a set of bilingual patterns allowing for reordering as described below: (1) align the monotone bilingual corpus with GIZA++3 (Och and Ney, 2003) and find the intersection of direct and inverse word alignments, resulting in the construction of the projection matrix P (see below); (2) parse the source and the target parts of the parallel corpus; (3) extract reordering patterns from the parallel non-isomorphic CFG-trees based on the word alignment intersection. Step 2 is straightforward; we explain aspects of Steps 1 and 3 in more detail below. Figure 1 shows an example of the generation of two lexicalized rules; we use this below in our explanations. Given two parse trees and a word alignment intersection, a projection matrix P is defin"
2009.eamt-1.27,N04-1021,0,0.0321988,"rdering can be driven by a lat1 www.statmt.org/moses/ Proceedings of the 13th Annual Conference of the EAMT, pages 197–204, Barcelona, May 2009 197 tice of syntactically motivated alternative translations (Elming, 2008) or be based on automatically extracted patterns driven by syntactical structure of the languages (see Crego and Mariño (2007b) as an example). Another recent implementation of the preprocessing approach to syntax-based reordering though an n-best list generation can be found in Li et al. (2007). Word class-based reordering patterns were part of Och’s Alignment Template system (Och et al., 2004). The modern state-of-the-art phrase-based translation system Moses, along with a distance based distortion model (Koehn et al., 2003), implements the phrase-based reordering (Tillmann and Zhang, 2005). Reordering algorithms specifically developed for an N -gram system include a constrained distance-based distortion model (Costa-jussà et al., 2006) and a linguistically motivated reordering model employing monotonic search graph extension (Crego and Mariño, 2007a). An example of a word order monotonization strategy can be found in Costa-jussà and Fonollosa (2006), where a monotone sequence of s"
2009.eamt-1.27,P05-1069,0,0.017168,"ranslations (Elming, 2008) or be based on automatically extracted patterns driven by syntactical structure of the languages (see Crego and Mariño (2007b) as an example). Another recent implementation of the preprocessing approach to syntax-based reordering though an n-best list generation can be found in Li et al. (2007). Word class-based reordering patterns were part of Och’s Alignment Template system (Och et al., 2004). The modern state-of-the-art phrase-based translation system Moses, along with a distance based distortion model (Koehn et al., 2003), implements the phrase-based reordering (Tillmann and Zhang, 2005). Reordering algorithms specifically developed for an N -gram system include a constrained distance-based distortion model (Costa-jussà et al., 2006) and a linguistically motivated reordering model employing monotonic search graph extension (Crego and Mariño, 2007a). An example of a word order monotonization strategy can be found in Costa-jussà and Fonollosa (2006), where a monotone sequence of source words is translated into the reordered sequence using SMT techniques. In Xia and Mccord (2004) the authors present a hybrid system for French-English translation, based on the principle of automa"
2009.eamt-1.27,C04-1073,0,0.198444,"Missing"
2009.eamt-1.27,P01-1067,0,0.117426,", so the rule output functioning as an input to the next rule can lead to situations reverting the change of word order that the previously applied rule made. Therefore, the rules that can be ambiguous when applied sequentially are pruned according to the higher probability principle. For example, for the pair of patterns with the same lexicon (which is empty for a general rule leading to a recurring contradiction NP@0 VP@1 → VP@1 NP@0 p1, VP@0 NP@1 → NP@1 VP@0 p2 ), the less probable rule is removed. Finally, there are three resulting parameter tables analogous to the ""r-table"" as stated in (Yamada and Knight, 2001), consisting of POS- and constituent-based patterns allowing for reordering and monotone distortion. 4.5 Source-side monotonization Rule application is performed as a bottom-up parse tree traversal following two principles: (1) the longest possible rule is applied, i.e. among a set of nested rules, the rule with a longest left-side covering is selected. For example, in the case of the appearance of an NN JJ RB sequence and presence of the two reordering rules NN@0 JJ@1 → ... and NN@0 JJ@1 RB@2 → ... the latter pattern will be applied. (2) the rule containing the maximum lexical information is"
2009.eamt-1.27,N04-1035,0,\N,Missing
2009.eamt-1.27,2006.iwslt-evaluation.17,1,\N,Missing
2009.eamt-1.27,C08-1027,0,\N,Missing
2009.eamt-1.27,P02-1040,0,\N,Missing
2009.eamt-1.27,P04-1083,0,\N,Missing
2009.eamt-1.27,W05-0909,0,\N,Missing
2009.eamt-1.27,P07-1091,0,\N,Missing
2009.eamt-1.27,P05-1033,0,\N,Missing
2009.eamt-1.27,D07-1079,0,\N,Missing
2009.eamt-1.27,W06-3119,0,\N,Missing
2010.eamt-1.12,popovic-ney-2006-pos,0,0.0248284,"Missing"
2010.eamt-1.12,W06-3101,1,0.93751,"Missing"
2010.eamt-1.12,2009.eamt-1.8,1,0.756731,"Missing"
2010.eamt-1.12,C00-2162,0,0.0200814,"mpute another probability to the translation units based on the probability of translating word per word of the unit. The probability estimated by lexical models tends to be in some situations less sparse than the probability given directly by the translation model. Many additional feature functions can also be introduced in the SMT framework to improve the translation, like the word or the phrase bonus. Although SMT systems provide, in general, good performance, it has been demonstrated in recent papers that the addition of linguistic information can be highly useful in this kind of systems (Niessen and Ney, 2000; Popovi´c and Ney, 2004; Popovi´c and Ney, 2006; Popovi´c et al., 2006). automatic translation meets their standards. Large amounts of bilingual texts are needed to further develop new systems. N-II3 , developed at the UPC mainly for the Spanish-Catalan pair, is an engine based on an Ngram translation model integrated in an optimized log-linear combination of additional features. Although it is mainly statistical, additional linguistic rules are included in order to solve some errors caused by the statistical translation, such as ambiguity in adjective and possessive pronouns, orthographic er"
2010.eamt-1.12,P03-1021,0,0.0146839,"the overlap in phrases. Thus, given a source string sJ1 = s1 . . . sj . . . sJ to be translated into a target string tI1 = t1 . . . ti . . . tI , the aim is to choose, among all possible target strings, the string with the highest probability: t˜I1 = argmax P (tI1 |sJ1 ) tI1 where I and J are the number of words of the target and source sentence, respectively. The first SMT systems were reformulated using Bayes’ rule. In recent systems, such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och, 2003). This approach leads to maximising a linear combination of feature functions: t˜ = argmax t nP M o m=1 λm hm (t, s) . Given a target sentence and a foreign sentence, the translation model tries to assign a probability that tI1 generates sJ1 . While these probabilities can be estimated by thinking about how each individual word is translated, modern statistical MT is based on the intuition that a better way to compute these probabilities is by considering the behavior of phrases (sequences of words). The intuition of phrase-based statistical MT is to use phrases as well as single words as the"
2010.eamt-1.12,P02-1040,0,0.0928963,"human evaluation based on the expert knowledge about the errors encountered at several linguistic levels: orthographic, morphological, lexical, semantic and syntactic. The results obtained in these experiments show that some linguistic errors could have more influence than other at the time of performing a perceptual evaluation. 1 Introduction One of the aims in the research community is to find accurate evaluation methods that allow analyzing and comparing the performance of these translation systems. The most commonly used evaluation methods are the standard automatic measures such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover and Dorr, 2006) and WER (McCowan, 2004 et al.), as well as the use of human native evaluators that analyze and compare translated sentences according to a general perception of the linguistic quality. In this paper, these evaluation methods are used to evaluate and compare two translation systems based on the statistical approaches in the Catalanto-Spanish language pair: Google Translate and Nc 2010 European Association for Machine Translation. II; this one developed at the Universitat Polit`ecnica de Catalunya (UPC). In addition, a new human evaluation m"
2010.eamt-1.12,popovic-ney-2004-towards,0,0.0153117,"Missing"
2010.eamt-1.12,2006.amta-papers.25,0,0.0482924,"he errors encountered at several linguistic levels: orthographic, morphological, lexical, semantic and syntactic. The results obtained in these experiments show that some linguistic errors could have more influence than other at the time of performing a perceptual evaluation. 1 Introduction One of the aims in the research community is to find accurate evaluation methods that allow analyzing and comparing the performance of these translation systems. The most commonly used evaluation methods are the standard automatic measures such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover and Dorr, 2006) and WER (McCowan, 2004 et al.), as well as the use of human native evaluators that analyze and compare translated sentences according to a general perception of the linguistic quality. In this paper, these evaluation methods are used to evaluate and compare two translation systems based on the statistical approaches in the Catalanto-Spanish language pair: Google Translate and Nc 2010 European Association for Machine Translation. II; this one developed at the Universitat Polit`ecnica de Catalunya (UPC). In addition, a new human evaluation method is applied, based on an expert linguistic evalua"
2010.eamt-1.12,vilar-etal-2006-error,0,0.775854,"Missing"
2010.eamt-1.12,W09-0401,0,\N,Missing
2010.eamt-1.12,J06-4004,1,\N,Missing
2020.acl-srw.10,D14-1179,0,0.0492172,"Missing"
2020.acl-srw.10,P16-2058,1,0.884703,"Missing"
2020.acl-srw.10,N18-1032,0,0.0573029,"to wordlevel information in a task-agnostic way. While the approaches by Bojanowski et al. (2017), Zhao et al. (2018) and Li et al. (2018) aim at computing pre-trained word representations, other proposals integrate the computation of the word representation in the overall NMT model, either combining information from character level, like those by Luong and Manning (2016) Costa-juss`a and Fonollosa (2016), from n-gram level, like the one by Ataman and Federico (2018), or from multiple granularities like the work by Chen et al. (2018). Some other approaches like those by Wang et al. (2019) and Gu et al. (2018b) try to extend this idea to obtain multilingual conceptual representations from character-level representations. Nevertheless, in all those approaches, the decoder only has access to the aggregated wordlevel information and not to the original subwordlevel information. This, while mitigating the unknown word problem, cannot handle the scenario where copying from source to target is necessary, like with unseen proper names or with compositional structures like numbers. To the best of our knowledge, this type of neural architectures that condense subword/character-level information into word-l"
2020.acl-srw.10,P82-1020,0,0.653665,"Missing"
2020.acl-srw.10,W18-1205,0,0.0182072,"er is then used at every layer of the decoder as key and value of the multi-head attention. In these operations, the token representations in the sequences in the source batch are masked according to the original sequence lengths in tokens. Related Work The main difficulty in profiting from word-level information in subword-based NMT architectures is the word-subword token level mismatch. Several lines of research have studied how to combine subword-level representations into wordlevel information in a task-agnostic way. While the approaches by Bojanowski et al. (2017), Zhao et al. (2018) and Li et al. (2018) aim at computing pre-trained word representations, other proposals integrate the computation of the word representation in the overall NMT model, either combining information from character level, like those by Luong and Manning (2016) Costa-juss`a and Fonollosa (2016), from n-gram level, like the one by Ataman and Federico (2018), or from multiple granularities like the work by Chen et al. (2018). Some other approaches like those by Wang et al. (2019) and Gu et al. (2018b) try to extend this idea to obtain multilingual conceptual representations from character-level representations. Neverthe"
2020.acl-srw.10,P16-1100,0,0.0551171,"Missing"
2020.acl-srw.10,P18-2049,0,0.0173807,"rd-based NMT architectures is the word-subword token level mismatch. Several lines of research have studied how to combine subword-level representations into wordlevel information in a task-agnostic way. While the approaches by Bojanowski et al. (2017), Zhao et al. (2018) and Li et al. (2018) aim at computing pre-trained word representations, other proposals integrate the computation of the word representation in the overall NMT model, either combining information from character level, like those by Luong and Manning (2016) Costa-juss`a and Fonollosa (2016), from n-gram level, like the one by Ataman and Federico (2018), or from multiple granularities like the work by Chen et al. (2018). Some other approaches like those by Wang et al. (2019) and Gu et al. (2018b) try to extend this idea to obtain multilingual conceptual representations from character-level representations. Nevertheless, in all those approaches, the decoder only has access to the aggregated wordlevel information and not to the original subwordlevel information. This, while mitigating the unknown word problem, cannot handle the scenario where copying from source to target is necessary, like with unseen proper names or with compositional struct"
2020.acl-srw.10,P14-5010,0,0.00326134,"l information. Table 1: BLEU scores on IWSLT14 German-English for different subword combination strategies. Nsw 3 3 5 en-de 28.75 28.29 Base Transformer Word-level info copied to subwords Word-subword model + word-level info BLEU 33.53 34.02 34.46 en-ro 27.02 27.29 27.82 Table 4: BLEU scores measured on the WMT16 English-Romanian data, with lemmas as linguistic info. The word-level linguistic information used was only the lemma (using a vocabulary of 40k lemmas), which is the feature that should provide the largest improvement according to Sennrich and Haddow (2016). We used Stanford CoreNLP (Manning et al., 2014) to annotate the corpus with the English lemmas. The obtained results are shown in Table 4, where our proposed approach obtains the best BLEU score compared to the base Transformer model (Vaswani et al., 2017) without any wordlevel information, and to copying the word-level info to subwords (Sennrich and Haddow, 2016). Table 2: BLEU scores on the IWSLT14 German(e) (d) English test set for different values of Nsw and Nsw , using GRU as subword combination strategy. Once determined that using GRU as subword (e) (d) combination and setting Nsw = 5 and Nsw = 3 is the hyperparameter configuration t"
2020.acl-srw.10,Q17-1010,0,0.0471234,"e input token embeddings. The output of the encoder is then used at every layer of the decoder as key and value of the multi-head attention. In these operations, the token representations in the sequences in the source batch are masked according to the original sequence lengths in tokens. Related Work The main difficulty in profiting from word-level information in subword-based NMT architectures is the word-subword token level mismatch. Several lines of research have studied how to combine subword-level representations into wordlevel information in a task-agnostic way. While the approaches by Bojanowski et al. (2017), Zhao et al. (2018) and Li et al. (2018) aim at computing pre-trained word representations, other proposals integrate the computation of the word representation in the overall NMT model, either combining information from character level, like those by Luong and Manning (2016) Costa-juss`a and Fonollosa (2016), from n-gram level, like the one by Ataman and Federico (2018), or from multiple granularities like the work by Chen et al. (2018). Some other approaches like those by Wang et al. (2019) and Gu et al. (2018b) try to extend this idea to obtain multilingual conceptual representations from"
2020.acl-srw.10,N19-4009,0,0.0216232,"ality with different hyperparameter sets in order to understand their effect on the model. In order to study the effectiveness of the proposed model with other approaches to incorporate word-level information into a subword-based model, we used the WMT16 English-Romanian data with the back-translated synthetic data from (Sennrich et al., 2016a), using a shared subword vocabulary of 40k merge operations. We used the proposal by (Sennrich and Haddow, 2016) as baseline, and compared it to a vanilla Transformer baseline and to our proposed method. For all experiments, we used the fairseq library (Ott et al., 2019), either with its built-in models for the baselines or with custom model implementations for the approach by Sennrich and Haddow (2016) and for our own proposed architecture. For the IWSLT14 de-en and en-de baselines we used the Transformer architecture (Vaswani et al., 2017) with the hyperparameters proposed by the fairseq authors1 , namely 6 layers in encoder and decoder, 4 attention heads, embedding size of 512 and 1024 for the feedforward expansion size, together with dropout of 0.3 and a total batch size of 4000 tokens, using label smoothing of 0.1. For the WMT16 en-ro baseline we used th"
2020.acl-srw.10,W16-2209,0,0.162527,"ower and vocabulary size, especially statistically extracted subword vocabulary strategies like Byte Pair Encoding (BPE) (Sennrich et al., 2016b). Models with word-level token vocabularies can incorporate word-level information as extra input to the model by combining it one-to-one with the token representations. Some examples of word-level information are Part of Speech (POS) tags, syntactic dependency relationships or lemmas. In order to make use of word-level information in models with subword-level token vocabularies, a usual approach is to assign the word information to all its subwords (Sennrich and Haddow, 2016). This approach, despite improving the translation quality, introduces an information assignment mismatch. We propose to modify the Transformer architecture (Vaswani et al., 2017) to combine the learned subword representations into word representations in the encoder block. This allows to naturally incorporate any extra word-level information directly at the level of word-level representations. This work is structured as follows: the relevant related work is described in section 2; the proposed In Neural Machine Translation, using wordlevel tokens leads to degradation in translation quality. T"
2020.acl-srw.10,W16-2323,0,0.152549,"tokens, which is hardly enough to fit the number of symbols in a complete word-based vocabulary. Compositional word structures like numbers pose further problems with such a granularity level, as well as proper nouns. When word-based vocabularies are used, the vocabulary is built with the most frequent surface forms in the training data, which normally leads to degradation of translation quality. Subword-level token granularity offers a compromise between representational power and vocabulary size, especially statistically extracted subword vocabulary strategies like Byte Pair Encoding (BPE) (Sennrich et al., 2016b). Models with word-level token vocabularies can incorporate word-level information as extra input to the model by combining it one-to-one with the token representations. Some examples of word-level information are Part of Speech (POS) tags, syntactic dependency relationships or lemmas. In order to make use of word-level information in models with subword-level token vocabularies, a usual approach is to assign the word information to all its subwords (Sennrich and Haddow, 2016). This approach, despite improving the translation quality, introduces an information assignment mismatch. We propose"
2020.acl-srw.10,P16-1162,0,0.371237,"tokens, which is hardly enough to fit the number of symbols in a complete word-based vocabulary. Compositional word structures like numbers pose further problems with such a granularity level, as well as proper nouns. When word-based vocabularies are used, the vocabulary is built with the most frequent surface forms in the training data, which normally leads to degradation of translation quality. Subword-level token granularity offers a compromise between representational power and vocabulary size, especially statistically extracted subword vocabulary strategies like Byte Pair Encoding (BPE) (Sennrich et al., 2016b). Models with word-level token vocabularies can incorporate word-level information as extra input to the model by combining it one-to-one with the token representations. Some examples of word-level information are Part of Speech (POS) tags, syntactic dependency relationships or lemmas. In order to make use of word-level information in models with subword-level token vocabularies, a usual approach is to assign the word information to all its subwords (Sennrich and Haddow, 2016). This approach, despite improving the translation quality, introduces an information assignment mismatch. We propose"
2020.acl-srw.10,D18-1059,0,0.0210813,"The output of the encoder is then used at every layer of the decoder as key and value of the multi-head attention. In these operations, the token representations in the sequences in the source batch are masked according to the original sequence lengths in tokens. Related Work The main difficulty in profiting from word-level information in subword-based NMT architectures is the word-subword token level mismatch. Several lines of research have studied how to combine subword-level representations into wordlevel information in a task-agnostic way. While the approaches by Bojanowski et al. (2017), Zhao et al. (2018) and Li et al. (2018) aim at computing pre-trained word representations, other proposals integrate the computation of the word representation in the overall NMT model, either combining information from character level, like those by Luong and Manning (2016) Costa-juss`a and Fonollosa (2016), from n-gram level, like the one by Ataman and Federico (2018), or from multiple granularities like the work by Chen et al. (2018). Some other approaches like those by Wang et al. (2019) and Gu et al. (2018b) try to extend this idea to obtain multilingual conceptual representations from character-level repr"
2020.lrec-1.677,P19-1620,0,0.0174709,"English, hindering advancement in Multilingual QA research. Several approaches based on cross-lingual learning and synthetic corpora generation have been proposed. Crosslingual learning refers to zero, and few-shot techniques applied to transfer the knowledge of a QA model trained on many source examples to a given target language with fewer training data. (Artetxe et al., 2019; Lee and Lee, 2019; Liu et al., 2019a) On the other hand, synthetic corpora generation methods are machine-translation (MT) based designed to automatically generate language-specific QA datasets as training resources (Alberti et al., 2019; Lee et al., 2018; T¨ure and Boschee, 2016). Additionally, a multilingual QA system based on MT at test time has also been explored (Asai et al., 2018) In this paper, we follow the synthetic corpora generation approach. In particular, we developed the Translate-Align-Retrieve (TAR) method, based on MT and unsupervised alignment algorithm to translate an English QA dataset to Spanish automatically. Indeed, we applied our method to the popular SQuAD v1.1 generating its first Spanish version. We then trained two Spanish QA systems by fine-tuning the pre-trained Multilingual-BERT model. Finally,"
2020.lrec-1.677,N19-1423,0,0.122724,"Missing"
2020.lrec-1.677,P17-4012,0,0.0106044,"entually a joint source-target BPE segmentation (Sennrich et al., 2016) with a maximum of 50k BPE symbols. Then, we filtered out sentences longer than 80 tokens and removed all source-target duplicates. The final corpora consist of almost 6.5M parallel sentences for the training set, 5k sentence for the validation and 1k for the test set. The pre-processing pipeline is performed with the scripts in the Moses repository3 and the Subword-nmt repository4 . We then trained the NMT system with the Transformer model (Vaswani et al., 2017). We used the implementation available in OpenNMT-py toolkit (Klein et al., 2017) in its default configuration for 200k steps with one GeForce GTX TITAN X device. Additionally, we shared the source and target vocabularies and consequently, we also share the corresponding source an target embeddings between the encoder and decoder. After the training, our best model is obtained by averaging across the final three consecutive checkpoints. Finally, we evaluated the NMT system with the BLEU score (Papineni et al., 2002) on our test set. The model achieved a BLEU score of 45.60 point showing that the it is good enough to be used as a pre-trained English-Spanish translator suita"
2020.lrec-1.677,Q19-1026,0,0.0459891,"Missing"
2020.lrec-1.677,L18-1437,0,0.0608273,"Missing"
2020.lrec-1.677,D19-1283,0,0.0280763,"Missing"
2020.lrec-1.677,P19-1227,0,0.0373135,"Missing"
2020.lrec-1.677,P02-1040,0,0.108564,"rd-nmt repository4 . We then trained the NMT system with the Transformer model (Vaswani et al., 2017). We used the implementation available in OpenNMT-py toolkit (Klein et al., 2017) in its default configuration for 200k steps with one GeForce GTX TITAN X device. Additionally, we shared the source and target vocabularies and consequently, we also share the corresponding source an target embeddings between the encoder and decoder. After the training, our best model is obtained by averaging across the final three consecutive checkpoints. Finally, we evaluated the NMT system with the BLEU score (Papineni et al., 2002) on our test set. The model achieved a BLEU score of 45.60 point showing that the it is good enough to be used as a pre-trained English-Spanish translator suitable for our purpose. 2.2. Source-Translation Context-Alignment The role of the alignment component is to compute the alignment between the context sentences and their trans2 https://github.com/facebookresearch/LASER https://github.com/moses-smt/mosesdecoder 4 https://github.com/rsennrich/subword-nmt lations. We relied on an efficient and accurate unsuper¨ vised word alignment called eflomal (Ostling and Tiedemann, 2016) based on a Bayes"
2020.lrec-1.677,P18-2124,0,0.0320292,"AR method to generated the SQuAD-es v1.1 dataset, the first large-scale training resources for Spanish QA. Finally, we employed the SQuADes v1.1 dataset to train QA systems that achieved state-ofthe-art perfomance on the Spanish QA task, demonstrating the efficacy of the TAR approach for synthetic corpora generation. Therefore, we make the SQuAD-es dataset freely available and encourage its usage for multilingual QA. The results achieved so far encourage us to look forward and extend our approach in future works. First of all, we will apply the TAR method to translated the SQuAD v2.0 dataset (Rajpurkar et al., 2018) and other large-scale extractive QA such as Natural Questions(Kwiatkowski et al., 2019). Moreover, we will also exploit the modularity of the TAR method to support languages other than Spanish to prove the validity of our approach for synthetic corpora generation. 6 https://rajpurkar.github.io/SQuAD-explorer/ Acknowledgements This work is supported in part by the Spanish Ministerio de Econom´ıa y Competitividad, the European Regional Development Fund and the Agencia Estatal de Investigaci´on, through the postdoctoral senior grant Ram´on y Cajal (FEDER/MINECO) amd the project PCIN-2017-079 (AE"
2020.lrec-1.677,P16-1162,0,0.00591661,"rtetxe and Schwenk, 2018a; Artetxe and Schwenk, 2018b) to extract N-way parallel corpora from Wikipedia. Then, to further increase the size of the parallel data, we gathered additional resources from the open-source OPUS corpus (Tiedemann, 2012). Eventually, we selected data from 5 different resources, such as Wikipedia, TED-2013, News-Commentary, Tatoeba and OpenSubTitles (Lison and Tiedemann, 2016; Wolk and Marasek, 2015; Tiedemann, 2012). The data pre-processing pipeline consisted of punctuation normalization, tokenisation, true-casing and eventually a joint source-target BPE segmentation (Sennrich et al., 2016) with a maximum of 50k BPE symbols. Then, we filtered out sentences longer than 80 tokens and removed all source-target duplicates. The final corpora consist of almost 6.5M parallel sentences for the training set, 5k sentence for the validation and 1k for the test set. The pre-processing pipeline is performed with the scripts in the Moses repository3 and the Subword-nmt repository4 . We then trained the NMT system with the Transformer model (Vaswani et al., 2017). We used the implementation available in OpenNMT-py toolkit (Klein et al., 2017) in its default configuration for 200k steps with on"
2020.lrec-1.677,D16-1055,0,0.0544352,"Missing"
2020.lrec-1.677,tiedemann-2012-parallel,0,0.0191968,"ted the first TAR component from scratch, by training an NMT model for English to Spanish direction. Our NMT parallel corpus is created by collecting the en-es parallel data from several resources. We first collected data from the WikiMatrix project (Schwenk et al., 2019) which uses state-of-the-art multilingual sentence embeddings techniques from the LASER toolkit2 (Artetxe and Schwenk, 2018a; Artetxe and Schwenk, 2018b) to extract N-way parallel corpora from Wikipedia. Then, to further increase the size of the parallel data, we gathered additional resources from the open-source OPUS corpus (Tiedemann, 2012). Eventually, we selected data from 5 different resources, such as Wikipedia, TED-2013, News-Commentary, Tatoeba and OpenSubTitles (Lison and Tiedemann, 2016; Wolk and Marasek, 2015; Tiedemann, 2012). The data pre-processing pipeline consisted of punctuation normalization, tokenisation, true-casing and eventually a joint source-target BPE segmentation (Sennrich et al., 2016) with a maximum of 50k BPE symbols. Then, we filtered out sentences longer than 80 tokens and removed all source-target duplicates. The final corpora consist of almost 6.5M parallel sentences for the training set, 5k senten"
2020.spnlp-1.1,D19-1633,0,0.0126353,"h a pattern describing the left and right dependencies of the token at that position in the Otok sequence. An example of dependency expansion could be [nsubj-advmod-HEAD-xcomp] for the word “likes” in the dependency parse tree from Figure 1. After each iteration, the output of the model is expanded.1 This consists of creating a new sequence Iterative Refinement Lee et al. (2018) propose a latent variable nonautoregressive machine translation model where first the target length is predicted by the model, and then, the decoder is iteratively applied to its own output to refine it. Mask-predict (Ghazvininejad et al., 2019) also predicts the target sentence length and then nonautoregressively predicts the sentence itself, iteratively refining it a fixed number of times, masking out and regenerating the tokens it is least confident 1 The expansion of the output to be fed as input in the next iteration occurs in the CPU outside of the neural model itself. 2 by combining the tokens from Itok , Otok and Oexp . This process is illustrated in Figure 2, making use of the dependency tree from Figure 1. When there is a padding token [pad] in the output (either Otok or Oexp ), this means that the output at that position i"
2020.spnlp-1.1,Q19-1042,0,0.0129358,"to train a new kind of language model where the token generation order is driven by the dependency parse tree of the sentence and where the generation process is iterative. 3 Insertion-based Generation Stern et al. (2019) propose a conditional generative model that iteratively generates tokens plus the position at which they should be inserted within the sequence. Emelianenko et al. (2019) further propose to optimize the generation order by sampling from the ordering permutations. Instead, Chan et al. (2019) optimize a lower bound of the marginalized probability over every possible ordering. Gu et al. (2019a) handle the generation order as a latent variable that is captured as the relative position through self-attention, optimizing the ELBO to train the model. Levenshtein Transformer (Gu et al., 2019b) is a non-autoregressive approach trained with reinforcement learning (RL) to generate token insertion and deletion actions. While it benefits from the same generation speed-ups over autoregressive models as our model, it has the added difficulty of learning an insertion/deletion policy using RL without any linguistically or empirically motivated priors, which can be slow or difficult to obtain co"
2020.spnlp-1.1,P19-1122,0,0.0190442,"et al., 2016) are recursive models that operate with a stack of symbols that can be populated with terminals or nonterminals, or “reduced” to generate a syntactic constituent, obtaining as a result a sentence and its associated constituency parse tree. Shen et al. (2018) use skip-connections to integrate constituent relations with RNNs, learning the underlying dependency structures by leveraging a syntactic distance together with structured 1 Proceedings of 4th Workshop on Structured Prediction for NLP, pages 1–10 c November 20, 2020. 2020 Association for Computational Linguistics attention. Akoury et al. (2019) use a simplified constituency tree as latent variables, modeling it autoregressively to later use it as input for a nonautoregressive transformer that generates the output sentence. Ordered neurons (Shen et al., 2019) are modified LSTMs where the latent sentence tree structure is used to control the dependencies between recurrent units with a special “master” input and forget gates. about. Lawrence et al. (2019) follow a similar approach and start with a sequence of placeholder tokens (all the same) of a specified length, and they iteratively replace them with normal tokens via masked LM-styl"
2020.spnlp-1.1,N18-1086,0,0.0212141,"with a similar structure to an n-gram LM, but where the context of a word is its preceding bigram plus a list of preceding words whose parent does not precede it. Shen et al. (2008) make use of the dependency tree in a probabilistic LM, computing the probability of each word conditioned on its parent and the sibling words between both. Mirowski and Vlachos (2015) propose a dependency LM based on RNNs, where the dependency tree is decomposed into a collection of unrolls, that is, paths from the root to one of the leaves, and where the probability of a word can be predicted from these unrolls. Buys and Blunsom (2018) propose a shift-reduce transition-based LSTM (Hochreiter and Schmidhuber, 1997) dependency LM that can be used for language modeling and generation by means of dynamic programming. Our experiments show that this paradigm is effective at text generation, with quality between LSTMs and Transformers, and comparable diversity, requiring less than half their decoding steps, and its generation process allows direct control over the syntactic constructions of the generated text, enabling the induction of stylistic variations. 1 Related Work Introduction The currently dominant text generation paradig"
2020.spnlp-1.1,D19-1001,0,0.016544,"stance together with structured 1 Proceedings of 4th Workshop on Structured Prediction for NLP, pages 1–10 c November 20, 2020. 2020 Association for Computational Linguistics attention. Akoury et al. (2019) use a simplified constituency tree as latent variables, modeling it autoregressively to later use it as input for a nonautoregressive transformer that generates the output sentence. Ordered neurons (Shen et al., 2019) are modified LSTMs where the latent sentence tree structure is used to control the dependencies between recurrent units with a special “master” input and forget gates. about. Lawrence et al. (2019) follow a similar approach and start with a sequence of placeholder tokens (all the same) of a specified length, and they iteratively replace them with normal tokens via masked LM-style inference. As the masking strategy for the training data, the authors propose different stochastic processes to randomly select which placeholders are to be uncovered. 2.3 Our proposal is to train a new kind of language model where the token generation order is driven by the dependency parse tree of the sentence and where the generation process is iterative. 3 Insertion-based Generation Stern et al. (2019) prop"
2020.spnlp-1.1,D18-1149,0,0.0201327,"m a vocabulary with all possible textual tokens (terminal tokens). The second output, Oexp , is a sequence of tokens called expansion placeholders, which are taken from a separate vocabulary. Each expansion placeholder is associated with a pattern describing the left and right dependencies of the token at that position in the Otok sequence. An example of dependency expansion could be [nsubj-advmod-HEAD-xcomp] for the word “likes” in the dependency parse tree from Figure 1. After each iteration, the output of the model is expanded.1 This consists of creating a new sequence Iterative Refinement Lee et al. (2018) propose a latent variable nonautoregressive machine translation model where first the target length is predicted by the model, and then, the decoder is iteratively applied to its own output to refine it. Mask-predict (Ghazvininejad et al., 2019) also predicts the target sentence length and then nonautoregressively predicts the sentence itself, iteratively refining it a fixed number of times, masking out and regenerating the tokens it is least confident 1 The expansion of the output to be fed as input in the next iteration occurs in the CPU outside of the neural model itself. 2 by combining th"
2020.spnlp-1.1,W02-0109,0,0.412669,"Missing"
2020.spnlp-1.1,N16-1024,0,0.0318915,"Zipser, 1989). Other architectures, such as Transformer (Vaswani et al., 2017), while not intrinsically sequential, have also been targeted for sequential generation. On the other hand, some recent lines of research have focused on nonsequential generation. In this work, we propose a new paradigm for text generation and language modeling called Iterative Expansion Language Model, which generates the final sequence following a token ordering defined by the sentence dependency parse by iteratively expanding each level of the tree. 2.2 Syntax-driven Generation Recurrent neural network grammars (Dyer et al., 2016) are recursive models that operate with a stack of symbols that can be populated with terminals or nonterminals, or “reduced” to generate a syntactic constituent, obtaining as a result a sentence and its associated constituency parse tree. Shen et al. (2018) use skip-connections to integrate constituent relations with RNNs, learning the underlying dependency structures by leveraging a syntactic distance together with structured 1 Proceedings of 4th Workshop on Structured Prediction for NLP, pages 1–10 c November 20, 2020. 2020 Association for Computational Linguistics attention. Akoury et al."
2020.spnlp-1.1,P15-2084,0,0.0176595,"ency treedriven LMs (§2.1), syntax-driven generation (§2.2), insertion-based approaches (§2.3) and iterative refinement approaches (§2.4). 2.1 Dependency LMs The use of dependency parse trees to drive a language model was first proposed by Chelba et al. (1997), with a similar structure to an n-gram LM, but where the context of a word is its preceding bigram plus a list of preceding words whose parent does not precede it. Shen et al. (2008) make use of the dependency tree in a probabilistic LM, computing the probability of each word conditioned on its parent and the sibling words between both. Mirowski and Vlachos (2015) propose a dependency LM based on RNNs, where the dependency tree is decomposed into a collection of unrolls, that is, paths from the root to one of the leaves, and where the probability of a word can be predicted from these unrolls. Buys and Blunsom (2018) propose a shift-reduce transition-based LSTM (Hochreiter and Schmidhuber, 1997) dependency LM that can be used for language modeling and generation by means of dynamic programming. Our experiments show that this paradigm is effective at text generation, with quality between LSTMs and Transformers, and comparable diversity, requiring less th"
2020.spnlp-1.1,P02-1040,0,0.107163,"onducted this experiment with the wordlevel models trained on EMNLP2017 News data. We compute the ratio of adjectives per sentence to verify the increased presence of adjectives, while controlling quality and diversity measures over the generated text for potential degradation. Experimental Setup Unconditional Text Generation We conducted experiments on unconditional text generation following the methodology used by Caccia et al. (2020). The goal is to assess both the quality and diversity of the text generated by the model and the baselines. For the quality evaluation, we use the BLEU score (Papineni et al., 2002) over the test set, where each generated sentence is evaluated against the whole test set as a reference. For diversity, we used the self-BLEU score (Zhu et al., 2018), computed using as references the rest of the generated sentences. For each model, the temperature of the final softmax τ is tuned to generate text in the closest quality/diversity regime to the training data. Iterative expansion LMs are compared against a standard LM baselines, namely, AWD-LSTM2 (Merity et al., 2018) and a Transformer LM (Vaswani et al., 2017), both with word (w) and BPE subword (sw) vocabularies. The models 5"
2020.spnlp-1.1,P16-1162,0,0.0177126,"ne dependency to the left and one to the right. For each word with more than one dependency on any of its sides, we rearrange the tree to force left-to-right dependencies. Although this tree binarization reduces the degree of parallelism, it reduces data sparsity and allows handling constructions with a number of dependencies may otherwise be too large for the model to properly capture, such as enumerations (e.g., “I bought a pair of shoes, an umbrella, a beautiful jacket and a bracelet”). Iterative expansion LMs can be naturally extended to subword vocabularies, like byte-pair encoding (BPE; Sennrich et al., 2016): for each word, we decompose its node in the tree into as many 3.2 Training For training iterative expansion LMs, the main input of the model is the tokens at one of the levels of the dependency parse tree (Itok ), while the output is the following level tokens (Otok ) and expansion placeholders (Oexp ). A secondary input to the model are the dependency indexes, which are used in the head position embedding. The model is trained with the categorical crossentropy for both tokens and expansion placeholders, then adding both sublosses into the final loss (with equal weights). Tokens generated in"
2020.spnlp-1.1,W19-3620,0,0.0201931,"et al., 2019b) is a non-autoregressive approach trained with reinforcement learning (RL) to generate token insertion and deletion actions. While it benefits from the same generation speed-ups over autoregressive models as our model, it has the added difficulty of learning an insertion/deletion policy using RL without any linguistically or empirically motivated priors, which can be slow or difficult to obtain convergence in practice. By comparison, our approachmakes uses a linguistically motivated prior for word insertion in a fully supervised way, avoiding the optimization difficulties of RL. Welleck et al. (2019) use cost minimization imitation learning to learn a policy to generate a binary tree that is used to drive the token generation. 2.4 Iterative Expansion LMs ROOT nsubj poss My advmod dog also xcomp likes eating dobj sausage Figure 1: Example of dependency parse tree. The input vocabulary contains terminal tokens as well as non-terminal special tokens called dependency placeholders, each of which is associated with one of the possible dependency relations to the heads. For the dependency tree in Figure 1, the dependency placeholders are [poss], [nsubj], [advmod], [xcomp], [dobj] and [ROOT]. Th"
2020.spnlp-1.1,P08-1066,0,0.0715277,"endency parse tree is used to drive the Transformer model to generate sentences iteratively. In this section, we provide an overview of works related to ours, including dependency treedriven LMs (§2.1), syntax-driven generation (§2.2), insertion-based approaches (§2.3) and iterative refinement approaches (§2.4). 2.1 Dependency LMs The use of dependency parse trees to drive a language model was first proposed by Chelba et al. (1997), with a similar structure to an n-gram LM, but where the context of a word is its preceding bigram plus a list of preceding words whose parent does not precede it. Shen et al. (2008) make use of the dependency tree in a probabilistic LM, computing the probability of each word conditioned on its parent and the sibling words between both. Mirowski and Vlachos (2015) propose a dependency LM based on RNNs, where the dependency tree is decomposed into a collection of unrolls, that is, paths from the root to one of the leaves, and where the probability of a word can be predicted from these unrolls. Buys and Blunsom (2018) propose a shift-reduce transition-based LSTM (Hochreiter and Schmidhuber, 1997) dependency LM that can be used for language modeling and generation by means o"
2020.spnlp-1.1,J93-2004,0,\N,Missing
2020.spnlp-1.1,P14-5010,0,\N,Missing
2020.spnlp-1.1,P06-4018,0,\N,Missing
2020.spnlp-1.1,N19-1423,0,\N,Missing
2020.webnlg-1.19,2020.webnlg-1.7,0,0.0612738,"Missing"
2020.webnlg-1.19,W13-2102,0,0.0778741,"Missing"
2020.webnlg-1.19,2020.webnlg-1.5,1,0.855731,"Missing"
2020.webnlg-1.19,W13-0108,0,0.0283629,"omain (Reiter and Dale, 2000) in the past two decades. Bontcheva et al. (2004) work in the medical domain, where they use a traditional NLG approach to generate sentences from RDF data filtering repetitive RDF, and then group coherent triples aggregating the generated sentences in order to produce the final ones. (Cimiano et al., 2013) generate cooking recipes from semantic web 1 https://webnlg-challenge.loria.fr/ challenge_2020/ data, using a large corpus to extract lexicon in the cooking domain, which is then used in conjunction with a traditional NLG approach to generate cooking receipts. (Duma and Klein, 2013) use a method which works well on RDF triples in a seen domain but fails with unseen ones. Their aim is to learn sentence templates from parallel RDF data and text corpora by means of aligning entities in RDF triples with entities mentioned in sentences, and then extracting these templates from the aligned sentences by replacing the entity men- tion with a unique token. We decided to only participate in the English version of the RDF-to-Text challenge (Castro Ferreira et al., 2020). We used a model based on the Transformer encoded-decoder architecture (Vaswani et al., 2017). Moreover, inspired"
2020.webnlg-1.19,P07-2045,0,0.00929399,"l the information present in the triples. 3 Preprocessing In this section we describe the first steps performed on data. There is a common step, delexicalisation, that has been performed since the very beginning of this challenge, back to 2017 2 . We decided to avoid this step due to the implementation of our BT method that does not contain the required mapping: from individual entities to generic words. The very first data processing guide is defined as follows, and it is exemplified in Table 1. First of all, we linearise the RDF input and split the camelCase notation. Then, Moses Tokenizer (Koehn et al., 2007) is applied to separate punctuation from words, preserving special tokens such as dates, and normalize characters. Finally, Byte Pair Encoding (BPE) (Sennrich et al., 2015) is applied to enable the model to be more robust to unseen data. This is a traditional technique that increases the translation quality of models. BPE is learned with the training plus validation procedure and is used for the source and target vocabulary. This way, the model is trained for both receiving and predicting BPE encoded vocabulary, also for the test set. Finally, 2 https://webnlg-challenge.loria.fr/ challenge_201"
2020.webnlg-1.19,P16-1009,0,0.0341249,"s to learn sentence templates from parallel RDF data and text corpora by means of aligning entities in RDF triples with entities mentioned in sentences, and then extracting these templates from the aligned sentences by replacing the entity men- tion with a unique token. We decided to only participate in the English version of the RDF-to-Text challenge (Castro Ferreira et al., 2020). We used a model based on the Transformer encoded-decoder architecture (Vaswani et al., 2017). Moreover, inspired by previous work in the MT field, we enlarged the original corpus by means of Back Translation (BT) (Sennrich et al., 2016). The rest of the document is organised as follows. First, in Section 2 we take a deeper dive into the task formulation. Next, in Section 3 the preprocessing plan is explained. Then, in Section 4 we depict the Transformer model architecture adapted to our problem. Thereafter, we briefly describe postprocessing in Section 5. Finally, in Section 6 we summarize the implementation of BT over the original challenge followed by brief results and conclusions in Sections 7 and 8 respectively. 2 Task Formulation The goal of the RDF-to-Text task is to generate text from a set of triples, which are words"
2020.webnlg-1.19,2020.emnlp-demos.4,0,0.0129618,"ed an external monolingual corpus of the target language to perform augmentation of the source data. In order to do so, we implemented a distance-based approach to the training data since entities appearing in the corpus were annotated. Taking this into account, not only was the team capable of scrapping Wikipedia pages of most similar entities to the ones in the original corpus, but we were also able to limit scrapping to the first three paragraph in each page without loss of quality. In order to determine the most similar entities, an embedding distance was computed regarding Wikipedia2Vec (Yamada et al., 2020) that allows to query for entities rather than words. The current approach to solve the BackTranslation, text-to-RDF, implies using parsing trees that guarantee that elements in the RDF appear in the text. Consequently, this implementation 169 Results In Table 2, we show the results obtained in the test set for the competition. One remarkable aspect is that there is not a significant difference between the performance in the seen and unseen domain regarding the METEOR, TER and chrF++ metric. On the other hand, there exists a performance drop based on BLEU score in the unseen data with respect"
2020.webnlg-1.5,P07-2045,0,0.0143933,"nce, and TER (Snover et al., 2006); for which lower values means better performance. 3 put of intermediate models. However, the system will be adapted to allow different experiments. 4.1 Preprocessing First of all, each RDF is delexicalise as suggested in the WebNLG Challenge 2017. Table 1 illustrates how one RDF that establishes a relationship between specific entities (Rome, Italy) can be generalised to any pair of entities of the same type (CITY, COUNTRY). Hence, we also need to delexicalise every target sentence to match the delexicalise input during training phase. Then, Moses tokenizer (Koehn et al., 2007) is applied to separate punctuation from words, preserving special tokens such as dates, and normalize characters. Finally, BPE is also applied to improve the translation quality. More details about the advantages and disadvantages of this technique will be discused in Section 6.1. Related Work The present task was also proposed in the WebNLG Challenge 20172 . Submissions to this challenge included different approaches, such as: Neural Machine Translation (NMT), Statistical Machine Translation (SMT) and Pipeline systems (Gardent et al., 2017b). The best model regarding automatic metrics was su"
2020.webnlg-1.5,W05-0909,0,0.116232,"ibution 4.0 International. Type of Text RDF Target Sentence Lexicalise h Rome, capital of, Italy i Rome is the capital of Italy. Delexicalise h CITY, capitalOf, COUNTRY i CITY is the capital of COUNTRY. Table 1: Lexicalise and delexicalise language. Finally, we aim to generate a sequence of sentences S, which consists of a sequence of words [w1 , ..., wm ]. The resulting sentences in S should be grammatically correct and should also contain all the information present in the KB K. In order to assess the correctness of S, the following metrics are studied: BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), chrF++ (Popovi´c, 2017); for which greater values mean better performance, and TER (Snover et al., 2006); for which lower values means better performance. 3 put of intermediate models. However, the system will be adapted to allow different experiments. 4.1 Preprocessing First of all, each RDF is delexicalise as suggested in the WebNLG Challenge 2017. Table 1 illustrates how one RDF that establishes a relationship between specific entities (Rome, Italy) can be generalised to any pair of entities of the same type (CITY, COUNTRY). Hence, we also need to delexicalise every target sentence to mat"
2020.webnlg-1.5,W19-5206,0,0.0199258,"(Sennrich et al., 2016). First of all, BT trains an intermediate system on the parallel data which is used to translate the target monolingual data into the source language, i.e. text-to-RDF. The latter, results in a parallel corpus where the source, RDF, is synthetic MT output while the target is genuine text written by humans. Afterwards, the generated synthetic parallel corpus is added to the real bitext in order to train a final model that will translate from the source to the target language, equivalently RDF-to-text. Moreover, we also studied the performance of Tagged Back-Translation (Caswell et al., 2019). This technique adds an extra token at the beginning of each synthetic instance allowing the model to differentiate them from real data. 7 Models Summary In the following, the best models obtained in each of the considered experiments as well as their performance are presented. Moreover, we also provide a comparison between our models and some of the most relevant models in the RDF-to-Text domain. 6.3.1 Monolingual Data We used english monolingual data extracted from Wikipedia. The scrapped pages were from most similar entities to the ones in the training corpus following an embedding distanc"
2020.webnlg-1.5,W18-6501,0,0.0182988,"oder Bidirectional Long Short-Term Memory architecture with attention3 . Although the model falls in NMT field, it is not an strict end-to-end approach as it matches input and output with delexicalised templates. Recently, most of the taken approaches to solve this NLG task are based on encoder-decoder architectures as well as Graph Neural Networks (GNN). The main idea behind these methods is to exploit the input structure, which can be seen as a graph. It can be empirically shown how encoderdecoder GNN architectures can achieve similar results to the best submission in WebNLG Challenge 2017 (Marcheggiani and Perez-Beltrachini, 2018) or even improve these benchmarks (Trisedya et al., 2018). 4 4.2 Transformer model The team implemented a sequence-to-sequence, encoder-decoder based on the Transformer model proposed in (Vaswani et al., 2017). Moreover, Transformer can be interpreted as a special case of GNN, mentioned by Chaitanya Joshi4 . Hence, this approach is also aligned with the latest research, mainly based on GNN, conducted to solve the RDFto-text task. This model is implemented5 with an attention mechanism to allow modeling dependencies regardless their distance in the input or output sequences. This results in a fu"
2020.webnlg-1.5,mendes-etal-2012-dbpedia,0,0.057544,"Missing"
2020.webnlg-1.5,P02-1040,0,0.106329,"r Computational Linguistics Attribution 4.0 International. Type of Text RDF Target Sentence Lexicalise h Rome, capital of, Italy i Rome is the capital of Italy. Delexicalise h CITY, capitalOf, COUNTRY i CITY is the capital of COUNTRY. Table 1: Lexicalise and delexicalise language. Finally, we aim to generate a sequence of sentences S, which consists of a sequence of words [w1 , ..., wm ]. The resulting sentences in S should be grammatically correct and should also contain all the information present in the KB K. In order to assess the correctness of S, the following metrics are studied: BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), chrF++ (Popovi´c, 2017); for which greater values mean better performance, and TER (Snover et al., 2006); for which lower values means better performance. 3 put of intermediate models. However, the system will be adapted to allow different experiments. 4.1 Preprocessing First of all, each RDF is delexicalise as suggested in the WebNLG Challenge 2017. Table 1 illustrates how one RDF that establishes a relationship between specific entities (Rome, Italy) can be generalised to any pair of entities of the same type (CITY, COUNTRY). Hence, we also need to delexi"
2020.webnlg-1.5,P17-1017,0,0.0825935,"e input during training phase. Then, Moses tokenizer (Koehn et al., 2007) is applied to separate punctuation from words, preserving special tokens such as dates, and normalize characters. Finally, BPE is also applied to improve the translation quality. More details about the advantages and disadvantages of this technique will be discused in Section 6.1. Related Work The present task was also proposed in the WebNLG Challenge 20172 . Submissions to this challenge included different approaches, such as: Neural Machine Translation (NMT), Statistical Machine Translation (SMT) and Pipeline systems (Gardent et al., 2017b). The best model regarding automatic metrics was submitted by the University of Melbourne. Such model consisted of an encoderdecoder Bidirectional Long Short-Term Memory architecture with attention3 . Although the model falls in NMT field, it is not an strict end-to-end approach as it matches input and output with delexicalised templates. Recently, most of the taken approaches to solve this NLG task are based on encoder-decoder architectures as well as Graph Neural Networks (GNN). The main idea behind these methods is to exploit the input structure, which can be seen as a graph. It can be em"
2020.webnlg-1.5,D14-1162,0,0.0820536,"Missing"
2020.webnlg-1.5,W17-3518,0,0.0890937,"e input during training phase. Then, Moses tokenizer (Koehn et al., 2007) is applied to separate punctuation from words, preserving special tokens such as dates, and normalize characters. Finally, BPE is also applied to improve the translation quality. More details about the advantages and disadvantages of this technique will be discused in Section 6.1. Related Work The present task was also proposed in the WebNLG Challenge 20172 . Submissions to this challenge included different approaches, such as: Neural Machine Translation (NMT), Statistical Machine Translation (SMT) and Pipeline systems (Gardent et al., 2017b). The best model regarding automatic metrics was submitted by the University of Melbourne. Such model consisted of an encoderdecoder Bidirectional Long Short-Term Memory architecture with attention3 . Although the model falls in NMT field, it is not an strict end-to-end approach as it matches input and output with delexicalised templates. Recently, most of the taken approaches to solve this NLG task are based on encoder-decoder architectures as well as Graph Neural Networks (GNN). The main idea behind these methods is to exploit the input structure, which can be seen as a graph. It can be em"
2020.webnlg-1.5,W17-4770,0,0.0226014,"Missing"
2020.webnlg-1.5,P16-1009,0,0.369834,"up to 7 BLEU points, hence, opening a window for surpassing state-of-the-art results with appropriate architectures1 . 1 can be divided into: text-to-text generation or datato-text generation, according to Gatt and Krahmer (2017). The latter is the most common approach taken to solve the RDF-to-Text task, since it is based on a mapping from structured data to text. However, we focus on applying MT architectures and techniques, which are considered for text-totext generation. To further improve the quality of the text generated by our models, we assess the advantages of Back-Translation (BT) (Sennrich et al., 2016) for this task, as well as, compare the benefits of learning the embeddings from scratch to the alternative of providing pretrained embeddings. The specific contributions of our work to the field are: • We train an encoder-decoder Transformer architecture, in end-to-end and pipeline manners, for RDF-to-Text task. Introduction A Knowledge Base (KB) is a large source of information represented in a structured way. The information structure is based on Resource Description Framework (RDF), which consist of three elements: hsubject, predicate, objecti. Thus, it establishes relations (predicate) be"
2020.webnlg-1.5,W18-6543,0,0.0117568,"xicalisation step was giving, it is no longer present using these pretrained embeddings. However, these embeddings have been built using a large amount Ablation Experiment and Results The first experiment analyses the influence of the number of BPE subwords. The second experiment study the improvements of pretrained embeddings against learned embeddings. Last experiment is designed to enlarge training data by means of different BT approaches. 6.1 Byte Pair Encoding It has been demonstrated that the method used to treat rare items in data-to-text generation strongly impacts system performance (Shimorina and Gardent, 2018). The authors (Shimorina and Gardent, 2018) also stated that character-based techniques, eter tuning and optimizing approaches. 6 https://gitlab.com/shimorina/ webnlg-dataset 42 BT Model Transformer Parsing Parsing Parsing Parsing Parsing Tagged No No No Yes No Yes Corpus 79639 79639 79639 79639 155897 155897 Embedding GloVe GloVe Learned Learned Learned Learned Dimension 300 300 256 256 256 256 BPE — — 5000 5000 10000 10000 BLEU (↑) 31.62 38.26 41.97 43.37 44.01 44.22 METEOR (↑) 0.30 0.35 0.39 0.40 0.40 0.40 chrF++ (↑) 0.54 0.61 0.65 0.67 0.67 0.67 TER (↓) 0.61 0.51 0.45 0.43 0.44 0.43 Table"
2020.webnlg-1.5,2006.amta-papers.25,0,0.329132,"Missing"
2020.webnlg-1.5,P18-1151,0,0.0601293,"hough the model falls in NMT field, it is not an strict end-to-end approach as it matches input and output with delexicalised templates. Recently, most of the taken approaches to solve this NLG task are based on encoder-decoder architectures as well as Graph Neural Networks (GNN). The main idea behind these methods is to exploit the input structure, which can be seen as a graph. It can be empirically shown how encoderdecoder GNN architectures can achieve similar results to the best submission in WebNLG Challenge 2017 (Marcheggiani and Perez-Beltrachini, 2018) or even improve these benchmarks (Trisedya et al., 2018). 4 4.2 Transformer model The team implemented a sequence-to-sequence, encoder-decoder based on the Transformer model proposed in (Vaswani et al., 2017). Moreover, Transformer can be interpreted as a special case of GNN, mentioned by Chaitanya Joshi4 . Hence, this approach is also aligned with the latest research, mainly based on GNN, conducted to solve the RDFto-text task. This model is implemented5 with an attention mechanism to allow modeling dependencies regardless their distance in the input or output sequences. This results in a fundamental feature for NLG, since automated generation of"
2020.webnlg-1.5,2020.emnlp-demos.4,0,0.292463,"with unseen relations in the test set. These datasets are based on DBPedia, which is a multilingual KB that was built from several kinds of structured information included in Wikipedia (Mendes et al., 2012). 6 6.2 Embedding Analysis In the MT field, the most common approach is to learn embeddings from scratch due to amount of available data. Although this is not our case, in which thousands of instances are provided, we present a comparison between using learned embeddings and pretrained embeddings. We found that suitable embeddings could be: GloVe (Pennington et al., 2014) and Wikipedia2Vec (Yamada et al., 2020) since instances are extracted from Wikipedia. The preprocessing and postprocessing pipeline had to be slightly adapted to allow the use of pretrained embeddings. These embeddings rely directly on the lexicalise format as shown in Table 1. Notice that the generalization that the delexicalisation step was giving, it is no longer present using these pretrained embeddings. However, these embeddings have been built using a large amount Ablation Experiment and Results The first experiment analyses the influence of the number of BPE subwords. The second experiment study the improvements of pretraine"
2020.wmt-1.10,W19-5311,1,0.824015,"Missing"
2020.wmt-1.10,W18-6459,0,0.0200238,"ependently tokenized using BPE (Sennrich et al., 2016b) with 32 thousand operations. Table 1 the estatistics for each language. Tamil data has been tokenized at word-level using Indic-NLP (Kunchukuttan, 2020) and then tokenized with BPE with 16 thousand operations. sentences 1758872 1758872 1663458 1663458 1681466 1681466 1769606 1769606 1770112 1770112 words 40265543 40265543 37698204 40808518 37410662 43056346 41803882 43156309 41211543 45196313 Table 1: Corpus statistics in number of words and sentences for the language pairs of the Multilingual initial system. Related Work Previous works (Choudhary et al., 2018) have shown that Indian languages are usually a challenge for NMT systems due to their difference in terms of vocabulary and grammar compared to western languages such as English. Also, standard preprocessing methods do not always work well with them, so specific solutions are required to obtain good results. In the context of NMT, previous systems, such as MIDAS (Choudhary et al., 2018), proved that the use of subword units leads to significant improvements in translation quality when applied to Tamil by preventing Out of Vocabulary words in at generation time. 4 DE-EN lang DE EN DE ES DE FR"
2020.wmt-1.10,P19-2033,1,0.885817,"Missing"
2020.wmt-1.10,D16-1026,0,0.0366142,"Missing"
2020.wmt-1.10,W18-2703,0,0.0118072,"is usually more available, as it does not require any additional labeling. A common approach to benefit from monolingual data is back-translation (Sennrich et al., 2016a), which consists of translating a monolingual corpus to generate synthetic corpora that can be later employed to continue training. Similar techniques create a synthetic pseudo-parallel corpus through a pivot language (Casas et al., 2019) that can be then trained similarly to back-translation when data is available between the desired language pair and a pivot high resource language. More recently, iterative back-translation (Hoang et al., 2018) was proposed. This technique allows the system to generate synthetic data while updating the system, so better the new data improves as the system trains. On the other hand, several works on Multilingual NMT have shown benefits for low resource language pairs by allowing positive transfer from the high resource languages, boosting the performance of the low resource ones. Different architectures have been proposed that show this behavior, from universal models where all parameters are shared between all languages (Johnson et al., 2017), to architectures that share a common device that maps re"
2020.wmt-1.10,P07-2045,0,0.00535908,"k-translation with monolingual corpora. 3 DE-ES DE-FR EN-ES EN-FR Corpora and Data Preparation All proposed systems in this work are constrained using exclusively data provided by the task’s organization. The multilingual initial system was trained using Europarl v8, for all translation directions between English, French, Spanish, and German. For English-Tamil PMIndia, Tanzil v1, The UFAL EnTam corpus, The NLPC UOM En-Ta corpus, Wikimatrix, and Wikitiles. As monolingual Tamil data, we used News Crawl, while for English, we used News-commentary. We processed all non-Tamil data following Moses (Koehn et al., 2007) scripts provided by the organization. For each one, we applied punctuation normalization, tokenization, and true-casing. Then each language is independently tokenized using BPE (Sennrich et al., 2016b) with 32 thousand operations. Table 1 the estatistics for each language. Tamil data has been tokenized at word-level using Indic-NLP (Kunchukuttan, 2020) and then tokenized with BPE with 16 thousand operations. sentences 1758872 1758872 1663458 1663458 1681466 1681466 1769606 1769606 1770112 1770112 words 40265543 40265543 37698204 40808518 37410662 43056346 41803882 43156309 41211543 45196313 T"
2020.wmt-1.10,N19-4009,0,0.0208164,"mil translation direction is trained by freezing the English encoder and training the Tamil decoder to force the shared representation. In this case, we also notice the positive transfer compared to the baseline trained with just parallel data. See in Figure 1 the schema of the supervised pretraining that we have just described. Implementation. For this work, all encoders and decoders were implemented using the Transformer (Vaswani et al., 2017) architecture, with 6 layers, 8 heads, 512 embedding size, and 2048 feed-forward size for each of them, and everything was implemented using Fairseq’s(Ott et al., 2019) 0.6 release. The multilingual NMT model was trained in a single NVIDIA TITAN XP for 50 thousand updates using adam optimizer with 0.001 as learning, 4000 warmup updates and updating every 16 batches of 2000 tokens. Adding Tamil-English and EnglishTamil directions to the system took approximately 45 thousand updates using the same parameters and GPU configuration. 5.2 Monolingual Unsupervised Fine-tuning Methodology. The previous process has benefited from the additional corpus from the Multilingual NMT system, but as stated before, monolingual data is another common source of improvement for"
2020.wmt-1.10,W17-2619,0,0.0133349,"mance of the low resource ones. Different architectures have been proposed that show this behavior, from universal models where all parameters are shared between all languages (Johnson et al., 2017), to architectures that share a common device that maps representations into a shared represen134 Proceedings of the 5th Conference on Machine Translation (WMT), pages 134–138 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics corpus tation space (Firat et al., 2016; Zhu et al., 2020), to architectures that do not share parameters (Escolano et al., 2019; Escolano et al.; Schwenk and Douze, 2017). In the context of the WMT20 Tamil-English news shared task, as the provided parallel data is limited, we resorted to a combination of both proposed methods by incrementally train the new language pair into a Multilingual NMT system using the provided parallel data, to later fine-tune the system using iterative-back-translation with monolingual corpora. 3 DE-ES DE-FR EN-ES EN-FR Corpora and Data Preparation All proposed systems in this work are constrained using exclusively data provided by the task’s organization. The multilingual initial system was trained using Europarl v8, for all transla"
2020.wmt-1.10,P16-1009,0,0.232831,"the target language in the context of machine translation, which may affect attention and decoding in NMT systems. Low Resource NMT Modern NMT systems benefit from having hundreds of thousands or even millions of parallel sentences. When working with low resource language pairs, the two main approaches are the use of monolingual corpora and multilingual NMT. While parallel data may be difficult to obtain for low resource languages, monolingual data is usually more available, as it does not require any additional labeling. A common approach to benefit from monolingual data is back-translation (Sennrich et al., 2016a), which consists of translating a monolingual corpus to generate synthetic corpora that can be later employed to continue training. Similar techniques create a synthetic pseudo-parallel corpus through a pivot language (Casas et al., 2019) that can be then trained similarly to back-translation when data is available between the desired language pair and a pivot high resource language. More recently, iterative back-translation (Hoang et al., 2018) was proposed. This technique allows the system to generate synthetic data while updating the system, so better the new data improves as the system t"
2020.wmt-1.10,P16-1162,0,0.0630549,"the target language in the context of machine translation, which may affect attention and decoding in NMT systems. Low Resource NMT Modern NMT systems benefit from having hundreds of thousands or even millions of parallel sentences. When working with low resource language pairs, the two main approaches are the use of monolingual corpora and multilingual NMT. While parallel data may be difficult to obtain for low resource languages, monolingual data is usually more available, as it does not require any additional labeling. A common approach to benefit from monolingual data is back-translation (Sennrich et al., 2016a), which consists of translating a monolingual corpus to generate synthetic corpora that can be later employed to continue training. Similar techniques create a synthetic pseudo-parallel corpus through a pivot language (Casas et al., 2019) that can be then trained similarly to back-translation when data is available between the desired language pair and a pivot high resource language. More recently, iterative back-translation (Hoang et al., 2018) was proposed. This technique allows the system to generate synthetic data while updating the system, so better the new data improves as the system t"
2020.wmt-1.10,2020.acl-main.150,0,0.0304985,"esource language pairs by allowing positive transfer from the high resource languages, boosting the performance of the low resource ones. Different architectures have been proposed that show this behavior, from universal models where all parameters are shared between all languages (Johnson et al., 2017), to architectures that share a common device that maps representations into a shared represen134 Proceedings of the 5th Conference on Machine Translation (WMT), pages 134–138 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics corpus tation space (Firat et al., 2016; Zhu et al., 2020), to architectures that do not share parameters (Escolano et al., 2019; Escolano et al.; Schwenk and Douze, 2017). In the context of the WMT20 Tamil-English news shared task, as the provided parallel data is limited, we resorted to a combination of both proposed methods by incrementally train the new language pair into a Multilingual NMT system using the provided parallel data, to later fine-tune the system using iterative-back-translation with monolingual corpora. 3 DE-ES DE-FR EN-ES EN-FR Corpora and Data Preparation All proposed systems in this work are constrained using exclusively data pr"
2021.eacl-main.80,2005.mtsummit-papers.11,0,0.0194176,"Missing"
2021.eacl-main.80,P07-2045,0,0.00991062,"guage j as target, the system is trained using the language-specific encoder ei and decoder dj . Adding New Languages Since parameters are not shared between the independent encoders and decoders, the joint training enables the addition of 945 Algorithm 1 Multilingual training step languages (without being multi-parallel). For Russian-English, we used 1 million training sentences from the Yandex corpus2 . As validation and test set, we used newstest2012 and newstest2013 from WMT3 , which is multi-parallel across all the above languages. All data were preprocessed using standard Moses scripts (Koehn et al., 2007) We evaluate our approach in 3 different settings: (i) the initial training, covering all combinations of German, French, Spanish and English; (ii) adding new languages, tested with RussianEnglish in both directions; and (iii) zero-shot translation, covering all combinations between Russian and the rest of the languages. Additionally we compare two configurations which consists in using non-tied or tied embeddings. In the language-specific approach tied embeddings consists in using language-wise word embeddings: for one language, we use the same word embeddings. Whereas, in the case of non-tie"
2021.eacl-main.80,W18-6309,0,0.0605765,"e will be using when describing our approach. We denote the encoder and the decoder for the ith language in the system as ei and di , respectively. For languagespecific scenarios, both the encoder and decoder are considered independent modules that can be freely interchanged to work in all translation directions. Language-specific Encoder-Decoders which may or may not share parameters at some point. Sharing parameters. Firat et al. (2016b) proposed extending the bilingual recurrent neural machine translation architecture (Bahdanau et al., 2015) to the multilingual case (V´azquez et al., 2019; Lu et al., 2018) by designing a shared attention-based mechanism between the languagespecific encoders and decoders to create a language independent representation. As the language specific components rely on the shared modules, modifying those components to add a new language or add further data to the system would require retraining the whole system (similarly to the previous shared approach). (Lakew et al., 2018) proposes a model based on the addition of new languages to an already trained system by vocabulary adaptation and transfer learning. While limited, it requires some retraining to adapt the model t"
2021.eacl-main.80,N19-4009,0,0.0256589,"ering all combinations between Russian and the rest of the languages. Additionally we compare two configurations which consists in using non-tied or tied embeddings. In the language-specific approach tied embeddings consists in using language-wise word embeddings: for one language, we use the same word embeddings. Whereas, in the case of non-tied, the encoder and the decoder of each language have different word embeddings. Tied embeddings in the shared system means that both encoder and decoder share the same word embeddings. All experiments were done using the Transformer provided by Fairseq(Ott et al., 2019) 4 . We used 6 layers, each with 8 attention heads, an embedding size of 512 dimensions, and a vocabulary size of 32k subword tokens with Byte Pair Encoding (Sennrich et al., 2016) (in total for the shared encoders/decoders and per pair for language-specific encoder-decoders). Dropout was 0.1 for the shared approach and 0.3 for language-specific encoders/decoders. Both approaches were trained with an effective batch size of 32k tokens for approximately 200k updates, using the validation loss for early stopping. We used Adam (Kingma and Ba, 2015) as the optimizer, with learning rate of 0.001 an"
2021.eacl-main.80,P15-1166,0,0.0443479,"opportunities for improving this area have dramatically expanded. Thanks to the encoder-decoder architecture, there are viable alternatives to expensive pairwise translation based on classic paradigms1 . The main proposal in this direction is the universal encoder-decoder (joh, 2017) with massive multilingual enhancements (Arivazhagan et al., 2019). While this approach enables zero-shot translation and is beneficial for low-resource languages, it has multiple drawbacks: (i) the entire 1 2 Related Work Multilingual neural machine translation can refer to translating from one-to-many languages (Dong et al., 2015), from many-to-one (Zoph and Knight, 2016) and many-to-many (joh, 2017). Within the many-to-many paradigm, existing approaches can http://www.euromatrixplus.net 944 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 944–948 April 19 - 23, 2021. ©2021 Association for Computational Linguistics out parameter or vocabulary-sharing and on enforcing a compatible representation between the jointly trained languages. The advantage of the approach is that it does not require retraining to add new languages and increasing the number of lang"
2021.eacl-main.80,P19-2033,1,0.881824,"Missing"
2021.eacl-main.80,P16-1162,0,0.0109143,"nguage-specific approach tied embeddings consists in using language-wise word embeddings: for one language, we use the same word embeddings. Whereas, in the case of non-tied, the encoder and the decoder of each language have different word embeddings. Tied embeddings in the shared system means that both encoder and decoder share the same word embeddings. All experiments were done using the Transformer provided by Fairseq(Ott et al., 2019) 4 . We used 6 layers, each with 8 attention heads, an embedding size of 512 dimensions, and a vocabulary size of 32k subword tokens with Byte Pair Encoding (Sennrich et al., 2016) (in total for the shared encoders/decoders and per pair for language-specific encoder-decoders). Dropout was 0.1 for the shared approach and 0.3 for language-specific encoders/decoders. Both approaches were trained with an effective batch size of 32k tokens for approximately 200k updates, using the validation loss for early stopping. We used Adam (Kingma and Ba, 2015) as the optimizer, with learning rate of 0.001 and 4000 warmup steps. 1: procedure M ULTILINGUALT RAINING S TEP 2: N ← Number of languages in the system 3: S = {s0,0 , ..., sN,N } ← {(ei , dj )} 4: E = {e0 , ..., eN } ← Language-"
2021.eacl-main.80,N16-1101,0,0.152258,"hare any parameter across these modules, which allows to add new languages incrementally without retraining the entire system. 3.1 Definitions We next define the notation that we will be using when describing our approach. We denote the encoder and the decoder for the ith language in the system as ei and di , respectively. For languagespecific scenarios, both the encoder and decoder are considered independent modules that can be freely interchanged to work in all translation directions. Language-specific Encoder-Decoders which may or may not share parameters at some point. Sharing parameters. Firat et al. (2016b) proposed extending the bilingual recurrent neural machine translation architecture (Bahdanau et al., 2015) to the multilingual case (V´azquez et al., 2019; Lu et al., 2018) by designing a shared attention-based mechanism between the languagespecific encoders and decoders to create a language independent representation. As the language specific components rely on the shared modules, modifying those components to add a new language or add further data to the system would require retraining the whole system (similarly to the previous shared approach). (Lakew et al., 2018) proposes a model base"
2021.eacl-main.80,W19-4305,0,0.166274,"Missing"
2021.eacl-main.80,D16-1026,0,0.0342706,"Missing"
2021.eacl-main.80,N16-1004,0,0.0133294,"have dramatically expanded. Thanks to the encoder-decoder architecture, there are viable alternatives to expensive pairwise translation based on classic paradigms1 . The main proposal in this direction is the universal encoder-decoder (joh, 2017) with massive multilingual enhancements (Arivazhagan et al., 2019). While this approach enables zero-shot translation and is beneficial for low-resource languages, it has multiple drawbacks: (i) the entire 1 2 Related Work Multilingual neural machine translation can refer to translating from one-to-many languages (Dong et al., 2015), from many-to-one (Zoph and Knight, 2016) and many-to-many (joh, 2017). Within the many-to-many paradigm, existing approaches can http://www.euromatrixplus.net 944 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 944–948 April 19 - 23, 2021. ©2021 Association for Computational Linguistics out parameter or vocabulary-sharing and on enforcing a compatible representation between the jointly trained languages. The advantage of the approach is that it does not require retraining to add new languages and increasing the number of languages does not vary the quality of the lan"
2021.iwslt-1.11,N19-1006,0,0.0611936,"Missing"
2021.iwslt-1.11,D19-1165,0,0.149095,"scription We built an end-to-end ST system, mainly composed of pre-trained modules. We couple a Wav2Vec 2.0 encoder (Baevski et al., 2020) and an mBART decoder (Liu et al., 2020a), following the strategy proposed by Li et al. (2021). When combining these two models, there is a length discrepancy between the target sentence length and the encoder output. For this reason, it is necessary to use a module to shorten the encoder output, which we refer to as the Length Adaptor. Additionally, we introduce an Adapter module to reduce the gap between the different modalities of the pre-trained models (Bapna and Firat, 2019). A method that Escolano et al. (2020) proved to be beneficial for ST models. 2.1 Pre-trained modules Our motivation is to get the most out of pretrained components, which were obtained by selfsupervision or supervised tasks. Concretely, we use a Wav2Vec 2.0 encoder and an mBART decoder, both trained initially by self-supervision and fineFigure 1: System overview. The original architecture proposed by Li et al. (2021) includes a pre-trained Wav2Vec 2.0 as the encoder, a pre-trained mBART decoder and a Length Adaptor. In this work, we add an Adapter module after the encoder. tuned for ASR and m"
2021.iwslt-1.11,2015.iwslt-evaluation.1,0,0.0772296,"Missing"
2021.iwslt-1.11,N19-1423,0,0.0322306,"Missing"
2021.iwslt-1.11,N19-1202,0,0.0518619,"Missing"
2021.iwslt-1.11,2020.iwslt-1.8,0,0.0652154,"Missing"
2021.iwslt-1.11,2020.acl-main.703,0,0.0236372,"trained with 53.2k hours of untranscribed speech from LibriVox (Kahn et al., 2020), fine-tuned on the 960h of transcribed speech from Librispeech (Panayotov et al., 2015), and on pseudo-labels (Xu et al., 2020). mBART is a sequence-to-sequence denoising autoencoder, which reconstructs the input text sen111 put sequences. It achieves an 8x down-sampling of the encoder representation by applying a stack of 3 convolutional layers with a kernel size of 3 and a stride of 2. 2.3 Figure 2: Adapter module tence given a corrupted version of it (Liu et al., 2020a). It follows the same approach as BART (Lewis et al., 2020) but, instead of using just English monolingual data, it is trained with multiple languages. This strategy does not require any parallel corpora, so it can be used as a pre-training step and then fine-tuned for MT tasks. Specifically, we use the 12-layer Transformer decoder of an mBART model, fine-tuned on multilingual MT, from English to 49 languages (Tang et al., 2020). 2.2 We follow the LayerNorm and Attention (LNA) fine-tuning strategy proposed by Li et al. (2021). The main idea is that only some of the modules of Wav2Vec 2.0 and mBART need to be fine-tuned to build a system capable of ST."
2021.iwslt-1.11,2021.acl-long.68,0,0.184511,"n those with the given segmentation, we also decided to work with a custom segmentation algorithm. We base it on the approach of Potapczyk et al. (2019), but we replace the silence detection tool with an ASR system (§3.4). Our experiments on the IWSLT 2019 test set, show that our system works better when the data are segmented with our own segmentation algorithm (§4.3). 2 System description We built an end-to-end ST system, mainly composed of pre-trained modules. We couple a Wav2Vec 2.0 encoder (Baevski et al., 2020) and an mBART decoder (Liu et al., 2020a), following the strategy proposed by Li et al. (2021). When combining these two models, there is a length discrepancy between the target sentence length and the encoder output. For this reason, it is necessary to use a module to shorten the encoder output, which we refer to as the Length Adaptor. Additionally, we introduce an Adapter module to reduce the gap between the different modalities of the pre-trained models (Bapna and Firat, 2019). A method that Escolano et al. (2020) proved to be beneficial for ST models. 2.1 Pre-trained modules Our motivation is to get the most out of pretrained components, which were obtained by selfsupervision or su"
2021.iwslt-1.11,2020.tacl-1.47,0,0.153834,"ith own segmentation algorithms are strictly better than those with the given segmentation, we also decided to work with a custom segmentation algorithm. We base it on the approach of Potapczyk et al. (2019), but we replace the silence detection tool with an ASR system (§3.4). Our experiments on the IWSLT 2019 test set, show that our system works better when the data are segmented with our own segmentation algorithm (§4.3). 2 System description We built an end-to-end ST system, mainly composed of pre-trained modules. We couple a Wav2Vec 2.0 encoder (Baevski et al., 2020) and an mBART decoder (Liu et al., 2020a), following the strategy proposed by Li et al. (2021). When combining these two models, there is a length discrepancy between the target sentence length and the encoder output. For this reason, it is necessary to use a module to shorten the encoder output, which we refer to as the Length Adaptor. Additionally, we introduce an Adapter module to reduce the gap between the different modalities of the pre-trained models (Bapna and Firat, 2019). A method that Escolano et al. (2020) proved to be beneficial for ST models. 2.1 Pre-trained modules Our motivation is to get the most out of pretrained c"
2021.iwslt-1.11,W18-6319,0,0.0135912,"eters of each effect from the ranges shown at Table 3. 3.4 5 riod, which is identified by the absence of English characters in it. The algorithm terminates when the max segment length condition is satisfied or no further splits are possible due to a minimum untranscribable period length, which we set to 0.2 seconds. We test max seg len ∈ [5, 25], and for each value we produce a segmentation, generate translations using one of our ST systems 6 , use the mwerSegmenter 7 software to align the generated translations with the reference translations, and finally obtain a BLEU score using SACREBLEU (Post, 2018). We find that the maximum BLEU score is obtained using max seg len = 22 seconds (Figure 3), which we use to segment the IWSLT 2020 and 2021 test sets for our submission. Data Segmentation Similarly to 2019 and 2020 (Niehues et al., 2019; Ansari et al., 2020), this year’s evaluation data are segmented using an automatic tool (Meignier and Merlin, 2010), which does not ensure that segments are proper sentences nor that they are aligned with the translated text. This assigns extra importance to developing methods for proper segmentation of the audio data, which was confirmed in the previous year"
2021.iwslt-1.11,2020.iwslt-1.9,0,0.158522,"the slow inference time (Weiss et al., 2017). Nevertheless, while there are plenty of data available to train ASR and MT systems, there are not as many datasets for ST, despite some recent efforts (Di Gangi et al., 2019a; Wang et al., 2020b). Moreover, this approach is inherently more difficult because the encoder has to perform both acoustic modeling and semantic encoding. For these reasons, end-to-end ST systems still struggle to achieve the performance of cascade ST models. Still, last year’s IWSLT was the first time an end-to-end system had the best performance in the evaluation campaign (Potapczyk and Przybysz, 2020; Ansari et al., 2020). Hence, given the increasing interest in end-to-end ST systems, and the potential gains from advancing research on them, we decided to focus on developing such a system for this year’s offline task. When there are not enough data for a task, a common practice is to use pre-trained components, like BERT (Devlin et al., 2019) for various NLP tasks. In the ST field, the idea of pre-training the encoder for ASR was introduced by Berard et al. (2018) and has become a standard technique for developing modern end-to-end systems (Pino et al., 2019; Di Gangi et al., 2019b). By co"
2021.iwslt-1.11,2020.aacl-demo.6,0,0.303336,"on (MT) model, which is known as cascade system. However, in recent years, end-to-end models have gained popularity within the research community. These systems are encoder-decoder architectures capable of directly translating speech without intermediate symbolic representations. This approach solves classical shortcomings of cascade ST systems, e.g. the error propagation or the slow inference time (Weiss et al., 2017). Nevertheless, while there are plenty of data available to train ASR and MT systems, there are not as many datasets for ST, despite some recent efforts (Di Gangi et al., 2019a; Wang et al., 2020b). Moreover, this approach is inherently more difficult because the encoder has to perform both acoustic modeling and semantic encoding. For these reasons, end-to-end ST systems still struggle to achieve the performance of cascade ST models. Still, last year’s IWSLT was the first time an end-to-end system had the best performance in the evaluation campaign (Potapczyk and Przybysz, 2020; Ansari et al., 2020). Hence, given the increasing interest in end-to-end ST systems, and the potential gains from advancing research on them, we decided to focus on developing such a system for this year’s off"
2021.iwslt-1.11,2020.lrec-1.517,0,0.0362141,"on (MT) model, which is known as cascade system. However, in recent years, end-to-end models have gained popularity within the research community. These systems are encoder-decoder architectures capable of directly translating speech without intermediate symbolic representations. This approach solves classical shortcomings of cascade ST systems, e.g. the error propagation or the slow inference time (Weiss et al., 2017). Nevertheless, while there are plenty of data available to train ASR and MT systems, there are not as many datasets for ST, despite some recent efforts (Di Gangi et al., 2019a; Wang et al., 2020b). Moreover, this approach is inherently more difficult because the encoder has to perform both acoustic modeling and semantic encoding. For these reasons, end-to-end ST systems still struggle to achieve the performance of cascade ST models. Still, last year’s IWSLT was the first time an end-to-end system had the best performance in the evaluation campaign (Potapczyk and Przybysz, 2020; Ansari et al., 2020). Hence, given the increasing interest in end-to-end ST systems, and the potential gains from advancing research on them, we decided to focus on developing such a system for this year’s off"
adell-etal-2012-buceador,J06-4004,1,\N,Missing
adell-etal-2012-buceador,garcia-mateo-etal-2004-transcrigal,0,\N,Missing
adell-etal-2012-buceador,luengo-etal-2010-modified,1,\N,Missing
C12-2032,2007.mtsummit-papers.1,0,0.0678624,"Missing"
C12-2032,P06-2005,0,0.0641919,"Missing"
C12-2032,N10-1064,0,0.237242,"Missing"
C12-2032,P00-1037,0,0.367428,"Missing"
C12-2032,C10-2022,0,0.0369224,"Missing"
C12-2032,W11-2107,0,0.0416469,"Missing"
C12-2032,W12-3133,1,0.866024,"Missing"
C12-2032,C08-1056,0,0.0318832,"Missing"
C12-2032,D07-1091,0,0.0249106,"Missing"
C12-2032,P09-1096,0,0.032549,"Missing"
C12-2032,pighin-etal-2012-faust,1,0.873397,"Missing"
C12-2032,P02-1019,0,0.101524,"Missing"
costa-jussa-etal-2008-using,popovic-ney-2006-pos,0,\N,Missing
costa-jussa-etal-2008-using,carreras-etal-2004-freeling,0,\N,Missing
costa-jussa-etal-2008-using,2005.mtsummit-papers.36,1,\N,Missing
costa-jussa-etal-2008-using,P05-2012,0,\N,Missing
costa-jussa-etal-2008-using,W00-0508,0,\N,Missing
costa-jussa-etal-2008-using,J04-4002,0,\N,Missing
costa-jussa-etal-2008-using,knight-al-onaizan-1998-translation,0,\N,Missing
costa-jussa-etal-2008-using,J04-2004,0,\N,Missing
costa-jussa-etal-2008-using,J04-2003,0,\N,Missing
costa-jussa-etal-2008-using,A00-1031,0,\N,Missing
costa-jussa-etal-2008-using,J06-4004,1,\N,Missing
costa-jussa-etal-2008-using,W05-0831,0,\N,Missing
costa-jussa-etal-2008-using,N03-1017,0,\N,Missing
costa-jussa-etal-2008-using,J03-1002,0,\N,Missing
costa-jussa-etal-2008-using,P05-1069,0,\N,Missing
costa-jussa-etal-2008-using,N04-1033,0,\N,Missing
costa-jussa-etal-2008-using,N03-1019,0,\N,Missing
costa-jussa-etal-2008-using,P03-1019,0,\N,Missing
costa-jussa-etal-2010-automatic,E06-1032,0,\N,Missing
costa-jussa-etal-2010-automatic,P02-1040,0,\N,Missing
costa-jussa-etal-2010-automatic,J06-4004,1,\N,Missing
costa-jussa-etal-2010-automatic,P03-1021,0,\N,Missing
D07-1045,N06-2001,0,0.0137965,"or phrase-based statistical machine translation systems is needed. In particular, the problem of generalization to new translations seems to be promising to us. This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al., 2007). In this approach words are decomposed into several factors. These factors are trans437 lated and a target phrase is generated. This model could be complemented by a factored continuous tuple N-gram. Factored word language models were already successfully used in speech recognition (Bilmes and Kirchhoff, 2003; Alexandrescu and Kirchhoff, 2006) and an extension to machine translation seems to be promising. The described smoothing method was explicitly developed to tackle the data sparseness problem in tasks like the B TEC corpus. It is well known from language modeling that careful smoothing is less important when large amounts of data are available. We plan to investigate whether this also holds for smoothing of the probabilities in phrase- or tuplebased statistical machine translation systems. 6 Acknowledgments This work has been partially funded by the European Union under the integrated project T C -S TAR (IST2002-FP6-506738), b"
D07-1045,N03-2002,0,0.0217703,"of probabilities in N-gram- or phrase-based statistical machine translation systems is needed. In particular, the problem of generalization to new translations seems to be promising to us. This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al., 2007). In this approach words are decomposed into several factors. These factors are trans437 lated and a target phrase is generated. This model could be complemented by a factored continuous tuple N-gram. Factored word language models were already successfully used in speech recognition (Bilmes and Kirchhoff, 2003; Alexandrescu and Kirchhoff, 2006) and an extension to machine translation seems to be promising. The described smoothing method was explicitly developed to tackle the data sparseness problem in tasks like the B TEC corpus. It is well known from language modeling that careful smoothing is less important when large amounts of data are available. We plan to investigate whether this also holds for smoothing of the probabilities in phrase- or tuplebased statistical machine translation systems. 6 Acknowledgments This work has been partially funded by the European Union under the integrated project"
D07-1045,W06-1607,0,0.0566714,"or instance found in (Chen and Goodman, 1999). Language models and phrase tables have in common that the probabilities of rare events may be overestimated. However, in language modeling probability mass must be redistributed in order to account for the unseen n-grams. Generalization to unseen events is less important in phrase-based SMT systems since the system searches only for the best segmentation and the best matching phrase pair among the existing ones. We are only aware of one work that performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). Two types of phrase-table smoothing were compared: black-box and glass-box methods. Black-methods do not look inside phrases but instead treat them as atomic objects. By these means, all the methods developed for language modeling can be used. Glass-box methods decompose P (˜ e|˜f ) ˜ into a set of lexical distributions P (e|f ). For instance, it was suggested to use IBM-1 probabilities (Och et al., 2004), or other lexical translation probabilities (Koehn et al., 2003; Zens and Ney, 2004). Some form of glass-box smoothing is now used in all state-of-the-art statistical machine translation sy"
D07-1045,2003.mtsummit-tttt.3,0,0.0660512,"ware of one work that performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). Two types of phrase-table smoothing were compared: black-box and glass-box methods. Black-methods do not look inside phrases but instead treat them as atomic objects. By these means, all the methods developed for language modeling can be used. Glass-box methods decompose P (˜ e|˜f ) ˜ into a set of lexical distributions P (e|f ). For instance, it was suggested to use IBM-1 probabilities (Och et al., 2004), or other lexical translation probabilities (Koehn et al., 2003; Zens and Ney, 2004). Some form of glass-box smoothing is now used in all state-of-the-art statistical machine translation systems. Another approach related to phrase table smoothing is the so-called N-gram translation model (Mari˜no et al., 2006). In this model, bilingual tuples are used instead of the phrase pairs and n-gram probabilities are considered rather than relative frequencies. Therefore, smoothing is obtained using the standard techniques developed for language modeling. In addition, a context dependence of the phrases is introduced. On the other hand, some restrictions on the seg"
D07-1045,P07-2045,0,0.00408354,"ion task (over 40 BLEU percentage). Using the continuous space model for the translation and target language model, an improvement of 2.5 BLEU on the development data and 1.5 BLEU on the test data was observed. Despite these encouraging results, we believe that additional research on improved estimation of probabilities in N-gram- or phrase-based statistical machine translation systems is needed. In particular, the problem of generalization to new translations seems to be promising to us. This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al., 2007). In this approach words are decomposed into several factors. These factors are trans437 lated and a target phrase is generated. This model could be complemented by a factored continuous tuple N-gram. Factored word language models were already successfully used in speech recognition (Bilmes and Kirchhoff, 2003; Alexandrescu and Kirchhoff, 2006) and an extension to machine translation seems to be promising. The described smoothing method was explicitly developed to tackle the data sparseness problem in tasks like the B TEC corpus. It is well known from language modeling that careful smoothing i"
D07-1045,P02-1038,0,0.0206542,") is to produce a target sentence e from a source sentence f . Among all possible target language sentences the one with the highest probability is chosen: e∗ = arg max Pr(e|f ) = arg max Pr(f |e) Pr(e) e X = arg max{exp( e where Pr(f |e) is the translation model and Pr(e) is the target language model. This approach is usually referred to as the noisy source-channel approach in statistical machine translation (Brown et al., 1993). λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase translation probabilities P (˜ e|˜f ) and P (˜f |˜ e) are usually obtained using relative frequency estimates. Statistical learning theory, however, tells us that relative frequency estimates have several drawbacks, in particular high variance and low bias. Phrase tables may contain several millions of entries, most of which appear only once or twice, which means that we are confronted with a data sparseness problem. Surprisingly, there seems to be little work addressing the issue of smoothing of the phrase table probabilities. On the other hand, smoothing of relative frequency es"
D07-1045,W99-0604,0,0.0167882,"Missing"
D07-1045,N04-1021,0,0.0338653,"est matching phrase pair among the existing ones. We are only aware of one work that performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). Two types of phrase-table smoothing were compared: black-box and glass-box methods. Black-methods do not look inside phrases but instead treat them as atomic objects. By these means, all the methods developed for language modeling can be used. Glass-box methods decompose P (˜ e|˜f ) ˜ into a set of lexical distributions P (e|f ). For instance, it was suggested to use IBM-1 probabilities (Och et al., 2004), or other lexical translation probabilities (Koehn et al., 2003; Zens and Ney, 2004). Some form of glass-box smoothing is now used in all state-of-the-art statistical machine translation systems. Another approach related to phrase table smoothing is the so-called N-gram translation model (Mari˜no et al., 2006). In this model, bilingual tuples are used instead of the phrase pairs and n-gram probabilities are considered rather than relative frequencies. Therefore, smoothing is obtained using the standard techniques developed for language modeling. In addition, a context dependence of the phrase"
D07-1045,2006.iwslt-papers.2,1,0.869281,"Missing"
D07-1045,takezawa-etal-2002-toward,0,0.016304,"den layer (200 to 500). Therefore, in previous applications of the continuous space n-gram model, the output was limited to the s most frequent units, s ranging between 2k and 12k (Schwenk, 2007). This is called a short-list. Train (bitexts) Dev Eval Sents 20k 489 500 Words 155.4/166.3k 5.2k 6k 4 Experimental Evaluation In this work we report results on the Basic Traveling Expression Corpus (B TEC) as used in the 2006 evaluations of the international workshop on spoken language translation (I WSLT). This corpus consists of typical sentences from phrase books for tourists in several languages (Takezawa et al., 2002). We report results on the supplied development corpus of 489 sentences and the official test set of the I WSLT’06 evaluation. The main measure is the BLEU score, using seven reference translations. The scoring is case insensitive and punctuations are ignored. Details on the available data are summarized in Table 1. We concentrated first on the translation from Italian to English. All participants in the I WSLT evaluation achieved much better performances for this language pair than for the other considered translation directions. This makes it more difficult to achieve additional improvements"
D07-1045,N04-1033,0,0.0469216,"t performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). Two types of phrase-table smoothing were compared: black-box and glass-box methods. Black-methods do not look inside phrases but instead treat them as atomic objects. By these means, all the methods developed for language modeling can be used. Glass-box methods decompose P (˜ e|˜f ) ˜ into a set of lexical distributions P (e|f ). For instance, it was suggested to use IBM-1 probabilities (Och et al., 2004), or other lexical translation probabilities (Koehn et al., 2003; Zens and Ney, 2004). Some form of glass-box smoothing is now used in all state-of-the-art statistical machine translation systems. Another approach related to phrase table smoothing is the so-called N-gram translation model (Mari˜no et al., 2006). In this model, bilingual tuples are used instead of the phrase pairs and n-gram probabilities are considered rather than relative frequencies. Therefore, smoothing is obtained using the standard techniques developed for language modeling. In addition, a context dependence of the phrases is introduced. On the other hand, some restrictions on the segmentation of the sour"
D07-1045,2005.mtsummit-papers.36,1,\N,Missing
E09-1049,A00-1031,0,0.0697185,"Missing"
E09-1049,J06-4004,1,0.93598,"Missing"
E09-1049,A00-2018,0,0.102428,"ppeared (Mariño et al., 2006). It stemms from 1 2 www.statmt.org/moses/ www.cs.cmu.edu/∼zollmann/samt Proceedings of the 12th Conference of the European Chapter of the ACL, pages 424–432, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 424 Koehn et al. (2003), and word alignment, which is generated by the grow-diag-final-and method (expanding the alignment by adding directly neighboring alignment points and alignment points in the diagonal neighborhood) (Och and Ney, 2003). Meanwhile, the target of the training corpus is parsed with Charniak’s parser (Charniak, 2000), and each phrase is annotated with the constituent that spans the target side of the rules. The set of non-terminals is extended by means of conditional and additive categories according to Combinatory Categorical Grammar (CCG) (Steedman, 1999). Under this approach, new rules can be formed. For example, RB+VB, can represent an additive constituent consisting of two synthetically generated adjacent categories 3 , i.e., an adverb and a verb. Furthermore, DTNP can indicate an incomplete noun phrase with a missing determiner to the left. The rule recursive generalization procedure coincides with"
E09-1049,P04-1083,0,0.012174,"s approach deals with bilingual units, called tuples, which are composed of one or more words from the source language and zero or more words from the target one. The N -gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization. Prior to the SMT revolution, a major part of MT systems was developed using rule-based algorithms; however, starting from the 1990’s, syntax-driven systems based on phrase hierarchy have gained popularity. A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and nonisomorphic tree-to-tree mappings (Eisner, 2003). The orthodox phrase-based model was enhanced in Chiang (2005), where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences betwee"
E09-1049,P05-1033,0,0.267731,"ms allow for linguistically motivated word reordering by implementing word order monotonization. Prior to the SMT revolution, a major part of MT systems was developed using rule-based algorithms; however, starting from the 1990’s, syntax-driven systems based on phrase hierarchy have gained popularity. A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and nonisomorphic tree-to-tree mappings (Eisner, 2003). The orthodox phrase-based model was enhanced in Chiang (2005), where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few years. In Crego et al. (2005b), the N -gram-based system is contrasted with a state-of-the-art phrase-based framework,"
E09-1049,P02-1038,0,0.0750674,"enalty model that is used to compensate for the system’s preference for short output sentences; (1) o p(f1J |eI1 ) · p(eI1 ) (2) • source-to-target and target-to-source lexicon models as shown in Och and Ney (2004)). where I and J represent the number of words in the target and source languages, respectively. Modern state-of-the-art SMT systems operate with the bilingual units extracted from the parallel corpus based on word-to-word alignment. They are enhanced by the maximum entropy approach and the posterior probability is calculated as a loglinear combination of a set of feature functions (Och and Ney, 2002). Using this technique, the additional models are combined to determine the translation hypothesis, as shown in (3): 3.3 Extended word reordering where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. An extended monotone distortion model based on the automatically learned reordering rules was implemented as described in Crego and Mariño (2006). Based on the word-to-word alignment, tuples were extracted by an unfolding technique. As a result, the tuples were broken into smaller tuples, and these were sequenced in the ord"
E09-1049,J03-1002,0,0.0107441,"system1 (Koehn et al., 2007). In parallel to the phrasebased approach, the N -gram-based approach appeared (Mariño et al., 2006). It stemms from 1 2 www.statmt.org/moses/ www.cs.cmu.edu/∼zollmann/samt Proceedings of the 12th Conference of the European Chapter of the ACL, pages 424–432, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 424 Koehn et al. (2003), and word alignment, which is generated by the grow-diag-final-and method (expanding the alignment by adding directly neighboring alignment points and alignment points in the diagonal neighborhood) (Och and Ney, 2003). Meanwhile, the target of the training corpus is parsed with Charniak’s parser (Charniak, 2000), and each phrase is annotated with the constituent that spans the target side of the rules. The set of non-terminals is extended by means of conditional and additive categories according to Combinatory Categorical Grammar (CCG) (Steedman, 1999). Under this approach, new rules can be formed. For example, RB+VB, can represent an additive constituent consisting of two synthetically generated adjacent categories 3 , i.e., an adverb and a verb. Furthermore, DTNP can indicate an incomplete noun phrase w"
E09-1049,2005.iwslt-1.23,1,0.949935,"Missing"
E09-1049,J04-4002,0,0.849615,"this study, we intend to compare the differences and similarities of the statistical N -grambased SMT approach and the SAMT system. The comparison is performed on a small Arabic-toEnglish translation task from the news domain. 2 SAMT system A criticism of phrase-based models is data sparseness. This problem is even more serious when the source, the target, or both languages are inflectional and rich in morphology. Moreover, phrasebased models are unable to cope with global reordering because the distortion model is based on movement distance, which may face computational resource limitations (Och and Ney, 2004). This problem was successfully addressed when the MT system based on generalized hierarchically structured phrases was introduced and discussed in Chiang (2005). It operates with only two markers (a substantial phrase category and ""a glue marker""). Moreover, a recent work (Zollmann and Venugopal, 2006) reports significant improvement in terms of translation quality if complete or partial syntactic categories (derived from the target side parse tree) are assigned to the phrases. N −→ f1 . . . fm /e1 . . . en can be extended by another existing rule 2.1 Modeling M −→ fi . . . fu /ej . . . ev A"
E09-1049,W06-3101,0,0.179067,"Missing"
E09-1049,D07-1079,0,0.0705047,"hrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few years. In Crego et al. (2005b), the N -gram-based system is contrasted with a state-of-the-art phrase-based framework, while in DeNeefe et al. (2007), the authors seek to estimate the advantages, weakest points and possible overlap between syntaxbased MT and phrase-based SMT. In Zollmann et al. (2008) the comparison of phrase-based , ""Chiang’s style"" hirearchical system and SAMT is proIn this paper we compare and contrast two approaches to Machine Translation (MT): the CMU-UKA Syntax Augmented Machine Translation system (SAMT) and UPC-TALP N-gram-based Statistical Machine Translation (SMT). SAMT is a hierarchical syntax-driven translation system underlain by a phrase-based model and a target part parse tree. In N-gram-based SMT, the transl"
E09-1049,P99-1039,0,0.0244718,"on for Computational Linguistics 424 Koehn et al. (2003), and word alignment, which is generated by the grow-diag-final-and method (expanding the alignment by adding directly neighboring alignment points and alignment points in the diagonal neighborhood) (Och and Ney, 2003). Meanwhile, the target of the training corpus is parsed with Charniak’s parser (Charniak, 2000), and each phrase is annotated with the constituent that spans the target side of the rules. The set of non-terminals is extended by means of conditional and additive categories according to Combinatory Categorical Grammar (CCG) (Steedman, 1999). Under this approach, new rules can be formed. For example, RB+VB, can represent an additive constituent consisting of two synthetically generated adjacent categories 3 , i.e., an adverb and a verb. Furthermore, DTNP can indicate an incomplete noun phrase with a missing determiner to the left. The rule recursive generalization procedure coincides with the one proposed in Chiang (2005), but violates the restrictions introduced for singlecategory grammar; for example, rules that contain adjacent generalized elements are not discarded. Thus, each rule vided. In this study, we intend to compare"
E09-1049,N07-1063,0,0.0827557,"hrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few years. In Crego et al. (2005b), the N -gram-based system is contrasted with a state-of-the-art phrase-based framework, while in DeNeefe et al. (2007), the authors seek to estimate the advantages, weakest points and possible overlap between syntaxbased MT and phrase-based SMT. In Zollmann et al. (2008) the comparison of phrase-based , ""Chiang’s style"" hirearchical system and SAMT is proIn this paper we compare and contrast two approaches to Machine Translation (MT): the CMU-UKA Syntax Augmented Machine Translation system (SAMT) and UPC-TALP N-gram-based Statistical Machine Translation (SMT). SAMT is a hierarchical syntax-driven translation system underlain by a phrase-based model and a target part parse tree. In N-gram-based SMT, the transl"
E09-1049,P03-2041,0,0.0185431,"zero or more words from the target one. The N -gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization. Prior to the SMT revolution, a major part of MT systems was developed using rule-based algorithms; however, starting from the 1990’s, syntax-driven systems based on phrase hierarchy have gained popularity. A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and nonisomorphic tree-to-tree mappings (Eisner, 2003). The orthodox phrase-based model was enhanced in Chiang (2005), where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few years. In Crego et al. (2005b), the N -gram-based system"
E09-1049,vilar-etal-2006-error,0,0.202918,"Missing"
E09-1049,N06-2013,0,0.0278002,"word alignment is automatically computed by using GIZA++ (Och and Ney, 2004) in both directions, which are made symmetric by using the grow-diag-final-and operation. The experiments were done on a dual-processor Pentium IV Intel Xeon Quad Core X5355 2.66 GHz machine with 24 G of RAM. All computational times and memory size results are approximated. 4.2 Arabic data preprocessing Arabic is a VSO (SVO in some cases) prodrop language with rich templatic morphology, where words are made up of roots and affixes and clitics agglutinate to words. For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. For disambiguation, only diacritic unigram statistics were employed. For tokenization, the D3 scheme with -TAGBIES option was used. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. Step Parsing Rules extraction Filtering&merging Weights tuning Testing Time Memory 1.5h 10h 3h 40h 2h 80Mb 3.5Gb 4.0Gb 3Gb 3Gb Table 2: SAMT: Computational resources. Evaluation scores including results of system combi"
E09-1049,P01-1067,0,0.0698137,"ch are composed of one or more words from the source language and zero or more words from the target one. The N -gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization. Prior to the SMT revolution, a major part of MT systems was developed using rule-based algorithms; however, starting from the 1990’s, syntax-driven systems based on phrase hierarchy have gained popularity. A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and nonisomorphic tree-to-tree mappings (Eisner, 2003). The orthodox phrase-based model was enhanced in Chiang (2005), where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few y"
E09-1049,N03-1017,0,0.0214179,"Missing"
E09-1049,W06-3119,0,0.494674,"was developed using rule-based algorithms; however, starting from the 1990’s, syntax-driven systems based on phrase hierarchy have gained popularity. A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and nonisomorphic tree-to-tree mappings (Eisner, 2003). The orthodox phrase-based model was enhanced in Chiang (2005), where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few years. In Crego et al. (2005b), the N -gram-based system is contrasted with a state-of-the-art phrase-based framework, while in DeNeefe et al. (2007), the authors seek to estimate the advantages, weakest points and possible overlap between syntaxbased MT and phrase-based SMT. In Zollmann"
E09-1049,P07-2045,0,0.00619513,"ranslation task (1.5M tokens in the training corpus). Human error analysis clarifies advantages and disadvantages of the systems under consideration. Finally, we combine the output of both systems to yield significant improvements in translation quality. 1 Introduction There is an ongoing controversy regarding whether or not information about the syntax of language can benefit MT or contribute to a hybrid system. Classical IBM word-based models were recently augmented with a phrase translation capability, as shown in Koehn et al. (2003), or in more recent implementation, the MOSES MT system1 (Koehn et al., 2007). In parallel to the phrasebased approach, the N -gram-based approach appeared (Mariño et al., 2006). It stemms from 1 2 www.statmt.org/moses/ www.cs.cmu.edu/∼zollmann/samt Proceedings of the 12th Conference of the European Chapter of the ACL, pages 424–432, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 424 Koehn et al. (2003), and word alignment, which is generated by the grow-diag-final-and method (expanding the alignment by adding directly neighboring alignment points and alignment points in the diagonal neighborhood) (Och and Ney, 2003). Meanwhil"
E09-1049,C08-1144,0,0.0509788,"l, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few years. In Crego et al. (2005b), the N -gram-based system is contrasted with a state-of-the-art phrase-based framework, while in DeNeefe et al. (2007), the authors seek to estimate the advantages, weakest points and possible overlap between syntaxbased MT and phrase-based SMT. In Zollmann et al. (2008) the comparison of phrase-based , ""Chiang’s style"" hirearchical system and SAMT is proIn this paper we compare and contrast two approaches to Machine Translation (MT): the CMU-UKA Syntax Augmented Machine Translation system (SAMT) and UPC-TALP N-gram-based Statistical Machine Translation (SMT). SAMT is a hierarchical syntax-driven translation system underlain by a phrase-based model and a target part parse tree. In N-gram-based SMT, the translation process is based on bilingual units related to word-to-word alignment and statistical modeling of the bilingual context following a maximumentropy"
E09-1049,W04-3250,0,0.193171,"Missing"
E09-1049,N04-1022,0,0.121499,"Missing"
E09-1049,W02-1018,0,0.0475634,"asted with the SAMT chunk-based rules construction, is presented in Figure 1. The reordering strategy is additionally supported by a 4-gram LM of reordered source POS tags. In training, POS tags are reordered according to the extracted reordering patterns and word-toword links. The resulting sequence of source POS tags is used to train the n-gram LM. 3.1 N-gram-based translation system 3.4 The N -gram approach to SMT is considered to be an alternative to the phrase-based translation, where a given source word sequence is decomposed into monolingual phrases that are then translated one by one (Marcu and Wong, 2002). The N -gram-based approach regards translation as a stochastic process that maximizes the joint probability p(f, e), leading to a decomposition based on bilingual n-grams. The core part of the system constructed in this way is a translation model (TM), which is based on bilingual units, called tuples, that are extracted from a word alignment (performed with GIZA++ tool4 ) according to certain constraints. A bilingual TM actually constitutes an n-gram LM of tuples, which approximates the joint probability between the languages under consideration and can be seen here as a LM, where the langua"
J06-4004,W05-0823,1,0.838951,"Missing"
J06-4004,W00-0508,0,0.0142428,"y approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state perspective—more specifically, from"
J06-4004,J96-1002,0,0.02988,"Missing"
J06-4004,J90-2002,0,0.81424,"nslation was conceived as the problem of finding a sentence by decoding a given “encrypted” version of it (Weaver 1955). Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time (Hutchins 1986). Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data. The first SMT systems were developed in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions. For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora (Brown et al. 1993). In the case of target"
J06-4004,J93-2003,0,0.0556776,"d in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions. For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora (Brown et al. 1993). In the case of target language probabilities, these were generally trained from monolingual data by using n-grams. Present SMT systems have evolved from the original ones in such a way that mainly differ from them in two respects: first, word-based translation models have been ∗ Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 replaced by phrase-b"
J06-4004,J04-2004,0,0.856987,"models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state perspective—more specifically, from the work of Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier work the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilin˜ 2002). An alternagual units, which will be referred to as tuples (de Gispert and Marino tive approach, which relies on bilingual-unit un"
J06-4004,N04-1033,0,0.0140655,"Missing"
J06-4004,2005.iwslt-1.23,1,0.883592,"Missing"
J06-4004,2005.mtsummit-papers.37,1,0.855856,"Missing"
J06-4004,2006.amta-papers.4,1,0.763242,"Missing"
J06-4004,P05-2012,1,0.804737,"Missing"
J06-4004,C86-1155,0,0.079146,"ament Plenary Sessions (EPPS). 1. Introduction The beginnings of statistical machine translation (SMT) can be traced back to the early fifties, closely related to the ideas from which information theory arose (Shannon and Weaver 1949) and inspired by works on cryptography (Shannon 1949, 1951) during World War II. According to this view, machine translation was conceived as the problem of finding a sentence by decoding a given “encrypted” version of it (Weaver 1955). Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time (Hutchins 1986). Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data. The first SMT systems were developed in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for"
J06-4004,knight-al-onaizan-1998-translation,0,0.230855,"more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state persp"
J06-4004,N03-1017,0,0.0258625,"Missing"
J06-4004,2005.mtsummit-papers.36,1,0.177804,"Missing"
J06-4004,P00-1056,0,0.0440511,"tence pairs are removed from the training data to allow for a better performance of the alignment tool. Sentence pairs are removed according to the following two criteria: r r Fertility filtering: removes sentence pairs with a word ratio larger than a predefined threshold value. Length filtering: removes sentence pairs with at least one sentence of more than 100 words in length. This helps to maintain bounded alignment computational times. After preprocessing, word-to-word alignments are performed in both directions, source-to-target and target-to-source. In our system implementation, GIZA++ (Och and Ney 2000) is used for computing the alignments. A total of five iterations for models IBM-1 and HMM, and three iterations for models IBM-3 and IBM-4, are performed. Then, the obtained alignment sets are used for computing the intersection and the union of alignments from which tuples and embedded-word tuples are extracted, respectively. 4.2.2 Tuple Extraction and Pruning. A tuple set for each translation direction is extracted from the union set of alignments while avoiding source-nulled tuples by using the procedure described in Section 2.2.2. Then, the resulting tuple vocabularies are pruned accordin"
J06-4004,P02-1038,0,0.884384,"034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003) which are directly estimated from aligned bilingual corpora by considering relative frequencies, and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitesta"
J06-4004,J03-1002,0,0.00706932,"gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3. Both translation directions, Spanish to English (ES → EN) and English to Spanish (EN → ES), are considered in each table. In the case of Table 2, model size and translation accuracy are evaluated against the type of alignment set used for extracting tuples. Three different alignment sets are considered: source-to-target, the union of source-to-target and target-to-source, and the “refined” alignment method described by Och and Ney (2003). For the results presented in Table 2, a pruning parameter value of N = 20 was used for the Spanish-to-English direction, while a value of N = 30 was used for the English-to-Spanish direction. As can be clearly seen in Table 2, the union alignment set happens to be the most favorable one for extracting tuples in both translation directions since it provides a significantly better translation accuracy, in terms of BLEU score, than the other two alignment sets considered. Notice also in Table 2 that the union set is the one providing the smallest model sizes according to the number of bigrams a"
J06-4004,P02-1040,0,0.105596,"alignment sets. Notice that BLEU measurements in this table correspond to translations computed by using the tuple n-gram model alone. Direction Alignment set Tuple voc. Bigrams Trigrams BLEU ES → EN Source-to-target union refined Source-to-target union refined 1.920 2.040 2.111 1.813 2.023 2.081 6.426 6.009 6.851 6.263 6.092 6.920 2.353 1.798 2.398 2.268 1.747 2.323 0.4424 0.4745 0.4594 0.4152 0.4276 0.4193 EN → ES when tuples are extracted from different alignment sets and when different pruning parameters are used, respectively. Translation accuracy is measured in terms of the BLEU score (Papineni et al. 2002), which is computed here for translations generated by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3. Both translation directions, Spanish to English (ES → EN) and English to Spanish (EN → ES), are considered in each table. In the case of Table 2, model size and translation accuracy are evaluated against the type of alignment set used for extracting tuples. Three different alignment sets are considered: source-to-target, the union of source-to-targ"
J06-4004,N03-2036,0,0.00459439,"k the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilin˜ 2002). An alternagual units, which will be referred to as tuples (de Gispert and Marino tive approach, which relies on bilingual-unit unigram probabilities, was developed by Tillmann and Xia (2003); in contrast, the approach presented here considers bilingualunit n-gram probabilities. In addition to the tuple n-gram translation model, the translation system presented here implements four specific feature functions that are log-linearly combined along with the translation model for performing the decoding ˜ et al. 2005). (Marino This article is intended to provide a detailed description of the n-gram-based translation system, as well as to demonstrate the system performance in a widedomain, large-vocabulary translation task. The article is structured as follows. First, Section 2 presents"
J06-4004,2002.tmi-tutorials.2,0,0.201063,"Missing"
J06-4004,2004.iwslt-evaluation.14,1,\N,Missing
J06-4004,N04-1021,0,\N,Missing
khalilov-etal-2010-towards,E09-1049,1,\N,Missing
khalilov-etal-2010-towards,N04-4026,0,\N,Missing
khalilov-etal-2010-towards,E99-1010,0,\N,Missing
khalilov-etal-2010-towards,C96-1094,0,\N,Missing
khalilov-etal-2010-towards,J90-2002,0,\N,Missing
khalilov-etal-2010-towards,W02-0706,0,\N,Missing
khalilov-etal-2010-towards,J04-4002,0,\N,Missing
khalilov-etal-2010-towards,C08-1144,0,\N,Missing
khalilov-etal-2010-towards,P07-2045,0,\N,Missing
khalilov-etal-2010-towards,U07-1019,0,\N,Missing
khalilov-etal-2010-towards,J06-4004,1,\N,Missing
khalilov-etal-2010-towards,P02-1038,0,\N,Missing
khalilov-etal-2010-towards,J03-1002,0,\N,Missing
khalilov-etal-2010-towards,2005.iwslt-1.23,1,\N,Missing
khalilov-etal-2010-towards,vilar-etal-2006-error,0,\N,Missing
khalilov-etal-2010-towards,2009.mtsummit-papers.7,0,\N,Missing
N07-2035,2006.iwslt-evaluation.18,1,0.890672,"Missing"
N07-2035,2005.iwslt-1.23,1,0.905555,"Missing"
N07-2035,J06-4004,1,0.895476,"Missing"
N07-2035,E06-1005,1,0.799617,"ingual N -gram language model. In the phrase-based model, no monotonicity restriction is imposed on the segmentation and the probabilities are normally estimated simply by relative frequencies. This paper extends the analysis of both systems performed in (Crego et al., 2005a) by additionally performing a manual error analysis of both systems, which were the ones used by UPC and RWTH in the last Tc-Star evaluation. Furthermore, we will propose a way to combine both systems in order to improve the quality of translations. Experiments combining several kinds of MT systems have been presented in (Matusov et al., 2006), based only on the single best output of each system. Recently, a more straightforward approach of both systems has been performed in (Costa-juss` a et al., 2006) which simply selects, for each sentence, one of the provided hypotheses. This paper is organized as follows. In section 2, we briefly describe the phrase and the N -gram-based baseline systems. In the next section we present the evaluation framework. In Section 4 we report a structural comparison performed for both systems and, afterwards, in Section 5, we analyze the errors of both systems. Finally, in the last two sections we resc"
N07-2035,vilar-etal-2006-error,1,0.861806,"Missing"
N07-2035,N04-1033,1,0.812475,"n N -gram-based one. The exhaustive analysis includes a comparison of the translation models in terms of efficiency (number of translation units used in the search and computational time) and an examination of the errors in each system’s output. Additionally, we combine both systems, showing accuracy improvements. 1 Introduction Statistical machine translation (SMT) has evolved from the initial word-based translation models to more advanced models that take the context surrounding the words into account. The so-called phrase-based and N -gram-based models are two examples of these approaches (Zens and Ney, 2004; Mari˜ no et al., 2006). In current state-of-the-art SMT systems, the phrase-based or the N -gram-based models are usually the main features in a log-linear framework, reminiscent of the maximum entropy modeling approach. Two basic issues differentiate the N -gram-based system from the phrase-based one: the training data is sequentially segmented into bilingual units; and the probability of these units is estimated as a bilingual N -gram language model. In the phrase-based model, no monotonicity restriction is imposed on the segmentation and the probabilities are normally estimated simply by"
N07-2035,W06-3120,1,\N,Missing
P16-2058,D15-1041,0,0.219831,"cture has changed in that we are using a convolutional neural network (CNN) and a highway network over characters before the attention-based mechanism of the encoder. This is a significant difference from previous work (Sennrich et al., 2015) which uses the neural MT architecture from (Bahdanau et al., 2015) without modification to deal with subword units (but not including unigram characters). Subword-based representations have already been explored in Natural Language Processing (NLP), e.g. for POS tagging (Santos and Zadrozny, 2014), name entity recognition (Santos and aes, 2015), parsing (Ballesteros et al., 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al., 2015). These previous works show different advantages of using character-level information. In our case, with the new characterNeural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard look"
P16-2058,D15-1176,0,0.136834,"ch word tj is predicted based on a recurrent hidden state, the previously predicted word tj−1 , and a context vector. This context vector is obtained from the weighted sum of the annotations hk , which in turn, is computed through an alignment model αjk (a feedforward neural network). This neural MT approach has achieved competitive results against the standard phrase-based system in the WMT 2015 evaluation (Jean et al., 2015). based neural MT architecture, we take advantage of intra-word information, which is proven to be extremely useful in other NLP applications (Santos and Zadrozny, 2014; Ling et al., 2015a), especially when dealing with morphologically rich languages. When using the character-based source word embeddings in MT, there ceases to be unknown words in the source input, while the size of the target vocabulary remains unchanged. Although the target vocabulary continues with the same limitation as in the standard neural MT system, the fact that there are no unknown words in the source helps to reduce the number of unknowns in the target. Moreover, the remaining unknown target words can now be more successfully replaced with the corresponding source-aligned words. As a consequence, we"
P16-2058,W14-4012,0,0.240981,"Missing"
P16-2058,P16-2058,1,0.106153,"Missing"
P16-2058,W15-3904,0,0.0272269,"ponding word. The system architecture has changed in that we are using a convolutional neural network (CNN) and a highway network over characters before the attention-based mechanism of the encoder. This is a significant difference from previous work (Sennrich et al., 2015) which uses the neural MT architecture from (Bahdanau et al., 2015) without modification to deal with subword units (but not including unigram characters). Subword-based representations have already been explored in Natural Language Processing (NLP), e.g. for POS tagging (Santos and Zadrozny, 2014), name entity recognition (Santos and aes, 2015), parsing (Ballesteros et al., 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al., 2015). These previous works show different advantages of using character-level information. In our case, with the new characterNeural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway"
P16-2058,P14-2111,0,0.0280027,"onvolutional neural network (CNN) and a highway network over characters before the attention-based mechanism of the encoder. This is a significant difference from previous work (Sennrich et al., 2015) which uses the neural MT architecture from (Bahdanau et al., 2015) without modification to deal with subword units (but not including unigram characters). Subword-based representations have already been explored in Natural Language Processing (NLP), e.g. for POS tagging (Santos and Zadrozny, 2014), name entity recognition (Santos and aes, 2015), parsing (Ballesteros et al., 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al., 2015). These previous works show different advantages of using character-level information. In our case, with the new characterNeural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. T"
P16-2058,P16-1160,0,0.347901,"out of the eyes where officials lose sight of it causing the officers to lose sight of it Table 2: Translation examples. Phrase NN NN+Src CHAR CHAR+Src De-&gt;En 20.99 18.83 20.64 21.40 22.10 En-&gt;De 17.04 16.47 17.15 19.53 20.22 tation in vocabulary size. In this paper we have proposed a modification to the standard encoder/decoder neural MT architecture to use unlimited-vocabulary character-based source word embeddings. The improvement in BLEU is about 1.5 points in German-to-English and more than 3 points in English-to-German. As further work, we are currently studying different alternatives (Chung et al., 2016) to extend the character-based approach to the target side of the neural MT system. Table 3: De-En BLEU results. number of out-of-vocabulary words of the test set is shown in Table 1. The character-based embedding has an impact in learning a better translation model at various levels, which seems to include better alignment, reordering, morphological generation and disambiguation. Table 2 shows some examples of the kind of improvements that the character-based neural MT system is capable of achieving compared to baseline systems. Examples 1 and 2 show how the reduction of source unknowns impro"
P16-2058,W15-3014,0,0.0250557,"ociation for Computational Linguistics, pages 357–361, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics sentences. The decoder also becomes a GRU and each word tj is predicted based on a recurrent hidden state, the previously predicted word tj−1 , and a context vector. This context vector is obtained from the weighted sum of the annotations hk , which in turn, is computed through an alignment model αjk (a feedforward neural network). This neural MT approach has achieved competitive results against the standard phrase-based system in the WMT 2015 evaluation (Jean et al., 2015). based neural MT architecture, we take advantage of intra-word information, which is proven to be extremely useful in other NLP applications (Santos and Zadrozny, 2014; Ling et al., 2015a), especially when dealing with morphologically rich languages. When using the character-based source word embeddings in MT, there ceases to be unknown words in the source input, while the size of the target vocabulary remains unchanged. Although the target vocabulary continues with the same limitation as in the standard neural MT system, the fact that there are no unknown words in the source helps to reduce"
P16-2058,D13-1176,0,0.0623351,"e proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task. 1 Introduction Machine Translation (MT) is the set of algorithms that aim at transforming a source language into a target language. For the last 20 years, one of the most popular approaches has been statistical phrase-based MT, which uses a combination of features to maximise the probability of the target sentence given the source sentence (Koehn et al., 2003). Just recently, the neural MT approach has appeared (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) and obtained state-of-the-art results. Among its different strengths neural MT does not need to pre-design feature functions beforehand; optimizes the entire system at once because 357 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 357–361, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics sentences. The decoder also becomes a GRU and each word tj is predicted based on a recurrent hidden state, the previously predicted word tj−1 , and a context v"
P16-2058,N03-1017,0,0.00982693,"MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task. 1 Introduction Machine Translation (MT) is the set of algorithms that aim at transforming a source language into a target language. For the last 20 years, one of the most popular approaches has been statistical phrase-based MT, which uses a combination of features to maximise the probability of the target sentence given the source sentence (Koehn et al., 2003). Just recently, the neural MT approach has appeared (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) and obtained state-of-the-art results. Among its different strengths neural MT does not need to pre-design feature functions beforehand; optimizes the entire system at once because 357 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 357–361, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics sentences. The decoder also becomes a GRU and each word tj is predicted bas"
P16-2058,P07-2045,0,0.00662165,"rd embedding In the target size we are still limited in vocabulary by the softmax layer at the output of the network and we kept the standard target word embeddings in our experiments. However, the results seem to show that the affix-aware representation of the source words has a positive influence on all the components of the network. The global optimization of the integrated model forces the translation model and the internal vector representation of the target words to follow the affix-aware codification of the source words. 4 Baseline systems The phrase-based system was built using Moses (Koehn et al., 2007), with standard parameters such as grow-final-diag for alignment, GoodTuring smoothing of the relative frequencies, 5gram language modeling using Kneser-Ney discounting, and lexicalized reordering, among others. The neural-based system was built using the software from DL4MT2 available in github. We generally used settings from previous work (Jean et al., 2015): networks have an embedding of 620 and a dimension of 1024, a batch size of 32, and no dropout. We used a vocabulary size of 90 thousand words in German-English. Also, as proposed in (Jean et al., 2015) we replaced unknown words (UNKs)"
P16-2058,P16-1100,0,\N,Missing
P19-2033,D16-1163,0,0.0564736,"Missing"
P19-2033,P19-1120,0,0.0203202,"X and Y , our objective is to train independent encoders and decoders for each language, ex , dx and ey , dy that produce compatible sentence representations h(x), h(y). For instance, given a sentence x in language X, we can obtain a representation h(x) from that the encoder ex that can be used to either generate a sentence reconstruction using decoder dx or a translation using decoder dy . With this objective in mind, we propose a training schedule that combines two tasks (auto-encoding and translation) and the two translation directions simultaneously by optimizing the following loss: 2016; Kim et al., 2019) to improve the performance of new translation directions by taking benefit of the information of a previous model. These approaches are particularly useful in low resources scenarios when a previous model trained with orders of magnitude more examples is available. This paper proposes a proof of concept of a new multilingual NMT approach. The current approach is based on joint training without parameter or vocabulary sharing by enforcing a compatible representation between the jointly trained languages and using multitask learning (Dong et al., 2015). This approach is shown to offer a scalabl"
P19-2033,2005.mtsummit-papers.11,0,0.222987,"language is required to perform the translation, once the added modules are trained the zero-shot translation is performed without generating the language used for training as the sentence representations in the shared space are compatible with all the modules in the system. A current limitation is the need to use the same vocabulary for the shared language (X) in both training steps. The use of subwords (Sennrich et al., 2015) mitigates the impact of this constraint. 5 Data and Implementation Experiments are conducted using data extracted from the UN (Ziemski et al., 2016) and EPPS datasets (Koehn, 2005) that provide 15 million parallel sentences between English and Spanish, German and French. newstest2012 and new238 System Baseline Joint Added lang ES-EN 32.60 29.70 - EN-ES 32.90 30.74 - FR-EN 31.81 30.93 DE-EN 28.96 27.63 stest2013 were used as validation and test sets, respectively. These sets provide parallel data between the four languages that allow for zero-shot evaluation. Preprocessing consisted of a pipeline of punctuation normalization, tokenization, corpus filtering of longer sentences than 80 words and true-casing. These steps were performed using the scripts available from Moses"
P19-2033,P07-2045,0,0.00799226,"hat provide 15 million parallel sentences between English and Spanish, German and French. newstest2012 and new238 System Baseline Joint Added lang ES-EN 32.60 29.70 - EN-ES 32.90 30.74 - FR-EN 31.81 30.93 DE-EN 28.96 27.63 stest2013 were used as validation and test sets, respectively. These sets provide parallel data between the four languages that allow for zero-shot evaluation. Preprocessing consisted of a pipeline of punctuation normalization, tokenization, corpus filtering of longer sentences than 80 words and true-casing. These steps were performed using the scripts available from Moses (Koehn et al., 2007). Preprocessed data is later tokenized into BPE subwords (Sennrich et al., 2015) with a vocabulary size of 32000 tokens. We ensure that the vocabularies are independent and reusable when new languages were added by creating vocabularies monolingually, i.e. without having access to other languages during the code generation. Our second experiment consists of incrementally adding different languages to the system, in this case, German and French. Note that, since we freeze the weights while adding the new language, the order in which we add new languages does not have any impact on performance."
P19-2033,P15-1166,0,\N,Missing
P19-2033,Q17-1024,0,\N,Missing
P19-2033,L16-1561,0,\N,Missing
P19-2033,W17-2619,0,\N,Missing
P19-2033,D19-3026,1,\N,Missing
P19-2033,P16-1162,0,\N,Missing
W05-0827,J90-2002,0,0.823493,"ard phrase-based translation system. We describe a modified method for the phrase extraction which deals with larger phrases while keeping a reasonable number of phrases. We also propose additional features which lead to a clear improvement in the performance of the translation. We present results with the EuroParl task in the direction Spanish to English and results from the evaluation of the shared task “Exploiting Parallel Texts for Statistical Machine Translation” (ACL Workshop on Parallel Texts 2005). e˜ = argmax P (f |e)P (e) This translation model is known as the sourcechannel approach [1] and it consists on a language model P (e) and a separate translation model P (f |e) [5]. In the last few years, new systems tend to use sequences of words, commonly called phrases [8], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. ( M ) X e˜ = argmax λm hm (e, f ) (3) e 1 Introduction Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source"
W05-0827,N03-1017,0,0.157199,"se additional features which lead to a clear improvement in the performance of the translation. We present results with the EuroParl task in the direction Spanish to English and results from the evaluation of the shared task “Exploiting Parallel Texts for Statistical Machine Translation” (ACL Workshop on Parallel Texts 2005). e˜ = argmax P (f |e)P (e) This translation model is known as the sourcechannel approach [1] and it consists on a language model P (e) and a separate translation model P (f |e) [5]. In the last few years, new systems tend to use sequences of words, commonly called phrases [8], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. ( M ) X e˜ = argmax λm hm (e, f ) (3) e 1 Introduction Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the"
W05-0827,N04-1021,0,0.0750731,"the probability was taken to be 10−40 . In addition, we have calculated the IBM−1 Model This BP is clearly overestimated due to sparse- 1. ness. On the other, note that ”la que no” cannot be considered an unusual trigram in Spanish. I X J Y 1 Hence, the language model does not penalise this P (e|f ; M 1) = p(ei |fj ) (7) target sequence either. So, the total probability (J + 1)I I=1 j=0 (P (f |e)P (e)) would be higher than desired. In order to somehow compensate these unreiliable probabilities we have studied the inclusion of 4.4 Language Model the posterior [12] and lexical probabilities [1] [10] The English language model plays an important as additional features. role in the source channel model, see equation (2), and also in its modification, see equation (3). The English language model should give an idea of the 4.2 Feature P (e|f ) sentence quality that is generated. In order to estimate the posterior phrase probabilAs default language model feature, we use a stanity, we compute again the relative frequency but re- dard word-based trigram language model generated placing the count of the target phrase by the count with smoothing Kneser-Ney and interpolation (by of the source phra"
W05-0827,J04-4002,0,0.458218,"), e˜ = argmax P (e|f ) (1) e 0 This work has been supported by the European Union under grant FP6-506738 (TC-STAR project). (2) e m=1 The features functions, hm , are the system models (translation model, language model and others) and weigths, λi , are typically optimized to maximize a scoring function. It is derived from the Maximum Entropy approach suggested by [13] [14] for a natural language understanding task. It has the advantatge that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phrase-extraction algorythm in [11]. It also combines several interesting features and it reports an important improvement from the baseline. It is organized as follows. Section 2 introduces the baseline; the following section explains the modification in the phrase extraction; section 4 shows the different features which have been taken into account; section 5 presents the evaluation framework; and 149 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 149–154, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 the final section shows some conclusions on the experiments in the pape"
W05-0827,N04-1033,0,0.504097,". Baseline The baseline is based on the source-channel approach, and it is composed of the following models which later will be combined in the decoder. The Translation Model. It is based on bilingual phrases, where a bilingual phrase (BP ) is simply two monolingual phrases (M P ) in which each one is supposed to be the translation of each other. A monolingual phrase is a sequence of words. Therefore, the basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [17]. During training, the system has to learn a dictionary of phrases. We begin by aligning the training corpus using GIZA++ [6], which is done in both translation directions. We take the union of both alignments to obtain a symmetrized word alignment matrix. This alignment matrix is the starting point for the phrase based extraction. Next, we define the criterion to extract the set of BP of the sentence pair (fjj12 ; eii21 ) and the alignment matrix A ⊆ J ∗I, which is identical to the alignment criterion described in [11]. BP (f1J , eI1 , A) = {(fjj12 , eii21 ) : 3 Phrase Extraction Motivation."
W05-0827,P02-1038,0,0.102199,") = P (you|la que no) = 0.23 case, the probability was taken to be 10−40 . In addition, we have calculated the IBM−1 Model This BP is clearly overestimated due to sparse- 1. ness. On the other, note that ”la que no” cannot be considered an unusual trigram in Spanish. I X J Y 1 Hence, the language model does not penalise this P (e|f ; M 1) = p(ei |fj ) (7) target sequence either. So, the total probability (J + 1)I I=1 j=0 (P (f |e)P (e)) would be higher than desired. In order to somehow compensate these unreiliable probabilities we have studied the inclusion of 4.4 Language Model the posterior [12] and lexical probabilities [1] [10] The English language model plays an important as additional features. role in the source channel model, see equation (2), and also in its modification, see equation (3). The English language model should give an idea of the 4.2 Feature P (e|f ) sentence quality that is generated. In order to estimate the posterior phrase probabilAs default language model feature, we use a stanity, we compute again the relative frequency but re- dard word-based trigram language model generated placing the count of the target phrase by the count with smoothing Kneser-Ney and i"
W05-0827,2001.mtsummit-papers.68,0,0.0362208,"d the development divided in two sets. This material corresponds to the transcriptions of the sessions from October the 21st to October the 28th. It has been distributed by ELDA2 . Results are reported for Spanish-to-English translations. 1 http://www.tcstar.org/ 2 http://www.elda.org/ Experiments The decoder used for the presented translation system is reported in [2]. This decoder is called MARIE and it takes into account simultaneously all the 7 features functions described above. It implements a beam-search strategy. As evaluation criteria we use: the Word Error Rate (WER), the BLEU score [15] and the NIST score [3]. As follows we report the results for several experiments that show the performance of: the baseline, adding the posterior probability, IBM Model 1 and IBM1−1 , and, finally, the modification of the phrases extraction. Optimisation. Significant improvements can be obtained by tuning the parameters of the features adequately. In the complet system we have 7 parameters to tune: the relatives frecuencies P (f |e) and P (e|f ), IBM Model 1 and its inverse, the word penalty, the phrase penalty and the weight of the language model. We applied the widely used algorithm SIMPLEX"
W05-0827,koen-2004-pharaoh,0,\N,Missing
W05-0827,P02-1040,0,\N,Missing
W06-1609,W05-0831,0,0.0718496,"Missing"
W06-1609,N03-1017,0,0.0145061,"e (S) into a reordered source language (S’), which allows for an improved translation into the target language (T). The SMT task changes from S2T to S’2T which leads to a monotonized word alignment and shorter translation units. In addition, the use of classes in SMR helps to infer new word reorderings. Experiments are reported in the EsEn WMT06 tasks and the ZhEn IWSLT05 task and show significant improvement in translation quality. 1 Introduction During the last few years, SMT systems have evolved from the original word-based approach (Brown et al., 1993) to phrase-based translation systems (Koehn et al., 2003). In parallel to the phrase-based approach, the use of bilingual n-grams gives comparable results, as shown by Crego et al. (2005a). Two basic issues differentiate the n-gram-based system from the phrasebased: training data are monotonously segmented into bilingual units; and, the model considers ngram probabilities rather than relative frequencies. This translation approach is described in detail by Mari˜no et al. (2005). The n-gram-based system follows a maximum entropy approach, in which a log-linear combination of multiple models is im70 Proceedings of the 2006 Conference on Empirical Meth"
W06-1609,2005.mtsummit-papers.36,1,0.789709,"Missing"
W06-1609,mauser-etal-2006-training,0,0.0232632,"Missing"
W06-1609,P02-1038,0,0.122392,"Missing"
W06-1609,2005.iwslt-1.23,1,0.911138,"Missing"
W06-1609,J93-2003,0,\N,Missing
W06-3120,A00-1031,0,0.0229195,"ese figures in the optimization function. 3 Shared Task Results 3.1 Data The data provided for this shared task corresponds to a subset of the official transcriptions of the European Parliament Plenary Sessions, and it is available through the shared task website at: http://www.statmt.org/wmt06/shared-task/. The development set used to tune the system consists of a subset (500 first sentences) of the official development set made available for the Shared Task. We carried out a morphological analysis of the data. The English POS-tagging has been carried out using freely available T N T tagger (Brants, 2000). In the Spanish case, we have used the F reeling (Carreras et al., 2004) analysis tool which generates the POS-tagging for each input word. 3.2 Systems configurations The baseline system is the same for all tasks and includes the following features functions: cp, pp, lm, ibm1, ibm1−1 , wb, pb. The POStag target language model has been used in those tasks for which the tagger was available. Table 1 shows the reordering configuration used for each task. The Block Reordering (application 2) has been used when the source language belongs to the Romanic family. The length of the block is limited t"
W06-3120,W05-0827,1,0.879283,"Missing"
W06-3120,2005.iwslt-1.23,1,0.907982,"Missing"
W06-3120,W06-3125,1,0.884485,"Missing"
W06-3120,P05-2012,1,0.889176,"Missing"
W06-3120,W05-0831,0,0.0297232,"Full verb forms The morphology of the verbs usually differs in each language. Therefore, it is interesting to classify the verbs in order to address the rich variety of verbal forms. Each verb is reduced into its base form and reduced POS tag as explained in (de Gispert, 2005). This transformation is only done for the alignment, and its goal is to simplify the work of the word alignment improving its quality. Block reordering (br) The difference in word order between two languages is one of the most significant sources of error in SMT. Related works either deal with reordering in general as (Kanthak et al., 2005) or deal with local reordering as (Tillmann and Ney, 2003). We report a local reordering technique, which is implemented as a preprocessing stage, with two applications: (1) to improve only alignment quality, and (2) to improve alignment quality and to infer reordering in translation. Here, we present a short explanation of the algorithm, for further details see Costa-juss`a and Fonollosa (2006). 142 Proceedings of the Workshop on Statistical Machine Translation, pages 142–145, c New York City, June 2006. 2006 Association for Computational Linguistics of the bilingual phrase, and no word on ei"
W06-3120,W06-3114,0,0.0221223,"ts to observe its efficiency in all the pairs used in this evaluation. The rgraph has been applied in those cases where: we do not use br2 (there is no sense in applying them simultaneously); and we have the tagger for the source language model available. In the case of the pair GeEn, we have not experimented any reordering, we left the application of both reordering approaches as future work. 3.3 Discussion Table 2 presents the BLEU scores evaluated on the test set (using TRUECASE) for each configuration. The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006). For both, Es2En and Fr2En tasks, br helps slightly. The improvement of the approach depends on the quality of the alignment. The better alignments allow to extract higher quality Alignment Blocks (Costa-juss`a and Fonollosa, 2006). The En2Es task is improved when adding both br1 and rgraph. Similarly, the En2Fr task seems to perform fairly well when using the rgraph. In this case, the improvement of the approach depends on the quality of the alignment patterns (Crego et al., 2006). However, it has the advantage of delaying the final decision of reordering to the overall search, where all mod"
W06-3120,N03-1017,0,0.00728769,"o infer reordering in translation. Here, we present a short explanation of the algorithm, for further details see Costa-juss`a and Fonollosa (2006). 142 Proceedings of the Workshop on Statistical Machine Translation, pages 142–145, c New York City, June 2006. 2006 Association for Computational Linguistics of the bilingual phrase, and no word on either side of the phrase is aligned to a word out of the phrase. We limit the maximum size of any given phrase to 7. The huge increase in computational and storage cost of including longer phrases does not provide a significant improvement in quality (Koehn et al., 2003) as the probability of reappearance of larger phrases decreases. 2.3 Figure 1: Example of an Alignment Block, i.e. a pair of consecutive blocks whose target translation is swapped This reordering strategy is intended to infer the most probable reordering for sequences of words, which are referred to as blocks, in order to monotonize current data alignments and generalize reordering for unseen pairs of blocks. Given a word alignment, we identify those pairs of consecutive source blocks whose translation is swapped, i.e. those blocks which, if swapped, generate a correct monotone translation. Fi"
W06-3120,J04-4002,0,0.0268059,"created). Based on this information, the source side of the bilingual corpora are reordered. In case of applying the reordering technique for purpose (1), we modify only the source training corpora to realign and then we recover the original order of the training corpora. In case of using Block Reordering for purpose (2), we modify all the source corpora (both training and test), and we use the new training corpora to realign and build the final translation system. 2.2 Phrase Extraction Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in Och and Ney (2004). A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: words are consecutive along both sides 143 Feature functions Conditional and posterior probability (cp, pp) Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. The target language model (lm) consists of an n-gram model, in which the probability of a translation hypothesis is approximated by the product of word n-gram probabilities. As default language model feature, we use a standard word-base"
W06-3120,carreras-etal-2004-freeling,0,\N,Missing
W06-3120,J03-1005,0,\N,Missing
W06-3120,N04-1033,0,\N,Missing
W06-3125,W05-0823,1,0.872672,"Missing"
W06-3125,A00-1031,0,0.117706,"d over the development set for each of the six translation directions considered. 163 This baseline system is actually very similar to the system used for last year’s shared task “Exploiting Parallel Texts for Statistical Machine Translation” of ACL’05 Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond (Banchs et al., 2005), whose results are available at: http://www.statmt.org/wpt05/ mt-shared-task/. A more detailed description of the system can be found in (2005). The tools used for POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English. All language models were estimated using the SRI language modeling toolkit. Word-to-word alignments were extracted with GIZA++. Improvements in word-toword alignments were achieved through verb group classification as described in (de Gispert, 2005). 3 Reordering Framework In this section we outline the reordering framework used for the experiments (Crego and Mari˜no, 2006). A highly constrained reordered search is performed by means of a set of reordering patterns (linguistically motivated rewrite patterns) which are used to extend the monotone search graph with additional arcs."
W06-3125,carreras-etal-2004-freeling,0,0.0548444,"Missing"
W06-3125,W06-3120,1,0.883993,"Missing"
W06-3125,N04-1033,0,0.0842803,"Missing"
W06-3125,P05-2012,1,0.901179,"Missing"
W06-3125,N03-1017,0,0.00542142,"luation with a tagged target language model (using Part-Of-Speech tags). For both Spanish-English translation directions and the English-to-French translation task, the baseline system allows for linguistically motivated sourceside reorderings. 2 Baseline N-gram-based SMT System 1 Introduction The statistical machine translation approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams (de Gispert and Mari˜no, 2002). This translation model differs from the well known phrase-based translation approach (Koehn et al., 2003) in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This translation approach is described in detail in (Mari˜no et al., 2005). For those translation tasks with Spanish or English as target language, an additional tagged (usAs already mentioned, the translation model used here is based on bilingual n-grams. It actually constitutes a language model of bilingual units, referred to as tuples, which approximates the joint probability between source and target languages by us"
W06-3125,2005.mtsummit-papers.36,1,0.909254,"Missing"
W06-3125,J93-2003,0,\N,Missing
W07-0720,W06-3125,1,0.849606,"Missing"
W07-0720,W06-1609,1,0.795127,"Missing"
W07-0720,W06-3114,0,0.0213063,"this system participation in the ACL 2007 SECOND WORK SHOP ON STATISTICAL MACHINE TRANSLA TION . Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks. 2 Baseline N-gram-based SMT System 1 Introduction Based on estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternatively to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in (Koehn and Monz, 2005; Koehn and Monz, 2006). Given the challenge of domain adaptation, efforts have been focused on improving strategies for Ngram-based SMT which could generalize better. Specifically, a novel reordering strategy is explored. It is based on extending the search by using precomputed statistical information. Results are promising while keeping computational expenses at a similar level as monotonic search. Additionally, a bonus for tuples from the out-of-domain corpus is The translation model is based on bilingual n-grams. It actually constitutes a language model of bilingual units, referred to as tuples, which approximat"
W07-0720,J06-4004,1,0.847357,"Missing"
W07-0720,E99-1010,0,0.731366,"smaller tuples which reduces the translation vocabulary sparseness. These new tuples are used to build the SMT system. 3 Baseline System Enhanced with a Weighted Reordering Input Graph This section briefly describes the statistical machine reordering (SMR) technique. Further details on the architecture of SMR system can be found on (Costa-juss`a and Fonollosa, 2006). 3.1 Concept The SMR system can be seen as a SMT system which translates from an original source language (S) to a reordered source language (S’), given a target language (T). The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). Figure 1: SMR approach in the (A) training step (B) in the test step (the weight of each arch is in brackets). 3.2 Using SMR technique to improve SMT training The original source corpus S is translated into the reordered source corpus S’ with the SMR system. Figure 1 (A) shows the corresponding block diagram. The reordered training source corpus and the original training target corpus are used to build the SMT system. The main difference here is that the training is computed with the S’2T task instead of"
W07-0720,W05-0820,0,\N,Missing
W07-0721,atserias-etal-2006-freeling,0,0.0228604,"Missing"
W07-0721,P05-1066,0,0.1211,"Missing"
W07-0721,W06-1609,1,0.902305,"Missing"
W07-0721,W07-0720,1,0.861306,"Missing"
W07-0721,2005.iwslt-1.23,1,0.910133,"Missing"
W07-0721,W05-0831,0,0.0437225,"rom the phrase-based: training data is monotonically segmented into bilingual units; and, the model considers ngram probabilities rather than relative frequencies. The n-gram-based system follows a maximum entropy approach, in which a log-linear combination of multiple models is implemented (Mari˜no et al., 2006), as an alternative to the source-channel approach. Introducing reordering capabilities is important in both systems. Recently, new reordering strategies have been proposed such as the reordering of each source sentence to match the word order in the corresponding target sentence, see Kanthak et al. (2005) and Mari˜no et al. (2006). These approaches are applied in the training set and they lack of reordering generalization. Applied both in the training and decoding step, Collins et al. (2005) describe a method for introducing syntactic information for reordering in SMT. This approach is applied as a pre-processing step. Differently, Crego et al. (2006) presents a reordering approach based on reordering patterns which is coupled with decoding. The reordering patterns are learned directly from word alignment and all reorderings have the same probability. In our previous work (Costa-juss`a and Fon"
W07-0721,N03-1017,0,0.0112124,"ees the translation quality improvement due to reordering at a very low increase of computational cost. The SMR approach is capable of generalizing reorderings, which have been learned during training, by using word classes instead of words themselves. We experiment with statistical and morphological classes in order to choose those which capture the most probable reorderings. Satisfactory results are reported in the WMT07 Es/En task. Our system outperforms in terms of BLEU the WMT07 Official baseline system. 1 Introduction Nowadays, statistical machine translation is mainly based on phrases (Koehn et al., 2003). In parallel to this phrasebased approach, the use of bilingual n-grams gives comparable results, as shown by Crego et al. (2005). Two basic issues differentiate the n-gram-based system from the phrase-based: training data is monotonically segmented into bilingual units; and, the model considers ngram probabilities rather than relative frequencies. The n-gram-based system follows a maximum entropy approach, in which a log-linear combination of multiple models is implemented (Mari˜no et al., 2006), as an alternative to the source-channel approach. Introducing reordering capabilities is importa"
W07-0721,J06-4004,1,0.820724,"Missing"
W07-0721,E99-1010,0,0.217093,"nstraints. As a result of these constraints, only one segmentation is possible for a given sentence pair. In addition to the bilingual n-gram translation model, the baseline system implements a log-linear combination of feature functions, which are described as follows: • A target language model. This feature consists of a 4-gram model of words, which is trained from the target side of the bilingual corpus. • A class target language model. This feature consists of a 5-gram model of words classes, which is trained from the target side of the bilingual corpus using the statistical classes from (Och, 1999). • A word bonus function. This feature introduces a bonus based on the number of target words contained in the partial-translation hypothesis. It is used to compensate for the system’s preference for short output sentences. • A source-to-target lexicon model. This feature, which is based on the lexical parameters of the IBM Model 1 (Brown et al., 1993), provides a complementary probability for each tuple in the translation table. These lexicon parameters are obtained from the source-to-target alignments. • A target-to-source lexicon model. Similarly to the previous feature, this feature is ba"
W07-0721,popovic-ney-2006-pos,0,0.0343665,"mation about order, might be solved by training classes in the reordered training source corpus. In other words, we monotonized the training corpus with the alignment information (i.e. reorder the source corpus in the way that matches the target corpus under the alignment links criterion). After that, we train the statistical classes, hereinafter, called statistical reordered classes. In some pair of languages, as for example English/Spanish, the reordering that may be performed is related to word’s morphology (i.e. TAGS). Some TAGS rules (with some lexical exceptions) can be extracted as in (Popovic and Ney, 2006) where they were applied with reordering purposes as a preprocessing step. Another approach that has related TAGS and reordering was presented in (Crego and Mari˜no, 2006) where instead of rules, they learned reordering patterns based on TAGS as named in this paper’s introduction. Hence, the SMR techTrain Dev Test Sentences Words Vocabulary Sentences Words Vocabulary Sentences Words Vocabulary Spanish English 1,3M 37,9M 35,5M 138,9k 133k 2 000 2 000 60.5k 58.7k 8.1k 6.5k 2 000 2 000 60,2k 58k 8,2k 6,5k Table 1: Corpus Statistics. nique may take advantage of the morphological information. Notic"
W07-0721,J93-2003,0,\N,Missing
W08-0315,W08-0315,1,0.0512755,"Missing"
W08-0315,J90-2002,0,0.809551,"Missing"
W08-0315,W07-0718,0,0.152941,"Missing"
W08-0315,carreras-etal-2004-freeling,0,0.138533,"Missing"
W08-0315,W06-3114,0,0.151633,"Missing"
W08-0315,P00-1056,0,0.073606,"Missing"
W08-0315,J04-4002,0,0.0735646,"Missing"
W08-0315,W05-0820,0,\N,Missing
W08-0315,A00-1031,0,\N,Missing
W08-0315,J06-4004,1,\N,Missing
W09-0414,J04-4002,0,0.0266217,"is translated into target language using translation table, (3) the target phrases are reordered to be inherent in the target language. A bilingual phrase (which in the context of SMT do not necessarily coincide with their linguistic analogies) is any pair of m source words and n target words that satisfies two basic constraints: (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. Given a sentence pair and a corresponding wordto-word alignment, phrases are extracted following the criterion in (Och and Ney, 2004). The probability of the phrases is estimated by relative frequencies of their appearance in the training corpus. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85–89, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 85 Classically, a phrase-based translation system implements a log-linear model in which a foreign language sentence f1J = f1 , f2 , ..., fJ is translated into another language eI1 = e1 , e2 , ..., eI by searching for the translation hypothesis eˆI1 maximizing a log-linear combination of several feature model"
W09-0414,J90-2002,0,0.572532,"he probability of the phrases is estimated by relative frequencies of their appearance in the training corpus. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85–89, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 85 Classically, a phrase-based translation system implements a log-linear model in which a foreign language sentence f1J = f1 , f2 , ..., fJ is translated into another language eI1 = e1 , e2 , ..., eI by searching for the translation hypothesis eˆI1 maximizing a log-linear combination of several feature models (Brown et al., 1990): eˆI1 = arg max eI1 ( M X w P (w) = λEuroparl · PEuroparl + λN C · PNwC (1) w and PNwC are probabilities aswhere PEuroparl signed to the word sequence w by the LM estimated on Europarl and NC data, respectively. The scale factor values are automatically optimized to obtain the lowest perplexity ppl(w) produced by the interpolated LM P (w). We used the standard script compute − best − mix from the SRI LM package (Stolcke, 2002) for optimization. On the next step, the optimized coefficients λEuroparl and λN C are generalized on the interpolated translation and reordering models. In other words,"
W09-0414,W07-0717,0,0.0343889,"he corresponding improvement in BLEU score is presented in Section 3.3 and summary of the obtained results (Table 4). ) λm hm (eI1 , f1J ) m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. 2.1 Translation models interpolation We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estève, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. In the framework of the evaluation campaign, there were two Spanish-to-English parallel training corpora available: Europarl v.4 corpus (about 50M tokens) and News Commentary (NC) corpus (about 2M tokens). The test dataset provided by the organizers this year was from the news domain, so we considered the Europarl training corpus as &quot;out-of-domain&quot; data and the News Commentary as &quot;in-domain&quot; training material. Unfortunately, the in-domain corpus is much smaller in size, however the Europar"
W09-0414,W07-0733,0,0.0262365,"l LMs and the 2009 development set (English and Spanish references) can be found in Table 1, while the corresponding improvement in BLEU score is presented in Section 3.3 and summary of the obtained results (Table 4). ) λm hm (eI1 , f1J ) m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. 2.1 Translation models interpolation We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estève, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. In the framework of the evaluation campaign, there were two Spanish-to-English parallel training corpora available: Europarl v.4 corpus (about 50M tokens) and News Commentary (NC) corpus (about 2M tokens). The test dataset provided by the organizers this year was from the news domain, so we considered the Europarl training corpus as &quot;out-of-domain&quot; data and the News Commentary as &quot;in-domain&quot; tra"
W09-0414,P07-2045,0,0.00511337,"investigate the translation models (TMs) interpolation for a state-of-the-art phrase-based translation system. Inspired by the work presented in (Schwenk and Estève, 2008), we attack this challenge using the coefficients obtained for the corresponding monolingual language models (LMs) for TMs interpolation. On the second step, we have performed additional word reordering experiments, comparing the results obtained with a statistiTALP-UPC phrase-based SMT The system developed for this year’s shared task is based on a state-of-the-art SMT system implemented within the open-source MOSES toolkit (Koehn et al., 2007). A phrase-based translation is considered as a three step algorithm: (1) the source sequence of words is segmented in phrases, (2) each phrase is translated into target language using translation table, (3) the target phrases are reordered to be inherent in the target language. A bilingual phrase (which in the context of SMT do not necessarily coincide with their linguistic analogies) is any pair of m source words and n target words that satisfies two basic constraints: (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned t"
W09-0414,N04-1022,0,\N,Missing
W09-0414,2009.eamt-1.27,1,\N,Missing
W09-2310,W05-0909,0,0.0428613,"as 26 POS and 23 syntactic categories. As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kpart and kgener to relatively low values, 1 and 3, respectively. Evaluation conditions were case-insensitive and with punctuation marks considered. The targetside 4-gram language model was estimated using the SRILM toolkit (Stolcke, 2002) and modified Kneser-Ney discounting with interpolation. The highest BLEU score (Papineni et al., 2002) was chosen as the optimization criterion. Apart from BLEU, a standard automatic measure METEOR (Banerjee and Lavie, 2005) was used for evaluation. 5.4 Table 1: Basic statistics of the BTEC training corpus. Arabic data preprocessing Results The scores considered are: BLEU scores obtained for the development set as the final point of the MERT procedure (Dev), and BLEU and METEOR scores obtained on test dataset (Test). We present BTEC results (Tables 2), characterized by relatively short sentence length, and the results obtained on the NIST corpus (Tables 3) with much longer sentences and much need of global reordering. Plain BL SBR SBR+lattice Dev BLEU 48.31 48.46 48.75 48.90 BLEU 45.02 47.10 47.52 48.78 Test METE"
W09-2310,P05-1066,0,0.277951,"Missing"
W09-2310,W06-1609,1,0.883808,"approach is described in Nießen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed. A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-jussà and Fonollosa (2008). The technique proposed in this study is most similar to the one proposed for French-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for FrenchEnglish translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. We propose using a word distortion model not only to monotonize the source part"
W09-2310,2008.amta-papers.6,1,0.850574,"reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-jussà and Fonollosa (2008). The technique proposed in this study is most similar to the one proposed for French-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for FrenchEnglish translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. We propose using a word distortion model not only to monotonize the source part of the corpus (using a different approach to rewrite rule organization from Xia and McCord), but also to extend the search space during decoding. 3 4 Syntax-based reordering coupled with word graph Our"
W09-2310,2006.iwslt-evaluation.18,1,0.905153,"Missing"
W09-2310,2007.mtsummit-papers.16,0,0.0292467,"ins for the reordering search as described in Kanthak et al. (2005) and Crego et al. (2005). The main criticism of such systems is that the constraints are not lexicalized. Recently there has been interest in SMT exploiting non-monotonic decoding which allow for extension of the search space and linguistic information involvement. The variety of such models includes a constrained distance-based reordering (Costa-jussà et al., 2006); and a constrained version of distortion model where the reordering search problem is tackled through a set of linguistically motivated rules used during decoding (Crego and Mariño, 2007). Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 78–86, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics A quite popular class of reordering algorithms is a monotonization of the source part of the parallel corpus prior to translation. The first work on this approach is described in Nießen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed. A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al.,"
W09-2310,2005.mtsummit-papers.37,0,0.117726,"Missing"
W09-2310,J04-2003,0,0.0331549,"tance-based reordering (Costa-jussà et al., 2006); and a constrained version of distortion model where the reordering search problem is tackled through a set of linguistically motivated rules used during decoding (Crego and Mariño, 2007). Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 78–86, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics A quite popular class of reordering algorithms is a monotonization of the source part of the parallel corpus prior to translation. The first work on this approach is described in Nießen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed. A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of"
W09-2310,N06-2013,0,0.0595218,"s characteristic in global reordering studies. A training set statistics can be found in Table 1. Sentences Words ASL Voc BTEC Ar En 24.9 K 24.9 K 225 K 210 K 9.05 8.46 11.4 K 7.6 K NIST50K Ar En 50 K 50 K 1.2 M 1.35 M 24.61 26.92 55.3 36.3 used to test the translation quality has 500 sentences, 4.1 K words and is also provided with 6 reference translations. The NIST50K development set consists of 1353 sentences and 43 K words; the test data contains 1056 sentences and 33 K running words. Both datasets have 4 reference translations per sentence. 5.2 We took a similar approach to that shown in Habash and Sadat (2006), using the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.3 The BTEC development dataset consists of 489 sentences and 3.8 K running words, with 6 humanmade reference translations per sentence; the dataset 83 Experimental setup We used the Stanford Parser (Klein and Manning, 2003) for both langua"
W09-2310,P08-2020,0,0.0131323,"tions are motivated by the crossed links found in the word alignment and, conIn order to improve reordering power of the translation system, we implemented an additional reordering as described in Crego and Mariño (2006). Multiple word segmentations is encoded in a lattice, which is then passed to the input of the decoder, containing reordering alternatives consistent with the previously extracted rules. The decoder takes the n-best reordering of a source sentence coded in the form of a word lattice. This approach is in line with recent research tendencies in SMT, as described for example in (Hildebrand et al., 2008; Xu et al., 2005). Originally, word lattice algorithms do not involve syntax into reordering process, there(a) Monotonic search, plain text: >n +h mTEm *w tAryx Eryq >n S +h 1 mTEm 2 *w 3 tAryx 4 Eryq 5 L (b) Word lattice, plain text: >n +h mTEm *w tAryx Eryq mTEm +h mTEm Eryq tAryx *w >n S 2 1 4 3 >n 7 6 5 >n L 10 9 8 mTEm mTEm tAryx tAryx +h Eryq *w (c) Word lattice, reordered text: >n +h mTEm *w Eryq tAryx tAryx tAryx Eryq *w *w *w +h Eryq mTEm mTEm mTEm >n S tAryx 1 2 3 >n 4 5 6 7 10 9 8 11 12 >n 13 14 L *w mTEm +h *w *w Eryq Eryq Eryq Eryq Figure 5: Comparative example of a monotone sear"
W09-2310,2005.mtsummit-papers.35,0,0.0339199,"ed local orientations enriched with probabilities are learned from training data. During decoding, translation is viewed as a monotone block sequence generation process with the possibility to swap a pair of neighbor blocks. http://www.statmt.org/moses/ 79 ηd0 @d0 . . . ηdk @dk |Lexicon|p1 (1) where ηi ∈ N for all 0 ≤ i ≤ k; (do . . . dk ) is a permutation of (0 . . . k); Lexicon comes from the source-side set of words for each ηi ; and p1 is a probability associated with the rule. Figure 1 gives two examples of the rule format. 4.2 Rules extraction Concept. Inspired by the ideas presented in Imamura et al. (2005), where monolingual correspondences of syntactic nodes are used during decoding, we extract a set of bilingual patterns allowing for reordering as described below: (1) align the monotone bilingual corpus with GIZA++ (Och and Ney, 2003) and find the intersection of direct and inverse word alignments, resulting in the construction of the projection matrix P (see below)); (2) parse the source and the target parts of the parallel corpus; (3) extract reordering patterns from the parallel non-isomorphic CFG-trees based on the word alignment intersection. Step 2 is straightforward; we explain aspects"
W09-2310,W05-0831,0,0.0927259,"ave appeared over the past few years. Word class-based reordering was a part of Och’s Alignment Template system (Och et al., 2004); the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order. The state-of-the-art SMT system Moses implements a distance-based reordering model (Koehn et al., 2003) and a distortion model, operating with rewrite patterns extracted from a phrase alignment table (Tillman, 2004). Many SMT models implement the brute force approach, introducing several constrains for the reordering search as described in Kanthak et al. (2005) and Crego et al. (2005). The main criticism of such systems is that the constraints are not lexicalized. Recently there has been interest in SMT exploiting non-monotonic decoding which allow for extension of the search space and linguistic information involvement. The variety of such models includes a constrained distance-based reordering (Costa-jussà et al., 2006); and a constrained version of distortion model where the reordering search problem is tackled through a set of linguistically motivated rules used during decoding (Crego and Mariño, 2007). Proceedings of SSST-3, Third Workshop on S"
W09-2310,P03-1054,0,0.00299978,"to that shown in Habash and Sadat (2006), using the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.3 The BTEC development dataset consists of 489 sentences and 3.8 K running words, with 6 humanmade reference translations per sentence; the dataset 83 Experimental setup We used the Stanford Parser (Klein and Manning, 2003) for both languages, Penn English Treebank (Marcus et al., 1993) and Penn Arabic Treebank set (Kulick et al., 2006). The English Treebank is provided with 48 POS and 14 syntactic tags, the Arabic Treebank has 26 POS and 23 syntactic categories. As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kpart and kgener to relatively low values, 1 and 3, respectively. Evaluation conditions were case-insensitive and with punctuation marks considered. The targetside 4-gram language model was estimated using the SRILM toolkit (Stolcke,"
W09-2310,N03-1017,0,0.0203752,"Missing"
W09-2310,P07-2045,0,0.00740083,"sentation &quot;CFG form&quot;. We formally define a CFG in the usual way as G = hN, T, R, Si, where N is a set of nonterminal symbols (corresponding to source-side phrase and partof-speech tags); T is a set of source-side terminals (the lexicon), R is a set of production rules of the form η → γ, with η ∈ N and γ, which is a sequence of terminal and nonterminal symbols; and S ∈ N is the distinguished symbol. The reordering rules then have the form η0 @0 . . . ηk @k → Baseline phrase-based SMT systems The reference system which was used as a translation mechanism is the state-of-the-art Moses-based SMT (Koehn et al., 2007). The training and weights tuning procedures can be found on the Moses web page1 . Classical phrase-based translation is considered as a three step algorithm: (1) the source sequence of words is segmented into phrases, (2) each phrase is translated into the target language using a translation table, (3) the target phrases are reordered to fit the target language. The probabilities of the phrases are estimated by relative frequencies of their appearance in the training corpus. 1 In baseline experiments we used a phrase dependent lexicalized reordering model, as proposed in Tillmann (2004). Acco"
W09-2310,W04-3250,0,0.325388,"Missing"
W09-2310,J93-2004,0,0.0311303,"m for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.3 The BTEC development dataset consists of 489 sentences and 3.8 K running words, with 6 humanmade reference translations per sentence; the dataset 83 Experimental setup We used the Stanford Parser (Klein and Manning, 2003) for both languages, Penn English Treebank (Marcus et al., 1993) and Penn Arabic Treebank set (Kulick et al., 2006). The English Treebank is provided with 48 POS and 14 syntactic tags, the Arabic Treebank has 26 POS and 23 syntactic categories. As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kpart and kgener to relatively low values, 1 and 3, respectively. Evaluation conditions were case-insensitive and with punctuation marks considered. The targetside 4-gram language model was estimated using the SRILM toolkit (Stolcke, 2002) and modified Kneser-Ney discounting with interpolation. T"
W09-2310,J03-1002,0,0.00410782,"atmt.org/moses/ 79 ηd0 @d0 . . . ηdk @dk |Lexicon|p1 (1) where ηi ∈ N for all 0 ≤ i ≤ k; (do . . . dk ) is a permutation of (0 . . . k); Lexicon comes from the source-side set of words for each ηi ; and p1 is a probability associated with the rule. Figure 1 gives two examples of the rule format. 4.2 Rules extraction Concept. Inspired by the ideas presented in Imamura et al. (2005), where monolingual correspondences of syntactic nodes are used during decoding, we extract a set of bilingual patterns allowing for reordering as described below: (1) align the monotone bilingual corpus with GIZA++ (Och and Ney, 2003) and find the intersection of direct and inverse word alignments, resulting in the construction of the projection matrix P (see below)); (2) parse the source and the target parts of the parallel corpus; (3) extract reordering patterns from the parallel non-isomorphic CFG-trees based on the word alignment intersection. Step 2 is straightforward; we explain aspects of Steps 1 and 3 in more detail below. Figures 1 and 2 show an example of the extraction of two lexicalized rules for a parallel Arabic-English sentence: Arabic: h*A hW fndq +k English: this is your hotel Given two parse trees and a w"
W09-2310,N04-1021,0,0.0351906,"incorporate any linguistic analysis and work at the surface level of word forms. However, more recently MT systems are moving towards including additional linguistic and syntactic informative sources (for example, source- and/or targetside syntax) into word reordering process. In this paper we propose using a syntactic reordering system operating with fully, partially and non- lexicalized reordering patterns, which are applied on the step 78 Related work Many reordering algorithms have appeared over the past few years. Word class-based reordering was a part of Och’s Alignment Template system (Och et al., 2004); the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order. The state-of-the-art SMT system Moses implements a distance-based reordering model (Koehn et al., 2003) and a distortion model, operating with rewrite patterns extracted from a phrase alignment table (Tillman, 2004). Many SMT models implement the brute force approach, introducing several constrains for the reordering search as described in Kanthak et al. (2005) and Crego et al. (2005). The main criticism of such systems is that the constraints are not lexicalized. Rec"
W09-2310,P02-1040,0,0.0778072,"Missing"
W09-2310,N04-4026,0,0.222367,"ng with fully, partially and non- lexicalized reordering patterns, which are applied on the step 78 Related work Many reordering algorithms have appeared over the past few years. Word class-based reordering was a part of Och’s Alignment Template system (Och et al., 2004); the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order. The state-of-the-art SMT system Moses implements a distance-based reordering model (Koehn et al., 2003) and a distortion model, operating with rewrite patterns extracted from a phrase alignment table (Tillman, 2004). Many SMT models implement the brute force approach, introducing several constrains for the reordering search as described in Kanthak et al. (2005) and Crego et al. (2005). The main criticism of such systems is that the constraints are not lexicalized. Recently there has been interest in SMT exploiting non-monotonic decoding which allow for extension of the search space and linguistic information involvement. The variety of such models includes a constrained distance-based reordering (Costa-jussà et al., 2006); and a constrained version of distortion model where the reordering search problem"
W09-2310,D07-1077,0,0.0155249,"ird Workshop on Syntax and Structure in Statistical Translation, pages 78–86, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics A quite popular class of reordering algorithms is a monotonization of the source part of the parallel corpus prior to translation. The first work on this approach is described in Nießen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed. A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-jussà and Fonollosa (2008). The technique proposed in this study is most similar to the one propose"
W09-2310,C04-1073,0,0.0336371,"etween German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-jussà and Fonollosa (2008). The technique proposed in this study is most similar to the one proposed for French-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for FrenchEnglish translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. We propose using a word distortion model not only to monotonize the source part of the corpus (using a different approach to rewrite rule organization from Xia and McCord), but also to extend the search space during decoding. 3 4 Syntax-based reordering coupled with word graph Our syntax-based reordering system requires access to source and target language parse trees and word alignments intersections. 4.1 Notation S"
W09-2310,2005.iwslt-1.18,0,0.173457,"e crossed links found in the word alignment and, conIn order to improve reordering power of the translation system, we implemented an additional reordering as described in Crego and Mariño (2006). Multiple word segmentations is encoded in a lattice, which is then passed to the input of the decoder, containing reordering alternatives consistent with the previously extracted rules. The decoder takes the n-best reordering of a source sentence coded in the form of a word lattice. This approach is in line with recent research tendencies in SMT, as described for example in (Hildebrand et al., 2008; Xu et al., 2005). Originally, word lattice algorithms do not involve syntax into reordering process, there(a) Monotonic search, plain text: >n +h mTEm *w tAryx Eryq >n S +h 1 mTEm 2 *w 3 tAryx 4 Eryq 5 L (b) Word lattice, plain text: >n +h mTEm *w tAryx Eryq mTEm +h mTEm Eryq tAryx *w >n S 2 1 4 3 >n 7 6 5 >n L 10 9 8 mTEm mTEm tAryx tAryx +h Eryq *w (c) Word lattice, reordered text: >n +h mTEm *w Eryq tAryx tAryx tAryx Eryq *w *w *w +h Eryq mTEm mTEm mTEm >n S tAryx 1 2 3 >n 4 5 6 7 10 9 8 11 12 >n 13 14 L *w mTEm +h *w *w Eryq Eryq Eryq Eryq Figure 5: Comparative example of a monotone search (a), word latti"
W09-2310,P01-1067,0,0.0647201,"tput functioning as an input to the next rule can lead to situations reverting the change of word order that the previously applied rule made. Therefore, the rules that can be ambiguous when applied sequentially during decoding are pruned according to the higher probability principle. For example, for the pair of patterns with the same lexicon (which is empty for a general rule leading to a recurring contradiction NP@0 VP@1 → VP@1 NP@0 p1, VP@0 NP@1 → NP@1 VP@0 p2 ), the less probable rule is removed. Finally, there are three resulting parameter tables analogous to the &quot;r-table&quot; as stated in (Yamada and Knight, 2001), consisting of POS- and constituentbased patterns allowing for reordering and monoInitial rule: Part. lexic. rules: General rule: tone distortion (examples can be found in Table 5). 4.4 Source-side monotonization Rule application is performed as a bottom-up parse tree traversal following two principles: (1) the longest possible rule is applied, i.e. among a set of nested rules, the rule with a longest left-side covering is selected. For example, in the case of the appearance of an NN JJ RB sequence and presence of the two reordering rules NN@0 JJ@1 → ... and NN@0 JJ@1 RB@2 → ... the latter pa"
W12-3133,P08-1087,0,0.0134042,"$* Figure 2: Above, flow diagram of the training of simplified morphology translation models. Below, Spanish morphology generation as an independent classification task. Type PLAIN TARGET: TARGET+PoS (Gen. Sur.): TARGET+PoS (Simpl. PoS): Text la Comisi´on puede llegar a paralizar el programa la Comisi´on VMIP3S0[poder] llegar a paralizar el programa la Comisi´on VMIPpn0[poder] llegar a paralizar el programa Table 4: Example of morphology simplification steps taken for Spanish verbs. be summarized in four categories: i) factored models (Koehn and Hoang, 2007), enriched input models (Avramidis and Koehn, 2008; Ueffing and Ney, 2003), segmented translation (Virpioja et al., 2007) and morphology generation (Toutanova et al., 2008; de Gispert and Mari˜no, 2008). Our strategy for dealing with morphology generation is based in the latter approach (de Gispert and Mari˜no, 2008) (Figure 2). We center our strategy in simplifying only verb forms as previous studies indicate that they contribute to the main improvement (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). That strategy makes clear the real impact of morphology simplification by providing an upper bound oracle for the studied scenarios. The"
W12-3133,J90-2002,0,0.616365,"s. For the Spanish to English task we submitted a baseline system that uses all parallel training data and a combination of different target language models (LM) and Part-Of-Speech (POS) language models. A similar configuration was submitted for the Baseline system: Phrase-Based SMT Classically, a phrase-based translation system implements a log-linear model in which a foreign language sentence f1j = f1 , f2 , . . . , fj is translated into another language sentence eI1 = e1 , e2 , . . . , eI by searching for the translation hypothesis that maximizes a log-linear combination of feature models (Brown et al., 1990): eˆI1 = arg max eI1 ( M X I J m hm e 1 , f 1 m=1 ) (1) where the separate feature functions hm refer to the system models and the set of m refers to the weights corresponding to these models. As feature functions we used the standard models available 275 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 275–282, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics the$ NATO$ mission$ oﬃcially$ Corpus ended$ EPPS la$ DAFS$ misión$ NCFS$ de$ SPS$ la$ DAFS$ OTAN$ NP$ terminó$ VMIS3S0$ oﬁcialmente$ RG$ News.Com UN Sent. Eng Spa Eng Spa Eng Sp"
W12-3133,W07-0722,0,0.0230476,"ut-ofdomain corpus (in our case the European Parliament) and we aim to adapt to another domain that has limited data, either only monolingual or hopefully bilingual as well. The WMT Translation Task focuses on adapting the system to a news domain, offering an in-domain parallel corpus to work with. In case of additional target monolingual data, previous works have focused on language model interpolations (Bulyko et al., 2007; Mohit et al., 2009; Wu et al., 2008). When parallel in-domain data is available, the latest researches have focused on mixture model adaptation of the translation model (Civera and Juan, 2007; Foster and Kuhn, 2007; Foster et al., 2010). Our work is closer to the latest approaches. We used the in-domain parallel data to adapt the translation model, but focusing on the decoding errors that the out-of-domain baseline system made while translating the in-domain corpus. The idea is to detect where the system made its mistakes and use the in-domain data to teach it how to correct them. Our approach began with a baseline system built with the Parliament and the United Nations parallel corpora but without the News parallel corpus. The rest of the configuration remained the same for the b"
W12-3133,P11-1004,0,0.0153204,"eration to deal with the problems raised by morphological rich languages such as Spanish. Second we present a domain adaptation strategy that consists in deriving new units into the phrase-table. 3.2 Morphology simplification The first improvement strategy is based on morphology simplification when translating from English to Spanish. The problems raised when translating from a language such as English into richer morphology languages are well known and are a research line of interest nowadays (Popovic and Ney, 2004; Koehn and Hoang, 2007; de Gispert and Mari˜no, 2008; Toutanova et al., 2008; Clifton and Sarkar, 2011). In that direction, inflection causes a very large targetlanguage lexicon with a significant data sparsity problem. In addition, system output is limited only to the inflected phrases available in the parallel training corpus. Hence, SMT systems cannot generate proper inflections unless they have learned them from the appropriate phrases. That would require to have a parallel corpus containing all possible word inflections for all phrases available, which it is an unfeasible task. The morphology related problems in MT have been addressed from different approaches and may"
W12-3133,W11-2107,0,0.213955,"Missing"
W12-3133,W07-0717,0,0.0268143,"our case the European Parliament) and we aim to adapt to another domain that has limited data, either only monolingual or hopefully bilingual as well. The WMT Translation Task focuses on adapting the system to a news domain, offering an in-domain parallel corpus to work with. In case of additional target monolingual data, previous works have focused on language model interpolations (Bulyko et al., 2007; Mohit et al., 2009; Wu et al., 2008). When parallel in-domain data is available, the latest researches have focused on mixture model adaptation of the translation model (Civera and Juan, 2007; Foster and Kuhn, 2007; Foster et al., 2010). Our work is closer to the latest approaches. We used the in-domain parallel data to adapt the translation model, but focusing on the decoding errors that the out-of-domain baseline system made while translating the in-domain corpus. The idea is to detect where the system made its mistakes and use the in-domain data to teach it how to correct them. Our approach began with a baseline system built with the Parliament and the United Nations parallel corpora but without the News parallel corpus. The rest of the configuration remained the same for the baseline. With this alte"
W12-3133,W06-1607,0,0.0526363,"15 M 8.38 M Words 49.40 M 52.66 M 3.73 M 4.33 M 205.68 M 239.40 M Vocab. 124.03 k 154.67 k 62.70 k 73.97 k 575.04 k 598.54 k avg.len. 26.05 27.28 24.20 28.09 24.54 28.56 Figure 1: Factored phrase-based MT based on translation from surface to surface and Part-of-Speech Table 1: English-Spanish corpora statistics for NAACL-WMT 2012 after cleaning process on Moses, i.e., relative frequencies, lexical weights, word and phrase penalty, wbe-msd-bidirectional-fe reordering models and two language models, one for surface and one for POS tags. Phrase scoring was computed using Good-Turing discounting (Foster et al., 2006). The tuning process was done using MERT (Och, 2003) with Minimum Bayes-Risk decoding (MBR) (Kumar and Bryne, 2004) on Moses and focusing on minimizing the BLEU score (Papineni et al., 2002) of the development set. Final translations were also computed using MBR decoding. Additionally to the settings mentioned before, we worked with a factored version of the corpus. Factored corpora augments surface forms with additional information, such as POS tags or lemmas as shown in Figure 1. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing to build factor-specific la"
W12-3133,D10-1044,0,0.0582046,"Missing"
W12-3133,2011.eamt-1.18,1,0.812022,"Missing"
W12-3133,D07-1091,0,0.0297742,"e present an improvement strategy based on morphology simplification plus generation to deal with the problems raised by morphological rich languages such as Spanish. Second we present a domain adaptation strategy that consists in deriving new units into the phrase-table. 3.2 Morphology simplification The first improvement strategy is based on morphology simplification when translating from English to Spanish. The problems raised when translating from a language such as English into richer morphology languages are well known and are a research line of interest nowadays (Popovic and Ney, 2004; Koehn and Hoang, 2007; de Gispert and Mari˜no, 2008; Toutanova et al., 2008; Clifton and Sarkar, 2011). In that direction, inflection causes a very large targetlanguage lexicon with a significant data sparsity problem. In addition, system output is limited only to the inflected phrases available in the parallel training corpus. Hence, SMT systems cannot generate proper inflections unless they have learned them from the appropriate phrases. That would require to have a parallel corpus containing all possible word inflections for all phrases available, which it is an unfeasible task. The morphology related problems"
W12-3133,P07-2045,0,0.00929111,"eLing (Padr´o et al., 2010). Freeling tokenization is able to deal with contractions (“del” ! “de el”) and clitics separation (“c´ompramelo” ! “compra me lo”) in Spanish and English. Stemming was performed using Snowball (Porter, 2001). Surface text was lowercased conditionally based on the POS tagging: proper nouns and adjectives were separated from other POS categories to determine if a string should be fully lowercased (no special property), partially lowercased (proper noun or adjective) or not lowercased at all (acronym). Bilingual corpora were cleaned with cleancorpus-n script of Moses (Koehn et al., 2007) removing all sentence pair with more than 70 words in any language, considering the already tokenized data. That script also ensures a maximum length ratio below of nine (9) words between source and target sentences. Postprocessing in both languages consisted of a recasing step using Moses recaser script. Furthermore we built an additional script in order to check the casing of output names with respect to source sentence names and case them accordingly, with exception of names placed at beginning of the sentence. After recasing, a final detokenization step was performed using standard Moses"
W12-3133,2005.mtsummit-papers.11,0,0.0149384,"odel. Initially, a LM was built for every corpus and then they were combined to produce de final LM. Table 2 presents the statistics of each corpora, again after the cleaning process. 2.1 For internal testing we used the News 2011’s data and concatenated the remaining three years of News data as a single parallel corpus for development. Table 3 shows the statistics for these two sets and includes in the last rows the statistics of the official test set for this year’s translation task. Corpus used The baseline system was trained using all parallel corpora, i.e. the European Parliament (EPPS) (Koehn, 2005), News Commentary and United Nations. Table 1 shows the statistics of the training data after the cleaning process described later on Subsection 2.2. Regarding the monolingual data, there was also more News corpora separated by years for Spanish and English and there was the Gigaword monolingual corpus for English. All data can be found on the Translation Task’s website1 . We used all News corpora (and Gigaword for English) to build the lan1 http://www.statmt.org/wmt12/translation-task.html 276 Corpus EPPS News.Com. UN News.07 News.08 News.09 News.10 News.11 Giga Eng Spa Eng Spa Eng Spa Eng Sp"
W12-3133,N04-1022,0,0.0330698,"k 598.54 k avg.len. 26.05 27.28 24.20 28.09 24.54 28.56 Figure 1: Factored phrase-based MT based on translation from surface to surface and Part-of-Speech Table 1: English-Spanish corpora statistics for NAACL-WMT 2012 after cleaning process on Moses, i.e., relative frequencies, lexical weights, word and phrase penalty, wbe-msd-bidirectional-fe reordering models and two language models, one for surface and one for POS tags. Phrase scoring was computed using Good-Turing discounting (Foster et al., 2006). The tuning process was done using MERT (Och, 2003) with Minimum Bayes-Risk decoding (MBR) (Kumar and Bryne, 2004) on Moses and focusing on minimizing the BLEU score (Papineni et al., 2002) of the development set. Final translations were also computed using MBR decoding. Additionally to the settings mentioned before, we worked with a factored version of the corpus. Factored corpora augments surface forms with additional information, such as POS tags or lemmas as shown in Figure 1. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing to build factor-specific language models with higherorder n-grams. These higher-order language models usually help to obtain more syntacticall"
W12-3133,2009.eamt-1.22,0,0.0247568,"10). 3.3 Domain adaptation Depending on the available resources, different domain adaptation techniques are possible. Usually, the baseline system is built with a large out-ofdomain corpus (in our case the European Parliament) and we aim to adapt to another domain that has limited data, either only monolingual or hopefully bilingual as well. The WMT Translation Task focuses on adapting the system to a news domain, offering an in-domain parallel corpus to work with. In case of additional target monolingual data, previous works have focused on language model interpolations (Bulyko et al., 2007; Mohit et al., 2009; Wu et al., 2008). When parallel in-domain data is available, the latest researches have focused on mixture model adaptation of the translation model (Civera and Juan, 2007; Foster and Kuhn, 2007; Foster et al., 2010). Our work is closer to the latest approaches. We used the in-domain parallel data to adapt the translation model, but focusing on the decoding errors that the out-of-domain baseline system made while translating the in-domain corpus. The idea is to detect where the system made its mistakes and use the in-domain data to teach it how to correct them. Our approach began with a base"
W12-3133,J03-1002,0,0.00485021,"Missing"
W12-3133,P03-1021,0,0.0060492,"0 M Vocab. 124.03 k 154.67 k 62.70 k 73.97 k 575.04 k 598.54 k avg.len. 26.05 27.28 24.20 28.09 24.54 28.56 Figure 1: Factored phrase-based MT based on translation from surface to surface and Part-of-Speech Table 1: English-Spanish corpora statistics for NAACL-WMT 2012 after cleaning process on Moses, i.e., relative frequencies, lexical weights, word and phrase penalty, wbe-msd-bidirectional-fe reordering models and two language models, one for surface and one for POS tags. Phrase scoring was computed using Good-Turing discounting (Foster et al., 2006). The tuning process was done using MERT (Och, 2003) with Minimum Bayes-Risk decoding (MBR) (Kumar and Bryne, 2004) on Moses and focusing on minimizing the BLEU score (Papineni et al., 2002) of the development set. Final translations were also computed using MBR decoding. Additionally to the settings mentioned before, we worked with a factored version of the corpus. Factored corpora augments surface forms with additional information, such as POS tags or lemmas as shown in Figure 1. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing to build factor-specific language models with higherorder n-grams. These higher"
W12-3133,padro-etal-2010-freeling,0,0.0632915,"Missing"
W12-3133,P02-1040,0,0.0865944,"phrase-based MT based on translation from surface to surface and Part-of-Speech Table 1: English-Spanish corpora statistics for NAACL-WMT 2012 after cleaning process on Moses, i.e., relative frequencies, lexical weights, word and phrase penalty, wbe-msd-bidirectional-fe reordering models and two language models, one for surface and one for POS tags. Phrase scoring was computed using Good-Turing discounting (Foster et al., 2006). The tuning process was done using MERT (Och, 2003) with Minimum Bayes-Risk decoding (MBR) (Kumar and Bryne, 2004) on Moses and focusing on minimizing the BLEU score (Papineni et al., 2002) of the development set. Final translations were also computed using MBR decoding. Additionally to the settings mentioned before, we worked with a factored version of the corpus. Factored corpora augments surface forms with additional information, such as POS tags or lemmas as shown in Figure 1. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing to build factor-specific language models with higherorder n-grams. These higher-order language models usually help to obtain more syntactically correct output. Concretely we map input source surfaces to target surface"
W12-3133,popovic-ney-2004-towards,0,0.0328289,"ent strategies. First we present an improvement strategy based on morphology simplification plus generation to deal with the problems raised by morphological rich languages such as Spanish. Second we present a domain adaptation strategy that consists in deriving new units into the phrase-table. 3.2 Morphology simplification The first improvement strategy is based on morphology simplification when translating from English to Spanish. The problems raised when translating from a language such as English into richer morphology languages are well known and are a research line of interest nowadays (Popovic and Ney, 2004; Koehn and Hoang, 2007; de Gispert and Mari˜no, 2008; Toutanova et al., 2008; Clifton and Sarkar, 2011). In that direction, inflection causes a very large targetlanguage lexicon with a significant data sparsity problem. In addition, system output is limited only to the inflected phrases available in the parallel training corpus. Hence, SMT systems cannot generate proper inflections unless they have learned them from the appropriate phrases. That would require to have a parallel corpus containing all possible word inflections for all phrases available, which it is an unfeasible task. The morph"
W12-3133,2006.amta-papers.25,0,0.0252999,"us consists of the source side, the output translation and the target side, also called the target correction. The output translation and its reference are then compare to detect possible mistakes that the system caused during decoding. The translation was used as a pivot to find a wordto-word alignment between the source side and the target correction. The word-to-word alignment between source side and translation was provided by Moses during decoding. The word-to-word alignment between the output translation and target correction was obtained following these steps: 1. Translation Edit Rate (Snover et al., 2006) between each output translation and target correction sentence pair was computed to obtain its edit path and detect which words do not change between sentences. Words that did not change were directly linked 2. Going from left to right, for each unaligned word wout on the output translation sentence and each word wtrg on the target correction sentence, a similarity function was computed between them and wout got aligned with the word wtrg that maximized this similarity. The similarity function was defined as a linear combination of features that considered if the words wout and wtrg were iden"
W12-3133,P08-1059,0,0.116941,"simplification plus generation to deal with the problems raised by morphological rich languages such as Spanish. Second we present a domain adaptation strategy that consists in deriving new units into the phrase-table. 3.2 Morphology simplification The first improvement strategy is based on morphology simplification when translating from English to Spanish. The problems raised when translating from a language such as English into richer morphology languages are well known and are a research line of interest nowadays (Popovic and Ney, 2004; Koehn and Hoang, 2007; de Gispert and Mari˜no, 2008; Toutanova et al., 2008; Clifton and Sarkar, 2011). In that direction, inflection causes a very large targetlanguage lexicon with a significant data sparsity problem. In addition, system output is limited only to the inflected phrases available in the parallel training corpus. Hence, SMT systems cannot generate proper inflections unless they have learned them from the appropriate phrases. That would require to have a parallel corpus containing all possible word inflections for all phrases available, which it is an unfeasible task. The morphology related problems in MT have been addressed from different approaches an"
W12-3133,2007.mtsummit-papers.65,0,0.0443673,"Missing"
W12-3133,C08-1125,0,0.0365424,"Missing"
W12-3133,2011.eamt-1.20,1,\N,Missing
W13-2215,W11-2107,0,0.0192041,"E models we used the data from the WMT13 shared task on quality estimation (System Selection Quality Estimation at Sentence Level task5 ), which contains the test sets from other WMT campaigns with human assessments. We used five groups of features, namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the follow"
W13-2215,W12-3133,1,0.860966,"Missing"
W13-2215,2012.amta-monomt.1,1,0.796291,"Missing"
W13-2215,2013.mtsummit-papers.9,1,0.774503,"Missing"
W13-2215,padro-etal-2010-freeling,0,0.0202206,"Missing"
W13-2215,W06-1607,0,0.0198766,"s with additional information, such as POS tags or lemmas. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing the construction of factor-specific language models with higher-order n-grams. Such language models can help to obtain syntactically more correct outputs. We used the standard models available in Moses as feature functions: relative frequencies, lexical weights, word and phrase penalties, wbe-msdbidirectional-fe reordering models, and two language models (one for surface and one for POS tags). Phrase scoring was computed using GoodTuring discounting (Foster et al., 2006). As aforementioned, we developed five factored Moses-based independent systems with different 134 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 134–140, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the"
W13-2215,P02-1040,0,0.0862502,"on Statistical Machine Translation, pages 134–140, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation"
W13-2215,D08-1090,0,0.0227644,"namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the following parameters: linear kernel, expanding the working set by 9 variables at each iteration, for a maximum of 50,000 iterations and with a cache size of 100 for kernel evaluations. The trade-off parameter was empirically set to 0.001. Table 2 sh"
W13-2215,2011.eamt-1.18,1,0.897081,"Missing"
W13-2215,P10-1063,0,0.0150842,"task5 ), which contains the test sets from other WMT campaigns with human assessments. We used five groups of features, namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the following parameters: linear kernel, expanding the working set by 9 variables at each iteration, for a maximum of 50,000 iterations and"
W13-2215,D11-1125,0,0.0166218,"nal Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation Task’s website1 . We used all the News corpora to busld the language model (LM). Firstly, a LM was built for every cor"
W13-2215,P07-2045,0,0.10399,"such as morphology generation, training sentence filtering, and domain adaptation through unit derivation. The results show a coherent improvement on TER, METEOR, NIST, and BLEU scores when compared to our baseline system. 1 Introduction 2 The TALP-UPC center (Center for Language and Speech Technologies and Applications at Universitat Polit`ecnica de Catalunya) focused on the English to Spanish translation of the WMT13 shared task. Our primary (contrastive) run is an internal system selection comprised of different training approaches (without CommonCrawl, unless stated): (a) Moses Baseline (Koehn et al., 2007b), (b) Moses Baseline + Morphology Generation (Formiga et al., 2012b), (c) Moses Baseline + News Adaptation (Henr´ıquez Q. et al., 2011), (d) Moses Baseline + News Adaptation + Morphology Generation , and (e) Moses Baseline + News Adaptation + Filtered CommonCrawl Adaptation (Barr´on-Cede˜no et al., 2013). Our secondary run includes is the full training strategy marked as (e) in the previous description. The main differences with respect to our last year’s participation (Formiga et al., 2012a) are: i) the inclusion of the CommonCrawl corpus, using Baseline system: Phrase-Based SMT Our contrib"
W13-2215,2005.mtsummit-papers.11,0,0.0437825,"s for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation Task’s website1 . We used all the News corpora to busld the language model (LM). Firstly, a LM was built for every corpus independently. Afterwards, they were combined to produce de final LM. For internal testing we used the News 2011 and News 2012 data and concatenated the remaining three years of News data as a single parallel corpus for development. We processed the corpora as in our participation to WMT12 (Formiga et al"
W13-2215,P03-1021,0,\N,Missing
W13-2215,2011.eamt-1.20,1,\N,Missing
W13-2244,2012.amta-papers.13,1,0.318131,"Missing"
W13-2244,2013.mtsummit-papers.9,1,0.683064,"Missing"
W13-2244,P10-1063,0,0.0136334,"indicators. Secondly, we used Random Forests (Breiman, 2001), the rationale was the same as ranking-topairwise implementation from SVMlight . However, SVMlight considers two different data preprocessing methods depending on the kernel of the classifier: LINEAR and RBF-Kernel. We used the same data-preprocessing algorithm from SVMlight in order to train a Random Forest classifier with ties (three classes: {0,-1,1}) based upon the pairwise relations. We used the Random Forests implementation of scikit-learn toolkit (Pedregosa et al., 2011) with 50 estimators. 3.2 Pseudo-Reference-based Features Soricut and Echihabi (2010) introduced the concept of pseudo-reference-based features (PR) for translation ranking estimation. The principle is that, in the lack of human-produced references, automatic ones are still good for differentiating good from bad translations. One or more secondary MT systems are required to generate translations starting from the same input, which are Once the classes are given by the Random For360 Note that a considerable amount of the features described in the baseline group (QQE and AQE) fall in this category. In this subsection we include some extra features we devised to capture source– t"
W13-2244,P12-3024,1,0.541573,"Missing"
W13-2244,W05-0635,0,0.0232696,"measure for “pseudo-prefixes” (considering only up to four initial characters for every token). 3.4 We interpolated different language models comprising the WMT’12 Monolingual corpora (EPPS, News, UN and Gigafrench for English). The interpolation weights were computed as to minimize the perplexity according to the WMT Translation Task test data (2008-2010)4 . The features are as follow: 5. Based on semantic information (SEM) Twelve features calculated with named entity- and semantic role-based evaluation measures (again, provided by A SIYA). Sentences are automatically annotated using SwiRL (Surdeanu and Turmo, 2005) and BIOS (Surdeanu et al., 2005). We used these features in the de-en subtask only. 9. Language Model Features (LM) Two log-probabilities of the translation candidate with respect to the above described interpolated language models over word forms and PoS labels. 6. Explicit semantic analysis (ESA) Two versions of explicit semantic analysis (Gabrilovich and Markovitch, 2007), a semantic similarity measure, built on top of Wikipedia (we used the opening paragraphs of 100k Wikipedia articles as in 2010). 3.3 Adapted Language-Model Features 4 Experiments and Results In this section we describe t"
W13-2244,P04-1077,0,0.0383445,"Missing"
W13-2244,N03-2021,0,0.0179653,"be calculated with any evaluation measure or text similarity function, which gives us all feature variants in this group. We consider the following PR-based features: 7. Alignment-based features (ALG / ALGPR) One measure calculated over the aligned words between a candidate translation and the source (ALG); and two measures based on the comparison between these alignments for two different translations (e.g., candidate and pseudo-reference) and the source (ALGPR).3 3. Derived from A SIYA’s metrics (APR) Twenty-three PR features, including GTM-l (l∈{1,2,3}) to reward different length matching (Melamed et al., 2003), four variants of ROUGE (-L, -S*, -SU* and -W) (Lin and Och, 2004), WER (Nießen et al., 2000), PER (Tillmann et al., 1997), TER, and TERbase (i.e., without stemming, synonymy look-up, nor paraphrase support) (Snover et al., 2009), and all the shallow and full parsing measures (i.e., constituency and dependency parsing, PoS, chunking and lemmas) that A SIYA provides either for Spanish or English as target languages. 8. Length model (LeM) A measure to estimate the quality likelihood of a candidate sentence by considering the “expected length” of a proper translation from the source. The measure"
W13-2244,niessen-etal-2000-evaluation,0,0.0675224,"re variants in this group. We consider the following PR-based features: 7. Alignment-based features (ALG / ALGPR) One measure calculated over the aligned words between a candidate translation and the source (ALG); and two measures based on the comparison between these alignments for two different translations (e.g., candidate and pseudo-reference) and the source (ALGPR).3 3. Derived from A SIYA’s metrics (APR) Twenty-three PR features, including GTM-l (l∈{1,2,3}) to reward different length matching (Melamed et al., 2003), four variants of ROUGE (-L, -S*, -SU* and -W) (Lin and Och, 2004), WER (Nießen et al., 2000), PER (Tillmann et al., 1997), TER, and TERbase (i.e., without stemming, synonymy look-up, nor paraphrase support) (Snover et al., 2009), and all the shallow and full parsing measures (i.e., constituency and dependency parsing, PoS, chunking and lemmas) that A SIYA provides either for Spanish or English as target languages. 8. Length model (LeM) A measure to estimate the quality likelihood of a candidate sentence by considering the “expected length” of a proper translation from the source. The measure was introduced by (Pouliquen et al., 2003) to identify document translations. We estimated it"
W16-2336,D15-1041,0,0.0220783,"ings We look at this task as a bilinear prediction task as proposed by (Madhyastha et al., 2014). The proposed model makes use of word embeddings of both languages with no additional features. The basic function is formulated —the probability of a target word given a source word— as log-linear model and takes the following form: exp{φs˜(s)&gt; W φt˜(t)} Pr(t|s; W ) = P &gt; 0 t0 exp{φs˜(s) W φt˜(t )} Segments (1) As a solution to those drawbacks, new alternative character-based word embeddings have been recently proposed for tasks as language modeling (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al., 2015) or part-of-speech tagging (Ling et al., 2015; Santos and Zadrozny, 2014). For our system we selected the best characterbased embedding architecture proposed by Kim et al. (Kim et al., 2016). The computation of the representation of each word starts with a characterbased embedding layer that associates each word (sequence of characters) with a sequence of vectors. This sequence of vectors is then processed with a set of 1D convolution filters of different lengths (from 1 to 7 characters) followed with a max pooling layer and two additional highway layers. The output of the second highway layer"
W16-2336,P03-1021,0,0.0143318,"these systems BTT (big translation table). For the in-domain system, a 5-gram language model is estimated on the target side of the corpus using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002) (SLM, small language model). For the extended systems, we use all the monolingual corpora available and the target side of the large parallel corpus (BLM, big language model). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric on devBio. Data Our main corpus is the compilation of the corpora assigned for the shared task, which was built using scientific publications gathered from the Scielo database. We focus on the Spanish–English language pair, for which the size of the corpora is summarised in Table 1. We further increase the vocabulary of the system by using standard parallel corpora for the Spanish–English language pair (i.e., UN corpora, Europarl corpora, News corpus, etc.2 ). This corpus appears as Quest in Table 1. For the monolingual corpus we use"
W16-2336,N03-1017,0,0.0306671,"system. http://www.statmt.org/wmt16 463 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 463–468, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 3 The Translation System Table 1: Size of the parallel (top) and monolingual (bottom) corpora used to train the translation systems The TALP-UPC translation system is built on three different components. We describe their theoretical basis in the following subsections. 3.1 Corpus Biomedical Quest Phrase-based SMT The standard phrase-based machine translation system (Koehn et al., 2003) focuses on finding the most probable target sentence given the source sentence. The phrase-based system has evolved from the noisy-channel to the log-linear model which combines a set of feature functions in the decoder, including the translation and language model, the reordering model and the lexical models. Although the phrase-based system is a commoditized technology used at the academic and commercial level, there are still many challenges to solve, such as OOVs. 3.2 Bio-mono/en Bio-mono/es Wikipedia/en Wikipedia/es 3.3 Words Vocab 6 6 0.3 · 106 0.5 · 106 1 · 10 13 · 106 0.1 · 106 0.01 ·"
W16-2336,padro-stanilovsky-2012-freeling,0,0.0258286,"Missing"
W16-2336,P02-1040,0,0.0955039,"ation table). For the in-domain system, a 5-gram language model is estimated on the target side of the corpus using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002) (SLM, small language model). For the extended systems, we use all the monolingual corpora available and the target side of the large parallel corpus (BLM, big language model). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric on devBio. Data Our main corpus is the compilation of the corpora assigned for the shared task, which was built using scientific publications gathered from the Scielo database. We focus on the Spanish–English language pair, for which the size of the corpora is summarised in Table 1. We further increase the vocabulary of the system by using standard parallel corpora for the Spanish–English language pair (i.e., UN corpora, Europarl corpora, News corpus, etc.2 ). This corpus appears as Quest in Table 1. For the monolingual corpus we use an English and Spanish Wikipedia dump3 ."
W16-2336,P07-2045,0,0.0331048,"small translation table). For more general systems, we also use the Quest data; we name these systems BTT (big translation table). For the in-domain system, a 5-gram language model is estimated on the target side of the corpus using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002) (SLM, small language model). For the extended systems, we use all the monolingual corpora available and the target side of the large parallel corpus (BLM, big language model). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric on devBio. Data Our main corpus is the compilation of the corpora assigned for the shared task, which was built using scientific publications gathered from the Scielo database. We focus on the Spanish–English language pair, for which the size of the corpora is summarised in Table 1. We further increase the vocabulary of the system by using standard parallel corpora for the Spanish–English language pair (i.e., UN corpora, Europarl corpora, News corpus, etc.2"
W16-2336,D15-1176,0,0.0263254,"n using Bilingual Word-Embeddings We look at this task as a bilinear prediction task as proposed by (Madhyastha et al., 2014). The proposed model makes use of word embeddings of both languages with no additional features. The basic function is formulated —the probability of a target word given a source word— as log-linear model and takes the following form: exp{φs˜(s)&gt; W φt˜(t)} Pr(t|s; W ) = P &gt; 0 t0 exp{φs˜(s) W φt˜(t )} Segments (1) As a solution to those drawbacks, new alternative character-based word embeddings have been recently proposed for tasks as language modeling (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al., 2015) or part-of-speech tagging (Ling et al., 2015; Santos and Zadrozny, 2014). For our system we selected the best characterbased embedding architecture proposed by Kim et al. (Kim et al., 2016). The computation of the representation of each word starts with a characterbased embedding layer that associates each word (sequence of characters) with a sequence of vectors. This sequence of vectors is then processed with a set of 1D convolution filters of different lengths (from 1 to 7 characters) followed with a max pooling layer and two additional highway layers. Th"
W16-2336,2006.iwslt-papers.2,1,0.820112,"Missing"
W16-2336,K15-1031,0,0.0532749,"Missing"
W16-2336,C14-1017,1,0.845249,"rms of perplexity (Mikolov et al., 2010). They are also a good re-ranking option in tasks such as speech recognition and machine translation. However, the standard lookup-based word embeddings are limited to a finite-size vocabulary for both computational and sparsity reasons. Moreover, the orthographic representation of the words is completely ignored. The standard learning process is blind to the presence of stems, prefixes, suffixes and any other kind of affixes in words. Vocabulary Expansion using Bilingual Word-Embeddings We look at this task as a bilinear prediction task as proposed by (Madhyastha et al., 2014). The proposed model makes use of word embeddings of both languages with no additional features. The basic function is formulated —the probability of a target word given a source word— as log-linear model and takes the following form: exp{φs˜(s)&gt; W φt˜(t)} Pr(t|s; W ) = P &gt; 0 t0 exp{φs˜(s) W φt˜(t )} Segments (1) As a solution to those drawbacks, new alternative character-based word embeddings have been recently proposed for tasks as language modeling (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al., 2015) or part-of-speech tagging (Ling et al., 2015; Santos and Zadrozny, 201"
W16-2336,D13-1140,0,0.0225269,"er-based neural language model. Section 2 presents some related work to our approach. Next, Section 3 introduces the theoretical aspects of the system components and Section 4 the experiments. Finally, we justify our choice for the final submission and draw the conclusions in Section 5. 1 Related Work On the other hand, there have been several language models used for rescoring in SMT. For example, neural feed-forward language models (Schwenk et al., 2006) have been used to rescore both n-gram-based and phrase-based systems. Mikolov (2012) re-ranks n-best lists with recurrent neural networks. Vaswani et al. (2013) combine feed-forward language models, with rectified linear units and noise-contrastive estimation. Luong et al. (2015) propose to use deeper neural models which improve re-ranking. In this paper, we are using Kim et al. (2016) a characterbased language model to re-rank the output of the phrase-based system. http://www.statmt.org/wmt16 463 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 463–468, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 3 The Translation System Table 1: Size of the parallel (top) and m"
W16-2336,P15-2118,0,0.065368,"Missing"
W16-2336,N15-1176,0,0.031756,"Missing"
W16-2336,J03-1002,0,0.00913989,"in-domain system, we use only the biomedical data made available for the task (STT systems, small translation table). For more general systems, we also use the Quest data; we name these systems BTT (big translation table). For the in-domain system, a 5-gram language model is estimated on the target side of the corpus using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002) (SLM, small language model). For the extended systems, we use all the monolingual corpora available and the target side of the large parallel corpus (BLM, big language model). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric on devBio. Data Our main corpus is the compilation of the corpora assigned for the shared task, which was built using scientific publications gathered from the Scielo database. We focus on the Spanish–English language pair, for which the size of the corpora is summarised in Table 1. We further increase the vocabulary of the system by using standard parallel corpora"
W17-4123,P16-2058,1,0.871981,"Missing"
W17-4123,N16-1155,0,0.0299876,"n behaves. In this paper, we propose to use the fullycharacter neural machine translation architecture (Lee et al., 2016) but using bytes instead of characters. We compare the performance of character against byte-based neural machine translation among similar languages (Catalan/Spanish and Portuguese/Brazilian) and relatively far languages (in terms of alphabet) (German/Finnish/TurkishEnglish). As far as we are concerned, we are not aware of any research work in neural machine translation that has experimented with bytes. Related work can be found int the area of natural language processing. Gillick et al. (2016) propose an neural network that reads text as bytes and use this model in tasks of Part-of-Speech and Named Entity Recognition. The recent investigation of Irie et al (2017) describes the use of a byte-level convolutional layer (instead of character-level) in the neural language model (Irie et al., 2017), which is applied to low resource speech recognition. This paper presents experiments comparing character-based and byte-based neural machine translation systems. The main motivation of the byte-based neural machine translation system is to build multilingual neural machine translation systems"
W17-4123,Q17-1024,0,0.0719885,"Missing"
W17-4123,P07-2045,0,0.00993537,"pairs, we used all data parallel data provided in the evaluation. For German-English, we used: europarl v.7, news commentary v.12, common crawl and rapid corpus of EU press releases. For Finnish-English, we used europarl v.8, wiki headlines and rapid corpus of EU press releases. For Turkish-English, we used setimes2. The German and Finish test set is the news 2015 evaluation set, for Turkish the test set is the news 2016 evaluation set. Preprocessing consisted in cleaning empty sentences, limiting sentences up to 50 words, tokenization and truecasing for each language using tools from Moses (Koehn et al., 2007). Table 1 shows details about the corpus statistics after preprocessing. Byte-based Neural Machine Translation The byte-based Neural Machine Translation changes the character representation of words to the byte representation. Each sentence is represented as the concatenation of bytes that form its characters in utf-8 encoding. No explicit vocabulay is used but we can consider the byte representation as a vocabulary of 256 positions in which every possible byte can be represented. This modifications provides the following improvements over the previously seen architecture. • Both languages sha"
W17-4123,W17-2619,0,0.0250891,"tage of the current setting is that interlingua is not manually designed but it seems that it can be automatically extracted (Johnson et al., 2016). In addition, this multilingual environment seems to allow to build translation systems among language pairs that do not have parallel corpus available (Johnson et al., 2016), what is called “zeroshot translation”. These two motivations (interlingua and zeroshot translation) are strong enough to motivate the entire commmunity to experiment towards multilingual architectures. Recently, there have appeared works in multilingual word representations (Schwenk et al., 2017; Espa˜na-Bonet et al., 2017) Most multilingual works are at the level of words. As multilingual character research we can find (Lee et al., 2016) which goes from manyto-one languages in translation and achieves improvements for several language pairs. Previous work on character-based neural machine transla2 Character-based Neural Machine Translation Our system uses the architecture from (Lee et al., 2016) where a character-level neural MT model that maps the source character sequence to the target character sequence. The main difference in the encoder architecture of the standard neural 154 P"
W17-4123,tiedemann-2012-parallel,0,0.029509,"redicts each target character. 3 4 Experimental Framework In this section we detail experimental corpora, architecture and parameters that we used. 4.1 Data and Preprocessing For Catalan-Spanish, We use a large corpus extracted from ten years of the paper edition of a bilingual Catalan newspaper, El Peri´odico (Costajuss`a et al., 2014). The Spanish-Catalan corpus is partially available via ELDA (Evaluations and Language Resources Dis-tribution Agency) in catalog number ELRA-W0053. Development and test sets are extracted from the same corpus. For Portuguese-Brazilian, we used the OPUS corpus (Tiedemann, 2012) which is a growing collection of translated texts from the web. In particular, for Portuguese-Brazilian the source corpus are from Ubuntu and GNOME. We extracted the parallel text from translation memories (TMX format) and from the complete text, we extracted a collection of development and test set. Finally, we used WMT 2017 1 corpus data for German, Finish and Turkish to English. For the three language pairs, we used all data parallel data provided in the evaluation. For German-English, we used: europarl v.7, news commentary v.12, common crawl and rapid corpus of EU press releases. For Finn"
W17-4123,Q17-1026,0,\N,Missing
W17-4725,P11-2031,0,0.103711,"Missing"
W17-4725,P16-2058,1,0.895913,"Missing"
W17-4725,W16-2323,0,0.144924,"n the decoder architecture is that the single-layer feedforward network computes the attention score of next target character (instead of word) to be generated with every source segment representation. And afterwards, a two-layer character-level decoder takes the source context vector from the attention mechanism and predicts each target character. Introduction Neural Machine Translation (MT) has been proven to reach state-of-the-art results in the last couple of years. The baseline encoder-decoder architecture has been improved by an attentionbased mechanism citebahdanau:2015, subword units (Sennrich et al., 2016b), character-based encoders (Costa-juss`a and Fonollosa, 2016) or even with generative adversarial nets (Yang et al., 2017), among many others. Despite its successful beginnings, the neural MT approach still has many challenges to solve and improvements to incorporate into the system. However, since the system is computationally expensive and training models may last for several weeks, it is not feasible to conduct multiple experiments for a mid-sized laboratory. For the same 283 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 283–287 c Copenhag"
W17-4725,P07-2045,0,0.0387424,"ative results on NMT. In this system description, we describe our participation on German-English and Finnish-English for the News Task. Our system is a fully characterto-character neural MT (Lee et al., 2016) system with additional rescoring from the inverse direction model. In parallel to our final system, we also experimented with multilingual character-tocharacter system using German, Finnish and Turkish on the source side and English on the target side. Unfortunately, these last experiments did not work. All our systems are contrasted with a standard phrase-based system built with Moses (Koehn et al., 2007). In this paper, we describe the TALPUPC participation in the News Task for German-English and Finish-English. Our primary submission implements a fully character to character neural machine translation architecture with an additional rescoring of a n-best list of hypothesis using a forced back-translation to the source sentence. This model gives consistent improvements on different pairs of languages for the language direction with the lowest performance while keeping the quality in the direction with the highest performance. 2 Additional experiments are reported for multilingual character to"
W17-4725,N18-1122,0,0.0692399,"Missing"
W17-4725,Q17-1026,0,\N,Missing
W17-4725,P16-1162,0,\N,Missing
W17-5309,W17-5301,0,0.0143604,"ctors to conduct the prediction. (6) 3 Experiments It first feed all hidden states ht through a nonlinearity to get ut as the hidden representation of ht . Then it uses a sof tmax function to catch the normalized importance weight matrix αt . After that, the sentence representation vector h is computed by a weighted sum of all hidden states ht with the weight matrix αt . The context vector uω can be seen as a high-level representation of the importance of informative words. 2.4 3.1 Data We evaluated our approach on the Multi-Genre NLI (MNLI) corpus, as a shared task for RepEval 2017 workshop (Nangia et al., 2017). We train our CIAN model on a mixture of MNLI and SNLI corpus, by using a full MNLI training set and a randomly selected 20 percent of the SNLI training set at each epoch. Character-level Intra Attention Network 3.2 The overall architecture of the Character-level Intra Attention Network (CIAN) is shown in Figure 2. The CIAN model is consisted with 7 layers, of which the first and the last layers are the same with our baseline model. The 4 layers in middle are our augmented layers that has been introduced in this section. Hyper Parameters The BiLSTM encoder layer use 300D hidden states, thus 6"
W17-5309,D15-1075,0,0.0591592,"R. Costa-juss`a and Jos´e A. R. Fonollosa TALP Research Center Universitat Polit`ecnica de Catalunya han.yang@est.fib.upc.edu {marta.ruiz,jose.fonollosa}@upc.edu Abstract traditional baselines in many NLP tasks (Dai and Le, 2015). There are also convolutional neural network (CNN; LeCun et al., 1989) based encoders, which concatenate the sentence information by applying multiple convolving filters over the sentence. CNNs have achieved state-of-the-art results on various NLP tasks (Collobert et al., 2011). To evaluate the quality of the NLI model, the Stanford Natural Language Inference (SNLI; Bowman et al., 2015) corpus of 570K sentence pairs was introduced. It serves as a standard benchmark for NLI task. However, most of the sentences in SNLI corpus are short and simple, which limit the room for fine-grained comparisons between models. Currently, a more comprehensive Multi-Genre NLI corpus (MNLI; Williams et al., 2017) of 433K sentence pairs was released, aiming at evaluating large-scale NLI models. Authors gave out some baseline results accompanied by the publish of MNLI corpus, the BiLSTM model achieves an accuracy of 67.5, and the Enhanced Sequential Inference Model (Chen et al., 2016) achieves an"
W17-5309,D14-1162,0,0.0846724,"fine-grained comparisons between models. Currently, a more comprehensive Multi-Genre NLI corpus (MNLI; Williams et al., 2017) of 433K sentence pairs was released, aiming at evaluating large-scale NLI models. Authors gave out some baseline results accompanied by the publish of MNLI corpus, the BiLSTM model achieves an accuracy of 67.5, and the Enhanced Sequential Inference Model (Chen et al., 2016) achieves an accuracy of 72.4. Among those encoders for NLI task, most of them use word-level embedding, and initialize the weight of the embedding layer with pre-trained word vectors such as GloVe (Pennington et al., 2014). The pre-trained word vectors helps the encoders to catch richer semantic information. However, it also has its downside. As the growth of vocabulary size in the modern corpus, there will be more and more out-of-vocabulary (OOV) words that are not presented in the pre-trained word embedding vector. As the word-level embedding is blind to subword information (e.g. morphemes), it leads to high perplexities for those OOV words. In this paper, we use the BiLSTM model from (Williams et al., 2017) as the baseline model for the evaluation of the MNLI corpus. To augment the baseline model, firstly, a"
W17-5309,D16-1053,0,0.038415,"Missing"
W17-5309,D15-1044,0,0.0277734,"ingle representation vector of each sentence. However, this has its bottleneck as we intuitively know that not all words (hidden states) contribute equally to the sentence representation. To augment the performance of RNN based encoder, the concept of attention mechanism was introduced by (Bahdanau et al., 2014) for machine translation. Attention mechanism is a hidden layer which computes a categorical distribution to make a soft-selection over source elements (Kim et al., 2017). It has recently demonstrated success on tasks such as parsing text (Vinyals et al., 2015), sentence summarization (Rush et al., 2015) and Character-level Convolutional Neural Network In the baseline model, the input xt to the BiLSTM encoder layer at time t is sequence of pre-trained word embeddings. Those pre-trained word embeddings can boost the performance of the model. However, it is limited to the finite-size of vocabulary. Here we replace the word embedding layer with a character-level convolutional neural network (CharCNN; Kim et al., 2016) for language 47 also on a wide range of NLP tasks (Cheng et al., 2016). Here we implemented the Intra Attention mechanism introduced by (Yang et al., 2016) for document classificat"
W17-5309,P16-2058,1,0.881226,"Missing"
W17-5309,N16-1174,0,0.254284,"ory Networks (LSTM; Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (Chung et al., 2014). RNNs have surpassed the performance of 46 Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 46–50, c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics convolutional neural network (CharCNN; Kim et al., 2016) is applied. We use the CharCNN to replace the word embedding layer in the baseline model, which will be computed from the characters of corresponding word. Secondly, the intra attention mechanism introduced by (Yang et al., 2016) will be applied, to enhance the model with a richer information of substructures of a sentence. 2 2.1 modeling, which also achieved success in machine translation (Costa-Juss`a and Fonollosa, 2016). We define the text sentence input as vector C k ∈ d×l R , where k ∈ K is the k-th word in a sentence, d is the dimensionality of character embeddings, l is the length of characters in k-th word. Then a set of narrow convolutions between C k and filter H is applied, followed with a max-over-time (max pooling) as shown in Equation 1-2. Model Development f k [i] = tanh(hC k [∗, i : i + ω − 1], Hi + b"
W17-5309,P82-1020,0,0.845688,"Missing"
W18-6406,D16-1250,0,0.0218591,"s. Geographical location has also led to differences in the loanwords borrowed by each language. 355 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 355–360 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64033 3 Attention-based NMT with an auxiliary text (denoising) auto-encoding loss, whose internal sentence representation is aligned with the ones from the translation task by means of a discriminator in feature space. Pre-trained cross-lingual embeddings (Artetxe et al., 2016, 2017) can be used complementarily to further reduce the need for parallel data. Finding parallel data from a similar source language and the same target language (or vice versa) and adding it to the original parallel corpus. With such a composite training data set, a wordpiece-level vocabulary can leverage the common word stems between the similar languages and profit from the combined amount of data. This approach is used in the present work, as described in sections 5 and 6.1. Multilingual zero-shot translation (Johnson et al., 2017) also uses parallel corpora from different source and tar"
W18-6406,P17-1042,0,0.0362811,"Missing"
W18-6406,P07-2045,0,0.00689592,"k are constrained using exclusively parallel data provided by the organization. For the English - Finnish language pair the data employed is the Europarl corpus version 7 and 8, Paracrawl corpus, Rapid corpus of EU press releases and Wiki Headlines corpus. For the English - Estonian data the Europarl v8 corpus, Paracrawl and Rapid corpus of EU press releases corpus were employed. All language pairs have been preprocessed following the proposed scripts by the organization of the conference. The pipeline consisted in normalizing punctuation, tokenization and truecasing using the standard Moses (Koehn et al., 2007) 356 scripts. With the addition that, for tokenization, no escaping of special characters was performed. For the language pair of English - Estonian we found that from Paracrawl corpus a considerable number of sentences were not suitable sentences in the intended languages, but apparently random sequences of upper case characters. In order to remove them, an additional step of language detection was performed using library langdetect (Danil´ak, 2017), which is a port to Python of library language-detection (Shuyo, 2010). The criteria for removing noisy sentences from the dataset was that eithe"
W18-6406,D18-1549,0,0.0139989,"e originally available parallel corpus and train on it a new source language to target language translation system. Pivoting approaches use a third resource-rich language as pivot and train translation systems from source language to pivot and from pivot to target language. These auxiliary systems can either be used in cascade to obtain source-to-target translations, or be used to build syntethic parallel source-target corpora (i.e. pseudocorpus approach). A recent application of pivoting techniques to NMT can be found in (Costa-juss`a et al., 2018). Adversarial learning (Lample et al., 2018; Artetxe et al., 2018) in a multi-task learning setup, 5 Corpora and Data preparation All proposed systems in this work are constrained using exclusively parallel data provided by the organization. For the English - Finnish language pair the data employed is the Europarl corpus version 7 and 8, Paracrawl corpus, Rapid corpus of EU press releases and Wiki Headlines corpus. For the English - Estonian data the Europarl v8 corpus, Paracrawl and Rapid corpus of EU press releases corpus were employed. All language pairs have been preprocessed following the proposed scripts by the organization of the conference. The pipel"
W18-6406,D14-1179,0,0.0327078,"Missing"
W18-6406,D15-1166,0,0.0570156,"get language, the source sentence is prefixed with a token that specifies which language the target sentence belongs to. This approach aims at implicitly learning language-independent internal representations, enabling the translation of low resource language pairs (and even language pairs where there is zero parallel data available) to profit from the combined language pair training data. The first competitive NMT systems were based on the sequence-to-sequence architecture (Cho et al., 2014; Sutskever et al., 2014), especially with the addition of attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015), either using Gated Recurrent Units (GRU) (Cho et al., 2014) or LongShort Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997). Sequence-to-sequence with attention was the state of the art NMT model until the Transformer architecture (Vaswani et al., 2017) was proposed. This model does not rely on recurrent units or convolutional networks, but only on attention layers, combining them with several other architectural elements: positional embeddings (Gehring et al., 2017), layer normalization (Ba et al., 2016), residual connections (He et al., 2016) and dropout (Srivastava et al., 2014)."
W18-6406,P16-1009,0,0.0508579,"Missing"
W18-6406,1983.tc-1.13,0,0.656265,"Missing"
W19-3811,N06-2015,0,0.136549,"Missing"
W19-3811,Q18-1042,0,0.0610365,"odel 2. replacement names 1 and 2 are switched. So, as figure 3 shows, we get one text with each name in each position. For example lets say we get the text: ”In the late 1980s Jones began working with 78 3 Experimental Framework 3.1 Task details Name A Name B None The objective of the task is that of a classification problem. Where the output for every entry is the probability of the pronoun referencing name A, name B or Neither. 3.2 Stage 1 Train 1105 1060 289 Test 874 925 201 Stage 2 Train 1979 1985 490 Table 1: Dataset distribution for the datasets of stages 1 and 2. Data The GAP dataset (Webster et al., 2018) created by Google AI Language was the dataset used for this task. This dataset consists of 8908 co-reference labeled pairs sampled from Wikipedia, also it’s split perfectly between male and female representation. Each entry of the dataset consists of a short text, a pronoun that is present in the text and its offset and two different names (name A and name B) also present in the text. The pronoun refers to one of these two names and in some cases, none of them. The GAP dataset doesn’t contain any neutral pronouns such as it or they. For the two different stages of the competition different da"
W19-5311,D16-1026,0,0.148645,"Missing"
W19-5311,W17-4102,0,0.0231212,"Missing"
W19-5311,W08-0509,0,0.0461152,"re are some precedents for subword tokenization in SMT, like the work by Kunchukuttan and Bhattacharyya (2016, 2017). The use of subword tokenization leads to longer token sequence lengths compared to the usual word-based vocabularies of SMT systems. In order to cope with this fact, we configured the subword-based SMT systems to have longer ngram order for their Language Models (LM) and phrase tables: the typical n-gram order used is 3 and we used 6. All other Moses configuration settings are the standard ones, using KenLM as language model (Heafield, 2011; Heafield et al., 2013) and MGIZA++ (Gao and Vogel, 2008) for alignment. The data used to create the respective target-side LMs consisted of the target side of the parallel data used for training. Some improvement could have been gained by using the available extra monolingual English and Kazakh data for the LMs. pora based on the Russian data and then combine it with the parallel English-Kazakh data. Further justification of the technique used can be found in section 2. In pivoting approaches, the final translation quality does not get influenced significantly if synthetic data is used for the source language side; on the other hand, using syntheti"
W19-5311,N19-4009,0,0.0318804,"vel tokenization (SMT(w)): we trained a Moses system on the parallel Kazakh-English data, using normal word-level tokenization • Statistical Machine Translation with subword-level tokenization (SMT(sw)): we trained a Moses system on the parallel Kazakh-English data, using BPE tokenization with 10K merge operations2 . Moses default values were used for the rest of configuration settings . • Neural Machine Translation (NMT): we trained a Transformer model on the parallel Kazakh-English data, using BPE tokenization with 10K merge operations, separately for source and target. We used the fairseq (Ott et al., 2019) implementation with the same hyperparameters as the IWSLT model, namely an embedding dimensionality of 512, 6 layers of attention, 4 attention heads and 1024 for the feedwordward expansion dimensionality. The translation quality BLEU scores of the aforedescribed baselines were very low, as shown in table 4. In order to evaluate the pivot translation systems described in section 5.1, we also measured the BLEU scores in the respective held out test sets, obtaining 36.05 BLEU for the Russian→English system and 21.06 for the Russian→Kazakh system. With these pivot systems, we created two pseudo-p"
W19-5311,P02-1040,0,0.10634,"In order to evaluate the pivot translation systems described in section 5.1, we also measured the BLEU scores in the respective held out test sets, obtaining 36.05 BLEU for the Russian→English system and 21.06 for the Russian→Kazakh system. With these pivot systems, we created two pseudo-parallel synthetic corpora, merged them with the parallel data and trained a self-attention NMT model that obtained BLEU scores one order of magnitude above the chosen baselines, as shown in table 4. Experiments and Results In order to assess the translation quality of the systems, we computed the BLEU score (Papineni et al., 2002) over the respective held out test sets. As there is not much literature of current NMT approaches being applied to English-Kazakh, we prepared different baselines to gauge the range of BLEU values to expect: • Rule-based machine translation system (RBMT): we used the Apertium system (Forcada et al., 2011; Sundetova et al., 2014; Assem and Aida, 2013), which is based on transfer rules distilled from linguistic knowledge. Using the BLEU score to compare an 2 The low number of BPE merge operations is justified with the low amount of training data 159 When we tested the final Kazakh→English syste"
W19-5311,W11-2123,0,0.0175043,"2K merge operations each. Although not frequent, there are some precedents for subword tokenization in SMT, like the work by Kunchukuttan and Bhattacharyya (2016, 2017). The use of subword tokenization leads to longer token sequence lengths compared to the usual word-based vocabularies of SMT systems. In order to cope with this fact, we configured the subword-based SMT systems to have longer ngram order for their Language Models (LM) and phrase tables: the typical n-gram order used is 3 and we used 6. All other Moses configuration settings are the standard ones, using KenLM as language model (Heafield, 2011; Heafield et al., 2013) and MGIZA++ (Gao and Vogel, 2008) for alignment. The data used to create the respective target-side LMs consisted of the target side of the parallel data used for training. Some improvement could have been gained by using the available extra monolingual English and Kazakh data for the LMs. pora based on the Russian data and then combine it with the parallel English-Kazakh data. Further justification of the technique used can be found in section 2. In pivoting approaches, the final translation quality does not get influenced significantly if synthetic data is used for t"
W19-5311,W17-2619,0,0.0708846,"ion of a synthetic pseudo-parallel corpus of translated data between the source and target language through the pivot, and train a system as done in the back translation approach. Finally, multilingual systems are recently showing nice improvements. Among the different types of multilingual systems there are the many-to-one approaches and the many-to-many approaches. The former is aiming to translate to one single language and can simply concatenate source languages (Zoph and Knight, 2016; Tubay and Costajuss`a, 2018). However, the latter either needs to use independent encoders and decoders (Schwenk and Douze, 2017; Firat et al., 2016; Escolano et al., 2019) or when using universal encoder and decoders (Johnson et al., 2017) needs to add a tag in the source input to let the system know to which language it is translating. This many-to-many systems are an alternative to pivot systems. However, most these multilingual systems are not able to achieve the level of performance of pivot systems yet. In the frame of the WMT19 news translation shared task several of the aforementioned techniques are applicable. An English+Russian→Kakakh multilingual system could be trained, but the amount of Kazakh-Russian data"
W19-5311,P13-2121,0,0.0630838,"Missing"
W19-5311,P16-1009,0,0.339049,"age. This way, we used English-Russian and Kazakh-Russian data to train intermediate translation systems that we then 2 Low-resource NMT There are several different approaches that can improve translation quality in under-resourced scenarios. In this section, we provide an overview of some of the dominant techniques and justify their application in the frame of this shared task. While for low resource languages there is limited parallel data, monolingual data is often available in greater quantities. A common strategy to integrate this monolingual data into the NMT system is back-translation (Sennrich et al., 2016a), which consists in generating synthetic data by translating monolingual data of the target language into the source language that would be then fed to the system to further train it. 155 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 155–162 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics to this scenario. The cascade approach, however, would not allow to profit from the existing parallel English-Kazakh data, making the pseudo-parallel corpus approach the most sensible option. Another common s"
W19-5311,P16-1162,0,0.474151,"age. This way, we used English-Russian and Kazakh-Russian data to train intermediate translation systems that we then 2 Low-resource NMT There are several different approaches that can improve translation quality in under-resourced scenarios. In this section, we provide an overview of some of the dominant techniques and justify their application in the frame of this shared task. While for low resource languages there is limited parallel data, monolingual data is often available in greater quantities. A common strategy to integrate this monolingual data into the NMT system is back-translation (Sennrich et al., 2016a), which consists in generating synthetic data by translating monolingual data of the target language into the source language that would be then fed to the system to further train it. 155 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 155–162 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics to this scenario. The cascade approach, however, would not allow to profit from the existing parallel English-Kazakh data, making the pseudo-parallel corpus approach the most sensible option. Another common s"
W19-5311,P07-2045,0,0.0187399,"Missing"
W19-5311,W18-6449,1,0.89165,"Missing"
W19-5311,P07-1108,0,0.140785,"Missing"
W19-5311,N07-1061,0,0.0969472,"Missing"
W19-5311,L16-1561,0,0.0137195,"ipts for preprocessing, including tokenization, truecasing and cleaning, using the same settings as for the aggressive EnglishRussian data cleaning described before. From the combined corpus, we extracted 4000 lines as development data and 1000 segments as hold out test set, leaving the rest for training. The statistics of the resulting training corpus are shown in table 3. English-Russian The available parallel English-Russian corpora for the shared task included News Commentary v14, Wiki Titles v1, Common Crawl corpus, ParaCrawl v3, Yandex Corpus and the United Nations Parallel Corpus v1.0 (Ziemski et al., 2016). Following the rationale exposed for the EnglishKazakh Wiki Titles data, we also dropped the English-Russian Wiki Titles data. Among the other corpora, some are of very large size. In order to assemble a manageable final training dataset and taking into account the high presence of garbage in the crawled datasets, before combining the individual corpora, we filtered each corpus and selected from each a random sample of segments. For the filtering, we applied heuristic criteria based on our visual inspection of the data, including elimination of lines with repeated separation characters (like"
W19-5311,N16-1004,0,0.030125,"o the pivot to the target system, obtaining a source to target translation. An alternative to this approach could be the generation of a synthetic pseudo-parallel corpus of translated data between the source and target language through the pivot, and train a system as done in the back translation approach. Finally, multilingual systems are recently showing nice improvements. Among the different types of multilingual systems there are the many-to-one approaches and the many-to-many approaches. The former is aiming to translate to one single language and can simply concatenate source languages (Zoph and Knight, 2016; Tubay and Costajuss`a, 2018). However, the latter either needs to use independent encoders and decoders (Schwenk and Douze, 2017; Firat et al., 2016; Escolano et al., 2019) or when using universal encoder and decoders (Johnson et al., 2017) needs to add a tag in the source input to let the system know to which language it is translating. This many-to-many systems are an alternative to pivot systems. However, most these multilingual systems are not able to achieve the level of performance of pivot systems yet. In the frame of the WMT19 news translation shared task several of the aforementione"
W19-5418,L16-1470,0,0.782028,"Missing"
W19-5418,C18-1111,0,0.0156794,"ion Domain adaptation in Neural Machine Translation (NMT) remains one of the main challenges (Koehn and Knowles, 2017). Domain-specific translations are especially relevant for industrial applications where there is a need for achieving both fluency and terminology in translations. Current state-of-the-art NMT systems achieve high performances when trained with large-scale parallel corpora. However, most of the time, largescale parallel corpora are not available for specific domains. Consequently, NMT models perform poorly for domain-specific translation when trained in low-resource scenario (Chu and Wang, 2018). Several works have been proposed to overcome the lack of domain parallel data by leveraging on both monolingual domain data (Domhan and Hieber, 2017; Currey et al., 2017) and parallel out-of-domain data (Wang et al., 2017; van der Wees et al., 2017) to improve the performance of domain-specific systems. Furthermore, some attempts have been made to directly insert external knowledge into NMT models through termi2 BabelNet In our work, in order to collect biomedical terms, the domain category of each word was detected with the help of BabelNet (Navigli and Ponzetto, 2012). Specifically, we ext"
W19-5418,P02-1040,0,0.103767,"r sequence of tokens. It is also important to notice that all the terms that are not present in the terminology list, like ”hypertension” and ”clot” in the examples, might be split into subwords. These examples show how the effectiveness of bpe-term segmentation depends entirely on the size and quality of the terminology list. 4 5 Experiments This section describes the experiments we performed. We first start with the data collection and preprocessing processes. Then, we describe trained systems and their evaluations. Finally, we present the results of the competition in terms of BLEU score. (Papineni et al., 2002). 5.1 Data collection We gathered data from the resources provided in the official WMT19 web page and from the OPUS collection. For our submissions, all the available biomedical parallel sentences for en/es are chosen both in plain text and Dublin Core format. Then, data have been parsed and merged to create the training and validation sets. Finally, we cleaned the datasets by removing empty sentences and duplicates. In particular, we selected Scielo (Soares et al., 2018), (Neves et al., 2016), UFAL, Pubmed, Medline, IBECS (Villegas et al., 2018) and EMEA (Tiedemann, 2012) sources for the trai"
W19-5418,W17-4715,0,0.0137694,"t for industrial applications where there is a need for achieving both fluency and terminology in translations. Current state-of-the-art NMT systems achieve high performances when trained with large-scale parallel corpora. However, most of the time, largescale parallel corpora are not available for specific domains. Consequently, NMT models perform poorly for domain-specific translation when trained in low-resource scenario (Chu and Wang, 2018). Several works have been proposed to overcome the lack of domain parallel data by leveraging on both monolingual domain data (Domhan and Hieber, 2017; Currey et al., 2017) and parallel out-of-domain data (Wang et al., 2017; van der Wees et al., 2017) to improve the performance of domain-specific systems. Furthermore, some attempts have been made to directly insert external knowledge into NMT models through termi2 BabelNet In our work, in order to collect biomedical terms, the domain category of each word was detected with the help of BabelNet (Navigli and Ponzetto, 2012). Specifically, we extracted a list of biomedical terms from our training data using the BabelNet API. To capture biomedical-related domains, we refer to the ”biomedical” definition in the Babel"
W19-5418,D17-1158,0,0.0189004,"ns are especially relevant for industrial applications where there is a need for achieving both fluency and terminology in translations. Current state-of-the-art NMT systems achieve high performances when trained with large-scale parallel corpora. However, most of the time, largescale parallel corpora are not available for specific domains. Consequently, NMT models perform poorly for domain-specific translation when trained in low-resource scenario (Chu and Wang, 2018). Several works have been proposed to overcome the lack of domain parallel data by leveraging on both monolingual domain data (Domhan and Hieber, 2017; Currey et al., 2017) and parallel out-of-domain data (Wang et al., 2017; van der Wees et al., 2017) to improve the performance of domain-specific systems. Furthermore, some attempts have been made to directly insert external knowledge into NMT models through termi2 BabelNet In our work, in order to collect biomedical terms, the domain category of each word was detected with the help of BabelNet (Navigli and Ponzetto, 2012). Specifically, we extracted a list of biomedical terms from our training data using the BabelNet API. To capture biomedical-related domains, we refer to the ”biomedical” d"
W19-5418,L18-1546,0,0.31012,"scribe trained systems and their evaluations. Finally, we present the results of the competition in terms of BLEU score. (Papineni et al., 2002). 5.1 Data collection We gathered data from the resources provided in the official WMT19 web page and from the OPUS collection. For our submissions, all the available biomedical parallel sentences for en/es are chosen both in plain text and Dublin Core format. Then, data have been parsed and merged to create the training and validation sets. Finally, we cleaned the datasets by removing empty sentences and duplicates. In particular, we selected Scielo (Soares et al., 2018), (Neves et al., 2016), UFAL, Pubmed, Medline, IBECS (Villegas et al., 2018) and EMEA (Tiedemann, 2012) sources for the training set and Khresmoi (Duˇsek et al., 2017) for the validation set. Domain features Following the domain control approach (Kobus et al., 2016), we enrich the data with a word-level binary feature by means of the biomedical terminology. Every word belonging to the terminology list has been labelled as biomedical, while all others as a general domain. The resulting binary feature is then embedded into a dense vector and combined with the word vector. The most common combina"
W19-5418,tiedemann-2012-parallel,0,0.0360055,"f BLEU score. (Papineni et al., 2002). 5.1 Data collection We gathered data from the resources provided in the official WMT19 web page and from the OPUS collection. For our submissions, all the available biomedical parallel sentences for en/es are chosen both in plain text and Dublin Core format. Then, data have been parsed and merged to create the training and validation sets. Finally, we cleaned the datasets by removing empty sentences and duplicates. In particular, we selected Scielo (Soares et al., 2018), (Neves et al., 2016), UFAL, Pubmed, Medline, IBECS (Villegas et al., 2018) and EMEA (Tiedemann, 2012) sources for the training set and Khresmoi (Duˇsek et al., 2017) for the validation set. Domain features Following the domain control approach (Kobus et al., 2016), we enrich the data with a word-level binary feature by means of the biomedical terminology. Every word belonging to the terminology list has been labelled as biomedical, while all others as a general domain. The resulting binary feature is then embedded into a dense vector and combined with the word vector. The most common combination strategy consists in concatenating the feature embedding with the word em5.2 Data preprocessing Da"
W19-5418,P07-2045,0,0.0244784,"word belonging to the terminology list has been labelled as biomedical, while all others as a general domain. The resulting binary feature is then embedded into a dense vector and combined with the word vector. The most common combination strategy consists in concatenating the feature embedding with the word em5.2 Data preprocessing Data are preprocessed following the standard pipeline by normalizing punctuation, tokenization and true-casing. We also removed sentences longer than 80 tokens and shorter than 2 tokens. For the previous steps, we used the scripts found in the Moses distribution (Koehn et al., 2007). Eventually, we trained shared byte-pairs encoding (BPE) (Sennrich et al., 2015) on both source and 152 Segmentation Bpe Sentence ”the intr@@ ig@@ u@@ ing pro@@ ble@@ m of cal@@ ci@@ fic@@ ation and os@@ s@@ ific@@ ation ; ne@@ ed to un@@ der@@ st@@ and it for the comp@@ re@@ h@@ ens@@ ion of b@@ one phys@@ io@@ path@@ ology .” ”inhibition of T@@ AF@@ I activity also resulted in a tw@@ of@@ old increase in clot lysis whereas inhibition of both factor XI and T@@ AF@@ I activity had no additional effect . ” ”a 5@@ 7-@@ year-old male with hepatos@@ plen@@ omegaly , p@@ ancy@@ topenia and hyperte"
W19-5418,P17-2089,0,0.0175669,"r achieving both fluency and terminology in translations. Current state-of-the-art NMT systems achieve high performances when trained with large-scale parallel corpora. However, most of the time, largescale parallel corpora are not available for specific domains. Consequently, NMT models perform poorly for domain-specific translation when trained in low-resource scenario (Chu and Wang, 2018). Several works have been proposed to overcome the lack of domain parallel data by leveraging on both monolingual domain data (Domhan and Hieber, 2017; Currey et al., 2017) and parallel out-of-domain data (Wang et al., 2017; van der Wees et al., 2017) to improve the performance of domain-specific systems. Furthermore, some attempts have been made to directly insert external knowledge into NMT models through termi2 BabelNet In our work, in order to collect biomedical terms, the domain category of each word was detected with the help of BabelNet (Navigli and Ponzetto, 2012). Specifically, we extracted a list of biomedical terms from our training data using the BabelNet API. To capture biomedical-related domains, we refer to the ”biomedical” definition in the BabelNet as stated, ”The science of dealing with the mai"
W19-5418,W17-3204,0,0.0133577,"lies only on one ingredient, a biomedical terminology list. We first extracted such a terminology list by labelling biomedical words in our training dataset using the BabelNet API. Then, we designed a data preparation strategy to insert the terms information at a token level. Finally, we trained the Transformer model (Vaswani et al., 2017) with this termsinformed data. Our best-submitted system ranked 2nd and 3rd for Spanish-English and English-Spanish translation directions, respectively. 1 Introduction Domain adaptation in Neural Machine Translation (NMT) remains one of the main challenges (Koehn and Knowles, 2017). Domain-specific translations are especially relevant for industrial applications where there is a need for achieving both fluency and terminology in translations. Current state-of-the-art NMT systems achieve high performances when trained with large-scale parallel corpora. However, most of the time, largescale parallel corpora are not available for specific domains. Consequently, NMT models perform poorly for domain-specific translation when trained in low-resource scenario (Chu and Wang, 2018). Several works have been proposed to overcome the lack of domain parallel data by leveraging on bo"
