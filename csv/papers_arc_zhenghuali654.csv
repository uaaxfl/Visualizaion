2021.naacl-main.144,A Unified Span-Based Approach for Opinion Mining with Syntactic Constituents,2021,-1,-1,4,1,3688,qingrong xia,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Fine-grained opinion mining (OM) has achieved increasing attraction in the natural language processing (NLP) community, which aims to find the opinion structures of {``}Who expressed what opinions towards what{''} in one sentence. In this work, motivated by its span-based representations of opinion expressions and roles, we propose a unified span-based approach for the end-to-end OM setting. Furthermore, inspired by the unified span-based formalism of OM and constituent parsing, we explore two different methods (multi-task learning and graph convolutional neural network) to integrate syntactic constituents into the proposed model to help OM. We conduct experiments on the commonly used MPQA 2.0 dataset. The experimental results show that our proposed unified span-based approach achieves significant improvements over previous works in the exact F1 score and reduces the number of wrongly-predicted opinion expressions and roles, showing the effectiveness of our method. In addition, incorporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations."
2021.findings-emnlp.149,{APGN}: Adversarial and Parameter Generation Networks for Multi-Source Cross-Domain Dependency Parsing,2021,-1,-1,3,0.869565,6800,ying li,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Thanks to the strong representation learning capability of deep learning, especially pre-training techniques with language model loss, dependency parsing has achieved great performance boost in the in-domain scenario with abundant labeled training data for target domains. However, the parsing community has to face the more realistic setting where the parsing performance drops drastically when labeled data only exists for several fixed out-domains. In this work, we propose a novel model for multi-source cross-domain dependency parsing. The model consists of two components, i.e., a parameter generation network for distinguishing domain-specific features, and an adversarial network for learning domain-invariant representations. Experiments on a recently released NLPCC-2019 dataset for multi-domain dependency parsing show that our model can consistently improve cross-domain parsing performance by about 2 points in averaged labeled attachment accuracy (LAS) over strong BERT-enhanced baselines. Detailed analysis is conducted to gain more insights on contributions of the two components."
2021.findings-emnlp.406,Stacked {AMR} Parsing with Silver Data,2021,-1,-1,2,1,3688,qingrong xia,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Lacking sufficient human-annotated data is one main challenge for abstract meaning representation (AMR) parsing. To alleviate this problem, previous works usually make use of silver data or pre-trained language models. In particular, one recent seq-to-seq work directly fine-tunes AMR graph sequences on the encoder-decoder pre-trained language model and achieves new state-of-the-art results, outperforming previous works by a large margin. However, it makes the decoding relatively slower. In this work, we investigate alternative approaches to achieve competitive performance at faster speeds. We propose a simplified AMR parser and a pre-training technique for the effective usage of silver data. We conduct extensive experiments on the widely used AMR2.0 dataset and the results demonstrate that our Transformer-based AMR parser achieves the best performance among the seq2graph-based models. Furthermore, with silver data, our model achieves competitive results with the SOTA model, and the speed is an order of magnitude faster. Detailed analyses are conducted to gain more insights into our proposed model and the effectiveness of the pre-training technique."
2021.emnlp-main.707,Data Augmentation with Hierarchical {SQL}-to-Question Generation for Cross-domain Text-to-{SQL} Parsing,2021,-1,-1,3,0,10058,kun wu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Data augmentation has attracted a lot of research attention in the deep learning era for its ability in alleviating data sparseness. The lack of labeled data for unseen evaluation databases is exactly the major challenge for cross-domain text-to-SQL parsing. Previous works either require human intervention to guarantee the quality of generated data, or fail to handle complex SQL queries. This paper presents a simple yet effective data augmentation framework. First, given a database, we automatically produce a large number of SQL queries based on an abstract syntax tree grammar. For better distribution matching, we require that at least 80{\%} of SQL patterns in the training data are covered by generated queries. Second, we propose a hierarchical SQL-to-question generation model to obtain high-quality natural language questions, which is the major contribution of this work. Finally, we design a simple sampling strategy that can greatly improve training efficiency given large amounts of generated data. Experiments on three cross-domain datasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement."
2021.conll-1.23,"A Coarse-to-Fine Labeling Framework for Joint Word Segmentation, {POS} Tagging, and Constituent Parsing",2021,-1,-1,3,0,11358,yang hou,Proceedings of the 25th Conference on Computational Natural Language Learning,0,"The most straightforward approach to joint word segmentation (WS), part-of-speech (POS) tagging, and constituent parsing is converting a word-level tree into a char-level tree, which, however, leads to two severe challenges. First, a larger label set (e.g., {\mbox{$\geq$}} 600) and longer inputs both increase computational costs. Second, it is difficult to rule out illegal trees containing conflicting production rules, which is important for reliable model evaluation. If a POS tag (like VV) is above a phrase tag (like VP) in the output tree, it becomes quite complex to decide word boundaries. To deal with both challenges, this work proposes a two-stage coarse-to-fine labeling framework for joint WS-POS-PAR. In the coarse labeling stage, the joint model outputs a bracketed tree, in which each node corresponds to one of four labels (i.e., phrase, subphrase, word, subword). The tree is guaranteed to be legal via constrained CKY decoding. In the fine labeling stage, the model expands each coarse label into a final label (such as VP, VP*, VV, VV*). Experiments on Chinese Penn Treebank 5.1 and 7.0 show that our joint model consistently outperforms the pipeline approach on both settings of w/o and w/ BERT, and achieves new state-of-the-art performance."
2021.ccl-1.48,æ°æ®æ æ³¨æ¹æ³æ¯è¾ç ç©¶:ä»¥ä¾å­å¥æ³æ æ æ³¨ä¸ºä¾(Comparison Study on Data Annotation Approaches: Dependency Tree Annotation as Case Study),2021,-1,-1,3,0,11772,mingyue zhou,Proceedings of the 20th Chinese National Conference on Computational Linguistics,0,"{``}æ°æ®æ æ³¨æéè¦çèèå ç´ æ¯æ°æ®çè´¨éåæ æ³¨ä»£ä»·ãæä»¬è°ç åç°èªç¶è¯­è¨å¤çé¢åçæ°æ®æ æ³¨å·¥ä½éå¸¸éç¨æºæ äººæ ¡çæ æ³¨æ¹æ³ä»¥éä½ä»£ä»·;åæ¶,å¾å°æå·¥ä½ä¸¥æ ¼å¯¹æ¯ä¸åæ æ³¨æ¹æ³,ä»¥æ¢è®¨æ æ³¨æ¹æ³å¯¹æ æ³¨è´¨éåä»£ä»·çå½±åãè¯¥æåå©ä¸ä¸ªæççæ æ³¨å¢é,ä»¥ä¾å­å¥æ³æ°æ®æ æ³¨ä¸ºæ¡ä¾,å®éªå¯¹æ¯äºæºæ äººæ ¡ãåäººç¬ç«æ æ³¨ãåæ¬æéè¿èååä¸¤ç§æ¹æ³ææ°æåºçäººæºç¬ç«æ æ³¨æ¹æ³,å¾å°äºä¸äºåæ­¥çç»è®ºã{''}"
2021.acl-long.452,An In-depth Study on Internal Structure of {C}hinese Words,2021,-1,-1,4,1,11773,chen gong,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Unlike English letters, Chinese characters have rich and specific meanings. Usually, the meaning of a word can be derived from its constituent characters in some way. Several previous works on syntactic parsing propose to annotate shallow word-internal structures for better utilizing character-level information. This work proposes to model the deep internal structures of Chinese words as dependency trees with 11 labels for distinguishing syntactic relationships. First, based on newly compiled annotation guidelines, we manually annotate a word-internal structure treebank (WIST) consisting of over 30K multi-char words from Chinese Penn Treebank. To guarantee quality, each word is independently annotated by two annotators and inconsistencies are handled by a third senior annotator. Second, we present detailed and interesting analysis on WIST to reveal insights on Chinese word formation. Third, we propose word-internal structure parsing as a new task, and conduct benchmark experiments using a competitive dependency parser. Finally, we present two simple ways to encode word-internal structures, leading to promising gains on the sentence-level syntactic parsing task."
2020.emnlp-main.562,{D}u{SQL}: A Large-Scale and Pragmatic {C}hinese Text-to-{SQL} Dataset,2020,-1,-1,5,0,10059,lijie wang,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Due to the lack of labeled data, previous research on text-to-SQL parsing mainly focuses on English. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the cross-domain text-to-SQL task, containing 200 databases, 813 tables, and 23,797 question/SQL pairs. Our new dataset has three major characteristics. First, by manually analyzing questions from several representative applications, we try to figure out the true distribution of SQL queries in real-life needs. Second, DuSQL contains a considerable proportion of SQL queries involving row or column calculations, motivated by our analysis on the SQL query distributions. Finally, we adopt an effective data construction framework via human-computer collaboration. The basic idea is automatically generating SQL queries based on the SQL grammar and constrained by the given database. This paper describes in detail the construction process and data statistics of DuSQL. Moreover, we present and compare performance of several open-source text-to-SQL parsers with minor modification to accommodate Chinese, including a simple yet effective extension to IRNet for handling calculation SQL queries."
2020.coling-main.183,Multi-grained {C}hinese Word Segmentation with Weakly Labeled Data,2020,-1,-1,2,1,11773,chen gong,Proceedings of the 28th International Conference on Computational Linguistics,0,"In contrast with the traditional single-grained word segmentation (SWS), where a sentence corresponds to a single word sequence, multi-grained Chinese word segmentation (MWS) aims to segment a sentence into multiple word sequences to preserve all words of different granularities. Due to the lack of manually annotated MWS data, previous work train and tune MWS models only on automatically generated pseudo MWS data. In this work, we further take advantage of the rich word boundary information in existing SWS data and naturally annotated data from dictionary example (DictEx) sentences, to advance the state-of-the-art MWS model based on the idea of weak supervision. Particularly, we propose to accommodate two types of weakly labeled data for MWS, i.e., SWS data and DictEx data by employing a simple yet competitive graph-based parser with local loss. Besides, we manually annotate a high-quality MWS dataset according to our newly compiled annotation guideline, consisting of over 9,000 sentences from two types of texts, i.e., canonical newswire (NEWS) and non-canonical web (BAIKE) data for better evaluation. Detailed evaluation shows that our proposed model with weakly labeled data significantly outperforms the state-of-the-art MWS model by 1.12 and 5.97 on NEWS and BAIKE data in F1."
2020.coling-main.266,Semantic Role Labeling with Heterogeneous Syntactic Knowledge,2020,-1,-1,3,1,3688,qingrong xia,Proceedings of the 28th International Conference on Computational Linguistics,0,"Recently, due to the interplay between syntax and semantics, incorporating syntactic knowledge into neural semantic role labeling (SRL) has achieved much attention. Most of the previous syntax-aware SRL works focus on explicitly modeling homogeneous syntactic knowledge over tree outputs. In this work, we propose to encode \textit{heterogeneous} syntactic knowledge for SRL from both explicit and implicit representations. First, we introduce graph convolutional networks to explicitly encode multiple heterogeneous dependency parse trees. Second, we extract the implicit syntactic representations from syntactic parser trained with heterogeneous treebanks. Finally, we inject the two types of heterogeneous syntax-aware representations into the base SRL model as extra inputs. We conduct experiments on two widely-used benchmark datasets, i.e., Chinese Proposition Bank 1.0 and English CoNLL-2005 dataset. Experimental results show that incorporating heterogeneous syntactic knowledge brings significant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge."
2020.coling-main.338,Semi-supervised Domain Adaptation for Dependency Parsing via Improved Contextualized Word Representations,2020,-1,-1,2,0.869565,6800,ying li,Proceedings of the 28th International Conference on Computational Linguistics,0,"In recent years, parsing performance is dramatically improved on in-domain texts thanks to the rapid progress of deep neural network models. The major challenge for current parsing research is to improve parsing performance on out-of-domain texts that are very different from the in-domain training data when there is only a small-scale out-domain labeled data. To deal with this problem, we propose to improve the contextualized word representations via adversarial learning and fine-tuning BERT processes. Concretely, we apply adversarial learning to three representative semi-supervised domain adaption methods, i.e., direct concatenation (CON), feature augmentation (FA), and domain embedding (DE) with two useful strategies, i.e., fused target-domain word representations and orthogonality constraints, thus enabling to model more pure yet effective domain-specific and domain-invariant representations. Simultaneously, we utilize a large-scale target-domain unlabeled data to fine-tune BERT with only the language model loss, thus obtaining reliable contextualized word representations that benefit for the cross-domain dependency parsing. Experiments on a benchmark dataset show that our proposed adversarial approaches achieve consistent improvement, and fine-tuning BERT further boosts parsing accuracy by a large margin. Our single model achieves the same state-of-the-art performance as the top submitted system in the NLPCC-2019 shared task, which uses ensemble models and BERT."
2020.acl-main.297,Syntax-Aware Opinion Role Labeling with Dependency Graph Convolutional Networks,2020,-1,-1,4,0.931234,3689,bo zhang,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Opinion role labeling (ORL) is a fine-grained opinion analysis task and aims to answer {``}who expressed what kind of sentiment towards what?{''}. Due to the scarcity of labeled data, ORL remains challenging for data-driven methods. In this work, we try to enhance neural ORL models with syntactic knowledge by comparing and integrating different representations. We also propose dependency graph convolutional networks (DEPGCN) to encode parser information at different processing levels. In order to compensate for parser inaccuracy and reduce error propagation, we introduce multi-task learning (MTL) to train the parser and the ORL model simultaneously. We verify our methods on the benchmark MPQA corpus. The experimental results show that syntactic information is highly valuable for ORL, and our final MTL model effectively boosts the F1 score by 9.29 over the syntax-agnostic baseline. In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT). Our best model achieves 4.34 higher F1 score than the current state-ofthe-art."
2020.acl-main.302,Efficient Second-Order {T}ree{CRF} for Neural Dependency Parsing,2020,37,0,2,0,8660,yu zhang,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation. As the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss. This paper for the first time presents a second-order TreeCRF extension to the biaffine parser. For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF. To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation. Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data. We release our code at https://github.com/yzhangcs/crfpar."
S19-2002,{HLT}@{SUDA} at {S}em{E}val-2019 Task 1: {UCCA} Graph Parsing as Constituent Tree Parsing,2019,10,1,2,0,24937,wei jiang,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"This paper describes a simple UCCA semantic graph parsing approach. The key idea is to convert a UCCA semantic graph into a constituent tree, in which extra labels are deliberately designed to mark remote edges and discontinuous nodes for future recovery. In this way, we can make use of existing syntactic parsing techniques. Based on the data statistics, we recover discontinuous nodes directly according to the output labels of the constituent parser and use a biaffine classification model to recover the more complex remote edges. The classification model and the constituent parser are simultaneously trained under the multi-task learning framework. We use the multilingual BERT as extra features in the open tracks. Our system ranks the first place in the six English/German closed/open tracks among seven participating systems. For the seventh cross-lingual track, where there is little training data for French, we propose a language embedding approach to utilize English and German training data, and our result ranks the second place."
P19-1229,Semi-supervised Domain Adaptation for Dependency Parsing,2019,0,2,1,1,3691,zhenghua li,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"During the past decades, due to the lack of sufficient labeled data, most studies on cross-domain parsing focus on unsupervised domain adaptation, assuming there is no target-domain training data. However, unsupervised approaches make limited progress so far due to the intrinsic difficulty of both domain adaptation and parsing. This paper tackles the semi-supervised domain adaptation problem for Chinese dependency parsing, based on two newly-annotated large-scale domain-aware datasets. We propose a simple domain embedding approach to merge the source- and target-domain training data, which is shown to be more effective than both direct corpus concatenation and multi-task learning. In order to utilize unlabeled target-domain data, we employ the recent contextualized word representations and show that a simple fine-tuning procedure can further boost cross-domain parsing accuracy by large margin."
N19-1118,Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations,2019,0,7,2,0.168919,6801,meishan zhang,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Syntax has been demonstrated highly effective in neural machine translation (NMT). Previous NMT models integrate syntax by representing 1-best tree outputs from a well-trained parsing system, e.g., the representative Tree-RNN and Tree-Linearization methods, which may suffer from error propagation. In this work, we propose a novel method to integrate source-side syntax implicitly for NMT. The basic idea is to use the intermediate hidden representations of a well-trained end-to-end dependency parser, which are referred to as syntax-aware word representations (SAWRs). Then, we simply concatenate such SAWRs with ordinary word embeddings to enhance basic NMT models. The method can be straightforwardly integrated into the widely-used sequence-to-sequence (Seq2Seq) NMT models. We start with a representative RNN-based Seq2Seq baseline system, and test the effectiveness of our proposed method on two benchmark datasets of the Chinese-English and English-Vietnamese translation tasks, respectively. Experimental results show that the proposed approach is able to bring significant BLEU score improvements on the two datasets compared with the baseline, 1.74 points for Chinese-English translation and 0.80 point for English-Vietnamese translation, respectively. In addition, the approach also outperforms the explicit Tree-RNN and Tree-Linearization methods."
K19-2014,{SUDA}-{A}libaba at {MRP} 2019: Graph-Based Models with {BERT},2019,0,1,6,0,884,yue zhang,Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 Conference on Natural Language Learning,0,"In this paper, we describe our participating systems in the shared task on Cross- Framework Meaning Representation Parsing (MRP) at the 2019 Conference for Computational Language Learning (CoNLL). The task includes five frameworks for graph-based meaning representations, i.e., DM, PSD, EDS, UCCA, and AMR. One common characteristic of our systems is that we employ graph-based methods instead of transition-based methods when predicting edges between nodes. For SDP, we jointly perform edge prediction, frame tagging, and POS tagging via multi-task learning (MTL). For UCCA, we also jointly model a constituent tree parsing and a remote edge recovery task. For both EDS and AMR, we produce nodes first and edges second in a pipeline fashion. External resources like BERT are found helpful for all frameworks except AMR. Our final submission ranks the third on the overall MRP evaluation metric, the first on EDS and the second on UCCA."
D19-1541,A Syntax-aware Multi-task Learning Framework for {C}hinese Semantic Role Labeling,2019,0,0,2,1,3688,qingrong xia,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Semantic role labeling (SRL) aims to identify the predicate-argument structure of a sentence. Inspired by the strong correlation between syntax and semantics, previous works pay much attention to improve SRL performance on exploiting syntactic knowledge, achieving significant results. Pipeline methods based on automatic syntactic trees and multi-task learning (MTL) approaches using standard syntactic trees are two common research orientations. In this paper, we adopt a simple unified span-based model for both span-based and word-based Chinese SRL as a strong baseline. Besides, we present a MTL framework that includes the basic SRL module and a dependency parser module. Different from the commonly used hard parameter sharing strategy in MTL, the main idea is to extract implicit syntactic representations from the dependency parser as external inputs for the basic SRL model. Experiments on the benchmarks of Chinese Proposition Bank 1.0 and CoNLL-2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax."
P18-1252,Supervised Treebank Conversion: Data and Approaches,2018,0,8,2,0,29210,xinzhou jiang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Treebank conversion is a straightforward and effective way to exploit various heterogeneous treebanks for boosting parsing performance. However, previous work mainly focuses on unsupervised treebank conversion and has made little progress due to the lack of manually labeled data where each sentence has two syntactic trees complying with two different guidelines at the same time, referred as bi-tree aligned data. In this work, we for the first time propose the task of supervised treebank conversion. First, we manually construct a bi-tree aligned dataset containing over ten thousand sentences. Then, we propose two simple yet effective conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two conversion approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multi-treebank exploitation and leads to significantly higher parsing accuracy."
L18-1706,{M}-{CNER}: A Corpus for {C}hinese Named Entity Recognition in Multi-Domains,2018,0,0,3,0,30309,qi lu,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1183,Distantly Supervised {NER} with Partial Annotation Learning and Reinforcement Learning,2018,0,16,3,0,30310,yaosheng yang,Proceedings of the 27th International Conference on Computational Linguistics,0,"A bottleneck problem with Chinese named entity recognition (NER) in new domains is the lack of annotated data. One solution is to utilize the method of distant supervision, which has been widely used in relation extraction, to automatically populate annotated training data without humancost. The distant supervision assumption here is that if a string in text is included in a predefined dictionary of entities, the string might be an entity. However, this kind of auto-generated data suffers from two main problems: incomplete and noisy annotations, which affect the performance of NER models. In this paper, we propose a novel approach which can partially solve the above problems of distant supervision for NER. In our approach, to handle the incomplete problem, we apply partial annotation learning to reduce the effect of unknown labels of characters. As for noisy annotation, we design an instance selector based on reinforcement learning to distinguish positive sentences from auto-generated annotations. In experiments, we create two datasets for Chinese named entity recognition in two domains with the help of distant supervision. The experimental results show that the proposed approach obtains better performance than the comparison systems on both two datasets."
I17-1006,Dependency Parsing with Partial Annotations: An Empirical Comparison,2017,28,5,2,0,884,yue zhang,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"This paper describes and compares two straightforward approaches for dependency parsing with partial annotations (PA). The first approach is based on a forest-based training objective for two CRF parsers, i.e., a biaffine neural network graph-based parser (Biaffine) and a traditional log-linear graph-based parser (LLGPar). The second approach is based on the idea of constrained decoding for three parsers, i.e., a traditional linear graph-based parser (LGPar), a globally normalized neural network transition-based parser (GN3Par) and a traditional linear transition-based parser (LTPar). For the test phase, constrained decoding is also used for completing partial trees. We conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar."
D17-1072,Multi-Grained {C}hinese Word Segmentation,2017,0,1,2,1,11773,chen gong,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Traditionally, word segmentation (WS) adopts the single-grained formalism, where a sentence corresponds to a single word sequence. However, Sproat et al. (1997) show that the inter-native-speaker consistency ratio over Chinese word boundaries is only 76{\%}, indicating single-grained WS (SWS) imposes unnecessary challenges on both manual annotation and statistical modeling. Moreover, WS results of different granularities can be complementary and beneficial for high-level applications. This work proposes and addresses multi-grained WS (MWS). We build a large-scale pseudo MWS dataset for model training and tuning by leveraging the annotation heterogeneity of three SWS datasets. Then we manually annotate 1,500 test sentences with true MWS annotations. Finally, we propose three benchmark approaches by casting MWS as constituent parsing and sequence labeling. Experiments and analysis lead to many interesting findings."
P16-1033,Active Learning for Dependency Parsing with Partial Annotation,2016,37,4,1,1,3691,zhenghua li,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
K16-2021,Finding Arguments as Sequence Labeling in Discourse Parsing,2016,19,1,2,0,35478,ziwei fan,Proceedings of the {C}o{NLL}-16 shared task,0,"This paper describes our system for the CoNLL-2016 Shared Task on Shallow Discourse Parsing on English. We adopt a cascaded framework consisting of nine components, among which six are casted as sequence labeling tasks and the remaining three are treated as classification problems. All our sequence labeling and classification models are implemented based on linear models with averaged perceptron training. Our feature sets are mostly borrowed from previous works. The main focus of our effort is to recall cases when Arg1 locates at sentences far before the connective phrase, with some yet limited success."
D16-1072,Fast Coupled Sequence Labeling on Heterogeneous Annotations via Context-aware Pruning,2016,0,3,1,1,3691,zhenghua li,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1202,Distributed Representations for Building Profiles of Users and Items from Text Reviews,2016,19,1,3,0,21231,wenliang chen,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we propose an approach to learn distributed representations of users and items from text comments for recommendation systems. Traditional recommendation algorithms, e.g. collaborative filtering and matrix completion, are not designed to exploit the key information hidden in the text comments, while existing opinion mining methods do not provide direct support to recommendation systems with useful features on users and items. Our approach attempts to construct vectors to represent profiles of users and items under a unified framework to maximize word appearance likelihood. Then, the vector representations are used for a recommendation task in which we predict scores on unobserved user-item pairs without given texts. The recommendation-aware distributed representation approach is fully supported by effective and efficient learning algorithms over massive text archive. Our empirical evaluations on real datasets show that our system outperforms the state-of-the-art baseline systems."
P15-1172,Coupled Sequence Labeling on Heterogeneous Annotations: {POS} Tagging as a Case Study,2015,30,18,1,1,3691,zhenghua li,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"In order to effectively utilize multiple datasets with heterogeneous annotations, this paper proposes a coupled sequence labeling model that can directly learn and infer two heterogeneous annotations simultaneously, and to facilitate discussion we use Chinese part-ofspeech (POS) tagging as our case study. The key idea is to bundle two sets of POS tags together (e.g. xe2x80x9c[NN, n]xe2x80x9d), and build a conditional random field (CRF) based tagging model in the enlarged space of bundled tags with the help of ambiguous labelings. To train our model on two non-overlapping datasets that each has only one-side tags, we transform a one-side tag into a set of bundled tags by considering all possible mappings at the missing side and derive an objective function based on ambiguous labelings. The key advantage of our coupled model is to provide us with the flexibility of 1) incorporating joint features on the bundled tags to implicitly learn the loose mapping between heterogeneous annotations, and 2) exploring separate features on one-side tags to overcome the data sparseness problem of using only bundled tags. Experiments on benchmark datasets show that our coupled model significantly outperforms the state-ofthe-art baselines on both one-side POS tagging and annotation conversion tasks. The codes and newly annotated data are released for non-commercial usage.1 xe2x88x97Correspondence author. http://hlt.suda.edu.cn/ zhli"
W14-6835,"{C}hinese Spelling Error Detection and Correction Based on Language Model, Pronunciation, and Shape",2014,6,8,2,0,21229,junjie yu,Proceedings of The Third {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"S pell ing check is an important preprocessing task when dealing with user generated texts such as tweets and product comments. Compared with some western languages such as English, Chinese spelling check is more complex because there is no word delimiter in Chinese written texts and misspelled characters can only be determined in word level. Our system works as follows. First, we use character-level n-gram language model s to detect potential misspelled characters with low probabilities below some predefined threshold. Second, for each potential incorrect character, we generate a candidate set based on pronunciation and shape similarities. Third, we filter some candidate corrections if the candidate cannot form a legal word with its neighbors according to a word dictionary. Finally, we find the best candidate with highest language model probability. If the probability is higher than a predefined threshold, then we replace the original character; or we consider the original character as correct and take no action. Our preliminary experiments shows that our simple method can achieve relatively high precision but low recall."
P14-1043,Ambiguity-aware Ensemble Training for Semi-supervised Dependency Parsing,2014,43,15,1,1,3691,zhenghua li,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level,"
C14-3006,"Dependency Parsing: Past, Present, and Future",2014,0,0,2,0,21231,wenliang chen,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts",0,"Dependency parsing has gained more and more interest in natural language processing in recent years due to its simplicity and general applicability for diverse languages. The international conference of computational natural language learning (CoNLL) has organized shared tasks on multilingual dependency parsing successively from 2006 to 2009, which leads to extensive progress on dependency parsing in both theoretical and practical perspectives. Meanwhile, dependency parsing has been successfully applied to machine translation, question answering, text mining, etc. To date, research on dependency parsing mainly focuses on data-driven supervised approaches and results show that the supervised models can achieve reasonable performance on in-domain texts for a variety of languages when manually labeled data is provided. However, relatively less effort is devoted to parsing out-domain texts and resource-poor languages, and few successful techniques are bought up for such scenario. This tutorial will cover all these research topics of dependency parsing and is composed of four major parts. Especially, we will survey the present progress of semi-supervised dependency parsing, web data parsing, and multilingual text parsing, and show some directions for future work. In the first part, we will introduce the fundamentals and supervised approaches for dependency parsing. The fundamentals include examples of dependency trees, annotated treebanks, evaluation metrics, and comparisons with other syntactic formulations like constituent parsing. Then we will introduce a few mainstream supervised approaches, i.e., transition-based, graph-based, easy-first, constituent-based dependency parsing. These approaches study dependency parsing from different perspectives, and achieve comparable and state-of-the-art performance for a wide range of languages. Then we will move to the hybrid models that combine the advantages of the above approaches. We will also introduce recent work on efficient parsing techniques, joint lexical analysis and dependency parsing, multiple treebank exploitation, etc. In the second part, we will survey the work on semi-supervised dependency parsing techniques. Such work aims to explore unlabeled data so that the parser can achieve higher performance. This tutorial will present several successful techniques that utilize information from different levels: whole tree level, partial tree level, and lexical level. We will discuss the advantages and limitations of these existing techniques. In the third part, we will survey the work on dependency parsing techniques for domain adaptation and web data. To advance research on out-domain parsing, researchers have organized two shared tasks, i.e., the CoNLL 2007 shared task and the shared task of syntactic analysis of non-canonical languages (SANCL 2012). Both two shared tasks attracted many participants. These participants tried different techniques to adapt the parser trained on WSJ texts to out-domain texts with the help of large-scale unlabeled data. Especially, we will present a brief survey on text normalization, which is proven to be very useful for parsing web data. In the fourth part, we will introduce the recent work on exploiting multilingual texts for dependency parsing, which falls into two lines of research. The first line is to improve supervised dependency parser with multilingual texts. The intuition behind is that ambiguities in the target language may be unambiguous in the source language. The other line is multilingual transfer learning which aims to project the syntactic knowledge from the source language to the target language."
C14-1075,Soft Cross-lingual Syntax Projection for Dependency Parsing,2014,43,10,1,1,3691,zhenghua li,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper proposes a simple yet effective framework of soft cross-lingual syntax projection to transfer syntactic structures from source language to target language using monolingual treebanks and large-scale bilingual parallel text. Here, soft means that we only project reliable dependencies to compose high-quality target structures. The projected instances are then used as additional training data to improve the performance of supervised parsers. The major issues for this idea are 1) errors from the source-language parser and unsupervised word aligner; 2) intrinsic syntactic non-isomorphism between languages; 3) incomplete parse trees after projection. To handle the first two issues, we propose to use a probabilistic dependency parser trained on the target-language treebank, and prune out unlikely projected dependencies that have low marginal probabilities. To make use of the incomplete projected syntactic structures, we adopt a new learning technique based on ambiguous labelings. For a word that has no head words after projection, we enrich the projected structure with all other words as its candidate heads as long as the newly-added dependency does not cross any projected dependencies. In this way, the syntactic structure of a sentence becomes a parse forest (ambiguous labels) instead of a single parse tree. During training, the objective is to maximize the mixed likelihood of manually labeled instances and projected instances with ambiguous labelings. Experimental results on benchmark data show that our method significantly outperforms a strong baseline supervised parser and previous syntax projection methods."
P12-1071,Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars,2012,41,19,1,1,3691,zhenghua li,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guidelines for parsing. Several types of transformation patterns (TP) are designed to capture the systematic annotation inconsistencies among different tree-banks. Based on such TPs, we design quasi-synchronous grammar features to augment the baseline parsing models. Our approach can significantly advance the state-of-the-art parsing accuracy on two widely used target tree-banks (Penn Chinese Treebank 5.1 and 6.0) using the Chinese Dependency Treebank as the source treebank. The improvements are respectively 1.37% and 1.10% with automatic part-of-speech tags. Moreover, an indirect comparison indicates that our approach also outperforms previous work based on treebank conversion."
C12-1103,A Separately Passive-Aggressive Training Algorithm for Joint {POS} Tagging and Dependency Parsing,2012,34,20,1,1,3691,zhenghua li,Proceedings of {COLING} 2012,0,"Recent study shows that parsing accuracy can be largely improved by the joint optimization of part-of-speech (POS) tagging and dependency parsing. However, the POS tagging task does not benefit much from the joint framework. We argue that the fundamental reason behind is because the POS features are overwhelmed by the syntactic features during the joint optimization, and the joint models only prefer such POS tags that are favourable solely from the parsing viewpoint. To solve this issue, we propose a separately passive-aggressive learning algorithm (SPA), which is designed to separately update the POS features weights and the syntactic feature weights under the joint optimization framework. The proposed SPA is able to take advantage of previous joint optimization strategies to significantly improve the parsing accuracy, but also overcome their shortages to significantly boost the tagging accuracy by effectively solving the syntax-insensitive POS ambiguity issues. Experiments on the Chinese Penn Treebank 5.1 (CTB5) and the English Penn Treebank (PTB) demonstrate the effectiveness of our proposed methodology and empirically verify our observations as discussed above. We achieve the best tagging and parsing accuracies on both datasets, 94.60% in tagging accuracy and 81.67% in parsing accuracy on CTB5, and 97.62% and 93.52% on PTB."
C12-1188,Stacking Heterogeneous Joint Models of {C}hinese {POS} Tagging and Dependency Parsing,2012,34,4,4,0.141509,6801,meishan zhang,Proceedings of {COLING} 2012,0,"Previous joint models of Chinese part-of-speech (POS) tagging and dependency parsing are extended from either graphor transition-based dependency models. Our analysis shows that the two models have different error distributions. In addition, integration of graphand transition-based dependency parsers by stacked learning (stacking) has achieved significant improvements. These motivate us to study the problem of stacking graphand transition-based joint models. We conduct experiments on Chinese Penn Treebank 5.1 (CTB5.1). The results demonstrate that the guided transition-based joint model obtains better performance than the guided graph-based joint model. Further, we introduce a constituent-based joint model which derives the POS tag sequence and dependency tree from the output of PCFG parsers, and then integrate it into the guided transition-based joint model. Finally, we achieve the best performance on CTB5.1, 94.95% in tagging accuracy and 83.98% in parsing accuracy respectively. TITLE AND ABSTRACT IN CHINESE xe9x87x87xe7x94xa8xe5xa0x86xe6x96xb9xe6xb3x95xe8x9ex8dxe5x90x88xe5xbcx82xe7xa7x8dxe7x9ax84xe4xb8xadxe6x96x87xe8xafx8dxe6x80xa7xe5x92x8cxe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8b xe8xbfx87xe5x8exbbxe7x9ax84xe4xb8xadxe6x96x87xe8xafx8dxe6x80xa7xe5x92x8cxe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8bxe5x9fxbaxe6x9cxacxe4xb8x8axe9x83xbdxe6xa0xb9xe6x8dxaexe5x9fxbaxe4xbax8exe5x9bxbexe7x9ax84xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe5x88x86xe6x9ex90xe6xa8xa1xe5x9ex8bxe6x88x96xe8x80x85 xe5x9fxbaxe4xbax8exe8xbdxacxe7xa7xbbxe7x9ax84xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe5x88x86xe6x9ex90xe6xa8xa1xe5x9ex8bxe8xbfx9bxe8xa1x8cxe6x8bx93xe5xb1x95xe8x80x8cxe5xbdxa2xe6x88x90xe7x9ax84xe3x80x82xe6x88x91xe4xbbxacxe7x9ax84xe5x88x86xe6x9ex90xe7xbbx93xe6x9ex9cxe8xa1xa8xe6x98x8exe8xbfx99xe4xb8xa4xe7xa7x8dxe4xb8x8dxe5x90x8cxe7x9ax84xe6xa8xa1xe5x9ex8b xe9x94x99xe8xafxafxe5x88x86xe5xb8x83xe5xb9xb6xe4xb8x8dxe4xb8x80xe6xa0xb7,xe8x80x8cxe4xb8x94xe5x9cxa8xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe4xb8xad,xe5xb0x86xe5x9fxbaxe4xbax8exe5x9bxbexe7x9ax84xe6xa8xa1xe5x9ex8bxe5x92x8cxe5x9fxbaxe4xbax8exe8xbdxacxe7xa7xbbxe7x9ax84xe6xa8xa1xe5x9ex8bxe4xbdxbfxe7x94xa8xe5xa0x86xe6x96xb9xe6xb3x95xe8x9ex8d xe5x90x88xe4xb9x8bxe5x90x8e,xe8x83xbdxe5xa4x9fxe6x98xbexe8x91x97xe7x9ax84xe6x8fx90xe5x8dx87xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe7x9ax84xe6x80xa7xe8x83xbd,xe8xbfx99xe4xbax9bxe4xbfx83xe4xbdxbfxe6x88x91xe4xbbxacxe8xbfx9bxe4xb8x80xe6xadxa5xe7xa0x94xe7xa9xb6xe9x87x87xe7x94xa8xe5xa0x86xe6x96xb9xe6xb3x95xe5x8exbbxe8x9ex8dxe5x90x88xe5x9fxba xe4xbax8exe5x9bxbexe7x9ax84xe5x92x8cxe5x9fxbaxe4xbax8exe8xbdxacxe7xa7xbbxe7x9ax84xe8xafx8dxe6x80xa7xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8bxe3x80x82xe6x88x91xe4xbbxacxe5x9cxa8xe4xb8xadxe6x96x87xe5xaexbexe5xb7x9exe6xa0x91xe5xbax935.1xe7x89x88xe6x9cxac(CTB5.1)xe4xb8x8a xe8xbfx9bxe8xa1x8cxe8xafx95xe9xaax8c,xe5xaex9exe9xaax8cxe7xbbx93xe6x9ex9cxe8xa1xa8xe6x98x8e,xe7x9bxb8xe6xafx94xe4xbdxbfxe7x94xa8xe5x9fxbaxe4xbax8exe5x9bxbexe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8bxe4xb8xbaxe8xa2xabxe6x8cx87xe5xafxbcxe6xa8xa1xe5x9ex8b,xe9x87x87xe7x94xa8xe8xbdxacxe7xa7xbbxe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1 xe5x9ex8bxe4xb8xbaxe8xa2xabxe6x8cx87xe5xafxbcxe6xa8xa1xe5x9ex8bxe8x83xbdxe5x8fx96xe5xbex97xe8xbex83xe5xa5xbdxe7x9ax84xe6x80xa7xe8x83xbdxe3x80x82xe6x9bxb4xe8xbfx9bxe4xb8x80xe6xadxa5,xe6x88x91xe4xbbxacxe4xbbx8bxe7xbbx8dxe4xbax86xe5x9fxbaxe4xbax8exe7x9fxadxe8xafxadxe5x8fxa5xe6xb3x95xe7xbbx93xe6x9ex84xe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1 xe5x9ex8b,xe5xaex83xe4xbbx8exe4xb8x80xe4xb8xaaxe5x8fxa5xe5xadx90xe7x9ax84xe6xa6x82xe7x8ex87xe7x9fxadxe8xafxadxe6x96x87xe6xb3x95xe5x88x86xe6x9ex90xe5x99xa8xe8xbex93xe5x87xbaxe7xbbx93xe6x9ex9cxe4xb8xadxe6x8fx90xe5x8fx96xe5x8fxa5xe5xadx90xe7x9ax84xe8xafx8dxe6x80xa7xe5xbax8fxe5x88x97xe4xbbxa5xe5x8fx8axe4xbex9dxe5xadx98xe6xa0x91xe7xbbx93 xe6x9ex9c,xe7x84xb6xe5x90x8exe6x88x91xe4xbbxacxe9x87x87xe7x94xa8xe5x9fxbaxe4xbax8exe7x9fxadxe8xafxadxe5x8fxa5xe6xb3x95xe7xbbx93xe6x9ex84xe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8bxe6x9bxb4xe8xbfx9bxe4xb8x80xe6xadxa5xe6x8cx87xe5xafxbcxe5x9fxbaxe4xbax8exe8xbdxacxe7xa7xbbxe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8b,xe6x9cx80xe7xbbx88 xe6x88x91xe4xbbxacxe5x9cxa8CTB5.1xe7x9ax84xe6x95xb0xe6x8dxaexe4xb8x8axe5x8fx96xe5xbex97xe4xbax86xe6x9cx80xe5xa5xbdxe7xbbx93xe6x9ex9c,xe8xafx8dxe6x80xa7xe6xa0x87xe6xb3xa8xe5x87x86xe7xa1xaexe7x8ex87xe8xbexbexe5x88xb094.95%,xe5x90x8cxe6x97xb6,xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95 xe5x87x86xe7xa1xaexe7x8ex87xe8xbexbexe5x88xb083.98%xe3x80x82"
I11-1171,Improving {C}hinese {POS} Tagging with Dependency Parsing,2011,20,5,1,1,3691,zhenghua li,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Recent research usually models POS tagging as a sequential labeling problem, in which only local context features can be used. Due to the lack of morphological inflections, many tagging ambiguities in Chinese are difficult to handle unless consulting larger contexts. In this paper, we try to improve Chinese POS tagging by using long-distance dependencies produced by a statistical dependency parser. Experimental results show that, despite error propagation, the syntactic features can significantly improve the tagging accuracy from 93.88% to 94.41% (p < 10 xe2x88x925 ). Detailed analysis shows that these features are helpful for ambiguous pairs like {NN,VV} and {DEC,DEG}. 1"
D11-1109,Joint Models for {C}hinese {POS} Tagging and Dependency Parsing,2011,37,57,1,1,3691,zhenghua li,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Part-of-speech (POS) is an indispensable feature in dependency parsing. Current research usually models POS tagging and dependency parsing independently. This may suffer from error propagation problem. Our experiments show that parsing accuracy drops by about 6% when using automatic POS tags instead of gold ones. To solve this issue, this paper proposes a solution by jointly optimizing POS tagging and dependency parsing in a unique model. We design several joint models and their corresponding decoding algorithms to incorporate different feature sets. We further present an effective pruning strategy to reduce the search space of candidate POS tags, leading to significant improvement of parsing speed. Experimental results on Chinese Penn Treebank 5 show that our joint models significantly improve the state-of-the-art parsing accuracy by about 1.5%. Detailed analysis shows that the joint method is able to choose such POS tags that are more helpful and discriminative from parsing viewpoint. This is the fundamental reason of parsing accuracy improvement."
C10-3004,{LTP}: A {C}hinese Language Technology Platform,2010,6,261,2,0,1017,wanxiang che,Coling 2010: Demonstrations,0,"LTP (Language Technology Platform) is an integrated Chinese processing platform which includes a suite of high performance natural language processing (NLP) modules and relevant corpora. Especially for the syntactic and semantic parsing modules, we achieved good results in some relevant evaluations, such as CoNLL and SemEval. Based on XML internal data representation, users can easily use these modules and corpora by invoking DLL (Dynamic Link Library) or Web service APIs (Application Program Interface), and view the processing results directly by the visualization tool."
W09-1207,Multilingual Dependency-based Syntactic and Semantic Parsing,2009,17,46,2,0,1017,wanxiang che,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"Our CoNLL 2009 Shared Task system includes three cascaded components: syntactic parsing, predicate classification, and semantic role labeling. A pseudo-projective high-order graph-based model is used in our syntactic dependency parser. A support vector machine (SVM) model is used to classify predicate senses. Semantic role labeling is achieved using maximum entropy (MaxEnt) model based semantic role classification and integer linear programming (ILP) based post inference. Finally, we win the first place in the joint task, including both the closed and open challenges."
W08-2134,A Cascaded Syntactic and Semantic Dependency Parsing System,2008,5,29,2,0,1017,wanxiang che,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"We describe our CoNLL 2008 Shared Task system in this paper. The system includes two cascaded components: a syntactic and a semantic dependency parsers. A first-order projective MSTParser is used as our syntactic dependency parser. In order to overcome the shortcoming of the MSTParser, that it cannot model more global information, we add a relabeling stage after the parsing to distinguish some confusable labels, such as ADV, TMP, and LOC. Besides adding a predicate identification and a classification stages, our semantic dependency parsing simplifies the traditional four stages semantic role labeling into two: a maximum entropy based argument classification and an ILP-based post inference. Finally, we gain the overall labeled macro F1 = 82.66, which ranked the second position in the closed challenge."
