2005.jeptalnrecital-court.15,briscoe-carroll-2002-robust,0,0.0637296,"Missing"
2005.jeptalnrecital-court.15,C96-2183,0,0.314129,"Missing"
2008.eamt-1.17,N03-1017,0,0.0120284,"pendency does not occur in the phrase under consideration. The ordering of the features according to Information Gain values were consistent with that obtained for the Italian → English system of [6]. As expected, the source phrase as well as its concatenated POS tags are the most discriminative features for predicting the translation of the phrase. Immediate context words and POS tags are the next promising features, the right context being 6 7 8 It does perform slightly better than the IGTree algorithm used in [6]. We relied on an in-house version of the standard phrase extraction procedure [1] for collecting the context information required for each source phrase. Using POS instead of words as dependency targets slightly decreases performance. 116 12th EAMT conference, 22-23 September 2008, Hamburg, Germany more discriminative than the left one. Dependency-based features were found less informative, which can be explained by the fact that often, the immediate context already captures the discriminative information. In an attempt to boost the selection of the most probable target phrase according to context disambiguation, we added to the log-linear combination a binary feature prop"
2008.eamt-1.17,vilar-etal-2006-error,0,0.108912,"Missing"
2008.eamt-1.17,P07-1005,0,0.0616928,"lity P (e|f, C(f )) by simply normalizing those weights. We used the Tribl hybrid algorithm6 of the TiMBL software package [8] as a classifier. Building such a classifier is mainly a matter of collecting training examples {(hf, C(f )i, e)} for all the phrases f seen in context C(f ) and translated as e.7 This classification implicitly performs smoothing by returning the example in the tree matching on most features. In case of an exact match, the actual class (i.e. target phrase) seen in the training material is returned. In case of a mismatch, a majority vote is performed. Approaches such as [5, 6, 4] relied on information extracted from the immediate context of a source phrase. To begin with, we considered this information as well, and represented an example by a fixed-length vector encoding the words of the source phrase f , their POS tags, the POS tags of two words on the left and the two words on the right of f , as well as the associated words. This is illustrated in Figure 1. source our1 /prp [declaration2 /nn of3 /prp] rights4 /nns is5 /vbz unique6 /jj target notre1 d´eclaration2 des3 droits4 est5 unique6 example (hdeclaration of, nn prp,nil,prp,nns,vbz,nil,our,rights,isi,d´eclarati"
2008.eamt-1.17,2007.tmi-papers.28,0,0.228672,"rce phrase, and e is a target phrase. 114 12th EAMT conference, 22-23 September 2008, Hamburg, Germany existing word-sense disambiguation systems into a phrase-based SMT system. Both systems use local collocations and word and POS from the immediate context. In [3], bag-of-word context and basic dependency features are used. [5] trained Support Vector Machine classifiers for every possible candidate translation of a source phrase. They considered words of the immediate context (5 tokens to the left and to the right), n-grams (up to size 3) of words, POS and lemmas, as well as chunking labels. [6] trained a global memory-based classifier that performs implicit smoothing of the probability estimates. The classifier makes use of words and POS information of the immediate context of the phrase (2 tokens to the left and to the right). The experimental conditions and the gains reported in the aforementioned studies differ significantly. However, it is interesting to note that all but one attempts are considering English as the target language. Given the fact that this type of integration only requires linguistic analysis for the source language, we may interpret this as a particular difficu"
2008.eamt-1.17,2005.mtsummit-papers.11,0,0.0108207,"n the left one. Dependency-based features were found less informative, which can be explained by the fact that often, the immediate context already captures the discriminative information. In an attempt to boost the selection of the most probable target phrase according to context disambiguation, we added to the log-linear combination a binary feature proposed by [6] which equals 1 for the target phrase that obtains the highest probability P (e|f, C(f )), and 0 otherwise. 3 Experiments and evaluation 3.1 Baseline and context-aware systems We used the French-English Europarl bitext compiled by [9]. To keep the data to a manageable size, we considered a subset of 95,734 sentences that the Stanford parser [10] could parse,9 for training our global classifier. Following the standard practice, we performed phrase extraction for at most 7 word-long phrases using Giza++ and the grow-diag-final-and heuristics [1]. We obtained approximately 11,5M phrases; 3,7M of which are potentially useful for translating the dev and test corpora gathering 475 and 472 bisentences respectively. We built a baseline system from the set of contextless extracted biphrases. We investigated two context-aware system"
2008.eamt-1.17,de-marneffe-etal-2006-generating,0,0.00595076,"ften, the immediate context already captures the discriminative information. In an attempt to boost the selection of the most probable target phrase according to context disambiguation, we added to the log-linear combination a binary feature proposed by [6] which equals 1 for the target phrase that obtains the highest probability P (e|f, C(f )), and 0 otherwise. 3 Experiments and evaluation 3.1 Baseline and context-aware systems We used the French-English Europarl bitext compiled by [9]. To keep the data to a manageable size, we considered a subset of 95,734 sentences that the Stanford parser [10] could parse,9 for training our global classifier. Following the standard practice, we performed phrase extraction for at most 7 word-long phrases using Giza++ and the grow-diag-final-and heuristics [1]. We obtained approximately 11,5M phrases; 3,7M of which are potentially useful for translating the dev and test corpora gathering 475 and 472 bisentences respectively. We built a baseline system from the set of contextless extracted biphrases. We investigated two context-aware systems as well. System S1 uses the features from the immediate context only and is a replication of the system describ"
2008.eamt-1.17,P03-1021,0,0.00497162,"We investigated two context-aware systems as well. System S1 uses the features from the immediate context only and is a replication of the system described in [6]. System S2 uses the same features plus the 16 most informative dependency features found empirically. Following a practical note in [7], we filtered out from the phrase tables of S1 and S2 entries for which p(e|f ) &lt; 0.0002. This reduces experimentation time dramatically without impacting translation results very much. No such filtering was done for the baseline system. Models weights were optimized using Minimum Error Rate Training [11], and decoding was performed using the Moses10 open source PBSMT decoder. All of our translation engines share the same target language model: a Kneser-Ney smoothed trigram model we trained on the French part of the whole Europarl corpus thanks to the SRILM toolkit [12]. For running S1 and S2 systems, we proceeded sentence by sentence. We first classified each sequence of words of a given sentence offline, thus producing a context-aware phrase-table. This table was merged to the main one. Since Moses is not designed to handle context-dependent phrases, we had to serialize each token in the sou"
2008.eamt-1.17,2007.mtsummit-papers.11,0,\N,Missing
2008.eamt-1.17,D08-1076,0,\N,Missing
2008.jeptalnrecital-long.23,P05-1074,0,0.220993,"Missing"
2008.jeptalnrecital-long.23,N03-1003,0,0.246187,"Missing"
2008.jeptalnrecital-long.23,P06-1036,0,0.0261933,"Missing"
2008.jeptalnrecital-long.23,N06-1058,0,0.0645304,"Missing"
2008.jeptalnrecital-long.23,N03-1017,0,0.0121129,"Missing"
2008.jeptalnrecital-long.23,P98-1116,0,0.229315,"Missing"
2008.jeptalnrecital-long.23,W04-1013,0,0.00886436,"Missing"
2008.jeptalnrecital-long.23,N03-1024,0,0.360847,"Missing"
2008.jeptalnrecital-long.23,P02-1040,0,0.0768942,"Missing"
2008.jeptalnrecital-long.23,W04-3219,0,0.0861136,"Missing"
2008.jeptalnrecital-long.23,2007.tmi-papers.28,0,0.0285417,"Missing"
2008.jeptalnrecital-long.23,N06-1057,0,0.0245232,"Missing"
2009.jeptalnrecital-court.28,P05-1048,0,0.295677,"Missing"
2009.jeptalnrecital-court.28,W06-2606,0,0.0423906,"Missing"
2009.jeptalnrecital-court.28,2008.amta-srw.3,0,0.0259168,"Missing"
2009.jeptalnrecital-court.28,2005.mtsummit-papers.11,0,0.0623989,"Missing"
2009.jeptalnrecital-court.28,N03-1017,0,0.0219395,"Missing"
2009.jeptalnrecital-court.28,W09-0407,0,0.055301,"Missing"
2009.jeptalnrecital-court.28,J06-4004,1,0.880029,"Missing"
2009.jeptalnrecital-court.28,2009.jeptalnrecital-long.16,1,0.785303,"Missing"
2009.jeptalnrecital-court.28,P04-1063,0,0.0604008,"Missing"
2009.jeptalnrecital-court.28,2001.mtsummit-papers.46,0,0.14582,"Missing"
2009.jeptalnrecital-court.28,P02-1040,0,0.076177,"Missing"
2009.jeptalnrecital-court.28,N07-1029,0,0.0454675,"Missing"
2009.jeptalnrecital-court.28,2008.amta-srw.6,0,0.022181,"Missing"
2009.jeptalnrecital-court.28,N04-1023,0,0.0770885,"Missing"
2009.jeptalnrecital-court.28,P07-1108,0,0.0239088,"Missing"
2009.jeptalnrecital-court.28,W08-0309,0,\N,Missing
2009.jeptalnrecital-demonstration.2,J08-4005,0,0.0548956,"Missing"
2009.jeptalnrecital-demonstration.2,2008.jeptalnrecital-long.23,1,0.755463,"Missing"
2009.jeptalnrecital-demonstration.2,N03-1024,0,0.0677999,"Missing"
2009.jeptalnrecital-long.16,2007.mtsummit-tutorials.1,0,0.260788,"ystème standard de traduction statistique basé sur les segments, le score attribué aux différentes traductions d’un segment ne dépend pas du contexte dans lequel il apparaît. Plusieurs travaux récents tendent à montrer l’intérêt de prendre en compte le contexte source lors de la traduction, mais ces études portent sur des systèmes traduisant vers l’anglais, une langue faiblement fléchie. Dans cet article, nous décrivons nos expériences sur la prise en compte du contexte source dans un système statistique traduisant de l’anglais vers le français, basé sur l’approche proposée par Stroppa et al. (2007). Nous étudions l’impact de différents types d’indices capturant l’information contextuelle, dont des dépendances syntaxiques typées. Si les mesures automatiques d’évaluation de la qualité d’une traduction ne révèlent pas de gains significatifs de notre système par rapport à un système à l’état de l’art ne faisant pas usage du contexte, une évaluation manuelle conduite sur 100 phrases choisies aléatoirement est en faveur de notre système. Cette évaluation fait également ressortir que la prise en compte de certaines dépendances syntaxiques est bénéfique à notre système. Abstract. In standard ph"
2009.jeptalnrecital-long.16,de-marneffe-etal-2006-generating,0,0.0327867,"Missing"
2009.jeptalnrecital-long.16,W07-0719,0,0.0348446,"Missing"
2009.jeptalnrecital-long.16,W08-0302,0,0.0273901,"Missing"
2009.jeptalnrecital-long.16,2005.mtsummit-papers.11,0,0.0727143,"Missing"
2009.jeptalnrecital-long.16,N03-1017,0,0.0224914,"Missing"
2009.jeptalnrecital-long.16,2006.jeptalnrecital-long.19,1,0.712081,"Missing"
2009.jeptalnrecital-long.16,2008.jeptalnrecital-long.22,0,0.0905881,"Missing"
2009.jeptalnrecital-long.16,P03-1021,0,0.0092088,"Missing"
2009.jeptalnrecital-long.16,2007.tmi-papers.28,0,0.0472749,"Missing"
2009.jeptalnrecital-long.16,vilar-etal-2006-error,0,0.0501421,"Missing"
2010.iwslt-papers.12,E06-1005,1,0.922753,"arget language pair. The disadvantage of this approach is that both the translation into the pivot language, and the translation into the target language are error-prone – and typically, these errors add up. As a result, on comparable training resources, we can expect the translation quality of a pivot system to be significantly lower that the quality of a “direct” system1 . Since Kay [2] has first predicted the usefulness of multilingual resources, several approaches have been proposed to utilize resources and data available in more than two languages for MT. Multi-source machine translation [3, 4, 5] denotes techniques to translate documents which are available in two or more source languages. One approach that has recently been shown to be very effective [4, 6] is to use individual bilingual MT systems to translate the source documents independently of each other into one document each in the target language, and then to use MT system combination (ibid.) to generate a consensus translation out of these different target translations. In this paper, we will investigate to what extent 1 Within this paper, we use the term “direct system” to denote a (statistical) MT system that has been trai"
2010.iwslt-papers.12,E09-1082,0,0.0772212,"arget language pair. The disadvantage of this approach is that both the translation into the pivot language, and the translation into the target language are error-prone – and typically, these errors add up. As a result, on comparable training resources, we can expect the translation quality of a pivot system to be significantly lower that the quality of a “direct” system1 . Since Kay [2] has first predicted the usefulness of multilingual resources, several approaches have been proposed to utilize resources and data available in more than two languages for MT. Multi-source machine translation [3, 4, 5] denotes techniques to translate documents which are available in two or more source languages. One approach that has recently been shown to be very effective [4, 6] is to use individual bilingual MT systems to translate the source documents independently of each other into one document each in the target language, and then to use MT system combination (ibid.) to generate a consensus translation out of these different target translations. In this paper, we will investigate to what extent 1 Within this paper, we use the term “direct system” to denote a (statistical) MT system that has been trai"
2010.iwslt-papers.12,W09-0407,1,0.812949,"e – and typically, these errors add up. As a result, on comparable training resources, we can expect the translation quality of a pivot system to be significantly lower that the quality of a “direct” system1 . Since Kay [2] has first predicted the usefulness of multilingual resources, several approaches have been proposed to utilize resources and data available in more than two languages for MT. Multi-source machine translation [3, 4, 5] denotes techniques to translate documents which are available in two or more source languages. One approach that has recently been shown to be very effective [4, 6] is to use individual bilingual MT systems to translate the source documents independently of each other into one document each in the target language, and then to use MT system combination (ibid.) to generate a consensus translation out of these different target translations. In this paper, we will investigate to what extent 1 Within this paper, we use the term “direct system” to denote a (statistical) MT system that has been trained on a bilingual corpus between source and target language, and does not utilize any pivot or bridge languages. 299 Proceedings of the 7th International Workshop o"
2010.iwslt-papers.12,D07-1005,0,0.143443,"Missing"
2010.iwslt-papers.12,N06-1003,0,0.0986054,"Missing"
2010.iwslt-papers.12,N07-1061,0,0.498978,"ta are available. Unfortunately, if one wants to be able to translate from many possible source languages into many possible target languages, separate MT systems for each possible pair of source and target language have to be trained, on bilingual data in this specific language pair. Quite often this is not possible, especially where rare or unrelated languages are involved. Significant amounts of bilingual in-domain training data may be unavailable; the number of systems to train and to tune may be too high. One approach to overcome this problem has been proposed e.g. by Utiyama and Isahara [1]: A third, more frequent language is utilized as a pivot or bridge language. Ideally, sufficient bilingual language resources are available for both the pair of source and pivot language, and for the pair of pivot and target language. The final translation is then obtained by going via the bridge language, either by generating full translations of the source sentence in this bridge language, or by using the bilingual data to build translation models for the source–target language pair. The disadvantage of this approach is that both the translation into the pivot language, and the translation i"
2010.iwslt-papers.12,2001.mtsummit-papers.46,1,0.565201,"arget language pair. The disadvantage of this approach is that both the translation into the pivot language, and the translation into the target language are error-prone – and typically, these errors add up. As a result, on comparable training resources, we can expect the translation quality of a pivot system to be significantly lower that the quality of a “direct” system1 . Since Kay [2] has first predicted the usefulness of multilingual resources, several approaches have been proposed to utilize resources and data available in more than two languages for MT. Multi-source machine translation [3, 4, 5] denotes techniques to translate documents which are available in two or more source languages. One approach that has recently been shown to be very effective [4, 6] is to use individual bilingual MT systems to translate the source documents independently of each other into one document each in the target language, and then to use MT system combination (ibid.) to generate a consensus translation out of these different target translations. In this paper, we will investigate to what extent 1 Within this paper, we use the term “direct system” to denote a (statistical) MT system that has been trai"
2010.iwslt-papers.12,W09-2503,1,0.895966,"Missing"
2010.iwslt-papers.12,D10-1064,1,0.878502,"Missing"
2010.iwslt-papers.12,P07-1092,0,0.267577,"Missing"
2010.iwslt-papers.12,J06-4004,1,0.89411,"1000 sentences from the common subset (same sentences for all languages). 301 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 MT Sys 1' Src ... MT Sys m' Piv 1 ... MT Sys 1'' Piv m MT Sys m'' ... Direct MT Sys Hyp 1 ... GIZA++alignment Hyp m Reordering Network generation, weighting, rescoring Consensus Translation Hyp m+1 Figure 1: Structure of the multipivot system 5.2. Translation engines The translation engine for these experiments implements the n-gram-based approach to statistical machine translation detailed by Marino et al. [21]. The overall translation accuracy is comparable to state-of-the-art phrase-based translation engines such as the MOSES system [22]. In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram language model of (source,target) pairs [23]. Training such a model requires to reorder source sentences so as to match the target word order. Reordering hypotheses are computed before decoding takes place via a stochastic finite-state automaton that builds a lattice with the most promising hypotheses according to a set of rewrite rules previously co"
2010.iwslt-papers.12,P09-1018,0,0.147907,"Missing"
2010.iwslt-papers.12,P07-2045,0,0.00550476,"en Language Translation Paris, December 2nd and 3rd, 2010 MT Sys 1' Src ... MT Sys m' Piv 1 ... MT Sys 1'' Piv m MT Sys m'' ... Direct MT Sys Hyp 1 ... GIZA++alignment Hyp m Reordering Network generation, weighting, rescoring Consensus Translation Hyp m+1 Figure 1: Structure of the multipivot system 5.2. Translation engines The translation engine for these experiments implements the n-gram-based approach to statistical machine translation detailed by Marino et al. [21]. The overall translation accuracy is comparable to state-of-the-art phrase-based translation engines such as the MOSES system [22]. In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram language model of (source,target) pairs [23]. Training such a model requires to reorder source sentences so as to match the target word order. Reordering hypotheses are computed before decoding takes place via a stochastic finite-state automaton that builds a lattice with the most promising hypotheses according to a set of rewrite rules previously collected from the training bi-texts using the word alignments. In addition to the bilingual n-gram model, our SMT system implements"
2010.iwslt-papers.12,2008.amta-srw.6,0,0.0340832,"Missing"
2010.iwslt-papers.12,eisele-2006-parallel,0,0.0566411,"Missing"
2010.iwslt-papers.12,J03-1002,1,0.0201737,"puts from different systems was first shown to produce superior results in automatic speech recognition (ASR). Voting schemes like the ROVER approach of Fiscus [16] create confusion networks (CNs) from the output of different ASR systems for the same audio input. The consensus recognition hypothesis is generated by weighted majority voting. This approach has later been adapted to MT as well [17]. In this paper, we follow the approach of Matusov et al [4, 18]: An unsupervised monolingual word alignment is trained between all pairs of hypotheses for each source sentence using the GIZA++ toolkit [19]. These alignments are then used to reorder all individual hypotheses to one selected (“primary”) hypothesis, which defines the word order in the consensus translation. A CN is then generated from these reordered hypotheses. As there is no obvious way to determine the best primary hypothesis, separate CNs are generated for all possible primary hypotheses, which are then combined 300 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 Table 1: Corpus statistics for the experimental setup. to a single word lattice. This lattice is then r"
2010.iwslt-papers.12,2005.mtsummit-papers.11,0,0.0506804,"k Words 13.4k 13.5k 14.0k 14.6k 10.1k 16.1k 14.1k 14.3k 14.2k 14.5k 12.7k Dev Test Voc. OOV Words Voc. OOV 3.2k 104 25.9k 5.1k 226 3.5k 120 26.0k 5.5k 245 2.8k 39 27.2k 4.0k 63 3.3k 56 28.6k 5.0k 88 4.3k 244 19.6k 7.1k 407 3.2k 47 31.5k 4.8k 87 3.9k 72 27.2k 6.2k 159 3.4k 61 28.1k 5.1k 99 3.1k 76 27.5k 4.8k 162 3.4k 49 28.3k 5.2k 118 3.3k 116 24.5k 5.2k 226 5. Experimental setup 5.1. Training and Development data For experimenting with our approach, we built translation systems to serve as direct or pivot systems using a phrase-based MT engine for several language pairs of the Europarl corpus [20], which is available in 11 languages: Danish (da), German (de), English (en) , Spanish (es), Finnish (fi), French (fr), Greek (el), Italian (it), Dutch (nl), Portuguese (pt) and Swedish (sv). We also decided to study three source–target language pairs, two for which translation accuracy, as measured by automatic metrics, is moderate, (de–en) and (fr–de), and one for which translation accuracy, is much higher. (fr–en). This allowed us to check whether the improvements provided by our method carry over even in situations where the baseline is high; conversely, it also allows us to assess whether"
2010.iwslt-papers.12,J04-2004,0,0.0336595,"gnment Hyp m Reordering Network generation, weighting, rescoring Consensus Translation Hyp m+1 Figure 1: Structure of the multipivot system 5.2. Translation engines The translation engine for these experiments implements the n-gram-based approach to statistical machine translation detailed by Marino et al. [21]. The overall translation accuracy is comparable to state-of-the-art phrase-based translation engines such as the MOSES system [22]. In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram language model of (source,target) pairs [23]. Training such a model requires to reorder source sentences so as to match the target word order. Reordering hypotheses are computed before decoding takes place via a stochastic finite-state automaton that builds a lattice with the most promising hypotheses according to a set of rewrite rules previously collected from the training bi-texts using the word alignments. In addition to the bilingual n-gram model, our SMT system implements eight additional models which are linearly combined following a discriminative modeling framework [24]: two lexicalized reordering models [25], which attempt to"
2010.iwslt-papers.12,P02-1038,1,0.508083,"ned using a n-gram language model of (source,target) pairs [23]. Training such a model requires to reorder source sentences so as to match the target word order. Reordering hypotheses are computed before decoding takes place via a stochastic finite-state automaton that builds a lattice with the most promising hypotheses according to a set of rewrite rules previously collected from the training bi-texts using the word alignments. In addition to the bilingual n-gram model, our SMT system implements eight additional models which are linearly combined following a discriminative modeling framework [24]: two lexicalized reordering models [25], which attempt to model the orientation of the current translation unit according to the previous as well as the ordering of the next unit with respect to the current unit, a target-language model which provides information about the target language structure and fluency; two lexicon models, which constitute complementary translation models computed for each given tuple; a ‘weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations."
2010.iwslt-papers.12,N04-4026,0,0.0237976,"urce,target) pairs [23]. Training such a model requires to reorder source sentences so as to match the target word order. Reordering hypotheses are computed before decoding takes place via a stochastic finite-state automaton that builds a lattice with the most promising hypotheses according to a set of rewrite rules previously collected from the training bi-texts using the word alignments. In addition to the bilingual n-gram model, our SMT system implements eight additional models which are linearly combined following a discriminative modeling framework [24]: two lexicalized reordering models [25], which attempt to model the orientation of the current translation unit according to the previous as well as the ordering of the next unit with respect to the current unit, a target-language model which provides information about the target language structure and fluency; two lexicon models, which constitute complementary translation models computed for each given tuple; a ‘weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations. For this study, we used 3-gram bilingual"
2010.iwslt-papers.12,P96-1041,0,0.0557653,"ccording to the previous as well as the ordering of the next unit with respect to the current unit, a target-language model which provides information about the target language structure and fluency; two lexicon models, which constitute complementary translation models computed for each given tuple; a ‘weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations. For this study, we used 3-gram bilingual tuple and 3-gram target language models built using Kneser-Ney smoothing [26]; training was performed with the SRI language modeling toolkit [27]. After preprocessing the corpora with standard tokenization tools, word-to-word GIZA++ [19] alignments are performed in both directions, followed by the growdiag-final-and heuristic [28]. 5.3. Experiments The two principal research questions we wanted to answer with the experiments for this paper were: Can we use the multi-pivot approach instead of a direct source– target system, with comparable translation scores? And secondly, can we use the multi-pivot approach to improve the output of an existing direct system? The former"
2010.iwslt-papers.12,2005.iwslt-1.8,0,0.0213956,"slation models computed for each given tuple; a ‘weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations. For this study, we used 3-gram bilingual tuple and 3-gram target language models built using Kneser-Ney smoothing [26]; training was performed with the SRI language modeling toolkit [27]. After preprocessing the corpora with standard tokenization tools, word-to-word GIZA++ [19] alignments are performed in both directions, followed by the growdiag-final-and heuristic [28]. 5.3. Experiments The two principal research questions we wanted to answer with the experiments for this paper were: Can we use the multi-pivot approach instead of a direct source– target system, with comparable translation scores? And secondly, can we use the multi-pivot approach to improve the output of an existing direct system? The former question is most relevant for the “matrix” scenario sketched in Section 3, where we have a large number of possible source and target languages, and do not want to build separate translation systems for each individual pair – either because we want to sa"
2010.iwslt-papers.12,2009.mtsummit-papers.7,0,\N,Missing
2010.jeptalnrecital-court.18,P05-1074,0,0.148687,"Missing"
2010.jeptalnrecital-court.18,N03-1003,0,0.0954374,"Missing"
2010.jeptalnrecital-court.18,2010.jeptalnrecital-recital.5,1,0.496316,"Missing"
2010.jeptalnrecital-court.18,D08-1021,0,0.100887,"Missing"
2010.jeptalnrecital-court.18,C08-1013,0,0.149237,"Missing"
2010.jeptalnrecital-court.18,P99-1044,0,0.228551,"Missing"
2010.jeptalnrecital-court.18,N03-1016,0,0.0475047,"Missing"
2010.jeptalnrecital-court.18,2008.jeptalnrecital-long.23,1,0.853768,"Missing"
2010.jeptalnrecital-court.18,J03-1002,0,0.0123418,"Missing"
2010.jeptalnrecital-court.18,N03-1024,0,0.563199,"Missing"
2010.jeptalnrecital-long.13,D09-1129,0,0.0627687,"Missing"
2010.jeptalnrecital-long.13,max-wisniewski-2010-mining,1,0.783004,"Missing"
2011.jeptalnrecital-long.33,2010.jeptalnrecital-recital.5,1,0.860909,"Missing"
2011.jeptalnrecital-long.33,A94-1019,0,0.0542167,"Missing"
2011.jeptalnrecital-long.33,W09-3102,0,0.0650934,"Missing"
2011.jeptalnrecital-long.33,I05-5002,0,0.087889,"Missing"
2011.jeptalnrecital-long.33,P08-4006,0,0.0470072,"Missing"
2011.jeptalnrecital-long.33,J10-3003,0,0.0590089,"Missing"
2011.jeptalnrecital-long.33,2008.jeptalnrecital-long.23,1,0.825827,"Missing"
2011.jeptalnrecital-long.33,D10-1064,1,0.89007,"Missing"
2011.jeptalnrecital-long.33,max-wisniewski-2010-mining,1,0.850881,"Missing"
2011.jeptalnrecital-long.33,2010.jeptalnrecital-long.13,1,0.853233,"Missing"
2011.jeptalnrecital-long.33,N10-1056,0,0.0334747,"Missing"
2011.jeptalnrecital-long.33,zesch-etal-2008-extracting,0,0.0211202,"Missing"
2011.jeptalnrecital-long.38,P05-1074,0,0.0841195,"Missing"
2011.jeptalnrecital-long.38,N03-1003,0,0.0657265,"Missing"
2011.jeptalnrecital-long.38,P01-1008,0,0.146473,"Missing"
2011.jeptalnrecital-long.38,2010.jeptalnrecital-recital.5,1,0.812219,"Missing"
2011.jeptalnrecital-long.38,2010.jeptalnrecital-court.18,1,0.783574,"Missing"
2011.jeptalnrecital-long.38,D08-1021,0,0.0432575,"Missing"
2011.jeptalnrecital-long.38,C08-1013,0,0.0271679,"Missing"
2011.jeptalnrecital-long.38,C10-1027,1,0.871956,"Missing"
2011.jeptalnrecital-long.38,W09-3102,0,0.0252497,"Missing"
2011.jeptalnrecital-long.38,I05-5002,0,0.054168,"Missing"
2011.jeptalnrecital-long.38,P08-4006,0,0.0400888,"Missing"
2011.jeptalnrecital-long.38,W03-1608,0,0.0648712,"Missing"
2011.jeptalnrecital-long.38,P99-1044,0,0.11808,"Missing"
2011.jeptalnrecital-long.38,N06-1058,0,0.0538278,"Missing"
2011.jeptalnrecital-long.38,P07-2045,0,0.00674383,"Missing"
2011.jeptalnrecital-long.38,P98-1116,0,0.0309369,"Missing"
2011.jeptalnrecital-long.38,2008.jeptalnrecital-long.23,1,0.87252,"Missing"
2011.jeptalnrecital-long.38,W09-2503,1,0.885153,"Missing"
2011.jeptalnrecital-long.38,J03-1002,0,0.00695544,"Missing"
2011.jeptalnrecital-long.38,N03-1024,0,0.0817453,"Missing"
2011.jeptalnrecital-long.38,W04-3219,0,0.0613814,"Missing"
2011.jeptalnrecital-long.38,W09-0441,0,0.0401231,"Missing"
2011.jeptalnrecital-long.38,2010.amta-papers.18,0,0.0870546,"Missing"
2012.iwslt-papers.20,C08-1064,0,0.0951023,"enario is that there is no guarantee that appropriate data will be available for the input text as regards e.g. genre, phraseology, theme vocabulary, or even effects of original language. Thus, adaptation will be performed with the objective of modeling some a priori conﬁdence into the system’s ability to translate short translation units. Another consequence of our setting is that online adaptation is necessary and is in fact the only solution. We therefore propose an on-the-ﬂy pipeline consisting of the following stages : sampling at the level of translation units is performed (similarly to [8, 9]) for selecting sentences from the training data, and instance weighting is applied for scoring phrase pairs (e.g. [6]). Based on these computations, two additional scores are then produced: the ﬁrst estimates the goodness of each collected source phrase as a translation unit for the language pair at hand; the second estimates how much conﬁdence should be put in the adaptated translation distribution for each source phrase1 . An important result of the paper will be the description of a document-level contrastive evaluation scheme that enables a more interpretable analysis of the differences b"
2012.iwslt-papers.20,D11-1033,0,0.114003,"m: given an in-domain training corpus and out-of-domain corpora, a ﬁxed number of sentences are selected in the out-of-domain corpora on the basis of their similarity to the in-domain cor1 Note that in the present work, the effect of this score will only be to act as a segmentation model, so that some segmentation may be preferred over some other. Future work will include searching for more translation examples for those unreliable phrases, as hinted by [5], and having recourse to automatic paraphrasing (e.g. [10]) of those phrases. pus. These sentences may be denoted as pseudo in-domain data [11], where it is hoped that, given the selected number of sentences to draw, performance will be improved. This approach is in fact ﬂawed in a particular respect, as it does not provide any guarantee that instances of rare units will be selected, speciﬁcally if they do not occur in sentences resembling the in-domain data. This has been sometimes solved by ad-hoc strategies to recover infrequent units [12]. We would like instead to make use of all available training corpora. Sampling at the level of phrases is an efﬁcient solution to achieve this goal [8, 9]. Indeed, sufﬁx arrays [13] offer fast a"
2012.iwslt-papers.20,E12-1016,0,0.0908219,"examples for those unreliable phrases, as hinted by [5], and having recourse to automatic paraphrasing (e.g. [10]) of those phrases. pus. These sentences may be denoted as pseudo in-domain data [11], where it is hoped that, given the selected number of sentences to draw, performance will be improved. This approach is in fact ﬂawed in a particular respect, as it does not provide any guarantee that instances of rare units will be selected, speciﬁcally if they do not occur in sentences resembling the in-domain data. This has been sometimes solved by ad-hoc strategies to recover infrequent units [12]. We would like instead to make use of all available training corpora. Sampling at the level of phrases is an efﬁcient solution to achieve this goal [8, 9]. Indeed, sufﬁx arrays [13] offer fast access to phrase instances in large corpora, and can be used to select a given number of instances of phrases, rather than sentences, thereby ensuring that all the phrases present in a corpus are appropriately covered.2 Previous approaches to sampling have resorted to random deterministic sampling, which picks a given number of examples by scanning the sufﬁx array index at ﬁxed intervals (hence the appa"
2012.iwslt-papers.20,N03-1017,0,0.0250979,"able. Although no deﬁnitive criterion as to what constitutes a good phrase translation unit has emerged3 , the two following criteria have been proposed: 2 Callison-Burch et al. [8] found that a sample size of 100 was sufﬁcient for German-to-English phrase-based SMT, while Lopez [9] determined that 300 was an appropriate value for Chinese-English hierarchical SMT. We will use a larger sample size of 1,000 in our experiments in an attempt to let instance weighting ﬁnd the most appropriate examples from a larger sample. 3 For instance, limiting phrases to constituents was found to be suboptimal [14]. The very deﬁnition of what a phrase is with respect to the SMT problem poses many interesting research questions, see e.g. [15]. 293 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 • Given some word alignment between a source and target parallel corpora, the absence of an aligned target phrase for a given source phrase may suggest that the corresponding failure of the extraction process should be accounted for in the translation model. Lopez [9] therefore proposes the following coherent estimate of the translation conditional probability: pcohe"
2012.iwslt-papers.20,2011.iwslt-papers.10,1,0.817563,"eria have been proposed: 2 Callison-Burch et al. [8] found that a sample size of 100 was sufﬁcient for German-to-English phrase-based SMT, while Lopez [9] determined that 300 was an appropriate value for Chinese-English hierarchical SMT. We will use a larger sample size of 1,000 in our experiments in an attempt to let instance weighting ﬁnd the most appropriate examples from a larger sample. 3 For instance, limiting phrases to constituents was found to be suboptimal [14]. The very deﬁnition of what a phrase is with respect to the SMT problem poses many interesting research questions, see e.g. [15]. 293 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 • Given some word alignment between a source and target parallel corpora, the absence of an aligned target phrase for a given source phrase may suggest that the corresponding failure of the extraction process should be accounted for in the translation model. Lopez [9] therefore proposes the following coherent estimate of the translation conditional probability: pcoherent (e|f ) = c(f, e) c(f ) (2) where c(f ), the number of occurrences of the source phrase, corresponds to the total number of a"
2012.iwslt-papers.20,P10-1049,0,0.0544116,"= c(f, e) c(f ) (2) where c(f ), the number of occurrences of the source phrase, corresponds to the total number of attempted extractions, in lieu of the traditional summation over  all extracted translations for f , e c(e , f ). • It has been observed that the traditional heuristic approach to phrase pair extraction does not offer a consistent view over the training and the actual use of phrases by decoders. It is thus possible to have recourse to a forced alignment which results in the decoder producing what it believes is the best alignment for a given training sentence. Wuebker et al. [16] implement this idea using leaving-one-out, so that the phrase examples for each training bi-sentence are not used to decode it, and subsequently estimate their system’s models on the resulting alignment. Even though this intuition does not guarantee that the retained phrases are intrinsically good translation units, they were selected as pertaining to best derivations allowing to reproduce the reference target sentence. We exploit the two above ideas as follows. First, we use some pre-trained standard phrase-based system to translate its own training corpus. Instead of sticking strictly to le"
2012.iwslt-papers.20,W07-0717,0,0.130643,"of any kind of ”in-domain” data, hence the name ”any-text translation”. In this context, we present a new approach to contextually adapt a translation model onthe-ﬂy, and present several experimental results where this approach outperforms conventionaly trained baselines. We also present a document-level contrastive evaluation whose results can be easily interpreted, even by non-specialists. 1. Introduction It is now a well-established fact in Statistical Machine Translation that systems must be adapted to each particular input text. Adaptation has been tackled in a variety of ways (see e.g. [1, 2, 3]), most notably by adapting the translation model, by adapting the target language model, and by adaptating the tuning set. In most of these works, it is assumed that the bilingual training corpus can be partitioned into “indomain” and “out-of-domain” subsets relative to the input text, and that there exists some smaller “in-domain” held-out corpus to tune the system. In typical settings, large bilingual corpora are collected opportunistically; as a result, the amount of data that do not resemble closely the input text largely outweights the data that appear to be the most relevant. Using as m"
2012.iwslt-papers.20,2007.mtsummit-papers.11,0,0.0605489,"difference of entropy values between the previous situation and the more informative situation of a given model provides some account of how much conﬁdence should be put in the collective contribution of all weighted examples. We thus used the following as a new feature in our experiments involving adapted translation models: Hunif (f ) = − hconf idence (f ) = = p(e|f ) log(p(e|f )) = log( Hunif (f ) − H(f ) (5)  1 )+ − log( piw (e|f ) log(piw (e|f )) c(f ) e 5 Inverse translation models and lexical weighting are in a way meant to compensate for this. 6 Context-dependent phrase tables (e.g. [17]) is a way to address this. 294 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Corpus newsco (in) ted (out) test newsco tuning #lines 934 934 1,859 #tok.en 22.4K 19.6K 44.2K #tok.fr 25.3K 20.3K 48.8K ppl.en 316.19 265.63 307.14 ppl.fr 211.07 164.57 222.79 oov.en 629 238 1,700 oov.fr 273 273 1,558 Table 1: Tuning and test documents statistics This value increases when either the number of examples for f is high or when the entropy of the adapted translation distribution is low. 5. Experiments We now describe experiments intended to show whether o"
2012.iwslt-papers.20,E12-1055,0,0.0227216,"ough better coverage: in particular, it seems to improve the alignment of some rare translation units, which would otherwise be misaligned, and yield inappropriate phrase pairs. On the other hand, adding more bilingual data increases the possibility of encountering new translations, and makes the translation of phrases more ambiguous, sometimes in a detrimental way, since not all corresponding translations (or senses) are appropriate for the input text. The data sparseness and the ambiguity problem thus entertain a repulsion relationship that is at the core of the adaptation problem (see e.g. [4]), even though the recent work of Haddow and Koehn [5] concludes that good coverage is more important than appropriate scoring: adding out-of-domain corpora containing examples of rare units beneﬁts more to translation than the inclusion of inappropriate examples of frequent units harms it. A practical solution is to use all the available training data, but to consider differently translation examples depending on their relevance to the input text, possibly at the corpus [1], sentence [6] or phrase [3] level. As noted e.g. by Haddow and Koehn [5], although the in-domain vs. out-ofdomain distin"
2012.iwslt-papers.20,D09-1074,0,0.0394755,"ty problem thus entertain a repulsion relationship that is at the core of the adaptation problem (see e.g. [4]), even though the recent work of Haddow and Koehn [5] concludes that good coverage is more important than appropriate scoring: adding out-of-domain corpora containing examples of rare units beneﬁts more to translation than the inclusion of inappropriate examples of frequent units harms it. A practical solution is to use all the available training data, but to consider differently translation examples depending on their relevance to the input text, possibly at the corpus [1], sentence [6] or phrase [3] level. As noted e.g. by Haddow and Koehn [5], although the in-domain vs. out-ofdomain distinction is frequently used, precise deﬁnitions are still lacking; in their words, “it is normally understood that data from the same domain is in some sense similar (for example in the words and grammatical constructions used)” and, in their experiments, they characterize domain differences in terms of word distributions and out-of-vocabulary rates. While some domain distinctions are clearly undebatable, such as when opposing e.g. News commentaries and parliamentary speeches, other distinct"
2012.iwslt-papers.20,2010.iwslt-papers.5,0,0.172375,"se of tuning techniques relying on a development bitext from the same data source or domain. • Training data was collected opportunistically and no speciﬁc document metadata (e.g. genre, document boundaries) are available for the full data set. Note that the issues of adapting alignments and target language models will not be considered in this work. As to the former, it has previously been shown that using all the available corpora during word alignment tends to improve 292 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 translation performance [7, 5], so our word alignment models will be built ofﬂine using all available parallel data. As to the latter, there is a large body of works addressing language model adaptation which all report improvements over non-adapted language models (e.g. [1]). We leave it to our future work to evaluate whether the effects of all types of adaptations can be compounded. This paper is to our knowledge the ﬁrst attempt at studying the scenario of what we call here “any-text translation”, with the notable absence of some predeﬁned identiﬁable indomain training and tuning corpora. An important aspect of our scen"
2012.iwslt-papers.20,J82-2005,0,0.72969,"enario is that there is no guarantee that appropriate data will be available for the input text as regards e.g. genre, phraseology, theme vocabulary, or even effects of original language. Thus, adaptation will be performed with the objective of modeling some a priori conﬁdence into the system’s ability to translate short translation units. Another consequence of our setting is that online adaptation is necessary and is in fact the only solution. We therefore propose an on-the-ﬂy pipeline consisting of the following stages : sampling at the level of translation units is performed (similarly to [8, 9]) for selecting sentences from the training data, and instance weighting is applied for scoring phrase pairs (e.g. [6]). Based on these computations, two additional scores are then produced: the ﬁrst estimates the goodness of each collected source phrase as a translation unit for the language pair at hand; the second estimates how much conﬁdence should be put in the adaptated translation distribution for each source phrase1 . An important result of the paper will be the description of a document-level contrastive evaluation scheme that enables a more interpretable analysis of the differences b"
2012.iwslt-papers.20,D12-1037,0,0.0342671,"n and is taken from presentations from TED talks9 (ted). These conditions allow us to compare situations where tuning corpora of various degrees of appropriateness are available and can be identiﬁed as more appropriate; we will also simulate the availability of a “perfect” tuning set by performing self-tuning. Lastly, our training corpus, described in Table 2, contains two sub-corpora of in-domain News commentaries (newsco) and out-of-domain parliamentary debates (epps). These sub-corpora will be either used separately or jointly. 7 Performing tuning set adaptation at the document-level as in [18] will be part of our future work. 8 http://www.statmt.org/wmt12 9 Available from IWSLT’11: http://iwslt2011.org Corpus newsco epps newsco+epps domain w.r.t. test in out mixed # lines 137K 1,982M 2,119M # tokens.en 3,381M 54,170M 57,551M # tokens.fr 4,017M 59,702M 63,790M Table 2: Training corpora statistics 5.2. Systems 5.2.1. Off-line baseline systems We build standard phrase-based systems using moses10 , and use MERT for tuning parameters. We compare the following conditions: training on all available data (newsco+epps), as well as using two separate phrase tables built from newsco and epps"
2012.iwslt-papers.20,2005.eamt-1.19,0,0.031576,"tables built from newsco and epps (i.e. multiple alternative decoding paths) as is standard practice in domain adaptation where corpus boundaries are known [1]. 5.2.2. On-the-ﬂy adapted systems We build various adapted systems on-the-ﬂy. All use the word alignments produced by Giza++11 on the full newsco+epps corpus, as out-of-domain data may improve alignment quality in our situation [7]. We test the three following sampling and instance-weighting strategies for estimating translation model: (a) random sampling and uniform weighting [8, 9] (RND), (b) using tf.idf values of training sentences [19] (IR), and (c) perplexity values of training sentences relative to each test document (PPL).12 An important difference with our baseline systems is that we do not estimate a back-translation model (p(f |e)) as this proves costly using sampling; [9] reported that this model does not have a signiﬁcant impact on translation performance for large training corpora. Furthermore, we believe that such a model should in fact not be needed, were the translation model appropriately estimated (i.e. contextually appropriate), as there would be no need to compensate for the “ambiguity” in this model by cons"
2012.iwslt-papers.20,2006.amta-papers.25,0,0.0655461,"exical weighting models, which are meant to model intra-biphrase cohesion. 295 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 perfect tuning set for each input document, each document is tuned independently using the reference corpus and the best optimization point is used for testing; this is obviously an oracle situation, and will be denoted as such in our results for moses and our adapted systems. 5.3. Evaluation setting and contrastive document-level evaluation We will compare our various settings using the wellestablished BLEU [20] and TER [21] metrics, using initially the full test corpus made up of the full collection of documents. Absolute values being always difﬁcult to interpret, we propose to resort to contrastive evaluation between two systems. Our contrastive document-level evaluation is performed as follows: given two systems we wish to compare, a single conﬁguration, and a target evaluation metrics, we look on a per document basis which system outperformed the other for some interval (e.g. “1-2 BLEU increase”, “0.50.75 TER decrease”). We then compute statistics over the entire document set. Considering a particular signiﬁc"
2012.iwslt-papers.20,cartoni-meyer-2012-extracting,0,0.0406631,"s the opposite, and darker colors indicates larger differences. 100 fr2en en2fr percentage of correctly translated source phrases 90 80 70 60 50 40 30 20 10 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 confidence model value intervals 5 5.5 6 6.5 Figure 3: Percentage of correctly translated source phrases in the trace of the decoder for the PPL+all systems against score value intervals of the conﬁdence model (conf). which correspond to the large majority of our training data, previous studies have shown that French as an original language is signiﬁcantly more represented than English as an original language [22]. Experimenting with other corpora in which original language is known may help us to conﬁrm this hypothesis. Our adapted systems have recourse to sampling, and consequently do not use a reverse translation model [9], thus resulting in systems that may be built very efﬁciently, even for large data set conditions. Most previously published domain adaptation techniques cannot be applied directly to our studied scenario, as the availability of an in-domain training corpus is almost always assumed. Note that the newsco part of our training corpus was in fact “in-domain” w.r.t. our test documents."
2012.iwslt-papers.20,D10-1044,0,\N,Missing
2012.iwslt-papers.20,P02-1040,0,\N,Missing
2012.iwslt-papers.20,W07-0733,0,\N,Missing
2012.iwslt-papers.20,P10-2001,0,\N,Missing
2012.iwslt-papers.20,P05-1032,0,\N,Missing
2012.iwslt-papers.20,W12-3154,0,\N,Missing
2013.iwslt-papers.16,D10-1091,0,0.181897,"large enough quantities of adapted data are available, e.g. by using SMT systems in conjunction with translation memories, which has yielded much interest into the study and use of human postediting and tools for supporting this activity. Such performance levels typically correspond to the utilization of the best translation hypothesis produced by a given system, which is a reflection of the system’s relative evaluation of the translations in its search space. Previous oracle studies have shown that the best attainable performance of such systems was in fact much higher than their best output [1]. This is achieved by relaxing pruning and reordering constraints imposed on decoders, and maximizing some evaluation metrics score rather than the system’s own scoring function. Such studies are useful, in particular, to make explicit the potential of a given system configuration (training data, extraction procedures, etc.) and to possibly exhibit the difficult parts of a source text (e.g. [2])) as well as the possible defects of reference translations. A lesson that can be drawn from these results is the poor adequacy of the internal scores of translation quality used by current systems. Ano"
2013.iwslt-papers.16,2007.tmi-papers.13,0,0.935893,"nd consequently not straightforwardly reusable. Some large collections of manually revised translations have been collected [4, 5], which can be used e.g. for sub-sentential confidence estimation. However, such data sets are costly to acquire, in particular for some language pairs, and may again be, on some aspects, too specific to a given version of the MT system used. In this article, we describe an approach to build a related resource, but for a modest cost and with possibly wider applicability. We resort to greedy rewriting of translation hypotheses, in a similar spirit to Langlais et al. [6], to find the sequence of rewriting steps which maximizes the quality of translation hypotheses with respect to some evaluation metrics and reference translation(s). Individual rewritings are based on the repertoire of biphrases units of some phrasebased SMT systems, and thus do not have to correspond to plausible rewritings made by human translators. While we aim to use such a resource to learn to identify improvable fragments (e.g. [4]) and learn discriminative rerankers (e.g. [7]), we will here focus on a systematic study of such an artificial resource. Our experiments will study the follow"
2013.iwslt-papers.16,P06-1096,0,0.0772324,"r applicability. We resort to greedy rewriting of translation hypotheses, in a similar spirit to Langlais et al. [6], to find the sequence of rewriting steps which maximizes the quality of translation hypotheses with respect to some evaluation metrics and reference translation(s). Individual rewritings are based on the repertoire of biphrases units of some phrasebased SMT systems, and thus do not have to correspond to plausible rewritings made by human translators. While we aim to use such a resource to learn to identify improvable fragments (e.g. [4]) and learn discriminative rerankers (e.g. [7]), we will here focus on a systematic study of such an artificial resource. Our experiments will study the following factors: • rewriting operations: we will use a revised and extended set of previously used operations [6], and introduce an original operation which allows source sentence rewriting (rewrite), as well as a target phrase deletion operation (remove); • training data size: we will use 5 different sizes of training data, where training data are split independently from their relation to the test data; • number of available reference translations: we will be able to verify whether ph"
2013.iwslt-papers.16,D07-1103,0,0.0345669,"a target phrase deletion operation (remove); • training data size: we will use 5 different sizes of training data, where training data are split independently from their relation to the test data; • number of available reference translations: we will be able to verify whether phenomena observed when a single reference translation is available can also be observed when as many as 7 reference translations allow for a much more robust evaluation of translation quality; • phrase table filtering: we will use unfiltered phrase tables and phrase tables filtered using a significance testing criterion [8]; • target language: we will use French as the source language, and 10 other European languages as target languages, with exactly the same training data; • beam size: finally, we will also consider various beam sizes to get some account of the quantity of search errors made by our greedy decoder, although this aspect is not central to the present study. The remainder of this article is organized as follows. Section 2 introduces greedy oracle decoding and describes the operations that we have used in this work. Section 3 presents our choice of data, systems, and search settings for this work. O"
2013.iwslt-papers.16,P01-1030,0,0.237257,"ount of the quantity of search errors made by our greedy decoder, although this aspect is not central to the present study. The remainder of this article is organized as follows. Section 2 introduces greedy oracle decoding and describes the operations that we have used in this work. Section 3 presents our choice of data, systems, and search settings for this work. Our experiments are then detailed in Section 4. We finally summarize our main findings and present some of our future work in Section 5. 2. Greedy oracle decoding Greedy decoding for Statistical Machine Translation was introduced in [9], as a fast solution to the NP-complete problem of finding the best translation hypothesis from a translation engine’s search space.1 Although such a technique was shown to produce more search errors than its dynamic programming-based counterpart for max-derivation approximation, Langlais et al. [6] described an implementation of greedy search decoding that could improve the best hypothesis from a then state-of-the-art DP-decoder. Subsequent work using a Gibbs sampler for approximating maximum translation decoding [10] showed, however, the adequacy of the approximations made by recent decoders"
2013.iwslt-papers.16,takezawa-etal-2002-toward,0,0.0210868,"1 languages using English as pivot. From the collected data, we extracted held-out, later entries as tuning and test sets (see Table 1). We used French as our sole source language, and experimented with all other possible target languages. English was used as the main target language of the study, notably in settings where the training data was reduced to smaller fractions. Furthermore, in order to verify how our oracles would behave in situations where the evaluation metrics could make use of several possible reference translations, we also used the BTEC corpus of basic traveling expressions [11], allowing us to use 16 references for tuning our baseline systems and 7 references for evaluating them on the French to English language pair (see Table 1). We built state-of-the-art phrase-based SMT systems using the open source Moses system4 , using standard settings and models and MERT [12] for optimizing the parameters on the tuning set. Trigrams target language models were es3 http://statmt.org/europarl/ 4 http://www.statmt.org/moses previous ... le projet qui ferait gagner le plus de temps sur un ferroviaire15 trajet16 tr`es long17 ... the project which would win the more time on a rail"
2013.iwslt-papers.16,C04-1072,0,0.0980572,"ra. timated from the bilingual training data only, using KneserNey smoothing. Results for all baseline systems and all training conditions are reported in Table 1, using BLEU and TER as complementary indicators of translation performance. We used the greedy search operations described in Section 2. We implemented various approximations to speed up decoding. In particular, we limited candidate replacements for replace, split and merge to phrases that contain at least one token in common with the reference translation, except for the 50 most frequent tokens.5 We used sentencelevel smoothed BLEU [13] as our objective function for greedy decoding (using a single (Europarl) or several reference translations (BTEC)), but will use corpus-level BLEU and individual n-gram precisions, as well as TER, to report translation performance. 4. Experiments and analysis 4.1. Rewriting operations Using our main language pair, French to English, we experimented with each individual rewriting operation, as well as with the full set; see Table 2. The two operations that individually lead to the largest improvements are not surprisingly those that have access to replacement translations from the phrase table"
2013.iwslt-papers.16,E09-1082,0,0.148759,"arget languages (Table 4) show that languages that benefit the most from this increased reachability (more than +1 BLEU and -1 TER) mostly corresponds to languages with lower baseline scores, indicating that alignment difficulty (considering that the exact same training data were used for all language pairs) is responsible to some extent. Table 5: Examples of English source rewritings (note that English was used as source language here for illustration purposes) and their new reachable French reference translation fragment. Positive applications of such an operation, as previously proposed by [16, 17] using source paraphrase lattices, include a large typology of configurations largely not limited to strict paraphrase phenomena, as illustrated on Figure 5. For instance, using English as the source language for illustration purposes, correctly translating the English word buying (in not by buying other countries’ quotas) by rachat (in the expected translation non par le rachat du ”droit a` polluer” d’un autre pays) can only be done by translating the noun purchase instead. Studying source rewriting patterns on part-of-speeches (see Table 5) shows that French, with a rich verbal inflection sy"
2013.iwslt-papers.16,N07-1064,0,0.0753243,"t parts of a source text (e.g. [2])) as well as the possible defects of reference translations. A lesson that can be drawn from these results is the poor adequacy of the internal scores of translation quality used by current systems. Another interesting potential use of oracle studies is that they can produce useful data under the form of individual post-editing steps that may be used to improve existing translation hypotheses. Initial attempts at automatic postediting of SMT output approached the problem as one of second-pass translation between automatic predictions and correct translations [3]. Among the drawbacks of such approaches, large quantities of texts have to be translated to learn post-editing models, which are then furthermore specific to a given version of a given system and consequently not straightforwardly reusable. Some large collections of manually revised translations have been collected [4, 5], which can be used e.g. for sub-sentential confidence estimation. However, such data sets are costly to acquire, in particular for some language pairs, and may again be, on some aspects, too specific to a given version of the MT system used. In this article, we describe an a"
2013.iwslt-papers.16,P11-1022,0,0.157572,"a under the form of individual post-editing steps that may be used to improve existing translation hypotheses. Initial attempts at automatic postediting of SMT output approached the problem as one of second-pass translation between automatic predictions and correct translations [3]. Among the drawbacks of such approaches, large quantities of texts have to be translated to learn post-editing models, which are then furthermore specific to a given version of a given system and consequently not straightforwardly reusable. Some large collections of manually revised translations have been collected [4, 5], which can be used e.g. for sub-sentential confidence estimation. However, such data sets are costly to acquire, in particular for some language pairs, and may again be, on some aspects, too specific to a given version of the MT system used. In this article, we describe an approach to build a related resource, but for a modest cost and with possibly wider applicability. We resort to greedy rewriting of translation hypotheses, in a similar spirit to Langlais et al. [6], to find the sequence of rewriting steps which maximizes the quality of translation hypotheses with respect to some evaluation"
2013.iwslt-papers.16,potet-etal-2012-collection,0,0.0243126,"a under the form of individual post-editing steps that may be used to improve existing translation hypotheses. Initial attempts at automatic postediting of SMT output approached the problem as one of second-pass translation between automatic predictions and correct translations [3]. Among the drawbacks of such approaches, large quantities of texts have to be translated to learn post-editing models, which are then furthermore specific to a given version of a given system and consequently not straightforwardly reusable. Some large collections of manually revised translations have been collected [4, 5], which can be used e.g. for sub-sentential confidence estimation. However, such data sets are costly to acquire, in particular for some language pairs, and may again be, on some aspects, too specific to a given version of the MT system used. In this article, we describe an approach to build a related resource, but for a modest cost and with possibly wider applicability. We resort to greedy rewriting of translation hypotheses, in a similar spirit to Langlais et al. [6], to find the sequence of rewriting steps which maximizes the quality of translation hypotheses with respect to some evaluation"
2013.iwslt-papers.16,D10-1064,1,0.835256,"which allows us to turn around the common acceptance that unique reference translations are poor representations of acceptable translations, and to claim that the specificities of a unique source text sometimes are responsible for (automatic) translation difficulty. Previously, Schroeder et al. [16] had shown the potential of using many human rewritings of input texts, and Khalilov and Sima’an [18] had shown the potential of using reorderings of input texts, but to our knowledge this work is the first to focus on the contribution of local indirect translation.10 Paraphrasing the training data [19, 20] in a carefull manner is one way to provide access to such knowledge during translation. Other salient results of our study include the empirical demonstration that pruned phrase tables significantly limit the potential of SMT systems, and that current SMT systems have the potential to already produce very good translation hypotheses even for difficult language pairs, however difficult this may be to achieve in practice. Part of our intended future work will focus on identifying high-quality greedy sequences of rewriting operations, and to compare them to edit sequences made by human post-edit"
2013.iwslt-papers.16,I11-1090,0,0.0248068,"which allows us to turn around the common acceptance that unique reference translations are poor representations of acceptable translations, and to claim that the specificities of a unique source text sometimes are responsible for (automatic) translation difficulty. Previously, Schroeder et al. [16] had shown the potential of using many human rewritings of input texts, and Khalilov and Sima’an [18] had shown the potential of using reorderings of input texts, but to our knowledge this work is the first to focus on the contribution of local indirect translation.10 Paraphrasing the training data [19, 20] in a carefull manner is one way to provide access to such knowledge during translation. Other salient results of our study include the empirical demonstration that pruned phrase tables significantly limit the potential of SMT systems, and that current SMT systems have the potential to already produce very good translation hypotheses even for difficult language pairs, however difficult this may be to achieve in practice. Part of our intended future work will focus on identifying high-quality greedy sequences of rewriting operations, and to compare them to edit sequences made by human post-edit"
2013.iwslt-papers.7,C88-1016,0,0.425317,"a sentence-aligned parallel corpus, which is a crucial component of state-of-the-art Statistical Machine Translation (SMT) technology. One of the most prominent approaches nowadays is Phrase-based Statistical Machine Translation, which is built upon the word alignment output. The problem of learning sub-sentential alignment from parallel texts is well-known, and numerous proposals have been put forward to perform this task. Those methods roughly fall into two main categories, broadly described here as the probabilistic and the associative approaches. The probabilistic approach, introduced in [1], considers the problems of identifying links between words or groups of words in parallel sentences. This approach consists in defining a probabilistic model (e.g. IBM models [2]) of the parallel corpus, the parameters of which are estimated by a global optimization process which simultaneously considers all possible associations in the entire corpus. Due to its tight integration within the SMT framework, this approach is by far the most widely used. However, it is characterized by a number of shortcomings, in particular: • Its parameters have to be estimated and optimized based on the entire"
2013.iwslt-papers.7,J93-2003,0,0.0606461,"ys is Phrase-based Statistical Machine Translation, which is built upon the word alignment output. The problem of learning sub-sentential alignment from parallel texts is well-known, and numerous proposals have been put forward to perform this task. Those methods roughly fall into two main categories, broadly described here as the probabilistic and the associative approaches. The probabilistic approach, introduced in [1], considers the problems of identifying links between words or groups of words in parallel sentences. This approach consists in defining a probabilistic model (e.g. IBM models [2]) of the parallel corpus, the parameters of which are estimated by a global optimization process which simultaneously considers all possible associations in the entire corpus. Due to its tight integration within the SMT framework, this approach is by far the most widely used. However, it is characterized by a number of shortcomings, in particular: • Its parameters have to be estimated and optimized based on the entire parallel corpus, hence all units in the parallel corpus have to be aligned simultaneously. This makes it a time-consuming process, especially when working on large parallel corpo"
2013.iwslt-papers.7,N10-1062,0,0.0603724,"n particular: • Its parameters have to be estimated and optimized based on the entire parallel corpus, hence all units in the parallel corpus have to be aligned simultaneously. This makes it a time-consuming process, especially when working on large parallel corpora. In addition, many aligned parallel sentence pairs are never used to translate an input text. • New data are constantly made available. It is a waste of resource to run the alignment process repeatedly for the whole corpus when only a proportionally low number of new sentences are added. These shortcomings are addressed notably in [3], which uses the online EM algorithm of [4] to implement online learning for the HMM alignment model. Associative approaches were introduced in [5]. They do not rely on an alignment model, but rather on independence statistical measures such as the Dice coefficient, mutual information [5, 6], or likelihood ratio [7]. In this approach, a local maximization process is used, where each sentence is processed independently. An associative sub-sentential alignment method, named Anymalign, was introduced in [8, 9]. This method relies on simple comparisons on (source and target) word occurrence distri"
2013.iwslt-papers.7,C94-2178,0,0.299307,"gned parallel sentence pairs are never used to translate an input text. • New data are constantly made available. It is a waste of resource to run the alignment process repeatedly for the whole corpus when only a proportionally low number of new sentences are added. These shortcomings are addressed notably in [3], which uses the online EM algorithm of [4] to implement online learning for the HMM alignment model. Associative approaches were introduced in [5]. They do not rely on an alignment model, but rather on independence statistical measures such as the Dice coefficient, mutual information [5, 6], or likelihood ratio [7]. In this approach, a local maximization process is used, where each sentence is processed independently. An associative sub-sentential alignment method, named Anymalign, was introduced in [8, 9]. This method relies on simple comparisons on (source and target) word occurrence distribution over randomly sampled sub-corpora. Words with the same occurrence distribution over a particular sub-corpus are extracted as an association. The more often two words are associated, the better the association score between them, and the more likely they are to be mutual translations."
2013.iwslt-papers.7,J93-1003,0,0.124091,"are never used to translate an input text. • New data are constantly made available. It is a waste of resource to run the alignment process repeatedly for the whole corpus when only a proportionally low number of new sentences are added. These shortcomings are addressed notably in [3], which uses the online EM algorithm of [4] to implement online learning for the HMM alignment model. Associative approaches were introduced in [5]. They do not rely on an alignment model, but rather on independence statistical measures such as the Dice coefficient, mutual information [5, 6], or likelihood ratio [7]. In this approach, a local maximization process is used, where each sentence is processed independently. An associative sub-sentential alignment method, named Anymalign, was introduced in [8, 9]. This method relies on simple comparisons on (source and target) word occurrence distribution over randomly sampled sub-corpora. Words with the same occurrence distribution over a particular sub-corpus are extracted as an association. The more often two words are associated, the better the association score between them, and the more likely they are to be mutual translations. This method was shown to"
2013.iwslt-papers.7,R09-1040,0,0.0179237,"ionally low number of new sentences are added. These shortcomings are addressed notably in [3], which uses the online EM algorithm of [4] to implement online learning for the HMM alignment model. Associative approaches were introduced in [5]. They do not rely on an alignment model, but rather on independence statistical measures such as the Dice coefficient, mutual information [5, 6], or likelihood ratio [7]. In this approach, a local maximization process is used, where each sentence is processed independently. An associative sub-sentential alignment method, named Anymalign, was introduced in [8, 9]. This method relies on simple comparisons on (source and target) word occurrence distribution over randomly sampled sub-corpora. Words with the same occurrence distribution over a particular sub-corpus are extracted as an association. The more often two words are associated, the better the association score between them, and the more likely they are to be mutual translations. This method was shown to produce better results than state-ofthe-art methods on bilingual lexicon constitution tasks, when the evaluation is performed by comparing word associations with reference dictionaries, but faile"
2013.iwslt-papers.7,2012.eamt-1.62,1,0.828471,"ed sub-corpora. Words with the same occurrence distribution over a particular sub-corpus are extracted as an association. The more often two words are associated, the better the association score between them, and the more likely they are to be mutual translations. This method was shown to produce better results than state-ofthe-art methods on bilingual lexicon constitution tasks, when the evaluation is performed by comparing word associations with reference dictionaries, but failed to perform on par with state-of-the-art methods for building SMT phrase tables. It was subsequently improved in [10], in which a recursive binary segmentation algorithm is used to process the output of Anymalign so as to obtain better sub-sentential alignments at the sentence level. While this improvement yields a performance that is comparable with the statistical approach, it can do so by processing large numbers of randomly sampled sub-corpora in order to obtain an accurate association measure and a good coverage for the entire corpus. In this work, we propose a method to adapt Anymalign in order to align the parallel sentences on a per-need basis, meaning that it can also be used to accurately align new"
2013.iwslt-papers.7,J97-3002,0,0.649942,"Missing"
2013.iwslt-papers.7,P07-2045,0,0.0103763,"to build an association table T (the same kind of table as the table in step 5 in Figure 1) Using T as the input of the binary segmentation algorithm (cf. Section 2.2), a new alignment A0 is computed if distance(A − A0 ) &lt;  then return A end if NumIter+=1 end while return A 3. Experiments 3.1. Experimental settings In this section, we describe experiments intended to test the performance of the associative sub-sentential alignment approach described in Section 2. We will focus on measuring the impact of several alignment strategies for a phrase-based SMT system. We will use the Moses toolkit [13], which can be regarded as state-of-the-art for building SMT systems. Moses will be used in all configurations to build phrase tables and reordering tables from alignment matrices, and its decoder will be used to build candidate translations during optimization (using standard MERT [14]) and testing. Translation performance will be measured by classical corpus-based metrics, BLEU [15] and TER [16]. All results are average scores computed on the test set for 3 independent optimization runs on the development set [17]. Experiments will be conducted on three language pairs and two main corpora, a"
2013.iwslt-papers.7,D12-1089,0,0.0162464,"ion runs on the development set [17]. Experiments will be conducted on three language pairs and two main corpora, and we will make use of several reference translations when possible. We will also resort to oracle decoding using a greedy, approximate local search strategy and a number of phrase-based operators [18] to get some account of the best translation score attainable given each specific phrase table. We will furthermore consider the compactness of the produced phrase tables, as it can be regarded as a desirable quality of phrase tables licencing works on phrase table pruning (see e.g. [19]), and anormally large phrase table may in fact only artificially inflate oracle results. Two sets of experiments will be carried out in this work. The first set of experiments is designed to validate the quality of the alignment generated by our method (henceforth sba, for sampling-based alignment) on some predefined bilingual corpus against a state-of-art alignment pipeline, based on giza++ [20], using default parameters from Moses. This approach is refered to as giza++ . The second set of experiments aims to assess the ability to align new bilingual data. For this experiment, we will focus"
2013.iwslt-papers.7,J03-1002,0,0.0164095,"rase table. We will furthermore consider the compactness of the produced phrase tables, as it can be regarded as a desirable quality of phrase tables licencing works on phrase table pruning (see e.g. [19]), and anormally large phrase table may in fact only artificially inflate oracle results. Two sets of experiments will be carried out in this work. The first set of experiments is designed to validate the quality of the alignment generated by our method (henceforth sba, for sampling-based alignment) on some predefined bilingual corpus against a state-of-art alignment pipeline, based on giza++ [20], using default parameters from Moses. This approach is refered to as giza++ . The second set of experiments aims to assess the ability to align new bilingual data. For this experiment, we will focus on adding sentence pairs from a very large (unaligned) bilingual corpus, chosen on the basis that they contain translations for previously out-ofvocabulary tokens. Our approach will be compared against the same alignment pipeline using the augmented parallel corpus. This strategy is however costly as it requires to retrain the complete models, so we also performed a comparison with alignments obta"
2013.iwslt-papers.7,takezawa-etal-2002-toward,0,0.0266256,"very large (unaligned) bilingual corpus, chosen on the basis that they contain translations for previously out-ofvocabulary tokens. Our approach will be compared against the same alignment pipeline using the augmented parallel corpus. This strategy is however costly as it requires to retrain the complete models, so we also performed a comparison with alignments obtained using the orginal alignment models, without any retraining. 3.2. Data sets Experiments were performed on two parallel corpora, described in Table 1: BTEC is a small English-French subpart of the Basic Travel Expression Corpus [21]; and HIT is a corpus of basic expressions built for the Beijing 2008 Olympics, used here in English, French and Chinese. We used the BTEC development set of 2003 (devel03) and BTEC test set of 2009 (test09) as our development and test set, which are described in Table 2. Note that the former has 16 reference translations available for English, and the latter has 7, allowing for a somehow more interpretable measure of performance for language pairs with English as the target language. We will describe in Section 3.4 experiments that make Corpus BTEC HIT EPPS supp WMT # lines 20K 62K 1,982K 3.3"
2013.iwslt-papers.7,W08-0509,0,0.0164413,"parliamentary debates, as well as a substantially larger corpus from the translation task of the Workshop on Statistical Machine Translation (WMT)2 : both are described in Table 1. Our development and test sets will remain the same for all experiments. English and French texts are normalized and tokenized by our in-house tools, and Chinese texts are segmented by a CRF-based Chinese word segmenter3 . 3.3. Basic alignment task This experiment aims to assess the quality of the subsentential alignment generated by our method on a full bilingual parallel corpus. We use the giza++ implementation of [22] as a competitive baseline, with default settings : 5 iterations of IBM1, HMM, IBM3, and IBM4, in both directions (source to target and target to source). As for our alignment method, its alignment quality depends on the number of sub-corpora (N ) that are drawn for each sentence pair. In this work, we choose a constant value of N = 1000 for all sentence pairs. The self-convergency normalization process is repeated for a maximum of 10 iterations. The results for the two alignment methods are reported in Table 3, where we compare them on 2 parallel corpus (BTEC and HIT) and their simple concate"
2013.iwslt-papers.7,2010.iwslt-papers.5,0,0.0519606,"r, composite training corpus evaluation The previous hypothesis seems to hold when considering the larger task corresponding to the concatenation of the two parallel corpora (BTEC+HIT), where HIT data outnumber BTEC data by more than 3:1. Results are however less clearcut here: for instance, our approach still performs better on French→English (average of +0.75 BLEU on one-best hypotheses), but fares worse in terms of oracle performance (average of -0.43 BLEU). These results include a reflection of the fact that giza++ improves its alignment with more data, even when adding out-of-domain data [23]. At this stage of our work, we do not control which particular sentence pairs are drawn in our samples, so assessing the impact of a larger overall sentence pool cannot be done. 3.3.5. Difficult language pair evaluation Lastly, we turn to the more difficult Chinese→English condition, which is significantly more difficult than its French→English counterpart (27.88 BLEU vs. 45.52 BLEU for the giza++ baselines). A similar pattern emerges for the two language pairs: one-best translation performance is comparable, but oracle results indicate a clear advantage for our sampling-based alignment (aver"
2013.iwslt-papers.7,2011.mtsummit-papers.10,0,0.0168133,"9 30.61 29.81 33.79 29.94 Table 4: Results of experiments where a supplementary corpus is pooled and aligned by several methods. only be performed on demand. Indeed, considering that all input sentences in our test set could be translated independently at large intervals of time, it would certainly not be conceivable, time-wise and computation-wise, to perform a full statistical alignment of the iteratively growing bilingual corpus. We will nonetheless report evaluation results for this situation below. Few works have previously considered the task of incremental alignment of parallel corpora [24, 25]. The focus in [25] is put on a careful selection of additional data, a reflection of the fact that not all training data can be beneficial for training and improving SMT systems [26]. For these experiments, we will concentrate on a very specific use of additional data with a conservative view4 : sentences will be pooled from a very large, any-domain parallel corpus (EPPS in Table 1) on the basis that they contain at least one occurrence of a word that is out-of-vocabulary (OOV) in the baseline parallel corpus5 . In order to study a condition where significant numbers of such OOVs exist, we us"
2013.iwslt-papers.7,C12-1010,0,0.011303,"9 30.61 29.81 33.79 29.94 Table 4: Results of experiments where a supplementary corpus is pooled and aligned by several methods. only be performed on demand. Indeed, considering that all input sentences in our test set could be translated independently at large intervals of time, it would certainly not be conceivable, time-wise and computation-wise, to perform a full statistical alignment of the iteratively growing bilingual corpus. We will nonetheless report evaluation results for this situation below. Few works have previously considered the task of incremental alignment of parallel corpora [24, 25]. The focus in [25] is put on a careful selection of additional data, a reflection of the fact that not all training data can be beneficial for training and improving SMT systems [26]. For these experiments, we will concentrate on a very specific use of additional data with a conservative view4 : sentences will be pooled from a very large, any-domain parallel corpus (EPPS in Table 1) on the basis that they contain at least one occurrence of a word that is out-of-vocabulary (OOV) in the baseline parallel corpus5 . In order to study a condition where significant numbers of such OOVs exist, we us"
2013.iwslt-papers.7,E12-1016,0,0.0528085,"input sentences in our test set could be translated independently at large intervals of time, it would certainly not be conceivable, time-wise and computation-wise, to perform a full statistical alignment of the iteratively growing bilingual corpus. We will nonetheless report evaluation results for this situation below. Few works have previously considered the task of incremental alignment of parallel corpora [24, 25]. The focus in [25] is put on a careful selection of additional data, a reflection of the fact that not all training data can be beneficial for training and improving SMT systems [26]. For these experiments, we will concentrate on a very specific use of additional data with a conservative view4 : sentences will be pooled from a very large, any-domain parallel corpus (EPPS in Table 1) on the basis that they contain at least one occurrence of a word that is out-of-vocabulary (OOV) in the baseline parallel corpus5 . In order to study a condition where significant numbers of such OOVs exist, we used the HIT corpus as our main corpus, relatively to which our test set contains 79 unique OOVs (436 occurrences). Our additional training data (EPPS) provided matches for 65 of them."
2013.iwslt-papers.7,P03-1021,0,0.00902301,"nts 3.1. Experimental settings In this section, we describe experiments intended to test the performance of the associative sub-sentential alignment approach described in Section 2. We will focus on measuring the impact of several alignment strategies for a phrase-based SMT system. We will use the Moses toolkit [13], which can be regarded as state-of-the-art for building SMT systems. Moses will be used in all configurations to build phrase tables and reordering tables from alignment matrices, and its decoder will be used to build candidate translations during optimization (using standard MERT [14]) and testing. Translation performance will be measured by classical corpus-based metrics, BLEU [15] and TER [16]. All results are average scores computed on the test set for 3 independent optimization runs on the development set [17]. Experiments will be conducted on three language pairs and two main corpora, and we will make use of several reference translations when possible. We will also resort to oracle decoding using a greedy, approximate local search strategy and a number of phrase-based operators [18] to get some account of the best translation score attainable given each specific phra"
2013.iwslt-papers.7,P02-1040,0,0.0876146,"ance of the associative sub-sentential alignment approach described in Section 2. We will focus on measuring the impact of several alignment strategies for a phrase-based SMT system. We will use the Moses toolkit [13], which can be regarded as state-of-the-art for building SMT systems. Moses will be used in all configurations to build phrase tables and reordering tables from alignment matrices, and its decoder will be used to build candidate translations during optimization (using standard MERT [14]) and testing. Translation performance will be measured by classical corpus-based metrics, BLEU [15] and TER [16]. All results are average scores computed on the test set for 3 independent optimization runs on the development set [17]. Experiments will be conducted on three language pairs and two main corpora, and we will make use of several reference translations when possible. We will also resort to oracle decoding using a greedy, approximate local search strategy and a number of phrase-based operators [18] to get some account of the best translation score attainable given each specific phrase table. We will furthermore consider the compactness of the produced phrase tables, as it can be r"
2013.iwslt-papers.7,C08-1064,0,0.354307,"lignment of rare words and its cascading effects. Figure 4 illustrates a case where the rare French word d´eguis´es (here: in costumes) was only correctly aligned by our technique, and where the negative consequences for the two giza/moses baselines could be important (at least, for our experiments, no translation for d´eguis´es alone could be extracted from this sentence pair by giza++ here). The framework that we have described for targeted additional data selection from parallel corpora will be the basis for our future work. We can, by principle, work at the level of tera-scale translation [28], by accessing efficiently (using suffix arrays) large quantities of unaligned parallel corpora, and perform transpotting and phrase table construction on a per-need basis. However, considering the diversity in nature, origin and quality of all possibly additional training examples, some adaptation should be performed so as to introduce preferences for the most promising examples, and hence extracted translations. In this context, the most realistic scenario will be a follow-up to our previous work on any-text translation [29], where notably little or no a priori knowledge exists about (additi"
2013.iwslt-papers.7,2006.amta-papers.25,0,0.033098,"ssociative sub-sentential alignment approach described in Section 2. We will focus on measuring the impact of several alignment strategies for a phrase-based SMT system. We will use the Moses toolkit [13], which can be regarded as state-of-the-art for building SMT systems. Moses will be used in all configurations to build phrase tables and reordering tables from alignment matrices, and its decoder will be used to build candidate translations during optimization (using standard MERT [14]) and testing. Translation performance will be measured by classical corpus-based metrics, BLEU [15] and TER [16]. All results are average scores computed on the test set for 3 independent optimization runs on the development set [17]. Experiments will be conducted on three language pairs and two main corpora, and we will make use of several reference translations when possible. We will also resort to oracle decoding using a greedy, approximate local search strategy and a number of phrase-based operators [18] to get some account of the best translation score attainable given each specific phrase table. We will furthermore consider the compactness of the produced phrase tables, as it can be regarded as a"
2013.iwslt-papers.7,2012.iwslt-papers.20,1,0.824339,". We can, by principle, work at the level of tera-scale translation [28], by accessing efficiently (using suffix arrays) large quantities of unaligned parallel corpora, and perform transpotting and phrase table construction on a per-need basis. However, considering the diversity in nature, origin and quality of all possibly additional training examples, some adaptation should be performed so as to introduce preferences for the most promising examples, and hence extracted translations. In this context, the most realistic scenario will be a follow-up to our previous work on any-text translation [29], where notably little or no a priori knowledge exists about (additional) training examples, and adaptation should be performed on-the-fly. Finally, it seems obvious that the search for new translations, and in particular for unknown words and phrases as well as poorly adapted phrases, should also be pursued in less parallel corpora (see e.g. [30]). It is then an interesting question to consider how our technique would fare and how it could be adapted to work indifferently on parallel or reasonably comparable sentence pairs. 5. Acknowledgements This work was partially funded by the French Stat"
2013.iwslt-papers.7,P11-2031,0,0.0216776,"gnment strategies for a phrase-based SMT system. We will use the Moses toolkit [13], which can be regarded as state-of-the-art for building SMT systems. Moses will be used in all configurations to build phrase tables and reordering tables from alignment matrices, and its decoder will be used to build candidate translations during optimization (using standard MERT [14]) and testing. Translation performance will be measured by classical corpus-based metrics, BLEU [15] and TER [16]. All results are average scores computed on the test set for 3 independent optimization runs on the development set [17]. Experiments will be conducted on three language pairs and two main corpora, and we will make use of several reference translations when possible. We will also resort to oracle decoding using a greedy, approximate local search strategy and a number of phrase-based operators [18] to get some account of the best translation score attainable given each specific phrase table. We will furthermore consider the compactness of the produced phrase tables, as it can be regarded as a desirable quality of phrase tables licencing works on phrase table pruning (see e.g. [19]), and anormally large phrase ta"
2013.iwslt-papers.7,2012.amta-papers.2,0,0.0388714,"Missing"
2014.iwslt-papers.9,N13-1073,0,0.0357672,"1. Introduction Statistical Machine Translation (SMT) has considerably matured in the past decade and is nowadays a competitive option in most practical machine-assisted translation scenarios. A notable fact about SMT technology is that the construction of high-performance systems is extremely expensive. Even if using appropriate computing resources and parallel programming techniques, building systems for very large data sets requires a significant processing time before any translation can be produced. If individual processing steps may be greatly accelerated, including e.g. word alignment [1] or system tuning [2], the requirement to process the entire parallel data significantly delays the availability of a trained system. And even though a careful pre-selection of bilingual sentences may greatly reduce the size of the training material [3], this selection is itself time-consuming and is not justified when one only needs to translate a handful of documents or documents from multiple domains. In addition, the trained translation models are static. In a state-of-the-art system, all models are extracted from a predefined parallel corpus, and are then used to translate any type of inp"
2014.iwslt-papers.9,P13-1031,0,0.0201309,"istical Machine Translation (SMT) has considerably matured in the past decade and is nowadays a competitive option in most practical machine-assisted translation scenarios. A notable fact about SMT technology is that the construction of high-performance systems is extremely expensive. Even if using appropriate computing resources and parallel programming techniques, building systems for very large data sets requires a significant processing time before any translation can be produced. If individual processing steps may be greatly accelerated, including e.g. word alignment [1] or system tuning [2], the requirement to process the entire parallel data significantly delays the availability of a trained system. And even though a careful pre-selection of bilingual sentences may greatly reduce the size of the training material [3], this selection is itself time-consuming and is not justified when one only needs to translate a handful of documents or documents from multiple domains. In addition, the trained translation models are static. In a state-of-the-art system, all models are extracted from a predefined parallel corpus, and are then used to translate any type of input text. However, new"
2014.iwslt-papers.9,E12-1016,0,0.0115943,"n of high-performance systems is extremely expensive. Even if using appropriate computing resources and parallel programming techniques, building systems for very large data sets requires a significant processing time before any translation can be produced. If individual processing steps may be greatly accelerated, including e.g. word alignment [1] or system tuning [2], the requirement to process the entire parallel data significantly delays the availability of a trained system. And even though a careful pre-selection of bilingual sentences may greatly reduce the size of the training material [3], this selection is itself time-consuming and is not justified when one only needs to translate a handful of documents or documents from multiple domains. In addition, the trained translation models are static. In a state-of-the-art system, all models are extracted from a predefined parallel corpus, and are then used to translate any type of input text. However, new data are constantly made available, and the state-of-the-art SMT approaches cannot seamlessly take advantage of them to improve their performance. Incorporating newly available data can help to increase the n-gram coverage and to i"
2014.iwslt-papers.9,P05-1032,0,0.185327,"rove their performance. Incorporating newly available data can help to increase the n-gram coverage and to improve the parameter estimates of an existing system. These observations provide motivation for incorporating newly available data into existing systems, in particular when the new data is known to be directly relevant to the application documents. Previous works have empirically shown that not all phrase translation examples are necessary to reach top performance, so that phrase tables can be built on a per-need basis for a given input text using random sampling of translation examples [4, 5]. The main strength of these approaches is that they reduce the computation time of translation models and make it possible to extract translations from very large parallel data, even with arbitrarily long translation units. However, these approaches still require to align all the available parallel data at the word level, a serious bottleneck when working with very large amounts of parallel data. In this work, we propose to experiment with an architecture where word alignments are only computed on a perneed basis. This proposal naturally enables efficient, plugand-play use of any newly availa"
2014.iwslt-papers.9,C08-1064,0,0.109183,"rove their performance. Incorporating newly available data can help to increase the n-gram coverage and to improve the parameter estimates of an existing system. These observations provide motivation for incorporating newly available data into existing systems, in particular when the new data is known to be directly relevant to the application documents. Previous works have empirically shown that not all phrase translation examples are necessary to reach top performance, so that phrase tables can be built on a per-need basis for a given input text using random sampling of translation examples [4, 5]. The main strength of these approaches is that they reduce the computation time of translation models and make it possible to extract translations from very large parallel data, even with arbitrarily long translation units. However, these approaches still require to align all the available parallel data at the word level, a serious bottleneck when working with very large amounts of parallel data. In this work, we propose to experiment with an architecture where word alignments are only computed on a perneed basis. This proposal naturally enables efficient, plugand-play use of any newly availa"
2014.iwslt-papers.9,N10-1062,0,0.0179721,"e to extract translations from very large parallel data, even with arbitrarily long translation units. However, these approaches still require to align all the available parallel data at the word level, a serious bottleneck when working with very large amounts of parallel data. In this work, we propose to experiment with an architecture where word alignments are only computed on a perneed basis. This proposal naturally enables efficient, plugand-play use of any newly available parallel data, as well as online learning of system parameters. This is similar to the objectives of stream-based SMT [6], but crucially does not require the actual alignment of all available data. This means that we are able to develop systems even faster: as our experiments show, immediate integration of newly translated documents, combined with online tuning, make it possible to dispense altogether with the development step. This pragmatic solution offers both the capacity to deliver translations to users much earlier, but also to quickly improve subsequent automatic translations. The rest of the paper is organized as follows. In Section 2, we describe our framework for efficient on-demand development of SMT"
2014.iwslt-papers.9,2013.iwslt-papers.7,1,0.728572,"pproximation can be used instead: p(¯ s|t¯) = min(1.0, p(t¯|¯ s) × f req(¯ s) ) f req(t¯) (2) where f req(·) is the relative frequency of the given phrase in the entire corpus. The numerator (p(t¯|¯ s) × f req(¯ s)) represents the predicted joint probability of s¯ and t¯. 2.2. On-demand word alignment The second main peculiarity of our architecture is the ability to perform word alignment on demand for a subset of selected bi-sentences. Word and phrase alignments are required to compute Equation (1), and are obtained using our implementation of the sampling-based alignment method described in [8], which relies on ideas originally introduced in [9]. In this approach, the word alignment between a pair of parallel sentences is generated by a recursive binary segmentation process. Starting with a sentence-level alignment (explicitly available in the parallel corpus), segmentation is performed recursively to match smaller blocks until no block can be further segmented. This process can be viewed as approximate top-down ITG parsing [10], where matching blocks are determined based on association scores between the words in the source and target sentences. In this study, association scores fo"
2014.iwslt-papers.9,2012.eamt-1.62,1,0.84098,"0, p(t¯|¯ s) × f req(¯ s) ) f req(t¯) (2) where f req(·) is the relative frequency of the given phrase in the entire corpus. The numerator (p(t¯|¯ s) × f req(¯ s)) represents the predicted joint probability of s¯ and t¯. 2.2. On-demand word alignment The second main peculiarity of our architecture is the ability to perform word alignment on demand for a subset of selected bi-sentences. Word and phrase alignments are required to compute Equation (1), and are obtained using our implementation of the sampling-based alignment method described in [8], which relies on ideas originally introduced in [9]. In this approach, the word alignment between a pair of parallel sentences is generated by a recursive binary segmentation process. Starting with a sentence-level alignment (explicitly available in the parallel corpus), segmentation is performed recursively to match smaller blocks until no block can be further segmented. This process can be viewed as approximate top-down ITG parsing [10], where matching blocks are determined based on association scores between the words in the source and target sentences. In this study, association scores for the words in the source part of the bi-sentences o"
2014.iwslt-papers.9,J97-3002,0,0.156911,"ord and phrase alignments are required to compute Equation (1), and are obtained using our implementation of the sampling-based alignment method described in [8], which relies on ideas originally introduced in [9]. In this approach, the word alignment between a pair of parallel sentences is generated by a recursive binary segmentation process. Starting with a sentence-level alignment (explicitly available in the parallel corpus), segmentation is performed recursively to match smaller blocks until no block can be further segmented. This process can be viewed as approximate top-down ITG parsing [10], where matching blocks are determined based on association scores between the words in the source and target sentences. In this study, association scores for the words in the source part of the bi-sentences of interest are generated by a sampling-based transpotting method, which 1 Querying a suffix array for a phrase of k words can be performed in (k + log(|C|)) operations, where |C |is the corpus size. A suffix array can be constructed in O(|C |log(|C|)) time. 2 Although this model has been shown to be non essential, we use it for the stability of our systems, especially when untuned systems"
2014.iwslt-papers.9,P07-2045,0,0.00453915,"in a translation sample of s¯, S[¯ s]. The sentence pairs in S[¯ s] are then aligned by our on-demand word alignment, where the generated alignments are denoted as AS[¯s] , and are then used to extract the translations and to compute model parameters θ s¯ for the source phrase s¯. This process is repeated for all source phrases in Σ[d], and the resulting translation table can then be used by a phrase-based decoder to translate the input text into the target language. Besides the translation models, the other models in our system are the same as in the default configuration of the moses system [11], including the lexical weighting and lexicalized reordering models. These models are also computed on-demand based on the computed word alignments. Algorithm 1 On-demand development procedure Data: training corpus C, Input: an input document d, sample size M compute Σ[d] for all s¯ ∈ Σ[d] do S[¯ s] = rnd(M, C, s¯) // Sampling AS[¯s] = owa(S[¯ s]) // Alignment estimate(θ s¯, S[¯ s], AS[¯s] ) // Estimation end for 3. Experiments In this section, we have chosen to illustrate two favorable use cases of our framework in order to demonstrate its capabilities and flexibility. The data used in this w"
2014.iwslt-papers.9,N12-1047,0,0.0265654,"ign the full bi-corpus and the moses scripts to extract a huge phrase table and a reordering table for the entire parallel corpus (respectively 20Gb and 7.5Gb com3 http://www.statmt.org/wmt13 4 http://www.statmt.org/wmt14 5 http://summaries.cochrane.org 6 http://www.kyloo.net/software/doku.php/mgiza: overview pressed on disk), which have to be filtered for each input text. The medical-domain LM was trained on the French side of WMT’14 medical data (containing 4.8M sentences and 78M tokens). The system was optimized with KBMIRA, a variant of the Margin Infused Relaxation Algorithm described in [12], on the Cochrane development set. Translations are computed with the moses phrase-based decoder. Results are reported using the BLEU [13] and TER [14] metrics. In this first scenario, we consider a situation where a stream of documents needs to be translated. After each document has been automatically processed, we also make the plausible assumption that it is post-edited by a human translator, thus providing new data that can be used to update both the models and parameters of the systems before translating the next document. This situation is illustrated using the Cochrane dataset, where we"
2014.iwslt-papers.9,P02-1040,0,0.0951362,"ively 20Gb and 7.5Gb com3 http://www.statmt.org/wmt13 4 http://www.statmt.org/wmt14 5 http://summaries.cochrane.org 6 http://www.kyloo.net/software/doku.php/mgiza: overview pressed on disk), which have to be filtered for each input text. The medical-domain LM was trained on the French side of WMT’14 medical data (containing 4.8M sentences and 78M tokens). The system was optimized with KBMIRA, a variant of the Margin Infused Relaxation Algorithm described in [12], on the Cochrane development set. Translations are computed with the moses phrase-based decoder. Results are reported using the BLEU [13] and TER [14] metrics. In this first scenario, we consider a situation where a stream of documents needs to be translated. After each document has been automatically processed, we also make the plausible assumption that it is post-edited by a human translator, thus providing new data that can be used to update both the models and parameters of the systems before translating the next document. This situation is illustrated using the Cochrane dataset, where we take the 100 documents constituting the test set (see Table 1) to simulate the document stream. In the following, we describe a series of"
2014.iwslt-papers.9,2006.amta-papers.25,0,0.00978038,"d 7.5Gb com3 http://www.statmt.org/wmt13 4 http://www.statmt.org/wmt14 5 http://summaries.cochrane.org 6 http://www.kyloo.net/software/doku.php/mgiza: overview pressed on disk), which have to be filtered for each input text. The medical-domain LM was trained on the French side of WMT’14 medical data (containing 4.8M sentences and 78M tokens). The system was optimized with KBMIRA, a variant of the Margin Infused Relaxation Algorithm described in [12], on the Cochrane development set. Translations are computed with the moses phrase-based decoder. Results are reported using the BLEU [13] and TER [14] metrics. In this first scenario, we consider a situation where a stream of documents needs to be translated. After each document has been automatically processed, we also make the plausible assumption that it is post-edited by a human translator, thus providing new data that can be used to update both the models and parameters of the systems before translating the next document. This situation is illustrated using the Cochrane dataset, where we take the 100 documents constituting the test set (see Table 1) to simulate the document stream. In the following, we describe a series of increasingly"
2014.iwslt-papers.9,D09-1079,0,0.0234682,"be mostly attributed to the absence of tuning. However, translations for the test set are delivered much faster, where our system is x36 times wall clock faster than moses. 7 We used the moses decoder in our experiments, whose default parameters are: 0.3 for all 7 reordering features, including 6 lexical reordering features and 1 distance-based reordering feature; 0.2 for all 5 translation features; 0.5 for the language model and −1 for the word penalty. 8 In this work, the language model is still pre-trained. Future work will include the incremental / on-demand estimation of language models [15]. 216 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2500 8 6 2000 Time(ms)/Token 4 ∆BLEU 1500 1000 on-demand on-demand+spec 2 0 2 4 500 6 0 0 20 40 60 Document id 80 8 0 100 Figure 1: Evolution of the average per token processing time for a sequence of documents. As mentioned before, the computed word alignments are cached and are available for translating subsequent documents. To further analyze the effect of the cache, Figure 1 shows how the average per token processing time decreases as more and more documents from the s"
2014.iwslt-papers.9,E14-1042,0,0.0108161,"of data used. However, for those domain-specific terms, phraseology or long phrases which usually only exist in the in-domain data, we could use the spec phrase table to translate them. 9 In fact, the Cochrane dataset used in this study is made of two parts: a large portion of the data was translated by human translator from scratch, while a smaller amount a document where actually produced through postedition. We still use this data as a post-edited corpus in our experiments, although these two kinds of data are slightly different. We believe this does not affect our experimental conclusions [16]. 20 40 60 Document id 80 100 Figure 2: Document-level comparison with moses system in English-to-French translation direction. The y-axis represents the difference in BLEU score (∆BLEU) between our systems and the vanilla moses system for each document in the sequence. Results in Table 2 show that the additional table (+spec) helps to significantly improve translation quality over the raw on-demand configuration (+3.7 BP), for a modest additional processing time of half an hour for aligning the content of the first 99 documents. Since the spec table for document dt is estimated based on the p"
2014.iwslt-papers.9,W07-0733,0,0.026144,"system development. In practice, the system’s weights are retuned after each document has been translated (and post-edited) as follows: Taking the previous weights as the initial point, we run the parameter tuning process (here KBMIRA) on the just translated and post-edited document; the resulting parameter values are then averaged with the parameter values of the 10 previous documents10 , and then used for translating the next document. Additionally, in order to leverage the spec table, we also allow here the spec phrase table to compete with the phrase table estimated from the static corpus [17] instead of having the latter take precedence. Results for this last configuration are given in Table 2 (+online). Our simple online tuning yields a significant improvement (+4.1 BP) over the untuned on-demand+spec configuration. Even though the two configurations cannot be directly compared at the corpuslevel, since our system integrates a growing set of in-domain data, while moses on its part greatly benefits from the indomain development data, we still note that our framework now outperforms the moses baseline (+2.3 BP). More interestingly, comparison at the document-level (see Figure 3) de"
2014.iwslt-papers.9,W00-0507,0,0.0887616,"e translation quality at the beginning is not very competitive. Also, its incremental adaptation scheme quickly improves its performance, especially on long and repetitive documents. 4. Related Work Our framework provides an innovative methodology that is also suitable for interactive MT: we measured wall clock times of less than 1 minute (before any cache is available) to build translation tables for individual sentences, making it practical to integrate system development within interactive human post-editing. Interactive Machine Translation (IMT) was pioneered by projects such as TransType [18], where an SMT system assists the human translator by proposing translation completions that the translator can accept, modify or ignore. IMT was later further developed to enable more types of interaction [19, 20] and to integrate the result of the interaction to influence future choices of the system. More recently, online learning was introduced in the IMT framework [21] to improve the exploitation of the translator’s feedback. A similar idea was also presented in [22]. In this work, the input document is processed sentence by sentence. After the translation of each sentence, the MT output"
2014.iwslt-papers.9,J09-1002,0,0.013332,"rk provides an innovative methodology that is also suitable for interactive MT: we measured wall clock times of less than 1 minute (before any cache is available) to build translation tables for individual sentences, making it practical to integrate system development within interactive human post-editing. Interactive Machine Translation (IMT) was pioneered by projects such as TransType [18], where an SMT system assists the human translator by proposing translation completions that the translator can accept, modify or ignore. IMT was later further developed to enable more types of interaction [19, 20] and to integrate the result of the interaction to influence future choices of the system. More recently, online learning was introduced in the IMT framework [21] to improve the exploitation of the translator’s feedback. A similar idea was also presented in [22]. In this work, the input document is processed sentence by sentence. After the translation of each sentence, the MT output and the post-edited translation are analyzed and used to extract postediting rules. These rules are then used to automatically process the MT output so as to improve the quality of output translations. 5. Conclusio"
2014.iwslt-papers.9,P09-4005,0,0.0134991,"rk provides an innovative methodology that is also suitable for interactive MT: we measured wall clock times of less than 1 minute (before any cache is available) to build translation tables for individual sentences, making it practical to integrate system development within interactive human post-editing. Interactive Machine Translation (IMT) was pioneered by projects such as TransType [18], where an SMT system assists the human translator by proposing translation completions that the translator can accept, modify or ignore. IMT was later further developed to enable more types of interaction [19, 20] and to integrate the result of the interaction to influence future choices of the system. More recently, online learning was introduced in the IMT framework [21] to improve the exploitation of the translator’s feedback. A similar idea was also presented in [22]. In this work, the input document is processed sentence by sentence. After the translation of each sentence, the MT output and the post-edited translation are analyzed and used to extract postediting rules. These rules are then used to automatically process the MT output so as to improve the quality of output translations. 5. Conclusio"
2014.iwslt-papers.9,N10-1079,0,0.0223225,"build translation tables for individual sentences, making it practical to integrate system development within interactive human post-editing. Interactive Machine Translation (IMT) was pioneered by projects such as TransType [18], where an SMT system assists the human translator by proposing translation completions that the translator can accept, modify or ignore. IMT was later further developed to enable more types of interaction [19, 20] and to integrate the result of the interaction to influence future choices of the system. More recently, online learning was introduced in the IMT framework [21] to improve the exploitation of the translator’s feedback. A similar idea was also presented in [22]. In this work, the input document is processed sentence by sentence. After the translation of each sentence, the MT output and the post-edited translation are analyzed and used to extract postediting rules. These rules are then used to automatically process the MT output so as to improve the quality of output translations. 5. Conclusion This work has addressed the issue of how the computationally expensive cost of the development of high-performance 220 Proceedings of the 11th International Wor"
2014.iwslt-papers.9,2013.mtsummit-papers.24,0,0.0123177,"nt within interactive human post-editing. Interactive Machine Translation (IMT) was pioneered by projects such as TransType [18], where an SMT system assists the human translator by proposing translation completions that the translator can accept, modify or ignore. IMT was later further developed to enable more types of interaction [19, 20] and to integrate the result of the interaction to influence future choices of the system. More recently, online learning was introduced in the IMT framework [21] to improve the exploitation of the translator’s feedback. A similar idea was also presented in [22]. In this work, the input document is processed sentence by sentence. After the translation of each sentence, the MT output and the post-edited translation are analyzed and used to extract postediting rules. These rules are then used to automatically process the MT output so as to improve the quality of output translations. 5. Conclusion This work has addressed the issue of how the computationally expensive cost of the development of high-performance 220 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 SMT systems, which typic"
2015.lilt-12.6,P93-1002,0,0.493111,"azon are in the ’fiction’ category (source: http://authorearnings.com/report/the-50k-report/). 2 For instance, J. K. Rowling’s Harry Potter has already been translated into over 70 languages. 3 An example implementation is at http://www.doppeltext.com/. 4 / L I LT VOLUME 12, ISSUE 6 O CTOBER 2015 be grouped into two main families: on the one hand, length-based approaches (Gale and Church, 1991, Brown et al., 1991) exploit the fact that a short sentence has a short translation, and a long sentence has a long translation. On the other hand, lexical matching approaches (Kay and Röscheisen, 1993, Chen, 1993, Simard et al., 1993, Melamed, 1999, Ma, 2006) identify sure anchor points for the alignment using bilingual dictionaries or crude surface similarities between word forms. Length-based approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results but at higher computational cost. The majority of the state-of-the-art approaches to the problem (Langlais, 1998, Simard and Plamondon, 1998, Moore, 2002, Varga et al., 2005, Braune and Fraser, 2010, Lamraoui and Langlais, 2013) combine both types of information. The goal of these methods, however, is pr"
2015.lilt-12.6,P91-1023,0,0.496955,"ap” of the bitext, trying to include as many anchors as possible, while also remaining close to the bitext “diagonal”. A post-processing step will take sentence boundaries into account to deliver the final sentence alignment. Note that GMA uses no length cues, and also that it has been shown to perform particularly well at spotting large omissions in a bitext (Melamed, 1996). Moore’s (2002) approach implements a two-pass, coarse-to-fine, strategy: a first pass, based on sentence length cues, computes a first alignment according to the principles of length-based approaches (Brown et al., 1991, Gale and Church, 1991). This initial alignment is used to train a simplified version of IBM model 1 (Brown et al., 1993), which provides the alignment system with lexical association scores. These scores are then used to refine the measure of association between sentences. This approach is primarily aimed at delivering high-confidence, 1:1 sentence alignments to be used as training material for data-intensive MT. Sentences that cannot be reliably aligned are discarded from the resulting alignment. Hunalign is described in (Varga et al., 2005). It also implements a two-pass strategy which resembles Moore’s approach."
2015.lilt-12.6,J93-1004,0,0.912338,"CTOBER 2015 ilar to (Munteanu and Marcu, 2005, Smith et al., 2010, Éva Mújdricza-Maydt et al., 2013). These steps are discussed below in detail. 3.1 A MaxEnt Model for Parallel Sentences Any sentence alignment method needs, at some point, to assess the level of parallelism of a sentence pair, based on a surface description of these sentences. As discussed above, two kinds of clues are widely employed in existing systems to perform such assessment: sentence lengths and lexical information. Most dynamic programming-based approaches further impose a prior probability distribution on link types (Gale and Church, 1993). Our system combines all the available clues in a principled, rather than heuristic, way, using a MaxEnt model.13 For any pair of sentences l = (e, f ), the model computes a link posterior probability p(Y = y|e, f ), where Y is a binary variable for the existence of an alignment link. The rationale for using MaxEnt is (a) that it is possible to efficiently integrate as many features as desired into the model, and (b) that we expect the resulting posterior probabilities to be less peaked towards extreme values than what we have observed with generative alignment models such as Moore’s model. W"
2015.lilt-12.6,2005.mtsummit-papers.11,0,0.515001,"rmance of the baseline sentence alignment tools on these four corpora, using the standard link-level F-measure. As explained above, the two larger corpora are aligned at the paragraph level, meaning that such resources cannot be readily used to compute alignment quality scores. Our solution has been to refine this coarse alignment by running the Gale and Church (1991) alignment program to compute within-paragraph sentence alignments, keeping the paragraph alignments unchanged from the reference. This approach is similar to the procedure used to align the Europarl corpus at the sentence level (Koehn, 2005), where reliable paragraph boundaries are readily derived from speaker turns or session changes. As a result, these semiautomatic references only contain a restricted number of link types as computed by Gale and Church (1991) program: 1:0, 0:1, 1:1, 1:2, and 2:1. We then take these partially correct alignments as pseudo-references for the purpose of evaluating alignment tools – keeping in mind that the corresponding results will only be approximate. Our main evaluation results are in Table 2. For more details, see the Appendix, Tables I, II and III. Regarding the gold corpus (manual en-fr), th"
2015.lilt-12.6,2013.mtsummit-papers.10,0,0.149492,"a long translation. On the other hand, lexical matching approaches (Kay and Röscheisen, 1993, Chen, 1993, Simard et al., 1993, Melamed, 1999, Ma, 2006) identify sure anchor points for the alignment using bilingual dictionaries or crude surface similarities between word forms. Length-based approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results but at higher computational cost. The majority of the state-of-the-art approaches to the problem (Langlais, 1998, Simard and Plamondon, 1998, Moore, 2002, Varga et al., 2005, Braune and Fraser, 2010, Lamraoui and Langlais, 2013) combine both types of information. The goal of these methods, however, is primarily to deliver high-precision parallel sentence pairs to fuel SMT systems or feed translation memories in specialized domains. They can then safely prune blocks of sentence pairs whenever their alignment is uncertain; some methods even only target highconfidence, 1:1, alignment links. A significant part of recent developments in sentence alignment have tried to make the large-scale harvesting of parallel sentence pairs work also for noisy parallel data (Éva Mújdricza-Maydt et al., 2013), as well as for comparable"
2015.lilt-12.6,P98-1117,0,0.173728,"s. Section 3 presents our two-pass alignment algorithm; one of its distinguishing feature is its use of external resources to improve its decisions. Experiments on two language pairs (English-French and English-Spanish) are presented and discussed in Section 4, before we recap our main findings and discuss further prospects in Section 5. 2 Aligning literary texts: solved or unsolved? Commenting on the unsatisfactory results achieved by all sentence alignment systems during the Arcade evaluation campaign (Véronis and Langlais, 2000) on the single test book, Jules Verne’s De la terre à la lune, Langlais et al. (1998) hint that: these poor results are linked to the literary nature of the corpus, where translation is freer and more interpretative, They express a general feeling that literary texts should be more difficult to align than, say, technical documents. However, assessing the real difficulty of the task is in itself challenging, for lack of a large set of books annotated with a reference (gold) alignment. For instance, the recent study of Éva MújdriczaMaydt et al. (2013) on English-German alignment used only three books for evaluation. In this section, we aim to provide a more precise answer to thi"
2015.lilt-12.6,ma-2006-champollion,0,0.0290568,"://authorearnings.com/report/the-50k-report/). 2 For instance, J. K. Rowling’s Harry Potter has already been translated into over 70 languages. 3 An example implementation is at http://www.doppeltext.com/. 4 / L I LT VOLUME 12, ISSUE 6 O CTOBER 2015 be grouped into two main families: on the one hand, length-based approaches (Gale and Church, 1991, Brown et al., 1991) exploit the fact that a short sentence has a short translation, and a long sentence has a long translation. On the other hand, lexical matching approaches (Kay and Röscheisen, 1993, Chen, 1993, Simard et al., 1993, Melamed, 1999, Ma, 2006) identify sure anchor points for the alignment using bilingual dictionaries or crude surface similarities between word forms. Length-based approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results but at higher computational cost. The majority of the state-of-the-art approaches to the problem (Langlais, 1998, Simard and Plamondon, 1998, Moore, 2002, Varga et al., 2005, Braune and Fraser, 2010, Lamraoui and Langlais, 2013) combine both types of information. The goal of these methods, however, is primarily to deliver high-precision parallel sent"
2015.lilt-12.6,C96-2129,0,0.175554,"roach included in this sample, and yet one of the most effective: assuming “sure” lexical anchor points in the bitext map, obtained e.g. using bilingual dictionaries or cognatebased heuristics, GMA greedily builds a so-called “sentence map” of the bitext, trying to include as many anchors as possible, while also remaining close to the bitext “diagonal”. A post-processing step will take sentence boundaries into account to deliver the final sentence alignment. Note that GMA uses no length cues, and also that it has been shown to perform particularly well at spotting large omissions in a bitext (Melamed, 1996). Moore’s (2002) approach implements a two-pass, coarse-to-fine, strategy: a first pass, based on sentence length cues, computes a first alignment according to the principles of length-based approaches (Brown et al., 1991, Gale and Church, 1991). This initial alignment is used to train a simplified version of IBM model 1 (Brown et al., 1993), which provides the alignment system with lexical association scores. These scores are then used to refine the measure of association between sentences. This approach is primarily aimed at delivering high-confidence, 1:1 sentence alignments to be used as t"
2015.lilt-12.6,J99-1003,0,0.636382,"y (source: http://authorearnings.com/report/the-50k-report/). 2 For instance, J. K. Rowling’s Harry Potter has already been translated into over 70 languages. 3 An example implementation is at http://www.doppeltext.com/. 4 / L I LT VOLUME 12, ISSUE 6 O CTOBER 2015 be grouped into two main families: on the one hand, length-based approaches (Gale and Church, 1991, Brown et al., 1991) exploit the fact that a short sentence has a short translation, and a long sentence has a long translation. On the other hand, lexical matching approaches (Kay and Röscheisen, 1993, Chen, 1993, Simard et al., 1993, Melamed, 1999, Ma, 2006) identify sure anchor points for the alignment using bilingual dictionaries or crude surface similarities between word forms. Length-based approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results but at higher computational cost. The majority of the state-of-the-art approaches to the problem (Langlais, 1998, Simard and Plamondon, 1998, Moore, 2002, Varga et al., 2005, Braune and Fraser, 2010, Lamraoui and Langlais, 2013) combine both types of information. The goal of these methods, however, is primarily to deliver high-precision pa"
2015.lilt-12.6,moore-2002-fast,0,0.600086,"sentence has a short translation, and a long sentence has a long translation. On the other hand, lexical matching approaches (Kay and Röscheisen, 1993, Chen, 1993, Simard et al., 1993, Melamed, 1999, Ma, 2006) identify sure anchor points for the alignment using bilingual dictionaries or crude surface similarities between word forms. Length-based approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results but at higher computational cost. The majority of the state-of-the-art approaches to the problem (Langlais, 1998, Simard and Plamondon, 1998, Moore, 2002, Varga et al., 2005, Braune and Fraser, 2010, Lamraoui and Langlais, 2013) combine both types of information. The goal of these methods, however, is primarily to deliver high-precision parallel sentence pairs to fuel SMT systems or feed translation memories in specialized domains. They can then safely prune blocks of sentence pairs whenever their alignment is uncertain; some methods even only target highconfidence, 1:1, alignment links. A significant part of recent developments in sentence alignment have tried to make the large-scale harvesting of parallel sentence pairs work also for noisy p"
2015.lilt-12.6,J05-4003,0,0.543729,"of information. The goal of these methods, however, is primarily to deliver high-precision parallel sentence pairs to fuel SMT systems or feed translation memories in specialized domains. They can then safely prune blocks of sentence pairs whenever their alignment is uncertain; some methods even only target highconfidence, 1:1, alignment links. A significant part of recent developments in sentence alignment have tried to make the large-scale harvesting of parallel sentence pairs work also for noisy parallel data (Éva Mújdricza-Maydt et al., 2013), as well as for comparable bilingual corpora (Munteanu and Marcu, 2005, Smith et al., 2010), using supervised learning techniques. While these restrictions are reasonable for the purpose of training SMT systems,4 for other applications, such as bitext visualization, translator training, automatic translation checking, the alignment for the entire bitext should be computed. This is especially the case for multilingual works of fiction: the parts that are more difficult for automatic alignment algorithms (usually involving highly non-literal translations or large blocks of insertions/deletions) often correspond to parts where a reader might also look for help. For"
2015.lilt-12.6,2010.amta-papers.14,0,0.381855,"Missing"
2015.lilt-12.6,N10-1063,0,0.17645,"of these methods, however, is primarily to deliver high-precision parallel sentence pairs to fuel SMT systems or feed translation memories in specialized domains. They can then safely prune blocks of sentence pairs whenever their alignment is uncertain; some methods even only target highconfidence, 1:1, alignment links. A significant part of recent developments in sentence alignment have tried to make the large-scale harvesting of parallel sentence pairs work also for noisy parallel data (Éva Mújdricza-Maydt et al., 2013), as well as for comparable bilingual corpora (Munteanu and Marcu, 2005, Smith et al., 2010), using supervised learning techniques. While these restrictions are reasonable for the purpose of training SMT systems,4 for other applications, such as bitext visualization, translator training, automatic translation checking, the alignment for the entire bitext should be computed. This is especially the case for multilingual works of fiction: the parts that are more difficult for automatic alignment algorithms (usually involving highly non-literal translations or large blocks of insertions/deletions) often correspond to parts where a reader might also look for help. For such tasks, it seems"
2015.lilt-12.6,C10-1124,0,0.0197394,"f state-of-the-art methods for literary texts, both in terms of their precision and recall, using large collections of publicly available novels; (b) develop, analyse and improve an algorithm initially introduced in (Yu et al., 2012a,b), which was shown to outperform a significant sample of sentence alignment tools on a set of manually aligned corpora. The rest of this paper is organized as follows: we briefly review, in Section 2, several state-of-the-art sentence alignment tools and evaluate their performance, first on two small reference datasets of gold alignments, then on a 4 The work by Uszkoreit et al. (2010), however, shows that this procedure actually discards a lot of useful data. S ENTENCE ALIGNMENT FOR LITERARY TEXTS / 5 much larger set of approximately correct alignments. Section 3 presents our two-pass alignment algorithm; one of its distinguishing feature is its use of external resources to improve its decisions. Experiments on two language pairs (English-French and English-Spanish) are presented and discussed in Section 4, before we recap our main findings and discuss further prospects in Section 5. 2 Aligning literary texts: solved or unsolved? Commenting on the unsatisfactory results ac"
2015.lilt-12.6,W12-2505,1,0.148175,"nment algorithms (usually involving highly non-literal translations or large blocks of insertions/deletions) often correspond to parts where a reader might also look for help. For such tasks, it seems that both precision and recall are to be maximized. This paper therefore reconsiders the task of full-text sentence alignment with two goals: (a) re-evaluate the actual performance of state-of-the-art methods for literary texts, both in terms of their precision and recall, using large collections of publicly available novels; (b) develop, analyse and improve an algorithm initially introduced in (Yu et al., 2012a,b), which was shown to outperform a significant sample of sentence alignment tools on a set of manually aligned corpora. The rest of this paper is organized as follows: we briefly review, in Section 2, several state-of-the-art sentence alignment tools and evaluate their performance, first on two small reference datasets of gold alignments, then on a 4 The work by Uszkoreit et al. (2010), however, shows that this procedure actually discards a lot of useful data. S ENTENCE ALIGNMENT FOR LITERARY TEXTS / 5 much larger set of approximately correct alignments. Section 3 presents our two-pass alig"
bouamor-etal-2012-contrastive,I05-5002,0,\N,Missing
bouamor-etal-2012-contrastive,W04-3219,0,\N,Missing
bouamor-etal-2012-contrastive,J10-3003,0,\N,Missing
bouamor-etal-2012-contrastive,C04-1077,0,\N,Missing
bouamor-etal-2012-contrastive,W03-1004,0,\N,Missing
bouamor-etal-2012-contrastive,P02-1040,0,\N,Missing
bouamor-etal-2012-contrastive,P01-1008,0,\N,Missing
bouamor-etal-2012-contrastive,C10-1149,0,\N,Missing
bouamor-etal-2012-contrastive,P08-4006,0,\N,Missing
bouamor-etal-2012-contrastive,W07-0734,0,\N,Missing
bouamor-etal-2012-contrastive,W09-0621,0,\N,Missing
bouamor-etal-2012-contrastive,P11-1020,0,\N,Missing
bouamor-etal-2012-contrastive,E09-1082,0,\N,Missing
bouamor-etal-2012-contrastive,J08-4005,0,\N,Missing
bouamor-etal-2012-contrastive,shimohata-etal-2004-building,0,\N,Missing
bouamor-etal-2012-contrastive,max-wisniewski-2010-mining,1,\N,Missing
C04-1166,W00-1404,0,0.0278582,"tion to systems presenting the user with the evolving text of the document (often called the feedback or control text) in her language, following from the WYSIWYM (What You See Is What You Meant) approach (Power and Scott, 1998).2 Anchors (or active zones) in the text of the evolving document allow the user to specify further its semantics by making choices presented to her in her language. The underlying content representation is then used to generate the text of the document in as many languages as the system supports. The MDA (Multilingual Document Authoring) system (Dymetman et al., 2000; Brun et al., 2000) follows the WYSIWYM approach, but puts a strong emphasis on the well-formedness of document semantic content. More particularly, document content can be specified in terms of communicative goals, allowing the selection of messages which are contrastive within the modelled class of documents in no more steps than is needed to identify a predefined communicative goal. Figure 1 illustrates the architecture of a MDA system. A MDA grammar specifies the possible content representations of a document in terms of trees of typed semantic objects in a formalism inspired from Definite Clause Grammars (P"
C04-1166,C00-1036,0,0.109045,"re of a MDA system creation to systems presenting the user with the evolving text of the document (often called the feedback or control text) in her language, following from the WYSIWYM (What You See Is What You Meant) approach (Power and Scott, 1998).2 Anchors (or active zones) in the text of the evolving document allow the user to specify further its semantics by making choices presented to her in her language. The underlying content representation is then used to generate the text of the document in as many languages as the system supports. The MDA (Multilingual Document Authoring) system (Dymetman et al., 2000; Brun et al., 2000) follows the WYSIWYM approach, but puts a strong emphasis on the well-formedness of document semantic content. More particularly, document content can be specified in terms of communicative goals, allowing the selection of messages which are contrastive within the modelled class of documents in no more steps than is needed to identify a predefined communicative goal. Figure 1 illustrates the architecture of a MDA system. A MDA grammar specifies the possible content representations of a document in terms of trees of typed semantic objects in a formalism inspired from Definit"
C04-1166,W99-0625,0,0.0208472,"ubparts that should belong to the solution, this information should be taken into account while reanalyzing the document, which is not the case in the current implementation. If the solution has to be present in the list of candidates returned, it should be as close to the top of the list as possible, so that the first choices for each underspecification represent the actual best choices. To this end, we intend to implement a second-pass analysis that would rerank the candidates produced by fuzzy inverted generation by computing text similarities over short passages such as those proposed in (Hatzivassiloglou et al., 1999). These techniques were much harder to implement during the search in the virtual space of documents produced by the document model, because partial content representations are not actual texts. 4 Acknowledgements Many thanks to Marc Dymetman, who supervised my work at Xerox Research Centre Europe and who originally came up with the concept of fuzzy inverted generation. Many thanks also to Christian Boitet, my university PhD supervisor, and to Anne-Lise Bully, C´edric Leray and Abdelkhalek Rherad for their programming work on the interface of the presented system. This work was funded by a PhD"
C04-1166,E03-3006,1,0.489897,"ormedness of document semantic content. More particularly, document content can be specified in terms of communicative goals, allowing the selection of messages which are contrastive within the modelled class of documents in no more steps than is needed to identify a predefined communicative goal. Figure 1 illustrates the architecture of a MDA system. A MDA grammar specifies the possible content representations of a document in terms of trees of typed semantic objects in a formalism inspired from Definite Clause Grammars (Pereira and Warren, 1980). 2 We have done a review of these systems in (Max, 2003a) in which we have identified and compared five families of approaches. 2 Figure 2: Document normalization in a given class of documents Considering all the possibilities offered by having the semantic description of a document, for example the exploitation within the Semantic Web, it seemed very interesting to reuse resources developed for controlled document authoring to analyze existing documents. Also, a corpus study of drug leaflets that we conducted (Max, 2003a) showed that documents from the same class of documents could contain a lot of variation, which can hamper the reader’s underst"
C04-1166,P98-2173,0,0.025623,"and a generation system produces multilingual versions of it. Updating documents is then done by updating the document content, and only some postediting may take place instead of full translation by a human translator. Systems implementing this paradigm range from template-based multilingual document 1 This is a reedition of the original article. Figure 1: Architecture of a MDA system creation to systems presenting the user with the evolving text of the document (often called the feedback or control text) in her language, following from the WYSIWYM (What You See Is What You Meant) approach (Power and Scott, 1998).2 Anchors (or active zones) in the text of the evolving document allow the user to specify further its semantics by making choices presented to her in her language. The underlying content representation is then used to generate the text of the document in as many languages as the system supports. The MDA (Multilingual Document Authoring) system (Dymetman et al., 2000; Brun et al., 2000) follows the WYSIWYM approach, but puts a strong emphasis on the well-formedness of document semantic content. More particularly, document content can be specified in terms of communicative goals, allowing the"
C04-1166,C90-3048,0,0.0344559,"are discussed. 1 Document normalization The authoring of documents in constrained domains and their translation into other languages is a very important activity in industrial settings. In some cases, the distinction between technical writers and technical translators has started to blur, so as to minimize the time and efforts needed to obtain multilingual documents. The paradigm of translation for monolinguals introduced by Kay in 1973 (Kay, 1997)1 led the way to a new conception of the authoring task, which first materialized with systems involving human disambiguation (e.g. (Boitet, 1989; Somers et al., 1990)). A related paradigm emerged in the 90s (Hartley and Paris, 1997), whereby a technical author is responsible for providing the content of a document and a generation system produces multilingual versions of it. Updating documents is then done by updating the document content, and only some postediting may take place instead of full translation by a human translator. Systems implementing this paradigm range from template-based multilingual document 1 This is a reedition of the original article. Figure 1: Architecture of a MDA system creation to systems presenting the user with the evolving tex"
C04-1166,C98-2168,0,\N,Missing
C10-1027,W09-0432,0,0.0225697,"ion system for 233 the pair A → B, for which the parallel data is sparse; assuming further that such parallel resources exist for pairs A → C and for C → B, it is then tempting to perform the translation indirectly through pivoting, by first translating from A to C, then from C to B. Direct implementations of this idea are discussed e.g. in (Utiyama and Isahara, 2007). Pivoting can also intervene earlier in the process, for instance as a means to automatically generate the missing parallel resource, an idea that has also been considered to adapt an existing translation systems to new domains (Bertoldi and Federico, 2009). Pivoting can finally be used to fix or improve the translation model: (Cohn and Lapata, 2007) augments the phrase table for a baseline bilingual system with supplementary phrases obtained by pivoting into a third language. Triangulation in translation Triangulation techniques are somewhat more general and only require the availabily of one auxiliary system (or one auxiliary parallel corpus). For instance, the authors of (Chen et al., 2008) propose to use the translation model of an auxiliary C → B system to filter-out the phrase-table of a primary A → B system. 2.2 Our framework As in other"
C10-1027,W08-0309,0,0.0145645,"ount all the available translations and scores. Various to lead to measurable improvements. 2 We plan to experiment next on using predictions at the document level. proposals have been made to efficiently perform such a combination, using auxiliary data structures such as n-best lists, word lattices or consensus networks (see for instance (Kumar and Byrne, 2004; Rosti et al., 2007; Matusov et al., 2008; Hildebrand and Vogel, 2008; Tromble et al., 2008)). Theses techniques have proven extremely effective and have allowed to deliver very significant gains in several recent evaluation campaigns (Callison-Burch et al., 2008). Multisource translation A related, yet more resourceful approach, consists in trying to combine several systems providing translations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by several Foreign→English systems, where output selection is based on the translation scores. The intuition that if a system assigns a high figure of merits to the translation of a particular sentence, then this translation should be preferred, is implemented in the"
C10-1027,P96-1041,0,0.0252501,"am model, our SMT system uses six additional models which are linearly combined following a discriminative modeling framework: two lexicalized reordering (Tillmann, 2004) models,a target-language model, two lexicon models, a ’weak’ distancebased distortion model, a word bonus model and a translation unit bonus model. Coefficients in this linear combination are tuned over development data with the MERT optimization toolkit4 , slightly modified to use our decoder’s n-best lists. For this study, we used 3-gram bilingual and 3-gram target language models built using modified Kneser-Ney smoothing (Chen and Goodman, 1996); model estimation was performed with the SRI language modeling toolkit.5 Target language 4 http://www.statmt.org/moses http://wwww.speech.sri.com/projects/ srilm 235 5 models were trained on the target side of the bitext corpora. After preprocessing the corpora with standard tokenization tools, word-to-word alignments are performed in both directions, source-to-target and target-to-source. In our system implementation, the GIZA++ toolkit6 is used to compute the word alignments. Then, the grow-diag-final-and heuristic is used to obtain the final alignments from which translation units are extr"
C10-1027,chen-etal-2008-improving,0,0.146052,"n the translation scores. The intuition that if a system assigns a high figure of merits to the translation of a particular sentence, then this translation should be preferred, is implemented in the M AX combination heuristics, whose relative (lack of) success is discussed in (Schwartz, 2008). A similar idea is explored in (Nomoto, 2004), where the sole target language model score is used to rank competing outputs. (Schroeder et al., 2009) propose to combine the available sources prior to translation, under the form of a multilingual lattice, which is decoded with a multisource phrase table. (Chen et al., 2008) integrate the available auxiliary information in a different manner, and discuss how to improve the translation model of the primary system: the idea is to use the entries in the phrase table of the auxiliary system to filter out those accidental correspondences that pollute the main translation model. The most effective implementation of multisource translation to date however consists in using mono-source system combination techniques (Schroeder et al., 2009). Translation through pivoting The use of auxiliary systems has also been proposed in another common situation, as a possible remedy t"
C10-1027,P07-1092,0,0.149323,"parallel resources exist for pairs A → C and for C → B, it is then tempting to perform the translation indirectly through pivoting, by first translating from A to C, then from C to B. Direct implementations of this idea are discussed e.g. in (Utiyama and Isahara, 2007). Pivoting can also intervene earlier in the process, for instance as a means to automatically generate the missing parallel resource, an idea that has also been considered to adapt an existing translation systems to new domains (Bertoldi and Federico, 2009). Pivoting can finally be used to fix or improve the translation model: (Cohn and Lapata, 2007) augments the phrase table for a baseline bilingual system with supplementary phrases obtained by pivoting into a third language. Triangulation in translation Triangulation techniques are somewhat more general and only require the availabily of one auxiliary system (or one auxiliary parallel corpus). For instance, the authors of (Chen et al., 2008) propose to use the translation model of an auxiliary C → B system to filter-out the phrase-table of a primary A → B system. 2.2 Our framework As in other works, we propose to make use of several MT systems (of any type) to improve translation perfor"
C10-1027,P03-2017,1,0.625206,"used as auxiliary information for the decoding of the direct system. Configurations 4 and 5 are instances of multisource translation, where a paraphrase or a translation of the source text is available. Lastly, configuration 6 illustrates the case where a human translator, with knowledge of the target language and at least of one of the available source languages, could influence the decoding by providing desired3 words (e.g. only for source words or phrases that would be judged difficult to translate). This human supervision through a feedback text in real time is similar to the proposal of (Dymetman et al., 2003). Given this framework, several questions arise, 3 The proposal as it is limits the hypotheses produced by the system to those that are attainable given its training data. It is conceivable, however, to find ways of introducing new knowledge in this framework. 234 the most important underlying this work being whether the performance of SMT systems can be improved by using other SMT systems. Another point of interest is whether improvements made to auxiliary systems can yield improvement to the direct system, without the latter undergoing any modification. 2.3 Furthermore, we are interested in"
C10-1027,2008.amta-srw.3,0,0.0297833,"iew here. System combination An often used strategy consists in combining the output of several systems for a fixed language pair, and to rescore the resulting set of hypotheses taking into account all the available translations and scores. Various to lead to measurable improvements. 2 We plan to experiment next on using predictions at the document level. proposals have been made to efficiently perform such a combination, using auxiliary data structures such as n-best lists, word lattices or consensus networks (see for instance (Kumar and Byrne, 2004; Rosti et al., 2007; Matusov et al., 2008; Hildebrand and Vogel, 2008; Tromble et al., 2008)). Theses techniques have proven extremely effective and have allowed to deliver very significant gains in several recent evaluation campaigns (Callison-Burch et al., 2008). Multisource translation A related, yet more resourceful approach, consists in trying to combine several systems providing translations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by several Foreign→English systems, where output selection is based on t"
C10-1027,N04-1022,0,0.043046,"has been implemented in many different ways which we briefly review here. System combination An often used strategy consists in combining the output of several systems for a fixed language pair, and to rescore the resulting set of hypotheses taking into account all the available translations and scores. Various to lead to measurable improvements. 2 We plan to experiment next on using predictions at the document level. proposals have been made to efficiently perform such a combination, using auxiliary data structures such as n-best lists, word lattices or consensus networks (see for instance (Kumar and Byrne, 2004; Rosti et al., 2007; Matusov et al., 2008; Hildebrand and Vogel, 2008; Tromble et al., 2008)). Theses techniques have proven extremely effective and have allowed to deliver very significant gains in several recent evaluation campaigns (Callison-Burch et al., 2008). Multisource translation A related, yet more resourceful approach, consists in trying to combine several systems providing translations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by"
C10-1027,J06-4004,1,0.865906,"Missing"
C10-1027,max-etal-2010-contrastive,1,0.929341,"nerally produce a search space that differs from that of the direct translation systems. As such, they create a new translation system out of various systems for which diagnosis becomes more difficult. This paper instead focusses on improving a single system, which should be state-of-the-art as regards data and models. We propose a framework in which information coming from external sources is used to boost lexical choices and guide the decoder into making more informed choices.1 1 We performed initial experiments where the complementary information was exploited during n-best list reranking (Max et al., 2010), but except for the multisource condition the list of hypotheses contained too little useful variation 232 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 232–240, Beijing, August 2010 Complementary sources can be of different nature: they can involve other automatic systems (for the same or different language pairs) and/or human knowledge. Furthermore, complementary information is injected at the lexical level, thus making targeted fine-grained lexical predictions useful. Importantly, those predictions are exploited at the sentence level2 ,"
C10-1027,P04-1063,0,0.0322173,"slations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by several Foreign→English systems, where output selection is based on the translation scores. The intuition that if a system assigns a high figure of merits to the translation of a particular sentence, then this translation should be preferred, is implemented in the M AX combination heuristics, whose relative (lack of) success is discussed in (Schwartz, 2008). A similar idea is explored in (Nomoto, 2004), where the sole target language model score is used to rank competing outputs. (Schroeder et al., 2009) propose to combine the available sources prior to translation, under the form of a multilingual lattice, which is decoded with a multisource phrase table. (Chen et al., 2008) integrate the available auxiliary information in a different manner, and discuss how to improve the translation model of the primary system: the idea is to use the entries in the phrase table of the auxiliary system to filter out those accidental correspondences that pollute the main translation model. The most effecti"
C10-1027,2001.mtsummit-papers.46,0,0.187357,"the former and syntactic models for the latter. Another promising approach consists in exploiting complementary sources of information in order to build better translations, as done by consensus-based system combination (e.g. (Matusov et al., 2008)). This, however, requires to François Yvon LIMSI-CNRS Univ. Paris Sud yvon@limsi.fr have several systems available for the same language pair. Considering that the same training data would be available to all systems, differences in translation modelling are expected to produce redundant and complementary hypotheses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of"
C10-1027,N07-1061,0,0.303217,"t and complementary hypotheses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of training data for a given language pair by producing translation hypotheses obtained by pivoting via an intermediary language for which better systems are available. These techniques generally produce a search space that differs from that of the direct translation systems. As such, they create a new translation system out of various systems for which diagnosis becomes more difficult. This paper instead focusses on improving a single system, which should be state-of-the-art as regards data and models. We propose a framework in which informa"
C10-1027,vilar-etal-2006-error,0,0.0375447,"Missing"
C10-1027,P07-1108,0,0.039341,"ses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of training data for a given language pair by producing translation hypotheses obtained by pivoting via an intermediary language for which better systems are available. These techniques generally produce a search space that differs from that of the direct translation systems. As such, they create a new translation system out of various systems for which diagnosis becomes more difficult. This paper instead focusses on improving a single system, which should be state-of-the-art as regards data and models. We propose a framework in which information coming from ext"
C10-1027,N07-1029,0,0.0453162,"Missing"
C10-1027,E09-1082,0,0.137318,"., 2008)). This, however, requires to François Yvon LIMSI-CNRS Univ. Paris Sud yvon@limsi.fr have several systems available for the same language pair. Considering that the same training data would be available to all systems, differences in translation modelling are expected to produce redundant and complementary hypotheses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of training data for a given language pair by producing translation hypotheses obtained by pivoting via an intermediary language for which better systems are available. These techniques generally produce a search space that differs from that of the d"
C10-1027,2008.amta-srw.6,0,0.0501316,"tactic models for the latter. Another promising approach consists in exploiting complementary sources of information in order to build better translations, as done by consensus-based system combination (e.g. (Matusov et al., 2008)). This, however, requires to François Yvon LIMSI-CNRS Univ. Paris Sud yvon@limsi.fr have several systems available for the same language pair. Considering that the same training data would be available to all systems, differences in translation modelling are expected to produce redundant and complementary hypotheses. Multisource translation (e.g. (Och and Ney, 2001; Schwartz, 2008)) is a variant, involving source texts available in several languages which can be translated by systems for different language pairs and whose outputs can be successfully combined into better translations (Schroeder et al., 2009). One theoretical expectation of multisource translation is that it can successfully reduce ambiguity of the original source text, but does so under the rare conditions of availability of existing (accurate) translations. In contrast, pivot-based system combination (e.g. (Utiyama and Isahara, 2007; Wu and Wang, 2007)) aims at compensating the lack of training data for"
C10-1027,2009.mtsummit-papers.14,0,0.0186901,"on hypotheses of a sentence in the target language, we can then boost the likeliness of the words and phrases occurring in these hypotheses by deriving an auxiliary language model for each test sentence. This allows us to integrate this auxiliary information during the search and thus provides a tighter integration with the direct system. This idea has successfully been used in speech recognition, using for instance close captions (Placeway and Lafferty, 1996) or an imperfect translation (Paulik et al., 2005) to provide auxiliary in-domain adaptation data for the recognizer’s language model. (Simard and Isabelle, 2009) proposed a similar approach in Machine Translation in which they use the target-side of an exact match in a translation memory to build language models on a per sentence basis used in their decoder. This strategy can be implemented in a straightforward manner, by simply training a language model using the n-best list as an adaptation corpus. Being automatically generated, hypotheses in the n-best list are not entirely reliable: in particular, they may contain very unlikely target sequences at the junction of two segments. It is however straightforward to filter these out using the available p"
C10-1027,N04-4026,0,0.0131259,"sh system for lexical boosting via triangulation through Spanish 3 Experiments and results 3.1 Translation engine In this study, we used our own machine translation engine, which implements the n-grambased approach to statistical machine translation (Mariño et al., 2006). The translation model is implemented as a stochastic finite-state transducer trained using a n-gram language model of (source,target) pairs. In addition to a bilingual n-gram model, our SMT system uses six additional models which are linearly combined following a discriminative modeling framework: two lexicalized reordering (Tillmann, 2004) models,a target-language model, two lexicon models, a ’weak’ distancebased distortion model, a word bonus model and a translation unit bonus model. Coefficients in this linear combination are tuned over development data with the MERT optimization toolkit4 , slightly modified to use our decoder’s n-best lists. For this study, we used 3-gram bilingual and 3-gram target language models built using modified Kneser-Ney smoothing (Chen and Goodman, 1996); model estimation was performed with the SRI language modeling toolkit.5 Target language 4 http://www.statmt.org/moses http://wwww.speech.sri.com/"
C10-1027,D08-1065,0,0.0285168,"An often used strategy consists in combining the output of several systems for a fixed language pair, and to rescore the resulting set of hypotheses taking into account all the available translations and scores. Various to lead to measurable improvements. 2 We plan to experiment next on using predictions at the document level. proposals have been made to efficiently perform such a combination, using auxiliary data structures such as n-best lists, word lattices or consensus networks (see for instance (Kumar and Byrne, 2004; Rosti et al., 2007; Matusov et al., 2008; Hildebrand and Vogel, 2008; Tromble et al., 2008)). Theses techniques have proven extremely effective and have allowed to deliver very significant gains in several recent evaluation campaigns (Callison-Burch et al., 2008). Multisource translation A related, yet more resourceful approach, consists in trying to combine several systems providing translations from different sources into the same target, provided such multilingual sources are available. (Och and Ney, 2001) propose to select the most promising translation amongst the hypotheses produced by several Foreign→English systems, where output selection is based on the translation scores."
D10-1064,E09-1003,0,0.0215304,"et al., 2009) derived from local contextual information in the training examples.2 These approaches are supported by the study of (Wisniewski et al., 2010) which shows that phrase-based SMT systems are expressive enough to achieve very high translation performance and therefore suggests a better scoring of phrases. The apparent tradeoff between the number of training examples and their appropriateness in each indivual context naturally asks for means of increasing the number of appropriate examples. Exploiting comparable corpora for acquiring translation equivalents (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) offers interesting prospects to this issue, but so far focus has not been so much on context appropriateness as on globally increasing the number of biphrase examples. 2 The study of (Carpuat, 2009) shows that the one translation per discourse hypothesis holds in some cases, but to our knowledge no SMT systems have attempted to exploit it yet. However, in our view, this finding does not contradict the need for estimating translation distributions at the individual phrase level, but they should be integrated as additional information. 656 Proceedings of the 2010 Conference on Empirical Methods"
D10-1064,2010.eamt-1.31,0,0.53094,"ion estimation in Statistical Phrasebased Translation (Koehn et al., 2003) is hampered by the availability of both too many and too few training instances. Recent results on tera-scale SMT (Lopez, 2008) show that access to many training examples1 can lead to significant improvements in translation quality. Also, providing indirect training instances via synonyms or paraphrases for previously unseen phrases can result in gains in translation quality, which are more apparent when little training data is originally available (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Aziz et al., 2010). Although there is a consensus on the importance of using more parallel data in SMT, it has never been formally shown that all additional training instances are actually useful in predicting contextually appropriate translation hypotheses. 1 To be more accurate, works such as that of (Lopez, 2008) have recourse to random sampling to build models of a manageable size in a reasonable amount of time. Attempts at limiting training parallel sentences to those resembling test data through thematic adaptation (Hildebrand et al., 2005) indeed confirm that large quantities of training data cannot comp"
D10-1064,P05-1074,0,0.828491,"e training corpus to estimate reliable translation probabilities in context. In such cases, one might be interested in finding more appropriate examples, which seems at first impossible using the sole original bilingual corpus. We can in fact consider the set of source phrases that have similar translations in context. This set is roughly made up of a subset of what can be referred to as paraphrases. One possible approach to extract local (i.e. phrasal) paraphrases precisely exploits similarity on the target side in another language by extracting source phrases that share common translations (Bannard and Callison-Burch, 2005), but recent approaches have combined this approach with 658 Figure 2: Examples of paraphrases obtained by pivoting via French; values indicate paraphrase probability as defined in (Bannard and Callison-Burch, 2005). similarity computation in the “source” (i.e. original) language (Callison-Burch, 2008; Max, 2008; Kok and Brockett, 2010). Figure 2 provides examples of English paraphrases obtained by automatically pivoting via French. As can be seen, some examples would be clearly useful to better estimate translations of the original source phrase: (Balkan War ↔ war in the Balkans) are syntacti"
D10-1064,2008.iwslt-papers.2,0,0.0758913,"o play to select appro6 The default strategy for most decoders is to copy out-ofvocabulary tokens into the final text. 7 Doing it in conjunction with our approach for improving the translation of known phrases is part of our future work. 659 priate translations among semantically-compatible translations (i.e., target side paraphrases) in the specific context of a generated target hypothesis. Lastly, automatic sentential paraphrasing has also been used in SMT to build alternative reference translations for parameter optimization (Madnani et al., 2008) and to build alternative training corpora (Bond et al., 2008). 3 Towards better exploitation of training corpora in phrase-based SMT In typical phrase-based SMT settings (Koehn et al., 2003), words from the source side of the corpus are first aligned to words on the target side and biphrases are extracted from each training sentence using some heuristics on the word alignments. A source phrase f in a sentence being translated may therefore be aligned to a variety of target phrases. In the example on Figure 3, f is aligned some number of times in the training corpus to target phrases e1 , e2 , e3 and e5 . Using the number of times f is paired with some t"
D10-1064,P05-1032,0,0.34342,"act that errors in automatic word alignment and non literal translations often produce useless biphrases, this results in rare but appropriate translations being very unlikely to be considered during decoding. Some approaches on source context modelling (Carpuat and Wu, 2007; Stroppa et al., 2007; Max et al., 2008; Haque et al., 2009) build classifiers offline for the phrases in a test set, so that context similarity can for example reinforce scores associated with rare but appropriate translations. However, heavy offline computation makes scaling to larger corpora an issue. Other approaches (Callison-Burch et al., 2005; Lopez, 2008) instead focus on accessing very large corpora. Indexing by suffix arrays is used to allow fast access to phrase instances in the corpus, and random sampling to avoid collecting the full set of examples has been shown to perform well. However, these approaches consider all instances of a phrase as equivalent for the estimation of its translations. These works converge on the need for accessing a sufficient number of examples that are relevant for any source phrase in context, fast enough to permit on-the-fly phrase table building. This paper proposes an intermediate step: the ful"
D10-1064,N06-1003,0,0.658275,"ively improving translation performance. 1 Introduction Phrase translation estimation in Statistical Phrasebased Translation (Koehn et al., 2003) is hampered by the availability of both too many and too few training instances. Recent results on tera-scale SMT (Lopez, 2008) show that access to many training examples1 can lead to significant improvements in translation quality. Also, providing indirect training instances via synonyms or paraphrases for previously unseen phrases can result in gains in translation quality, which are more apparent when little training data is originally available (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Aziz et al., 2010). Although there is a consensus on the importance of using more parallel data in SMT, it has never been formally shown that all additional training instances are actually useful in predicting contextually appropriate translation hypotheses. 1 To be more accurate, works such as that of (Lopez, 2008) have recourse to random sampling to build models of a manageable size in a reasonable amount of time. Attempts at limiting training parallel sentences to those resembling test data through thematic adaptation (Hildebrand et al., 2005) ind"
D10-1064,D08-1021,0,0.0709723,"text. This set is roughly made up of a subset of what can be referred to as paraphrases. One possible approach to extract local (i.e. phrasal) paraphrases precisely exploits similarity on the target side in another language by extracting source phrases that share common translations (Bannard and Callison-Burch, 2005), but recent approaches have combined this approach with 658 Figure 2: Examples of paraphrases obtained by pivoting via French; values indicate paraphrase probability as defined in (Bannard and Callison-Burch, 2005). similarity computation in the “source” (i.e. original) language (Callison-Burch, 2008; Max, 2008; Kok and Brockett, 2010). Figure 2 provides examples of English paraphrases obtained by automatically pivoting via French. As can be seen, some examples would be clearly useful to better estimate translations of the original source phrase: (Balkan War ↔ war in the Balkans) are syntactic variants that can generally substitute with each other, (Balkan War ↔ Balkans war) are character-level variants4 . Other examples, however, clearly illustrate the need for validation in context: (Dalai Lama’s ↔ of the Dalai Lama) require different syntactic contexts, and (I don’t see ↔ I do not beli"
D10-1064,2007.mtsummit-papers.11,0,0.340937,"manageable size in a reasonable amount of time. Attempts at limiting training parallel sentences to those resembling test data through thematic adaptation (Hildebrand et al., 2005) indeed confirm that large quantities of training data cannot compensate for the requirement for contextually appropriate training instances. In fact, it is important that phrase translation models adequatly reflect contextual preferences for each phrase occurrence in a text. A variety of recent works have used dynamically adapted translation models, where each phrase occurrence has its own translation distribution (Carpuat and Wu, 2007; Stroppa et al., 2007; Max et al., 2008; Gimpel and Smith, 2008; Haque et al., 2009) derived from local contextual information in the training examples.2 These approaches are supported by the study of (Wisniewski et al., 2010) which shows that phrase-based SMT systems are expressive enough to achieve very high translation performance and therefore suggests a better scoring of phrases. The apparent tradeoff between the number of training examples and their appropriateness in each indivual context naturally asks for means of increasing the number of appropriate examples. Exploiting comparable c"
D10-1064,W09-2404,0,0.0163839,"nough to achieve very high translation performance and therefore suggests a better scoring of phrases. The apparent tradeoff between the number of training examples and their appropriateness in each indivual context naturally asks for means of increasing the number of appropriate examples. Exploiting comparable corpora for acquiring translation equivalents (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) offers interesting prospects to this issue, but so far focus has not been so much on context appropriateness as on globally increasing the number of biphrase examples. 2 The study of (Carpuat, 2009) shows that the one translation per discourse hypothesis holds in some cases, but to our knowledge no SMT systems have attempted to exploit it yet. However, in our view, this finding does not contradict the need for estimating translation distributions at the individual phrase level, but they should be integrated as additional information. 656 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 656–666, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics The approach we take in this article is motivated by the f"
D10-1064,D10-1041,0,0.300698,"ses of the input (Schroeder et al., 2009), the approach we take here can be endogenous with respect to the original training data. It also significantly departs from previous work in that paraphrasing is not simply considered as a way of finding alternative wordings that can be translated given the original training data for out-of-vocabulary phrases only (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Aziz et al., 2010), but as a means to better estimate translations for any possible phrase. Also, as opposed to the work by (Schroeder et al., 2009; Onishi et al., 2010; Du et al., 2010), we do not encode paraphrases into input lattices to have them compete against each other to belong to the source sentential paraphrase that will lead to the highest scoring output sentence3 . Instead, we make use of all contextually appropriate paraphrases of a source phrase, which collectively evaluate the quality of each translation for that phrase. This work can thus be seen as a contribution towards shifting from global phrase translation distributions to contextual translation distributions for contextually equivalent source units. The remainder of this paper is organized as followed. I"
D10-1064,W08-0302,0,0.126977,"miting training parallel sentences to those resembling test data through thematic adaptation (Hildebrand et al., 2005) indeed confirm that large quantities of training data cannot compensate for the requirement for contextually appropriate training instances. In fact, it is important that phrase translation models adequatly reflect contextual preferences for each phrase occurrence in a text. A variety of recent works have used dynamically adapted translation models, where each phrase occurrence has its own translation distribution (Carpuat and Wu, 2007; Stroppa et al., 2007; Max et al., 2008; Gimpel and Smith, 2008; Haque et al., 2009) derived from local contextual information in the training examples.2 These approaches are supported by the study of (Wisniewski et al., 2010) which shows that phrase-based SMT systems are expressive enough to achieve very high translation performance and therefore suggests a better scoring of phrases. The apparent tradeoff between the number of training examples and their appropriateness in each indivual context naturally asks for means of increasing the number of appropriate examples. Exploiting comparable corpora for acquiring translation equivalents (Munteanu and Marcu"
D10-1064,2009.eamt-1.32,0,0.193493,"sentences to those resembling test data through thematic adaptation (Hildebrand et al., 2005) indeed confirm that large quantities of training data cannot compensate for the requirement for contextually appropriate training instances. In fact, it is important that phrase translation models adequatly reflect contextual preferences for each phrase occurrence in a text. A variety of recent works have used dynamically adapted translation models, where each phrase occurrence has its own translation distribution (Carpuat and Wu, 2007; Stroppa et al., 2007; Max et al., 2008; Gimpel and Smith, 2008; Haque et al., 2009) derived from local contextual information in the training examples.2 These approaches are supported by the study of (Wisniewski et al., 2010) which shows that phrase-based SMT systems are expressive enough to achieve very high translation performance and therefore suggests a better scoring of phrases. The apparent tradeoff between the number of training examples and their appropriateness in each indivual context naturally asks for means of increasing the number of appropriate examples. Exploiting comparable corpora for acquiring translation equivalents (Munteanu and Marcu, 2005; Abdul-Rauf an"
D10-1064,2005.eamt-1.19,0,0.0273882,"(Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Aziz et al., 2010). Although there is a consensus on the importance of using more parallel data in SMT, it has never been formally shown that all additional training instances are actually useful in predicting contextually appropriate translation hypotheses. 1 To be more accurate, works such as that of (Lopez, 2008) have recourse to random sampling to build models of a manageable size in a reasonable amount of time. Attempts at limiting training parallel sentences to those resembling test data through thematic adaptation (Hildebrand et al., 2005) indeed confirm that large quantities of training data cannot compensate for the requirement for contextually appropriate training instances. In fact, it is important that phrase translation models adequatly reflect contextual preferences for each phrase occurrence in a text. A variety of recent works have used dynamically adapted translation models, where each phrase occurrence has its own translation distribution (Carpuat and Wu, 2007; Stroppa et al., 2007; Max et al., 2008; Gimpel and Smith, 2008; Haque et al., 2009) derived from local contextual information in the training examples.2 These"
D10-1064,N06-1058,0,0.0327334,"l not only syntactic but also semantic and pragmatic information. 3. Robust translation evaluation: our approach is designed to reinforce estimates for any contextually-appropriate translations of a phrase, as shown by set E on Figure 3. It is therefore important to have some means of accepting them as subparts of valid translations. Robustness in Machine Translation evaluation is an active domain, and potential candidates include using BLEU-like metrics with multiple references, Human-targeted Translation Error Rate (Snover et al., 2006) and the use of paraphrases for reference translations (Kauchak and Barzilay, 2006). 661 In this paper, we want to evaluate whether an endogenous approach for finding paraphrases can lead to some improvement in translation performance. Note that we will not consider in this initial work the possibility of adding new translations to phrases (such as e4 for f on Figure 3) as it adds complexity and should be investigated when the other simpler cases can be handled successfully. In the following section, we describe experiments in which the original bilingual corpus is the only resource used to find potential paraphrases and to estimate phrase translations in context. We chose a"
D10-1064,N03-1017,0,0.0692961,", France aurelien.max@limsi.fr Abstract In this article, an original view on how to improve phrase translation estimates is proposed. This proposal is grounded on two main ideas: first, that appropriate examples of a given phrase should participate more in building its translation distribution; second, that paraphrases can be used to better estimate this distribution. Initial experiments provide evidence of the potential of our approach and its implementation for effectively improving translation performance. 1 Introduction Phrase translation estimation in Statistical Phrasebased Translation (Koehn et al., 2003) is hampered by the availability of both too many and too few training instances. Recent results on tera-scale SMT (Lopez, 2008) show that access to many training examples1 can lead to significant improvements in translation quality. Also, providing indirect training instances via synonyms or paraphrases for previously unseen phrases can result in gains in translation quality, which are more apparent when little training data is originally available (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Aziz et al., 2010). Although there is a consensus on the importance of usi"
D10-1064,N10-1017,0,0.0686424,"of a subset of what can be referred to as paraphrases. One possible approach to extract local (i.e. phrasal) paraphrases precisely exploits similarity on the target side in another language by extracting source phrases that share common translations (Bannard and Callison-Burch, 2005), but recent approaches have combined this approach with 658 Figure 2: Examples of paraphrases obtained by pivoting via French; values indicate paraphrase probability as defined in (Bannard and Callison-Burch, 2005). similarity computation in the “source” (i.e. original) language (Callison-Burch, 2008; Max, 2008; Kok and Brockett, 2010). Figure 2 provides examples of English paraphrases obtained by automatically pivoting via French. As can be seen, some examples would be clearly useful to better estimate translations of the original source phrase: (Balkan War ↔ war in the Balkans) are syntactic variants that can generally substitute with each other, (Balkan War ↔ Balkans war) are character-level variants4 . Other examples, however, clearly illustrate the need for validation in context: (Dalai Lama’s ↔ of the Dalai Lama) require different syntactic contexts, and (I don’t see ↔ I do not believe) are only interchangeable in spe"
D10-1064,C08-1064,0,0.141715,"his proposal is grounded on two main ideas: first, that appropriate examples of a given phrase should participate more in building its translation distribution; second, that paraphrases can be used to better estimate this distribution. Initial experiments provide evidence of the potential of our approach and its implementation for effectively improving translation performance. 1 Introduction Phrase translation estimation in Statistical Phrasebased Translation (Koehn et al., 2003) is hampered by the availability of both too many and too few training instances. Recent results on tera-scale SMT (Lopez, 2008) show that access to many training examples1 can lead to significant improvements in translation quality. Also, providing indirect training instances via synonyms or paraphrases for previously unseen phrases can result in gains in translation quality, which are more apparent when little training data is originally available (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Aziz et al., 2010). Although there is a consensus on the importance of using more parallel data in SMT, it has never been formally shown that all additional training instances are actually useful in pre"
D10-1064,J10-3003,0,0.0781879,"lation distribution, C(pk ) the context of a particular example of pk in the training corpus, simpara a function indicating the contextual similarity between a phrase context and a paraphrase context, and ej is any possible translation of f . Several requirements can be drawn from the previous description: 1. List of potential paraphrases: some mechanism for finding potential paraphrases for source phrases is required, and several such mechanisms could be combined. Pivoting via bilingual corpora, a natural strategy given the issue at hand, is just one among many different proposed strategies (Madnani and Dorr, 2010). 2. Contextual similarity measure: a similarity measure between the contexts of two phrases or two potential local paraphrases is required. This automatic measure should ideally be able to model not only syntactic but also semantic and pragmatic information. 3. Robust translation evaluation: our approach is designed to reinforce estimates for any contextually-appropriate translations of a phrase, as shown by set E on Figure 3. It is therefore important to have some means of accepting them as subparts of valid translations. Robustness in Machine Translation evaluation is an active domain, and"
D10-1064,2008.amta-papers.13,0,0.178104,"target language model nevertheless still has an important role to play to select appro6 The default strategy for most decoders is to copy out-ofvocabulary tokens into the final text. 7 Doing it in conjunction with our approach for improving the translation of known phrases is part of our future work. 659 priate translations among semantically-compatible translations (i.e., target side paraphrases) in the specific context of a generated target hypothesis. Lastly, automatic sentential paraphrasing has also been used in SMT to build alternative reference translations for parameter optimization (Madnani et al., 2008) and to build alternative training corpora (Bond et al., 2008). 3 Towards better exploitation of training corpora in phrase-based SMT In typical phrase-based SMT settings (Koehn et al., 2003), words from the source side of the corpus are first aligned to words on the target side and biphrases are extracted from each training sentence using some heuristics on the word alignments. A source phrase f in a sentence being translated may therefore be aligned to a variety of target phrases. In the example on Figure 3, f is aligned some number of times in the training corpus to target phrases e1 , e2 ,"
D10-1064,D09-1040,0,0.535654,"erformance. 1 Introduction Phrase translation estimation in Statistical Phrasebased Translation (Koehn et al., 2003) is hampered by the availability of both too many and too few training instances. Recent results on tera-scale SMT (Lopez, 2008) show that access to many training examples1 can lead to significant improvements in translation quality. Also, providing indirect training instances via synonyms or paraphrases for previously unseen phrases can result in gains in translation quality, which are more apparent when little training data is originally available (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Aziz et al., 2010). Although there is a consensus on the importance of using more parallel data in SMT, it has never been formally shown that all additional training instances are actually useful in predicting contextually appropriate translation hypotheses. 1 To be more accurate, works such as that of (Lopez, 2008) have recourse to random sampling to build models of a manageable size in a reasonable amount of time. Attempts at limiting training parallel sentences to those resembling test data through thematic adaptation (Hildebrand et al., 2005) indeed confirm that larg"
D10-1064,2008.eamt-1.17,1,0.941937,"me. Attempts at limiting training parallel sentences to those resembling test data through thematic adaptation (Hildebrand et al., 2005) indeed confirm that large quantities of training data cannot compensate for the requirement for contextually appropriate training instances. In fact, it is important that phrase translation models adequatly reflect contextual preferences for each phrase occurrence in a text. A variety of recent works have used dynamically adapted translation models, where each phrase occurrence has its own translation distribution (Carpuat and Wu, 2007; Stroppa et al., 2007; Max et al., 2008; Gimpel and Smith, 2008; Haque et al., 2009) derived from local contextual information in the training examples.2 These approaches are supported by the study of (Wisniewski et al., 2010) which shows that phrase-based SMT systems are expressive enough to achieve very high translation performance and therefore suggests a better scoring of phrases. The apparent tradeoff between the number of training examples and their appropriateness in each indivual context naturally asks for means of increasing the number of appropriate examples. Exploiting comparable corpora for acquiring translation equival"
D10-1064,max-etal-2010-contrastive,1,0.781688,"he second threshold was selected to reduce computation time. 663 phrase length 1 2 3 4 5 # phrases en fr 13,620 15,707 13,120 15,207 12,620 14,707 12,120 14,208 11,623 13,711 # paraphrased en fr 458 725 4,127 4,481 4,782 5,715 2,859 4,078 1,171 2,275 # paraphrases en fr 1,824 2,684 18,871 19,700 24,111 27,377 15,071 20,345 6,077 12,132 Figure 7: Statistics on numbers of phrases, numbers of paraphrased phrases and numbers of paraphrases per phrase length. to be important as regards information content in translation. We applied the contrastive lexical evaluation (CLE) methodology described in (Max et al., 2010), which indicates how many times source words grouped into user-defined classes were correctly translated or not across systems. These additional results are reported on Figure 9. On English to French translation, both additional features lead to improvements over the baseline with all metrics, including CLE, and their combination shows a strong improvement in TER (-1.55). CLE on content words reveals that the para feature seems particularly effective in reducing the number of words in all categories that only the baseline system translated correctly. Results on French to English translation a"
D10-1064,P09-1089,0,0.318438,"ction Phrase translation estimation in Statistical Phrasebased Translation (Koehn et al., 2003) is hampered by the availability of both too many and too few training instances. Recent results on tera-scale SMT (Lopez, 2008) show that access to many training examples1 can lead to significant improvements in translation quality. Also, providing indirect training instances via synonyms or paraphrases for previously unseen phrases can result in gains in translation quality, which are more apparent when little training data is originally available (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Aziz et al., 2010). Although there is a consensus on the importance of using more parallel data in SMT, it has never been formally shown that all additional training instances are actually useful in predicting contextually appropriate translation hypotheses. 1 To be more accurate, works such as that of (Lopez, 2008) have recourse to random sampling to build models of a manageable size in a reasonable amount of time. Attempts at limiting training parallel sentences to those resembling test data through thematic adaptation (Hildebrand et al., 2005) indeed confirm that large quantities of train"
D10-1064,W07-0737,0,0.0293977,"raphrase than not to translate a given phrase.6 In contrast, the work by (Mirkin et al., 2009) attempts to model context when using replacements for words (synonyms or hypernyms). The natural next step that we take here is to exploit the complementarity of the original bilingual training corpus for finding paraphrases and the monolingual (source) side of the same corpus for validating them in context. Furthermore, our focus here is not on paraphrasing unseen phrases7 , but possibly any phrase, or any phrase seen less than a given number of times, or any types of difficult-totranslate phrases (Mohit and Hwa, 2007). The recent work of (Resnik et al., 2010) uses crowdsourcing to obtain paraphrases for source phrases corresponding to mistranslated target phrases. The spotting of the incorrect target phrases and the paraphrasing of the source phrases can be automated. Promising oracle figures are obtained, validating the claim that some variations of the input sentence might be more easily translated than others by a given system. Paraphrases have also been used to represent alternative inputs encoded in lattices using existing (Schroeder et al., 2009) or automatically built paraphrases (Onishi et al., 201"
D10-1064,J05-4003,0,0.0177379,"el and Smith, 2008; Haque et al., 2009) derived from local contextual information in the training examples.2 These approaches are supported by the study of (Wisniewski et al., 2010) which shows that phrase-based SMT systems are expressive enough to achieve very high translation performance and therefore suggests a better scoring of phrases. The apparent tradeoff between the number of training examples and their appropriateness in each indivual context naturally asks for means of increasing the number of appropriate examples. Exploiting comparable corpora for acquiring translation equivalents (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) offers interesting prospects to this issue, but so far focus has not been so much on context appropriateness as on globally increasing the number of biphrase examples. 2 The study of (Carpuat, 2009) shows that the one translation per discourse hypothesis holds in some cases, but to our knowledge no SMT systems have attempted to exploit it yet. However, in our view, this finding does not contradict the need for estimating translation distributions at the individual phrase level, but they should be integrated as additional information. 656 Proceedings of the 2010"
D10-1064,P10-2001,0,0.190069,"r sentential paraphrases of the input (Schroeder et al., 2009), the approach we take here can be endogenous with respect to the original training data. It also significantly departs from previous work in that paraphrasing is not simply considered as a way of finding alternative wordings that can be translated given the original training data for out-of-vocabulary phrases only (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Aziz et al., 2010), but as a means to better estimate translations for any possible phrase. Also, as opposed to the work by (Schroeder et al., 2009; Onishi et al., 2010; Du et al., 2010), we do not encode paraphrases into input lattices to have them compete against each other to belong to the source sentential paraphrase that will lead to the highest scoring output sentence3 . Instead, we make use of all contextually appropriate paraphrases of a source phrase, which collectively evaluate the quality of each translation for that phrase. This work can thus be seen as a contribution towards shifting from global phrase translation distributions to contextual translation distributions for contextually equivalent source units. The remainder of this paper is organi"
D10-1064,D10-1013,0,0.390739,"f one of the phrases could provide larger quantities of training data for translating the other. In other words, we hypothesize that there may be more training data to learn a phrase’s translations in a bilingual corpus than what SMT approaches typically use. In contrast to previous attempts at using paraphrases to improve Statistical Machine Translation, which require external data in the form of additional parallel bilingual corpora (Callison-Burch et al., 2006), monolingual corpora (Marton et al., 2009), lexico-semantic resources (Mirkin et al., 2009; Aziz et al., 2010), or sub-sentential (Resnik et al., 2010) or sentential paraphrases of the input (Schroeder et al., 2009), the approach we take here can be endogenous with respect to the original training data. It also significantly departs from previous work in that paraphrasing is not simply considered as a way of finding alternative wordings that can be translated given the original training data for out-of-vocabulary phrases only (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Aziz et al., 2010), but as a means to better estimate translations for any possible phrase. Also, as opposed to the work by (Schroeder et al., 2009"
D10-1064,E09-1082,0,0.409429,"ing data for translating the other. In other words, we hypothesize that there may be more training data to learn a phrase’s translations in a bilingual corpus than what SMT approaches typically use. In contrast to previous attempts at using paraphrases to improve Statistical Machine Translation, which require external data in the form of additional parallel bilingual corpora (Callison-Burch et al., 2006), monolingual corpora (Marton et al., 2009), lexico-semantic resources (Mirkin et al., 2009; Aziz et al., 2010), or sub-sentential (Resnik et al., 2010) or sentential paraphrases of the input (Schroeder et al., 2009), the approach we take here can be endogenous with respect to the original training data. It also significantly departs from previous work in that paraphrasing is not simply considered as a way of finding alternative wordings that can be translated given the original training data for out-of-vocabulary phrases only (Callison-Burch et al., 2006; Marton et al., 2009; Mirkin et al., 2009; Aziz et al., 2010), but as a means to better estimate translations for any possible phrase. Also, as opposed to the work by (Schroeder et al., 2009; Onishi et al., 2010; Du et al., 2010), we do not encode paraph"
D10-1064,2006.amta-papers.25,0,0.0624565,"raphrases is required. This automatic measure should ideally be able to model not only syntactic but also semantic and pragmatic information. 3. Robust translation evaluation: our approach is designed to reinforce estimates for any contextually-appropriate translations of a phrase, as shown by set E on Figure 3. It is therefore important to have some means of accepting them as subparts of valid translations. Robustness in Machine Translation evaluation is an active domain, and potential candidates include using BLEU-like metrics with multiple references, Human-targeted Translation Error Rate (Snover et al., 2006) and the use of paraphrases for reference translations (Kauchak and Barzilay, 2006). 661 In this paper, we want to evaluate whether an endogenous approach for finding paraphrases can lead to some improvement in translation performance. Note that we will not consider in this initial work the possibility of adding new translations to phrases (such as e4 for f on Figure 3) as it adds complexity and should be investigated when the other simpler cases can be handled successfully. In the following section, we describe experiments in which the original bilingual corpus is the only resource used to fi"
D10-1064,2007.tmi-papers.28,0,0.0615317,"Missing"
D10-1064,D10-1091,0,0.027251,"aining data cannot compensate for the requirement for contextually appropriate training instances. In fact, it is important that phrase translation models adequatly reflect contextual preferences for each phrase occurrence in a text. A variety of recent works have used dynamically adapted translation models, where each phrase occurrence has its own translation distribution (Carpuat and Wu, 2007; Stroppa et al., 2007; Max et al., 2008; Gimpel and Smith, 2008; Haque et al., 2009) derived from local contextual information in the training examples.2 These approaches are supported by the study of (Wisniewski et al., 2010) which shows that phrase-based SMT systems are expressive enough to achieve very high translation performance and therefore suggests a better scoring of phrases. The apparent tradeoff between the number of training examples and their appropriateness in each indivual context naturally asks for means of increasing the number of appropriate examples. Exploiting comparable corpora for acquiring translation equivalents (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) offers interesting prospects to this issue, but so far focus has not been so much on context appropriateness as on globally i"
D10-1064,2010.amta-workshop.3,0,\N,Missing
D12-1066,P05-1074,0,0.234533,"f a pair are extracted from the other sentence, and the intersection of the sets for both directions is kept. Edit rate on word sequences (T ERp ) The T ERp tool (Snover et al., 2010) can be used to compute an optimal set of word and phrase edits that can transform one sentence into another one.9 Edit types are parameterized by one or more weights which were optimized towards F-measure by hill climbing with 100 random restarts using the held-out data set consisting of 125 sentence pairs for each corpus type. Translational equivalence (P IVOT) We exploited the paraphrase probability defined by Bannard and Callison-Burch (2005) on bilingual parallel corpora. We used the Europarl corpus10 of parliamentary debates in English and French, consisting of approximately 1.7 million parallel sentences, using each language as source and pivot in turn. G IZA ++ 9 Note that contrarily to what T ERp allows, we did not used the possibility of using word or phrase equivalents as those are only made available for English. This type of knowledge is however captured in part by the FASTR and P IVOT systems. 10 http://statmt.org/europarl Phrase pair features – edit distance between paraphrases, stem identity, bag-of-tokens similarity,"
D12-1066,W03-1004,0,0.0363564,"ally, there are substantially more positive paraphrase examples for French (19,427) than for English (12,593). 4 Related work Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study i"
D12-1066,P01-1008,0,0.118027,"summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. Faruqui and Pad´o (2011) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for"
D12-1066,W08-0906,0,0.0143987,"type contains more than half of the total number of examples for the two languages. Finally, there are substantially more positive paraphrase examples for French (19,427) than for English (12,593). 4 Related work Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-B"
D12-1066,P08-1077,0,0.0228721,"Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. Faruqui and Pad´o (2011) study the acquisition"
D12-1066,E12-1073,1,0.841294,"e pair candidates that include possible reference paraphrases will not penalize precision while not increasing recall. All performance values reported in the following sections will be obtained using 10-fold crossvalidation and averaging the results on each sub-test. All data sets of cross-validation contain 500 sentence pairs per corpus type, and 125 pairs are kept for development. 3.2 A framework for sub-sentential paraphrase identification We now describe the systems that will be tested on the various corpora described in section 2 using the methodology described in section 3.1. Following (Bouamor et al., 2012), a combination system is used to automatically weight paraphrase pair candidates produced by individual systems using a set of features aiming at recognizing paraphrases, as illustrated on Figure 3. Four individual systems have been used and are described below: the reasons for considering those systems include their free avail725 Statistical learning of word alignments (G IZA) The G IZA ++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. It was run on each monolingual corpus of sentence pairs in both directions, symmetrized a"
D12-1066,C08-1013,0,0.0746642,"using such types of paraphrases into applications would however often be too strongly context-dependent. 724 TEXT SPEECH SCENE 70 EVENT 60 60 50 50 40 40 30 30 20 20 10 10 0 COSINE*100 BLEU 1-TER METEOR 0 COSINE*100 BLEU 1-TER METEOR Figure 2: Sentence pair average similarities for all corpora for English (left) and French (right) using the cosine of token vectors, BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). 3 3.1 Bilingual experiments across corpus types Evaluation of paraphrase acquisition We followed the PARAMETRIC methodology described in (Callison-Burch et al., 2008) for assessing the performance of systems on the task of subsentential paraphrase acquisition. In this methodology, a set of paraphrase candidates extracted from a sentence pair is compared with a set of reference paraphrases, obtained through human annotation, by computing usual measures of precision (P ) and recall (R). The first value corresponds to the proportion of paraphrase candidates, denoted H, produced by a system and that are correct relative to the reference set containing sure and possible paraphrases, denoted Rall . Recall is obtained by measuring the proportion of the reference"
D12-1066,P11-1020,0,0.141813,"escribed in (Tiedemann, 2007), based on time frames and developed for bilingual subtitles, we then filtered out sentence pairs below a minimal edit distance threshold, and manually removed obvious errors made by the algorithm. a boy rides a bike on a dirt road . So he uses the photo booths to remind people what he looks like . e.g. So he uses the photo booths to remind people what he looks like. ↔ He uses those machines to remind the living of his face. a boy is riding on a bicycle fast . Pigeons have numerical abilities just like primates S CENE We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. Similarly to what we did for T EXT, we selected sentence pairs from clusters by minimal edit distance above a threshold. An important fact is that for English we were able to use what is described as “verified” descriptions. There were, however, far fewer descriptions available for French, and none had the “verified” status. We decided to use this corpus nonetheless, but with the knowledge that this source for French is of a substantially lower quality (this corpus type will therefore appear as “(S CENE)” in all tables to reflect this). Pig"
D12-1066,J08-4005,0,0.137572,"d alignment matrices on Figure 1. A corpus for each type has been collected for 2 languages, English and French, and comprises 625 sentence pairs per language. We now briefly describe how each corpus was built. 721 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 721–731, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics It is anticipated that the annual total foreign trade volume will exceed US$9 billion . T EXT For English, we used the MTC corpus1 (described in (Cohn et al., 2008)) consisting of sets of news article translations from Chinese, and for French the CESTA corpus2 consisting of sets of news article translations from English. For each sentence cluster, we selected sentence pairs with minimal edit distance above an empirically-selected threshold, covering all clusters first and then selecting from already used clusters to reach the target number of sentence pairs. It is estimated that the total annual volume of import and export will exceed 9 billion US dollars . He uses those machines to remind the living of his face . e.g. It is estimated that the total annu"
D12-1066,C04-1051,0,0.0997121,"act that this corpus type contains more than half of the total number of examples for the two languages. Finally, there are substantially more positive paraphrase examples for French (19,427) than for English (12,593). 4 Related work Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in ("
D12-1066,W11-0111,0,0.0659731,"Missing"
D12-1066,C04-1151,0,0.0143876,"y more positive paraphrase examples for French (19,427) than for English (12,593). 4 Related work Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisitio"
D12-1066,P08-4006,0,0.0272049,"dates differed from more than one day. We repeated the same selection procedure as for T EXT and S CENE to have a maximal cluster coverage and select more similar pairs first. e.g. Pigeons Have an Understanding of Numbers on Par With Primates ↔ Pigeons Have Numerical Abilities Just Like Primates Table 1 provides various statistics for these corpora. The first observation is that T EXT contains significantly larger sentences than the other types, more than twice as long as those of S PEECH. Annotation was performed following the guidelines proposed by Cohn et al. (2008)5 using the YAWAT tool (Germann, 2008), except that alignments where not initially obtained automatically so as not to bias our annotators’ work (there were two annotators per language). The main guidelines that they had to follow were that sure and possible paraphrases must be distinguished, smaller alignments were to be prefered but any-to-any alignments may be used, and sentences should be aligned as much as possible. Henceforth, we will only consider for all reported statistics and experiments those paraphrases that are not identity pairs (e.g. (a nice day ↔ a nice day)), as they are 4 http://news.google.com See http://staffww"
D12-1066,I11-1090,0,0.0115185,"to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. However, their low availability makes searching for less parallel corpora a necessity. In this study, we have attempted to identify corpora of various degrees of semantic textual simila"
D12-1066,P99-1044,0,0.00825571,"ividual systems have been used and are described below: the reasons for considering those systems include their free avail725 Statistical learning of word alignments (G IZA) The G IZA ++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. It was run on each monolingual corpus of sentence pairs in both directions, symmetrized alignments were kept and classical phrase extraction heuristics were applied (Koehn et al., 2003), without growing phrases with unaligned tokens. Linguistic knowledge on term variation (FASTR) The FASTR tool (Jacquemin, 1999) spots term variants in large corpora, where variants are described through metarules expressing how the morphosyntactic structure of a term variant can be derived from a given term by means of regular expressions on morphosyntactic categories. Paradigmatic variation can also be expressed with constraints between words, imposing that they be of the same morphological or semantic family using existing resources available in our two languages. Variants for all phrases from one sentence of a pair are extracted from the other sentence, and the intersection of the sets for both directions is kept."
D12-1066,N03-1017,0,0.00378411,"ates produced by individual systems using a set of features aiming at recognizing paraphrases, as illustrated on Figure 3. Four individual systems have been used and are described below: the reasons for considering those systems include their free avail725 Statistical learning of word alignments (G IZA) The G IZA ++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. It was run on each monolingual corpus of sentence pairs in both directions, symmetrized alignments were kept and classical phrase extraction heuristics were applied (Koehn et al., 2003), without growing phrases with unaligned tokens. Linguistic knowledge on term variation (FASTR) The FASTR tool (Jacquemin, 1999) spots term variants in large corpora, where variants are described through metarules expressing how the morphosyntactic structure of a term variant can be derived from a given term by means of regular expressions on morphosyntactic categories. Paradigmatic variation can also be expressed with constraints between words, imposing that they be of the same morphological or semantic family using existing resources available in our two languages. Variants for all phrases f"
D12-1066,P07-2045,0,0.00433743,"milarity of token context vectors for each phrase of a paraphrase (derived from counts in the large English-French parallel corpus from WMT’11 (http://www.statmt.org/ wmt11/translation-task.html) (approx. 30 million parallel sentences) System features – combination of the individual systems that proposed the paraphrase pair Table 3: Features used by our classifiers. Discretized intervals based on median values are used for real values, and binarized values are used for combinations. was used for word alignment and phrase translation probabilities were estimated from them by the M OSES system (Koehn et al., 2007). For each phrase of a sentence pair, we built its set of paraphrases, and extracted its paraphrase from the other sentence with highest probability. We repeated this process in both directions, and finally kept for each phrase its paraphrase pair from any direction with highest probability. Automatic validation of candidate paraphrases Taking the union of all paraphrase pair candidates from all the above systems for each sentence pair, we perform a Maximum Entropy two-class classification11 , which allows us to include features that were not necessarily exploited or straightforward to exploit"
D12-1066,N10-1017,0,0.0142172,"paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. Faruqui and Pad´o (2011) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for one language. Although their work is not directly comparable to ours, they report that robustness across domains i"
D12-1066,W07-0734,0,0.0712034,"Missing"
D12-1066,D10-1090,0,0.012901,"al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. Faruqui and Pad´o (2011) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for one language. Although their work is not directly comparable to ours, they report that robustness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sen"
D12-1066,J10-3003,0,0.11718,"escribes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. A detailed quantified typology of subsentential paraphrases found in our corpus types is given. 1 Introduction Sub-sentential paraphrases can be acquired from text pairs expressing the same meaning (Madnani and Dorr, 2010). If the semantic similarity of a text pair has a direct impact on the quality of the acquired paraphrases, it has, to our knowledge, never been shown what impact the type of original signal has on paraphrase acquisition. In this work, we consider four types of corpora, which we think are representative of the main types of original semantic signals: text pairs (roughly, sentences) originating a) from independent translations of a text (T EXT), b) from independent translations of a speech (S PEECH), c) from independent descriptions of a visual scene (S CENE), and d) from independent descriptio"
D12-1066,2008.amta-papers.13,0,0.0116239,"hey report that robustness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. However, their low availability makes searching for less parallel corpora a necessity. In this study, we have attempted to identify cor"
D12-1066,D09-1040,0,0.0290815,"ness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. However, their low availability makes searching for less parallel corpora a necessity. In this study, we have attempted to identify corpora of various degre"
D12-1066,D10-1064,1,0.833483,"s difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. However, their low availability makes searching for less parallel corpora a necessity. In this study, we have attempted to identify corpora of various degrees of seman"
D12-1066,P11-2096,0,0.0136934,". To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. Faruqui and Pad´o (2011) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for one language. Although their work is not directly comparable to ours, they report that robustness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pair"
D12-1066,E06-1021,0,0.0169469,"ase examples for French (19,427) than for English (12,593). 4 Related work Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several c"
D12-1066,J04-4002,0,0.00874287,"araphrase identification We now describe the systems that will be tested on the various corpora described in section 2 using the methodology described in section 3.1. Following (Bouamor et al., 2012), a combination system is used to automatically weight paraphrase pair candidates produced by individual systems using a set of features aiming at recognizing paraphrases, as illustrated on Figure 3. Four individual systems have been used and are described below: the reasons for considering those systems include their free avail725 Statistical learning of word alignments (G IZA) The G IZA ++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. It was run on each monolingual corpus of sentence pairs in both directions, symmetrized alignments were kept and classical phrase extraction heuristics were applied (Koehn et al., 2003), without growing phrases with unaligned tokens. Linguistic knowledge on term variation (FASTR) The FASTR tool (Jacquemin, 1999) spots term variants in large corpora, where variants are described through metarules expressing how the morphosyntactic structure of a term variant can be derived from a given term by means of r"
D12-1066,N03-1024,0,0.0385101,"d Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. Faruqui and Pad´o (2011) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for one language. Altho"
D12-1066,P02-1040,0,0.0840745,"Missing"
D12-1066,I05-1011,0,0.0215987,"Missing"
D12-1066,D10-1013,0,0.0152712,"ge. Although their work is not directly comparable to ours, they report that robustness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. However, their low availability makes searching for less parallel corpora"
D12-1066,E09-1082,0,0.0134125,"r corpora for one language. Although their work is not directly comparable to ours, they report that robustness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. However, their low availability makes searching for"
D12-1066,2006.amta-papers.25,0,0.0293574,"Missing"
D12-1066,W09-0621,0,0.0487281,"Missing"
D12-1066,P09-1094,0,0.0240288,"Pad´o (2011) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for one language. Although their work is not directly comparable to ours, they report that robustness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, mon"
D12-1066,S12-1051,0,\N,Missing
D12-1066,2010.amta-workshop.3,0,\N,Missing
D14-1133,P11-1022,0,0.21063,"itial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue of word to n-gram confidence estimation in SMT output (Zens and Ney, 2006; Ueffing and Ney, 2007; Bach et al., 2011; de Gispert et al., 2013), and some attempts have been made to exploit confidence estimates for lattice rescoring (Blackwood et al., 2010) or n-best reranking (Bach et al., 2011; Luong et al., 2014b). In this work, we present an approach in which 1261 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261–1272, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics new complete hypotheses are produced by rewriting existing hypotheses, and are scored using complex models that could not be used during the initial de"
D14-1133,C10-1009,0,0.0405529,"Missing"
D14-1133,C04-1046,0,0.0490447,"analysis for the first 70 test sentences. System BLEU fr-en TER BLEU en-fr TER 1-pass Moses reranker 32.5 33.0 47.7 47.3 32.3 32.8 49.9 49.4 rewriter semi-oracle 33.4(+0.4) 34.1(+1.1) 47.4(+0.1) 46.6(−0.7) 33.7(+0.9) 34.2(+1.4) 49.3(−0.1) 48.6(−0.8) Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks. 5 Related work Reranking of translation hypotheses n-best list reranking was extensively studied in (Och et al., 2004), using features not used in the initial decoder such as IBM1 scores (which also proved useful for word-level confidence estimation (Blatz et al., 2004)) and generative syntactic models. While the experiments in (Och et al., 2004) did not show any clear contribution of syntactic information used in this manner, the later work by Carter and Monz (2011) managed to successfully exploit syntactic features using discriminative language modeling for n-best reranking. Gimpel et al. (2013) outperformed n-best reranking by generating, with an expensive but simple method, diverse hypotheses used as training data. Recently, Luong et al. (2014b) reranked n-best lists using confidence scores at the hypothesis level computed from word-level confidence meas"
D14-1133,D13-1111,0,0.0808,"the TED Talks. 5 Related work Reranking of translation hypotheses n-best list reranking was extensively studied in (Och et al., 2004), using features not used in the initial decoder such as IBM1 scores (which also proved useful for word-level confidence estimation (Blatz et al., 2004)) and generative syntactic models. While the experiments in (Och et al., 2004) did not show any clear contribution of syntactic information used in this manner, the later work by Carter and Monz (2011) managed to successfully exploit syntactic features using discriminative language modeling for n-best reranking. Gimpel et al. (2013) outperformed n-best reranking by generating, with an expensive but simple method, diverse hypotheses used as training data. Recently, Luong et al. (2014b) reranked n-best lists using confidence scores at the hypothesis level computed from word-level confidence measures learnt from roughly 10,000 SMT system outputs annotated by humans. Rewriting of translation hypotheses Langlais et al. (2007) described a greedy search decoder, first introduced in (Germann et al., 2001), able to improve translations produced by a dynamic programming decoder using the same scoring function and translation table"
D14-1133,D12-1108,0,0.228647,"This is addressed in numerous works on reranking of the highest scored sub-space of hypotheses, on so-called n-best lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post-editing may seem to be too limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rew"
D14-1133,N09-2005,0,0.0196802,"ance and potential for improvement of our approach in spite of its simplicity. 1 Introduction The standard configuration of modern phrasebased Statistical Machine Translation (SMT) (Koehn et al., 2003) systems can produce very acceptable results on some tasks. However, early integration of better features to guide the search for the best hypothesis can result in significant improvements, an expression of the complexity of modeling translation quality. For instance, improvements have been obtained by integrating features into decoding that better model semantic coherence at the sentence level (Hasan and Ney, 2009) or syntactic well-formedness (Schwartz et Aur´elien Max LIMSI-CNRS, Orsay, France Univ. Paris Sud, Orsay, France aurelien.max@limsi.fr al., 2011). However, early use of such complex features typically comes at a high computational cost. Moreover, some informative features require or are better computed when complete translation hypotheses are available. This is addressed in numerous works on reranking of the highest scored sub-space of hypotheses, on so-called n-best lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al.,"
D14-1133,2007.mtsummit-ucnlg.1,0,0.0395328,"f such complex features typically comes at a high computational cost. Moreover, some informative features require or are better computed when complete translation hypotheses are available. This is addressed in numerous works on reranking of the highest scored sub-space of hypotheses, on so-called n-best lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post-editing may seem to be too limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy"
D14-1133,N03-1017,0,0.0763928,", we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function, corresponding to a 5.4 BLEU improvement over the original Moses baseline. We show that if an indication of which phrases require rewriting is provided, our automatic rewriting procedure yields an additional improvement of 1.5 BLEU. Various analyses, including a manual error analysis, further illustrate the good performance and potential for improvement of our approach in spite of its simplicity. 1 Introduction The standard configuration of modern phrasebased Statistical Machine Translation (SMT) (Koehn et al., 2003) systems can produce very acceptable results on some tasks. However, early integration of better features to guide the search for the best hypothesis can result in significant improvements, an expression of the complexity of modeling translation quality. For instance, improvements have been obtained by integrating features into decoding that better model semantic coherence at the sentence level (Hasan and Ney, 2009) or syntactic well-formedness (Schwartz et Aur´elien Max LIMSI-CNRS, Orsay, France Univ. Paris Sud, Orsay, France aurelien.max@limsi.fr al., 2011). However, early use of such comple"
D14-1133,2007.tmi-papers.13,0,0.654646,"otheses are available. This is addressed in numerous works on reranking of the highest scored sub-space of hypotheses, on so-called n-best lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post-editing may seem to be too limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is t"
D14-1133,N12-1005,0,0.0595506,"assigned by a baseline decoder (denoted as 1-pass Moses henceforth) to sentence pair (e, f ). We use the following posterior probability for α: P • SOUL models: SOUL models are structured output layer neural network language models (LMs) which have been shown to be useful in reranking tasks, for instance for WMT evaluations (Allauzen et al., 2013; P´echeux et al., 2014). SOUL scoring being too costly to be integrated during decoding, it fits perfectly the reranker scenario, which furthermore enables to use larger contexts for n-grams. We used both monolingual (Le et al., 2011) and bilingual (Le et al., 2012) SOUL 10-gram models, which were trained on the WMT’12 data. • POS language model: part-of-speech (POS) LMs have been shown to yield improvements in n-best list reranking (Carter and Monz, 2011). In this work, we trained a 6-gram POS LM using Witten-Bell smoothing. • IBM1 : the IBM1 scores (p(e|f ) and p(f |e)) of the complete hypothesis (Och et al., 2004). • phrase-based confidence score : bi-phrases are associated to a posterior probability, inspired from n-gram posterior probability estimation as defined in (de Gispert et al., 2013). Let E be the set of all hypotheses in the space of transl"
D14-1133,N12-1047,0,0.0246412,"l., 2004). • phrase-based confidence score : bi-phrases are associated to a posterior probability, inspired from n-gram posterior probability estimation as defined in (de Gispert et al., 2013). Let E be the set of all hypotheses in the space of translation hypotheses defined by 2 Note that we did not try to explore the independant contribution of each feature in this work. (1) Then, the logarithms of each phrase’s confidence score are summed to use as a confidence score for the complete hypothesis. 2.3 The rerankings of the hypotheses sets describe in this work are all performed with kb-mira (Cherry and Foster, 2012) using the initial features set of the decoder in conjunction with the following additional features:2 exp(H(e0 , f )) 00 e00 ∈E exp(H(e , f )) 0 P (α|F ) = Pe ∈Eα Rewriting phrase table Taking the whole translation table of the decoder as a rewriting phrase table to perform the greedy search produces very large neighborhoods that rewriter cannot handle due to the cost of the models that have to be computed. We tried two different approaches to extract a rewriting phrase table from the translation table of the system. We first tried a naive approach where the rewriting phrase table of rewriter"
D14-1133,C04-1072,0,0.0851278,"Missing"
D14-1133,C10-1027,1,0.825547,"r phrase is correct or not, our rewriter system could be used to determine automatically whether a rewriting system could (if asked to) attempt to improve locally a translation, or whether a human post-editor should already tackle working on improving it. As we showed in our manual error analysis in section 4.4, there are in fact many instances of errors that could not be recovered by our approach, be it because of its local rewriting strategy or of the bilingual resources or models used, so that some knowledge would have to be provided as hard constraints by a human translator, as hinted in (Crego et al., 2010). We could then finally have our rewriter system work in a turn-based fashion in collaboration with a human translator, fixing errors or making improvements that are being made possible by the last edits from the translator. Acknowledgments The authors would like to thank the anonymous reviewers and Guillaume Wisniewski for their useful remarks. Additional thanks go to Hai Son Le for “anticipating” the need for a large and efficient cache in his SOUL implementation, Quoc Khanh Do for his assistance on using SOUL, and Li Gong and Nicolas P´echeux for providing the authors with data used in the"
D14-1133,2014.eamt-1.23,0,0.152854,"potheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue of word to n-gram confidence estimation in SMT output (Zens and Ney, 2006; Ueffing and Ney, 2007; Bach et al., 2011; de Gispert et al., 2013), and some attempts have been made to exploit confidence estimates for lattice rescoring (Blackwood et al., 2010) or n-best reranking (Bach et al., 2011; Luong et al., 2014b). In this work, we present an approach in which 1261 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261–1272, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics new complete hypotheses are produced by rewriting existing hypotheses, and are scored using complex models that could not be used during the initial decoding. We will use as competitive baselines systems that rerank the output of an initial decoder using the complete set of available features, and will show that we manage to improve their translat"
D14-1133,W14-0301,0,0.191206,"potheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue of word to n-gram confidence estimation in SMT output (Zens and Ney, 2006; Ueffing and Ney, 2007; Bach et al., 2011; de Gispert et al., 2013), and some attempts have been made to exploit confidence estimates for lattice rescoring (Blackwood et al., 2010) or n-best reranking (Bach et al., 2011; Luong et al., 2014b). In this work, we present an approach in which 1261 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261–1272, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics new complete hypotheses are produced by rewriting existing hypotheses, and are scored using complex models that could not be used during the initial decoding. We will use as competitive baselines systems that rerank the output of an initial decoder using the complete set of available features, and will show that we manage to improve their translat"
D14-1133,W07-0732,0,0.174039,"004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post-editing may seem to be too limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue o"
D14-1133,P01-1030,0,0.0676499,"nd Monz (2011) managed to successfully exploit syntactic features using discriminative language modeling for n-best reranking. Gimpel et al. (2013) outperformed n-best reranking by generating, with an expensive but simple method, diverse hypotheses used as training data. Recently, Luong et al. (2014b) reranked n-best lists using confidence scores at the hypothesis level computed from word-level confidence measures learnt from roughly 10,000 SMT system outputs annotated by humans. Rewriting of translation hypotheses Langlais et al. (2007) described a greedy search decoder, first introduced in (Germann et al., 2001), able to improve translations produced by a dynamic programming decoder using the same scoring function and translation table. However, the more recent work by Arun et al. (2010) using a Gibbs sampler for approximating maximum translation decoding showed the adequacy of the approximations made by state-of-the-art decoders for finding the best translation in their search space. Other works were more directly targeted at automatic post-editing of SMT output, and approached the problem as one of second-pass translation between automatic predictions and correct translations (Simard et al., 2007;"
D14-1133,2013.iwslt-papers.16,1,0.935709,"nstance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post-editing may seem to be too limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue of word to n-gram confidence estimation in SMT output (Zens and Ney, 2006; Ueffing and Ney, 2007; Bach et al., 2011; de Gispert et al., 2013), and some attempts have been made to exploit confidence estimates for lattice rescoring (Blackwood et al., 2010) or n-best reranking (Bach et al., 2011; Luong et al., 2014b). In this work, we present an approach in which 1261 Proceedings of the 20"
D14-1133,P02-1040,0,0.0916465,"he WMT’13 data for TED Talks. Table 1 provides relevant statistics about the data used. Tasks Medical TED Talks Corpus Sentences Tokens (en-fr) train dev test LM 4.9M 500 1,000 78M - 91M 10k - 12k 21k - 26k - 146M train dev test LM 107 758 934 1,664 2M - 2.2M 20k - 20k 31k - 34k 6B - 2.5B Table 1: Corpora used in this work. 3 http://www.statmt.org/wmt14/ medical-task/ 4 https://wit3.fbk.eu/mt.php?release= 2013-01 5 http://www.statmt.org/wmt13 We first built a state-of-the-art phrase-based SMT system using Moses (Koehn et al., 2003) with standard settings. We tuned its parameters towards BLEU (Papineni et al., 2002) on the tuning dataset using the kb-mira implementation available in Moses with default parameters. Our results will be compared using BLEU and TER (Snover et al., 2006) to a) the initial best translation produced by the Moses decoder (1-pass Moses) and b) the best translation obtained by reranking the 1,000-best list of 1-pass Moses (reranker). Since reranker implements a well-documented approach and uses types of features commonly used in reranking tasks we will consider it as our main baseline. It was trained using kb-mira on the 1,000-best of the development data decoded by 1-pass Moses. I"
D14-1133,W06-1626,0,0.0257896,"eatures into decoding that better model semantic coherence at the sentence level (Hasan and Ney, 2009) or syntactic well-formedness (Schwartz et Aur´elien Max LIMSI-CNRS, Orsay, France Univ. Paris Sud, Orsay, France aurelien.max@limsi.fr al., 2011). However, early use of such complex features typically comes at a high computational cost. Moreover, some informative features require or are better computed when complete translation hypotheses are available. This is addressed in numerous works on reranking of the highest scored sub-space of hypotheses, on so-called n-best lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). W"
D14-1133,I13-1128,0,0.0260603,"y a dynamic programming decoder using the same scoring function and translation table. However, the more recent work by Arun et al. (2010) using a Gibbs sampler for approximating maximum translation decoding showed the adequacy of the approximations made by state-of-the-art decoders for finding the best translation in their search space. Other works were more directly targeted at automatic post-editing of SMT output, and approached the problem as one of second-pass translation between automatic predictions and correct translations (Simard et al., 2007; Dugast et al., 2007). The recent work of Zhu et al. (2013) attempts to repair translations by exploiting confidence estimates for examples derived from the similarity between source words in the input text and in training examples. Luong et al. (2014a) obtained improvements by computing word confidence estimation, trained on human annotated data, and large sets of lexical, syntactic and semantic features, for the words in the n-best list produced during a first-pass decoding, and performing a second-pass decoding exploiting these new scores. Confidence estimation of Machine Translation The Word Posterior Probability (WPP) proposed by Ueffing and Ney"
D14-1133,2012.eamt-1.34,0,0.0344159,"Missing"
D14-1133,P11-2038,0,0.0173245,"we could use a larger set of rewriting operations (Langlais et al., 2007), including the rewrite (sic) operation introduced in (Marie and Max, 2013) that paraphrases source phrases and then translates them. We could also possibly consider any phrase segmentation compatible with a specific word alignment rather than rely on specific phrase segmentations. This would allow us to attain faster some rewritings that could otherwise require several rewriting iterations and may never be attained by the greedy procedure. More features could also be used, for instance to model more fine-grained syntax (Post, 2011) or document-level lexical coherence (Hardmeier et al., 2012). However, anticipating that some features might be very expensive to compute, we could adapt our procedure to work in several passes: initial passes would tend to restrict the search space more and more using an initial set of features, before a more expensive pass would concentrate on a limited number of hypotheses. Figure 1 indeed already showed a much faster oracle improvement between 1-pass Moses and reranker for n-best list of small sizes. Another avenue for improvement lies in the possibility to perform the training of our rew"
D14-1133,P11-1063,0,0.275068,"Missing"
D14-1133,P06-2093,0,0.0350474,"Missing"
D14-1133,N07-1064,0,0.189931,"lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post-editing may seem to be too limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works ha"
D14-1133,2006.amta-papers.25,0,0.0476144,"1,000 78M - 91M 10k - 12k 21k - 26k - 146M train dev test LM 107 758 934 1,664 2M - 2.2M 20k - 20k 31k - 34k 6B - 2.5B Table 1: Corpora used in this work. 3 http://www.statmt.org/wmt14/ medical-task/ 4 https://wit3.fbk.eu/mt.php?release= 2013-01 5 http://www.statmt.org/wmt13 We first built a state-of-the-art phrase-based SMT system using Moses (Koehn et al., 2003) with standard settings. We tuned its parameters towards BLEU (Papineni et al., 2002) on the tuning dataset using the kb-mira implementation available in Moses with default parameters. Our results will be compared using BLEU and TER (Snover et al., 2006) to a) the initial best translation produced by the Moses decoder (1-pass Moses) and b) the best translation obtained by reranking the 1,000-best list of 1-pass Moses (reranker). Since reranker implements a well-documented approach and uses types of features commonly used in reranking tasks we will consider it as our main baseline. It was trained using kb-mira on the 1,000-best of the development data decoded by 1-pass Moses. In our experiments, rewriter rewrites the one-best hypothesis6 produced by reranker using the operators Replace, Split and Merge as described in section 2.1. 3.2 Baseline"
D14-1133,P13-4014,0,0.0224902,"Missing"
D14-1133,J07-1003,0,0.436822,"cause of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue of word to n-gram confidence estimation in SMT output (Zens and Ney, 2006; Ueffing and Ney, 2007; Bach et al., 2011; de Gispert et al., 2013), and some attempts have been made to exploit confidence estimates for lattice rescoring (Blackwood et al., 2010) or n-best reranking (Bach et al., 2011; Luong et al., 2014b). In this work, we present an approach in which 1261 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261–1272, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics new complete hypotheses are produced by rewriting existing hypotheses, and are scored using complex models that could not be used du"
D14-1133,vilar-etal-2006-error,0,0.0908079,"Missing"
D14-1133,W06-3110,0,0.0272005,"limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue of word to n-gram confidence estimation in SMT output (Zens and Ney, 2006; Ueffing and Ney, 2007; Bach et al., 2011; de Gispert et al., 2013), and some attempts have been made to exploit confidence estimates for lattice rescoring (Blackwood et al., 2010) or n-best reranking (Bach et al., 2011; Luong et al., 2014b). In this work, we present an approach in which 1261 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261–1272, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics new complete hypotheses are produced by rewriting existing hypotheses, and are scored using complex models th"
D14-1133,P07-2045,0,\N,Missing
D14-1133,N04-1021,0,\N,Missing
D14-1133,W09-1114,0,\N,Missing
D15-1120,P11-1022,0,0.0212069,"BLEU points when removing the phrase tables. TERppe 55 50 Moses 1-best PPE iteration 1 PPE iteration 2 PPE iteration 3 PPE iteration 4 PPE iteration 5 45 40 0.0 0.2 0.4 α 0.6 0.8 1.0 (b) Tuned with BLEU Figure 4: PPE results on the en→fr news task. 4 Discussion and future work We have introduced pre-post-editing, a minimalist interactive machine translation paradigm where a user is only asked to spot text fragments that may be used in the final translation. Our approach is quite comparable to the two-pass procedure described by Luong et al. (2014) using word-level confidence estimation (e.g. (Bach et al., 2011)) to update the cost of the search graph hypotheses. However, contrarily to Luong et al.’s work, our PPE framework is efficiently multi-pass, updates the models over iterations and relies on more informative annotations made at n-gram-level. Our evaluation based on simulated post-editing has revealed a large potential for translation improvement. Interestingly, the type of interaction defined 1043 fr→en tuned with BLEU TER BLEU en→fr tuned with BLEU TER BLEU Moses PPE w/ all models 52.7 40.4 28.6 49.7 51.8 41.0 31.1 49.5 PPE w/o negative-pt and negative-lm PPE w/o positive-pt and positive-lm 4"
D15-1120,N10-1080,0,0.0128471,"the task, the first iteration of PPE always yields a significant improvement over the Moses initial system (e.g. up to +9.8 BLEU and -8.2 TER for news fr→en). Unsurprisingly, tuning on a metric yields better results for the same metric for the first iteration; however, we note that this is not always true for the TER metric at later iterations (cf. news en→fr). More generally, tuning on the TER metric results in lower improvements for news, which are mostly concentrated on the first iterations; as systems tuned on BLEU have been found to produce better translations than systems tuned on TER (Cer et al., 2010), only BLEU tuning was used for medical.7 Improvements follow an interesting pattern over PPE iterations: for instance, on news fr→en, BLEU scores steadily increase after each new touch-based iteration and reach a gain of +21.1 BLEU and -12.3 TER over the initial Moses translation after 5 PPE iterations. Results are very comparable on both language pairs and both domains, e.g. gains of +12.1 BLEU and -9.7 TER are obtained on fr→en medical. The lesser amplitude of the gains obtained after 5 iterations may be attributed to the higher inimedical for both tasks LM 2.5B - 6B Table 1: Data used in t"
D15-1120,N12-1047,0,0.0219717,"illustrates 4 iterations of PPE from an initial translation hypothesis assuming a given target reference translation. 3 Experiments 3.1 Data and systems We ran experiments on two translation tasks of different domains: the WMT’14 Medical translation task (medical) and the WMT’11 news translation task (news) for the language pair en-fr on both translation directions. For both tasks we trained two competitive phrase-based SMT systems using Moses (Koehn et al., 2007) and WMT data6 (see Table 1). The tuning for all systems, including our iteration-specific PPE systems, was performed with kb-mira (Cherry and Foster, 2012). 3.2 An adapted evaluation metric: TERPPE Classical MT evaluation metrics cannot take into account the interactive cost of PPE, and thus do 1041 5 6 In this work, we did not exceed 5 iterations. http://www.statmt.org/wmt14 source c’ est la r´eponse a` une nouvelle prise de conscience selon laquelle les entreprises chinoises sont indispensables a` la survie e´ conomique de Taiwan PPE#0 PPE#1 PPE#2 PPE#3 this is the answer to a new awareness that Chinese companies are essential to the economic survival of Taiwan it is the response to a new awareness that Chinese firms are essential to Taiwan’s"
D15-1120,E14-1042,0,0.0536307,"ris Sud, Orsay, France aurelien.max@limsi.fr a draft translation to post-edit, we require a user to simply spot those segments of a draft translation that can participate in an acceptable translation. The corresponding information is then used by a SMT system in a soft way to improve the draft translation. This process may be iteratively repeated as long as enough improvements are obtained, and terminates with classical post-editing on the obtained translation, hence we dub it prepost-editing (PPE). We resort to simulated prepost-editing and post-editing, as in other works (Carl et al., 2011; Denkowski et al., 2014), to measure translation performance on some available reference translation using both classical metrics and a variant of the TER metric (Snover et al., 2006), where, essentially, the cost of a token matching operation is a parameterized fraction of the cost of the other token edit operations. With the implementation of appropriate strategies in the SMT system, we show under reasonable assumptions that this approach has the potential to significantly reduce the amount of human effort required to obtain a final translation. In the remainder of this article, we describe the technical details of"
D15-1120,W02-1020,0,0.402018,"red by our approach. 1 Introduction As shown by oracle studies (Wisniewski et al., 2010; Turchi et al., 2012; Marie and Max, 2013), Statistical Machine Translation (SMT) systems produce results that are of significantly lower quality than what could be produced from their available resources. As a pragmatic solution, human intervention is commonly used for improving automatic draft translations, in so-called post-editing (PE), but is also studied earlier in the translation process in a variety of interactive strategies, including e.g. completion assistance and local translation choices (e.g. (Foster et al., 2002; Koehn and Haddow, 2009; Gonz´alez-Rubio et al., 2013)). Although interactive machine translation does facilitate the work of the SMT system in certain situations by allowing it to make efficient use of knowledge contributed by the human translator, postediting has been shown to remain a faster alternative (Green et al., 2014). Nevertheless, this activity usually requires complex intervention from an expert translator (Carl et al., 2011). In this work we reduce interaction with an SMT system to its most basic form: similarly to what a human translator is likely to do when first reading Aur´el"
D15-1120,D13-1025,0,0.361755,"Missing"
D15-1120,D10-1091,0,0.0209208,"jamin.marie@limsi.fr Abstract We introduce pre-post-editing, possibly the most basic form of interactive translation, as a touch-based interaction with iteratively improved translation hypotheses prior to classical post-editing. We report simulated experiments that yield very large improvements on classical evaluation metrics (up to 21 BLEU) as well as on a parameterized variant of the TER metric that takes into account the cost of matching/touching tokens, confirming the promising prospects of the novel translation scenarios offered by our approach. 1 Introduction As shown by oracle studies (Wisniewski et al., 2010; Turchi et al., 2012; Marie and Max, 2013), Statistical Machine Translation (SMT) systems produce results that are of significantly lower quality than what could be produced from their available resources. As a pragmatic solution, human intervention is commonly used for improving automatic draft translations, in so-called post-editing (PE), but is also studied earlier in the translation process in a variety of interactive strategies, including e.g. completion assistance and local translation choices (e.g. (Foster et al., 2002; Koehn and Haddow, 2009; Gonz´alez-Rubio et al., 2013)). Although i"
D15-1120,D14-1130,0,0.371368,"rvention is commonly used for improving automatic draft translations, in so-called post-editing (PE), but is also studied earlier in the translation process in a variety of interactive strategies, including e.g. completion assistance and local translation choices (e.g. (Foster et al., 2002; Koehn and Haddow, 2009; Gonz´alez-Rubio et al., 2013)). Although interactive machine translation does facilitate the work of the SMT system in certain situations by allowing it to make efficient use of knowledge contributed by the human translator, postediting has been shown to remain a faster alternative (Green et al., 2014). Nevertheless, this activity usually requires complex intervention from an expert translator (Carl et al., 2011). In this work we reduce interaction with an SMT system to its most basic form: similarly to what a human translator is likely to do when first reading Aur´elien Max LIMSI-CNRS, Orsay, France Univ. Paris Sud, Orsay, France aurelien.max@limsi.fr a draft translation to post-edit, we require a user to simply spot those segments of a draft translation that can participate in an acceptable translation. The corresponding information is then used by a SMT system in a soft way to improve th"
D15-1120,2009.mtsummit-papers.8,0,0.178946,"1 Introduction As shown by oracle studies (Wisniewski et al., 2010; Turchi et al., 2012; Marie and Max, 2013), Statistical Machine Translation (SMT) systems produce results that are of significantly lower quality than what could be produced from their available resources. As a pragmatic solution, human intervention is commonly used for improving automatic draft translations, in so-called post-editing (PE), but is also studied earlier in the translation process in a variety of interactive strategies, including e.g. completion assistance and local translation choices (e.g. (Foster et al., 2002; Koehn and Haddow, 2009; Gonz´alez-Rubio et al., 2013)). Although interactive machine translation does facilitate the work of the SMT system in certain situations by allowing it to make efficient use of knowledge contributed by the human translator, postediting has been shown to remain a faster alternative (Green et al., 2014). Nevertheless, this activity usually requires complex intervention from an expert translator (Carl et al., 2011). In this work we reduce interaction with an SMT system to its most basic form: similarly to what a human translator is likely to do when first reading Aur´elien Max LIMSI-CNRS, Orsa"
D15-1120,P07-2045,0,0.00772149,"dels. The weights for all, old or new, models in the log-linear combination are found by tuning on a development set for each PPE iteration.5 Figure 3 illustrates 4 iterations of PPE from an initial translation hypothesis assuming a given target reference translation. 3 Experiments 3.1 Data and systems We ran experiments on two translation tasks of different domains: the WMT’14 Medical translation task (medical) and the WMT’11 news translation task (news) for the language pair en-fr on both translation directions. For both tasks we trained two competitive phrase-based SMT systems using Moses (Koehn et al., 2007) and WMT data6 (see Table 1). The tuning for all systems, including our iteration-specific PPE systems, was performed with kb-mira (Cherry and Foster, 2012). 3.2 An adapted evaluation metric: TERPPE Classical MT evaluation metrics cannot take into account the interactive cost of PPE, and thus do 1041 5 6 In this work, we did not exceed 5 iterations. http://www.statmt.org/wmt14 source c’ est la r´eponse a` une nouvelle prise de conscience selon laquelle les entreprises chinoises sont indispensables a` la survie e´ conomique de Taiwan PPE#0 PPE#1 PPE#2 PPE#3 this is the answer to a new awareness"
D15-1120,2013.iwslt-papers.16,1,0.748123,"-post-editing, possibly the most basic form of interactive translation, as a touch-based interaction with iteratively improved translation hypotheses prior to classical post-editing. We report simulated experiments that yield very large improvements on classical evaluation metrics (up to 21 BLEU) as well as on a parameterized variant of the TER metric that takes into account the cost of matching/touching tokens, confirming the promising prospects of the novel translation scenarios offered by our approach. 1 Introduction As shown by oracle studies (Wisniewski et al., 2010; Turchi et al., 2012; Marie and Max, 2013), Statistical Machine Translation (SMT) systems produce results that are of significantly lower quality than what could be produced from their available resources. As a pragmatic solution, human intervention is commonly used for improving automatic draft translations, in so-called post-editing (PE), but is also studied earlier in the translation process in a variety of interactive strategies, including e.g. completion assistance and local translation choices (e.g. (Foster et al., 2002; Koehn and Haddow, 2009; Gonz´alez-Rubio et al., 2013)). Although interactive machine translation does facilit"
D15-1120,2014.eamt-1.23,0,0.0239082,"Missing"
D15-1120,P02-1040,0,0.0930033,"green background. Phrases with a gray background indicate previously touched phrases but their tokens remain individually touchable by the user. Tasks Corpus Sentences Tokens (fr-en) 3.3 news train dev test 12M 2,525 3,003 383M - 318M 73k - 65k 85k - 74k train dev test specialized LM 4.9M 500 1,000 91M - 78M 12k - 10k 26k - 21k 146M - 78M To validate our approach, we initially used a simulated post-editing paradigm (Carl et al., 2011; Denkowski et al., 2014) in which non-post-edited reference translations are used in lieu of human post-editions. Results on TER (Snover et al., 2006) and BLEU (Papineni et al., 2002), tuning on both metrics, are provided in Tables 2 (news) and 3 (medical). First, we observe that whatever the metric and the task, the first iteration of PPE always yields a significant improvement over the Moses initial system (e.g. up to +9.8 BLEU and -8.2 TER for news fr→en). Unsurprisingly, tuning on a metric yields better results for the same metric for the first iteration; however, we note that this is not always true for the TER metric at later iterations (cf. news en→fr). More generally, tuning on the TER metric results in lower improvements for news, which are mostly concentrated on"
D15-1120,D10-1013,0,0.0349487,"Missing"
D15-1120,2006.amta-papers.25,0,0.514143,"ticipate in an acceptable translation. The corresponding information is then used by a SMT system in a soft way to improve the draft translation. This process may be iteratively repeated as long as enough improvements are obtained, and terminates with classical post-editing on the obtained translation, hence we dub it prepost-editing (PPE). We resort to simulated prepost-editing and post-editing, as in other works (Carl et al., 2011; Denkowski et al., 2014), to measure translation performance on some available reference translation using both classical metrics and a variant of the TER metric (Snover et al., 2006), where, essentially, the cost of a token matching operation is a parameterized fraction of the cost of the other token edit operations. With the implementation of appropriate strategies in the SMT system, we show under reasonable assumptions that this approach has the potential to significantly reduce the amount of human effort required to obtain a final translation. In the remainder of this article, we describe the technical details of pre-post-editing (Section 2), report experiments conducted on two translation directions and two domains (Section 3), and finally discuss our proposal and int"
D15-1120,N12-1046,0,0.05823,"Missing"
D15-1120,2010.amta-workshop.3,0,\N,Missing
E03-3006,W00-1404,0,0.702764,"nveying more or less subtle semantic distinctions. We argue that for documents of such an important nature, consistency of expression and of information presentation can not only be beneficial to the reader but also necessary to allow a clear and unambiguous understanding of the communicative intentions contained in different documents. Controlled document authoring systems can guarantee that the documents they produce are consistent as the production of the text is under the control of the system. An authoring system for drug leaflets conforming to Le Vidal specifications has been developed (Brun et al., 2000), showing that new documents could be written in a fully controlled way. But most existing documents, if they conform to some specifications, do not have these desirable properties across different drug vendors. Our research thus addresses this complementary issue: can we reuse the modelling of documents of such systems to analyze existing legacy documents from the same class of documents? Document normalization implies analyzing a legacy document into a semantically well-formed content representation, and producing a normalized version from that content representation. This expresses predefin"
E03-3006,C00-1036,0,0.902157,"e class of documents? Document normalization implies analyzing a legacy document into a semantically well-formed content representation, and producing a normalized version from that content representation. This expresses predefined communicative content present in the input document, in a structurally and linguistically controlled way. Predefined content reveals communicative intentions, which should ideally be described by an expert of the discourse domain. 3 Controlled document authoring There has been a recent trend to investigate controlled document authoring, e.g. (Power and Scott, 1998; Dymetman et al., 2000), where the focus is on obtaining document content representations by interaction with the user/author and producing multilingual versions of the final document from them. Typically, the user of these systems has to select possible semantic choices in active fields present in the evolving text of the document in the user's language. These selections iteratively refine list0fRroductWarnings(AllergyWarning, DurationWarning)::productWarnings(TypeOfSymptom, ActiveIngredient)-e ---&gt; ['PRODUCT WARNINCS'], AllergyWarning::allergyWarning(ActiveIngredient)-e, DurationWarning::durationWarning(TypeOfSymp"
E03-3006,W98-0705,0,0.0291929,"ccount of text content and are compared to evaluate content similarity. We defined our notion of content similarity from the fact, broadly accepted in the information retrieval community, that the more terms (and related terms) are shared by two texts, the more likely they are to be about the same topic. Text content can be roughly approximated by a vector containing all lemmatized forms of words and their associated number of occurrences. We call such a vector the lexical profile of a text. It has been shown that using sets of synonyms instead of word forms could improve similarity measures (Gonzalo et al., 1998), so we use synset profiles to account for lexico- semantic variation. Text profile construction Words in text fragments are first lemmatized and their part-ofspeech is disambiguated using the morphological analysis tools of XRCE. Their corresponding set of synonyms is then looked up through a lexico-semantic interface, and the corresponding synset key is used to index the word or expression. We have developed an annotation graphical interface that allows a human to annotate strings in MDA grammars by choosing the appropriate synset in the default lexico-semantic resource, WordNet (Miller et a"
E03-3006,W99-0625,0,0.0478179,"r a given source of documents so that it guarantees that the correct candidate is retained. 38 cally using more fine-grained measures. An approach can be to search for evidence of the presence of some text passages produced by competing semantic choices in the input document, and to rescore them appropriately. Given the constraints on the domain of the input documents, we hope that simple features will help significantly in disambiguating candidates, as for example distance constraints which have been shown to participate significantly in the evaluation of text similarity over short passages (Hatzivassiloglou et al., 1999). 5.5 Interactive disambiguation Due to its limitations, the proposed approach cannot guarantee that the correct candidate can be selected automatically. Recognizing similar communicative intentions challenges simple text matching techniques, and can require expert knowledge that is difficult to obtain a priori and to encode into automatic disambiguation rules. We therefore propose that automatic selection of candidates be done down to a level of confidence that would be determined so as to guarantee that the correct document is retained. We then envisage several modes of intervention from an"
E03-3006,2002.jeptalnrecital-recital.8,1,0.673758,"to propose application domains for document normalization, we attempted to identify do33 mains where documents of the same nature but from different origins where compiled into homogeneous collections. We focussed our attention on the pharmaceutical domain, which produces several yearly compendiums of drug leaflets, as for example the French Vidal de la Famille (OVP Editions du VIDAL, 1998). Producing pharmaceutical documents is the responsibility of the pharmaceutical companies which market the drugs. A study we conducted on a corpus of 50 patient pharmaceutical leaflets for pain relievers (Max, 2002) collected on drug vendor websites revealed several types of variations. The first observation was that the structures of the leaflets could vary considerably. For comparable drugs, we found that for example warning-related information could be presented in different ways. One of them was to divide them into two sections, Warnings and Side effects, another one had a threesection division into Drug interaction precautions, Warnings, and Alcohol warning. In the first case, drug interaction precautions effectively appeared in the more general Warnings section (You should ask your doctor before ta"
E03-3006,W03-2204,1,0.647982,"obtain the semantic structure corresponding to a raw document, and then produce from it a completely controlled version. If the raw document is bigger in scope from the documents that the authoring system models, then something similar to document summarization by content recognition and reformulation would be done. Incomplete representations after automatic analysis could be interactively completed, thus reentering controlled document authoring. Producing the document from the semantic representation in several languages would do some kind of normalizing translation of the original document (Max, 2003). We call the process of reconstructing such a semantic representation (and re-generating controlled text in the same language), which is common to all the above cases, document normalization. In this paper, we will first attempt to argue why document normalization could be of some use in the real world. We will then introduce our approach to document normalization, and describe a possible implementation. We will conclude by introducing our future work. 2 Why normalize documents? Text normalization often refers to techniques used to disambiguate text to facilitate its analysis (Mikheev, 2000)."
E03-3006,P98-2173,0,0.216181,"documents from the same class of documents? Document normalization implies analyzing a legacy document into a semantically well-formed content representation, and producing a normalized version from that content representation. This expresses predefined communicative content present in the input document, in a structurally and linguistically controlled way. Predefined content reveals communicative intentions, which should ideally be described by an expert of the discourse domain. 3 Controlled document authoring There has been a recent trend to investigate controlled document authoring, e.g. (Power and Scott, 1998; Dymetman et al., 2000), where the focus is on obtaining document content representations by interaction with the user/author and producing multilingual versions of the final document from them. Typically, the user of these systems has to select possible semantic choices in active fields present in the evolving text of the document in the user's language. These selections iteratively refine list0fRroductWarnings(AllergyWarning, DurationWarning)::productWarnings(TypeOfSymptom, ActiveIngredient)-e ---&gt; ['PRODUCT WARNINCS'], AllergyWarning::allergyWarning(ActiveIngredient)-e, DurationWarning::du"
E03-3006,C98-2168,0,\N,Missing
E12-1073,P05-1074,0,0.569991,"ms, increases the likelihood of finding very close contexts for sub-sentential units. Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. The work by Bhagat and Ravichandran (2008) describes an application of a similar technique on a very large scale. The hypothesis that two words or phrases are interchangeable if they share a common translation into one or more other languages has also been extensively studied in works on subsentential paraphrase acquisition. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora in several languages. The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al. (2008). The work of Callison-Burch (2008) has shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases. Another approach consists in modelling local paraphrasing identification rules. The work of Jacquemin (1999) on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological"
E12-1073,N03-1003,0,0.129166,"phrases, occur in similar contexts then they may be interchangeable has been extensively tested. The distributional hypothesis, attributed to Zellig Harris, was for example applied to syntactic dependency paths in the work of Lin and Pantel (2001). Their results take the form of equivalence patterns with two arguments such as {X asks for Y, X requests Y, X’s request for Y, X wants Y, Y is requested by X, . . .}. Using comparable corpora, where the same information probably exists under various linguistic forms, increases the likelihood of finding very close contexts for sub-sentential units. Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. The work by Bhagat and Ravichandran (2008) describes an application of a similar technique on a very large scale. The hypothesis that two words or phrases are interchangeable if they share a common translation into one or more other languages has also been extensively studied in works on subsentential paraphrase acquisition. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora i"
E12-1073,P01-1008,0,0.780616,"local paraphrasing identification rules. The work of Jacquemin (1999) on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora. When parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (Cohn et al., 2008). Barzilay and McKeown (2001) applied the distributionality hypothesis on such parallel sentences, and Pang et al. (2003) proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants. Finally, they has been a recent interest in automatic evaluation of paraphrases (Callison-Burch et al., 2008; Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011). 3 Experimental setting We used the main aspects of the methodology described by Cohn et al. (2008) for constructing evaluation corpora and assessing the performance of techniques on the task of sub-sentential paraphrase acquisition."
E12-1073,P08-1077,0,0.0788221,"ndency paths in the work of Lin and Pantel (2001). Their results take the form of equivalence patterns with two arguments such as {X asks for Y, X requests Y, X’s request for Y, X wants Y, Y is requested by X, . . .}. Using comparable corpora, where the same information probably exists under various linguistic forms, increases the likelihood of finding very close contexts for sub-sentential units. Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. The work by Bhagat and Ravichandran (2008) describes an application of a similar technique on a very large scale. The hypothesis that two words or phrases are interchangeable if they share a common translation into one or more other languages has also been extensively studied in works on subsentential paraphrase acquisition. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora in several languages. The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al. (2008). The work of Callison-Burch (2008) has shown how the monolingual context of a"
E12-1073,C08-1013,0,0.212209,"Missing"
E12-1073,D08-1021,0,0.0523473,"des local variations. The work by Bhagat and Ravichandran (2008) describes an application of a similar technique on a very large scale. The hypothesis that two words or phrases are interchangeable if they share a common translation into one or more other languages has also been extensively studied in works on subsentential paraphrase acquisition. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora in several languages. The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al. (2008). The work of Callison-Burch (2008) has shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases. Another approach consists in modelling local paraphrasing identification rules. The work of Jacquemin (1999) on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora. When parallel monolingual corpora aligned at the sentence level are available (e.g. multiple transl"
E12-1073,candito-etal-2010-statistical,0,0.022921,"Missing"
E12-1073,P11-1020,0,0.265691,"Missing"
E12-1073,J08-4005,0,0.252742,"nsists in modelling local paraphrasing identification rules. The work of Jacquemin (1999) on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora. When parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (Cohn et al., 2008). Barzilay and McKeown (2001) applied the distributionality hypothesis on such parallel sentences, and Pang et al. (2003) proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants. Finally, they has been a recent interest in automatic evaluation of paraphrases (Callison-Burch et al., 2008; Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011). 3 Experimental setting We used the main aspects of the methodology described by Cohn et al. (2008) for constructing evaluation corpora and assessing the performance of techniques on the task of sub-senten"
E12-1073,C04-1051,0,0.507008,"Missing"
E12-1073,P08-4006,0,0.16624,"indicates the proportion of tokens from the sentence pairs that could be manually aligned by a native-speaker annotator.2 Obviously, the more common tokens two sentences from a pair contain, the fewer subsentential paraphrases may be extracted from that pair. However, high lexical overlap increases the probability that two sentences be indeed paraphrases, and in turn the probability that some of their phrases be paraphrases. Furthermore, the tated corpora using them we considered all alignments as being correct. 2 The same annotator hand-aligned the 5*100=500 paraphrase pairs using the YAWAT (Germann, 2008) manual alignment tool. presence of common token may serve as useful clues to guide paraphrase extraction. For our experiments, we chose to use parallel monolingual corpora obtained by single language translation, the most direct resource type for acquiring sub-sentential paraphrase pairs. This allows us to define acceptable references for the task and resort to the most consensual evaluation technique for paraphrase acquisition to date. Using such corpora, we expect to be able to extract precise paraphrases (see Table 1), which will be natural candidates for further validation, which will be"
E12-1073,P99-1044,0,0.614531,"more other languages has also been extensively studied in works on subsentential paraphrase acquisition. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora in several languages. The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al. (2008). The work of Callison-Burch (2008) has shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases. Another approach consists in modelling local paraphrasing identification rules. The work of Jacquemin (1999) on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora. When parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (Cohn et al., 2008). Barzilay and McKeown (2001) applied the distributionality hypothesis"
E12-1073,P03-1054,0,0.00429764,"sider all phrases from 5 http://statmt.org/europarl the first sentence and search for variants in the other sentence, then do the reverse process and finally take the intersection of the two sets. 4.4 Syntactic similarity (Synt) The algorithm introduced by Pang et al. (2003) takes two sentences as input and merges them by top-down syntactic fusion guided by compatible syntactic substructure. A lexical blocking mechanism prevents constituents from fusionning when there is evidence of the presence of a word in another constituent of one of the sentence. We use the Berkeley Probabilistic parser (Klein and Manning, 2003) to obtain syntactic trees for English and its adapted version for French (Candito et al., 2010). Because this process is highly sensitive to syntactic parse errors, we use in our implementation k-best parses and retain the most compact fusion from any pair of candidate parses. 4.5 Edit rate on word sequences (TERp ) TERp (Translation Edit Rate Plus) (Snover et al., 2010) is a score designed for the evaluation of Machine Translation output. Its typical use takes a system hypothesis to compute an optimal set of word edits that can transform it into some existing reference translation. Edit type"
E12-1073,P07-2045,0,0.00309514,"ability between two phrases based on their translation probability through all possible pivot phrases as: X Ppara (p1 , p2 ) = Pt (piv|p1 )Pt (p2 |piv) piv where Pt denotes translation probabilies. We used the Europarl corpus5 of parliamentary debates in English and French, consisting of approximately 1.7 million parallel sentences : this allowed us to use the same resource to build paraphrases for English, using French as the pivot language, and for French, using English as the pivot language. The GIZA++ tool was used for word alignment and the M OSES Statistical Machine Translation toolkit (Koehn et al., 2007) was used to compute phrase translation probabilities from these word alignments. For each sentential paraphrase pair, we applied the following algorithm: for each phrase, we build the entire set of paraphrases using the previous definition. We then extract its best paraphrase as the one exactly appearing in the other sentence with maximum paraphrase probability, using a minimal threshold value of 10−4 . 4.3 Linguistic knowledge on term variation (Fastr) The FASTR tool (Jacquemin, 1999) was designed to spot term/phrase variants in large corpora. Variants are described through metarules express"
E12-1073,D10-1090,0,0.20541,"Missing"
E12-1073,J10-3003,0,0.0362016,"quivalent meaning at the phrasal level (including single words). For instance, the phrases six months and half a year form a paraphrase pair applicable in many different contexts, as they would appropriately denote the same concept. Although one can envisage to manually build high-coverage lists of synonyms, enumerating meaning equivalences at the level of phrases is too daunting a task for humans. Because this type of knowledge can however greatly benefit many NLP applications, automatic acquisition of such paraphrases has attracted a lot of attention (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010), and significant research efforts have been devoted to this objective (Callison-Burch, 2007; Bhagat, 2009; Madnani, 2010). Central to acquiring paraphrases is the need of assessing the quality of the candidate paraphrases produced by a given technique. Most works to date have resorted to human evaluation of paraphrases on the levels of grammaticality and meaning equivalence. Human evaluation is however often criticized as being both costly and non reproducible, and the situation is even more complicated by the inherent complexity of the task that can produce low inter-judge agreement. Taskbas"
E12-1073,P11-2096,0,0.341476,"Missing"
E12-1073,J04-4002,0,0.0481864,"ork. In this work, we consider the scenario where sentential paraphrases are available and words and phrases from one sentence can be aligned to words and phrases from the other sentence to form atomic paraphrase pairs. We now describe several techniques that perform the task of sub-sentential unit alignment. We have selected and implemented five techniques which we believe are representative of the type of knowledge that these techniques use, and have reused existing tools, initially developed for other tasks, when possible. 4.1 Statistical learning of word alignments (Giza) The GIZA++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. While originally developed in the bilingual context of Statistical Machine Translation, nothing prevents building such models on monolingual corpora. However, in order to build reliable models, it is necessary to use enough training material including minimal redundancy of words. To this end, we provided GIZA++ with all possible sentence pairs from our mutiply-translated corpus to improve the quality of its word alignments (note that 4 719 http://www.elda.org/article125.html we used symmetrized alignmen"
E12-1073,N03-1024,0,0.142135,"ariants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora. When parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (Cohn et al., 2008). Barzilay and McKeown (2001) applied the distributionality hypothesis on such parallel sentences, and Pang et al. (2003) proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants. Finally, they has been a recent interest in automatic evaluation of paraphrases (Callison-Burch et al., 2008; Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011). 3 Experimental setting We used the main aspects of the methodology described by Cohn et al. (2008) for constructing evaluation corpora and assessing the performance of techniques on the task of sub-sentential paraphrase acquisition. Pairs of related sentences are hand-aligned to define a set of reference atomic paraphrase p"
E12-1073,P08-1089,0,0.0629322,"lattice representation that encodes local variations. The work by Bhagat and Ravichandran (2008) describes an application of a similar technique on a very large scale. The hypothesis that two words or phrases are interchangeable if they share a common translation into one or more other languages has also been extensively studied in works on subsentential paraphrase acquisition. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora in several languages. The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al. (2008). The work of Callison-Burch (2008) has shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases. Another approach consists in modelling local paraphrasing identification rules. The work of Jacquemin (1999) on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora. When parallel monolingual corpora aligned at the sentence level"
F12-2002,W11-0207,1,0.881677,"Missing"
F12-2002,P07-2045,0,0.0132777,"Missing"
F12-2002,C02-1003,0,0.0575184,"Missing"
F12-2002,J04-4002,0,0.0172252,"Missing"
F12-2002,W09-1906,0,0.022249,"Missing"
F12-2002,N01-1026,0,0.137191,"Missing"
F12-2015,P05-1074,0,0.100015,"Missing"
F12-2015,N03-1003,0,0.0998693,"Missing"
F12-2015,P01-1008,0,0.128195,"Missing"
F12-2015,P08-1077,0,0.0466614,"Missing"
F12-2015,P11-2069,1,0.832338,"Missing"
F12-2015,I05-5001,0,0.0442445,"Missing"
F12-2015,candito-etal-2010-statistical,0,0.0313508,"Missing"
F12-2015,J08-4005,0,0.0391223,"Missing"
F12-2015,C08-1018,0,0.0238144,"Missing"
F12-2015,W09-3102,0,0.0252956,"Missing"
F12-2015,D11-1108,0,0.0281563,"Missing"
F12-2015,N10-1017,0,0.024775,"Missing"
F12-2015,J10-3003,0,0.0338433,"Missing"
F12-2015,2008.amta-papers.13,0,0.0544819,"Missing"
F12-2015,C04-1166,1,0.861184,"Missing"
F12-2015,max-wisniewski-2010-mining,1,0.905676,"Missing"
F12-2015,W08-1911,1,0.894545,"Missing"
F12-2015,P10-2001,0,0.0606923,"Missing"
F12-2015,N03-1024,0,0.0273338,"Missing"
F12-2015,I05-1011,0,0.070267,"Missing"
F12-2015,N07-1051,0,0.0193161,"Missing"
F12-2015,W04-3219,0,0.0732191,"Missing"
F12-2015,D10-1013,0,0.0613904,"Missing"
F12-2015,E09-1082,0,0.0252746,"Missing"
F12-2015,N10-2012,0,0.0230885,"Missing"
F12-2015,P09-1094,0,0.0541476,"Missing"
F12-2015,C10-1149,0,0.0375349,"Missing"
F12-2015,C10-1152,0,0.0607389,"Missing"
F12-2020,P05-1074,0,0.104377,"Missing"
F12-2020,W03-1004,0,0.0177066,"Missing"
F12-2020,P01-1008,0,0.134909,"Missing"
F12-2020,W08-0906,0,0.0568488,"Missing"
F12-2020,P08-1077,0,0.04689,"Missing"
F12-2020,C08-1013,0,0.0337731,"Missing"
F12-2020,W11-2504,0,0.0299908,"Missing"
F12-2020,P11-1020,0,0.0337687,"Missing"
F12-2020,J08-4005,0,0.0286401,"Missing"
F12-2020,D10-1113,0,0.0284105,"Missing"
F12-2020,C04-1051,0,0.0305195,"Missing"
F12-2020,P10-2017,0,0.0473081,"Missing"
F12-2020,W11-0111,0,0.0510101,"Missing"
F12-2020,C04-1151,0,0.0202999,"Missing"
F12-2020,P08-4006,0,0.0272382,"Missing"
F12-2020,P99-1044,0,0.0159824,"Missing"
F12-2020,P07-2045,0,0.00680795,"Missing"
F12-2020,N03-1017,0,0.00672789,"Missing"
F12-2020,N10-1017,0,0.0209541,"Missing"
F12-2020,P10-4008,0,0.0247424,"Missing"
F12-2020,W07-0734,0,0.0414566,"Missing"
F12-2020,D10-1090,0,0.0286066,"Missing"
F12-2020,J10-3003,0,0.0436927,"Missing"
F12-2020,2008.amta-papers.13,0,0.0208433,"Missing"
F12-2020,D09-1040,0,0.0235767,"Missing"
F12-2020,D10-1064,1,0.874667,"Missing"
F12-2020,P11-2096,0,0.0210605,"Missing"
F12-2020,E06-1021,0,0.0267946,"Missing"
F12-2020,J04-4002,0,0.0126002,"Missing"
F12-2020,N03-1024,0,0.0196593,"Missing"
F12-2020,P02-1040,0,0.085289,"Missing"
F12-2020,I05-1011,0,0.0668427,"Missing"
F12-2020,D10-1013,0,0.0442446,"Missing"
F12-2020,E09-1082,0,0.0227366,"Missing"
F12-2020,2006.amta-papers.25,0,0.0530812,"Missing"
F12-2020,W09-0621,0,0.0209422,"Missing"
F12-2020,P09-1094,0,0.0370838,"Missing"
F14-2002,2013.iwslt-papers.7,1,0.830631,"Missing"
F14-2002,2010.amta-papers.21,0,0.0722326,"Missing"
F14-2002,2012.eamt-1.62,1,0.90627,"Missing"
F14-2002,2010.eamt-1.37,0,0.0750873,"Missing"
F14-2002,C08-1064,0,0.0624856,"Missing"
F14-2002,J03-1002,0,0.0176318,"Missing"
F14-2002,P02-1040,0,0.106788,"Missing"
F14-2002,P13-1135,0,0.0569748,"Missing"
F14-2002,2006.amta-papers.25,0,0.117792,"Missing"
F14-2002,J97-3002,0,0.115511,"Missing"
F14-2002,W06-3111,0,0.0455425,"Missing"
max-etal-2010-contrastive,E09-1010,0,\N,Missing
max-etal-2010-contrastive,W09-0441,0,\N,Missing
max-etal-2010-contrastive,P02-1040,0,\N,Missing
max-etal-2010-contrastive,W09-0401,0,\N,Missing
max-etal-2010-contrastive,P07-2045,0,\N,Missing
max-etal-2010-contrastive,J04-2004,0,\N,Missing
max-etal-2010-contrastive,W07-0734,0,\N,Missing
max-etal-2010-contrastive,J06-4004,1,\N,Missing
max-etal-2010-contrastive,E09-1082,0,\N,Missing
max-etal-2010-contrastive,2005.mtsummit-papers.11,0,\N,Missing
max-etal-2010-contrastive,vilar-etal-2006-error,0,\N,Missing
max-etal-2010-contrastive,2008.amta-srw.6,0,\N,Missing
max-etal-2010-contrastive,carpuat-wu-2008-evaluation,0,\N,Missing
max-wisniewski-2010-mining,C08-1018,0,\N,Missing
max-wisniewski-2010-mining,D08-1021,0,\N,Missing
max-wisniewski-2010-mining,P05-1074,0,\N,Missing
max-wisniewski-2010-mining,E09-1082,0,\N,Missing
max-wisniewski-2010-mining,zesch-etal-2008-extracting,0,\N,Missing
P03-2017,J93-2003,0,0.00655908,"Missing"
P03-2017,1997.mtsummit-papers.1,0,0.0908424,"Missing"
P03-2017,W02-1020,0,0.0288727,"Missing"
P03-2017,E03-3006,1,0.826117,"Missing"
P03-2017,P02-1038,0,0.0228667,"Missing"
P03-2017,P98-2173,0,0.0823743,"Missing"
P03-2017,P01-1067,1,0.758419,"Missing"
P03-2017,W00-1404,1,\N,Missing
P03-2017,C98-2168,0,\N,Missing
P11-2069,P05-1074,0,0.0676412,"oses. We show that the tunable TER-PLUS metric from Machine Translation evaluation can achieve good performance on this task and that it can effectively exploit information coming from complementary sources. 1 Introduction The acquisition of subsentential paraphrases has attracted a lot of attention recently (Madnani and Dorr, 2010). Techniques are usually developed for extracting paraphrase candidates from specific types of corpora, including monolingual parallel corpora (Barzilay and McKeown, 2001), monolingual comparable corpora (Del´eger and Zweigenbaum, 2009), bilingual parallel corpora (Bannard and Callison-Burch, 2005), and edit histories of multi-authored text (Max and Wisniewski, 2010). These approaches face two main issues, which correspond to the typical measures of precision, or how appropriate the extracted paraphrases are, and of recall, or how many of the paraphrases present in a given corpus can be found effectively. To start with, both measures are often hard to compute in practice, as 1) the definition of what makes an acceptable paraphrase pair is still a research question, and 2) it is often impractical to extract a complete set of acceptable paraphrases 395 from most resources. Second, as rega"
P11-2069,C08-1013,0,0.0203037,"ties. We are finally also in the process of conducting a careful study of the characteristics of the paraphrase pairs that each technique can extract with high confidence, so that we can improve our hybridation experiments by considering confidence values at the paraphrase level using Machine Learning. This way, we may be able to use an edit rate computation algorithm such as TER-PLUS as a more efficient system combiner for paraphrase extraction methods than what was proposed here. A potential application of this would be an alternative proposal to the paraphrase evaluation metric PARAMETRIC (Callison-Burch et al., 2008), where individual techniques, outputing word alignments or not, could be evaluated from the ability of the informated edit rate technique to use correct equivalence units. 4 Indeed, measuring the precision on the union yields a poor performance of 23.96, but with the highest achievable value of 50.56 for recall. Similarly, the maximum value for precision with a good recall can be obtained by taking the intersection of the results of TER Ppara and G IZA ++, which yields a value of 60.39. 399 Acknowledgments This work was partly funded by a grant from LIMSI. The authors wish to thank the anonym"
P11-2069,candito-etal-2010-statistical,0,0.0892695,"Missing"
P11-2069,J08-4005,0,0.747644,"et al., 2004) often contain unrelated segments that should not be aligned to form a subsentential paraphrase pair. Using bilingual corpora to acquire paraphrases indirectly by pivoting through other languages is faced, in particular, with the issue of phrase polysemy, both in the source and in the pivot languages. It has previously been noted that highly parallel monolingual corpora, typically obtained via multiple translation into the same language, constitute the most appropriate type of corpus for extracting high quality paraphrases, in spite of their rareness (Barzilay and McKeown, 2001; Cohn et al., 2008; Bouamor et al., 2010). We build on this claim here to propose an original approach for the task of subsentential alignment based on the computation of a minimum edit rate between two sentential paraphrases. More precisely, we concentrate on the alignment of atomic paraphrase pairs (Cohn et al., 2008), where the words from both paraphrases are aligned as a whole to the words of the other paraphrase, as opposed to composite paraphrase pairs obtained by joining together adjacent paraphrase pairs or possibly adding unaligned words. Figure 1 provides examples of atomic paraphrase pairs derived fr"
P11-2069,W09-3102,0,0.426347,"Missing"
P11-2069,C04-1051,0,0.0546249,"measures are often hard to compute in practice, as 1) the definition of what makes an acceptable paraphrase pair is still a research question, and 2) it is often impractical to extract a complete set of acceptable paraphrases 395 from most resources. Second, as regards the precision of paraphrase acquisition techniques in particular, it is notable that most works on paraphrase acquisition are not based on direct observation of larger paraphrase pairs. Even monolingual corpora obtained by pairing very closely related texts such as news headlines on the same topic and from the same time frame (Dolan et al., 2004) often contain unrelated segments that should not be aligned to form a subsentential paraphrase pair. Using bilingual corpora to acquire paraphrases indirectly by pivoting through other languages is faced, in particular, with the issue of phrase polysemy, both in the source and in the pivot languages. It has previously been noted that highly parallel monolingual corpora, typically obtained via multiple translation into the same language, constitute the most appropriate type of corpus for extracting high quality paraphrases, in spite of their rareness (Barzilay and McKeown, 2001; Cohn et al., 2"
P11-2069,P08-4006,0,0.0199569,"11.92 18.47 17.10 6.94 21.02 20.28 3.41 18.94 16.44 13.57 19.30 16.35 f1 16.25 9.30 4.21 18.77 4.31 19.26 19.52 16.52 19.14 18.95 11.91 20.92 21.33 6.15 18.47 17.59 18.58 17.96 21.02 Figure 2: Results on the test set on French and English for the individual techniques and TER P hybrid systems. Column headers of the form “→ c” indicate that TER P was tuned on criterion c. figures reveal that the French corpus tends to contain more literal translations, possibly due to the original languages of the sentences, which are closer to the target language than Chinese is to English. We used the YAWAT (Germann, 2008) interactive alignment tool and measure inter-annotator agreement over a subset and found it to be similar to the value reported by Cohn et al. (2008) for English. Results for all individual techniques in the two languages are given on Figure 2. We first note that all techniques fared better on the French corpus than on the English corpus. This can certainly be explained by the fact that the former results from more literal translations, which are consequently easier to word-align. TERMT (i.e. TER tuned for Machine Translation evaluation) performs significantly worse on all metrics for both la"
P11-2069,P99-1044,0,0.0302307,"models it is necessary to use enough training material including minimal redundancy of words. To this end, we will be using monolingual corpora made up of multiply-translated sentences, allowing us to provide GIZA++ with all possible sentence pairs to improve the quality of its word alignments (note that following common practice we used symetrized alignments from the alignments in both directions). This constitutes an advantage for this technique that the following techniques working on each sentence pair independently do not have. Symbolic expression of linguistic variation The FASTR tool (Jacquemin, 1999) was designed to spot term variants in large corpora. Variants are described through metarules expressing how the morphosyntactic structure of a term variant can be derived from a given term by means of regular expressions on word categories. Paradigmatic variation can also be expressed by defining constraints between words to force them to belong to the same morphological or semantic family, both constraints relying on preexisting repertoires available for English and French. To compute candidate paraphrase pairs using FASTR, we first consider all the phrases from the first sentence and searc"
P11-2069,J10-3003,0,0.064709,", we present a novel way of tackling the monolingual alignment problem on pairs of sentential paraphrases by means of edit rate computation. In order to inform the edit rate, information in the form of subsentential paraphrases is provided by a range of techniques built for different purposes. We show that the tunable TER-PLUS metric from Machine Translation evaluation can achieve good performance on this task and that it can effectively exploit information coming from complementary sources. 1 Introduction The acquisition of subsentential paraphrases has attracted a lot of attention recently (Madnani and Dorr, 2010). Techniques are usually developed for extracting paraphrase candidates from specific types of corpora, including monolingual parallel corpora (Barzilay and McKeown, 2001), monolingual comparable corpora (Del´eger and Zweigenbaum, 2009), bilingual parallel corpora (Bannard and Callison-Burch, 2005), and edit histories of multi-authored text (Max and Wisniewski, 2010). These approaches face two main issues, which correspond to the typical measures of precision, or how appropriate the extracted paraphrases are, and of recall, or how many of the paraphrases present in a given corpus can be found"
P11-2069,max-wisniewski-2010-mining,1,0.799494,"tion can achieve good performance on this task and that it can effectively exploit information coming from complementary sources. 1 Introduction The acquisition of subsentential paraphrases has attracted a lot of attention recently (Madnani and Dorr, 2010). Techniques are usually developed for extracting paraphrase candidates from specific types of corpora, including monolingual parallel corpora (Barzilay and McKeown, 2001), monolingual comparable corpora (Del´eger and Zweigenbaum, 2009), bilingual parallel corpora (Bannard and Callison-Burch, 2005), and edit histories of multi-authored text (Max and Wisniewski, 2010). These approaches face two main issues, which correspond to the typical measures of precision, or how appropriate the extracted paraphrases are, and of recall, or how many of the paraphrases present in a given corpus can be found effectively. To start with, both measures are often hard to compute in practice, as 1) the definition of what makes an acceptable paraphrase pair is still a research question, and 2) it is often impractical to extract a complete set of acceptable paraphrases 395 from most resources. Second, as regards the precision of paraphrase acquisition techniques in particular,"
P11-2069,J04-4002,0,0.0177494,"acquisition, which we will only briefly introduce (see (Bouamor et al., 2010) for more details). As explained previously, we want to evaluate whether and how their candidate paraphrase pairs can be used to improve paraphrase acquisition on sentential paraphrases using TER- PLUS. We selected these three techniques for the complementarity of types of information that they use: statistical word alignment without a priori linguistic knowledge, symbolic expression of linguistic variation exploiting a priori linguistic knowledge, and syntactic similarity. Statistical Word Alignment The GIZA++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. While originally developped in the bilingual context of Machine Translation, nothing prevents building such models on monolingual corpora. However, in order to build reliable models it is necessary to use enough training material including minimal redundancy of words. To this end, we will be using monolingual corpora made up of multiply-translated sentences, allowing us to provide GIZA++ with all possible sentence pairs to improve the quality of its word alignments (note that following common practice w"
P11-2069,N03-1024,0,0.520245,"Missing"
P11-2069,N07-1051,0,0.0463616,"Missing"
P11-2069,P08-1089,0,0.0201834,"t at the subsentential level from sentential paraphrases and the possibility of informing this search with paraphrase candidates coming from other techniques. Our experiments have shown that in some circumstances some techniques have a good complementarity and manage to improve results significantly. We are currently studying hard-to-align subsentential paraphrases from the type of corpora we used in order to get a better understanding of the types of knowledge required to improve automatic acquisition of these units. Our future work also includes the acquisition of paraphrase patterns (e.g. (Zhao et al., 2008)) to generalize the acquired equivalence units to more contexts, which could be both used in applications and to attempt improving further paraphrase acquisition techniques. Integrating the use of patterns within an edit rate computation technique will however raise new difficulties. We are finally also in the process of conducting a careful study of the characteristics of the paraphrase pairs that each technique can extract with high confidence, so that we can improve our hybridation experiments by considering confidence values at the paraphrase level using Machine Learning. This way, we may"
P11-2069,P01-1008,0,\N,Missing
P15-2091,D10-1059,0,0.06254,"Missing"
P15-2091,P08-2040,0,0.0285784,"ture functions are difficult to integrate into the decoder mainly because they are not easily decomposable, very costly to compute and/or only available after complete hypotheses have been posited. Usually such complex features are used through the rescoring and reranking of the k-best translation hypotheses produced by the decoder (Och et al., 2004). Although this reranking pass is performed over the best part of the decoder search space, it is limited by the actual diversity expressed in the k-best list. Additionally, reranking being performed on a list generated by a simpler 2 Related Work Chen et al. (2008a; 2008b) expand the k-best list of the decoder using three methods. One of them involves re-decodings using models trained on the decoder k-best list to integrate posterior knowledge during the next re-decoding. The new k-best 554 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 554–559, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics list produced by the decoder is concatenated to the original one and then reranked with comp"
P15-2091,C08-1014,0,0.423489,"ture functions are difficult to integrate into the decoder mainly because they are not easily decomposable, very costly to compute and/or only available after complete hypotheses have been posited. Usually such complex features are used through the rescoring and reranking of the k-best translation hypotheses produced by the decoder (Och et al., 2004). Although this reranking pass is performed over the best part of the decoder search space, it is limited by the actual diversity expressed in the k-best list. Additionally, reranking being performed on a list generated by a simpler 2 Related Work Chen et al. (2008a; 2008b) expand the k-best list of the decoder using three methods. One of them involves re-decodings using models trained on the decoder k-best list to integrate posterior knowledge during the next re-decoding. The new k-best 554 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 554–559, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics list produced by the decoder is concatenated to the original one and then reranked with comp"
P15-2091,J07-1003,0,0.0105646,"on both directions. For both tasks we trained two strong baseline systems using data provided by WMT4 . Statistics about the training, development and testing data are presented in Table 1. Tasks Corpus news medical for both tasks Sentences Tokens (Fr-En) train dev test 12M 2,525 3,003 383M - 318M 73k - 65k 85k - 74k train dev test in-domain LM 4.9M 500 1,000 91M - 78M 12k - 10k 26k - 21k 146M - 78M LM • NNM: bilingual and monolingual neural network models with a structured output layer (SOUL) (Le et al., 2012) • POSLM: 6-gram POS language model • WPP: count-based word posterior probability (Ueffing and Ney, 2007) • TagRatio: ratio of translation hypothesis by number of source tokens tagged as: verb, noun or adjective • Syntax: depth, number of nodes and number of unary rules of the syntactic parse normalized by the hypothesis length (Carter and Monz, 2011) 2.5B - 6B • IBM1: IBM1 features (Och et al., 2004; Hildebrand and Vogel, 2008) Table 1: Data used in our experiments. 4.2 Part-of-speech tagging and syntactic parsing were respectively performed with the Stanford Part-of-speech Tagger (Toutanova and Manning, 2000) and the Shift-Reduce parser of Zhu et al. (2013). We report the individual performance"
P15-2091,P13-1043,0,0.0177412,"ased word posterior probability (Ueffing and Ney, 2007) • TagRatio: ratio of translation hypothesis by number of source tokens tagged as: verb, noun or adjective • Syntax: depth, number of nodes and number of unary rules of the syntactic parse normalized by the hypothesis length (Carter and Monz, 2011) 2.5B - 6B • IBM1: IBM1 features (Och et al., 2004; Hildebrand and Vogel, 2008) Table 1: Data used in our experiments. 4.2 Part-of-speech tagging and syntactic parsing were respectively performed with the Stanford Part-of-speech Tagger (Toutanova and Manning, 2000) and the Shift-Reduce parser of Zhu et al. (2013). We report the individual performance of each feature set in Table 2 and the Rerank performance when using all feature sets. As expected, the NNM feature set brings most of the improvements and attain by itself nearly the BLEU score of Rerank when using all feature sets for the news task with a gain of 1.4 and 1.1 BLEU respectively for En→Fr and Fr→En over the Moses MT system For our experiments we used the Moses phrasebased SMT toolkit (Koehn et al., 2007) with default settings and features, including the five features from the translation table, and kb-mira tuning (Cherry and Foster, 2012)."
P15-2091,N12-1047,0,0.0294731,"rser of Zhu et al. (2013). We report the individual performance of each feature set in Table 2 and the Rerank performance when using all feature sets. As expected, the NNM feature set brings most of the improvements and attain by itself nearly the BLEU score of Rerank when using all feature sets for the news task with a gain of 1.4 and 1.1 BLEU respectively for En→Fr and Fr→En over the Moses MT system For our experiments we used the Moses phrasebased SMT toolkit (Koehn et al., 2007) with default settings and features, including the five features from the translation table, and kb-mira tuning (Cherry and Foster, 2012). Rerank is trained using kb-mira on the 1,000-best list generated by Moses on the development set with the 4 http://www.statmt.org/wmt14 556 improvements of 3.0 and 4.0 BLEU respectively for Fr→En and En→Fr. These improvements illustrate the strong potential of our set of complex features to provide more accurate scores for translation hypotheses than the set of features used during the initial decoding. All studied configurations yield improvements with multi-pass Moses over the Moses baseline, showing the advantage of extracting from the main translation table misused bi-phrases according t"
P15-2091,N12-1059,0,0.0311206,"king pass using complex features, assuming that a better hypothesis can be very close to this seed hypothesis (Marie and Max, 2014). Nevertheless, this rewriting only explores a small search space, limited by the greedy search algorithm that concentrates on individual, local rewritings. Other works proposed methods to produce more diverse lists of hypotheses by iteratively encouraging the decoder to produce translations that are different from the previous one (Gimpel et al., 2013) or by making small changes to the scoring function to extract k-best lists from other parts of the search space (Devlin and Matsoukas, 2012). Some useful diversity can be obtained as these hypotheses can be combined using SMT system combination or help to better train reranking systems. But in spite of the introduction of more diversity, these methods do not guarantee that eventually lists containing hypotheses that are more relevant to complex features will be obtained. ferences. On the one hand, there are n-grams from the decoder one-best hypothesis that are not found any more in the Rerank one-best; on the other hand, there are n-grams that only exist in the Rerank one-best hypothesis. Since the decoder produces word alignments"
P15-2091,D13-1111,0,0.0192318,"ently, we proposed a rewriting system that explores in a greedy fashion the neighborhood of the one-best hypothesis found by the reranking pass using complex features, assuming that a better hypothesis can be very close to this seed hypothesis (Marie and Max, 2014). Nevertheless, this rewriting only explores a small search space, limited by the greedy search algorithm that concentrates on individual, local rewritings. Other works proposed methods to produce more diverse lists of hypotheses by iteratively encouraging the decoder to produce translations that are different from the previous one (Gimpel et al., 2013) or by making small changes to the scoring function to extract k-best lists from other parts of the search space (Devlin and Matsoukas, 2012). Some useful diversity can be obtained as these hypotheses can be combined using SMT system combination or help to better train reranking systems. But in spite of the introduction of more diversity, these methods do not guarantee that eventually lists containing hypotheses that are more relevant to complex features will be obtained. ferences. On the one hand, there are n-grams from the decoder one-best hypothesis that are not found any more in the Rerank"
P15-2091,D12-1108,0,0.0131289,"nfigurations. OUT: configuration with a translation table containing bi-phrases of the Moses 1-best not in the Rerank 1-best. IN: configuration with a translation table containing bi-phrases of the Rerank 1-best not in the Moses 1-best. For all configuration the main translation table is still used but does not contain the extracted bi-phrases. OUT iteration-specific translation tables (“IN and OUT”) yields a performance situated between using IN and OUT separately, but which still consistently improves over the baseline Rerank. 5 course coherence modelling over iterations (Ture et al., 2012; Hardmeier et al., 2012). Going further, we could study the effect of using other hypotheses instead of the Rerank one-best to perform the comparison with the Moses one-best hypothesis. For instance, we can reasonably expect that making this comparison with the output of a rewriting system, such as the one proposed in our previous work (Marie and Max, 2014), could extract more misused and useful bi-phrases on which to base our translation table partitioning since this rewriting system’s output is usually better than the Rerank one-best and not in the k-best list of the decoder. Discussion and future work We have pres"
P15-2091,2008.amta-srw.3,0,0.0412611,"Missing"
P15-2091,P07-2045,0,0.00378968,"ic parsing were respectively performed with the Stanford Part-of-speech Tagger (Toutanova and Manning, 2000) and the Shift-Reduce parser of Zhu et al. (2013). We report the individual performance of each feature set in Table 2 and the Rerank performance when using all feature sets. As expected, the NNM feature set brings most of the improvements and attain by itself nearly the BLEU score of Rerank when using all feature sets for the news task with a gain of 1.4 and 1.1 BLEU respectively for En→Fr and Fr→En over the Moses MT system For our experiments we used the Moses phrasebased SMT toolkit (Koehn et al., 2007) with default settings and features, including the five features from the translation table, and kb-mira tuning (Cherry and Foster, 2012). Rerank is trained using kb-mira on the 1,000-best list generated by Moses on the development set with the 4 http://www.statmt.org/wmt14 556 improvements of 3.0 and 4.0 BLEU respectively for Fr→En and En→Fr. These improvements illustrate the strong potential of our set of complex features to provide more accurate scores for translation hypotheses than the set of features used during the initial decoding. All studied configurations yield improvements with mul"
P15-2091,N12-1005,0,0.0236446,"l translation task (medical) and the WMT’11 news translation task (news) for the language pair FrEn on both directions. For both tasks we trained two strong baseline systems using data provided by WMT4 . Statistics about the training, development and testing data are presented in Table 1. Tasks Corpus news medical for both tasks Sentences Tokens (Fr-En) train dev test 12M 2,525 3,003 383M - 318M 73k - 65k 85k - 74k train dev test in-domain LM 4.9M 500 1,000 91M - 78M 12k - 10k 26k - 21k 146M - 78M LM • NNM: bilingual and monolingual neural network models with a structured output layer (SOUL) (Le et al., 2012) • POSLM: 6-gram POS language model • WPP: count-based word posterior probability (Ueffing and Ney, 2007) • TagRatio: ratio of translation hypothesis by number of source tokens tagged as: verb, noun or adjective • Syntax: depth, number of nodes and number of unary rules of the syntactic parse normalized by the hypothesis length (Carter and Monz, 2011) 2.5B - 6B • IBM1: IBM1 features (Och et al., 2004; Hildebrand and Vogel, 2008) Table 1: Data used in our experiments. 4.2 Part-of-speech tagging and syntactic parsing were respectively performed with the Stanford Part-of-speech Tagger (Toutanova"
P15-2091,D14-1133,1,0.907876,"on for Computational Linguistics list produced by the decoder is concatenated to the original one and then reranked with complex features, which yields improvements over a reranking performed on the original k-best list. The reranking pass is done out of the loop and the redecodings do not exploit the reranking result that used the complex features. Recently, we proposed a rewriting system that explores in a greedy fashion the neighborhood of the one-best hypothesis found by the reranking pass using complex features, assuming that a better hypothesis can be very close to this seed hypothesis (Marie and Max, 2014). Nevertheless, this rewriting only explores a small search space, limited by the greedy search algorithm that concentrates on individual, local rewritings. Other works proposed methods to produce more diverse lists of hypotheses by iteratively encouraging the decoder to produce translations that are different from the previous one (Gimpel et al., 2013) or by making small changes to the scoring function to extract k-best lists from other parts of the search space (Devlin and Matsoukas, 2012). Some useful diversity can be obtained as these hypotheses can be combined using SMT system combination"
P15-2091,P11-1063,0,0.0190661,"produce better k-best lists with Moses. Using in the same system both IN and 60 55 50 BLEU Moses 1-best Rerank 1-best 1,000-best average 1,000-best oracle 45 40 35 0 1 2 iteration 3 4 Figure 2: BLEU score evolution over iterations for the IN configuration on the test set of the medical En→Fr translation task. baseline. Among the other feature sets, POSLM performs well, especially for the medical task with an improvement of 0.3 and 0.5 BLEU for En→Fr and Fr→En, respectively. Some types of our complex features have already been used during decoding, although sometimes for a very important cost (Schwartz et al., 2011). Our feature sets are to be considered only as experimental parameters, as any other feature types usually used during reranking could also be used. Features medical En→Fr Fr→En news En→Fr Fr→En Moses 38.8 37.1 31.1 28.6 + MosesNorm + NNM + POSLM + WPP + TagRatio + Syntax + IBM1 38.9 41.9 39.2 39.1 38.9 38.8 39.1 37.2 38.9 37.7 37.1 37.3 37.2 37.2 31.1 32.5 31.1 31.2 31.1 31.2 30.9 28.7 29.8 28.9 28.6 28.8 28.9 28.8 Rerank 42.8 40.1 32.5 29.9 Table 2: Reranking results for each set of features added individually; Rerank uses the full set. 4.3 Results Table 3 presents our results for different"
P15-2091,W00-1308,0,0.175061,"al., 2012) • POSLM: 6-gram POS language model • WPP: count-based word posterior probability (Ueffing and Ney, 2007) • TagRatio: ratio of translation hypothesis by number of source tokens tagged as: verb, noun or adjective • Syntax: depth, number of nodes and number of unary rules of the syntactic parse normalized by the hypothesis length (Carter and Monz, 2011) 2.5B - 6B • IBM1: IBM1 features (Och et al., 2004; Hildebrand and Vogel, 2008) Table 1: Data used in our experiments. 4.2 Part-of-speech tagging and syntactic parsing were respectively performed with the Stanford Part-of-speech Tagger (Toutanova and Manning, 2000) and the Shift-Reduce parser of Zhu et al. (2013). We report the individual performance of each feature set in Table 2 and the Rerank performance when using all feature sets. As expected, the NNM feature set brings most of the improvements and attain by itself nearly the BLEU score of Rerank when using all feature sets for the news task with a gain of 1.4 and 1.1 BLEU respectively for En→Fr and Fr→En over the Moses MT system For our experiments we used the Moses phrasebased SMT toolkit (Koehn et al., 2007) with default settings and features, including the five features from the translation tab"
P15-2091,N12-1046,0,\N,Missing
P15-2091,N04-1021,0,\N,Missing
toney-etal-2008-evaluation,walker-etal-2000-evaluation,0,\N,Missing
W03-2204,C00-1036,0,\N,Missing
W03-2204,C94-1012,0,\N,Missing
W03-2204,W00-1404,0,\N,Missing
W03-2204,P98-2173,0,\N,Missing
W03-2204,C98-2168,0,\N,Missing
W04-0915,C02-1170,0,0.0144229,"ative to a normalization model could be achieved by text paraphrasing at the level of the sentence, but this is too specific to us. 2 Automatic analysis of the communicative content of a document in constrained domain Several approaches have already been experimented to analyze the content of documents in constrained domains, which can vary depending on the amount of surface analysis of the text. One type of approach uses information extraction techniques such as pattern matching that use strong predictions on the content and attempt to fill templates derived from a model of the domain (e.g. (Blanchon, 2002)), thus not giving too much importance to syntactic structure. Another type of approach first performs a syntactic analysis of the text, from which semantic dependencies can be extracted. The system presented in (Brun and Hag`ege, 2003) derives normalized predicates encoding the meaning of documents from semantic dependencies found by a robust parser. This allows obtaining identical semantic interpretations for paraphrases such as ProductX is a colorless, non flammable liquid and ProductX is a liquid that has no colour and that does not burn easily. These approaches require an encoding of temp"
W04-0915,W03-1606,0,0.0697192,"Missing"
W04-0915,W00-1404,0,0.0163281,"lementation of a prototype system for interactive document normalization based on the two presented approaches. 4 Interactive document normalization system Systems implementing controlled document authoring (Hartley and Paris, 1997) are based on an interaction with an author who makes semantic choices that define the content of a document, from which multilingual textual versions can be produced. Therefore, these systems integrate resources that can be used to represent document content and to generate textual versions of the documents. The MDA system developed at XRCE (Dymetman et al., 2000; Brun et al., 2000) uses a formalism inspired from Definite Clause Grammars (Pereira and Warren, 1980) that encodes both the abstract semantic syntax of well-formed documents and the concrete syntax for the documents in several languages.5 MDA grammars contain the definition of semantic objects of a given semantic type, which are used to build typed abstract semantic trees. Importantly, the formalism can encode the three levels for a normalization model that we described in our introduction: semantic objects can be of any granularity and can thus be communicative goals; the communicative structure is described b"
W04-0915,C00-1036,0,0.0133131,"n, we introduce our implementation of a prototype system for interactive document normalization based on the two presented approaches. 4 Interactive document normalization system Systems implementing controlled document authoring (Hartley and Paris, 1997) are based on an interaction with an author who makes semantic choices that define the content of a document, from which multilingual textual versions can be produced. Therefore, these systems integrate resources that can be used to represent document content and to generate textual versions of the documents. The MDA system developed at XRCE (Dymetman et al., 2000; Brun et al., 2000) uses a formalism inspired from Definite Clause Grammars (Pereira and Warren, 1980) that encodes both the abstract semantic syntax of well-formed documents and the concrete syntax for the documents in several languages.5 MDA grammars contain the definition of semantic objects of a given semantic type, which are used to build typed abstract semantic trees. Importantly, the formalism can encode the three levels for a normalization model that we described in our introduction: semantic objects can be of any granularity and can thus be communicative goals; the communicative stru"
W04-0915,P03-2017,1,0.886547,"e documents so that they become valid relative to the normalization model. Figure 3: Fuzzy inverted generation highest similarity score with the input document is then considered to be the most likely candidate. 3 Interactive validation of the correct communicative content Relying solely on information retrieval techniques to associate a normalized content representation to an input document is unfortunately unlikely to yield good results, even if linguistically-oriented techniques can improve accuracy (Arampatzis et al., 2000). We have advocated an interactive approach to text understanding (Dymetman et al., 2003) where the input text is used as a source of information to assist the user in re-authoring its content. Following fuzzy inverted generation, an interactive negotiation can take place between the system and its hypotheses (the candidate content representations) on the one hand, and a human expert on the second. A naive way would be to let the expert choose which hypothesis is correct based on the normalized text associated with each one of them. But this would be a tedious and error-prone process. Rather, underspecifications from analysis can be found by building a compact representation of th"
W04-0915,E03-3006,1,0.915152,"PhD grant. from a text, some semantic distinctions need not be recognized. For example, the two following sentences found in a drug leaflet may not carry significantly different communicative goals in spite of their clear semantic differences: • Consult your doctor in case of pregnancy before taking this product. • Consult a health professional in case of pregnancy before taking this product. We have identified a domain of application, document normalization, where text interpretation can be limited in many cases to the interpretation of a text in terms of the communicative goals it conveys (Max, 2003a). We have defined document normalization as the process that first derives the normalized communicative content of a text in a constrained domain (e.g. drug leaflets), and then generates the normalized version of the text in the language of the original document. We considered three levels in a normalization model for documents in constrained domain: 1. Communicative goals: the communicative goals that can appear in a document in constrained domain belong to a predefined repertoire. 2. Communicative structure: the communicative structure describes the content of a document in terms of compat"
W04-1710,W99-0807,0,0.0218121,"let (and possibly tree animations), but also the insertion of exercises within online course material. We 13 Collaborative projects, such as the Papillon project for multilingual lexical resources, show that this approach can work if submitters can also benefit from the submissions of other contributors. plan to use this for the tutorial of the program. On the content side, several ideas have been submitted and will be implemented depending on time. Notably, it seems particularly interesting to provide actual linguistic data from corpora to students from which grammars can be inferred, as in (Borin and Dahllof, 1999). A new exercise type will ask students to write a grammar accounting for a given small corpus, which could already be morphologically annotated or not. Lexicons will be separated from grammars, in order to make them reusable when possible. Feature structures will also be supported, both for the edition of grammars and for the validation of syntactic derivations. A number of new features concern the graphical display of trees. Notably, it will be possible to collapse or expand subtrees (using the triangle notation), and to draw trees top-down with the terminal symbols immediately under the non"
W04-1710,W99-0803,0,0.026414,"troduced to the intricacies of grammar are no less concerned than any others. A typical exercise consists in asking students to analyze a sentence by means of its description as a syntactic tree. In introductory courses, either a context-free grammar is given to them before the exercise begins, or they have to build one of their own that can be used to analyze the sentence given. Obviously, the more exercises look like challenging “games” and the more they are easy to use and accessible, the more likely students are to invest time and effort in trying to do them (see e.g. (van Halteren, 2002; Gibbon and Carson-Berndsen, 1999)). If they spend a lot of time drawing, erasing parts of their trees, drawing them again or correcting them, and then waiting for minutes before their teaching assistant is available again, they may not find the whole exercise very captivating very long. But this type of exercise is essential to understand how the most basic of grammar formalism works and therefore to build a solid ground for the study of language analysis. Computers play a growing role in education, as the number of workshops dedicated to eLearning and related domains shows. While many institutions experience financial cuts,"
W04-1710,W02-0101,0,0.0688785,"Missing"
W04-1710,W02-0103,0,\N,Missing
W08-1911,P07-1044,0,0.194511,"is more important to her. For example, (Ferret and Zock, 2006) have proposed to present results from a dictionary enriched with topical associations in chunks to allow for categorial search. There will be cases where the user may find acceptable only grammatical results, while in other cases the user might accept agrammatical results provided they contain interesting suggestions. Moreover, it seems extremely important that result ranking can take into account the phrase substitution into the original context. s(p2 , p1 , C) = X λm hm (p1 , p2 , C) (2) m∈M 3.1 Control over fluency As noted by (Mutton et al., 2007), the notion of sentence-level fluency is not uniformely agreed upon, and its evaluation by human judges is sometimes found subjective, but in practice judges can obtain high levels of agreement about what can be considered fluent or not. Like (Callison-Burch, 2007), we can use a language model (L M) to assess the local fluency of a sentence after a phrase has been substituted with a rephrasing. A degradation in score (with a fluent original sentence) can indicate that the rephrasing segment should be adapted to the sentence, and/or that the sentence itself should be modified in order to integ"
W08-1911,P05-1074,0,0.579537,"ne raison → je suis d’accord avec vous), in specific grammatical contexts (e.g. pouvoir accueillir dans de bonnes conditions les pays → comme il se doit) and/or pragmatic contexts (e.g. c’est un bon d´ebut → nous partons du bon pied). tents of phrases, and the fact that their translations are not conditioned on their context. If phrase extraction is performed in two opposite directions, then it is possible to find the possible translations of a given phrase (and their conditional probabilities), and then to translate back those phrases into the original language. In this approach proposed by (Bannard and Callison-Burch, 2005), the second language acts as a pivot, as illustrated on figure 1. Because of the nature of the possible alignments, this pivot can represent various senses, which in context can be equivalent or comparable to that of the original phrase. In turn, the same phenomena can take place when translating back from the pivot phrases to the original language, and the resulting rephrasings can be equivalent or comparable in meaning to that of the original phrase in some context, may also be incomplete and/or require other changes in the rephrased sentence. Bannard and Callison-Burch have defined a parap"
W08-1911,J03-1002,0,0.0110014,"arcu, 2006)). In order to extract a comprehensive phrase lexicon, a very large number of sentences should be compared to extract potential rephrasings, which furthermore may often correspond to phrases that are too remotely connected. Parallel corpora provide the interesting advantage that it is reasonable to assume that elements from one side of the corpus should be aligned to elements on the other side, and that associations of elements can be reinforced by the number of times they occur in the corpus. Various approaches for word alignment from parallel corpora have been proposed (see e.g. (Och and Ney, 2003)), and the phrase-based approach to Statistical Machine Translation (Koehn et al., 2003) has led to the development of heuristics for obtaining alignments between phrases of any number of words. • The replacing element may require syntactic changes of the matrix, i.e. the text in which it is embedded. This occurs if the source word and the target word have different syntactic requirements, and this can be seen as a good reason to replace entire sentences, or at least sentence fragments. This assumes a pattern dictionary, where patterns achieving the same conceptual goal are grouped together. I"
W08-1911,N03-1003,0,0.231212,"d the relevant lexicon entries. In the latter case, the similarity used to select examples could take the context of the phrases into account in terms of dependency relationships. ing of take his place, due to the possessive determiner. The actual meaning of words depends on the context in which they are used. The work done by the team of Gross on lexicon-grammar (e.g. (Gross, 1984)) showed that a relatively small set of clause patterns and syntactic constraints suffices to cover most of common French. Comparable monolingual corpora have been used for automatic paraphrasing. Barzilay and Lee (Barzilay and Lee, 2003) learned paraphrasing patterns as pairs of word lattices, which are then used to produce sentence level paraphrases. Their corpus contained news agency articles on the same events, which allows precise sentence paraphrasing, but on a small sets of phenomena and for a limited domain. As sentential paraphrasing is more likely to alter meaning, Quirk et al. (Quirk et al., 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. Their corpus consisted of monolingual sentences extracted from a comparable corpus that were automatically aligned so as to allow aligned phras"
W08-1911,N03-1024,0,0.136956,"rns as pairs of word lattices, which are then used to produce sentence level paraphrases. Their corpus contained news agency articles on the same events, which allows precise sentence paraphrasing, but on a small sets of phenomena and for a limited domain. As sentential paraphrasing is more likely to alter meaning, Quirk et al. (Quirk et al., 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. Their corpus consisted of monolingual sentences extracted from a comparable corpus that were automatically aligned so as to allow aligned phrase extraction. Pang et al. (Pang et al., 2003) used parallel monolingual corpora built from news stories that had been independantly translated several times to learn lattices from a syntax-based alignment process. Bannard and Callison-Burch (Bannard and Callison-Burch, 2005) proposed to use pivot translation for paraphrasing phrases. Fujita (Fujita, 2005) proposed a transfer-and-revision framework using linguistic knowledge for generating paraphrases in Japanese and a model for error detection. At the lexical level, a recent evaluation on English lexical substitution was held (McCarthy and Navigli, 2007) in which systems had to find lexi"
W08-1911,W04-3219,0,0.164899,"984)) showed that a relatively small set of clause patterns and syntactic constraints suffices to cover most of common French. Comparable monolingual corpora have been used for automatic paraphrasing. Barzilay and Lee (Barzilay and Lee, 2003) learned paraphrasing patterns as pairs of word lattices, which are then used to produce sentence level paraphrases. Their corpus contained news agency articles on the same events, which allows precise sentence paraphrasing, but on a small sets of phenomena and for a limited domain. As sentential paraphrasing is more likely to alter meaning, Quirk et al. (Quirk et al., 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. Their corpus consisted of monolingual sentences extracted from a comparable corpus that were automatically aligned so as to allow aligned phrase extraction. Pang et al. (Pang et al., 2003) used parallel monolingual corpora built from news stories that had been independantly translated several times to learn lattices from a syntax-based alignment process. Bannard and Callison-Burch (Bannard and Callison-Burch, 2005) proposed to use pivot translation for paraphrasing phrases. Fujita (Fujita, 2005) proposed a transfer"
W08-1911,2007.tmi-papers.28,0,0.0292629,"Missing"
W08-1911,W04-2105,1,0.828682,"ries have called for, candidate, the, etc.) Different target phrases associated with a given source phrase can either represent paraphrases or phrases with different meanings. Among the limitations of this type of phrasal alignments are their inability to model non-consecutive words and to generalize the conLexicon of phrase rephrasings Dictionaries and semantic resources such as thesauri can be used to find words by following links of different kinds from a given entry point. WordNet (Fellbaum, 1998) is one such resource. For a proposal of other kinds of links and navigational aids see also (Zock and Bilac, 2004; Zock, 2006; Zock, 2007). Words are the traditional units that people expect to find in dictionaries. Whereas some types of dictionaries can contain multiword expressions, such as compound nouns and terms, those correspond to linguistically-motivated units. In order to rephrase phrases of any type with a dictionary, a writer may have to look up several words, combine various information and validate the result using her experience of the language or throught the use of a concordancer. Moreover, dictionary lookups 78 in all contexts (e.g. je vous donne raison → je suis d’accord avec vous), in"
W08-1911,P06-1036,1,0.837029,"sing English as pivot. puted, where M is the set of models used, hm is the logarithm of the normalized score of a model P and λm its weight (with m∈M λm = 1), and C is the original sentence and the placeholder for the rephrased phrase. to discard some of them. The proposed ranking should reflect as best as possible the preferences of the user for the task at hand in order to minimize reading time and maintain the user’s interest in using the phrase lexicon. It is essential to give the user some control over how the results are returned depending on what is more important to her. For example, (Ferret and Zock, 2006) have proposed to present results from a dictionary enriched with topical associations in chunks to allow for categorial search. There will be cases where the user may find acceptable only grammatical results, while in other cases the user might accept agrammatical results provided they contain interesting suggestions. Moreover, it seems extremely important that result ranking can take into account the phrase substitution into the original context. s(p2 , p1 , C) = X λm hm (p1 , p2 , C) (2) m∈M 3.1 Control over fluency As noted by (Mutton et al., 2007), the notion of sentence-level fluency is"
W08-1911,P84-1058,0,0.487594,"s proposed bilingual phrase lexicon could include rephrasing memory features to learn from interaction with the user, and concordancing features to display the context of use in the bilingual corpus of the segments used to build the relevant lexicon entries. In the latter case, the similarity used to select examples could take the context of the phrases into account in terms of dependency relationships. ing of take his place, due to the possessive determiner. The actual meaning of words depends on the context in which they are used. The work done by the team of Gross on lexicon-grammar (e.g. (Gross, 1984)) showed that a relatively small set of clause patterns and syntactic constraints suffices to cover most of common French. Comparable monolingual corpora have been used for automatic paraphrasing. Barzilay and Lee (Barzilay and Lee, 2003) learned paraphrasing patterns as pairs of word lattices, which are then used to produce sentence level paraphrases. Their corpus contained news agency articles on the same events, which allows precise sentence paraphrasing, but on a small sets of phenomena and for a limited domain. As sentential paraphrasing is more likely to alter meaning, Quirk et al. (Quir"
W08-1911,N03-1017,0,0.0204928,"Missing"
W08-1911,2005.mtsummit-papers.11,0,0.0140819,"e and its rephrasing, we use a model (L EM) that returns a proportion of lemmas for full words that only belong to a rephrasing over all such lemmas for an initial phrase and its rephrasing (see (Max, 2008)). 4 Experiments and evaluation We carried out an evaluation on the local rephrasing of French sentences, using English as the pivot language.2 We extracted phrase alignments of up to 7 word forms using the Giza++ alignment tool (Och and Ney, 2003) and the grow-diag-final-and heuristics described in (Koehn et al., 2003) on 948,507 sentences of the French-English part of the Europarl corpus (Koehn, 2005) and obtained some 42 million phrase pairs for which probabilities were estimated using maximum likelihood estimation. Statistics for the extracted lexicons are reported on figure 2. Entries of the monolingual phrase lexicon are built dynamically from the entries of the monolingual lexicons. For the L M model, we used a 5-gram language model trained on the French part of the corpus using Kneser-Ney smoothing. The robust parser for French S YNTEX (Bourigault et al., 2005) was used to obtain lemmas for word and labeled dependency relationships between words, used respectively for the L EM and D"
W08-1911,S07-1009,0,0.0349439,"ow aligned phrase extraction. Pang et al. (Pang et al., 2003) used parallel monolingual corpora built from news stories that had been independantly translated several times to learn lattices from a syntax-based alignment process. Bannard and Callison-Burch (Bannard and Callison-Burch, 2005) proposed to use pivot translation for paraphrasing phrases. Fujita (Fujita, 2005) proposed a transfer-and-revision framework using linguistic knowledge for generating paraphrases in Japanese and a model for error detection. At the lexical level, a recent evaluation on English lexical substitution was held (McCarthy and Navigli, 2007) in which systems had to find lexical synonyms and disambiguate the context. 6 There are several open issues to the presented work. Important issues are where the phrases can come from and the bias introduced by the resource used. Using a bilingual corpora such as the Europarl corpus with this pivot approach yields both generic and domain/genre-specific rephrasings, and it is important to be able to determine their appropriate context of use. It would also be interesting to investigate enriching this framework with phrases learnt from monolingual corpora from a given domain or genre, and to us"
W08-1911,P06-1011,0,0.0249205,"that a choice is appropriate for a given context, which can be quite difficult, for example when writing in a second language. by another does not have any consequences overall. This is often the case when a word is replaced by its synonym or a similar word. • An entire expression or sentence is replaced by its equivalent. In this case the problem is generally to obtain a good fit with regard to the surrounding text, the replacing unit being well-formed by definition. One way of obtaining phrase rephrasings is by looking at phrases that occur in similar contexts in a monolingual corpus (e.g. (Munteanu and Marcu, 2006)). In order to extract a comprehensive phrase lexicon, a very large number of sentences should be compared to extract potential rephrasings, which furthermore may often correspond to phrases that are too remotely connected. Parallel corpora provide the interesting advantage that it is reasonable to assume that elements from one side of the corpus should be aligned to elements on the other side, and that associations of elements can be reinforced by the number of times they occur in the corpus. Various approaches for word alignment from parallel corpora have been proposed (see e.g. (Och and Ney"
W09-0417,2007.tmi-papers.28,0,0.0611781,"Missing"
W09-0417,2007.mtsummit-papers.11,0,0.0245406,"ights, since they dispense with the lexical reordering model; these weights were tuned on the same dataset, using an in-house implementation of the simplex algorithm. 3 3.1 of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f , C(f )), where C(f ) is some contextual information about the source phrase f . An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): Extensions A context-aware system In phrase-based translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate trans"
W09-0417,J04-2004,0,0.0544024,"mate such large LMs, a vocabulary was first defined for both languages by including all tokens in the WMT parallel data. This initial vocabulary of 130K words was then extended by adding the most frequent words observed in the additional training data. This procedure yielded a vocabulary of one million words in both languages. 2.5 A N-code baseline N-code implements the n-gram-based approach to Statistical Machine Translation (Mariño et al., 2006). In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training such a model requires to reorder source sentences so as to match the target word order. This is also performed via a stochastic finite-state reordering model, which uses part-of-speech information to generalise reordering patterns beyond lexical regularities. The reordering model is trained on a version of the parallel corpora where the source sentences have been reordered via the unfold heuristics (Crego and Mariño, 2007). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models a"
W09-0417,P96-1041,0,0.135168,"es so as to match the target word order. This is also performed via a stochastic finite-state reordering model, which uses part-of-speech information to generalise reordering patterns beyond lexical regularities. The reordering model is trained on a version of the parallel corpora where the source sentences have been reordered via the unfold heuristics (Crego and Mariño, 2007). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models and bilingual tuple models built using Kneser-Ney backoff (Chen and Goodman, 1996). The maximum tuple size was also set to 7. Language model training The training data were divided into several sets based on dates on genres (resp. 7 and 9 sets for English and French). On each set, a standard 4-gram LM was estimated from the 1M word vocabulary with in-house tools using absolute discounting interpolated with lower order models. The resulting LMs were then linearly interpolated using interpolation coefficients chosen so as to minimise perplexity of the development set (dev2009a). Due to memory limitations, the final LMs were pruned using perplexity as pruning criterion. 2.6 Tu"
W09-0417,W08-0310,1,0.911154,"Missing"
W09-0417,W08-0302,0,0.0772343,"f the simplex algorithm. 3 3.1 of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f , C(f )), where C(f ) is some contextual information about the source phrase f . An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): Extensions A context-aware system In phrase-based translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate translation for a source phrase based on the target context. In fact, attempts at using source contexts in phrase-based SMT have to date failed to sho"
W09-0417,P07-2045,0,0.0111904,"Missing"
W09-0417,2008.eamt-1.17,1,0.839871,"ense with the lexical reordering model; these weights were tuned on the same dataset, using an in-house implementation of the simplex algorithm. 3 3.1 of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f , C(f )), where C(f ) is some contextual information about the source phrase f . An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): Extensions A context-aware system In phrase-based translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate translation for a source"
W09-0417,J06-4004,1,0.915103,"Missing"
W09-0417,P03-1021,0,0.00849652,"ts based on dates on genres (resp. 7 and 9 sets for English and French). On each set, a standard 4-gram LM was estimated from the 1M word vocabulary with in-house tools using absolute discounting interpolated with lower order models. The resulting LMs were then linearly interpolated using interpolation coefficients chosen so as to minimise perplexity of the development set (dev2009a). Due to memory limitations, the final LMs were pruned using perplexity as pruning criterion. 2.6 Tuning procedure The Moses-based systems were tuned using the implementation of minimum error rate training (MERT) (Och, 2003) distributed with the Moses decoder, using the development corpus (dev2009a). For the context-less systems, tuning concerned the 14 usual weights; tuning the Out of vocabulary word and perplexity To evaluate our vocabulary and LMs, we used the official devtest and test sets. The out-of-vocabulary (OOV) rate was drastically reduced by increasing 101 22 weights of the context-aware systems (see 3.1) proved to be much more challenging, and the weights used in our submissions are probably far from optimal. The N-code systems only rely on 9 weights, since they dispense with the lexical reordering m"
W09-0417,E03-1076,0,\N,Missing
W09-0417,C08-1098,0,\N,Missing
W09-0417,E09-3008,0,\N,Missing
W09-0417,J04-2003,0,\N,Missing
W09-0417,H05-1085,0,\N,Missing
W09-2503,2005.mtsummit-papers.11,0,0.0228347,"f full word lemmas for the original fragment and the paraphrased fragment, at least one lemma should not belong to both sets9 ; • neither the original fragment nor its paraphrase must be included into the other (only taking full words into account). 4 Experiments We have conducted experiments motivated by a text revision task that we report in this section by describing our baseline and context-aware subsentential paraphrasing systems and the results of a small-scale manual evaluation. 4.1 Data and systems We built two-way French-English SMT systems using 188,115 lines of the Europarl corpus (Koehn, 2005) of parliamentary debates with moses (Koehn et al., 2007) 10 . Our corpus was analyzed by the XIP robust parser (A¨ıt-Mokhtar et al., 2002) and its output tokenization was used. We built standard systems, as well as a contextual system for French→English as described in section 3.2 using an additional contextual score obTarget context for pivot SMT When decoding from the pivot hypothesis, we force our decoder to use provided sentence prefix and suffix corresponding to the “envelope” of the original fragment. Target context will thus be taken into account by the decoder. Furthermore, based on t"
W09-2503,N03-1003,0,0.0762979,"mar. These works were not motivated by the generation of high-quality paraphrases that could, for example, be reused in documents. The lack of structural information, the local nature of the paraphrasing performed and the fact that the context of the original sentences was not taken into account in the phrase-based approaches make it difficult to control meaning preservation during paraphrasing. 2 Related work Different sources have been considered for paraphrase acquisition techniques. (Pang et al., 2003), for example, apply syntactic fusion to multiple translations of individual sentences. (Barzilay and Lee, 2003; Dolan et al., 2004) acquire short paraphrases from comparable corpora, while (Bhagat and Ravichandran, 2008) considered the issue of acquiring short paraphrase patterns from huge amounts of comparable corpora. (Bannard and Callison-Burch, 2005) introduced a pivot approach to acquire short paraphrases from multilingual parallel corpora, a resource much more readily available than their monolingual counterpart. (Zhao et al., 2008b) acquire paraphrase patterns from bilingual corpora and report the various types obtained.3 (Callison-Burch, 2008) improves the pivot paraphrase acquisition techniqu"
W09-2503,W07-0716,0,0.148314,"e and present our future work in section 5. This family of works considered the acquisition of short paraphrases and their use in local paraphrasing of known units. Several works have tackled full sentence paraphrasing as a monolingual translation task relying on Statistical Machine Translation (SMT). For instance, (Quirk et al., 2004) used a phrase-based SMT decoder that uses local paraphrases acquired from comparable corpora to produce monotone sentential paraphrases. (Zhao et al., 2008a) acquired monolingual biphrases from various sources and used them with a phrase-based SMT decoder, and (Madnani et al., 2007) combined rules of their hierarchical decoders by pivot to obtain a monolingual grammar. These works were not motivated by the generation of high-quality paraphrases that could, for example, be reused in documents. The lack of structural information, the local nature of the paraphrasing performed and the fact that the context of the original sentences was not taken into account in the phrase-based approaches make it difficult to control meaning preservation during paraphrasing. 2 Related work Different sources have been considered for paraphrase acquisition techniques. (Pang et al., 2003), for"
W09-2503,2008.iwslt-papers.1,0,0.0195976,"disambiguate phrases using a state-ofthe-art WSD classifier, and (Stroppa et al., 2007) use a global memory-based classifier to find appropriate phrase translations in context. Context is often defined as local linguistic features such as surrounding words and their part-of-speech, but some works have experimented with more syntactic features (e.g. (Gimpel and Smith, 2008; Max et al., 2008; Haque et al., 2009)). Using an intermediate pivot language with bilingual translation in which a given language pair is low-resourced has led to improvements in translation performance (Wu and Wang, 2007; Bertoldi et al., 2008), but to our knowledge this approach has not been applied to full sentence paraphrasing. Several reasons may explain this, in particular the relative low quality of current MT approaches on full sentence translation, and the difficulties in controlling what is paraphrased and how. 3 The types of their paraphrase patterns are the following (numbers in parentheses indicate frequency in their database): phrase replacements (267); trivial changes (79); structural paraphrases (71); phrase reorderings (56); and addition of deletion of information that are claimed to not alter meaning (27). 19 3 Cont"
W09-2503,2008.eamt-1.17,1,0.924861,"e in particular proper Word Sense Disambiguation (WSD) is required in many cases. A variety of works have integrated context with some success into phrase-based and hierarchical decoders. For example, (Carpuat and Wu, 2007) disambiguate phrases using a state-ofthe-art WSD classifier, and (Stroppa et al., 2007) use a global memory-based classifier to find appropriate phrase translations in context. Context is often defined as local linguistic features such as surrounding words and their part-of-speech, but some works have experimented with more syntactic features (e.g. (Gimpel and Smith, 2008; Max et al., 2008; Haque et al., 2009)). Using an intermediate pivot language with bilingual translation in which a given language pair is low-resourced has led to improvements in translation performance (Wu and Wang, 2007; Bertoldi et al., 2008), but to our knowledge this approach has not been applied to full sentence paraphrasing. Several reasons may explain this, in particular the relative low quality of current MT approaches on full sentence translation, and the difficulties in controlling what is paraphrased and how. 3 The types of their paraphrase patterns are the following (numbers in parentheses indica"
W09-2503,P08-1077,0,0.0219056,"ple, be reused in documents. The lack of structural information, the local nature of the paraphrasing performed and the fact that the context of the original sentences was not taken into account in the phrase-based approaches make it difficult to control meaning preservation during paraphrasing. 2 Related work Different sources have been considered for paraphrase acquisition techniques. (Pang et al., 2003), for example, apply syntactic fusion to multiple translations of individual sentences. (Barzilay and Lee, 2003; Dolan et al., 2004) acquire short paraphrases from comparable corpora, while (Bhagat and Ravichandran, 2008) considered the issue of acquiring short paraphrase patterns from huge amounts of comparable corpora. (Bannard and Callison-Burch, 2005) introduced a pivot approach to acquire short paraphrases from multilingual parallel corpora, a resource much more readily available than their monolingual counterpart. (Zhao et al., 2008b) acquire paraphrase patterns from bilingual corpora and report the various types obtained.3 (Callison-Burch, 2008) improves the pivot paraphrase acquisition technique by using syntactic constraints at the level of constituents during phrase extraction. This works also uses s"
W09-2503,C08-1013,0,0.104979,"Missing"
W09-2503,D08-1021,0,0.592705,"e aurelien.max@limsi.fr Abstract when a word can be substituted with a synonym is a complex issue (Connor and Roth, 2007). When attempting paraphrasing on a higher level, such as arbitrary phrases or full sentences (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Zhao et al., 2008a), a first issue concerns the acquisition of elementary units, which in the general case do not exist in predefined dictionaries. Some paraphrasing strategy must then follow, which may consider the context of a substitution to guide the selection of appropriate units (Callison-Burch, 2008; Max, 2008). An important limitation to this family of works is the scarcity of corpora that can be used as reliable supervised training data. Indeed, strictly parallel sentence pairs, for instance, are not naturally produced in human activities.1 As a consequence, works on paraphrasing have recourse to costly human evaluation procedures, and an objective of automatic evaluation metrics is to rely on as little gold standard data as possible (CallisonBurch et al., 2008). A text revision task is an application of paraphrase generation where context may be used in an effective way. When a local"
W09-2503,2001.mtsummit-papers.46,0,0.344723,"Missing"
W09-2503,2007.mtsummit-papers.11,0,0.0267122,"al corpora and report the various types obtained.3 (Callison-Burch, 2008) improves the pivot paraphrase acquisition technique by using syntactic constraints at the level of constituents during phrase extraction. This works also uses syntactic constraints during phrase substitution, resulting in improvements in both grammatContext has been shown to play a crucial role in Machine Translation, where in particular proper Word Sense Disambiguation (WSD) is required in many cases. A variety of works have integrated context with some success into phrase-based and hierarchical decoders. For example, (Carpuat and Wu, 2007) disambiguate phrases using a state-ofthe-art WSD classifier, and (Stroppa et al., 2007) use a global memory-based classifier to find appropriate phrase translations in context. Context is often defined as local linguistic features such as surrounding words and their part-of-speech, but some works have experimented with more syntactic features (e.g. (Gimpel and Smith, 2008; Max et al., 2008; Haque et al., 2009)). Using an intermediate pivot language with bilingual translation in which a given language pair is low-resourced has led to improvements in translation performance (Wu and Wang, 2007;"
W09-2503,N03-1024,0,0.0861051,"(Madnani et al., 2007) combined rules of their hierarchical decoders by pivot to obtain a monolingual grammar. These works were not motivated by the generation of high-quality paraphrases that could, for example, be reused in documents. The lack of structural information, the local nature of the paraphrasing performed and the fact that the context of the original sentences was not taken into account in the phrase-based approaches make it difficult to control meaning preservation during paraphrasing. 2 Related work Different sources have been considered for paraphrase acquisition techniques. (Pang et al., 2003), for example, apply syntactic fusion to multiple translations of individual sentences. (Barzilay and Lee, 2003; Dolan et al., 2004) acquire short paraphrases from comparable corpora, while (Bhagat and Ravichandran, 2008) considered the issue of acquiring short paraphrase patterns from huge amounts of comparable corpora. (Bannard and Callison-Burch, 2005) introduced a pivot approach to acquire short paraphrases from multilingual parallel corpora, a resource much more readily available than their monolingual counterpart. (Zhao et al., 2008b) acquire paraphrase patterns from bilingual corpora an"
W09-2503,W04-3219,0,0.0803944,"and context-aware Machine Translation. We describe the main characteristics of our approach to subsentential paraphrasing in section 3. We then describe an evaluation protocol for evaluating our proposal and report the results of a human evaluation in section 4. We finally conclude and present our future work in section 5. This family of works considered the acquisition of short paraphrases and their use in local paraphrasing of known units. Several works have tackled full sentence paraphrasing as a monolingual translation task relying on Statistical Machine Translation (SMT). For instance, (Quirk et al., 2004) used a phrase-based SMT decoder that uses local paraphrases acquired from comparable corpora to produce monotone sentential paraphrases. (Zhao et al., 2008a) acquired monolingual biphrases from various sources and used them with a phrase-based SMT decoder, and (Madnani et al., 2007) combined rules of their hierarchical decoders by pivot to obtain a monolingual grammar. These works were not motivated by the generation of high-quality paraphrases that could, for example, be reused in documents. The lack of structural information, the local nature of the paraphrasing performed and the fact that"
W09-2503,2007.tmi-papers.28,0,0.0762429,"Missing"
W09-2503,C04-1051,0,0.0795555,"t motivated by the generation of high-quality paraphrases that could, for example, be reused in documents. The lack of structural information, the local nature of the paraphrasing performed and the fact that the context of the original sentences was not taken into account in the phrase-based approaches make it difficult to control meaning preservation during paraphrasing. 2 Related work Different sources have been considered for paraphrase acquisition techniques. (Pang et al., 2003), for example, apply syntactic fusion to multiple translations of individual sentences. (Barzilay and Lee, 2003; Dolan et al., 2004) acquire short paraphrases from comparable corpora, while (Bhagat and Ravichandran, 2008) considered the issue of acquiring short paraphrase patterns from huge amounts of comparable corpora. (Bannard and Callison-Burch, 2005) introduced a pivot approach to acquire short paraphrases from multilingual parallel corpora, a resource much more readily available than their monolingual counterpart. (Zhao et al., 2008b) acquire paraphrase patterns from bilingual corpora and report the various types obtained.3 (Callison-Burch, 2008) improves the pivot paraphrase acquisition technique by using syntactic"
W09-2503,P07-1108,0,0.0148343,"rpuat and Wu, 2007) disambiguate phrases using a state-ofthe-art WSD classifier, and (Stroppa et al., 2007) use a global memory-based classifier to find appropriate phrase translations in context. Context is often defined as local linguistic features such as surrounding words and their part-of-speech, but some works have experimented with more syntactic features (e.g. (Gimpel and Smith, 2008; Max et al., 2008; Haque et al., 2009)). Using an intermediate pivot language with bilingual translation in which a given language pair is low-resourced has led to improvements in translation performance (Wu and Wang, 2007; Bertoldi et al., 2008), but to our knowledge this approach has not been applied to full sentence paraphrasing. Several reasons may explain this, in particular the relative low quality of current MT approaches on full sentence translation, and the difficulties in controlling what is paraphrased and how. 3 The types of their paraphrase patterns are the following (numbers in parentheses indicate frequency in their database): phrase replacements (267); trivial changes (79); structural paraphrases (71); phrase reorderings (56); and addition of deletion of information that are claimed to not alter"
W09-2503,W08-0302,0,0.111444,"achine Translation, where in particular proper Word Sense Disambiguation (WSD) is required in many cases. A variety of works have integrated context with some success into phrase-based and hierarchical decoders. For example, (Carpuat and Wu, 2007) disambiguate phrases using a state-ofthe-art WSD classifier, and (Stroppa et al., 2007) use a global memory-based classifier to find appropriate phrase translations in context. Context is often defined as local linguistic features such as surrounding words and their part-of-speech, but some works have experimented with more syntactic features (e.g. (Gimpel and Smith, 2008; Max et al., 2008; Haque et al., 2009)). Using an intermediate pivot language with bilingual translation in which a given language pair is low-resourced has led to improvements in translation performance (Wu and Wang, 2007; Bertoldi et al., 2008), but to our knowledge this approach has not been applied to full sentence paraphrasing. Several reasons may explain this, in particular the relative low quality of current MT approaches on full sentence translation, and the difficulties in controlling what is paraphrased and how. 3 The types of their paraphrase patterns are the following (numbers in"
W09-2503,P08-1116,0,0.263825,"valuation protocol for evaluating our proposal and report the results of a human evaluation in section 4. We finally conclude and present our future work in section 5. This family of works considered the acquisition of short paraphrases and their use in local paraphrasing of known units. Several works have tackled full sentence paraphrasing as a monolingual translation task relying on Statistical Machine Translation (SMT). For instance, (Quirk et al., 2004) used a phrase-based SMT decoder that uses local paraphrases acquired from comparable corpora to produce monotone sentential paraphrases. (Zhao et al., 2008a) acquired monolingual biphrases from various sources and used them with a phrase-based SMT decoder, and (Madnani et al., 2007) combined rules of their hierarchical decoders by pivot to obtain a monolingual grammar. These works were not motivated by the generation of high-quality paraphrases that could, for example, be reused in documents. The lack of structural information, the local nature of the paraphrasing performed and the fact that the context of the original sentences was not taken into account in the phrase-based approaches make it difficult to control meaning preservation during par"
W09-2503,2009.eamt-1.32,0,0.0125648,"oper Word Sense Disambiguation (WSD) is required in many cases. A variety of works have integrated context with some success into phrase-based and hierarchical decoders. For example, (Carpuat and Wu, 2007) disambiguate phrases using a state-ofthe-art WSD classifier, and (Stroppa et al., 2007) use a global memory-based classifier to find appropriate phrase translations in context. Context is often defined as local linguistic features such as surrounding words and their part-of-speech, but some works have experimented with more syntactic features (e.g. (Gimpel and Smith, 2008; Max et al., 2008; Haque et al., 2009)). Using an intermediate pivot language with bilingual translation in which a given language pair is low-resourced has led to improvements in translation performance (Wu and Wang, 2007; Bertoldi et al., 2008), but to our knowledge this approach has not been applied to full sentence paraphrasing. Several reasons may explain this, in particular the relative low quality of current MT approaches on full sentence translation, and the difficulties in controlling what is paraphrased and how. 3 The types of their paraphrase patterns are the following (numbers in parentheses indicate frequency in their"
W09-2503,P08-1089,0,0.344078,"valuation protocol for evaluating our proposal and report the results of a human evaluation in section 4. We finally conclude and present our future work in section 5. This family of works considered the acquisition of short paraphrases and their use in local paraphrasing of known units. Several works have tackled full sentence paraphrasing as a monolingual translation task relying on Statistical Machine Translation (SMT). For instance, (Quirk et al., 2004) used a phrase-based SMT decoder that uses local paraphrases acquired from comparable corpora to produce monotone sentential paraphrases. (Zhao et al., 2008a) acquired monolingual biphrases from various sources and used them with a phrase-based SMT decoder, and (Madnani et al., 2007) combined rules of their hierarchical decoders by pivot to obtain a monolingual grammar. These works were not motivated by the generation of high-quality paraphrases that could, for example, be reused in documents. The lack of structural information, the local nature of the paraphrasing performed and the fact that the context of the original sentences was not taken into account in the phrase-based approaches make it difficult to control meaning preservation during par"
W09-2503,P07-2045,0,0.00410423,"he paraphrased fragment, at least one lemma should not belong to both sets9 ; • neither the original fragment nor its paraphrase must be included into the other (only taking full words into account). 4 Experiments We have conducted experiments motivated by a text revision task that we report in this section by describing our baseline and context-aware subsentential paraphrasing systems and the results of a small-scale manual evaluation. 4.1 Data and systems We built two-way French-English SMT systems using 188,115 lines of the Europarl corpus (Koehn, 2005) of parliamentary debates with moses (Koehn et al., 2007) 10 . Our corpus was analyzed by the XIP robust parser (A¨ıt-Mokhtar et al., 2002) and its output tokenization was used. We built standard systems, as well as a contextual system for French→English as described in section 3.2 using an additional contextual score obTarget context for pivot SMT When decoding from the pivot hypothesis, we force our decoder to use provided sentence prefix and suffix corresponding to the “envelope” of the original fragment. Target context will thus be taken into account by the decoder. Furthermore, based on the hypothesis that a paraphrase for an unmodified envelop"
W09-2503,P05-1074,0,\N,Missing
W11-1602,P05-1074,0,0.562204,"ote that using the web may not always be appropriate, or that at least it should be used in a different way than what we propose in this article, in particular in cases where the desired properties of the rewritten text are better described in controlled corpora. 11 The use of automatic targeted paraphrasing as an authoring aid has been illustrated by the work of Max and Zock (2008), in which writers are presented with potential paraphrases of sub-sentential fragments that they wish to reword. The automatic paraphrasing technique used is a contextual variant of bilingual translation pivoting (Bannard and Callison-Burch, 2005). It has also been proposed to externalize various text editing tasks, including proofreading, by having crowdsourcing functions on text directly from word processors (Bernstein et al., 2010). Text improvements may also be more specifically targeted for automatic applications. In the work by Resnik et al. (2010), rephrasings for specific phrases are acquired through crowdsourcing. Difficult-to-translate phrases in the source text are first identified, and monolingual contributors are asked to provide rephrasings in context. Collected rephrasings can then be used as input for a Machine Translat"
W11-1602,P01-1008,0,0.06346,"generation The acquisition of paraphrases, and in particular of sub-sentential paraphrases and paraphrase patterns, has attracted a lot of works with the advent of data-intensive Natural Language Processing (Madnani and Dorr, 2010). The techniques proposed have a strong relationship to the type of text corpus used 3 This verse from Apollinaire’s Nuit Rh´enane [which seems almost without rhythmic structure → whose cesura is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it"
W11-1602,P08-1077,0,0.0155111,"is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it should more generally be assessed in the specific context of some use of the paraphrase pair. This refers to the problem of substituability in context (e.g. (Connor and Roth, 2007; Zhao et al., 2007)), which is a well studied field at the lexical level and the object of evaluation campains (McCarthy and Navigli, 2009). Contextual phrase substitution poses the additional challenge that phrases are rarer than words, so that"
W11-1602,P11-2069,1,0.819058,"b-sentential paraphrases and paraphrase patterns, has attracted a lot of works with the advent of data-intensive Natural Language Processing (Madnani and Dorr, 2010). The techniques proposed have a strong relationship to the type of text corpus used 3 This verse from Apollinaire’s Nuit Rh´enane [which seems almost without rhythmic structure → whose cesura is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it should more generally be assessed in the specific context of"
W11-1602,I05-5001,0,0.656547,"sed corpus would contain enough information for appropriate modeling of the substituability in context decision. It is therefore tempting to consider using the Web as the largest available information source, in spite of several of its known limitations, including that data can be of varying quality. It has however been shown that a large range of NLP applications can be improved by exploiting n-gram counts from the Web (using Web document counts as a proxy) (Lapata and Keller, 2005). Paraphrase identification has been addressed previously, both using features computed from an offline corpus (Brockett and Dolan, 2005) and features computed from Web queries (Zhao et al., 2007). However, to our knowledge previous work exploiting information from the Web was limited to the identification of lexical paraphrases. Although the probability of finding phrase occurrences significantly increases by considering the Web, some phrases are still very rare or not present in search engine indexes. As in (Brockett and Dolan, 2005), we tackle our paraphrase identification task as one of monolingual classification. More precisely, considering an original phrase p within the context of sentence s, we seek to determine whether"
W11-1602,D08-1021,0,0.0262373,"l., 2008), it should more generally be assessed in the specific context of some use of the paraphrase pair. This refers to the problem of substituability in context (e.g. (Connor and Roth, 2007; Zhao et al., 2007)), which is a well studied field at the lexical level and the object of evaluation campains (McCarthy and Navigli, 2009). Contextual phrase substitution poses the additional challenge that phrases are rarer than words, so that building contextual and grammatical models to ensure that the generated rephrasings are both semantically compatible and grammatical is more complicated (e.g. (Callison-Burch, 2008)). The present work does not aim to present any original technique for paraphrase acquisition, but rather focusses on the task of sub-sentential paraphrase validation in context. We thus resort to some existing repertoire of phrasal paraphrase pairs. As explained in section 2, we use the W I C O PAC O corpus as a source of sub-sentential paraphrases: the phrase after rewriting can thus be used as a potential paraphrase in context.4 To obtain other candidates of various quality, we used two knowledge sources. The first uses automatic pivot translation (Bannard and Callison-Burch, 2005), where a"
W11-1602,candito-etal-2010-statistical,0,0.0325447,"Missing"
W11-1602,C96-2183,0,0.014623,"o-text realization problem. However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-text generation. The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al., 2009). For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al., 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). The generation process must produce a text having a meaning which is compatible with the definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraGabriel Illouz LIMSI-CNRS Univ. Paris Sud gabrieli@limsi.fr Anne Vilnat LIMSI-CNRS Univ. Paris Sud anne@limsi.fr phrasing for text simplification), while ensuring that it remains grammatically correct. Its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between the original text and the new one is more difficult to control, as the mapping from on"
W11-1602,C08-1018,0,0.0219686,"history of Wikipedia. 1 Introduction There are many instances where it is reasonable to expect machines to produce text automatically. Traditionally, this was tackled as a concept-to-text realization problem. However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-text generation. The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al., 2009). For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al., 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). The generation process must produce a text having a meaning which is compatible with the definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraGabriel Illouz LIMSI-CNRS Univ. Paris Sud gabrieli@limsi.fr Anne Vilnat LIMSI-CNRS Univ. Paris Sud anne@limsi.fr phrasing for text simplification), while ensuring that it remains grammatically correct. Its complexity, compared"
W11-1602,J08-4005,0,0.0146725,"in particular of sub-sentential paraphrases and paraphrase patterns, has attracted a lot of works with the advent of data-intensive Natural Language Processing (Madnani and Dorr, 2010). The techniques proposed have a strong relationship to the type of text corpus used 3 This verse from Apollinaire’s Nuit Rh´enane [which seems almost without rhythmic structure → whose cesura is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it should more generally be assessed in"
W11-1602,W09-3102,0,0.0384282,"Missing"
W11-1602,N10-1017,0,0.0119854,"). The techniques proposed have a strong relationship to the type of text corpus used 3 This verse from Apollinaire’s Nuit Rh´enane [which seems almost without rhythmic structure → whose cesura is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it should more generally be assessed in the specific context of some use of the paraphrase pair. This refers to the problem of substituability in context (e.g. (Connor and Roth, 2007; Zhao et al., 2007)), which is a well studied"
W11-1602,I05-5007,0,0.0221954,"re → whose cesura is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it should more generally be assessed in the specific context of some use of the paraphrase pair. This refers to the problem of substituability in context (e.g. (Connor and Roth, 2007; Zhao et al., 2007)), which is a well studied field at the lexical level and the object of evaluation campains (McCarthy and Navigli, 2009). Contextual phrase substitution poses the additional challenge that phrase"
W11-1602,J10-3003,0,0.0705506,"iel Illouz LIMSI-CNRS Univ. Paris Sud gabrieli@limsi.fr Anne Vilnat LIMSI-CNRS Univ. Paris Sud anne@limsi.fr phrasing for text simplification), while ensuring that it remains grammatically correct. Its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between the original text and the new one is more difficult to control, as the mapping from one text to another is very dependent on the rewriting context. The wide variety of techniques for acquiring phrasal paraphrases, which can subsequently be used by text paraphrasing techniques (Madnani and Dorr, 2010), the inherent polysemy of such linguistic units and the pragmatic constraints on their uses make it impossible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at a lexical level (Zhao et al., 2007). Hence, automatic contextual validation of candidate rewritings is a fundamental issue for text paraphrasing with phrasal units. In this article, we tackle the problem of what we call targeted paraphrasing, defined as the rewriting of a subpart of a sentence, as in e.g. (Resnik et al., 2010) where it is applied to making parts of"
W11-1602,2008.amta-papers.13,0,0.0182599,"original wording. The task of rewriting complete sentences has also been addressed in various works (e.g. (Barzilay and Lee, 2003; Quirk et al., 2004; Zhao et al., 2010)). It poses, however, numerous other challenges, in particular regarding how it could be correctly evaluated. Human judgments of whole sentence transformations are complex and intra- and inter-judge coherence is difficult to attain with hypotheses of comparable quality. Using sentential paraphrases to support a given task (e.g. providing alternative reference translations for optimizing Statistical Machine Translation systems (Madnani et al., 2008)) 2 It is to be noted that, in the scenario presented in (Resnik et al., 2010), monolingual contributors cannot predict how useful their rewritings will be to the underlying Machine Translation engine used. can be seen as a proxy for extrinsic evaluation of the quality of paraphrases, but it is not clear from published results that improvements on the task are clearly correlated with the quality of the produced paraphrases. Lastly, automatic metrics have been proposed for evaluating the grammaticality of sentences (e.g. (Mutton et al., 2007)). Automatic evaluation of sentential paraphrases has"
W11-1602,max-wisniewski-2010-mining,1,0.840188,"metrics have been proposed for evaluating the grammaticality of sentences (e.g. (Mutton et al., 2007)). Automatic evaluation of sentential paraphrases has not produced any consensual results so far, as they do not integrate task-specific considerations and can be strongly biased towards some paraphrasing techniques. In this work, we tackle the comparatively more modest task of sub-sentential paraphrasing applied to text revision. In order to use an unbiased task, we use a corpus of naturally-occurring rewritings from an authoring memory of Wikipedia articles. We use the W I C O PAC O corpus (Max and Wisniewski, 2010), a collection of local rephrasings from the edit history of Wikipedia which contains instances of lexical, syntactical and semantic rephrasings (Dutrey et al., 2011), the latter type being illustrated by the following example: Ce vers de Nuit rh´enane d’Apollinaire [qui paraˆıt presque sans structure rythmique → dont la c´esure est comme masqu´ee]. . . 3 The appropriateness of this corpus for our work is twofold: first, the fact that it contains naturallyoccurring rewritings provides us with an interesting source of text spans in context which have been rewritten. Moreover, for those instance"
W11-1602,W08-1911,1,0.937786,"et al., 2011). This study also reports that there is an important variety of rephrasing phenomena, as illustrated by the difficulty of reaching a good identification coverage using a rule-based term variant identification engine. 1 Note that using the web may not always be appropriate, or that at least it should be used in a different way than what we propose in this article, in particular in cases where the desired properties of the rewritten text are better described in controlled corpora. 11 The use of automatic targeted paraphrasing as an authoring aid has been illustrated by the work of Max and Zock (2008), in which writers are presented with potential paraphrases of sub-sentential fragments that they wish to reword. The automatic paraphrasing technique used is a contextual variant of bilingual translation pivoting (Bannard and Callison-Burch, 2005). It has also been proposed to externalize various text editing tasks, including proofreading, by having crowdsourcing functions on text directly from word processors (Bernstein et al., 2010). Text improvements may also be more specifically targeted for automatic applications. In the work by Resnik et al. (2010), rephrasings for specific phrases are"
W11-1602,C04-1166,1,0.795727,"ce text automatically. Traditionally, this was tackled as a concept-to-text realization problem. However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-text generation. The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al., 2009). For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al., 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). The generation process must produce a text having a meaning which is compatible with the definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraGabriel Illouz LIMSI-CNRS Univ. Paris Sud gabrieli@limsi.fr Anne Vilnat LIMSI-CNRS Univ. Paris Sud anne@limsi.fr phrasing for text simplification), while ensuring that it remains grammatically correct. Its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between t"
W11-1602,P07-1044,0,0.0127222,"optimizing Statistical Machine Translation systems (Madnani et al., 2008)) 2 It is to be noted that, in the scenario presented in (Resnik et al., 2010), monolingual contributors cannot predict how useful their rewritings will be to the underlying Machine Translation engine used. can be seen as a proxy for extrinsic evaluation of the quality of paraphrases, but it is not clear from published results that improvements on the task are clearly correlated with the quality of the produced paraphrases. Lastly, automatic metrics have been proposed for evaluating the grammaticality of sentences (e.g. (Mutton et al., 2007)). Automatic evaluation of sentential paraphrases has not produced any consensual results so far, as they do not integrate task-specific considerations and can be strongly biased towards some paraphrasing techniques. In this work, we tackle the comparatively more modest task of sub-sentential paraphrasing applied to text revision. In order to use an unbiased task, we use a corpus of naturally-occurring rewritings from an authoring memory of Wikipedia articles. We use the W I C O PAC O corpus (Max and Wisniewski, 2010), a collection of local rephrasings from the edit history of Wikipedia which"
W11-1602,P10-2001,0,0.0266855,"does not take the original wording into account. We therefore used a ratio of the language model score of the paraphrased sentence with the language model score of the original 6 http://research.microsoft.com/en-us/ collaboration/focus/cs/web-ngram.aspx 7 Note that in order to query on French text, we had to remove all diacritics for the service to behave correctly, independently of encodings: careful examination of ranked hypotheses showed that this trick allowed us to obtain results coherent with expectations. 14 sentence, after normalization by sentence length of the language model scores (Onishi et al., 2010): hLM ratio = LM (para) lm(para)1/length(para) = LM (orig) lm(orig)1/length(orig) (2) Contextless thematic model scores Cooccurring words are used in distributional semantics to account for common meanings of words. We build vector representations of cooccurrences for both the original phrase p and its paraphrase p0 . Our contextless thematic model is built in the following fashion: we query a search engine to retrieve the top N document snippets for phrase p. We then count frequencies for all content words in these snippets, and keep the set W of words appearing more than a fraction of N . We"
W11-1602,N03-1024,0,0.123923,"f paraphrases, and in particular of sub-sentential paraphrases and paraphrase patterns, has attracted a lot of works with the advent of data-intensive Natural Language Processing (Madnani and Dorr, 2010). The techniques proposed have a strong relationship to the type of text corpus used 3 This verse from Apollinaire’s Nuit Rh´enane [which seems almost without rhythmic structure → whose cesura is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it should more genera"
W11-1602,N07-1051,0,0.00991518,"and right boundary of the sub-sentential paraphrase is higher than 10. Syntactic dependency baseline When rewriting a subpart of a sentence, the fact that syntactic dependencies between the rewritten phrase and its context are the same than those of the original phrase and the same context can provide some information about the grammatical and semantic substituability of the two phrases (Zhao et al., 2007; Max and Zock, 2008). We thus build syntactic dependencies for both the original and rewritten sentence, using the French version (Candito et al., 2010) of the Berkeley probabilistic parser (Petrov and Klein, 2007), and consider the subset of dependencies for the two sentences that exist between a word inside the phrase under focus and a word outside it (Deporig and Deppara ). Our C ONT D EP baseline considers a sentence as a paraphrase iff Deppara = Deporig . 5.3 Evaluation results We used the models described in Section 4 to build a SVM classifier using the LIBSVM package (Chang and Lin, 2001). Accuracy results are reported on Figure 5. P OSSIBLE S URE S URER W EB LM 62.79 68.37 56.79 B OUND LM 54.88 36.27 51.41 C ONT D EP 48.53 51.90 42.69 C LASSIFIER 57.67 70.69 62.85 Figure 5: Accuracy results for"
W11-1602,W04-3219,0,0.02426,"expression to produce more confident translations for better estimated source units (Schroeder et al., 2009).2 For instance, the phrase in bold in the sentence The number of people known to have died has now reached 358 can be rewritten as 1) who died, 2) identified to have died and 3) known to have passed away. All such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey a meaning which is reasonably close to the original wording. The task of rewriting complete sentences has also been addressed in various works (e.g. (Barzilay and Lee, 2003; Quirk et al., 2004; Zhao et al., 2010)). It poses, however, numerous other challenges, in particular regarding how it could be correctly evaluated. Human judgments of whole sentence transformations are complex and intra- and inter-judge coherence is difficult to attain with hypotheses of comparable quality. Using sentential paraphrases to support a given task (e.g. providing alternative reference translations for optimizing Statistical Machine Translation systems (Madnani et al., 2008)) 2 It is to be noted that, in the scenario presented in (Resnik et al., 2010), monolingual contributors cannot predict how usef"
W11-1602,D10-1013,0,0.334238,"ed by text paraphrasing techniques (Madnani and Dorr, 2010), the inherent polysemy of such linguistic units and the pragmatic constraints on their uses make it impossible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at a lexical level (Zhao et al., 2007). Hence, automatic contextual validation of candidate rewritings is a fundamental issue for text paraphrasing with phrasal units. In this article, we tackle the problem of what we call targeted paraphrasing, defined as the rewriting of a subpart of a sentence, as in e.g. (Resnik et al., 2010) where it is applied to making parts of sentences easier to translate automatically. While this problem is simpler than full sentence rewriting, its study is justified as it should be handled correctly for the more complex task to be successful. Moreover, being simpler, it offers evaluation scenarios which make the performance on the task easier to assess. Our particular experiments here aim to assist a Wikipedia contributor in revising a text to improve its quality. For this, we use a collection of phrases that have been rewritten in Wikipedia, and test the substitutability of paraphrases com"
W11-1602,E09-1082,0,0.0196018,"word processors (Bernstein et al., 2010). Text improvements may also be more specifically targeted for automatic applications. In the work by Resnik et al. (2010), rephrasings for specific phrases are acquired through crowdsourcing. Difficult-to-translate phrases in the source text are first identified, and monolingual contributors are asked to provide rephrasings in context. Collected rephrasings can then be used as input for a Machine Translation system, which can positively exploit the increased variety in expression to produce more confident translations for better estimated source units (Schroeder et al., 2009).2 For instance, the phrase in bold in the sentence The number of people known to have died has now reached 358 can be rewritten as 1) who died, 2) identified to have died and 3) known to have passed away. All such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey a meaning which is reasonably close to the original wording. The task of rewriting complete sentences has also been addressed in various works (e.g. (Barzilay and Lee, 2003; Quirk et al., 2004; Zhao et al., 2010)). It poses, however, numerous other challenges, in particular regardin"
W11-1602,N10-2012,0,0.0235665,"ses proposed by sets of players using a compact word-lattice view. Note that in its standard definition, the game attributes higher scores to paraphrase candidates that are highly rated and rarer. hedit = TER(Lemorig , Lempara ) (1) Note that this model is not derived from information from the Web, in contrast to all the models described next. Language model score The likelihood of a sentence can be a good indicator of its grammaticality (Mutton, 2006). Language model probabilities can now be obtained from Web counts. In our experiments, we used the Microsoft Web N-gram Service6 for research (Wang et al., 2010) to obtain log likelihood scores for text units.7 However, this score is certainly not sufficient as it does not take the original wording into account. We therefore used a ratio of the language model score of the paraphrased sentence with the language model score of the original 6 http://research.microsoft.com/en-us/ collaboration/focus/cs/web-ngram.aspx 7 Note that in order to query on French text, we had to remove all diacritics for the service to behave correctly, independently of encodings: careful examination of ranked hypotheses showed that this trick allowed us to obtain results cohere"
W11-1602,P09-1094,0,0.0165748,"taken from a rewriting memory automatically extracted from the edit history of Wikipedia. 1 Introduction There are many instances where it is reasonable to expect machines to produce text automatically. Traditionally, this was tackled as a concept-to-text realization problem. However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-text generation. The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al., 2009). For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al., 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). The generation process must produce a text having a meaning which is compatible with the definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraGabriel Illouz LIMSI-CNRS Univ. Paris Sud gabrieli@limsi.fr Anne Vilnat LIMSI-CNRS Univ. Paris Sud anne@limsi.fr phrasing for text simplification), while e"
W11-1602,C10-1149,0,0.0124392,"ce more confident translations for better estimated source units (Schroeder et al., 2009).2 For instance, the phrase in bold in the sentence The number of people known to have died has now reached 358 can be rewritten as 1) who died, 2) identified to have died and 3) known to have passed away. All such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey a meaning which is reasonably close to the original wording. The task of rewriting complete sentences has also been addressed in various works (e.g. (Barzilay and Lee, 2003; Quirk et al., 2004; Zhao et al., 2010)). It poses, however, numerous other challenges, in particular regarding how it could be correctly evaluated. Human judgments of whole sentence transformations are complex and intra- and inter-judge coherence is difficult to attain with hypotheses of comparable quality. Using sentential paraphrases to support a given task (e.g. providing alternative reference translations for optimizing Statistical Machine Translation systems (Madnani et al., 2008)) 2 It is to be noted that, in the scenario presented in (Resnik et al., 2010), monolingual contributors cannot predict how useful their rewritings"
W11-1602,C10-1152,0,0.0258946,"nces where it is reasonable to expect machines to produce text automatically. Traditionally, this was tackled as a concept-to-text realization problem. However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-text generation. The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al., 2009). For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al., 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). The generation process must produce a text having a meaning which is compatible with the definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraGabriel Illouz LIMSI-CNRS Univ. Paris Sud gabrieli@limsi.fr Anne Vilnat LIMSI-CNRS Univ. Paris Sud anne@limsi.fr phrasing for text simplification), while ensuring that it remains grammatically correct. Its complexity, compared with concept-to-text generation, mostly stems from t"
W11-1602,N03-1003,0,\N,Missing
W11-1602,2010.amta-workshop.3,0,\N,Missing
W11-2135,W10-1704,1,0.806759,"the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010)), which aims at reducing the lexical redundancy and splitting complex compounds. Using the same pre-processing scheme to translate from English to German would require to postprocess the output to undo the pre-processing. As in our last year’s experiments (Allauzen et al., 2010), this pre-processing step could be achieved with a two-step decoding. However, by stacking two decoding steps, we may stack errors as well. Thus, for this direction, we used the German tokenizer provided by the organizers. 3.2 contains large portions that are not useful for translating news text. The first filter aime"
W11-2135,J92-4003,0,0.317321,"Missing"
W11-2135,J04-2004,0,0.208795,"is estimated and tuned as described in Section 4.1. Moreover, we also introduce in Section 4.2 the use of the SOUL language model (LM) (Le et al., 2011) in SMT. Based on neural networks, the SOUL LM can handle an arbitrary large vocabulary and a high order markovian assumption (up to 10-gram in this work). Finally, experimental results are reported in Section 5 both in terms of BLEU scores and translation edit rates (TER) measured on the provided newstest2010 dataset. 2 System Overview Our in-house n-code SMT system implements the bilingual n-gram approach to Statistical Machine Translation (Casacuberta and Vidal, 2004). Given a 1 This kind of characters was used for Teletype up to the seventies or early eighties. 309 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 309–315, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics source sentence sJ1 , a translation hypothesis tˆ1I is defined as the sentence which maximizes a linear combination of feature functions: ( ) M tˆ1I = arg max t1I ∑ λm hm (sJ1 ,t1I ) (1) m=1 a word-aligned corpus (using MGIZA++2 with default settings) in such a way that a unique segmentation of the bilingual corpus is achi"
W11-2135,W08-0310,1,0.792506,"Missing"
W11-2135,D10-1044,0,0.0594607,"Missing"
W11-2135,D08-1076,0,0.0565273,"iew of these rather inconclusive experiments, we chose to stick to the classical MERT for the submitted results. Optimization Issues Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al., 2007), MERT is the most widely used algorithm for system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11"
W11-2135,P03-1021,0,0.271147,"s (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase-based system: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT), see details in Section 5.4), using the provided newstest2009 data as development set. 2.1 Training Our translation model is estimated over a training corpus composed of tuple sequences using classical smoothing techniques. Tuples are extracted from 310 The resulting sequence of tuples (1) is further refined to avoid NULL words in the source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations"
W11-2135,P02-1040,0,0.0853055,"n models. To train the target language models, we also used all provided data and monolingual corpora released by the LDC for French and English. Moreover, all parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994). For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008). 3.1 Tokenization 4 We took advantage of our in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010)), which aims at reducing the lexical redundancy and splitting complex compounds. Using the same pre-processing scheme to translate from English to German would require t"
W11-2135,W10-1748,0,0.0323177,"for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11 in the French-English and GermanEnglish shared translation tasks, in both directions. For this year’s participation, we only used n-code, our open source Statistical Machine Translation system based on bilingual n-grams. Our contributions are threefold. First, we have shown that n-gram based systems can achieve state-of-the-art performance on large scale tasks in terms of automatic metrics such as BLEU. Then, as already sho"
W11-2135,C08-1098,0,0.0510396,"action and reordering rules. 3 Data Pre-processing and Selection We used all the available parallel data allowed in the constrained task to compute the word alignments, except for the French-English tasks where the United Nation corpus was not used to train our translation models. To train the target language models, we also used all provided data and monolingual corpora released by the LDC for French and English. Moreover, all parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994). For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008). 3.1 Tokenization 4 We took advantage of our in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the G"
W11-2135,2011.eamt-1.33,1,0.771526,"system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11 in the French-English and GermanEnglish shared translation tasks, in both directions. For this year’s participation, we only used n-code, our open source Statistical Machine Translation system based on bilingual n-grams. Our contributions are threefold."
W11-2135,N04-4026,0,0.357669,"estimated by using the n-gram assumption: K p(sJ1 ,t1I ) = ∏ p((s,t)k |(s,t)k−1 . . . (s,t)k−n+1 ) k=1 Figure 1: Tuple extraction from a sentence pair. where s refers to a source symbol (t for target) and (s,t)k to the kth tuple of the given bilingual sentence pair. It is worth noticing that, since both languages are linked up in tuples, the context information provided by this translation model is bilingual. In addition to the translation model, eleven feature functions are combined: a target-language model (see Section 4 for details); four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase-based system: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003) (Minimu"
W11-2135,D07-1080,0,0.0221974,"ever, showed any consistent and significant improvement for the majority of setups tried (with the exception of the BBN approach, that had almost always improved over n-best MERT, but for the sole French to English translation direction). Additional experiments with 9 complementary translation models as additional features were performed with lattice-MERT, but neither showed any substantial improvement. In the view of these rather inconclusive experiments, we chose to stick to the classical MERT for the submitted results. Optimization Issues Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al., 2007), MERT is the most widely used algorithm for system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a gen"
W11-2135,D07-1055,0,0.0524218,"Missing"
W12-2505,C10-2010,0,0.0128657,"hurch, 1991; Brown et al., 1991) rely on the fact that the translation of a short (resp. long) sentence is short (resp. long). On the other hand, lexical matching approaches (Kay and R¨oscheisen, 1993; Simard et al., 1993) identify sure anchor points for the alignment using bilingual dictionaries or surface similarities of word forms. Lengthbased approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results. Most state-of-the-art approaches use both types of information (Langlais, 1998; Simard and Plamondon, 1998; Moore, 2002; Varga et al., 2005; Braune and Fraser, 2010). In most applications, only high-confidence oneto-one sentence alignments are considered useful and kept for subsequent processing stages. Indeed, when the objective is to build subsentential align1 See, for instance, the Uplug toolbox which integrates several sentence alignment tools in a unified framework: http://sourceforge.net/projects/uplug/ 36 Workshop on Computational Linguistics for Literature, pages 36–44, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics ments (at the level of words, terms or phrases), other types of mappings between sentences are dee"
W12-2505,P91-1022,0,0.889898,"alignments have to be computed. Sentence alignment is generally thought to be fairly easy and many efficient sentence alignment programs are freely available1 . Such programs rely on two main assumptions: (i) the relative order of sentences is the same on the two sides of the bitext, and (ii) sentence parallelism can be identified using simple surface cues. Hypothesis (i) warrants efficient sentence alignment algorithms based on dynamic programming techniques. Regarding (ii), various surface similarity measures have been proposed: on the one hand, length-based measures (Gale and Church, 1991; Brown et al., 1991) rely on the fact that the translation of a short (resp. long) sentence is short (resp. long). On the other hand, lexical matching approaches (Kay and R¨oscheisen, 1993; Simard et al., 1993) identify sure anchor points for the alignment using bilingual dictionaries or surface similarities of word forms. Lengthbased approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results. Most state-of-the-art approaches use both types of information (Langlais, 1998; Simard and Plamondon, 1998; Moore, 2002; Varga et al., 2005; Braune and Fraser, 2010). In mos"
W12-2505,J93-2003,0,0.134342,"gnments Public domain tools Baseline alignments are computed using two open-source sentence alignment packages, the sentence alignment tool of Moore (2002)6 , and Hunalign (Varga et al., 2005). These two tools were chosen as representative of the current state-of-theart in sentence alignment. Moore’s approach implements a two-pass, coarse-to-fine, strategy: a first pass, based on sentence length cues, computes a first alignment according to the principles of lengthbased approaches (Brown et al., 1991; Gale and Church, 1991). This alignment is used to train a simplified version of IBM model 1 (Brown et al., 1993), which provides the alignment system with lexical association scores; these scores are then used to refine the measure of association between sentences. This approach is primarily aimed at delivering high confidence, one-to-one, sentence alignments to be used as training material for data-intensive MT. Sentences that cannot be reliably aligned are discarded from the resulting alignment. 3 Getting access to more recent books (or their translation) is problematic, due to copyright issues: literary works fall in the public domain 70 years after the death of their author. 4 http://www.gutenberg.o"
W12-2505,P91-1023,0,0.656701,"applications, sentence alignments have to be computed. Sentence alignment is generally thought to be fairly easy and many efficient sentence alignment programs are freely available1 . Such programs rely on two main assumptions: (i) the relative order of sentences is the same on the two sides of the bitext, and (ii) sentence parallelism can be identified using simple surface cues. Hypothesis (i) warrants efficient sentence alignment algorithms based on dynamic programming techniques. Regarding (ii), various surface similarity measures have been proposed: on the one hand, length-based measures (Gale and Church, 1991; Brown et al., 1991) rely on the fact that the translation of a short (resp. long) sentence is short (resp. long). On the other hand, lexical matching approaches (Kay and R¨oscheisen, 1993; Simard et al., 1993) identify sure anchor points for the alignment using bilingual dictionaries or surface similarities of word forms. Lengthbased approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results. Most state-of-the-art approaches use both types of information (Langlais, 1998; Simard and Plamondon, 1998; Moore, 2002; Varga et al., 2005; Braune and"
W12-2505,1994.amta-1.21,0,0.326632,"literary texts, translation often departs from a straight sentence-by-sentence alignment and using such a constraint can discard a significant proportion of the bitext. For MT, this is just a regrettable waste of potentially useful training material (Uszkoreit et al., 2010), all the more so as parallel literary texts constitute a very large reservoir of parallel texts online. For other applications implying to mine, visualize or read the actual translations in their context (second language learning (Nerbonne, 2000; Kraif and Tutin, 2011), translators training, automatic translation checking (Macklovitch, 1994), etc.), the entire bitext has to be aligned. Furthermore, areas where the translation is only partial or approximative need to be identified precisely. The work reported in this study aims to explore the quality of existing sentence alignment techniques for literary work and to explore the usability of a recently proposed multiple-pass approach, especially designed for recovering many-to-one pairings. In a nutshell, this approach uses sure one-to-one mappings detected in a first pass to train a discriminative sentence alignment system, which is then used to align the regions which remain prob"
W12-2505,moore-2002-fast,0,0.686607,"length-based measures (Gale and Church, 1991; Brown et al., 1991) rely on the fact that the translation of a short (resp. long) sentence is short (resp. long). On the other hand, lexical matching approaches (Kay and R¨oscheisen, 1993; Simard et al., 1993) identify sure anchor points for the alignment using bilingual dictionaries or surface similarities of word forms. Lengthbased approaches are fast but error-prone, while lexical matching approaches seem to deliver more reliable results. Most state-of-the-art approaches use both types of information (Langlais, 1998; Simard and Plamondon, 1998; Moore, 2002; Varga et al., 2005; Braune and Fraser, 2010). In most applications, only high-confidence oneto-one sentence alignments are considered useful and kept for subsequent processing stages. Indeed, when the objective is to build subsentential align1 See, for instance, the Uplug toolbox which integrates several sentence alignment tools in a unified framework: http://sourceforge.net/projects/uplug/ 36 Workshop on Computational Linguistics for Literature, pages 36–44, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics ments (at the level of words, terms or phrases), oth"
W12-2505,J05-4003,0,0.450031,"training sentence pairs, the optimal values of the parameters are set by optimizing numerically the conditional likelihood; optimization is performed here using L-BFGS (Liu and Nocedal, 1989); a Gaussian prior over the parameters is used to ensure numerical stability of the optimization. In this study, we used the following set of feature functions: Figure 2: Filling alignment gaps 3.1 1 + exp[− 1 PK Detecting parallelism Assuming the availability of a set of example parallel sentences, the first step of our approach consists in training a function for scoring candidate alignments. Following (Munteanu and Marcu, 2005), we train a Maximum Entropy classifier9 (Rathnaparkhi, 1998); in principle, many other binary classifiers would be possible here. Our motivation for using a maxent approach was to obtain, for each possible pair of sentences (f ,e), a link posterior probability P (link|f , e). We take the sentence alignments of the first step as positive examples. Negative examples are artificially generated as follows: for all pairs of positive instances (e, f ) and (e0 , f 0 ) such that e0 immediately follows e, we select the pair (e, f 0 ) as a negative example. This strategy produced a balanced corpus cont"
W12-2505,C10-1124,0,0.060009,"of words, terms or phrases), other types of mappings between sentences are deemed to be either insufficiently reliable or inappropriate. As it were, the one-to-one constraint is viewed as a proxy to literalness/compositionality of the translation and warrants the search of finer-grained alignments. However, for certain types of bitexts2 , such as literary texts, translation often departs from a straight sentence-by-sentence alignment and using such a constraint can discard a significant proportion of the bitext. For MT, this is just a regrettable waste of potentially useful training material (Uszkoreit et al., 2010), all the more so as parallel literary texts constitute a very large reservoir of parallel texts online. For other applications implying to mine, visualize or read the actual translations in their context (second language learning (Nerbonne, 2000; Kraif and Tutin, 2011), translators training, automatic translation checking (Macklovitch, 1994), etc.), the entire bitext has to be aligned. Furthermore, areas where the translation is only partial or approximative need to be identified precisely. The work reported in this study aims to explore the quality of existing sentence alignment techniques f"
W12-2505,J93-1004,0,\N,Missing
W12-3141,W10-1704,1,0.815478,"eriments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built in “true-case”. Compared to last year, the pre-processing of utf-8 characters was significantly improved. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which severely impacts both training (alignment) and decoding (due to unknown forms). When translating from German into English, the German side is thus normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 333 2010)), which aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008). 5.2 Bilingual corpora As for last year’s evaluation, we used all the available parallel data for the German-English language pair, while only a subpart of the French-English parallel"
W12-3141,E09-1010,1,0.834206,"lpful. As the IBM1 model is asymmetric, two models are estimated, one in both directions. Contrary to the reported results, these additional features do not yield significant improvements over the baseline system. We assume that the difficulty is to add information to an already extensively optimized system. Moreover, the IBM1 models are estimated on the same training corpora as the translation system, a fact that may explain the redundancy of these additional features. In a separate series of experiments, we also add WSD features calculated according to a variation of the method proposed in (Apidianaki, 2009). For each word of a subset of the input (source language) vocabulary, a simple WSD classifier produces a probability distribution over a set of translations8 . During reranking, each translation hypothesis is scanned and the word translations that match one of the proposed variant are rewarded using an additional score. While this method had given some Conclusion In this paper, we described our submissions to WMT’12 in the French-English and GermanEnglish shared translation tasks, in both directions. As for our last year’s participation, our main systems are built with n-code, the open source"
W12-3141,J93-2003,0,0.0934316,"ize .... u8 u9 u10 u11 u12 Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . in two parts (source and target), and by taking words as the basic units of the n-gram TM. This may seem to be a regression with respect to current state-ofthe-art SMT systems, as the shift from the wordbased model of (Brown et al., 1993) to the phrasebased models of (Zens et al., 2002) is usually considered as a major breakthrough of the recent years. Indeed, one important motivation for considering phrases was to capture local context in translation and reordering. It should however be emphasized that the decomposition of phrases into words is only re-introduced here as a way to mitigate the parameter estimation problems. Translation units are still pairs of phrases, derived from a bilingual segmentation in tuples synchronizing the source and target n-gram streams. In fact, the estimation policy described in section 4 will a"
W12-3141,P05-1032,0,0.0155644,"e hardware conditions. 7 Experimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that the sample is chosen randomly with respect to the full corpus but that the same sample is always returned for a given value of sample size, hereafter denoted N . In our experiments, we used N = 1, 000 and computed from the sample and the word alignments (we used the same tokenization and word alignments as in all other submitted systems) the same translation6 and lexical reordering models as the s"
W12-3141,J04-2004,0,0.03102,"code, an open source in-house Statistical Machine Translation (SMT) system based on bilingual n-grams1 . The main novelty of this year’s participation is the use, in a large scale system, of the continuous space translation models described in (Hai-Son et al., 2012). These models estimate the n-gram probabilities of bilingual translation units using neural networks. We also investigate an alternative approach where the translation probabilities of a phrase based system are estimated “on-the-fly” 1 http://ncode.limsi.fr/ 2 System overview n-code implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006). In this framework, translation is divided in two steps: a source reordering step and a (monotonic) translation step. Source reordering is based on a set of learned rewrite rules that non-deterministically reorder the input words. Applying these rules result in a finite-state graph of possible source reorderings, which is then searched for the best possible candidate translation. 2.1 Features Given a source sentence s of I words, the best translation hypothesis ˆt is defined as the sequence of J words that maximizes a linear combination of fea33"
W12-3141,2010.iwslt-papers.6,0,0.024222,"Missing"
W12-3141,W08-0310,1,0.883725,"Missing"
W12-3141,N12-1005,1,0.885285,"Missing"
W12-3141,P07-2045,0,0.00674765,"lt (31.7 BLEU point) that is slightly worst than the n-code baseline (32.0) and slightly better than the equivalent Moses baseline (31.5), but does it much faster. Model estimation for the test file is reduced to 2 hours and 50 minutes, with an additional overhead for loading and writing files of one and a half hours, compared to roughly 210 hours for our baseline systems under comparable hardware conditions. 7 Experimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that th"
W12-3141,C08-1064,0,0.0749035,"rimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that the sample is chosen randomly with respect to the full corpus but that the same sample is always returned for a given value of sample size, hereafter denoted N . In our experiments, we used N = 1, 000 and computed from the sample and the word alignments (we used the same tokenization and word alignments as in all other submitted systems) the same translation6 and lexical reordering models as the standard traini"
W12-3141,J06-4004,0,0.217743,"Missing"
W12-3141,P03-1021,0,0.0736745,"x lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT)) using the newstest2009 as development set and BLEU (Papineni et al., 2002) as the optimization criteria. 2.2 P (s, t) = Standard n-gram translation models During the training phase (Mari˜no et al., 2006), tuples are extracted from a word-aligned corpus (using MGIZA++3 with default settings) in such a way that a unique segmentation of the bilingual corpus is achieved. A baseline n-gram translation model is then estimated over a training corpus composed of tuple sequences using modified KnesserNey Smoothing (Chen and Goodman, 1998). 2.3 During decoding, sour"
W12-3141,P02-1040,0,0.0874389,"ation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT)) using the newstest2009 as development set and BLEU (Papineni et al., 2002) as the optimization criteria. 2.2 P (s, t) = Standard n-gram translation models During the training phase (Mari˜no et al., 2006), tuples are extracted from a word-aligned corpus (using MGIZA++3 with default settings) in such a way that a unique segmentation of the bilingual corpus is achieved. A baseline n-gram translation model is then estimated over a training corpus composed of tuple sequences using modified KnesserNey Smoothing (Chen and Goodman, 1998). 2.3 During decoding, source sentences are represented in the form of word lattices containing the most promising reordering hypotheses, s"
W12-3141,C08-1098,0,0.0308747,"ing (alignment) and decoding (due to unknown forms). When translating from German into English, the German side is thus normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 333 2010)), which aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008). 5.2 Bilingual corpora As for last year’s evaluation, we used all the available parallel data for the German-English language pair, while only a subpart of the French-English parallel data was selected. Word alignment models were trained using all the data, whereas the translation models were estimated on a subpart of the parallel data: the UN corpus was discarded for this step and about half of the French-English Giga corpus was filtered based on a perplexity criterion as in (Allauzen et al., 2011)). For French-English, we mainly upgraded the training material from last year by extracting th"
W12-3141,N04-4026,0,0.0279162,"max t,a M X ) λm hm (a, s, t) (1) L Y P (ui |ui−1 , ..., ui−n+1 ) (2) i=1 m=1 where λm is the weight associated with feature function hm and a denotes an alignment between source and target phrases. Among the feature functions, the peculiar form of the translation model constitute one of the main difference between the n-gram approach and standard phrase-based systems. This will be further detailled in section 2.2 and 3. In addition to the translation model, fourteen feature functions are combined: a target-language model (Section 5.3); four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT))"
W12-3141,2002.tmi-tutorials.2,0,0.0391067,"French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . in two parts (source and target), and by taking words as the basic units of the n-gram TM. This may seem to be a regression with respect to current state-ofthe-art SMT systems, as the shift from the wordbased model of (Brown et al., 1993) to the phrasebased models of (Zens et al., 2002) is usually considered as a major breakthrough of the recent years. Indeed, one important motivation for considering phrases was to capture local context in translation and reordering. It should however be emphasized that the decomposition of phrases into words is only re-introduced here as a way to mitigate the parameter estimation problems. Translation units are still pairs of phrases, derived from a bilingual segmentation in tuples synchronizing the source and target n-gram streams. In fact, the estimation policy described in section 4 will actually allow us to take into account larger cont"
W12-3141,D08-1039,0,\N,Missing
W12-3141,W11-2135,1,\N,Missing
W12-3141,N04-1021,0,\N,Missing
W12-4201,apidianaki-2008-translation,1,0.852565,"ained (because the two variants of the adjective are reduced to the same lemma). All lexicon entries satisfying the above criteria are retained and used for disambiguation. In these initial experiments, we disambiguate English words having less than 20 French translations in the lexicon. Each French translation of an English word that appears more than once in the training corpus4 is characterized by a weighted English feature vector built from the training data. Vector building The feature vectors corresponding to the translations are built by exploiting information from the source contexts (Apidianaki, 2008; Grefenstette, 1994). For each translation of an EN word w, we extract the content words that co-occur with w in the corresponding source sentences of the parallel corpus (i.e. the content words that occur in the same sentence as w whenever it is translated by this translation). The extracted source language words constitute the features of the vector built for the translation. For each translation Ti of w, let N be the number of features retained from the corresponding source context. Each feature Fj (1 ≤ j ≤ N) receives a total weight tw(Fj , Ti ) defined as the product of the feature’s glo"
W12-4201,E09-1010,1,0.961121,"at and Wu, 2007). This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data. Accordingly, pre-defined In a multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus (Carpuat and Wu, 2007; Chan et al., 2007), or by using more complex representations of the semantics of translations (Apidianaki, 2009; Mihalcea et al., 2010; Lefever and Hoste, 2010). However, integrating this semantic knowledge into Statistical Machine Translation (SMT) raises several challenges: the way in which the predictions of the WSD classifier have to be taken into account; the type of context exploited for disambiguation; the target words to be disambiguated (“all-words” WSD vs. WSD restricted to target words satisfying specific criteria); the use of a single classifier versus building separate classifiers for each source word; the quantity and type of data used for training the classifier (e.g., use of raw data or"
W12-4201,P05-1048,0,0.0813922,"ing some avenues for future work. 2 Related work Word sense disambiguation systems generally work at the word level: given an input word and its context, they predict its (most likely) meaning. At the same time, state-of-the-art translation systems all consider groups of words (phrases, tuples, etc.) rather than single words in the translation process. This discrepancy between the units used in MT and those used in WSD is one of the major difficulties in integrating word predictions into the decoder. This was, for instance, one of the reasons for the somewhat disappointing results obtained by Carpuat and Wu (2005) when the output of a WSD system was directly incorporated into a Chinese-English SMT system. Because of this difficulty, other crosslingual semantics works have considered only simplified tasks, like blank-filling, without addressing the integration of the WSD models in full-scale MT systems (Vickrey et al., 2005; Specia, 2006). Since the pioneering work of Carpuat and Wu (2005), several more successful ways to take WSD predictions into account have been proposed. For instance, Carpuat and Wu (2007) proposed to generalize the WSD system so that it performs a fully 2 phrasal multiword disambig"
W12-4201,D07-1007,0,0.397955,"improvements in translation performance, highlighting the usefulness of source side disambiguation for SMT. 1 Introduction Word Sense Disambiguation (WSD) is the task of identifying the sense of words in texts by reference to some pre-existing sense inventory. The selection of the appropriate inventory and WSD method strongly depends on the goal WSD intends to serve: recent methods are increasingly oriented towards the disambiguation needs of specific end applications, and explicitly aim at improving the overall performance of complex Natural Language Processing systems (Ide and Wilks, 2007; Carpuat and Wu, 2007). This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data. Accordingly, pre-defined In a multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus (Carpuat and Wu, 2007; Chan et al., 2007), or by using more complex representations of the semantics of translations (Apidianaki, 2009"
W12-4201,P07-1005,0,0.209625,"overall performance of complex Natural Language Processing systems (Ide and Wilks, 2007; Carpuat and Wu, 2007). This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data. Accordingly, pre-defined In a multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus (Carpuat and Wu, 2007; Chan et al., 2007), or by using more complex representations of the semantics of translations (Apidianaki, 2009; Mihalcea et al., 2010; Lefever and Hoste, 2010). However, integrating this semantic knowledge into Statistical Machine Translation (SMT) raises several challenges: the way in which the predictions of the WSD classifier have to be taken into account; the type of context exploited for disambiguation; the target words to be disambiguated (“all-words” WSD vs. WSD restricted to target words satisfying specific criteria); the use of a single classifier versus building separate classifiers for each source w"
W12-4201,C10-1027,1,0.871747,"If this is the case, the corresponding probabilities are additively accumulated for the current hypothesis. At the end, two features are appended to each hypothesis in the n-best list: the total score accumulated for the hypothesis and 5 the same score normalized by the number of words in the hypothesis. Two MERT initialization schemes were considered: (1) all model weights are initialized to zero, and (2) all the weights of “standard” features are initialized to the values found by MERT and the new WSD features to zero. 4.2 Local Language Models We propose to adapt the approach introduced in Crego et al. (2010) as an alternative way to integrate the WSD predictions within the decoder: for each sentence to be translated, an additional language model (LM) is estimated and taken into account during decoding. As this additional “local” model depends on the source sentence, it can be used as an external source of knowledge to reinforce translation hypotheses complying with criteria predicted from the whole source sentence. For instance, the unigram probabilities of the additional LM can be derived from the (word) predictions of a WSD system, bigram probabilities from the prediction of phrases and so on a"
W12-4201,2009.eamt-1.32,0,0.0163645,"er, given that the number of phrases is far larger than the number of words, this approach suffers from sparsity and computational problems, as it requires training a classifier for each entry of the phrase table. Chan et al. (2007) introduced a way to modify the rule weights of a hierarchical translation system to reflect the predictions of their WSD system. While their approach and ours are built on the same intuition (an adaptation of a model to incorporate word predictions) their work is specific to hierarchical systems, while ours can be applied to any decoder that uses a language model. Haque et al. (2009) et Haque et al. (2010) introduce lexico-syntactic descriptions in the form of supertags as source language context-informed features in a phrase-based SMT and a state-of-the-art hierarchical model, respectively, and report significant gains in translation quality. Closer to our work, Mauser et al. (2009) and Patry and Langlais (2011) train a global lexicon model that predicts the bag of output words from the bag of input words. As no explicit alignment between input and output words is used, words are chosen based on the (global) input context. For each input sentence, the decoder considers t"
W12-4201,2010.amta-papers.23,0,0.0316251,"Missing"
W12-4201,P07-2045,0,0.00398791,". 5 Evaluation 5.3 5.1 Table 2 reports the results of our experiments. It appears that, for the considered task, sense disambiguation improves translation performance: n-best rescoring results in a 0.37 BLEU improvement and using an additional language model brings about an improvement of up to a 0.88 BLEU. In both cases, MERT assigns a large weight to the additional feaExperimental Setting In all our experiments, we considered the TEDtalk English to French data set provided by the IWSLT’11 evaluation campaign, a collection of public speeches on a variety of topics. We used the Moses decoder (Koehn et al., 2007). The TED-talk corpus is a small data set made of a monolingual corpus (111, 431 sentences) used 6 Results 7 http://statmt.org/wmt08/scripts.tgz method baseline rescoring additional LM — WSD (zero init) WSD (reinit) oracle 3-gram oracle 2-gram oracle 1-gram IBM 1 WSD BLEU 29.63 30.00 29.58 43.56 39.36 42.92 30.18 30.51 METEOR 53.78 54.26 53.96 64.64 62.92 69.39 54.36 54.38 Table 2: Evaluation results on the TED-talk task of our two methods to integrate WSD predictions. baseline 67.57 45.97 51.79 52.17 PoS Nouns Verbs Adjectives Adverbs WSD 69.06 47.76 53.94 56.25 Table 3: Contrastive lexical e"
W12-4201,D09-1022,0,0.0449313,"n system to reflect the predictions of their WSD system. While their approach and ours are built on the same intuition (an adaptation of a model to incorporate word predictions) their work is specific to hierarchical systems, while ours can be applied to any decoder that uses a language model. Haque et al. (2009) et Haque et al. (2010) introduce lexico-syntactic descriptions in the form of supertags as source language context-informed features in a phrase-based SMT and a state-of-the-art hierarchical model, respectively, and report significant gains in translation quality. Closer to our work, Mauser et al. (2009) and Patry and Langlais (2011) train a global lexicon model that predicts the bag of output words from the bag of input words. As no explicit alignment between input and output words is used, words are chosen based on the (global) input context. For each input sentence, the decoder considers these word predictions as an additional feature that it uses to define a new model score which favors translation hypotheses containing words predicted by the global lexicon model. A difference between this approach and our work is that instead of using a global lexicon model, we disambiguate a subset of t"
W12-4201,max-etal-2010-contrastive,1,0.859696,"to the WSD method introduced in Section 3, these oracle experiments rely on sense predictions for all source words and not only content words. Surprisingly enough, predicting phrases instead of words results only in a small improvement. Additional experiments are required to explain why 2-gram oracle achieved such a low performance. 7 5.4 Contrastive lexical evaluation All the measures used for evaluating the impact of WSD information on translation show improvements, as discussed in the previous section. We complement these results with another measure of translation performance, proposed by Max et al. (2010), which allows for a more fine-grained contrastive evaluation of the translations produced by different systems. The method permits to compare the results produced by the systems on different word classes and to take into account the source words that were actually translated. We focus this evaluation on the classes of content words (nouns, adjectives, verbs and adverbs) on which WSD had an important coverage. Our aim is, first, to explore how these words are handled by a WSDinformed SMT system (the system using the local language models) compared to the baseline system that does not exploit a"
W12-4201,W09-2412,0,0.034286,"Missing"
W12-4201,J03-1002,0,0.00530526,"tions of a word and to assign a probability to each translation for new instances of the word in context. Each translation is represented by a source language feature vector that the classifier uses for disambiguation. All experiments carried out in this study are for the English (EN) - French (FR) language pair. 3.1 Source Language Feature Vectors Preprocessing The information needed by the classifier is gathered from the EN-FR training data provided for the IWSLT’11 evaluation task.1 The dataset consists of 107,268 parallel sentences, wordaligned in both translation directions using GIZA++ (Och and Ney, 2003). We disambiguate EN words found in the parallel corpus that satisfy the set of criteria described below. Two bilingual lexicons are built from the alignment results and filtered to eliminate spurious alignments. First, translation correspondences with a probability lower than a threshold are discarded;2 then translations are filtered by part-of-speech (PoS), keeping for each word only translations pertaining to the same grammatical category;3 finally, only intersecting alignments (i.e., correspondences found in the lexicons of both directions) are retained. Given that the lexicons contain wor"
W12-4201,I11-1074,0,0.589525,"redictions of their WSD system. While their approach and ours are built on the same intuition (an adaptation of a model to incorporate word predictions) their work is specific to hierarchical systems, while ours can be applied to any decoder that uses a language model. Haque et al. (2009) et Haque et al. (2010) introduce lexico-syntactic descriptions in the form of supertags as source language context-informed features in a phrase-based SMT and a state-of-the-art hierarchical model, respectively, and report significant gains in translation quality. Closer to our work, Mauser et al. (2009) and Patry and Langlais (2011) train a global lexicon model that predicts the bag of output words from the bag of input words. As no explicit alignment between input and output words is used, words are chosen based on the (global) input context. For each input sentence, the decoder considers these word predictions as an additional feature that it uses to define a new model score which favors translation hypotheses containing words predicted by the global lexicon model. A difference between this approach and our work is that instead of using a global lexicon model, we disambiguate a subset of the words in the input sentence"
W12-4201,P06-3010,0,0.142762,"slation process. This discrepancy between the units used in MT and those used in WSD is one of the major difficulties in integrating word predictions into the decoder. This was, for instance, one of the reasons for the somewhat disappointing results obtained by Carpuat and Wu (2005) when the output of a WSD system was directly incorporated into a Chinese-English SMT system. Because of this difficulty, other crosslingual semantics works have considered only simplified tasks, like blank-filling, without addressing the integration of the WSD models in full-scale MT systems (Vickrey et al., 2005; Specia, 2006). Since the pioneering work of Carpuat and Wu (2005), several more successful ways to take WSD predictions into account have been proposed. For instance, Carpuat and Wu (2007) proposed to generalize the WSD system so that it performs a fully 2 phrasal multiword disambiguation. However, given that the number of phrases is far larger than the number of words, this approach suffers from sparsity and computational problems, as it requires training a classifier for each entry of the phrase table. Chan et al. (2007) introduced a way to modify the rule weights of a hierarchical translation system to"
W12-4201,H05-1097,0,0.178488,"ngle words in the translation process. This discrepancy between the units used in MT and those used in WSD is one of the major difficulties in integrating word predictions into the decoder. This was, for instance, one of the reasons for the somewhat disappointing results obtained by Carpuat and Wu (2005) when the output of a WSD system was directly incorporated into a Chinese-English SMT system. Because of this difficulty, other crosslingual semantics works have considered only simplified tasks, like blank-filling, without addressing the integration of the WSD models in full-scale MT systems (Vickrey et al., 2005; Specia, 2006). Since the pioneering work of Carpuat and Wu (2005), several more successful ways to take WSD predictions into account have been proposed. For instance, Carpuat and Wu (2007) proposed to generalize the WSD system so that it performs a fully 2 phrasal multiword disambiguation. However, given that the number of phrases is far larger than the number of words, this approach suffers from sparsity and computational problems, as it requires training a classifier for each entry of the phrase table. Chan et al. (2007) introduced a way to modify the rule weights of a hierarchical transla"
W12-4201,W09-2413,0,\N,Missing
W12-4201,S10-1002,0,\N,Missing
W12-4201,N04-1021,0,\N,Missing
W13-1733,P05-1074,0,0.102401,"Missing"
W13-1733,J96-1002,0,0.063408,"to reduce frequent misclassifications (4.2). We finally conclude with a short discussion (section 5). This paper describes LIMSI’s participation to the first shared task on Native Language Identification. Our submission uses a Maximum Entropy classifier, using as features character and chunk n-grams, spelling and grammatical mistakes, and lexical preferences. Performance was slightly improved by using a twostep classifier to better distinguish otherwise easily confused native languages. 1 rnagata@konan-u.ac.jp 2 A Maximum Entropy model Our system is based on a classical maximum entropy model (Berger et al., 1996): Introduction This paper describes the submission from LIMSI to the 2013 shared task on Native Language Identification (Tetreault et al., 2013). The creation of this new challenge provided us with a dataset (12,100 TOEFL essays by learners of English of eleven native languages (Blanchard et al., 2013)) that was necessary to us to develop an initial framework for studying Native Language Identification in text. We expect that this challenge will draw conclusions that will provide the community with new insights into the impact of native language in foreign language writing. We believe that suc"
W13-1733,P10-1052,1,0.846774,"Missing"
W13-1733,J93-2004,0,0.0439984,"t will be described and compared in the following sections. 3.1 Basic features We used n-grams of characters up to length 4 as features. In order to reduce the size of the feature space and the sparsity of these features, we used a hash kernel (Shi et al., 2009) of size 216 with a hash family of size 4. This allowed us to significantly reduce the training time with no noticeable impact on the model’s performance. Our set of basic features also includes n-grams of part-of-speech (POS) tags and chunks up to length 3. Both were computed using an in-house CRF-based tagger trained on PennTreeBank (Marcus et al., 1993). The POS tags sequences were post-processed so that word tokens were used in lieu of their corresponding POS tags for the following: coordinating conjunctions, determiners, prepositions, modals, predeterminers, possessives, pronouns, and question adverbs (Nagata, 2013). For instance, from this sentence excerpt: [NP Some/DT people/NNS] [VP might/MD think/VB] [SBAR that/IN] [VP traveling/VBG] [PP in/IN]. . . we extract n-grams from the pseudo POS-tag sequence: Some NNS MD VB that VBG in. . . To extract these features, each document is processed using the ispell1 spell checker. This results in a"
W13-1733,P13-1112,1,0.881465,"th a hash family of size 4. This allowed us to significantly reduce the training time with no noticeable impact on the model’s performance. Our set of basic features also includes n-grams of part-of-speech (POS) tags and chunks up to length 3. Both were computed using an in-house CRF-based tagger trained on PennTreeBank (Marcus et al., 1993). The POS tags sequences were post-processed so that word tokens were used in lieu of their corresponding POS tags for the following: coordinating conjunctions, determiners, prepositions, modals, predeterminers, possessives, pronouns, and question adverbs (Nagata, 2013). For instance, from this sentence excerpt: [NP Some/DT people/NNS] [VP might/MD think/VB] [SBAR that/IN] [VP traveling/VBG] [PP in/IN]. . . we extract n-grams from the pseudo POS-tag sequence: Some NNS MD VB that VBG in. . . To extract these features, each document is processed using the ispell1 spell checker. This results in a list of incorrectly written word forms and a set of potential corrections. For each word, the best correction is next selected using a set of rules, which were built manually after a careful study of the training dataset. When a corrected word is found, the incorrect f"
W13-1733,P09-2034,0,0.0252392,"ven native language. Using the Google Translation online Statistical Machine Translation service4 , which proposed translations from and to English and all the native languages of the shared task, a further approximation had to be made as, in practice, we were only able to access the most likely translations for words in isolation: we considered only the best translation of the original English word in the native language, and then kept its best back-translation into English. We here note some common intuitions with the use of roundtrip translation as a Machine Translation evaluation metrics (Rapp, 2009). 4 http://translate.google.com 262 Table 1 provides various examples of backtranslations for English adjectives obtained via each native language. The samples from the Table show that our procedure produces a significant number of non identical back-translations. They also illustrate some types of undesirable results obtained, which led us to only consider as features for our classifier the proportion of words in essays for which the above-defined back-translation yielded the same word, considering all possible native languages. We only considered content words, as out-of-context back-transla"
W13-1733,W13-1706,0,0.0323657,"ion to the first shared task on Native Language Identification. Our submission uses a Maximum Entropy classifier, using as features character and chunk n-grams, spelling and grammatical mistakes, and lexical preferences. Performance was slightly improved by using a twostep classifier to better distinguish otherwise easily confused native languages. 1 rnagata@konan-u.ac.jp 2 A Maximum Entropy model Our system is based on a classical maximum entropy model (Berger et al., 1996): Introduction This paper describes the submission from LIMSI to the 2013 shared task on Native Language Identification (Tetreault et al., 2013). The creation of this new challenge provided us with a dataset (12,100 TOEFL essays by learners of English of eleven native languages (Blanchard et al., 2013)) that was necessary to us to develop an initial framework for studying Native Language Identification in text. We expect that this challenge will draw conclusions that will provide the community with new insights into the impact of native language in foreign language writing. We believe that such a research domain is crucial, not only for improving our understanding of language learning and language production processes, but also for de"
W13-2204,W10-1704,1,0.883815,"Missing"
W13-2204,D07-1091,0,0.0403744,"Spanishto-English direction but yields a 0.2 BLEU point decrease in the opposite direction. For the following experiments, all the available corpora are therefore used: News-Commentary, Europarl, filtered CommonCrawl and UN. For each of these corpora, a bilingual n-gram model is estimated and used by n-code as one individual model score. An additionnal TM is trained on the concatenation all these corpora, resulting in a total of 5 TMs. Moreover, n-code is able to handle additional “factored” bilingual models where the source side words are replaced by the corresponding lemma or even POS tag (Koehn and Hoang, 2007). Table 2 reports the scores obtained with different settings. In Table 2, big denotes the use of a wider context for n-gram TMs (n = 4, 5, 4 instead of 3, 4, 3 respectively for word-based, POS-based and lemma-based TMs). Using POS factored Spanish language model To train the language models, we assumed that the test set would consist in a selection of recent news texts and all the available monolingual data for Spanish were used, including the Spanish Gigaword, Third Edition. A vocabulary is first defined by including all tokens observed in the NewsCommentary and Europarl corpora. This vocabu"
W13-2204,J04-2004,0,0.0169884,"h and Spanish-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams, and continuous space models in a post-processing step. The main novelties of this year’s participation are the following: our first participation to the Spanish-English task; experiments with source pre-ordering; a tighter integration of continuous space language models using artificial text generation (for German); and the use of different tuning sets according to the original language of the text to be translated. 1 2 n-code implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006). In this framework, translation is divided in two steps: a source reordering step and a (monotonic) translation step. Source reordering is based on a set of learned rewrite rules that non-deterministically reorder the input words. Applying these rules result in a finite-state graph of possible source reorderings, which is then searched for the best possible candidate translation. Introduction This paper describes LIMSI’s submissions to the shared translation task of the Eighth Workshop on Statistical Machine Translation. LIMSI participated in th"
W13-2204,N12-1005,1,0.88366,"model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weight vector λ is learned using the Minimum Error Rate Training framework (MERT) (Och, 2003) and BLEU (Papineni et al., 2002) measured on nt09 (newstest2009) as the optimization criteria. 2.2 Concerning data pre-processing, we started from our submissions from last year (Le et al., 2012c) and mainly upgraded the corpora and the associated language-dependent pre-processing routines. We used in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built using the “true-case” scheme. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which severely impacts both training (alignment) and decoding (due to u"
W13-2204,W12-2701,1,0.912005,"model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weight vector λ is learned using the Minimum Error Rate Training framework (MERT) (Och, 2003) and BLEU (Papineni et al., 2002) measured on nt09 (newstest2009) as the optimization criteria. 2.2 Concerning data pre-processing, we started from our submissions from last year (Le et al., 2012c) and mainly upgraded the corpora and the associated language-dependent pre-processing routines. We used in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built using the “true-case” scheme. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which severely impacts both training (alignment) and decoding (due to u"
W13-2204,W12-2700,0,0.287257,"short range reorderings, they are inadequate to capture long-range reorderings, especially for language pairs that differ significantly in their syntax. A promising workaround is the source preordering method that can be considered similar, to some extent, to the reordering strategy implemented in n-code; the main difference is that the latter uses one deterministic (long-range) reordering on top of conventional distortion-based models, while the former only considers one single model delivering permutation lattices. The preordering approach is illustrated by the recent work of Neubig et al. (2012), where the authors use a discriminatively trained ITG parser to infer a single permutation of the source sentence. In this section, we investigate the use of this pre-ordering model in conjunction with the bilingual n-gram approach for translating English into German (see (Collins et al., 2005) for similar experiments with the reverse translation direction). Experiments are carried out with the same settings as described in (Neubig et al., 2012): given the source side of the parallel data (en), the parser is estimated to modify the original word order and to generate a new source side (en-mod"
W13-2204,P05-1066,0,0.110387,"Missing"
W13-2204,J12-4004,0,0.0182804,"language, so for sentences originally in this language, the baseline system was used. This system is used as our primary submission to the evaluation, with additional SOUL rescoring step. Therefore, to translate from English to German, the submitted system includes three BOLMs: one trained on all the monolingual data, one on artificial texts and a third one that uses the freely available deWack corpus3 (1.7 billion words). target LM base +genText +SOUL +genText+SOUL BLEU dev nt09 test nt10 15.3 16.5 15.5 16.8 16.4 17.6 16.5 17.8 8 Different tunings for different original languages As shown by Lembersky et al. (2012), the original language of a text can have a significant impact on translation performance. In this section, this effect is assessed on the French to English translation task. Training one SMT system per original language is impractical, since the required information is not available for most of parallel corpora. However, metadata provided by the WMT evaluation allows us to split the development and test sets according to the original language of the text. To ensure a sufficient amount of texts for each condition, we used the concatenation of newstest corpora for the years 2008, 2009, 2011, a"
W13-2204,W08-0310,1,0.86537,"Missing"
W13-2204,J06-4004,0,0.081732,"Missing"
W13-2204,moore-2002-fast,0,0.0869027,"Missing"
W13-2204,P03-1021,0,0.0399909,"ed reordering models (Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weight vector λ is learned using the Minimum Error Rate Training framework (MERT) (Och, 2003) and BLEU (Papineni et al., 2002) measured on nt09 (newstest2009) as the optimization criteria. 2.2 Concerning data pre-processing, we started from our submissions from last year (Le et al., 2012c) and mainly upgraded the corpora and the associated language-dependent pre-processing routines. We used in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built using the “true-case” scheme. As German is morphologically more comp"
W13-2204,padro-stanilovsky-2012-freeling,0,0.0628704,"Missing"
W13-2204,P02-1040,0,0.087289,"(Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weight vector λ is learned using the Minimum Error Rate Training framework (MERT) (Och, 2003) and BLEU (Papineni et al., 2002) measured on nt09 (newstest2009) as the optimization criteria. 2.2 Concerning data pre-processing, we started from our submissions from last year (Le et al., 2012c) and mainly upgraded the corpora and the associated language-dependent pre-processing routines. We used in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built using the “true-case” scheme. As German is morphologically more complex than English, the default pol"
W13-2204,C08-1098,0,0.0472101,"Missing"
W13-2204,P06-2093,0,0.0775468,"Missing"
W13-2204,N04-4026,0,0.0181647,"on hm and a denotes an alignment between source and target phrases. Among the feature functions, the peculiar form of the translation model constitutes one of the main difference between the n-gram approach and standard phrasebased systems. http://ncode.limsi.fr/ 62 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 62–69, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics 3 In addition to the translation model (TM), fourteen feature functions are combined: a targetlanguage model; four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weight vector λ is learned using the Minimum Error Rate Training framework (MERT) (Och, 2003) and BLEU (Papineni et al."
W13-2204,D11-1034,0,\N,Missing
W13-2204,D12-1077,0,\N,Missing
W13-2204,W11-2135,1,\N,Missing
W13-2204,2010.iwslt-papers.6,0,\N,Missing
W14-3330,W08-0310,1,0.895121,"Missing"
W14-3330,W11-2123,0,0.0112757,"22 -13 6 -7 27 16 -33 52 33 69 49 69 - concatenation khresmoi-summary see Section 3.4 from WMT’12 khresmoi-summary Table 1: Parallel corpora used in this work, along with the number of sentences and the number of English and French tokens, respectively. Weights (λk ) from our best N CODE configuration are indicated for each sub-corpora’s bilingual word language model (wrd-lm) and POS factor language model (pos-lm). lel data and all the available monolingual data1 , with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996), using the S RI LM (Stolcke, 2002) and K EN LM (Heafield, 2011) toolkits. Although more similar to term-toterm dictionaries, UMLS and W IKIPEDIA proved better to be included in the language model. The large out-of-domain language model used for WMT’13 (Allauzen et al., 2013) is additionaly used (see Table 1). needed on a new corpus in order to adapt the parameters to the new domain. 3 3.1 Data and Systems Preparation Corpora We use all the available (constrained) medical data extracted using the scripts provided by the organizers. This resulted in 7 sub-corpora from the medical domain with distinctive features. As outof-domain data, we reuse the data proc"
W14-3330,D11-1125,0,0.0362648,"Missing"
W14-3330,P05-1032,0,0.0608395,"Missing"
W14-3330,J04-2004,0,0.042241,"al articles. Our main submission uses a combination of N CODE (n-gram-based) and M OSES (phrase-based) output and continuous-space language models used in a post-processing step for each system. Other characteristics of our submission include: the use of sampling for building M OSES’ phrase table; the implementation of the vector space model proposed by Chen et al. (2013); adaptation of the POStagger used by N CODE to the medical domain; and a report of error analysis based on the typology of Vilar et al. (2006). 1 System Overview N CODE N CODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n-gram assumption to decompos"
W14-3330,W04-3237,0,0.0452726,"for E MEA. Table 1 summarizes the data used along with some statistics after the cleaning and pre-processing steps. 3.2 3.3 Part-of-Speech Tagging Medical data exhibit many peculiarities, including different syntactic constructions and a specific vocabulary. As standard POS-taggers are known not to perform very well for this type of texts, we use a specific model trained on the Penn Treebank and on medical data from the MedPost project (Smith et al., 2004). We use Wapiti (Lavergne et al., 2010), a state-of-the-art CRF implementation, with a standard feature set. Adaptation is performed as in (Chelba and Acero, 2004) using the out-of-domain model as a prior when training the in-domain model on medical data. On a medical test set, this adaptation leads to a 8 point reduction of the error rate. A standard model is used for WMT’13 data. For the French side, due to the lack of annotaded data for the medical domain, corpora are tagged using the TreeTagger (Schmid, 1994). Language Models A medical-domain 4-gram language model is built by concatenating the target side of the paral1 Attempting include one language model per sub-corpora yielded a significant drop in performance. 248 3.4 Proxy Test Set System Combi"
W14-3330,P07-2045,0,0.00663677,"Missing"
W14-3330,P96-1041,0,0.209049,"pos-lm term dictionary short titles -3 26 22 6 4 -7 -5 -15 -1 21 2 -17 -22 -13 6 -7 27 16 -33 52 33 69 49 69 - concatenation khresmoi-summary see Section 3.4 from WMT’12 khresmoi-summary Table 1: Parallel corpora used in this work, along with the number of sentences and the number of English and French tokens, respectively. Weights (λk ) from our best N CODE configuration are indicated for each sub-corpora’s bilingual word language model (wrd-lm) and POS factor language model (pos-lm). lel data and all the available monolingual data1 , with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996), using the S RI LM (Stolcke, 2002) and K EN LM (Heafield, 2011) toolkits. Although more similar to term-toterm dictionaries, UMLS and W IKIPEDIA proved better to be included in the language model. The large out-of-domain language model used for WMT’13 (Allauzen et al., 2013) is additionaly used (see Table 1). needed on a new corpus in order to adapt the parameters to the new domain. 3 3.1 Data and Systems Preparation Corpora We use all the available (constrained) medical data extracted using the scripts provided by the organizers. This resulted in 7 sub-corpora from the medical domain with di"
W14-3330,P13-1126,0,0.179519,"franc¸ais4 {firstname.lastname}@limsi.fr 2 Abstract 2.1 This paper describes LIMSI’s submission to the first medical translation task at WMT’14. We report results for EnglishFrench on the subtask of sentence translation from summaries of medical articles. Our main submission uses a combination of N CODE (n-gram-based) and M OSES (phrase-based) output and continuous-space language models used in a post-processing step for each system. Other characteristics of our submission include: the use of sampling for building M OSES’ phrase table; the implementation of the vector space model proposed by Chen et al. (2013); adaptation of the POStagger used by N CODE to the medical domain; and a report of error analysis based on the typology of Vilar et al. (2006). 1 System Overview N CODE N CODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, the source sentence is first reordered according to a set of rewriting rules so as to reproduc"
W14-3330,P10-1052,1,0.873159,"Missing"
W14-3330,N12-1047,0,0.0389371,"Missing"
W14-3330,2011.iwslt-evaluation.7,1,0.81625,"osal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (S OUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allows us to consider longer dependencies than the one used by conventional n-gram models (e.g. n = 10 instead of n = 4). Additionally, continuous models can also be easily and efficiently adapted as in (Lavergne et al., 2011). Starting from a previously trained S OUL model, only a few more training epochs are (2) where the f req(·) is the number of occurrences of the given phrase in the whole corpus, and the numerator p(¯ e|f¯) × f req(f¯) represents the predicted joint count of f¯ and e¯. The other models in this system are the same as in the default configuration of M OSES. 2.3 countdev (f¯j , e¯k )wc (f¯j , e¯k ) (3) j=0 k=0 We develop an alternative approach implementing an on-the-fly estimation of the parameter of a standard phrase-based model as in (Le et al., 2012b), also adding an inverse translation model"
W14-3330,P11-2031,0,0.0126431,"ces from PATTR -A BSTRACTS having the lowest perplexity according to 3-gram language models trained on both sides of the D EVEL set. This test set, denoted by L M T EST, is however highly biaised, especially because of the high redundancy in PATTR -A BSTRACTS, and should be used with great care when tuning or comparing systems. 3.5 Evaluation Metrics All BLEU scores (Papineni et al., 2002) are computed using cased multi-bleu with our internal tokenization. Reported results correspond to the average and standard deviation across 3 optimization runs to better account for the optimizer variance (Clark et al., 2011). Systems N CODE We use N CODE with default settings, 3gram bilingual translation models on words and 4gram bilingual translation factor models on POS, for each included corpora (see Table 1) and for the concatenation of them all. 4 4.1 OTF When using our OTF system, all indomain and out-of-domain data are concatenated, respectively. For both corpora, we use a maximum random sampling size of 1 000 examples and a maximum phrase length of 15. However, all sub-corpora but G IGA3 are used to compute the vectors for VSM features. Decoding is done with M OSES4 (Koehn et al., 2007). Experiments Tunin"
W14-3330,N12-1005,1,0.885486,"score between each phrase pair’s vector and the development set vector is added into the phrase table as a VSM feature. We also replace the joint count with the marginal count of the source/target phrase to compute an alternative average representation for the development set, thus adding two VSM additional features. 2.4 S OUL Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve discrete language models. As for our submitted translation systems to WMT’12 and WMT’13 (Le et al., 2012b; Allauzen et al., 2013), we take advantage of the recent proposal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (S OUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allows us to consider longer dependencies than the one used by conventional n-gram models (e.g. n = 10 instead of n = 4). Additionally, conti"
W14-3330,C08-1064,0,0.0173213,"sents the predicted joint count of f¯ and e¯. The other models in this system are the same as in the default configuration of M OSES. 2.3 countdev (f¯j , e¯k )wc (f¯j , e¯k ) (3) j=0 k=0 We develop an alternative approach implementing an on-the-fly estimation of the parameter of a standard phrase-based model as in (Le et al., 2012b), also adding an inverse translation model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on on-the-fly model estimation for SMT (CallisonBurch et al., 2005; Lopez, 2008), we first build a suffix array for the source corpus. Only a limited number of translation examples, selected by deterministic random sampling, are then used by traversing the suffix array appropriately. A coherent translation probability (Lopez, 2008) (which also takes into account examples where translation extraction failed) is then estimated. As we cannot compute exactly an inverse translation probability (because sampling is performed independently for each source phrase), we resort to the following approximation:   p(¯ e|f¯) × f req(f¯) ¯ p(f |¯ e) = min 1.0, f req(¯ e) J X K X Vector"
W14-3330,J06-4004,0,0.0612575,"Missing"
W14-3330,P03-1021,0,0.0609898,"Attempting include one language model per sub-corpora yielded a significant drop in performance. 248 3.4 Proxy Test Set System Combination As N CODE and OTF differ in many aspects and make different errors, we use system combination techniques to take advantage of their complementarity. This is done by reranking the concatenation of the 1 000-best lists of both systems. For each hypothesis within this list, we use two global features, corresponding either to the score computed by the corresponding system or 0 otherwise. We then learn reranking weights using Minimum Error Rate Training (MERT) (Och, 2003) on the development set for this combined list, using only these two features (SysComb-2). In an alternative configuration, we use the two systems without the S OUL rescoring, and add instead the five S OUL scores as features in the system combination reranking (SysComb-7). For this first edition of a Medical Translation Task, only a very small development set was made available (D EVEL in Table 1). This made both system design and tuning challenging. In fact, with such a small development set, conventional tuning methods are known to be very unstable and prone to overfitting, and it would be"
W14-3330,P02-1040,0,0.0986367,"it would be suboptimal to select a configuration based on results on the development set only.2 To circumvent this, we artificially created our own internal test set by randomly selecting 3 000 sentences out from the 30 000 sentences from PATTR -A BSTRACTS having the lowest perplexity according to 3-gram language models trained on both sides of the D EVEL set. This test set, denoted by L M T EST, is however highly biaised, especially because of the high redundancy in PATTR -A BSTRACTS, and should be used with great care when tuning or comparing systems. 3.5 Evaluation Metrics All BLEU scores (Papineni et al., 2002) are computed using cased multi-bleu with our internal tokenization. Reported results correspond to the average and standard deviation across 3 optimization runs to better account for the optimizer variance (Clark et al., 2011). Systems N CODE We use N CODE with default settings, 3gram bilingual translation models on words and 4gram bilingual translation factor models on POS, for each included corpora (see Table 1) and for the concatenation of them all. 4 4.1 OTF When using our OTF system, all indomain and out-of-domain data are concatenated, respectively. For both corpora, we use a maximum ra"
W14-3330,P06-2093,0,0.0274218,"ment data, respectively, and countdev (f¯j , e¯k ) is the joint count of phrase pairs (f¯j , e¯k ) found in the development set. The similarity score between each phrase pair’s vector and the development set vector is added into the phrase table as a VSM feature. We also replace the joint count with the marginal count of the source/target phrase to compute an alternative average representation for the development set, thus adding two VSM additional features. 2.4 S OUL Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve discrete language models. As for our submitted translation systems to WMT’12 and WMT’13 (Le et al., 2012b; Allauzen et al., 2013), we take advantage of the recent proposal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (S OUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allo"
W14-3330,N04-4026,0,0.0199797,"(§2.3), and POS-tagging adaptation to the medical domain (§3.3). We also performed a small-scale error analysis of the outputs of some of our systems (§5). K X λk fk (f , e, a) (1) k=1 where K feature functions (fk ) are weighted by a set of coefficients (λk ) and a denotes the set of hidden variables corresponding to the reordering and segmentation of the source sentence. Along with the n-gram translation models and target ngram language models, 13 conventional features are combined: 4 lexicon models similar to the ones used in standard phrase-based systems; 6 lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. Features are estimated during the training phase. Training source sentences are first reordered so as to match 246 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 246–253, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics (see Table 1). Each component wc (f¯, e¯) is a standard"
W14-3330,vilar-etal-2006-error,0,0.0370322,"Missing"
W16-2337,W14-3310,0,0.0473738,"Missing"
W16-2337,W15-3004,0,0.046834,"Missing"
W16-2337,W11-2123,0,0.0354078,"Missing"
W16-2337,2007.mtsummit-papers.33,0,0.146139,"Missing"
W16-2337,P96-1041,0,0.133579,"Missing"
W16-2337,N12-1047,0,0.0418397,"Missing"
W16-2337,2008.amta-srw.3,0,0.0600494,"Missing"
W16-2337,W08-0310,1,0.794017,"Missing"
W16-2337,P03-1054,0,0.0149962,"Missing"
W16-2337,W15-1001,0,0.0608976,"Missing"
W16-2337,P07-2045,0,0.0110378,"Missing"
W16-2337,2009.mtsummit-posters.7,0,0.0562614,"Missing"
W16-2337,N13-1073,0,0.0999667,"Missing"
W16-2337,E14-2008,0,0.0421507,"Missing"
W16-2337,N12-1005,1,0.905568,"Missing"
W16-2337,N03-1033,0,0.121475,"Missing"
W16-2337,2015.mtsummit-wpslt.8,0,0.0624102,"Missing"
W16-2337,I13-1128,0,0.0593276,"Missing"
W16-2337,W15-3016,1,0.873473,"Missing"
W16-2337,E06-1005,0,0.104963,"Missing"
W16-2337,P02-1040,0,0.0954205,"Missing"
W16-2337,W08-0329,0,0.056727,"Missing"
W16-2337,W12-3124,0,0.0441953,"Missing"
W18-3814,P05-1074,0,0.196505,"rbelnet, 1958; Chuquet and Paillard, 1989), which categorize different translation techniques apart from literal translations. But to the best of our knowledge, no automatic processing techniques explicitly implement these interlingual relations. As an important natural language understanding and generation task, machine translation (MT) has been seriously improved with first phrase-based statistical machine translation (PBMT) then recently with neural machine translation (NMT). MT has also been exploited to generate paraphrases from bilingual parallel corpus, which was originally proposed by Bannard and Callison-Burch (2005). The assumption is that two segments in the same language are potential paraphrases if they share common translations in a foreign language. Currently the largest resource of paraphrases, PPDB (Paraphrase Database) (Ganitkevitch et al., 2013), has been built following this method exploiting translational equivalence. Nonetheless, the work of Pavlick et al. (2015) revealed that there exist other relations than strict equivalence (paraphrase) in PPDB (i.e. Entailment (in two directions), Exclusion, Other related and Independent). The existence of these other relations in PPDB reflects the lack"
W18-3814,2012.eamt-1.60,0,0.0425191,"difficulties for recent NMT systems. The problems include, in particular, incomplete generalizations; translating common and syntactically flexible idioms, or crossing movement verbs e.g. swim across X → traverser X à la nage. The annotated corpus that we present here could also constitute a challenge set, for the purpose of evaluating MT systems when human translators resort to different translation relations. 3 Corpus In order to study translation relations for several pairs of languages, we have worked on a multilingual parallel corpus. This corpus is available from the Web inventory WIT3 (Cettolo et al., 2012), which gives access to a collection of transcribed and translated talks, including the corpus of TED Talks3 . This corpus was released for the evaluation campaign IWSLT 2013 and 2014.4 The source language, i.e. the original language in which the speakers expressed themselves, is English. We have calculated the intersection of a parallel corpus with translations in French5 , Chinese, Arabic, Spanish and Russian. The translation of subtitles for TED Talks is controlled by volunteers and language coordinators per language6 , which generally ensures good quality translations. The corpus to be ann"
W18-3814,N13-1073,0,0.0121859,"ogy, grammar, expression, etc. For English and French languages, the corpus has been tokenized by Stanford Tokenizer7 , and lemmatized by TreeTagger (Schmid, 1995) while keeping the tokenization of Stanford Tokenizer. The capital letters at the beginning of each sentence are kept only if these words always appear with capital initials elsewhere in the corpus, otherwise they are lowercased. We have used the tool THULAC (Li and Sun, 2009) for the segmentation of the Chinese corpus, and proceeded to several corrections before annotation. The words are automatically aligned by training FastAlign (Dyer et al., 2013), with its default parameters on each entire parallel corpus (i.e. 163 092 lines and 3 303 660 English tokens). We import 2 http://www.parseme.eu/ https://wit3.fbk.eu/ 4 We have used training corpus of 2014 (160 656 lines), development corpus (880 lines) and test corpus (1 556 lines) of 2010. 5 The sentence boundaries have been corrected in French test corpus to calculate the intersection. 6 https://www.ted.com/participate/translate/get-started 7 http://nlp.stanford.edu/software/tokenizer.shtml 3 103 these automatic alignments before annotation to accelerate the process, in particular for the"
W18-3814,N13-1092,0,0.23119,"Missing"
W18-3814,P08-4006,0,0.1314,"l suddenly discover what it would be like → et vous découvrirez ce que ce serait 12. Unaligned and no type attributed: function words necessary in one language but not in the other; segments not translated but which don’t impact the meaning; segments giving repeated information in context; translated segments which don’t correspond to any source segment: minus 271 degrees, colder than → moins 271 degrés, ce qui est plus froid the last example I have time to → le dernier exemple que j’ai le temps de 105 5 Annotation 5.1 Annotation tool and configuration We have used the Web application Yawat8 (Germann, 2008), which allows us to align words or segments (continuous or discontinuous), and then to assign labels adapted to our task on monolingual or bilingual units (see figure 2). Figure 2: Annotation interface of Yawat. Here is a trilingual example from our corpus: well, we use that great euphemism, ""trial and error"", which is exposed to be meaningless. eh bien, nous employons cet euphémisme, procéder par tâtonnements, qui est dénué de sens. 我们 ""we"" 普通人 ""ordinary people"" 会 ""particle for future tense"" 做 ""do"" 各种各样 ""diverse"" 的 ""particle for attribute"" 实验 ""experience"" 不断 ""continuously"" 地 ""particle for ad"
W18-3814,D17-1263,0,0.0829718,"ltiword Expressions) 2 is a European scientific network built up to elaborate universal terminologies and annotation guidelines for MWEs in 18 languages (Savary et al., 2015). Its main outcome is a multilingual 5-million word annotated corpus, which underlies a shared task on automatic identification of verbal MWEs (Savary et al., 2017). Our trilingual annotated corpus focuses on bilingual relation between translations, and we annotate all words in the corpus, including continuous and discontinuous MWEs. As a complement of MT evaluation metrics, which reflect imperfectly systems’ performance, Isabelle et al. (2017) have introduced a challenge set based on difficult linguistic materials. The authors could hence determine some remaining difficulties for recent NMT systems. The problems include, in particular, incomplete generalizations; translating common and syntactically flexible idioms, or crossing movement verbs e.g. swim across X → traverser X à la nage. The annotated corpus that we present here could also constitute a challenge set, for the purpose of evaluating MT systems when human translators resort to different translation relations. 3 Corpus In order to study translation relations for several p"
W18-3814,J09-4006,0,0.132799,"rench and English-Chinese corpora to validate our hierarchy of translation relations, since these two target languages are very dissimilar in several linguistic aspects: morphology, grammar, expression, etc. For English and French languages, the corpus has been tokenized by Stanford Tokenizer7 , and lemmatized by TreeTagger (Schmid, 1995) while keeping the tokenization of Stanford Tokenizer. The capital letters at the beginning of each sentence are kept only if these words always appear with capital initials elsewhere in the corpus, otherwise they are lowercased. We have used the tool THULAC (Li and Sun, 2009) for the segmentation of the Chinese corpus, and proceeded to several corrections before annotation. The words are automatically aligned by training FastAlign (Dyer et al., 2013), with its default parameters on each entire parallel corpus (i.e. 163 092 lines and 3 303 660 English tokens). We import 2 http://www.parseme.eu/ https://wit3.fbk.eu/ 4 We have used training corpus of 2014 (160 656 lines), development corpus (880 lines) and test corpus (1 556 lines) of 2010. 5 The sentence boundaries have been corrected in French test corpus to calculate the intersection. 6 https://www.ted.com/partici"
W18-3814,P15-1146,0,0.485166,"tatistical machine translation (PBMT) then recently with neural machine translation (NMT). MT has also been exploited to generate paraphrases from bilingual parallel corpus, which was originally proposed by Bannard and Callison-Burch (2005). The assumption is that two segments in the same language are potential paraphrases if they share common translations in a foreign language. Currently the largest resource of paraphrases, PPDB (Paraphrase Database) (Ganitkevitch et al., 2013), has been built following this method exploiting translational equivalence. Nonetheless, the work of Pavlick et al. (2015) revealed that there exist other relations than strict equivalence (paraphrase) in PPDB (i.e. Entailment (in two directions), Exclusion, Other related and Independent). The existence of these other relations in PPDB reflects the lack of semantic control during the paraphrasing process. We propose a categorization of translation relations which model human translators’ choices, and we annotate a multilingual (English, French, Chinese) parallel corpus of oral presentations, the TED Talks1 , with these relations. The annotation is still ongoing and we are developing a classifier based on these an"
