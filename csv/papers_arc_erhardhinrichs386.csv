2020.lrec-1.538,All That Glitters is Not Gold: A Gold Standard of Adjective-Noun Collocations for {G}erman,2020,-1,-1,4,0,17750,yana strakatova,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper we present the GerCo dataset of adjective-noun collocations for German, such as alter Freund {`}old friend{'} and tiefe Liebe {`}deep love{'}. The annotation has been performed by experts based on the annotation scheme introduced in this paper. The resulting dataset contains 4,732 positive and negative instances of collocations and covers all the 16 semantic classes of adjectives as defined in the German wordnet GermaNet. The dataset can serve as a reliable empirical basis for comparing different theoretical frameworks concerned with collocations or as material for data-driven approaches to the studies of collocations including different machine learning experiments. This paper addresses the latter issue by using the GerCo dataset for evaluating different models on the task of automatic collocation identification. We compare lexical association measures with static and contextualized word embeddings. The experiments show that word embeddings outperform methods based on statistical association measures by a wide margin."
2020.coling-main.269,When Beards Start Shaving Men: A Subject-object Resolution Test Suite for Morpho-syntactic and Semantic Model Introspection,2020,-1,-1,3,1,21365,patricia fischer,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper, we introduce the SORTS Subject-Object Resolution Test Suite of German minimal sentence pairs for model introspection. The full test suite consists of 18,502 transitive clauses with manual annotations of 8 word order patterns, 5 morphological and syntactic and 11 semantic property classes. The test suite has been constructed such that sentences are minimal pairs with respect to a property class. Each property has been selected with a particular focus on its effect on subject-object resolution, the second-most error-prone task within syntactic parsing of German after prepositional phrase attachment (Fischer et al., 2019). The size and detail of annotations make the test suite a valuable resource for natural language processing applications with syntactic and semantic tasks. We use dependency parsing to demonstrate how the test suite allows insights into the process of subject-object resolution. Based on the test suite annotations, word order and case syncretism can be identified as most important factors that affect subject-object resolution."
W19-5112,Semantic Modelling of Adjective-Noun Collocations Using {F}rame{N}et,2019,0,0,2,0,17750,yana strakatova,Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019),0,"In this paper we argue that Frame Semantics (Fillmore, 1982) provides a good framework for semantic modelling of adjective-noun collocations. More specifically, the notion of a frame is rich enough to account for nouns from different semantic classes and to model semantic relations that hold between an adjective and a noun in terms of Frame Elements. We have substantiated these findings by considering a sample of adjective-noun collocations from German such as {``}enger Freund{''} {`}close friend{'} and {``}starker Regen{''} {`}heavy rain{'}. The data sample is taken from different semantic fields identified in the German wordnet GermaNet (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010). The study is based on the electronic dictionary DWDS (Klein and Geyken, 2010) and uses the collocation extraction tool Wortprofil (Geyken et al., 2009). The FrameNet modelling is based on the online resource available at http://framenet.icsi.berkeley.edu. Since FrameNets are available for a range of typologically different languages, it is feasible to extend the current case study to other languages."
Q19-1025,No Word is an {I}sland{---}{A} Transformation Weighting Model for Semantic Composition,2019,25,0,4,1,25412,corina dima,Transactions of the Association for Computational Linguistics,0,"Composition models of distributional semantics are used to construct phrase representations from the representations of their words. Composition models are typically situated on two ends of a spectrum. They either have a small number of parameters but compose all phrases in the same way, or they perform word-specific compositions at the cost of a far larger number of parameters. In this paper we propose transformation weighting (TransWeight), a composition model that consistently outperforms existing models on nominal compounds, adjective-noun phrases, and adverb-adjective phrases in English, German, and Dutch. TransWeight drastically reduces the number of parameters needed compared with the best model in the literature by composing similar words in the same way."
2019.gwc-1.4,Including {S}wiss Standard {G}erman in {G}erma{N}et,2019,-1,-1,2,0,18015,eva huber,Proceedings of the 10th Global Wordnet Conference,0,"GermaNet (Henrich and Hinrichs, 2010; Hamp and Feldweg, 1997) is a comprehensive wordnet of Standard German spoken in the Federal Republic of Germany. The GermaNet team aims at modelling the basic vocabulary of the language. German is an official language or a minority language in many countries. It is an official language in Austria, Germany and Switzerland, each with its own codified standard variety (Auer, 2014, p. 21), and also in Belgium, Liechtenstein, and Luxemburg. German is recognized as a minority language in thirteen additional countries, including Brasil, Italy, Poland, and Russia. However, the different standard varieties of German are currently not represented in GermaNet. With this project, we make a start on changing this by including one variety, namely Swiss Standard German, into GermaNet. This shall give a more inclusive perspective on the German language. We will argue that Swiss Standard German words, Helvetisms, are best included into the already existing wordnet GermaNet, rather than creating them as a separate wordnet."
L18-1206,Bridging the {LAPPS} {G}rid and {CLARIN},2018,0,0,1,1,17752,erhard hinrichs,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-7603,Distributional regularities of verbs and verbal adjectives: Treebank evidence and broader implications,2017,-1,-1,4,1,21366,daniel kok,Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories,0,None
W17-0404,"Converting the {T}{\\\u}{B}a-{D}/{Z} Treebank of {G}erman to {U}niversal {D}ependencies""",2017,0,0,3,0.454545,11539,ccaugri ccoltekin,Proceedings of the {N}o{D}a{L}i{D}a 2017 Workshop on Universal Dependencies ({UDW} 2017),0,None
K17-3013,The parse is darc and full of errors: Universal dependency parsing with transition-based and graph-based algorithms,2017,15,1,4,0,32753,kuan yu,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We developed two simple systems for dependency parsing: darc, a transition-based parser, and mstnn, a graph-based parser. We tested our systems in the CoNLL 2017 UD Shared Task, with darc being the official system. Darc ranked 12th among 33 systems, just above the baseline. Mstnn had no official ranking, but its main score was above the 27th. In this paper, we describe our two systems, examine their strengths and weaknesses, and discuss the lessons we learned."
E17-2050,{PP} Attachment: Where do We Stand?,2017,22,1,4,1,21366,daniel kok,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Prepostitional phrase (PP) attachment is a well known challenge to parsing. In this paper, we combine the insights of different works, namely: (1) treating PP attachment as a classification task with an arbitrary number of attachment candidates; (2) using auxiliary distributions to augment the data beyond the hand-annotated training set; (3) using topological fields to get information about the distribution of PP attachment throughout clauses and (4) using state-of-the-art techniques such as word embeddings and neural networks. We show that jointly using these techniques leads to substantial improvements. We also conduct a qualitative analysis to gauge where the ceiling of the task is in a realistic setup."
W16-2012,Letter Sequence Labeling for Compound Splitting,2016,14,1,3,1,6735,jianqiang ma,"Proceedings of the 14th {SIGMORPHON} Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,None
W16-1908,Learning Phone Embeddings for Word Segmentation of Child-Directed Speech,2016,35,0,3,1,6735,jianqiang ma,Proceedings of the 7th Workshop on Cognitive Aspects of Computational Language Learning,0,"This paper presents a novel model that learns and exploits embeddings of phone ngrams for word segmentation in child language acquisition. Embedding-based models are evaluated on a phonemically transcribed corpus of child-directed speech, in comparison with their symbolic counterparts using the common learning framework and features. Results show that learning embeddings significantly improves performance. We make use of extensive visualization to understand what the model has learned. We show that the learned embeddings are informative for both word segmentation and phonology in general."
P16-2001,Transition-based dependency parsing with topological fields,2016,17,1,2,1,21366,daniel kok,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The topological field model is commonly used to describe the regularities in German word order. In this work, we show that topological fields can be predicted reliably using sequence labeling and that the predicted field labels can inform a transitionbased dependency parser."
W15-0122,Automatic Noun Compound Interpretation using Deep Neural Networks and Word Embeddings,2015,18,9,2,1,25412,corina dima,Proceedings of the 11th International Conference on Computational Semantics,0,"The present paper reports on the results of automatic noun compound interpretation for English using a deep neural network classifier and a selection of publicly available word embeddings to represent the individual compound constituents. The task at hand consists of identifying the semantic relation that holds between the constituents of a compound (e.g. WHOLEPART_OR_MEMBER_OF in the case of xe2x80x98robot armxe2x80x99, LOCATION in the case of xe2x80x98hillside homexe2x80x99). The experiments reported in the present paper use the noun compound dataset described in Tratz (2011), a revised version of the dataset used by Tratz and Hovy (2010) for training their Maximum Entropy classifier. Our experiments yield results that are comparable to those reported in Tratz and Hovy (2010) in a crossvalidation setting, but outperform their system on unseen compounds by a large margin."
P15-1167,Accurate Linear-Time {C}hinese Word Segmentation via Embedding Matching,2015,38,27,2,1,6735,jianqiang ma,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"This paper proposes an embedding matching approach to Chinese word segmentation, which generalizes the traditional sequence labeling framework and takes advantage of distributed representations. The training and prediction algorithms have linear-time complexity. Based on the proposed model, a greedy segmenter is developed and evaluated on benchmark corpora. Experiments show that our greedy segmenter achieves improved results over previous neural network-based word segmenters, and its performance is competitive with state-of-the-art methods, despite its simple feature set and the absence of external resources for training."
W14-0107,Modeling Prefix and Particle Verbs in {G}erma{N}et,2014,18,1,2,0,38880,christina hoppermann,Proceedings of the Seventh Global {W}ordnet Conference,0,"Verbal word formation processes involving prefixes and particles are highly productive in Germanic languages. The compositional semantics of such prefix and particle verbs requires an in-depth analysis of the interdependence of their constituent parts for adequately representing these types of complex verbs in lexical-semantic networks. The present paper introduces modeling principles that account for such language-specific phenomena in the German wordnet GermaNet (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010), considering the continuum between full semantic transparency and highly lexicalized meanings as well as the semantic contribution of the prefix or particle to the meaning of the complex verb as a whole."
W14-0109,Aligning Word Senses in {G}erma{N}et and the {DWDS} Dictionary of the {G}erman Language,2014,12,0,2,1,32946,verena henrich,Proceedings of the Seventh Global {W}ordnet Conference,0,"A comparison and alignment of lexical resources brings about considerable mutual benefits for all resources involved. For all sense distinctions that are completely parallel in two resources, such an alignment provides supporting external evidence for the validity of sense distinction and allows enriching word senses by information contained in the other resource. By contrast, for all non-matching sense distinctions, reason for revisiting and possibly revising the lexical entries in question is provided. The purpose of this paper is to compare the German wordnet GermaNet with the Digital Dictionary of the German Language (DWDS) and to align word senses in the two resources. The paper presents issues that arise in practice when such an alignment is performed and indicates the benefits that both resources will gain."
dima-etal-2014-tell,How to Tell a Schneemann from a Milchmann: An Annotation Scheme for Compound-Internal Relations,2014,0,3,3,1,25412,corina dima,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents a language-independent annotation scheme for the semantic relations that link the constituents of noun-noun compounds, such as Schneemann {`}snow man{'} or Milchmann {`}milk man{'}. The annotation scheme is hybrid in the sense that it assigns each compound a two-place label consisting of a semantic property and a prepositional paraphrase. The resulting inventory combines the insights of previous annotation schemes that rely exclusively on either semantic properties or prepositions, thus avoiding the known weaknesses that result from using only one of the two label types. The proposed annotation scheme has been used to annotate a set of 5112 German noun-noun compounds. A release of the dataset is currently being prepared and will be made available via the CLARIN Center T{\""u}bingen. In addition to the presentation of the hybrid annotation scheme, the paper also reports on an inter-annotator agreement study that has resulted in a substantial agreement among annotators."
de-smedt-etal-2014-clara,{CLARA}: A New Generation of Researchers in Common Language Resources and Their Applications,2014,68,0,2,0,17515,koenraad smedt,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"CLARA (Common Language Resources and Their Applications) is a Marie Curie Initial Training Network which ran from 2009 until 2014 with the aim of providing researcher training in crucial areas related to language resources and infrastructure. The scope of the project was broad and included infrastructure design, lexical semantic modeling, domain modeling, multimedia and multimodal communication, applications, and parsing technologies and grammar models. An international consortium of 9 partners and 12 associate partners employed researchers in 19 new positions and organized a training program consisting of 10 thematic courses and summer/winter schools. The project has resulted in new theoretical insights as well as new resources and tools. Most importantly, the project has trained a new generation of researchers who can perform advanced research and development in language resources and technologies."
hinrichs-krauwer-2014-clarin,The {CLARIN} Research Infrastructure: Resources and Tools for e{H}umanities Scholars,2014,28,19,1,1,17752,erhard hinrichs,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"CLARIN is the short name for the Common Language Resources and Technology Infrastructure, which aims at providing easy and sustainable access for scholars in the humanities and social sciences to digital language data and advanced tools to discover, explore, exploit, annotate, analyse or combine them, independent of where they are located. CLARIN is in the process of building a networked federation of European data repositories, service centers and centers of expertise, with single sign-on access for all members of the academic community in all participating countries. Tools and data from different centers will be interoperable so that data collections can be combined and tools from different sources can be chained to perform complex operations to support researchers in their work. Interoperability of language resources and tools in the federation of CLARIN Centers is ensured by adherence to TEI and ISO standards for text encoding, by the use of persistent identifiers, and by the observance of common protocols. The purpose of the present paper is to give an overview of language resources, tools, and services that CLARIN presently offers."
W12-3624,Annotating Coordination in the {P}enn {T}reebank,2012,11,10,3,0,1564,wolfgang maier,Proceedings of the Sixth Linguistic Annotation Workshop,0,"Finding coordinations provides useful information for many NLP endeavors. However, the task has not received much attention in the literature. A major reason for that is that the annotation of major treebanks does not reliably annotate coordination. This makes it virtually impossible to detect coordinations in which two conjuncts are separated by punctuation rather than by a coordinating conjunction. In this paper, we present an annotation scheme for the Penn Treebank which introduces a distinction between coordinating from non-coordinating punctuation. We discuss the general annotation guidelines as well as problematic cases. Eventually, we show that this additional annotation allows the retrieval of a considerable number of coordinate structures beyond the ones having a coordinating conjunction."
henrich-hinrichs-2012-comparative,A Comparative Evaluation of Word Sense Disambiguation Algorithms for {G}erman,2012,30,5,2,1,32946,verena henrich,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The present paper explores a wide range of word sense disambiguation (WSD) algorithms for German. These WSD algorithms are based on a suite of semantic relatedness measures, including path-based, information-content-based, and gloss-based methods. Since the individual algorithms produce diverse results in terms of precision and thus complement each other well in terms of coverage, a set of combined algorithms is investigated and compared in performance to the individual algorithms. Among the single algorithms considered, a word overlap method derived from the Lesk algorithm that uses Wiktionary glosses and GermaNet lexical fields yields the best F-score of 56.36. This result is outperformed by a combined WSD algorithm that uses weighted majority voting and obtains an F-score of 63.59. The WSD experiments utilize the German wordnet GermaNet as a sense inventory as well as WebCAGe (short for: Web-Harvested Corpus Annotated with GermaNet Senses), a newly constructed, sense-annotated corpus for this language. The WSD experiments also confirm that WSD performance is lower for words with fine-grained sense distinctions compared to words with coarse-grained senses."
hinrichs-zastrow-2012-automatic,"Automatic Annotation and Manual Evaluation of the Diachronic {G}erman Corpus {T}{\\\u}{B}a-{D}/{DC}""",2012,7,4,1,1,17752,erhard hinrichs,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents the Tu{\`I}Âbingen Baumbank des Deutschen Diachron (Tu{\`I}ÂBa-D/DC), a linguistically annotated corpus of selected diachronic materials from the German Gutenberg Project. It was automatically annotated by a suite of NLP tools integrated into WebLicht, the linguistic chaining tool used in CLARIN-D. The annotation quality has been evaluated manually for a subcorpus ranging from Middle High German to Modern High German. The integration of the Tu{\`I}ÂBa-D/DC into the CLARIN-D infrastructure includes metadata provision and harvesting as well as sustainable data storage in the Tu{\`I}Âbingen CLARIN-D center. The paper further provides an overview of the possibilities of accessing the Tu{\`I}ÂBa-D/DC data. Methods for full-text search of the metadata and object data and for annotation-based search of the object data are described in detail. The WebLicht Service Oriented Architecture is used as an integrated environment for annotation based search of the Tu{\`I}ÂBa-D/DC. WebLicht thus not only serves as the annotation platform for the Tu{\`I}ÂBa-D/DC, but also as a generic user interface for accessing and visualizing it."
dima-etal-2012-metadata,A Metadata Editor to Support the Description of Linguistic Resources,2012,6,9,3,0,29828,emanuel dima,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Creating and maintaining metadata for various kinds of resources requires appropriate tools to assist the user. The paper presents the metadata editor ProFormA for the creation and editing of CMDI (Component Metadata Infrastructure) metadata in web forms. This editor supports a number of CMDI profiles currently being provided for different types of resources. Since the editor is based on XForms and server-side processing, users can create and modify CMDI files in their standard browser without the need for further processing. Large parts of ProFormA are implemented as web services in order to reuse them in other contexts and programs."
dima-etal-2012-repository,A Repository for the Sustainable Management of Research Data,2012,6,2,3,0,29828,emanuel dima,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents the system architecture as well as the underlying workflow of the Extensible Repository System of Digital Objects (ERDO) which has been developed for the sustainable archiving of language resources within the T{\""u}bingen CLARIN-D project. In contrast to other approaches focusing on archiving experts, the described workflow can be used by researchers without required knowledge in the field of long-term storage for transferring data from their local file systems into a persistent repository."
E12-1039,{W}eb{CAG}e {--} A Web-Harvested Corpus Annotated with {G}erma{N}et Senses,2012,15,20,2,1,32946,verena henrich,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper describes an automatic method for creating a domain-independent sense-annotated corpus harvested from the web. As a proof of concept, this method has been applied to German, a language for which sense-annotated corpora are still in short supply. The sense inventory is taken from the German wordnet GermaNet. The web-harvesting relies on an existing mapping of GermaNet to the German version of the web-based dictionary Wiktionary. The data obtained by this method constitute WebCAGe (short for: Web-Harvested Corpus Annotated with GermaNet Senses), a resource which currently represents the largest sense-annotated corpus available for German. While the present paper focuses on one particular language, the method as such is language-independent."
R11-1057,"A Semi-Automatic, Iterative Method for Creating a Domain-Specific Treebank",2011,10,0,2,1,25412,corina dima,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"In this paper we present the development process of NLP-QT, a question treebank that will be used for data-driven parsing in the context of a domain-specific QA system for querying NLP resource metadata. We motivate the need to build NLP-QT as a resource in its own right, by comparing the Penn Treebank-style annotation scheme used for QuestionBank (Judge et al., 2006) with the modified NP annotation for the Penn Treebank introduced by Vadas and Curran (2007). We argue that this modified annotation scheme provides a better interface representation for semantic interpretation and show how it can be incorporated into the NLP-QT resource, without significant loss in parser performance. The parsing experiments reported in the paper confirm the feasibility of an iterative, semi-automatic construction of the NLP-QT resource similar to the approach taken for QuestionBank. At the same time, we propose to improve the iterative refinement technique used for QuestionBank by adopting Hwa (2001)xe2x80x99s heuristics for selecting additional material to be handcorrected and added to the data set at each iteration."
R11-1058,Determining Immediate Constituents of Compounds in {G}erma{N}et,2011,9,15,2,1,32946,verena henrich,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"In order to be able to systematically link compounds in GermaNet to their constituent parts, compound splitting needs to be applied recursively and has to identify the immediate constituents at each level of analysis. Existing tools for compound splitting for German only offer an analysis of all component parts of a compound at once without any grouping of subconstituents. Thus, existing tools for splitting compounds were adapted to overcome this issue. Algorithms combining three heterogeneous kinds of compound splitters are developed to achieve better results. The best overall result with an accuracy of 92.42% is achieved by a hybrid combined compound splitter that takes into account all knowledge provided by the individual compound splitters, and in addition some domain knowledge about German derivation morphology and compounding."
W10-1821,Chunking {G}erman: An Unsolved Problem,2010,12,4,3,0.447914,2237,sandra kubler,Proceedings of the Fourth Linguistic Annotation Workshop,0,"This paper describes a CoNLL-style chunk representation for the Tubingen Treebank of Written German, which assumes a flat chunk structure so that each word belongs to at most one chunk. For German, such a chunk definition causes problems in cases of complex prenominal modification. We introduce a flat annotation that can handle these structures via a stranded noun chunk."
P10-4004,{G}ern{E}di{T}: A Graphical Tool for {G}erma{N}et Development,2010,3,3,2,1,32946,verena henrich,Proceedings of the {ACL} 2010 System Demonstrations,0,"GernEdiT (short for: GermaNet Editing Tool) offers a graphical interface for the lexicographers and developers of GermaNet to access and modify the underlying GermaNet resource. GermaNet is a lexical-semantic wordnet that is modeled after the Princeton Word-Net for English. The traditional lexicographic development of GermaNet was error prone and time-consuming, mainly due to a complex underlying data format and no opportunity of automatic consistency checks. GernEdiT replaces the earlier development by a more userfriendly tool, which facilitates automatic checking of internal consistency and correctness of the linguistic resource. This paper presents all these core functionalities of GernEdiT along with details about its usage and usability."
P10-4005,{W}eb{L}icht: Web-Based {LRT} Services for {G}erman,2010,4,42,1,1,17752,erhard hinrichs,Proceedings of the {ACL} 2010 System Demonstrations,0,"This software demonstration presents WebLicht (short for: Web-Based Linguistic Chaining Tool), a web-based service environment for the integration and use of language resources and tools (LRT). WebLicht is being developed as part of the D-SPIN project. We-bLicht is implemented as a web application so that there is no need for users to install any software on their own computers or to concern themselves with the technical details involved in building tool chains. The integrated web services are part of a prototypical infrastructure that was developed to facilitate chaining of LRT services. WebLicht allows the integration and use of distributed web services with standardized APIs. The nature of these open and standardized APIs makes it possible to access the web services from nearly any programming language, shell script or workflow engine (UIMA, Gate etc.) Additionally, an application for integration of additional services is available, allowing anyone to contribute his own web service."
henrich-hinrichs-2010-gernedit,{G}ern{E}di{T} - The {G}erma{N}et Editing Tool,2010,2,4,2,1,32946,verena henrich,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper introduces GernEdiT (short for: GermaNet Editing Tool), a new graphical user interface for the lexicographers and developers of GermaNet, the German version of the Princeton WordNet. GermaNet is a lexical-semantic net that relates German nouns, verbs, and adjectives. Traditionally, lexicographic work for extending the coverage of GermaNet utilized the Princeton WordNet development environment of lexicographer files. Due to a complex data format and no opportunity of automatic consistency checks, this process was very error prone and time consuming. The GermaNet Editing Tool GernEdiT was developed to overcome these shortcomings. The main purposes of the GernEdiT tool are, besides supporting lexicographers to access, modify, and extend GermaNet data in an easy and adaptive way, as follows: Replace the standard editing tools by a more user-friendly tool, use a relational database as data storage, support export formats in the form of XML, and facilitate internal consistency and correctness of the linguistic resource. All these core functionalities of GernEdiT along with the main aspects of the underlying lexical resource GermaNet and its current database format are presented in this paper."
hinrichs-etal-2010-sustainability,Sustainability of Linguistic Data and Analysis in the Context of a Collaborative e{S}cience Environment,2010,4,1,1,1,17752,erhard hinrichs,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"For researchers, it is especially important that primary research data are preserved and made available on a long-term basis and to a wide variety of researchers. In order to ensure long-term availability of the archived data, it is imperative that the data to be stored is conformant with standardized data formats and best practices followed by the relevant research communities. Storing, managing, and accessing such standard-conformant data requires a repository-based infrastructure. Two projects at the University of T{\""u}bingen are realizing a collaborative eScience research environment with the help of eSciDoc for the university that supports long-term preservation of all kinds of data as well as a fine-grained and contextualized data management: the INF project and the BW-eSci(T) project. The task of the infrastructure (INF) project within the collaborative research centre {\^a}ÂÂEmergence of MeaningÂ (SFB 833) is to guarantee the long-term availability of the SFBs data. BW-eSci(T) is a joint project of the University of T{\""u}bingen and the Fachinformationszentrums (FIZ) Karlsruhe. The goal of this project is to develop a prototypical eScience research environment for the University of T{\""u}bingen."
hinrichs-etal-2010-weblicht,{W}eb{L}icht: Web-based {LRT} Services in a Distributed e{S}cience Infrastructure,2010,4,33,3,0,29723,marie hinrichs,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"eScience - enhanced science - is a new paradigm of scientific work and research. In the humanities, eScience environments can be helpful in establishing new workflows and lifecycles of scientific data. WebLicht is such an eScience environment for linguistic analysis, making linguistic tools and resources available network-wide. Today, most digital language resources and tools (LRT) are available by download only. This is inconvenient for someone who wants to use and combine several tools because these tools are normally not compatible with each other. To overcome this restriction, WebLicht makes the functionality of linguistic tools and the resources themselves available via the internet as web services. In WebLicht, several kinds of linguistic tools are available which cover the basic functionality of automatic and incremental creation of annotated text corpora. To make use of the more than 70 tools and resources currently available, the end user needs nothing more than just a common web browser."
heid-etal-2010-term,Term and Collocation Extraction by Means of Complex Linguistic Web Services,2010,14,3,3,0,24867,ulrich heid,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present a web service-based environment for the use of linguistic resources and tools to address issues of terminology and language varieties. We discuss the architecture, corpus representation formats, components and a chainer supporting the combination of tools into task-specific services. Integrated into this environment, single web services also become part of complex scenarios for web service use. Our web services take for example corpora of several million words as an input on which they perform preprocessing, such as tokenisation, tagging, lemmatisation and parsing, and corpus exploration, such as collocation extraction and corpus comparison. Here we present an example on extraction of single and multiword items typical of a specific domain or typical of a regional variety of German. We also give a critical review on needs and available functions from a user's point of view. The work presented here is part of ongoing experimentation in the D-SPIN project, the German national counterpart of CLARIN."
heid-etal-2010-corpus,A Corpus Representation Format for Linguistic Web Services: The {D}-{SPIN} Text Corpus Format and its Relationship with {ISO} Standards,2010,7,22,4,0,24867,ulrich heid,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In the framework of the preparation of linguistic web services for corpus processing, the need for a representation format was felt, which supports interoperability between different web services in a corpus processing pipeline, but also provides a well-defined interface to both, legacy tools and their data formats and upcoming international standards. We present the D-SPIN text corpus format, TCF, which was designed for this purpose. It is a stand-off XML format, inspired by the philosophy of the emerging standards LAF (Linguistic Annotation Framework) and its ``instances'' MAF for morpho-syntactic annotation and SynAF for syntactic annotation. Tools for the exchange with existing (best practice) formats are available, and a converter from MAF to TCF is being tested in spring 2010. We describe the usage scenario where TCF is embedded and the properties and architecture of TCF. We also give examples of TCF encoded data and describe the aspects of syntactic and semantic interoperability already addressed."
C10-1052,Standardizing Wordnets in the {ISO} Standard {LMF}: {W}ordnet-{LMF} for {G}erma{N}et,2010,8,17,2,1,32946,verena henrich,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"It has been recognized for quite some time that sustainable data formats play an important role in the development and curation of linguistic resources. The purpose of this paper is to show how GermaNet, the German version of the Princeton WordNet, can be converted to the Lexical Markup Framework (LMF), a published ISO standard (ISO-24613) for encoding lexical resources. The conversion builds on Wordnet-LMF, which has been proposed in the context of the EU KYOTO project as an LMF format for wordnets. The present paper proposes a number of crucial modifications and a set of extensions to Wordnet-LMF that are needed for conversion of wordnets in general and for conversion of Ger-maNet in particular."
E09-1047,Parsing Coordinations,2009,17,10,2,0.5,2237,sandra kubler,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"The present paper is concerned with statistical parsing of constituent structures in German. The paper presents four experiments that aim at improving parsing performance of coordinate structure: 1) reranking the n-best parses of a PCFG parser, 2) enriching the input to a PCFG parser by gold scopes for any conjunct, 3) reranking the parser output for all possible scopes for conjuncts that are permissible with regard to clause structure. Experiment 4 reranks a combination of parses from experiments 1 and 3.n n The experiments presented show that n-best parsing combined with reranking improves results by a large margin. Providing the parser with different scope possibilities and reranking the resulting parses results in an increase in F-score from 69.76 for the baseline to 74.69. While the F-score is similar to the one of the first experiment (n-best parsing and reranking), the first experiment results in higher recall (75.48% vs. 73.69%) and the third one in higher precision (75.43% vs. 73.26%). Combining the two methods results in the best result with an F-score of 76.69."
hinrichs-lau-2008-contrast,In Contrast - A Complex Discourse Connective,2008,14,0,1,1,17752,erhard hinrichs,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents a corpus-based study of the discourse connective Âin contrastÂ. The corpus data are drawn from the British National Corpus (BNC) and are analyzed at the levels of syntax, discourse structure, and compositional semantics. Following Webber et al. (2003), the paper argues that Âin contrastÂ crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as ÂthenÂ, ÂotherwiseÂ, and ÂneverthelessÂ. The compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to Âin contrastÂ, for which the notions of contrast pairs and contrast properties are essential."
broeder-etal-2008-foundation,Foundation of a Component-based Flexible Registry for Language Resources and Technology,2008,0,7,3,0,18402,daan broeder,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Within the CLARIN e-science infrastructure project it is foreseen to develop a component-based registry for metadata for Language Resources and Language Technology. With this registry it is hoped to overcome the problems of the current available systems with respect to inflexible fixed schema, unsuitable terminology and interoperability problems. The registry will address interoperability needs by refering to a shared vocabulary registered in data category registries as they are suggested by ISO."
W06-1614,Is it Really that Difficult to Parse {G}erman?,2006,13,39,2,0.789474,2237,sandra kubler,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a comparative study of probabilistic treebank parsing of German, using the Negra and TuBa-D/Z tree-banks. Experiments with the Stanford parser, which uses a factored PCFG and dependency model, show that, contrary to previous claims for other parsers, lexicalization of PCFG models boosts parsing performance for both treebanks. The experiments also show that there is a big difference in parsing performance, when trained on the Negra and on the TuBa-D/Z treebanks. Parser performance for the models trained on TuBa-D/Z are comparable to parsing results for English with the Stanford parser, when trained on the Penn treebank. This comparison at least suggests that German is not harder to parse than its West-Germanic neighbor language English."
W06-1101,Linguistic Distances,2006,29,19,2,0,38842,john nerbonne,Proceedings of the Workshop on Linguistic Distances,0,"In many theoretical and applied areas of computational linguistics researchers operate with a notion of linguistic distance or, conversely, linguistic similarity, which is the focus of the present workshop. While many CL areas make frequent use of such notions, it has received little focused attention, an honorable exception being Lebart & Rajman (2000). This workshop brings a number of these strands together, highlighting a number of common issues."
W05-0303,"A Unified Representation for Morphological, Syntactic, Semantic, and Referential Annotations",2005,11,25,1,1,17752,erhard hinrichs,Proceedings of the Workshop on Frontiers in Corpus Annotations {II}: Pie in the Sky,0,"This paper reports on the SYN-RA (SYNtax-based Reference Annotation) project, an on-going project of annotating German newspaper texts with referential relations. The project has developed an inventory of anaphoric and coreference relations for German in the context of a unified, XML-based annotation scheme for combining morphological, syntactic, semantic, and anaphoric information. The paper discusses how this unified annotation scheme relates to other formats currently discussed in the literature, in particular the annotation graph model of Bird and Liberman (2001) and the pie-in-the-sky scheme for semantic annotation."
W04-3231,A Hybrid Model for Morpho-Syntactic Annotation of {G}erman with a Large Tagset,2004,0,2,2,0,48469,julia trushkina,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,None
telljohann-etal-2004-tuba,"The T{\\\u}ba-{D}/{Z} Treebank: Annotating {G}erman with a Context-Free Backbone""",2004,5,59,2,0,32150,heike telljohann,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Abstract The purpose of this paper is to describe the T uBa-D/Z treebank of written German and to compare it to the independently developed TIGER treebank (Brants et al., 2002). Both treebanks, TIGER and T uBa-D/Z, use an annotation framework that is based on phrase structure grammar and that is enhanced by a level of predicate-argument structure. The comparison between the annotation schemes of the two treebanks focuses on the different treatments of free word order and discontinuous constituents in German as well as on differences in phrase-internal annotation."
Y02-1001,Robust Syntactic Annotation of Corpora and Memory-based Parsing,2001,-1,-1,1,1,17752,erhard hinrichs,"Proceedings of the 16th Pacific Asia Conference on Language, Information and Computation",0,None
hinrichs-etal-2002-hybrid,A Hybrid Architecture for Robust Parsing of {G}erman,2002,27,4,1,1,17752,erhard hinrichs,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper provides an overview of current research on a hybrid and robust parsing architecture for the morphological, syntactic and semantic annotation of German text corpora. The novel contribution of this research lies not in the individual parsing modules, each of which relies on state-of-the-art algorithms and techniques. Rather what is new about the present approach is the combination of these modules into a single architecture. This combination provides a means to significantly optimize the performance of each component, resulting in an increased accuracy of annotation."
P01-1045,From Chunks to Function-Argument Structure: A Similarity-Based Approach,2001,18,10,2,0.789474,2237,sandra kubler,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"Chunk parsing has focused on the recognition of partial constituent structures at the level of individual chunks. Little attention has been paid to the question of how such partial analyses can be combined into larger structures for complete utterances. Such larger structures are not only desirable for a deeper syntactic analysis. They also constitute a necessary prerequisite for assigning function-argument structure.The present paper offers a similarity-based algorithm for assigning functional labels such as subject, object, head, complement, etc. to complete syntactic structures on the basis of prechunked input.The evaluation of the algorithm has concentrated on measuring the quality of functional labels. It was performed on a German and an English treebank using two different annotation schemes at the level of function-argument structure. The results of 89.73 % correct functional labels for German and 90.40 % for English validate the general approach."
H01-1072,"{T}{\\\u}{SBL}: A Similarity-Based Chunk Parser for Robust Syntactic Processing""",2001,12,6,2,0.789474,2237,sandra kubler,Proceedings of the First International Conference on Human Language Technology Research,0,"Chunk parsing has focused on the recognition of partial constituent structures at the level of individual chunks. Little attention has been paid to the question of how such partial analyses can be combined into larger structures for complete utterances.The TuSBL parser extends current chunk parsing techniques by a tree-construction component that extends partial chunk parses to complete tree structures including recursive phrase structure as well as function-argument structure. TuSBL's tree construction algorithm relies on techniques from memory-based learning that allow similarity-based classification of a given input structure relative to a pre-stored set of tree instances from a fully annotated treebank.A quantitative evaluation of TuSBL has been conducted using a semi-automatically constructed treebank of German that consists of appr. 67,000 fully annotated sentences. The basic PARSEVAL measures were used although they were developed for parsers that have as their main goal a complete analysis that spans the entire input. This runs counter to the basic philosophy underlying TuSBL, which has as its main goal robustness of partially analyzed structures."
C96-1092,Applying Lexical Rules Under Subsumption,1996,16,5,1,1,17752,erhard hinrichs,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"Lexical rules are used in constraint based grammar formalisms such as Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag 1994) to express generalizations among lexical entries. This paper discusses a number of lexical rules from recent HPSG analyses of German (Hinrichs and Nakazawa 1994) and shows that the grammar in some cases vastly overgenerates and in other cases introduces massive spurious structural ambiguity, if lexical rules apply under unification. Such problems of overgeneration or spurious ambiguity do not arise, if a lexical rule applies to a given lexical entry iff the lexical entry is subsumed by the left-hand side of the lexical rule. Finally, the paper discusses computational consequences of applying lexical rules under subsumption."
C90-2025,Functor-Driven Natural Language Generation with Categorial-Unification Grammars,1990,11,5,2,0,42000,dale gerdemann,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,None
J88-2002,"Tense, Quantifiers, and Contexts",1988,19,32,1,1,17752,erhard hinrichs,Computational Linguistics,0,"This paper describes a compositional semantics for temporal expressions as part of the meaning representation language (MRL) of the JANUS system, a natural language understanding and generation system under joint development by BBN Labs and ISI. The analysis is based on a higher-order intensional logic described in detail in Hinrichs (1987a). Temporal expressions of English are translated into this language as quantifiers over times that bind temporal indices on predicates. The semantic evaluation of time-dependent predicates is defined relative to a set of discourse contexts, which, following Reichenbach (1947), include the parameters of speech time and reference time. The resulting context-dependent and multi-indexed interpretation of temporal expressions solves a set of well-known problems that arise when traditional systems of tense logic are applied to natural language semantics. Based on the principle of rule-to-rule translation, the compositional nature of the analysis provides a straightforward and well-defined interface between the parsing component and the semantic-interpretation component of JANUS."
P87-1002,A Compositional Semantics of Temporal Expressions in {E}nglish,1987,8,12,1,1,17752,erhard hinrichs,25th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a compositional semantics for temporal expressions as part of the meaning representation language (MRL) of the JANUS system, a natural language understanding and generation system under joint development by BBN Laboratoires and the Information Sciences Institute. The analysis is based on a higher order intensional logic described in detail in Hinrichs, Ayuso and Scha (1987). Temporal expressions of English are translated into this language as quantifiers over times which bind temporal indices on predicates. The semantic evaluation of time-dependent predicates is defined relative to a set of discourse contexts, which, following Reichenbach (1947), include the parameters of speech time and reference time. The resulting context-dependent and multi-indexed interpretation of temporal expressions solves a set of well-known problems that arise when traditional systems of tense logic are applied to natural language semantics. Based on the principle of rule-to-rule translation, the compositional nature of the analysis provides a straightforward and well-defined interface between the parsing component and the semantic interpretation component of JANUS."
H86-1001,Research and Development in Natural Language Processing at {BBN} {L}aboratories in the {S}trategic {C}omputing {P}rogram,1986,10,0,6,0,4279,ralph weischedel,"Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",0,"BBN's responsibility is to conduct research and development in natural language interface technology. This responsibility has three aspects:xe2x80xa2 to demonstrate state-of-the-art technology in a Strategic Computing application, collecting data regarding the effectiveness of the demonstrated heuristics,xe2x80xa2 to conduct research in natural language interface technology, as itemized in the description of JANUS later in this note, andxe2x80xa2 to integrate technology from other natural language interface contractors, including USC/Information Sciences Institute, the University of Pennsylvania, and the University of Massachusetts."
C86-1082,A Compositional Semantics for Directional Modifiers - Locative Case Reopened -,1986,8,4,1,1,17752,erhard hinrichs,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"This paper presents a model-theoretic semantics for directional modifiers in English. The semantic theory presupposed for the analysis is that of Montague Grammar (cf. Montague 1970, 1973) which makes it possible to develop a strongly compositional treatment of directional modifiers. Such a treatment has significant computational advantages over case-based treatments of directional modifiers that are advocated in the Al literature."
