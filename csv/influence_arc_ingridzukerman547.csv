2021.acl-long.415,N19-1357,0,0.0207755,"and are mostly applied to models with differentiable functions. Further, they may be sensitive 1 This approach does not aim to improve the transparency (Lipton, 2018) of the black-box model. to randomized model initializations or permuted data labels (Adebayo et al., 2018), which is undesirable. These methods can be computationally heavy in the case of complex black-box models (Wu and Ong, 2021), e.g., BERT (Devlin et al., 2018). Attention-based methods (Wiegreffe and Pinter, 2019) can only be applied to Transformer-based models (Vaswani et al., 2017), and their effectiveness is questionable (Jain and Wallace, 2019; Serrano and Smith, 2019). Perturbation-based methods approximate feature importance by observing changes in a model’s outcome after a feature is changed. They either consider changes in performance as an indicator of feature importance directly (Martens and Provost, 2014; Zeiler and Fergus, 2014; Schwab and Karlen, 2019), or they employ a higher-order approximation of the decision boundary (Ribeiro et al., 2016; Lundberg and Lee, 2017). Perturbation-based methods are typically computationally inefficient for explaining high-dimensional data, and they suffer from high variance due to perturba"
2021.acl-long.415,2020.emnlp-main.747,0,0.0277781,"statistical significance. Detailed precision and recall values of positive reviews appear in Appendix G. Faithfulness. We select the top-K important words generated by an explanation method and compute the precision, recall and F1 against the human-annotated rationales. It is worth noting that our L2E explainer is not supervised by human rationales directly. Instead, we use the same experimental setup as in Section 4.5 to ensure the L2E explainer is learning from the baseline algorithms rather than the human rationales. Table 4 displays the average values over all test instances. As noted by Carton et al. (2020), the rationales in the original dataset are not exhaustively identified by human annotators. For a particular event, we expect to observe a lower precision than recall, since the black-box model might still be able to utilize the words not being annotated in addition to the words annotated by a human. The results in Table 4 align with this hypothesis. For instance, besides LRP for the positive reviews and Kernel SHAP for both reviews, all baselines and the corresponding L2E have higher recall than precision. Furthermore, L2E outperforms the corresponding baseline A significantly in most cases"
2021.acl-long.415,2020.acl-main.409,0,0.0207685,"high-dimensional data, and they suffer from high variance due to perturbation randomness (Slack et al., 2020; Chen et al., 2019). Model-based Approaches. These approaches train the explainer with an objective function to improve efficiency at test time. The closest work to ours is by Schwab and Karlen (2019), who train an explainer using a causality-based explanation algorithm. However, these approaches do not learn from arbitrary algorithms or discretize feature weights — the high variation of continuous weights may impair the ability to capture the commonalities in an explanation algorithm. Jain et al. (2020) discretize the weights produced by an existing method, but they use these weights to build a faithful classifier for an underlying blackbox model, rather than using them to explain the model directly. Other works train a classifier and an explainer jointly in order to incorporate explainability directly into the classifier (Lei et al., 2016; Camburu et al., 2018). Unlike these approaches, we do not change the classifier or require an expensive process to collect human rationales, as done in (Camburu et al., 2018). Lastly, a few works use information-theoretic objectives to train an explainer"
2021.emnlp-main.233,2020.emnlp-main.255,0,0.011346,"ring LLE to three baselines on text classification tasks show that LLE can enhance the stability of the explanations for all seen tasks and maintain the same level of faithfulness to the black-box model as the teacher, while being up to 102 times faster at test time. Our ablation study shows that the ER mechanism in our LLE approach enhances the learning capabilities of the student explainer. Our code is available at https://github.com/situsnow/LLE. 1 Introduction 2021); perturbation-based methods, which observe changes in model performance after feature perturbation (Schwab and Karlen, 2019; Kim et al., 2020), or approximate the local decision boundary through perturbed samples (Ribeiro et al., 2016; Lundberg and Lee, 2017); and model-based methods, which train an explainer model by optimizing an explanation-meritorious objective,1 such as robustness/stability (Lakkaraju et al., 2020; AlvarezMelis and Jaakkola, 2018) that requires similar examples to have similar explanations. All these methods aim to explain static black-box models, whereas explaining dynamic ones, as in the lifelong learning (LL) (Silver et al., 2013) setting, is under-explored. We propose a Lifelong Explanation (LLE) approach t"
2021.inlg-1.12,2020.inlg-1.23,0,0.0452387,"Missing"
2021.inlg-1.12,2020.nl4xai-1.12,0,0.0346515,"se explanations address actual user expectations. 2 Related Work In 1990-2000, explanations derived from knowledge bases were enhanced by addressing aspects of users’ reasoning. Specifically, Zukerman and McConachy (1993) and Horacek (1997) considered potential inferences from explanations, omitting easily inferable information and addressing erroneous inferences; Korb et al. (1997) took into account reasoning fallacies when explaining the reasoning of Bayesian Networks; and Stone (2000) generated instructions from which users could draw appropriate inferences about actions to take. Recently, Krause and Vossen (2020) identified additional triggers that should be addressed in explanations. Current research on explanation generation focuses on explaining the predictions made by ML models – a sub-field called Explainable AI (XAI). In particular, neural networks have received a lot of attention owing to their superior performance on one hand, and their opaqueness on the other hand. A common first step in explaining the predictions 1 The participants in our study were told that they have an AI, but they were not informed about the specifics of the ML model. Other explanatory objectives include enhancing trust"
2021.inlg-1.12,W19-8402,1,0.924562,"be addressed in explanations. Current research on explanation generation focuses on explaining the predictions made by ML models – a sub-field called Explainable AI (XAI). In particular, neural networks have received a lot of attention owing to their superior performance on one hand, and their opaqueness on the other hand. A common first step in explaining the predictions 1 The participants in our study were told that they have an AI, but they were not informed about the specifics of the ML model. Other explanatory objectives include enhancing trust in the system, and helping debug a system (Reiter, 2019). of neural networks is to build a local surrogate explainer model that uses a transparent model to approximate the neighbourhood of an instance of interest. Linear regression (Ribeiro et al., 2016; ˇ Strumbelj and Kononenko, 2014; Lundberg and Lee, 2017), decision rules (Ribeiro et al., 2018) and DTs (van der Waa et al., 2018; Guidotti et al., 2019; Sokol and Flach, 2020a) have been employed for this purpose. A DT’s prediction is generally explained by tracing the path from the root to a predicted outcome (Guidotti et al., 2019; Stepin et al., 2020). Recently, researchers have generated class"
2021.inlg-1.12,N16-3020,0,0.0431343,"ural networks have received a lot of attention owing to their superior performance on one hand, and their opaqueness on the other hand. A common first step in explaining the predictions 1 The participants in our study were told that they have an AI, but they were not informed about the specifics of the ML model. Other explanatory objectives include enhancing trust in the system, and helping debug a system (Reiter, 2019). of neural networks is to build a local surrogate explainer model that uses a transparent model to approximate the neighbourhood of an instance of interest. Linear regression (Ribeiro et al., 2016; ˇ Strumbelj and Kononenko, 2014; Lundberg and Lee, 2017), decision rules (Ribeiro et al., 2018) and DTs (van der Waa et al., 2018; Guidotti et al., 2019; Sokol and Flach, 2020a) have been employed for this purpose. A DT’s prediction is generally explained by tracing the path from the root to a predicted outcome (Guidotti et al., 2019; Stepin et al., 2020). Recently, researchers have generated class-contrastive counterfactual explanations to enhance the explanations of DT predictions. Stepin et al. (2020) generated explanations that have a factual and a counterfactual component; the former is"
C02-1161,H92-1022,0,0.0120176,"rases. Like Harabagiu et al. (2001), we use WordNet to propose synonyms for the words in a query. However, they apply heuristics to select which words to paraphrase. In contrast, we use corpus-based information in the context of the entire query to calculate the score of a paraphrase and select which paraphrases to retain, and then use the paraphrase scores to influence the document retrieval process. 3 Resources Our system uses syntactic, semantic and statistical information for paraphrase generation. Syntactic information for each query was obtained from Brill’s part-of-speech (PoS) tagger (Brill, 1992). Semantic information consisting of different types of synonyms for the words in each query was obtained from WordNet (Miller et al., 1990). The corpus used for information retrieval and for the collection of statistical information was the LA Times portion of the NIST Text Research Collection (//trec.nist.gov). This corpus was small enough to satisfy our disk space limitations, and sufficiently large to yield statistically significant results (131,896 documents). Full-text indexing was performed for the documents in the LA Times collection, using lemmas (rather than words). The lemmas for th"
C02-1161,W98-0705,0,0.0547289,"lcea and Moldovan (1999) and Lytinen et al. (2000) used a machine readable thesaurus, specifically WordNet (Miller et al., 1990), to obtain the sense of a word, while Sch¨utze and Pedersen (1995) and Lin (1998) used automatically constructed thesauri. The improvements in retrieval performance reported in (Mitra et al., 1998) are comparable to those reported here (note that these researchers consider precision, while we consider recall). The results obtained by Sch¨utze and Pedersen (1995) and by Lytinen et al. (2000) are encouraging. However, experimental results reported in (Sanderson, 1994; Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Our approach to document retrieval differs from the above approaches in that the expansion of a query takes the form of alternative lexical paraphrases. Like Harabagiu et al. (2001), we use WordNet to propose synonyms for the words in a query. However, they apply heuristics to select which words to paraphrase. In contrast, we use corpus-based information in the context of the entire query to calculate the score of a paraphrase and selec"
C02-1161,P01-1037,0,0.0904363,"le to those reported here (note that these researchers consider precision, while we consider recall). The results obtained by Sch¨utze and Pedersen (1995) and by Lytinen et al. (2000) are encouraging. However, experimental results reported in (Sanderson, 1994; Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Our approach to document retrieval differs from the above approaches in that the expansion of a query takes the form of alternative lexical paraphrases. Like Harabagiu et al. (2001), we use WordNet to propose synonyms for the words in a query. However, they apply heuristics to select which words to paraphrase. In contrast, we use corpus-based information in the context of the entire query to calculate the score of a paraphrase and select which paraphrases to retain, and then use the paraphrase scores to influence the document retrieval process. 3 Resources Our system uses syntactic, semantic and statistical information for paraphrase generation. Syntactic information for each query was obtained from Brill’s part-of-speech (PoS) tagger (Brill, 1992). Semantic information"
C02-1161,P98-2127,0,0.0604725,", followed by our evaluation and concluding remarks.  This research was supported in part by Australian Research Council grant DP0209565. Bhavani Raskutti Telstra Research Laboratories 770 Blackburn Road Clayton, VICTORIA 3168 AUSTRALIA 2 Related Research The vocabulary mis-match between user queries and indexed documents is often addressed through query expansion. Two common techniques for query expansion are blind relevance feedback (Buckley et al., 1995; Mitra et al., 1998) and word sense disambiguation (WSD) (Mihalcea and Moldovan, 1999; Lytinen et al., 2000; Sch¨utze and Pedersen, 1995; Lin, 1998). Blind relevance feedback consists of retrieving a small number of documents using a query given by a user, and then constructing an expanded query that includes content words that appear frequently in these documents. This expanded query is used to retrieve a new set of documents. WSD often precedes query expansion to avoid retrieving irrelevant information. Mihalcea and Moldovan (1999) and Lytinen et al. (2000) used a machine readable thesaurus, specifically WordNet (Miller et al., 1990), to obtain the sense of a word, while Sch¨utze and Pedersen (1995) and Lin (1998) used automatically con"
C02-1161,P99-1020,0,0.0418174,"eval processes are described in Section 4. Section 5 presents sample paraphrases, followed by our evaluation and concluding remarks.  This research was supported in part by Australian Research Council grant DP0209565. Bhavani Raskutti Telstra Research Laboratories 770 Blackburn Road Clayton, VICTORIA 3168 AUSTRALIA 2 Related Research The vocabulary mis-match between user queries and indexed documents is often addressed through query expansion. Two common techniques for query expansion are blind relevance feedback (Buckley et al., 1995; Mitra et al., 1998) and word sense disambiguation (WSD) (Mihalcea and Moldovan, 1999; Lytinen et al., 2000; Sch¨utze and Pedersen, 1995; Lin, 1998). Blind relevance feedback consists of retrieving a small number of documents using a query given by a user, and then constructing an expanded query that includes content words that appear frequently in these documents. This expanded query is used to retrieve a new set of documents. WSD often precedes query expansion to avoid retrieving irrelevant information. Mihalcea and Moldovan (1999) and Lytinen et al. (2000) used a machine readable thesaurus, specifically WordNet (Miller et al., 1990), to obtain the sense of a word, while Sch"
C02-1161,C98-2122,0,\N,Missing
C10-1008,P04-1035,0,0.0183475,"t is used when the representative vectors of a class are boolean valued. It is calculated using Equation 2. 3.3 Irrelevant Feature Culling The MCST-SVM scheme provides a natural mechanism for reducing the dimensionality of feature vectors in order to address the overfitting 66 four-star rating for each review.5 Each sub-corpus is written by a different author (denoted Author A, B, C and D respectively), thus avoiding calibration error between individual authors and their ratings. Review texts are automatically filtered to leave only subjective sentences (motivated by the results described in (Pang and Lee, 2004)); the mean number of words per review in each subjectivefiltered sub-corpus is 435, 374, 455 and 292 respectively. problem. This is due to the fact that each internal decision node is trained using only the samples that belong to the classes relevant to this node. The reviews for these classes are likely to omit some of the words that appear in the reviews for classes that are relevant to other nodes, in particular in the lower layers of the tree. Consequently, an internal node can be trained using a subset of the features that occur in the entire training dataset. This subset contains only t"
C10-1008,P05-1015,0,0.732323,"logy Monash University bickerstaffe.adrian@gmail.com,Ingrid.Zukerman@monash.edu Abstract tagged datasets to predict the ratings of untagged reviews. Typical approaches to the rating scale problem include standard k-way classifiers, e.g., (Pang and Lee, 2005). However, these methods do not explicitly account for sample similarities, e.g., the samples with a “four star” rating being more similar to “three star” samples than to “one star” samples. Consequently, these methods generally do not perform well, while methods which incorporate sample similarity information achieve improved performance (Pang and Lee, 2005). Sample similarity in the multi-way sentiment detection setting has previously been considered by using Support Vector Machines (SVMs) in conjunction with a metric labeling metaalgorithm (Pang and Lee, 2005); by taking a semisupervised graph-based learning approach (Goldberg and Zhu, 2006); and by using “optimal stacks” of SVMs (Koppel and Schler, 2006). However, each of these methods have shortcomings (Section 2). Additionally, during the learning process, all approaches employ a set of word/punctuation features collected across all rating categories. Hence, the number of features may be ver"
C10-1008,W02-1011,0,0.0129645,"ss similarity into account during the tree construction process. We construct the decision tree as a Minimum Cost Spanning Tree (MCST), denoted MCST-SVM, based on inter-class similarity measured from feature values (Lorena and de Carvalho, 2005). Each of the decision tree leaves corresponds to a target class, and the interior nodes group classes into disjoint sets. For each internal node in the MCST, an SVM is trained to separate all the samples belonging to classes in its left subtree from those in its right subtree. We use linear SVMs, which have been shown to be effective text classifiers (Pang et al., 2002; Pang and Lee, 2005), and set the SVM parameters to match those used in (Pang and Lee, 2005).1 Figure 1 contrasts All three techniques suffer from long training times — an issue that is exacerbated by large data sets such as our corpus of approximately 5000 movie reviews (Section 4.1). Additional problems associated with these techniques are: (1) there is no bound on the generalisation error of OVA, (2) OVO schemes tend to overfit, and (3) the performance of a DAGSVM relies on the order in which classes are processed. This order is based on the class labels (rather than similarity between sam"
C10-1008,W06-3808,0,0.0865684,"ot explicitly account for sample similarities, e.g., the samples with a “four star” rating being more similar to “three star” samples than to “one star” samples. Consequently, these methods generally do not perform well, while methods which incorporate sample similarity information achieve improved performance (Pang and Lee, 2005). Sample similarity in the multi-way sentiment detection setting has previously been considered by using Support Vector Machines (SVMs) in conjunction with a metric labeling metaalgorithm (Pang and Lee, 2005); by taking a semisupervised graph-based learning approach (Goldberg and Zhu, 2006); and by using “optimal stacks” of SVMs (Koppel and Schler, 2006). However, each of these methods have shortcomings (Section 2). Additionally, during the learning process, all approaches employ a set of word/punctuation features collected across all rating categories. Hence, the number of features may be very large compared to the number of training samples, which can lead to the model overfitting the data. The main contribution of this paper is the use of hierarchical classifier trees which combine standard binary classifiers to perform multi-way classification (another approach to reduce mul"
C10-1008,N07-1038,0,0.102205,"ned similar labels. Metric labeling required a label-corrected item-similarity function, which was based on the observation that the Percentage of Positive Sentences (PSP) in reviews increased as their ratings increased. Notice, however, that item similarity was not incorporated into the first stage of classifier training. Metric labeling adjusted the output of the classifiers only after they were trained without considering rating similarities. Our approach accounts for intercategory relationships from the outset of classifier design, rather than addressing this issue with later adjustments. Snyder and Barzilay (2007) proposed the “Good Grief” algorithm, which considers multiple aspects of a situation (e.g., a restaurant review that covers service, ambiance and food), and yields a prediction that minimises the dissatisfaction (grief) regarding these aspects. This method significantly outperformed baseline methods and individual classifiers. At present, we do not consider separately different aspects of a review — a task we intend to undertake in the future. 3 Goldberg and Zhu (2006) proposed a semisupervised learning approach to the rating inference problem in scenarios where labeled training data is scarc"
C10-1008,H05-1044,0,0.0609474,"m for reducing the feature space of the problem. Our results show that this approach improves on state-of-the-art predictive performance for movie reviews with three-star and fourstar ratings, while simultaneously reducing training times and memory requirements. 1 Introduction A key problem in sentiment detection is to determine the polarity of sentiment in text. Much of the work on this problem has considered binary sentiment polarity (positive or negative) at granularity levels ranging from sentences (Yu and Hatzivassiloglou, 2003; Mao and Lebanon, 2006; McDonald et al., 2007) to documents (Wilson et al., 2005; Allison, 2008). This paper considers the more general problem of multi-way sentiment classification for discrete, ordinal rating scales, focusing on the document level, i.e., the problem of predicting the “star” rating associated with a review. This is a supervised learning task involving textual reviews that have been tagged with a rating. Ultimately, the goal is to use classifiers which have been trained on 62 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 62–70, Beijing, August 2010 relatively small numbers of labeled samples from the fo"
C10-1008,W03-1017,0,0.0434072,"of tagged sentiment-bearing texts. This type of classifier also provides a natural mechanism for reducing the feature space of the problem. Our results show that this approach improves on state-of-the-art predictive performance for movie reviews with three-star and fourstar ratings, while simultaneously reducing training times and memory requirements. 1 Introduction A key problem in sentiment detection is to determine the polarity of sentiment in text. Much of the work on this problem has considered binary sentiment polarity (positive or negative) at granularity levels ranging from sentences (Yu and Hatzivassiloglou, 2003; Mao and Lebanon, 2006; McDonald et al., 2007) to documents (Wilson et al., 2005; Allison, 2008). This paper considers the more general problem of multi-way sentiment classification for discrete, ordinal rating scales, focusing on the document level, i.e., the problem of predicting the “star” rating associated with a review. This is a supervised learning task involving textual reviews that have been tagged with a rating. Ultimately, the goal is to use classifiers which have been trained on 62 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 62"
C10-1008,P07-1055,0,0.0638775,"fier also provides a natural mechanism for reducing the feature space of the problem. Our results show that this approach improves on state-of-the-art predictive performance for movie reviews with three-star and fourstar ratings, while simultaneously reducing training times and memory requirements. 1 Introduction A key problem in sentiment detection is to determine the polarity of sentiment in text. Much of the work on this problem has considered binary sentiment polarity (positive or negative) at granularity levels ranging from sentences (Yu and Hatzivassiloglou, 2003; Mao and Lebanon, 2006; McDonald et al., 2007) to documents (Wilson et al., 2005; Allison, 2008). This paper considers the more general problem of multi-way sentiment classification for discrete, ordinal rating scales, focusing on the document level, i.e., the problem of predicting the “star” rating associated with a review. This is a supervised learning task involving textual reviews that have been tagged with a rating. Ultimately, the goal is to use classifiers which have been trained on 62 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 62–70, Beijing, August 2010 relatively small numb"
C10-2178,N04-1004,0,0.164661,"e target object, without recognizing the type of this object (Nickel and Stiefelhagen, 2003; Li and Jarvis, 2009). Most of the research in gesture and speech integration focuses on pointing gestures, employing speech as the main input modality, and using semantic fusion to combine spoken input with gesture. Different approaches are used for gesture detection, e.g., vision (Stiefelhagen et al., 2004; Brooks and Breazeal, 2006) and sensor glove (Corradini et al., 2002); and for language interpretation, e.g., dedicated grammars (Stiefelhagen et al., 2004; Brooks and Breazeal, 2006) and keywords (Einstein and Christoudias, 2004). Fusion is variously implemented using heuristics based on temporal overlap (Bolt, 1980; Johnston et al., 2002), querying a gesture-sensing module when ambiguous referents are identified (Fransen et al., 2007), or unification to determine which elements can be merged (Corradini et al., 2002; Stiefelhagen et al., 2004). These are sometimes combined with search techniques coupled with penalties (Einstein and Christoudias, 2004; Brooks and Breazeal, 2006). With the exception of Bolt’s system, these systems were tested on utterances that were quite short and constrained. Our approach integrates s"
C10-2178,J02-3001,0,0.01703,"information, where the concepts correspond to the words in the parent parse tree, and the relations are derived from syntactic information in the parse tree and prepositions. Each UCG can generate many ICGs. This is done by nominating different instantiated concepts and relations from the system’s knowledge base as potential realizations for each concept and relation in a UCG. Instantiated concepts are objects and actions in the domain (e.g., mug01, mug02 and cup01 are possible instantiations of the uninstantiated concept “mug”), and instantiated relations are similar to semantic role labels (Gildea and Jurafsky, 2002). The interpretation process continues until a preset number of sub-interpretations (including texts, parse trees, UCGs and ICGs) has been generated or all options have been exhausted. Figure 2 illustrates a UCG and an ICG for the request “get the large red folder on the table”. The intrinsic features of an object (lexical item, colour and size) are stored in the UCG node for this object. Structural features, which involve two objects (e.g., “folder-on-table”), are represented as sub-graphs of the UCG (and the ICG). 2.1 Estimating the probability of an ICG Scusi? ranks candidate ICGs according"
C10-2178,J95-1003,0,0.0770575,"rmine which elements can be merged (Corradini et al., 2002; Stiefelhagen et al., 2004). These are sometimes combined with search techniques coupled with penalties (Einstein and Christoudias, 2004; Brooks and Breazeal, 2006). With the exception of Bolt’s system, these systems were tested on utterances that were quite short and constrained. Our approach integrates spatial and temporal aspects of gesture into our probabilistic formalism (Zukerman et al., 2008), focusing on the effect of pointing on object salience. Other saliencebased approaches are described in (Einstein and Christoudias, 2004; Huls et al., 1995). However, they are not directly comparable with our approach, as they use salience to weigh the importance of factors pertaining to gesture-speech alignment, but there is no uncertainty associated with the visual salience resulting from pointing. Our use of a probabilistic parser enables us to handle more complex utterances than those considered by most speech-gesture systems (Section 2). At the same time, we do not yet handle speech disfluencies, which are currently handled by (Einstein and Christoudias, 2004; Stiefelhagen et al., 2004). Also, at present we do not consider the challenges per"
C10-2178,P02-1048,0,0.0263828,"the research in gesture and speech integration focuses on pointing gestures, employing speech as the main input modality, and using semantic fusion to combine spoken input with gesture. Different approaches are used for gesture detection, e.g., vision (Stiefelhagen et al., 2004; Brooks and Breazeal, 2006) and sensor glove (Corradini et al., 2002); and for language interpretation, e.g., dedicated grammars (Stiefelhagen et al., 2004; Brooks and Breazeal, 2006) and keywords (Einstein and Christoudias, 2004). Fusion is variously implemented using heuristics based on temporal overlap (Bolt, 1980; Johnston et al., 2002), querying a gesture-sensing module when ambiguous referents are identified (Fransen et al., 2007), or unification to determine which elements can be merged (Corradini et al., 2002; Stiefelhagen et al., 2004). These are sometimes combined with search techniques coupled with penalties (Einstein and Christoudias, 2004; Brooks and Breazeal, 2006). With the exception of Bolt’s system, these systems were tested on utterances that were quite short and constrained. Our approach integrates spatial and temporal aspects of gesture into our probabilistic formalism (Zukerman et al., 2008), focusing on the"
D17-1229,N16-1037,1,0.873035,"and Ingrid Zukerman and Gholamreza Haffari Faculty of Information Technology Monash University, Australia hung.tran,ingrid.zukerman,gholamreza.haffari@monash.edu Abstract et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017). Instance-based methods treat each utterance as an independent data point, which allows the application of general machine learning models, such as Support Vector Machines. Sequencelabeling methods include methods based on Hidden Markov Models (HMMs) (Stolcke et al., 2000) and neural networks (Kalchbrenner and Blunsom, 2013; Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017). This paper introduces a novel training/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neural network model is relatively si"
D17-1229,W13-3214,0,0.624006,"t Classification Quan Hung Tran and Ingrid Zukerman and Gholamreza Haffari Faculty of Information Technology Monash University, Australia hung.tran,ingrid.zukerman,gholamreza.haffari@monash.edu Abstract et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017). Instance-based methods treat each utterance as an independent data point, which allows the application of general machine learning models, such as Support Vector Machines. Sequencelabeling methods include methods based on Hidden Markov Models (HMMs) (Stolcke et al., 2000) and neural networks (Kalchbrenner and Blunsom, 2013; Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017). This paper introduces a novel training/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neural network model"
D17-1229,J00-3003,0,0.948704,"Missing"
D17-1229,E17-1041,1,0.89042,"ffari Faculty of Information Technology Monash University, Australia hung.tran,ingrid.zukerman,gholamreza.haffari@monash.edu Abstract et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017). Instance-based methods treat each utterance as an independent data point, which allows the application of general machine learning models, such as Support Vector Machines. Sequencelabeling methods include methods based on Hidden Markov Models (HMMs) (Stolcke et al., 2000) and neural networks (Kalchbrenner and Blunsom, 2013; Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017). This paper introduces a novel training/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neural network model is relatively simple, it outperforms more complex neural"
E17-1041,W13-3214,0,0.475914,"Missing"
E17-1041,D15-1166,0,0.0636385,"Missing"
E17-1041,J93-2003,0,0.0656162,"This model is combined with an attention mechanism that focuses on salient tokens in utterances. Our experimental results show that our model outperforms strong baselines on two popular datasets, Switchboard and MapTask; and our detailed empirical analysis highlights the impact of each aspect of our model. 1 Introduction The sequence-labeling task involves learning a model that maps an input sequence to an output sequence. Many NLP problems can be treated as sequence-labeling tasks, e.g., part-of-speech (PoS) tagging (Toutanova et al., 2003; Toutanova and Manning, 2000), machine translation (Brown et al., 1993) and automatic speech recognition (Gales and Young, 2008). Recurrent Neural Nets (RNNs) have been the workhorse model for many NLP sequence-labeling tasks, e.g., machine translation (Sutskever et al., 2014) and speech recognition (Amodei et al., 2015), due to their ability to capture long-range dependencies inherent in natural language. In this paper, we propose a hierarchical RNN for labeling a sequence of utterances (i.e., contributions) in a dialogue with their Dialogue Acts 428 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volu"
E17-1041,J00-3003,0,0.931309,"Missing"
E17-1041,W00-1308,0,0.05118,"Missing"
E17-1041,N03-1033,0,0.0463466,"ture long-range dependencies at the dialogue level and the utterance level. This model is combined with an attention mechanism that focuses on salient tokens in utterances. Our experimental results show that our model outperforms strong baselines on two popular datasets, Switchboard and MapTask; and our detailed empirical analysis highlights the impact of each aspect of our model. 1 Introduction The sequence-labeling task involves learning a model that maps an input sequence to an output sequence. Many NLP problems can be treated as sequence-labeling tasks, e.g., part-of-speech (PoS) tagging (Toutanova et al., 2003; Toutanova and Manning, 2000), machine translation (Brown et al., 1993) and automatic speech recognition (Gales and Young, 2008). Recurrent Neural Nets (RNNs) have been the workhorse model for many NLP sequence-labeling tasks, e.g., machine translation (Sutskever et al., 2014) and speech recognition (Amodei et al., 2015), due to their ability to capture long-range dependencies inherent in natural language. In this paper, we propose a hierarchical RNN for labeling a sequence of utterances (i.e., contributions) in a dialogue with their Dialogue Acts 428 Proceedings of the 15th Conference of the"
E17-1041,C10-2150,0,0.0405562,"Missing"
I13-1026,W11-2002,0,0.348728,"U module and expert annotators are addressees. Gold standard interpretations for these descriptions are produced by annotators on the basis of their understanding of what was said, e.g., an ambiguous utterance has more than one correct interpretation. The SLU system’s performance is evaluated on the basis of the rank of the correct interpretations. 1 Examples from our trials are marked with asterisks (∗). 225 International Joint Conference on Natural Language Processing, pages 225–233, Nagoya, Japan, 14-18 October 2013. utterances in terms of ASR Word Error Rate (WER), e.g., (Hirschman, 1998; Black et al., 2011). Möller (2008) provides a comprehensive collection of interaction parameters for evaluating telephone-based spoken dialogue services, which pertain to different aspects of an interaction, viz communication, cooperativity, task success, and spoken input. Our characterization of spoken utterances along the accuracy and knowledge dimensions is related to Möller’s task success category. However, in our case, these features pertain to the context, rather than the task. In addition, our characterization is linked to system development effort, i.e., how much effort should be invested to address utte"
I13-1026,W01-1614,0,0.0559482,"focuses on the interpretation of descriptions of household objects (Zukerman et al., 2008). Our contributions pertain to (1) the characterization of spoken utterances, (2) experimental design, and (3) quantitative evaluation metrics for an N-best list. Characterization of spoken utterances. According to (Jokinen and McTear, 2010), “in diagnostic-type evaluations, a representative test suite is used so as to produce a system’s performance profile with respect to a taxonomy of possible inputs”. In addition, one of the typical aims of an evaluation is to identify components that can be improved (Paek, 2001). These two factors in combination motivate a characterization of input utterances along two dimensions: accuracy and knowledge (Section 4). • Accuracy indicates whether an utterance describes an intended object precisely and unambiguously. For instance, when intending a blue plate, “the blue plate” is an accurate description if there is only one such plate in the room, while “the green plate” is inaccurate. • Knowledge indicates how much the SLU module knows about different factors of the interpretation process, e.g., vocabulary or geometric • In the Interpretive experiment, trial subjects an"
I13-1026,W09-3902,0,0.194332,"be invested to address utterances with certain characteristics; and to evaluation metrics, in the sense that the assessment of an interpretation depends on the accuracy of an utterance, and takes into account the capabilities of an SLU system. These two experiments, in combination with our characterization of spoken utterances, enable the comparison of system and human interpretations under different conditions. Quantitative evaluation metrics. Automatic Speech Recognizers (ASRs) and parsers often return N-best hypotheses to SLU modules, while many SLU systems return only one interpretation (DeVault et al., 2009; Jokinen and McTear, 2010; Black et al., 2011). However, maintaining N-best interpretations at the semantic and pragmatic level enables a Dialogue Manager (DM) to examine more than one interpretation, and discover features that guide appropriate responses and support error recovery. This ranking requirement, together with our experimental design, motivates the following metrics (Section 6). • For Interpretive experiments, we propose correlation measures, such as Spearman rank or Pearson correlation coefficient, to compare participants’ ratings of candidate interpretations with the scores give"
I13-1026,I13-1027,1,0.710784,"Missing"
I13-1026,W01-0902,0,\N,Missing
I13-1026,J09-2005,0,\N,Missing
I13-1027,P98-1013,0,0.00915833,"ese actions make ad hoc changes. The noisy channel model has been employed for various NLP tasks, such as ASR output correction (Ringger and Allen, 1996), spelling correction (Brill and Moore, 2000), and disfluency correction (Johnson and Charniak, 2004; Zwarts et al., 2010). Our approach differs from the traditional noisy channel approach in that it uses a word-error classifier to model the noisy channel, and semantic information to model the input characteristics. Shallow semantic parsers for SDSs have been used in (Coppola et al., 2009; Geertzen, 2009). Coppola et al. (2009) used FrameNet (Baker et al., 1998) to detect and filter the frames for target words, and employed a Support Vector Machine (SVM) classifier to perform semantic labeling. Geertzen (2009) used a shallow parser to detect semantic units only when a dependency parser failed to produce a parse tree. In contrast, our shallow semantic parser is part of a noisy channel model that post-processes the output of an ASR. 3.1 Word error classifier We investigated three classifiers to determine whether a word in the ASR textual output is correct: the Weka implementation of Decision Trees (Quinlan, 1993) and Naïve Bayes classifiers (Domingos a"
I13-1027,P00-1037,0,0.442042,"c words). Our mechanism was evaluated on a corpus of 295 spoken referring expressions, improving interpretation performance. 1 the stool to the left of the storm the left of Object Prep Specifier the plate in to play it in Object Noise Prep the table the table Landmark the microwave the microwave Landmark The idea of the noisy channel model is that a message is sent through a channel that introduces errors, and the receiver endeavours to reconstruct the original message by taking into account the characteristics of the noisy channel and of the transmitted information (Ringger and Allen, 1996; Brill and Moore, 2000; Zwarts et al., 2010). The system described in this paper handles three types of errors: noise (which is removed), missing prepositions (which are inserted), and mis-heard words (which are replaced). Table 1 shows two descriptions that illustrate these errors. The first row for each description displays what was spoken, the second row displays what was heard by the ASR, and the third row shows the semantic labels assigned to each segment in the description by a shallow semantic parser (Section 3.2). Specifically, in the first example, the preposition “to” is missing, and the object “stool” is"
I13-1027,N09-2022,0,0.0137607,"nd grammatical numbers to better match grammatical expectations. However, these actions make ad hoc changes. The noisy channel model has been employed for various NLP tasks, such as ASR output correction (Ringger and Allen, 1996), spelling correction (Brill and Moore, 2000), and disfluency correction (Johnson and Charniak, 2004; Zwarts et al., 2010). Our approach differs from the traditional noisy channel approach in that it uses a word-error classifier to model the noisy channel, and semantic information to model the input characteristics. Shallow semantic parsers for SDSs have been used in (Coppola et al., 2009; Geertzen, 2009). Coppola et al. (2009) used FrameNet (Baker et al., 1998) to detect and filter the frames for target words, and employed a Support Vector Machine (SVM) classifier to perform semantic labeling. Geertzen (2009) used a shallow parser to detect semantic units only when a dependency parser failed to produce a parse tree. In contrast, our shallow semantic parser is part of a noisy channel model that post-processes the output of an ASR. 3.1 Word error classifier We investigated three classifiers to determine whether a word in the ASR textual output is correct: the Weka implementatio"
I13-1027,W09-3729,0,0.0262894,"to better match grammatical expectations. However, these actions make ad hoc changes. The noisy channel model has been employed for various NLP tasks, such as ASR output correction (Ringger and Allen, 1996), spelling correction (Brill and Moore, 2000), and disfluency correction (Johnson and Charniak, 2004; Zwarts et al., 2010). Our approach differs from the traditional noisy channel approach in that it uses a word-error classifier to model the noisy channel, and semantic information to model the input characteristics. Shallow semantic parsers for SDSs have been used in (Coppola et al., 2009; Geertzen, 2009). Coppola et al. (2009) used FrameNet (Baker et al., 1998) to detect and filter the frames for target words, and employed a Support Vector Machine (SVM) classifier to perform semantic labeling. Geertzen (2009) used a shallow parser to detect semantic units only when a dependency parser failed to produce a parse tree. In contrast, our shallow semantic parser is part of a noisy channel model that post-processes the output of an ASR. 3.1 Word error classifier We investigated three classifiers to determine whether a word in the ASR textual output is correct: the Weka implementation of Decision Tre"
I13-1027,W09-3907,1,0.854975,"Missing"
I13-1027,P04-1005,0,0.0988166,"d Research This research combines three main elements: correction of ASR output, noisy channel models and shallow semantic parsing. López-Cózar and Griol (2010) used lexical approaches to replace, insert or delete words in a textual ASR output, and syntactic approaches to modify tenses of verbs and grammatical numbers to better match grammatical expectations. However, these actions make ad hoc changes. The noisy channel model has been employed for various NLP tasks, such as ASR output correction (Ringger and Allen, 1996), spelling correction (Brill and Moore, 2000), and disfluency correction (Johnson and Charniak, 2004; Zwarts et al., 2010). Our approach differs from the traditional noisy channel approach in that it uses a word-error classifier to model the noisy channel, and semantic information to model the input characteristics. Shallow semantic parsers for SDSs have been used in (Coppola et al., 2009; Geertzen, 2009). Coppola et al. (2009) used FrameNet (Baker et al., 1998) to detect and filter the frames for target words, and employed a Support Vector Machine (SVM) classifier to perform semantic labeling. Geertzen (2009) used a shallow parser to detect semantic units only when a dependency parser faile"
I13-1027,C10-1154,0,0.0972094,"was evaluated on a corpus of 295 spoken referring expressions, improving interpretation performance. 1 the stool to the left of the storm the left of Object Prep Specifier the plate in to play it in Object Noise Prep the table the table Landmark the microwave the microwave Landmark The idea of the noisy channel model is that a message is sent through a channel that introduces errors, and the receiver endeavours to reconstruct the original message by taking into account the characteristics of the noisy channel and of the transmitted information (Ringger and Allen, 1996; Brill and Moore, 2000; Zwarts et al., 2010). The system described in this paper handles three types of errors: noise (which is removed), missing prepositions (which are inserted), and mis-heard words (which are replaced). Table 1 shows two descriptions that illustrate these errors. The first row for each description displays what was spoken, the second row displays what was heard by the ASR, and the third row shows the semantic labels assigned to each segment in the description by a shallow semantic parser (Section 3.2). Specifically, in the first example, the preposition “to” is missing, and the object “stool” is mis-heard as “storm”;"
I13-1027,I13-1026,1,0.711099,"Missing"
I13-1027,C98-1013,0,\N,Missing
J09-4010,H01-1065,0,0.056,"Missing"
J09-4010,J05-3002,0,0.0130255,"f responses. An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response. This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work, we gather sets of sentences, and assume (but do not employ) existing approaches for their organization (Goldstein et al. 2000; Barzilay, Elhadad, and McKeown 2001; Barzilay and McKeown 2005). The appeal of a sentence-level approach is that it supports the generation of a “combination response” in situations where there is insufﬁcient evidence for a single document containing a full response, but there is enough evidence for parts of responses. Although such a combined response is generally less satisfactory than a full response, the information included in it may address a user’s problem or point the user in the right direction. As argued in the Introduction, when it comes to obtaining information quickly on-line, this option may be preferable to having to wait for a human-genera"
J09-4010,P00-1038,0,0.370051,"icular for partial responses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identiﬁed several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows. r r r Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006). Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al. 2000). A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was signiﬁcantly smaller than ours (Feng et al. 2006; Leuski et al. 2006). The representativeness of the sample size was not discussed in any of these studies. There are signiﬁcant practical difﬁculties associated with conducting the user studies needed to produce meaningful results for our system. Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in ord"
J09-4010,J96-2004,0,0.011257,"being compared. Each judge was given 20 of these cases, and was asked to assess the generated responses on the four criteria listed previously.14 We maximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges. In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges’ assessments would be comparable. Because the judges do not evaluate the same cases, we could not employ standard inter-annotator agreement measures (Carletta 1996). However, it is still necessary to 14 We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur. 620 Marom and Zukerman Empirical Study of Response Automation Methods have some measure of agreement, and control for bias from speciﬁc judges or speciﬁc cases. This was done by performing pairwise signiﬁcance testing, treating the data from two judges as independent samples (we used the Wilcoxon Rank-Sum Test for equal medians). We conducted this signiﬁcance test separately for each method and each of the f"
J09-4010,N03-1004,0,0.0320724,"n (this category encompasses Burke’s weighted, switching, and mixed sub-categories). Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome. More speciﬁcally, it belongs to Burke’s switching sub-category, where a single method is selected on a case-by-case basis. A similar approach is taken in Rotaru and Litman’s (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by Chu-Carroll et al. (2003) belongs to the merging category of approaches, where the output of an individual method can be used as input to a different method (this corresponds to Burke’s cascade sub-category). Because the results of all the methods are comparable, no learning is required: At each stage of the “cascade of methods,” the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics. Therefore, we need to learn from experience when to use each method. 9. Conclusion Despite its theoretical importance and commer"
J09-4010,W04-1017,0,0.01189,"nd is independent of particular requests. Thus, the SVM for SC1 has a higher reliability than that for SC3 , because it is easier for an SVM to learn when SC1 is appropriate (predominantly from the presence of the words faulty and repair). In order to ensure the relevance of the generated replies, we have placed tight restrictions on prediction probability and cluster cohesion (Table 3), which cause the Sent-Pred method to often return partial responses. Removing redundant sentences. After calculating the raw score of each sentence, we use a modiﬁed version of the Adaptive Greedy Algorithm by Filatova and Hatzivassiloglou (2004) to penalize redundant sentences in cohesive clusters. This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence—i.e., its score is not decremented). Speciﬁcally, given a sentence sk in cluster SCl which contains a sentence with a higher or equal score, the contribution of SCl to Score(sk ) (= Pr(SCl ) × Pr(sk |SCl )) is subtracted from Score(sk ). After applying these penalties, we retain only the sentences whose adjusted score"
J09-4010,W00-0405,0,0.0441605,"approach, because requests only predict or match portions of responses. An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response. This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work, we gather sets of sentences, and assume (but do not employ) existing approaches for their organization (Goldstein et al. 2000; Barzilay, Elhadad, and McKeown 2001; Barzilay and McKeown 2005). The appeal of a sentence-level approach is that it supports the generation of a “combination response” in situations where there is insufﬁcient evidence for a single document containing a full response, but there is enough evidence for parts of responses. Although such a combined response is generally less satisfactory than a full response, the information included in it may address a user’s problem or point the user in the right direction. As argued in the Introduction, when it comes to obtaining information quickly on-line, t"
J09-4010,W06-1303,0,0.0171961,"at they provide answers to queries. These systems addressed the evaluation issue as follows. r r r Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006). Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al. 2000). A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was signiﬁcantly smaller than ours (Feng et al. 2006; Leuski et al. 2006). The representativeness of the sample size was not discussed in any of these studies. There are signiﬁcant practical difﬁculties associated with conducting the user studies needed to produce meaningful results for our system. Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in order for a user study to be representative, a fairly large sample involving several hundreds of request–response pairs would have to be used. Further, user-based evaluations of the output produced by our system require the subjects to read relatively long request–"
J09-4010,N03-1020,0,0.031241,"Missing"
J09-4010,W06-0706,1,0.787894,"elect more than one sentence (see the subsequent discussion on removing redundant sentences). We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users’ requests.7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. During the 6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results. 7 We employed the LIBSVM package (Chang and Lin 2001). 605 Computational Linguistics Volume 35, Number 4 prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3. We then apply the following steps. 1. Calculate the scores of the sentences in the predicted SCs. 2. Remove redundant sentences from cohesive SCs; these are SCs which contain similar sentences. 3. Calculate the conﬁdence of the generated response. Calculating the score of a sentence. The score of each sentence sj is calcula"
J09-4010,J07-1004,0,0.071482,"Missing"
J09-4010,P06-1093,0,0.0313703,"uation of the responses generated by our system and people’s assessments of these responses is unclear, in particular for partial responses. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In Marom and Zukerman (2007a) we identiﬁed several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows. r r r Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006). Only an automatic evaluation was performed, which relied on having model responses (Berger and Mittal 2000; Berger et al. 2000). A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was signiﬁcantly smaller than ours (Feng et al. 2006; Leuski et al. 2006). The representativeness of the sample size was not discussed in any of these studies. There are signiﬁcant practical difﬁculties associated with conducting the user studies needed to produce meaningful results for our system. Fir"
J09-4010,C04-1128,0,0.0203706,"ere data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts). With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpus-based approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). Two applications that, like help-desk, deal with question–answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; 630 Marom and Zukerman Empirical Study of Response Automation Methods Berger et al. 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to"
J09-4010,C04-1143,0,\N,Missing
J14-2003,P06-2058,0,0.0698093,"Missing"
J14-2003,P05-1015,0,0.190251,"Missing"
J14-2003,D09-1026,0,0.0308171,"at of task-specific models (Mimno and McCallum 2008). As the focus of our work is on modeling authors, we experimented only with LDA and with the task-specific topic models discussed in Section 3 (AT and DADT, which model authors explicitly). The applicability of generic models to authorship attribution is an open question that would be interesting to investigate in the future. Nonetheless, most of the generic models surveyed here have properties that make them unsuitable for our purposes. Examples of generic upstream models include DiscLDA (Lacoste-Julien, Sha, and Jordan 2008), Labeled LDA (Ramage et al. 2009), and DMR (Mimno and McCallum 2008). The former two dedicate at least one topic to each metadata label, making them too computationally expensive to use on data sets with thousands of authors, such as the Blog and IMDb1M data sets (Section 5.1). In contrast to DiscLDA and Labeled LDA, DMR uses less topics by sharing them between labels. Mimno and McCallum (2008) showed that DMR outperformed AT on authorship attribution of multi-authored documents. Despite this, we decided to use AT, because we found in preliminary experiments that AT performs better than DMR on authorship attribution of single"
J14-2003,W06-1657,0,0.0124808,"nd Juola 2011). In this article, we focus on the closed-set attribution task, where training texts by the candidate authors are supplied in advance, and for each test text, the goal is to attribute the text to the correct author out of the candidate authors (Argamon and Juola 2011). Related tasks include open-set attribution, where some test texts may not have been written by any of the candidate authors, and verification, where texts by only one candidate author are supplied in advance, and the task is to verify whether test texts were written by the candidate author (Koppel and Schler 2004; Sanderson and Guenter 2006; Koppel, Schler, and Argamon 2011). Regardless of the task, a challenge currently faced by researchers in the field is addressing scenarios with many candidate authors and varying amounts of data per author (Argamon and Juola 2011; Koppel, Schler, and Argamon 2011; Luyckx and Daelemans 2011). This challenge is illustrated by the corpus chosen for the PAN’11 competition (Argamon and Juola 2011), which contains short e-mails by tens of authors. Other examples are Koppel, Schler, and Argamon’s (2011) work on a corpus of blog posts by thousands of authors, and Luyckx and Daelemans’s (2011) study"
J14-2003,P12-2052,1,0.478131,"Missing"
J14-2003,W11-0321,1,0.862961,"Missing"
J14-2003,U11-1015,0,0.0340356,"Missing"
N18-1115,W17-5526,0,0.0464322,"Missing"
N18-1115,D15-1181,0,0.0838655,"Missing"
N18-1115,N16-1037,1,0.926505,"code the 1277 Figure 2: CARNN for dialog. responses as shown in Figure 2, which depicts our architecture of CARNN for dialog. ∀l ∈ [1..L] : el = P osition Encoder(ylc ) (9) We then put a distribution over the candidate responses conditioned on the summarized dialog history hhis (Equation 10). P(y) = sof tmax(hThis ey1 , ..., hThis eyL ) 4.2 (10) Contextual language model Typically, language models operate at the sentence level, i.e., the sentences are treated independently. Several researchers have explored inter-sentence and inter-document level contextual information for language modelling (Ji et al., 2016a,b; Tran et al., 2016; Lau et al., 2017). Following Ji et al. (2016a,b), we investigate two types of contextual information: (i) the previous sentence context; and (ii) a latent variable capturing the connection information between sentences, such as discourse relation in the Penn Discourse Tree Bank dataset or Dialog Acts in the Switchboard dataset. Previous sentence context. The previous sentence (time-step t − 1) contextual information is encoded by a simplified version of the nCARNN, where the global context is absent. The final hidden vector of this sequence is then fed into the current"
N18-1115,P17-1033,0,0.0291302,"g. responses as shown in Figure 2, which depicts our architecture of CARNN for dialog. ∀l ∈ [1..L] : el = P osition Encoder(ylc ) (9) We then put a distribution over the candidate responses conditioned on the summarized dialog history hhis (Equation 10). P(y) = sof tmax(hThis ey1 , ..., hThis eyL ) 4.2 (10) Contextual language model Typically, language models operate at the sentence level, i.e., the sentences are treated independently. Several researchers have explored inter-sentence and inter-document level contextual information for language modelling (Ji et al., 2016a,b; Tran et al., 2016; Lau et al., 2017). Following Ji et al. (2016a,b), we investigate two types of contextual information: (i) the previous sentence context; and (ii) a latent variable capturing the connection information between sentences, such as discourse relation in the Penn Discourse Tree Bank dataset or Dialog Acts in the Switchboard dataset. Previous sentence context. The previous sentence (time-step t − 1) contextual information is encoded by a simplified version of the nCARNN, where the global context is absent. The final hidden vector of this sequence is then fed into the current recurrent computation (time-step t) as th"
N18-1115,E17-1001,0,0.204613,"understanding a piece of text may require far more than just extracting the information from that piece itself. If the piece of text is a paragraph of a document, the reader may have to consider it together with other paragraphs in the document and the topic of the document. To understand an utterance in a conversation, the utterance has to be put into the context of the conversation, which includes the goals of the participants and the dialog history. Hence the notion of context is an intrinsic component of language understanding. Inspired by recent works in dialog systems (Seo et al., 2017; Liu and Perez, 2017), we formalize the contextual sequence mapping problem as a sequence mapping problem with a strong controlling contextual element that regulates the flow of information. The system has two sources of signals: (i) the main text input, for example, the history utterance sequence in dialog systems or the sequence of words in language modelling; and (ii) the context signal, e.g., the previous utterance in a dialog system, the discourse information in contextual language modelling or the question in question answering. Our contribution in this work is two-fold. First, we propose a new family of rec"
N18-1115,miltsakaki-etal-2004-penn,0,0.0719031,"ot necessary. In the same spirit, our CARNN unit minimizes the use of non-linearity in the model to facilitate the ease of gradient flow. We also seek to keep the number of parameters to a minimum to improve trainability. We experiment with our models on a broad range of problems: dialog systems, contextual language modelling and question answering. Our systems outperform previous methods on several public datasets, which include the Babi Task 6 (Bordes and Weston, 2017) and the Frame dataset (Asri et al., 2017) for dialog, the Switchboard (Jurafsky et al., 1997) and Penn Discourse Tree Bank (Miltsakaki et al., 2004) for contextual language modelling, and the TrecQA 1274 Proceedings of NAACL-HLT 2018, pages 1274–1283 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics dataset (Wang et al., 2007) for question answering. We propose a different architecture for each task, but all models share the basic building block, the CARNN. 2 transition dynamic of RNN by removing the tanh non-linearity from the ˜cm . The equations for RAN are as follows: ˜cm = Wcx em Background and Notation gim = σ(Wih hm−1 + Wix em + bi ) Notation. As our paper describes several architectures with"
N18-1115,D17-1122,0,0.193232,"Missing"
N18-1115,I17-1057,1,0.797527,"d as follows. gum = σ(Wcu c + Weu em + bu ) gfm = σ(Wcf c + Wef em + bf ) hm = gum (gfm em ) + (1 − gum ) hm−1 (6) sCARNN can still be decomposed into a weighted sum of the sequence of input elements, and retains the parallel computation capability of the iCARNN. hM = guM gfM em + (1 − guM ) hM −1 = M X i=1 4 (gui gfi M Y (1 − guj )) ei j=i+1 (7) Summarizing the dialog history. The CARNN models take the embeddings of the sequence of utterances and produce the final representation hhis . We further enhance the output of the CARNN by adding the residual connection to the input (He et al., 2016; Tran et al., 2017), and the attention mechanism (Bahdanau et al., 2015) over the history. CARNN-based models for NLP problems In this section, we explain the details of our CARNN-based architectures for end-to-end dialog, language modelling and question answering. In each of these applications, one of the main design concerns is the choice of contextual information. As we will demonstrate in this section, the controlling context c can be derived from various sources: a sequence of words (dialog and question answering), a class variable (language modelling). Virtually any sources of strong information that can b"
N18-1115,N16-1090,1,0.796675,"2: CARNN for dialog. responses as shown in Figure 2, which depicts our architecture of CARNN for dialog. ∀l ∈ [1..L] : el = P osition Encoder(ylc ) (9) We then put a distribution over the candidate responses conditioned on the summarized dialog history hhis (Equation 10). P(y) = sof tmax(hThis ey1 , ..., hThis eyL ) 4.2 (10) Contextual language model Typically, language models operate at the sentence level, i.e., the sentences are treated independently. Several researchers have explored inter-sentence and inter-document level contextual information for language modelling (Ji et al., 2016a,b; Tran et al., 2016; Lau et al., 2017). Following Ji et al. (2016a,b), we investigate two types of contextual information: (i) the previous sentence context; and (ii) a latent variable capturing the connection information between sentences, such as discourse relation in the Penn Discourse Tree Bank dataset or Dialog Acts in the Switchboard dataset. Previous sentence context. The previous sentence (time-step t − 1) contextual information is encoded by a simplified version of the nCARNN, where the global context is absent. The final hidden vector of this sequence is then fed into the current recurrent computation"
N18-1115,D07-1003,0,0.710665,"periment with our models on a broad range of problems: dialog systems, contextual language modelling and question answering. Our systems outperform previous methods on several public datasets, which include the Babi Task 6 (Bordes and Weston, 2017) and the Frame dataset (Asri et al., 2017) for dialog, the Switchboard (Jurafsky et al., 1997) and Penn Discourse Tree Bank (Miltsakaki et al., 2004) for contextual language modelling, and the TrecQA 1274 Proceedings of NAACL-HLT 2018, pages 1274–1283 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics dataset (Wang et al., 2007) for question answering. We propose a different architecture for each task, but all models share the basic building block, the CARNN. 2 transition dynamic of RNN by removing the tanh non-linearity from the ˜cm . The equations for RAN are as follows: ˜cm = Wcx em Background and Notation gim = σ(Wih hm−1 + Wix em + bi ) Notation. As our paper describes several architectures with vastly different setups and input types, we introduce the following notation to maintain consistency and improve readability. First, the mth input to the recurrent unit will be denoted em . In language modelling, em is t"
N18-1115,P17-1062,0,0.0307967,"he second set of experiments, we use our end-to-end systems as “dialog managers”. The only difference compared to the end-to-end dialog setting is that the systems produce templatized responses instead of complete responses. Our motivation for this dialog manager setting is that in our preliminary experiments with the Babi dataset, we found out that many of the classification errors are due to very closely related responses, all of which fit the corresponding context. We argue that if we treat the systems as dialog managers, then we can delexicalize and group similar responses. Thus following Williams et al. (2017), we construct a templatized set of responses. For example, all the 1 Among the Babi tasks, we focus mainly on task 6, which is based on real human-machine interactions. The other five Babi datasets comprise synthetically generated data. 1279 Figure 4: CARNN for Question Answering. responses similar to “india house is in the west part of town” will be grouped into “ name is in the loc part of town”. The set of responses is reduced to 75 templatized responses. We call this new dataset “Babi reduced”.2 The third set of experiments is conducted on the Frame dataset. The general theme in this data"
P01-1070,A00-1041,0,0.02805,"ecise answers rather than returning documents, which is the more common IR goal. QA systems typically combine traditional IR statistical methods (Salton and McGill, 1983) with “shallow” NLP techniques. One approach to the QA task consists of applying the IR methods to retrieve documents relevant to a user’s question, and then using the shallow NLP to extract features from both the user’s question and the most promising retrieved documents. These features are then used to identify an answer within each document which best matches the user’s question. This approach was adopted in (Kupiec, 1993; Abney et al., 2000; Cardie et al., 2000; Moldovan et al., 2000). The NLP components of these systems employed hand-crafted rules to infer the type of answer expected. These rules were built by considering the first word of a question as well as larger patterns of words identified in the question. For example, the question “How far is Mars?” might be characterized as requiring a reply of type DISTANCE. Our work differs from traditional QA research in its use of statistical models to predict variables that represent a user’s informational goals. The variables under consideration include the type of the informatio"
P01-1070,P00-1071,0,0.0310385,"ents, which is the more common IR goal. QA systems typically combine traditional IR statistical methods (Salton and McGill, 1983) with “shallow” NLP techniques. One approach to the QA task consists of applying the IR methods to retrieve documents relevant to a user’s question, and then using the shallow NLP to extract features from both the user’s question and the most promising retrieved documents. These features are then used to identify an answer within each document which best matches the user’s question. This approach was adopted in (Kupiec, 1993; Abney et al., 2000; Cardie et al., 2000; Moldovan et al., 2000). The NLP components of these systems employed hand-crafted rules to infer the type of answer expected. These rules were built by considering the first word of a question as well as larger patterns of words identified in the question. For example, the question “How far is Mars?” might be characterized as requiring a reply of type DISTANCE. Our work differs from traditional QA research in its use of statistical models to predict variables that represent a user’s informational goals. The variables under consideration include the type of the information requested in a query, the level of detail o"
P01-1070,A00-1025,0,\N,Missing
P12-2052,D09-1026,0,0.0480406,"-Document Topic Model Background. Our definition of DADT is motivated by the observation that when authors write texts on the same issue, specific words must be used (e.g., texts about LDA are likely to contain the words “topic” and “prior”), while other words vary in frequency according to author style. Also, texts by the same author share similar style markers, independently of content (Koppel et al., 2009). DADT aims to separate document words from author words by generating them from two disjoint topic sets of T (D) document topics and T (A) author topics. Lacoste-Julien et al. (2008) and Ramage et al. (2009) (among others) also used disjoint topic sets to represent document labels, and Chemudugunta et al. (2006) separated corpus-level topics from document-specific words. However, we are unaware of any applications of these ideas to AA. The closest work we know of is by Mimno and McCallum (2008), whose DMR model outperformed AT in AA ± (D) ± (A) d ´ ydi Â ad D ®(D) µd(D) µa(A) zdi A D (D) ¯ Á(D) t T (D) ®(A) wdi Nd Á(A) t D (A) ¯ T (A) Figure 1: The Disjoint Author-Document Topic Model of multi-authored texts (DMR does not use disjoint topic sets). We use AT rather than DMR, since we found that A"
P12-2052,W11-0321,1,0.740043,"umanities research (Stamatatos, 2009). The traditional problem, which is the focus of our work, is to attribute test texts of unknown authorship to one of a set of known authors, whose training texts are supplied in advance (i.e., a supervised classification problem). While most of the early work on AA focused on formal texts with only a few possible authors, researchers have recently turned their attention to informal texts and tens to thousands of authors (Koppel et al., 2011). In parallel, topic models have gained popularity as a means of analysing such large text corpora (Blei, 2012). In (Seroussi et al., 2011), we showed that methods based on Latent Dirichlet Allocation (LDA) – a popular topic model by Blei et al. (2003) – yield good AA performance. However, LDA does not model authors explicitly, and we are not aware of any previous studies that apply author-aware topic models to traditional AA. This paper aims to address this gap. In addition to being the first (to the best of our knowledge) to apply Rosen-Zvi et al.’s (2004) Author-Topic Model (AT) to traditional AA, the main contribution of this paper is our Disjoint Author-Document Topic Model (DADT), which addresses AT’s limitations in the con"
P12-2052,P12-2052,1,0.106108,"Missing"
P12-2052,P06-2058,0,\N,Missing
P17-2083,W14-4012,0,0.0499521,"Missing"
P17-2083,N16-1037,1,0.86789,"se innovations. 1 Introduction Dialogue Act (DA) classification is a sequenceto-sequence learning task where a sequence of utterances is mapped into a sequence of DAs. Some works in DA classification treat each utterance as an independent instance (Julia et al., 2010; Gamb¨ack et al., 2011), which leads to ignoring important long-range dependencies in the dialogue history. Other works have captured inter-utterance relationships using models such as Hidden Markov Models (HMMs) (Stolcke et al., 2000; Surendran and Levow, 2006) or Recurrent Neural Networks (RNNs) (Kalchbrenner and Blunsom, 2013; Ji et al., 2016), where RNNs have been particularly successful. In this paper, we present a generative model of utterances and dialogue acts which conditions on the relevant part of the dialogue history. To this effect, we use the attention mechanism (Bahdanau et al., 2014) developed originally for sequence-tosequence models, which has proven effective in Machine Translation (Bahdanau et al., 2014; Luong et al., 2015) and DA classification (Shen and 2 Model Description Assume that we have a training dataset D comprising a collection of dialogues, where each dialogue consists of a sequence of utterances {yt }T"
P17-2083,W13-3214,0,0.0748741,"the effectiveness of each of these innovations. 1 Introduction Dialogue Act (DA) classification is a sequenceto-sequence learning task where a sequence of utterances is mapped into a sequence of DAs. Some works in DA classification treat each utterance as an independent instance (Julia et al., 2010; Gamb¨ack et al., 2011), which leads to ignoring important long-range dependencies in the dialogue history. Other works have captured inter-utterance relationships using models such as Hidden Markov Models (HMMs) (Stolcke et al., 2000; Surendran and Levow, 2006) or Recurrent Neural Networks (RNNs) (Kalchbrenner and Blunsom, 2013; Ji et al., 2016), where RNNs have been particularly successful. In this paper, we present a generative model of utterances and dialogue acts which conditions on the relevant part of the dialogue history. To this effect, we use the attention mechanism (Bahdanau et al., 2014) developed originally for sequence-tosequence models, which has proven effective in Machine Translation (Bahdanau et al., 2014; Luong et al., 2015) and DA classification (Shen and 2 Model Description Assume that we have a training dataset D comprising a collection of dialogues, where each dialogue consists of a sequence of"
P17-2083,D15-1166,0,0.137245,"Missing"
P17-2083,J00-3003,0,0.900694,"Missing"
P17-2083,P16-1122,0,0.0275065,"ut yn , and the previous hidden state hn−1 : αn = g(hn−1 , E yn ) , 2.2 For prediction, we choose the sequence of dialogue acts with the highest posterior probability: 0 0 arg max PΘ (z1:T |y1:T ) = arg max PΘ (z1:T , y1:T ) 0 0 z1:T eαn , αn0 n0 =1 e where g is a non-linear function. Once the attention is defined, the representation of the input is constructed as X c= anh n . (6) n The problem with this traditional attention model is that the final hidden state is a function of all the inputs, hence it is usually more “informative” than the earlier hidden states due to semantic accumulation (Wang et al., 2016). Thus, most of the attention signal is assigned to the hidden states toward the end of a sequence. In DA classification, this may not be desirable, since an important token with respect to a dialogue act can appear anywhere in an utterance. We call this the attention bias problem. We propose a novel gated attention mechanism, which is inspired by the gating mechanism in LSTMs, to fix the attention bias problem. Similar to the forget gate of LSTMs, we use the available information to calculate an attention gate that learns whether to allow the whole input signal to pass through or to forget al"
P17-2083,C10-2150,0,0.0601902,"Missing"
U12-1008,P08-1034,0,0.13463,"s) (Cortes and Vapnik, 1995) or Na¨ıve Bayes (NB) (Domingos and Pazzani, 1997). The sentiment expressed in word patterns has been exploited by considering word ngrams (Hu et al., 2007), applying feature selection to handle the resultant proliferation of features (Mukras et al., 2007). In addition, when performing multi-way classiﬁcation, approaches that consider class-label similarities (Bickerstaffe and Zukerman, 2010; Pang and Lee, 2005) generally outperform those that do not. Lexicon-based methods for sentiment analysis have been investigated in (Beineke et al., 2004; Taboada et al., 2011; Andreevskaia and Bergler, 2008; Melville et al., 2009) in the context of binary, rather than multi-way, sentiment classiﬁers. These methods often require intensive labour (e.g., via the Mechanical Turk service) to build up the lexicon (Taboada et al., 2011) or use a small, generic lexicon enhanced by sources from the Internet (Beineke et al., 2004). Andreevskaia and Bergler (2008) and Melville et al. (2009) employ a weighted average to combine information from the lexicon with the classiﬁ- Minh Duc Cao and Ingrid Zukerman. 2012. Experimental Evaluation of a Lexicon- and Corpus-based Ensemble for Multi-way Sentiment Analysi"
U12-1008,baccianella-etal-2010-sentiwordnet,0,0.0185919,"iment of every linguistic entity (i.e., word, phrase, sentence or review); and (2) it has appealing computational properties which facilitate the combination of the Beta distributions of those entities. The combination of the distributions of the words in a sentence yield a Beta distribution for the sentence, and the combination of the distributions for the sentences in a review yield a Beta distribution for the review. To fully exploit the grammatical structure of a sentence, we ﬁrst parse the sentence using the Stanford parser (Klein and Manning, 2003). We 1 We also considered SentiWordNet (Baccianella et al., 2010), but it yielded inferior results. 53 then map the sentiment values of a word from the lexicon to the α and β parameters of the Beta distribution for the word, while maintaining the constraint α + β = 1 (this constraint is relaxed for phrases, sentences and the entire review). Speciﬁcally, α = 1 for a strong positive word, and β = 1 for a strong negative word; a weak positive word is assigned α = 0.75, and a weak negative word β = 0.75; and α = β = 0.5 for a neutral word. We employ the function ⊕ to combine the distributions of individual words into distributions of successively higher-level s"
U12-1008,P04-1034,0,0.0153125,"ifferent types of heuristics, e.g., the modiﬁcation of the probabilities of individual sentences is additive, while sentence combination is multiplicative (as per the Beta distribution). The application of machine learning techniques or a hill-climbing procedure to determine parameter values that yield improved performance, as well as the consideration of different heuristics for negations, adverbial modiﬁers, sentence connectives and dealing with uncertainty, may be a profitable avenue of investigation after lexicon coverage is increased. 3 Combining the Lexicon with a Na¨ıve Bayes Classiﬁer Beineke et al. (2004) combined a lexicon with an NB classiﬁer by sourcing from a large corpus words that co-occur with known sentimental “anchor” words, and employing these words to train the classiﬁer. In contrast, like Andreevskaia and Bergler (2008) and Melville et al. (2009), we combine information from a lexicon with the classiﬁcation produced by a supervised machine learning method. However, in their systems, the weights assigned to each contributing method are based on this method’s performance on the training set, while our weights represent a method’s coverage of the current text. In addition, we employ m"
U12-1008,J96-1002,0,0.0284833,"Missing"
U12-1008,C10-1008,1,0.850284,"ysis is to determine the polarity of sentiment in text. Much of the work on this problem has considered binary sentiment polarity (positive or negative) at granularity levels ranging from sentences (Mao and Lebanon, 2006; McDonald et al., 2007) to documents (Wilson et al., 2005; Allison, 2008). Multiway polarity classiﬁcation, i.e., the problem of inferring the “star” rating associated with a review, has been attempted in several domains, e.g., restaurant reviews (Snyder and Barzilay, 2007) * The majority of this work was done while the ﬁrst author was at Monash University. and movie reviews (Bickerstaffe and Zukerman, 2010; Pang and Lee, 2005). Star ratings are more informative than positive/negative ratings, and are commonly given in reviews of ﬁlms, restaurants, books and consumer goods. However, because of this ﬁner grain, multi-way sentiment classiﬁcation is a more difﬁcult task than binary classiﬁcation. Hence, the results for multi-way classiﬁcation are typically inferior to those obtained for the binary case. Most of the research on sentiment analysis uses supervised classiﬁcation methods such as Maximum Entropy (Berger et al., 1996), Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) or Na¨ıve Bay"
U12-1008,P07-1056,0,0.0113126,"in a review is likely to be unimodal. The Beta distribution obtained from the lexicon guarantees this property, but the multinomial distribution used to train the NB classiﬁer does not. Further, the combination of the distributions obtained from the lexicon and the NB classiﬁer can lead to a bimodal distribution due to inconsistencies between the two input distributions. We posit that such bimodal sentences are unreliable, and propose the following heuristic to identify bimodal sentences.4 • Kitchen6 : This dataset was sourced from a large collection of kitchen appliance reviews collected by Blitzer et al. (2007) from Amazon product reviews. We selected 1000 reviews from each of the four classes considered by Blitzer et al., totalling 4000 reviews. The resultant dataset is denoted Kitchen4. • Music7 : We selected 4039 text samples of music reviews from the Amazon product review dataset compiled by Jindal and Liu (2008). To obtain a dataset with some degree of item consistency and reviewer reliability, we selected reviews for items that have at least 10 reviews written by users who have authored at least 10 reviews. The original reviews are associated with a 5-point rating scale, but we grouped the rev"
U12-1008,P03-1054,0,0.00323298,"distribution respectively, are wellsuited to represent the sentiment of every linguistic entity (i.e., word, phrase, sentence or review); and (2) it has appealing computational properties which facilitate the combination of the Beta distributions of those entities. The combination of the distributions of the words in a sentence yield a Beta distribution for the sentence, and the combination of the distributions for the sentences in a review yield a Beta distribution for the review. To fully exploit the grammatical structure of a sentence, we ﬁrst parse the sentence using the Stanford parser (Klein and Manning, 2003). We 1 We also considered SentiWordNet (Baccianella et al., 2010), but it yielded inferior results. 53 then map the sentiment values of a word from the lexicon to the α and β parameters of the Beta distribution for the word, while maintaining the constraint α + β = 1 (this constraint is relaxed for phrases, sentences and the entire review). Speciﬁcally, α = 1 for a strong positive word, and β = 1 for a strong negative word; a weak positive word is assigned α = 0.75, and a weak negative word β = 0.75; and α = β = 0.5 for a neutral word. We employ the function ⊕ to combine the distributions of i"
U12-1008,P07-1055,0,0.0337676,"Missing"
U12-1008,P05-1015,0,0.686538,"of sentiment in text. Much of the work on this problem has considered binary sentiment polarity (positive or negative) at granularity levels ranging from sentences (Mao and Lebanon, 2006; McDonald et al., 2007) to documents (Wilson et al., 2005; Allison, 2008). Multiway polarity classiﬁcation, i.e., the problem of inferring the “star” rating associated with a review, has been attempted in several domains, e.g., restaurant reviews (Snyder and Barzilay, 2007) * The majority of this work was done while the ﬁrst author was at Monash University. and movie reviews (Bickerstaffe and Zukerman, 2010; Pang and Lee, 2005). Star ratings are more informative than positive/negative ratings, and are commonly given in reviews of ﬁlms, restaurants, books and consumer goods. However, because of this ﬁner grain, multi-way sentiment classiﬁcation is a more difﬁcult task than binary classiﬁcation. Hence, the results for multi-way classiﬁcation are typically inferior to those obtained for the binary case. Most of the research on sentiment analysis uses supervised classiﬁcation methods such as Maximum Entropy (Berger et al., 1996), Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) or Na¨ıve Bayes (NB) (Domingos and"
U12-1008,N07-1038,0,0.0416755,"Missing"
U12-1008,J11-2001,0,0.596778,"t Vector Machines (SVMs) (Cortes and Vapnik, 1995) or Na¨ıve Bayes (NB) (Domingos and Pazzani, 1997). The sentiment expressed in word patterns has been exploited by considering word ngrams (Hu et al., 2007), applying feature selection to handle the resultant proliferation of features (Mukras et al., 2007). In addition, when performing multi-way classiﬁcation, approaches that consider class-label similarities (Bickerstaffe and Zukerman, 2010; Pang and Lee, 2005) generally outperform those that do not. Lexicon-based methods for sentiment analysis have been investigated in (Beineke et al., 2004; Taboada et al., 2011; Andreevskaia and Bergler, 2008; Melville et al., 2009) in the context of binary, rather than multi-way, sentiment classiﬁers. These methods often require intensive labour (e.g., via the Mechanical Turk service) to build up the lexicon (Taboada et al., 2011) or use a small, generic lexicon enhanced by sources from the Internet (Beineke et al., 2004). Andreevskaia and Bergler (2008) and Melville et al. (2009) employ a weighted average to combine information from the lexicon with the classiﬁ- Minh Duc Cao and Ingrid Zukerman. 2012. Experimental Evaluation of a Lexicon- and Corpus-based Ensemble"
U12-1008,H05-1044,0,0.0205862,"d approach. Section 3 describes the combination of the lexicon with an NB classiﬁer, followed by our heuristic for identifying sentimentambiguous sentences. Section 5 presents the results of our evaluation, and Section 6 offers concluding remarks. 2 Harnessing the Lexicon In this section, we present our framework for representing information from a lexicon, and combining this information into phrases, sentences and entire reviews, and our heuristics for modifying the sentiment of a word or phrase based on grammatical information. We report on the results obtained with the lexicon collected by Wilson et al. (2005), which contains 8221 sentimentcarrying words (most are open-class words, but there are a few modals, conjunctions and prepositions); each word is identiﬁed as positive, negative or neutral, and either strong or weak.1 The numeric rating of a review is inferred from the sentiment of the words in it, while taking into account the uncertainty arising from (1) the ambiguous sentiment of individual words, and (2) our ignorance due to the lack of understanding of the sentiment of some words. Instead of committing to a particular star rating for a review, we assign a probability to each star rating"
U13-1012,W07-1013,0,0.0267274,"spawned an increasing number of NLP tools for content analysis that help researchers and practitioners access the latest developments in their fields. Examples of these tools include: BANNER – a Named Entity Recognizer (NER) for the biomedical domain (Leaman and Gonzalez, 2008); ABNER – a NER for molecular biology (Settles, 2004); and Whatizit – a Web service which provides functionality to perform text-mining tasks (RebholzSchuhmann et al., 2008). These tools in turn require the development of annotated training corpora, e.g., (Kim et al., 2003; Rosario and Hearst, 2004; Kulick et al., 2004; Pestian et al., 2007; Jimeno-Yepes et al., 2008; Bada et al., 2012). Studies have been conducted to examine the performance of different NLP tools on a single corpus, e.g., (Jacob et al., 2013; Verspoor et al., 2012). However, experience shows that the characteristics of a corpus influence performance, e.g., (Cao and Zukerman, 2012) for sentiment analysis and (Pyysalo et al., 2008) in the biomedical space. In this paper, we analyze how the characteristics and annotation schemas of two corpora influence BANNER’s performance on the recognition of diseases (note that BANNER outperforms ABNER in the recognition of di"
U13-1012,U12-1008,1,0.830608,"logy (Settles, 2004); and Whatizit – a Web service which provides functionality to perform text-mining tasks (RebholzSchuhmann et al., 2008). These tools in turn require the development of annotated training corpora, e.g., (Kim et al., 2003; Rosario and Hearst, 2004; Kulick et al., 2004; Pestian et al., 2007; Jimeno-Yepes et al., 2008; Bada et al., 2012). Studies have been conducted to examine the performance of different NLP tools on a single corpus, e.g., (Jacob et al., 2013; Verspoor et al., 2012). However, experience shows that the characteristics of a corpus influence performance, e.g., (Cao and Zukerman, 2012) for sentiment analysis and (Pyysalo et al., 2008) in the biomedical space. In this paper, we analyze how the characteristics and annotation schemas of two corpora influence BANNER’s performance on the recognition of diseases (note that BANNER outperforms ABNER in the recognition of diseases (Leaman and Gonzalez, 2008)). The corpora in question are the Human Variome Project Corpus (HVPC) developed at NICTA (Verspoor et al., 2013), and the Arizona Disease Corpus (AZDC) – a popular medical resource developed at the University of Arizona (Leaman et al., 2009).1 Our results show that BANNER’s perf"
U13-1012,P04-1055,0,0.0127345,"nt growth of on-line biomedical literature has spawned an increasing number of NLP tools for content analysis that help researchers and practitioners access the latest developments in their fields. Examples of these tools include: BANNER – a Named Entity Recognizer (NER) for the biomedical domain (Leaman and Gonzalez, 2008); ABNER – a NER for molecular biology (Settles, 2004); and Whatizit – a Web service which provides functionality to perform text-mining tasks (RebholzSchuhmann et al., 2008). These tools in turn require the development of annotated training corpora, e.g., (Kim et al., 2003; Rosario and Hearst, 2004; Kulick et al., 2004; Pestian et al., 2007; Jimeno-Yepes et al., 2008; Bada et al., 2012). Studies have been conducted to examine the performance of different NLP tools on a single corpus, e.g., (Jacob et al., 2013; Verspoor et al., 2012). However, experience shows that the characteristics of a corpus influence performance, e.g., (Cao and Zukerman, 2012) for sentiment analysis and (Pyysalo et al., 2008) in the biomedical space. In this paper, we analyze how the characteristics and annotation schemas of two corpora influence BANNER’s performance on the recognition of diseases (note that BANNER"
U13-1012,W04-1221,0,0.0274044,"ona Disease Corpus. Our analysis of the performance of a state-of-the-art NER tool in terms of the characteristics and annotation schema of these corpora shows that these factors significantly affect performance. 1 Introduction The recent growth of on-line biomedical literature has spawned an increasing number of NLP tools for content analysis that help researchers and practitioners access the latest developments in their fields. Examples of these tools include: BANNER – a Named Entity Recognizer (NER) for the biomedical domain (Leaman and Gonzalez, 2008); ABNER – a NER for molecular biology (Settles, 2004); and Whatizit – a Web service which provides functionality to perform text-mining tasks (RebholzSchuhmann et al., 2008). These tools in turn require the development of annotated training corpora, e.g., (Kim et al., 2003; Rosario and Hearst, 2004; Kulick et al., 2004; Pestian et al., 2007; Jimeno-Yepes et al., 2008; Bada et al., 2012). Studies have been conducted to examine the performance of different NLP tools on a single corpus, e.g., (Jacob et al., 2013; Verspoor et al., 2012). However, experience shows that the characteristics of a corpus influence performance, e.g., (Cao and Zukerman, 20"
U13-1012,E12-2021,0,0.0140726,"els to input tokens, and considers the following features: (1) lemma for a token; (2) part of speech; (3) orthographic features, such as capitalization, presence of digits, prefixes and suffixes, and 2 and 3-character n-grams. • HVPC annotates only the last and most complete part of a disease coordination2 (e.g., in “breast and ovarian cancer”, “breast” is annotated as a body part3 ), while AZDC annotates a coordination as separate but overlapping mentions of a disease (e.g., “breast and ovarian cancer” and “ovarian cancer”). 2 This was originally done in response to the BRAT annotation tool (Stenetorp et al., 2012) not allowing annotation of discontinuous entities (since rectified). 3 A refinement is to consider (body-part, disease) related pairs as multi-word disease names, which would boost the mention-length counts for HVPC in Figure 2. 92 Parameter # of sentences # of tokens Total # of disease mentions # of unique disease mentions HVPC AZDC 2116 52454 1552 130 2783 79950 3228 1202 Parameter Frequency mean Frequency standard deviation Ratio of top N frequent mentions to all mentions N = 10 N = 20 N = 30 Table 1: Various quantitative parameters of HVPC and AZDC. Unique mentions refer to all (casesensi"
U13-1012,W04-3111,0,0.0455948,"dical literature has spawned an increasing number of NLP tools for content analysis that help researchers and practitioners access the latest developments in their fields. Examples of these tools include: BANNER – a Named Entity Recognizer (NER) for the biomedical domain (Leaman and Gonzalez, 2008); ABNER – a NER for molecular biology (Settles, 2004); and Whatizit – a Web service which provides functionality to perform text-mining tasks (RebholzSchuhmann et al., 2008). These tools in turn require the development of annotated training corpora, e.g., (Kim et al., 2003; Rosario and Hearst, 2004; Kulick et al., 2004; Pestian et al., 2007; Jimeno-Yepes et al., 2008; Bada et al., 2012). Studies have been conducted to examine the performance of different NLP tools on a single corpus, e.g., (Jacob et al., 2013; Verspoor et al., 2012). However, experience shows that the characteristics of a corpus influence performance, e.g., (Cao and Zukerman, 2012) for sentiment analysis and (Pyysalo et al., 2008) in the biomedical space. In this paper, we analyze how the characteristics and annotation schemas of two corpora influence BANNER’s performance on the recognition of diseases (note that BANNER outperforms ABNER in"
U13-1014,W11-2002,0,0.0678862,"Missing"
U13-1014,I13-1026,1,0.882388,"Missing"
U14-1007,W07-2307,0,0.0448355,"Missing"
U14-1007,I13-1026,1,0.8064,"Missing"
U14-1007,J12-1006,0,0.0261313,"Missing"
U14-1007,W11-2808,0,0.0319289,"Missing"
U14-1007,J06-2002,0,\N,Missing
U14-1016,P13-2116,0,0.067463,"Missing"
U14-1016,W09-1306,1,0.339189,", Lawrence Cavedon and Ingrid Zukerman. 2014. Challenges in Information Extraction from Tables in Biomedical Research Publications: a Dataset Analysis. In Proceedings of Australasian Language Technology Association Workshop, pages 118−122. structured text (e.g., table titles, footnotes and nontable prose discussing a table) must be considered to disambiguate table entries. 2 Analysis Design The dataset used in our analysis comprises a set of biomedical research papers discussing genetic variation. To build the dataset, we randomly sampled five articles from each of the three datasets used in (Wong et al., 2009) and (Yepes and Verspoor, 2013). The resulting sample contains 39 tables, with a total of 280 columns. We manually analysed the dataset to collect statistics regarding typical data types in the tables (Section 3.1). Columns in the tables were annotated with Semantic Types (STs) from the Unified Medical Language System (UMLS), which has 133 STs in total. To assign a label to a column in a table, the annotator first located a specific UMLS concept corresponding to a fine-grained type of the entities listed in the column (e.g., “[C0009221] Codon (nucleotide sequence)” for Columns 3-7 in Figure 1)"
U18-1007,J00-3003,0,0.601454,"Missing"
U18-1007,E17-1041,1,0.918862,"echniques have been proposed to capture the semantic correlation between utterances and DAs. Earlier on, statistical techniques such as Hidden Markov Models (HMMs) were widely used to recognise DAs (Stolcke et al., 2000; Julia et al., 2010). Recently, due to the enormous success of neural networks in sequence labeling/transduction tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Popov, 2016), several recurrent neural network (RNN) based architectures have been proposed to conduct DA classification, resulting in ∗ Equal contribution promising outcomes (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Despite the success of previous work in DA classification, there are still several fundamental issues. Firstly, most of the previous works rely on transcriptions (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Fewer of these focus on combining speech and textual signals (Julia et al., 2010), and even then, the textual signals in these works utilise the oracle transcriptions. We argue that in the context of a spoken dialog system, oracle transcriptions of utterances are usually not available, i.e. the agent does not have access to the human transcriptions. Speech and textual data"
U18-1007,D17-1229,1,0.625302,"echniques have been proposed to capture the semantic correlation between utterances and DAs. Earlier on, statistical techniques such as Hidden Markov Models (HMMs) were widely used to recognise DAs (Stolcke et al., 2000; Julia et al., 2010). Recently, due to the enormous success of neural networks in sequence labeling/transduction tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Popov, 2016), several recurrent neural network (RNN) based architectures have been proposed to conduct DA classification, resulting in ∗ Equal contribution promising outcomes (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Despite the success of previous work in DA classification, there are still several fundamental issues. Firstly, most of the previous works rely on transcriptions (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Fewer of these focus on combining speech and textual signals (Julia et al., 2010), and even then, the textual signals in these works utilise the oracle transcriptions. We argue that in the context of a spoken dialog system, oracle transcriptions of utterances are usually not available, i.e. the agent does not have access to the human transcriptions. Speech and textual data"
U18-1007,N16-1037,1,0.904418,"ue (Stolcke et al., 2000), numerous techniques have been proposed to capture the semantic correlation between utterances and DAs. Earlier on, statistical techniques such as Hidden Markov Models (HMMs) were widely used to recognise DAs (Stolcke et al., 2000; Julia et al., 2010). Recently, due to the enormous success of neural networks in sequence labeling/transduction tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Popov, 2016), several recurrent neural network (RNN) based architectures have been proposed to conduct DA classification, resulting in ∗ Equal contribution promising outcomes (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Despite the success of previous work in DA classification, there are still several fundamental issues. Firstly, most of the previous works rely on transcriptions (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Fewer of these focus on combining speech and textual signals (Julia et al., 2010), and even then, the textual signals in these works utilise the oracle transcriptions. We argue that in the context of a spoken dialog system, oracle transcriptions of utterances are usually not available, i.e. the agent does not have access to the human"
U18-1007,K16-1028,0,0.0661187,"Missing"
W00-1406,J99-1001,0,0.0320284,"kill Mr Body). Howates rebuttals which take into account a user's inever, this attempt also fails, leaving BIAS with a tentions, as done by BIAS. moderate belief in Mr Green's guilt. 4 Several researchers have dealt with different asIt is important to note that although BIAS' impects of argumentation; e.g., (Flowers et al., 1982; mediate objective is to strengthen its belief in the Quilici, 1992; Chu-Carroll and Carberry, 1995; Cargoal proposition, its primary purpose is to ""tell the berry and Lambert, 1999). Like BIAS, the system truth"" to the best of its knowledge (which may condescribed in Carberry and Lambert (1999) combined tradict its initial beliefs), rather than to win the arlinguistic and contextual knowledge to recognize a gument at all costs. Our algorithm supports this user's intentions from rejoinders. However, their attitude by retaining any sub-argument which has system did not generate rebuttals. Chu-Carroll and a significant impact on the goal or on a proposiCarberry (1995) provided a comprehensive approach tion on userPath. We use this disjunctive condition for proposal evaluation which focused on dialogue on impacts in order to address a situation where a strategies rather than argumentati"
W00-1406,C92-3136,0,0.0207736,"ttempts to generate an argument for and then generated a new argument in response to the goal node (by trying to reduce the belief in Mr these instructions. Neither of these systems generGreen's means and motive to kill Mr Body). Howates rebuttals which take into account a user's inever, this attempt also fails, leaving BIAS with a tentions, as done by BIAS. moderate belief in Mr Green's guilt. 4 Several researchers have dealt with different asIt is important to note that although BIAS' impects of argumentation; e.g., (Flowers et al., 1982; mediate objective is to strengthen its belief in the Quilici, 1992; Chu-Carroll and Carberry, 1995; Cargoal proposition, its primary purpose is to ""tell the berry and Lambert, 1999). Like BIAS, the system truth"" to the best of its knowledge (which may condescribed in Carberry and Lambert (1999) combined tradict its initial beliefs), rather than to win the arlinguistic and contextual knowledge to recognize a gument at all costs. Our algorithm supports this user's intentions from rejoinders. However, their attitude by retaining any sub-argument which has system did not generate rebuttals. Chu-Carroll and a significant impact on the goal or on a proposiCarberry"
W02-0227,J99-1001,0,\N,Missing
W03-1613,H92-1022,0,0.025535,"formed in the context of the TREC Question Answering task, where the system retrieves documents that contain answers to users’ queries. Node identification is performed in the context of a Natural Language (NL) interface to a Bayesian argumentation system (Zukerman and George, 2002). Here the system finds the nodes from a Bayesian network (BN) (Pearl, 1988) that best match a user’s NL sentences. Lexical paraphrases replace content words in a user’s input with their synonyms. We use the following information sources to perform this task: syntactic – obtained from Brill’s part-of-speech tagger (Brill, 1992); semantic – obtained from WordNet (Miller et al., 1990) and the Webster-1913 online dictionary; and statistical – obtained from our document collection. The statistical information is used to moderate the alternatives obtained from the semantic resources, by preferring query paraphrases that contain frequent word combinations. Our evaluation assessed the effect of lexical paraphrasing on our two applications. Its impact on document retrieval was evaluated using subsets of queries from the TREC8, TREC9 and TREC10 collections, and its effect on node identification was evaluated using paraphrase"
W03-1613,W98-0705,0,0.0296495,"s is often addressed through query expansion. Query expansion in turn is often preceded by Word sense disambiguation (WSD) in order to avoid retrieving irrelevant information. Mihalcea and Moldovan (1999) and Lytinen et al. (2000) used WordNet (Miller et al., 1990) to obtain the sense of a word. In contrast, Sch¨utze and Pedersen (1995) and Lin (1998) used a corpus-based approach where they automatically constructed a thesaurus on the basis of contextual information. The results obtained by Sch¨utze and Pedersen and by Lytinen et al. are encouraging. However, experimental results reported in (Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Harabagiu et al. (2001) offered a different form of query expansion, where they used WordNet to propose synonyms for the words in a query, and applied heuristics to select which words to paraphrase. Our approach to the vocabulary mis-match problem differs from WSD in that instead of returning the sense of the words in a sentence, we propose alternative lexical paraphrases. Like Langkilde and Knight (1998), when generating candidate para"
W03-1613,P01-1037,0,0.0605313,"cea and Moldovan (1999) and Lytinen et al. (2000) used WordNet (Miller et al., 1990) to obtain the sense of a word. In contrast, Sch¨utze and Pedersen (1995) and Lin (1998) used a corpus-based approach where they automatically constructed a thesaurus on the basis of contextual information. The results obtained by Sch¨utze and Pedersen and by Lytinen et al. are encouraging. However, experimental results reported in (Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Harabagiu et al. (2001) offered a different form of query expansion, where they used WordNet to propose synonyms for the words in a query, and applied heuristics to select which words to paraphrase. Our approach to the vocabulary mis-match problem differs from WSD in that instead of returning the sense of the words in a sentence, we propose alternative lexical paraphrases. Like Langkilde and Knight (1998), when generating candidate paraphrases, we use WordNet to propose synonyms for the words in a sentence. However, they chose among these synonyms using the word-sense rankings offered by WordNet, while we make our s"
W03-1613,P98-1116,0,0.0466612,"er, experimental results reported in (Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Harabagiu et al. (2001) offered a different form of query expansion, where they used WordNet to propose synonyms for the words in a query, and applied heuristics to select which words to paraphrase. Our approach to the vocabulary mis-match problem differs from WSD in that instead of returning the sense of the words in a sentence, we propose alternative lexical paraphrases. Like Langkilde and Knight (1998), when generating candidate paraphrases, we use WordNet to propose synonyms for the words in a sentence. However, they chose among these synonyms using the word-sense rankings offered by WordNet, while we make our selection using word-pair frequencies obtained from our corpus, and word-similarity information obtained from a thesaurus that is automatically constructed from the Webster Dictionary. It is worth noting that the retrieval performance obtained with paraphrases generated using our dictionary-based thesaurus compares favorably with that obtained using Lin’s context-based thesaurus. 3 A"
W03-1613,P98-2127,0,0.0160235,"ider the paraphrasing process, and in Section 5 the information retrieval procedures. Section 6 presents the results of our evaluation, followed by concluding remarks. 2 Related Research The vocabulary mis-match between a user’s queries and indexed documents is often addressed through query expansion. Query expansion in turn is often preceded by Word sense disambiguation (WSD) in order to avoid retrieving irrelevant information. Mihalcea and Moldovan (1999) and Lytinen et al. (2000) used WordNet (Miller et al., 1990) to obtain the sense of a word. In contrast, Sch¨utze and Pedersen (1995) and Lin (1998) used a corpus-based approach where they automatically constructed a thesaurus on the basis of contextual information. The results obtained by Sch¨utze and Pedersen and by Lytinen et al. are encouraging. However, experimental results reported in (Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Harabagiu et al. (2001) offered a different form of query expansion, where they used WordNet to propose synonyms for the words in a query, and applied heuristics to select"
W03-1613,P99-1020,0,0.0347707,"ated by people for some nodes in our BN (Section 6.2). In the next section we discuss related research. Section 3 describes our two applications. In Section 4, we consider the paraphrasing process, and in Section 5 the information retrieval procedures. Section 6 presents the results of our evaluation, followed by concluding remarks. 2 Related Research The vocabulary mis-match between a user’s queries and indexed documents is often addressed through query expansion. Query expansion in turn is often preceded by Word sense disambiguation (WSD) in order to avoid retrieving irrelevant information. Mihalcea and Moldovan (1999) and Lytinen et al. (2000) used WordNet (Miller et al., 1990) to obtain the sense of a word. In contrast, Sch¨utze and Pedersen (1995) and Lin (1998) used a corpus-based approach where they automatically constructed a thesaurus on the basis of contextual information. The results obtained by Sch¨utze and Pedersen and by Lytinen et al. are encouraging. However, experimental results reported in (Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Harabagiu et al. (2001)"
W03-1613,P02-1005,0,0.0637594,"ults. Our evaluation determines the effect of paraphrasing on retrieval performance, as well as the number of paraphrases that yields the best performance. For both applications, we submitted to the retrieval engine increasing sets of paraphrases as follows: first the lemmatized query or sentence alone (Set 0), next we added to the query or sentence up to 2 paraphrases (Set 2), then up to 5 paraphrases (Set 5), up to 12 paraphrases (Set 12), and up to a maximum of 19 paraphrases (Set 19).4 For the DocRet application, the number of retrieved documents was kept constant at 200, as suggested in (Moldovan et al., 2002). Since for the NodeID application there is only one correct node, it is critical to return this node most of the time. Hence, we considered 1 or 3 retrieved nodes. We experimented with different combinations of the operating parameters of the paraphrasing process as follows: (P1) WordNet alone or WordNet+Webster; (P2) Pr Slem&quot;  lem &quot;  – baseline measure or Webster similarity score (sim); and (P3) Pr lem  &quot;7 ctxt  &quot; – baseline measure or the approximation in Eqn. 8 (ctxt). The combination of these parameters yielded the four configurations in Table 1 (the remaining options are no"
W03-1613,C02-1047,1,0.917558,"f this problem: if a user’s vocabulary differs from that within the resource being accessed, the system may be unable to satisfy the user’s requirements. In this paper, we investigate the application of lexical paraphrasing to two different information access applications: document retrieval and node identification. Document retrieval is performed in the context of the TREC Question Answering task, where the system retrieves documents that contain answers to users’ queries. Node identification is performed in the context of a Natural Language (NL) interface to a Bayesian argumentation system (Zukerman and George, 2002). Here the system finds the nodes from a Bayesian network (BN) (Pearl, 1988) that best match a user’s NL sentences. Lexical paraphrases replace content words in a user’s input with their synonyms. We use the following information sources to perform this task: syntactic – obtained from Brill’s part-of-speech tagger (Brill, 1992); semantic – obtained from WordNet (Miller et al., 1990) and the Webster-1913 online dictionary; and statistical – obtained from our document collection. The statistical information is used to moderate the alternatives obtained from the semantic resources, by preferring"
W03-1613,C02-1161,1,0.533525,"Missing"
W03-1613,C98-1112,0,\N,Missing
W03-1613,C98-2122,0,\N,Missing
W03-2104,W02-0227,1,\N,Missing
W06-0706,P00-1038,0,0.420476,"ial responses of high precision may be better than complete responses with a lower precision. 4 Related Research There are very few reported attempts at corpusbased automation of help-desk responses. The retrieval system eResponder (Carmel et al., 2000) is similar to our Answer Retrieval method, where the system retrieves a list of request-response pairs and presents a ranked list of responses to the user. Our results show that due to the repetitions in the responses, multi-document summarization can be used to produce a single (possibly partial) representative response. This is recognized by Berger and Mittal (2000), who employ query-relevant summarization to generate responses. However, their corpus consists of FAQ 46 request-response pairs — a significantly different corpus to ours in that it lacks repetition and redundancy, and where the responses are not personalized. Lapalme and Kosseim (2003) propose a retrieval approach similar to our Answer Retrieval method, and a question-answering approach, but applied to a corpus of technical documents rather than request-response pairs. The methods presented in this paper combine different aspects of document retrieval, question-answering and multidocument su"
W06-0706,C04-1057,0,0.0622355,"ect an information item. For example, the absence of a particular term in a request may be a good predictive feature (which cannot be considered in traditional retrieval). Thus, prediction could yield replies that do not match particular query terms. • Observation O2 leads us to consider two levels of granularity: document and sentence. That is, we can obtain a document comprising a complete answer on the basis of a request (i.e., reuse an answer to a previous request), or we can obtain individual sentences and then combine them to compose an answer, as is done in multidocument summarization (Filatova and Hatzivassiloglou, 2004). The sentence-level granularity enables the re-use of a sentence for different responses, as well as the composition of partial responses. The methods developed on the basis of these two dimensions are: Retrieve Answer, Predict Answer, Predict Sentences, Retrieve Sentences and Hybrid Predict-Retrieve Sentences. The first four methods represent the possible combinations of information-gathering technique and level of granularity; the fifth method is a hybrid where the two information-gathering techniques are applied at the sentence level. The generation of responses under these different metho"
W06-0706,N03-1020,0,0.0335802,"Missing"
W06-0706,N04-3012,0,0.0186916,"es further consideration. As seen in Section 3, our f-score penalizes the Sentence Prediction and Hybrid methods when they produce good answers that are more informative than the model answer. As mentioned previously, a user study would provide a more conclusive evaluation of the system, and could be used to determine preferences regarding partial responses. Finally, we propose the following extensions to our current implementation. First, we would like to improve the representation used for clustering, prediction and retrieval by using features that incorporate word-based similarity metrics (Pedersen et al., 2004). Secondly, we intend to investigate a more focused sentence retrieval approach that utilizes syntactic matching of sentences. For example, if a sentence cluster is strongly predicted by a request, but the cluster is uncohesive because of a low verb agreement, then the retrieval should favour the sentences whose verbs match those in the request. Acknowledgments This research was supported in part by grant LP0347470 from the Australian Research Council and by an endowment from Hewlett-Packard. The authors also thank Hewlett-Packard for the extensive help-desk data, and Tony Tony for assistance"
W06-1319,J99-1001,0,0.0821257,"Missing"
W06-1319,P02-1038,0,0.0474356,"ic formalism, which considers interpretations comprising inferential relations, and then show how our formalism is extended to suppositions that account for the beliefs in an argument, and justifications that account for the inferences in an interpretation. Our evaluations with users show that the interpretations produced by our system are acceptable, and that there is strong support for the postulated suppositions and justifications. 1 Introduction The source-channel approach has been often used for word-based language tasks, such as speech recognition and machine translation (Epstein, 1996; Och and Ney, 2002). According to this approach, an addressee receives a noisy channel (language or speech wave), and decodes this channel to derive the source (idea). The selected source is that with the maximum posterior probability. In this paper, we apply the source-channel approach to the interpretation of arguments. This approach enables us to cast argument interpretation as a trade-off between conflicting factors, viz model complexity against data fit, and structure complexity against belief reasonableness. This trade-off is inspired by the Minimum Message Length (MML) Criterion – a model selection method"
W06-1319,P87-1030,0,0.421931,"Missing"
W09-3907,P06-2051,0,0.0685912,"Missing"
W09-3907,J94-4002,0,0.187799,"e parent parse tree, and the relations are derived from syntactic information in the parse tree and prepositions (Figure 1(a) illustrates UCGs U D and U I generated from the sentences “The mug is on the table. Clean it.”). Our algorithm requires sentence mode (declarative, imperative or interrogative3 ), and resolved references to determine how to combine the sentences in a sequence. Sentence mode is obtained using a classifier trained on part of our corpus (Section 2.2). The probability distribution for the referents of each identifier is obtained from the corpus and from rules derived from (Lappin and Leass, 1994; Ng et al., 2005) (Section 2.3). At this point, for each sentence Ti in a sequence, we have a list of UCGs, a list of modes, and lists of referents (one list for each identifier in the sentence). In Step 7, Algorithm 1 generates a tuple (Ui , Mi , Ri ) for each sentence Ti by selecting from these lists a UCG, a mode and a referent for each identifier (yielding a list of identifier-referent pairs). Each element in each (U, M, R) tuple is iteratively selected by traversing the appropriate list in descending order of probability. For instance, given sentences T1 , T2 , T3 , the top UCG for T1 is"
W09-3907,P06-4015,0,0.0684526,"Missing"
W09-3907,P03-1031,0,0.0772167,"Missing"
W11-0321,C10-1008,1,0.903666,"than positive examples, often yielding poor results due to class imbalance (Raskutti and Kowalczyk, 2004). Other setups, such as one-vs.-one or directed acyclic graph, require training O(A2 ) classifiers, making them impractical where thousands of authors exist. Multi-class SVMs have also been suggested, but they generally perform comparably to OVA while taking longer to train (Rifkin and Klautau, 2004). Hence, using SVMs for scenarios with many candidate authors is problematic (Koppel et al., 2011). Recent approaches to employing binary SVMs consider class similarity to improve performance (Bickerstaffe and Zukerman, 2010; Cheng et al., 2007). We leave experiments with such approaches for future work (Section 5). In this paper, we focus on authorship attribution with many candidate authors. This problem was previously addressed by Madigan et al. (2005) and Luyckx and Daelemans (2008), who worked on datasets with texts by 114 and 145 authors respectively. In both cases, the reported results were much poorer than those reported in the binary case. More recently, Koppel et al. (2011) considered author similarity to handle cases with thousands of candidate authors. Their method, which we use as our baseline, is de"
W11-0321,C04-1088,0,0.0320901,"pose ways of employing Latent Dirichlet Allocation in authorship attribution. We show that our approach yields state-of-the-art performance for both a few and many candidate authors, in cases where these authors wrote enough texts to be modelled effectively. 1 2 Introduction The problem of authorship attribution – attributing texts to their original authors – has received considerable attention in the last decade (Juola, 2006; Stamatatos, 2009). Most of the work in this field focuses on cases where texts must be attributed to one of a few candidate authors, e.g., (Mosteller and Wallace, 1964; Gamon, 2004). Recently, researchers have turned their attention to scenarios with tens to thousands of candidate authors (Koppel et al., 2011). In this paper, we study authorship attribution with few to many candidate authors, and introduce a new method that achieves state-of-the-art performance in the latter case. Our approach to authorship attribution consists of building models of authors and their texts using Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We compare these models to models built from texts Related Work The field of authorship attribution predates modern computing. For example,"
W11-0321,C08-1065,0,0.0267511,"lass SVMs have also been suggested, but they generally perform comparably to OVA while taking longer to train (Rifkin and Klautau, 2004). Hence, using SVMs for scenarios with many candidate authors is problematic (Koppel et al., 2011). Recent approaches to employing binary SVMs consider class similarity to improve performance (Bickerstaffe and Zukerman, 2010; Cheng et al., 2007). We leave experiments with such approaches for future work (Section 5). In this paper, we focus on authorship attribution with many candidate authors. This problem was previously addressed by Madigan et al. (2005) and Luyckx and Daelemans (2008), who worked on datasets with texts by 114 and 145 authors respectively. In both cases, the reported results were much poorer than those reported in the binary case. More recently, Koppel et al. (2011) considered author similarity to handle cases with thousands of candidate authors. Their method, which we use as our baseline, is described in Section 3.1. Our approach to authorship attribution utilises Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to build models of authors from their texts. LDA is a generative probabilistic model that is traditionally used to find topics in textual dat"
W16-5108,rebholz-schuhmann-etal-2010-calbc,0,0.0721443,"Missing"
W16-5108,P04-1055,0,0.0268938,"c information extraction requires annotated corpora for training and evaluating text mining systems. To date, biomedical text mining has focused on extracting information from prose, yielding a wealth of diverse annotated corpora for unstructured text. For example, gold and silver standard corpora have been developed for a variety of tasks, such as named entity recognition (Do˘gan et al., 2014; Kim et al., 2003; Rebholz-Schuhmann et al., 2010; Verspoor et al., 2013), entity linking (Bada et al., 2012; Do˘gan et al., 2014), and relation and event extraction (Kim et al., 2003; Lee et al., 2016; Rosario and Hearst, 2004; Verspoor et al., 2013). The source of these datasets also varies, e.g., corpora comprising research abstracts (Do˘gan et al., 2014; Kim et al., 2003; Rebholz-Schuhmann et al., 2010) versus full-text journal articles (Bada et al., 2012; Lee et al., 2016; Verspoor et al., 2013). In addition to prose, biomedical literature frequently presents information in other forms, such as tables and graphs. Several studies have shown that tables often contain important data and experimental results that are not mentioned in the main text of publications (Jimeno Yepes and Verspoor, 2013; Wong et al., 2009)"
W16-5108,U14-1016,1,0.852665,"luded in our dataset. This corpus covers topics related to the genetics of human colon cancer. The free-text parts of the papers are annotated using a small annotation schema comprising eleven named entity classes and thirteen binary relations between the entity classes. 3. An additional subset of ten papers (22 tables) was randomly sampled from Open Access subsets of three datasets comprising papers about genomic variation (Jimeno Yepes and Verspoor, 2013; Wong et al., 2009). These datasets did not contain annotations of unstructured text, but several of these papers were previously used in (Shmanina et al., 2014). 3.2 Development of Annotation Guidelines The table annotation guidelines were developed by the first author, who is a researcher in biomedical text mining. The guidelines contain four parts, each corresponding to a single annotation task: (1) cell group, (2) concept, (3) cell type, and (4) relation annotation. The initial versions of the guidelines were developed through several iterative attempts to annotate ten tables using the guidelines. After each annotation iteration, we tested whether the strict application of the guidelines yielded consistent and objective annotations, and revised th"
W16-5108,W09-1306,1,0.885111,"o and Hearst, 2004; Verspoor et al., 2013). The source of these datasets also varies, e.g., corpora comprising research abstracts (Do˘gan et al., 2014; Kim et al., 2003; Rebholz-Schuhmann et al., 2010) versus full-text journal articles (Bada et al., 2012; Lee et al., 2016; Verspoor et al., 2013). In addition to prose, biomedical literature frequently presents information in other forms, such as tables and graphs. Several studies have shown that tables often contain important data and experimental results that are not mentioned in the main text of publications (Jimeno Yepes and Verspoor, 2013; Wong et al., 2009). At the same time, Jimeno Yepes and Verspoor (2013) have shown that text mining techniques developed for prose tend to under-perform when applied to tables, because of the difference in how information is presented in tables and in text. For example, the arrangement of cells, which is meaningful for understanding table contents, is not taken into account by classical prose mining techniques. This calls for the development of specialised approaches to information extraction from tables (Table IE) in This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativec"
W19-5936,P17-1163,0,0.012925,"sions about dialogue acts over time (Horvitz et al., 2003; Liao et al., 2006). MDPs (Singh et al., 2002; Lemon, 2011), POMDPs (Williams and Young, 2007; Gaši´c and Young, 2014), and their extensions Hidden Information State Model (Young et al., 2010, 2013) and Conversational Entity Dialogue Model (Ultes et al., 2018) were used, often in combination with Reinforcement Learning (RL), to learn policies that optimize dialogue completion on the basis of feedback given by real or simulated users. Recently, deep learning has been applied to various aspects of SDSs (Wen et al., 2015; Li et al., 2016; Mrkšic et al., 2017; Prakash et al., 2016; Serban et al., 2017; Tseng et al., 2018; Yang et al., 2017). Wen et al. (2015) and Tseng et al. (2018) considered the generation of linguistically varied responses; Li et al. (2016) and Prakash et al. (2016) produced dialogue contributions of chatbots; and Serban et al. (2017) generated helpdesk responses and Twitter follow-up statements. Mrkšic et al. (2017) proposed a dialogue-state tracking framework, and Yang et al. (2017) a mechanism for slot tagging and user-intent and system-action prediction in slot-filling applications. A combination of deep learning and RL has"
W19-5936,P17-1045,0,0.0148487,"Prakash et al. (2016) produced dialogue contributions of chatbots; and Serban et al. (2017) generated helpdesk responses and Twitter follow-up statements. Mrkšic et al. (2017) proposed a dialogue-state tracking framework, and Yang et al. (2017) a mechanism for slot tagging and user-intent and system-action prediction in slot-filling applications. A combination of deep learning and RL has been used in end-to-end dialogue systems that query a knowledge-base, where user utterances are mapped to a clarification question or a knowledgebase query (Williams and Zweig, 2016; Zhao and Eskenazi, 2016; Dhingra et al., 2017). All these systems harness large corpora comprising requestresponse pairs to learn responses that are assumed to be better than alternative options. Like evaluations based on simulated users, human evaluations of (PO)MDP/RL systems focus on successful dialogue completion (Singh et al., 2002; Thomson et al., 2008; Young et al., 2010), while human evaluations of deep-learning systems assess individual responses (Wen et al., 2015; Li et al., 2016; Prakash et al., 2016; Serban et al., 2017; Dhingra et al., 2017). 308 (a) Positional relations in a room (b) Colour, size and positional relations on"
W19-5936,W12-1633,0,0.0189243,"Missing"
W19-5936,D16-1127,0,0.402775,"response types may be equally acceptable. Our study also reveals that situational risk influences the acceptability of some response types. 1 In the last two decades, research in response generation has focused on techniques that generate response policies that optimize dialogue completion, using Markov Decision Processes (MDPs), e.g., (Singh et al., 2002; Lemon, 2011), and Partially Observable MDPs (POMDPs), e.g., (Williams and Young, 2007; Gaši´c and Young, 2014). Recently, deep-learning algorithms have been used to generate dialogue responses on the basis of request-response pairs, e.g., (Li et al., 2016; Prakash et al., 2016; Serban et al., 2017). Human and simulation-based evaluations of MDP and POMDP systems focus on dialogue completion, while evaluations of deep-learning algorithms focus on individual responses. In this paper, we draw inspiration from research in Recommender Systems, where Amatriain et al. (2009) and Said and Bellogín (2018) showed that over time, users gave inconsistent ratings to items, leading to the “magic barrier” to prediction accuracy in Recommender Systems (Said and Bellogín, 2018). This prompted us to posit that people may also be inconsistent when assessing resp"
W19-5936,D16-1230,0,0.0934591,"., 2017). 308 (a) Positional relations in a room (b) Colour, size and positional relations on a table (c) Projective and positional relations on a table (d) Colour, size and positional relations in a room Figure 1: Household scenes used in our study The findings reported in this paper contribute to (PO)MDP/RL research by determining whether there are factors other than dialogue completion that affect the suitability of responses, and to deeplearning research by ascertaining whether indeed there is a single best response to each request. The research described in (Jurˇcíˇcek et al., 2011) and (Liu et al., 2016) shed light on ancillary aspects of human evaluations of system responses. The former compared evaluations by Amazon Mechanical Turk workers with evaluations by participants recruited for a lab experiment; and the latter conducted user studies to determine the validity of word-based evaluation metrics. This paper also addresses ancillary aspects of human response evaluations, viz the influence of temporal displacement and situational risk on users’ attitudes toward response types, and users’ opinions of response types obtained from different sources (including a classifier trained on a corpus"
W19-5936,W18-5039,0,0.0220204,"Missing"
W19-5936,W18-5032,0,0.0349267,"Missing"
W19-5936,D15-1199,0,0.0344602,"Missing"
W19-5936,W16-3601,0,0.0300087,"ses; Li et al. (2016) and Prakash et al. (2016) produced dialogue contributions of chatbots; and Serban et al. (2017) generated helpdesk responses and Twitter follow-up statements. Mrkšic et al. (2017) proposed a dialogue-state tracking framework, and Yang et al. (2017) a mechanism for slot tagging and user-intent and system-action prediction in slot-filling applications. A combination of deep learning and RL has been used in end-to-end dialogue systems that query a knowledge-base, where user utterances are mapped to a clarification question or a knowledgebase query (Williams and Zweig, 2016; Zhao and Eskenazi, 2016; Dhingra et al., 2017). All these systems harness large corpora comprising requestresponse pairs to learn responses that are assumed to be better than alternative options. Like evaluations based on simulated users, human evaluations of (PO)MDP/RL systems focus on successful dialogue completion (Singh et al., 2002; Thomson et al., 2008; Young et al., 2010), while human evaluations of deep-learning systems assess individual responses (Wen et al., 2015; Li et al., 2016; Prakash et al., 2016; Serban et al., 2017; Dhingra et al., 2017). 308 (a) Positional relations in a room (b) Colour, size and p"
W90-0121,J90-3008,0,0.101549,"Missing"
W90-0121,P88-1020,0,0.0524872,"Missing"
W90-0121,J88-3006,0,0.0803871,"Missing"
W94-0305,J88-3006,0,\N,Missing
W98-1222,H89-2048,0,0.0532303,"by a speaker. This information extraction process is part of the training phase for the lexical access component of a speech recognition system, where the pronunciation Thomas, Zukerman and Raskutti 175 probabilities are generated from a training corpus. The study was done on the TIMIT corpus (Fisher et al., , 1986) - - a collection of American-English read sentences with correct time-aligned acousticphonetic and orthographic (word-aligned) transcriptions. 1 The corpus contains 3696 sentences spoken by 462 speakers from 8 different dialect divisions across the United States. Previous work by Riley (1989) and Withgott and Chen (1993) used Classification and Regression Trees (CART) on a large number of different features of the corpus (such as genderi dialect and speaking rate) to obtain pronunciation information of intended phonemes. Our system obtain~ similar results using positional information and context, and using exact matches from uttered phones to intended phonemes to guide other matches. Work by Cohen et (d., (1987) on pronunciation used a couple of set sentences for multiple speakers, but did not cover a wide range of words (and thus different phone contexts). Our study considers the"
Y11-1039,N03-1020,0,0.0738015,"80. To expose each participant to as many summarisation configurations as possible, over their visit, we performed random selection without replacement over the 20 summarisation configurations. Additionally, for each exhibit area, we randomly varied the order in which the “correct” length summary vs. the short and long summaries were presented to the visitor. 4 Results 4.1 Clustering of Summarisation Configurations To determine the relative differentiation in content between the pre-generated summaries for each exhibit area, we performed a pairwise summary comparison using the ROUGE-2 metric (Lin and Hovy, 2003). For each pairing of the 20 summarisation configurations, we averaged across the different summary lengths and exhibit areas to generate an overall similarity. Based on these similarity values, we clustered the summarisation configurations using “oblivious” hierarchical agglomerative clustering over the three attributes of summarisation algorithm (binarised into the individual algorithms), pronoun filtering and content diversification. That is, we calculated the single attribute which leads to the (weighted) purest partitioning of the data at each level of the dendrogram in a bottom-up fashio"
Y11-1039,O01-1003,0,0.026584,"length was not strongly evident in our results. 5 Related Work While multi-document summarisation has been a highly active research area, personalised summarisation over single documents has received considerably less attention. 379 There has been work on personalized document search and summarisation in the medical domain for clinicians and patients, based on semantically-enhanced extraction of snippets and terminology standardisation (McKeown et al., 2001; McKeown et al., 2003). Here, however, the personalisation was at the level of two discrete user profiles, and not truly individualised. Radev et al. (2001) present the design of a search engine which supports recommendation, clustering, and personalised summarisation, but do not include any technical details or evaluation. Goren-Bar and Prete (2005) report on a preliminary situated experiment on content delivery in a museum, but found that the simple static text delivery method was preferred to the adaptive method. Berkovsky et al. (2008) present a method for generating summaries of different length, and demonstrate a correlation between the level of user interest in a topic area, and the preferred summary length. However, their method relies on"
Y11-1039,J02-4001,0,0.0509272,"s generated for different exhibit areas. A primary interest in this research is the determination of the utility of established multi-document summarisation algorithms for our novel summarisation task. In the following sections, we outline the summarisation algorithms trialled in this research, describe how we personalise summary length, and outline a simple method for content diversification, to avoid repetition in the summaries for individual exhibit areas. 3.1 Summarisation Algorithms For our experiments, we implemented five standard extractive summarisation algorithms from the literature (Radev et al., 2002): • First-N sentence algorithm (F IRST N): select the first N sentences of each document. • Lead-based algorithm (L EAD): select the first N sentences of each paragraph. • Centroid algorithm (C ENTROID): cluster the document collection using a variant of TF·IDF, and rank sentences through a weighted sum of token weights based on the cluster centroid, a sentence positional weight, and similarity with the first sentence (Radev et al., 2000). • LexRank algorithm (L EX R ANK): cluster the document collection, and rank sentences using a variant of PageRank (Brin and Page, 1998) over the component w"
Y11-1039,W00-0403,0,0.0749099,"vidual exhibit areas. 3.1 Summarisation Algorithms For our experiments, we implemented five standard extractive summarisation algorithms from the literature (Radev et al., 2002): • First-N sentence algorithm (F IRST N): select the first N sentences of each document. • Lead-based algorithm (L EAD): select the first N sentences of each paragraph. • Centroid algorithm (C ENTROID): cluster the document collection using a variant of TF·IDF, and rank sentences through a weighted sum of token weights based on the cluster centroid, a sentence positional weight, and similarity with the first sentence (Radev et al., 2000). • LexRank algorithm (L EX R ANK): cluster the document collection, and rank sentences using a variant of PageRank (Brin and Page, 1998) over the component words (Erkan and Radev, 2004). • Manifold-ranking algorithm (M ANIFOLD): score each sentence based on a manifold-ranking process, and rerank sentences based on a diversity penalty (Wan et al., 2007). Each algorithm was run over the primary and secondary document for a given exhibit area. All sentences were ranked, but only sentences from the primary document (that authored by Melbourne Museum) were candidates for selection in the final sum"
