A00-2025,C00-2140,1,0.763987,"Missing"
A97-1003,A94-1013,0,\N,Missing
C00-2140,W98-1421,0,0.0800889,". We believe that summarization of speech will become increasingly more important, as the amount of online audio data grows and demand for rapid browsing, skimming, and access of speech data increases. Another application which particularly pertains to our interest in spoken dialogue summarization would be the generation of meeting minutes for archival purposes and/or to update participants joining at later stages on the progress of the conversation so far. Summarization of dialogues within limited domains has been attempted within the context of the Verbmobil project (protocol generation&quot;, (Alexandersson and Poller, 1998)) or by SRI's MIMI summarizer (Kameyama et al., 1996). Recent work on spoken language summarization in unrestricted domains has focused almost exclusively on Broadcast News, mostly due to the spoken language track of recent TREC evaluations (Garofolo et al., 1997; Garofolo et al., 1999). (Waibel et al., 1998) describe a Meeting Browser where summaries can be generated using technology established for written texts. (Valenza et al., 1999) go one step further and incorporate knowledge from the speech recognizer (con dence scores) into their summarization system, as well. We argue that the nature"
C00-2140,W97-0703,0,0.0349591,"er out likely candidates for incomplete clauses due to speech repair or interruption by the other speaker. 6 Topic Segmentation Since Callhome dialogues are always multi-topical, segmenting them into topical units is an important step in our summarization system. This allows us to provide signature&quot; information (frequent content words) about every topic to the user as a help for faster browsing and accessing the data. Furthermore, the subsequent information condensation component can work on smaller parts of the dialogue and thus operate more eciently. Following (Boguraev and Kennedy, 1997; Barzilay and Elhadad, 1997) who use TextTiling (Hearst, 1997) for their summarization systems of written text, we adapted this algorithm (its block comparison version) for speech data: we choose turns to be minimal units and compute block similarity between blocks of k turns every d turns. We use 9 English and 15 Spanish Callhome dialogues, manually annotated for topic boundaries, to determine the optimum values for a set of TextTiling parameters and at the same time to evaluate the accuracy of this algorithm. To do this, we ran an n-fold cross-validation (jack-kni ng&quot;) where all dialogues but one are used to determine"
C00-2140,A97-1003,1,0.610422,"Missing"
C00-2140,J97-1003,0,0.17061,"due to speech repair or interruption by the other speaker. 6 Topic Segmentation Since Callhome dialogues are always multi-topical, segmenting them into topical units is an important step in our summarization system. This allows us to provide signature&quot; information (frequent content words) about every topic to the user as a help for faster browsing and accessing the data. Furthermore, the subsequent information condensation component can work on smaller parts of the dialogue and thus operate more eciently. Following (Boguraev and Kennedy, 1997; Barzilay and Elhadad, 1997) who use TextTiling (Hearst, 1997) for their summarization systems of written text, we adapted this algorithm (its block comparison version) for speech data: we choose turns to be minimal units and compute block similarity between blocks of k turns every d turns. We use 9 English and 15 Spanish Callhome dialogues, manually annotated for topic boundaries, to determine the optimum values for a set of TextTiling parameters and at the same time to evaluate the accuracy of this algorithm. To do this, we ran an n-fold cross-validation (jack-kni ng&quot;) where all dialogues but one are used to determine the best parameters (	rain set&quot;)"
C00-2140,A00-1043,0,0.0168147,"results from these experiments. Similar to other experiments in the summarization literature (Mani et al., 1998), we nd a wide performance variation across di erent texts. 8 Telegraphic Reduction The purpose of this component is to maximize information in a xed amount of space. We shorten the output of the summarizer to a 	elegraphic style&quot;; that way, more information can be included in a summary of k words (or n bytes). Since we only use shallow methods for textual analysis that do not generate a dependency structure, we cannot use complex methods for text reduction as described, e.g., in (Jing, 2000). Our method simply excludes words occurring in the stop list from the summary, except for some highly informative words such as I&quot; or 
ot&quot;. 9 User Interface and System Performance Since we want to enable interactive summarization which allows a user to browse through a dialogue quickly to search for information he is interested in, we have integrated our summarization system into a JAVA-based graphical user interface (Meeting Browser&quot;) (Bett et al., 2000). This interface also integrates the output of a speech recognizer (Yu et al., 1999), and can display a wide variety of information about"
C00-2140,P98-2237,1,0.794011,"rformance proved to be comparable with the results in the cited papers (F1 &gt; 0:85, error < 0:05).2 For several of the clean-up lter's components, we make use of Brill's POS tagger (Brill, 1994). For English, we use a modi ed version of Brill's original tag set, and the tagger was adapted and retrained for spoken language corpora (Callhome and Switchboard) (Zechner, 1997). For Spanish, we created our own tag set, derived from the LDC lexicon and from the CRATER project (Leon, 1994), and trained the tagger on manually annotated Callhome dialogues. Furthermore, a POS based shallow chunk parser (Zechner and Waibel, 1998) is used to lter out likely candidates for incomplete clauses due to speech repair or interruption by the other speaker. 6 Topic Segmentation Since Callhome dialogues are always multi-topical, segmenting them into topical units is an important step in our summarization system. This allows us to provide signature&quot; information (frequent content words) about every topic to the user as a help for faster browsing and accessing the data. Furthermore, the subsequent information condensation component can work on smaller parts of the dialogue and thus operate more eciently. Following (Boguraev and K"
C00-2140,A00-2025,1,0.777917,"Missing"
C00-2140,E99-1011,0,\N,Missing
C00-2140,C98-2232,1,\N,Missing
C98-2231,A97-1052,0,0.0165359,"frames for all senses of a verb (including semantic features, such as ""animacy"") . a mapper which tries to find the ""best match"" between the chunks found within a short clause and the subcat frames for the main verb in that clause The major blocks of the system architecture are depicted in Figure 1. We want to stress here that except for the development of the small POS grammar and the framemapper, the other components and resources were already present or quite simple to implement. There has also been significant work on (semi-)autolnatic induction of subcategorization frames (Manning, 1993; Briscoe and Carroll, 1997), such that even 3More d e t a i l s a b o u t t h e c h u n k p a r s e r c a n be f o u n d in (Zechner, 1997). 1449 semantic mapper ] frame representation Figure 1: Global system architecture without the important knowledge source from WordNet, a similar system could be built for other languages as well. Also, the Euro-WordNet project (Vossen et al., 1997) is currently underway in building WordNet resources for other European languages. 4 Preliminary Experiments We performed some initial experiments using the SWBD transcripts as input to the system. These were POS tagged, preprocessed, segm"
C98-2231,W97-0800,0,0.256495,"Missing"
C98-2231,P93-1032,0,\N,Missing
C98-2231,W97-0801,0,\N,Missing
J02-4003,W98-1421,0,0.038024,"nd understanding, even with automatic speech recognition input. Two examples of systems developed in that time frame are JANUS (Lavie et al. 1997) and VERBMOBIL (Wahlster 1993). In that context, several spoken-dialogue summarization systems have been developed whose goal it is to capture the essence of the task-based dialogues at hand. The MIMI system (Kameyama and Arima 1994; Kameyama, Kawai, and Arima 1996) deals with the travel reservation domain and uses a cascade of finite-state pattern recognizers to find the desired information. Within VERBMOBIL, a more knowledge-rich approach is used (Alexandersson and Poller 1998; Reithinger et al. 2000). The domain here is travel planning and negotiation of a trip. In addition to finite-state transducers for content extraction and statistical dialogue act recognition, VERBMOBIL also uses a dialogue processor and a summary generator that have access to a world knowledge database, a domain model, and a semantic database. The abstract representations built by this summarizer allow for summary generation in multiple languages. 3.2 Summarization of Spoken News Within the context of the Text Retrieval Conference (TREC) spoken document retrieval (SDR) conferences (Garofolo"
J02-4003,J97-1002,0,0.0318541,"Missing"
J02-4003,A97-1003,1,0.844371,"Missing"
J02-4003,J86-3001,0,0.0718952,"Missing"
J02-4003,J97-1003,0,0.0539015,"rs (and between automatic methods and coders) has been measured in the summarization literature with quite a wide range of methods: Rath, Resnick, and Savage (1961) use Kendall’s τ ; Kupiec, Pedersen, and Chen (1995) (among many others) use percentage agreement; and Aone, Okurowski, and Gorlinsky (1997) (among others) use the notions of precision, recall, and F1 -score, which are commonly employed in the information retrieval community. Similarly, in the literature on discourse segmentation and labeling, a variety of different agreement measures have been used, including precision and recall (Hearst 1997; Passonneau and Litman 1997), Krippendorff’s (1980) α (Passonneau and Litman 1997) and Cohen’s (1960) κ (Carletta et al. 1997). In this work, we use the two following metrics: (1) the κ-statistic in its extension for more than two coders (Davies and Fleiss 1982); and (2) precision, recall, and F1 -score.9 We will discuss the κ-statistic first. For intercoder agreement with respect to topical boundaries, agreement is found if boundaries fall within the same 50-word bin of a dialogue. Relevance agreements are computed at the word level. For relevance markings, we compute κ both for the three-wa"
J02-4003,J99-4003,0,0.0114573,"may occur at the end of an utterance, and they can be due to interruption by another speaker. Example: so we didn’t—they have not accepted our proposal. 5.3.3 Related Work. The past decade has produced a substantial amount of research in the area of detecting intonational and linguistic boundaries in conversational speech, as well as in the area of detecting and correcting speech disfluencies. Whereas earlier work tended to look at these phenomena in isolation (Nakatani and Hirschberg 1994; Stolcke and Shriberg 1996), more recent work has attempted to solve several tasks within one framework (Heeman and Allen 1999; Stolcke et al. 1998). Most approaches use some kind of prosodic information, such as duration of pauses, stress, and pitch contours, and most of them combine this prosodic information with information about word identity and sequence (n-grams, hidden Markov 461 Computational Linguistics Volume 28, Number 4 models). In the study of Stolcke et al. (1998), the goal was to detect sentence boundaries and a variety of speech disfluencies on a large portion of the SWITCHBOARD corpus. An explicit comparison was made between prosodic and word-based models, and the results showed that an n-gram model,"
J02-4003,W99-0306,0,0.0567194,"Missing"
J02-4003,J97-1005,0,0.00984774,"en automatic methods and coders) has been measured in the summarization literature with quite a wide range of methods: Rath, Resnick, and Savage (1961) use Kendall’s τ ; Kupiec, Pedersen, and Chen (1995) (among many others) use percentage agreement; and Aone, Okurowski, and Gorlinsky (1997) (among others) use the notions of precision, recall, and F1 -score, which are commonly employed in the information retrieval community. Similarly, in the literature on discourse segmentation and labeling, a variety of different agreement measures have been used, including precision and recall (Hearst 1997; Passonneau and Litman 1997), Krippendorff’s (1980) α (Passonneau and Litman 1997) and Cohen’s (1960) κ (Carletta et al. 1997). In this work, we use the two following metrics: (1) the κ-statistic in its extension for more than two coders (Davies and Fleiss 1982); and (2) precision, recall, and F1 -score.9 We will discuss the κ-statistic first. For intercoder agreement with respect to topical boundaries, agreement is found if boundaries fall within the same 50-word bin of a dialogue. Relevance agreements are computed at the word level. For relevance markings, we compute κ both for the three-way case (nucleus IUs, satellit"
J02-4003,P00-1040,0,0.143821,"omatic speech recognition input. Two examples of systems developed in that time frame are JANUS (Lavie et al. 1997) and VERBMOBIL (Wahlster 1993). In that context, several spoken-dialogue summarization systems have been developed whose goal it is to capture the essence of the task-based dialogues at hand. The MIMI system (Kameyama and Arima 1994; Kameyama, Kawai, and Arima 1996) deals with the travel reservation domain and uses a cascade of finite-state pattern recognizers to find the desired information. Within VERBMOBIL, a more knowledge-rich approach is used (Alexandersson and Poller 1998; Reithinger et al. 2000). The domain here is travel planning and negotiation of a trip. In addition to finite-state transducers for content extraction and statistical dialogue act recognition, VERBMOBIL also uses a dialogue processor and a summary generator that have access to a world knowledge database, a domain model, and a semantic database. The abstract representations built by this summarizer allow for summary generation in multiple languages. 3.2 Summarization of Spoken News Within the context of the Text Retrieval Conference (TREC) spoken document retrieval (SDR) conferences (Garofolo et al. 1997; Garofolo et"
J02-4003,ries-etal-2000-shallow,0,0.0149304,"ourse Structure. Previous work indicates that information about discourse structure from written texts can help in identifying the more salient and relevant sentences or clauses for summary generation (Marcu 1999; Miike et al. 1994). Much 451 Computational Linguistics Volume 28, Number 4 less exploration has been done, however, in the area of automatic analysis of discourse structure for non-task-oriented spoken dialogues in unrestricted domains, such as CALLHOME (LDC 1996). Research for those kinds of corpora reported in Jurafsky et al. (1998), Stolcke et al. (2000), Levin et al. (1999), and Ries et al. (2000) focuses more on detecting localized phenomena such as speech acts, dialogue games, or functional activities. We conjecture that there are two reasons for this: (1) free-flowing spontaneous conversations have much less structure than task-oriented dialogues, and (2) the automatic detection of hierarchical structure would be much harder than it is for written texts or dialogues based on a premeditated plan. Although we believe that in the long run attempts to automatically identify the discourse structure of spoken dialogues may benefit summarization, in this article, we greatly simplify this m"
J02-4003,J00-3003,0,0.129953,"Missing"
J02-4003,W97-0710,0,0.0928423,"Missing"
J02-4003,1993.mtsummit-1.11,0,0.0778398,"oken news in unrestricted domains. We shall discuss both of these areas in the following subsections, followed by a discussion of prosody-based emphasis detection in spoken language, and finally by a summary of research most closely related to the topic of this work. 3.1 Summarization of Dialogues in Restricted Domains During the past decade, there has been significant progress in the area of closeddomain spoken-dialogue translation and understanding, even with automatic speech recognition input. Two examples of systems developed in that time frame are JANUS (Lavie et al. 1997) and VERBMOBIL (Wahlster 1993). In that context, several spoken-dialogue summarization systems have been developed whose goal it is to capture the essence of the task-based dialogues at hand. The MIMI system (Kameyama and Arima 1994; Kameyama, Kawai, and Arima 1996) deals with the travel reservation domain and uses a cascade of finite-state pattern recognizers to find the desired information. Within VERBMOBIL, a more knowledge-rich approach is used (Alexandersson and Poller 1998; Reithinger et al. 2000). The domain here is travel planning and negotiation of a trip. In addition to finite-state transducers for content extrac"
J02-4003,C00-2140,1,0.920407,"Missing"
J02-4003,A00-2025,1,0.805722,"Missing"
J02-4003,J02-4001,0,\N,Missing
J02-4003,E99-1011,0,\N,Missing
J02-4003,P98-1009,0,\N,Missing
J02-4003,C98-1009,0,\N,Missing
N09-1050,N06-1028,1,0.54384,"cores computed on features by machine are found to have good agreement with scores provided by humans. A limited number of studies have been conducted on assessing speaking proficiency based on spontaneous speech. Moustroufas and Digalakis (Moustroufas and Digalakis, 2007) designed a system to automatically evaluate the pronunciation of foreign speakers using unknown text. The difference in the recognition results between a recognizer trained on speakers’ native languages (L1) and another recognizer trained on their learned languages (L2) was used for pronunciation scoring. Zechner and Bejar (Zechner and Bejar, 2006) presented a system to score non-native spontaneous speech using features derived from the recognition results. Following their work, an operational assessment system, TM SpeechRater , was implemented with further improvements (Zechner et al., 2007). There are some issues with the method to extract pronunciation features in the previous research on automated assessment of spontaneous speech (Zechner and Bejar, 2006; Zechner et al., 2007). For example, the acoustic model (AM) that was used to estimate a likelihood of a phoneme being spoken was well-fitted to non-native speech acoustic propertie"
N12-1011,P11-1073,1,0.396109,"anini,kzechner}@ets.org Abstract sentences verbatim (Cucchiarini et al., 1997; Bernstein et al., 2000; Cucchiarini et al., 2000; Witt and Young, 2000; Franco et al., 2000; Bernstein et al., 2010b), much less has been done about the scoring of spontaneous speech. For automated scoring of unrestricted, spontaneous speech, most automated systems have estimated the non-native speakers’ speaking proficiency primarily based on lowlevel speaking-related features, such as pronunciation, intonation, rhythm, rate of speech, and fluency (Cucchiarini et al., 2002; Zechner et al., 2007; Chen et al., 2009; Chen and Zechner, 2011a), although a few recent studies have explored features based on vocabulary and grammatical complexity (Zechner et al., 2007; Bernstein et al., 2010a; Bernstein et al., 2010b; Chen and Zechner, 2011b). Most previous research on automated speech scoring has focused on restricted, predictable speech. For automated scoring of unrestricted spontaneous speech, speech proficiency has been evaluated primarily on aspects of pronunciation, fluency, vocabulary and language usage but not on aspects of content and topicality. In this paper, we explore features representing the accuracy of the content of"
N12-1011,N09-1050,1,0.381274,"8541, USA {sxie,kevanini,kzechner}@ets.org Abstract sentences verbatim (Cucchiarini et al., 1997; Bernstein et al., 2000; Cucchiarini et al., 2000; Witt and Young, 2000; Franco et al., 2000; Bernstein et al., 2010b), much less has been done about the scoring of spontaneous speech. For automated scoring of unrestricted, spontaneous speech, most automated systems have estimated the non-native speakers’ speaking proficiency primarily based on lowlevel speaking-related features, such as pronunciation, intonation, rhythm, rate of speech, and fluency (Cucchiarini et al., 2002; Zechner et al., 2007; Chen et al., 2009; Chen and Zechner, 2011a), although a few recent studies have explored features based on vocabulary and grammatical complexity (Zechner et al., 2007; Bernstein et al., 2010a; Bernstein et al., 2010b; Chen and Zechner, 2011b). Most previous research on automated speech scoring has focused on restricted, predictable speech. For automated scoring of unrestricted spontaneous speech, speech proficiency has been evaluated primarily on aspects of pronunciation, fluency, vocabulary and language usage but not on aspects of content and topicality. In this paper, we explore features representing the acc"
N12-1011,W08-0912,1,0.94036,"uring the overall organization of an essay assumed a writing strategy that included an introductory paragraph, at least a three-paragraph body with each paragraph in the body consisting of a pair of main point, supporting idea elements, and a concluding paragraph. In another approach, the content of written essays were evaluated using LSA by comparing the test essays with essays of known quality in regard of their degree of conceptual relevance and the amount of relevant content (Foltz et al., 1999). There has been less work measuring spoken responses in terms of the higher level aspects. In (Zechner and Xi, 2008), the authors used a content feature together with other features related to vocabulary, pronunciation and fluency to build an automated scoring system for spontaneous high-entropy responses. This content feature was the cosine word vector product between a test response and the training responses which have the highest human score. The experimental results showed that this feature did not provide any further contribution above a baseline of only using non-content features, and for some tasks the system performance was even slightly worse after including this feature. However, we think the obs"
N13-1101,N04-1015,0,0.0487374,"mportant criterion in the human scoring rubrics for speaking assessments is the evaluation of coherence, which refers to the conceptual relations between different units within a response. Methods for automatically assessing discourse coherence in text documents have been widely studied in the context of applications such as natural language generation, document summarization, and assessment of text readability. For example, Foltz et al. (1998) measured the overall coherence of a text by utilizing Latent Semantic Analysis (LSA) to calculate the semantic relatedness between adjacent sentences. Barzilay and Lee (2004) introduced an HMM-based model for the document-level analysis of topics and topic transitions. Barzilay and Lapata (2005; 2008) presented an approach to coherence modeling which focused on the entities in the text and their grammatical transitions between adjacent sentences, and calculated the entity transition probabilities on the document level. Pitler et al. (2010) provided a summary of the performance of several different types of features for automated coherence evaluation, such as cohesive devices, adjacent sentence similarity, Coh-Metrix (Graesser et al., 2004), word cooccurrence patte"
N13-1101,P05-1018,0,0.0333252,"o the conceptual relations between different units within a response. Methods for automatically assessing discourse coherence in text documents have been widely studied in the context of applications such as natural language generation, document summarization, and assessment of text readability. For example, Foltz et al. (1998) measured the overall coherence of a text by utilizing Latent Semantic Analysis (LSA) to calculate the semantic relatedness between adjacent sentences. Barzilay and Lee (2004) introduced an HMM-based model for the document-level analysis of topics and topic transitions. Barzilay and Lapata (2005; 2008) presented an approach to coherence modeling which focused on the entities in the text and their grammatical transitions between adjacent sentences, and calculated the entity transition probabilities on the document level. Pitler et al. (2010) provided a summary of the performance of several different types of features for automated coherence evaluation, such as cohesive devices, adjacent sentence similarity, Coh-Metrix (Graesser et al., 2004), word cooccurrence patterns, and entity-grid. In addition to studies on well-formed text, researchers have also addressed coherence modeling on t"
N13-1101,J08-1001,0,0.031047,"un 4-fold crossvalidation on the 600 annotated responses. The correlation coefficient (r) and the weighted average F-Measure 2 are used as evaluation metrics. In this experiment, we examine the performance of the entity-grid features and a set of features produced by the e-rater® system (an automated writing assessment system for learner essays) (Attali and Burstein, 2006) to predict the coherence scores of the spontaneous spoken responses, where all the features are extracted from human transcriptions of the responses. 2.4 Entity Grid and e-rater Features First, we applied the algorithm from Barzilay and Lapata (2008) to extract entity-grid features, which calculated the vector of entity transition probabilities across adjacent sentences. Several different methods of representing the entities can be used before generating the entity-grid. First, all the entities can be described by their syntactic roles including S (Subject), O (Object), and X (Other). Alternatively, these roles can also be reduced to P (Present) or N (Absent). Furthermore, entities can be defined as salient, when they appear two or more times, otherwise as non-salient. In this study, 2 The data distribution in the experimental corpus is u"
N13-1101,N10-1099,0,0.0815943,"and evaluating the coherence prediction features, the spoken responses selected for this study were balanced based on the human scores as follows: 25 responses were selected randomly from each of the 4 score points (1 - 4) for each of the 6 test questions. In some cases, more than one response was selected from a given test-taker; in total, 471 distinct test-takers are represented in the data set. 2.2 Annotation and Analysis The coherence annotation guidelines used for the spoken responses in this study were modified based on the annotation guidelines developed for written essays described in Burstein et al. (2010). According to these guidelines, expert annotators provided each response with a score on a scale of 1 - 3. The three score points were defined as follows: 3 = highly coherent (contains no instances of confusing arguments or examples), 2 = somewhat coherent (contains some awkward points in which the speaker&apos;s line of argument is unclear), 1 = barely coherent (the entire response was confusing and hard to follow; it was intuitively incoherent as a whole and the annotators had difficulties in identifying specific weak points). For responses receiving a coherence score of 2, the annotators were r"
N13-1101,P11-1073,1,0.673929,"rch has been conducted into developing automated assessment systems to automatically score spontaneous speech from nonnative speakers with the goals of reducing the burden on human raters, improving reliability, and generating feedback that can be used by language learners. Various features related to different aspects of speaking proficiency have been exploited, such as delivery features for pronunciation, prosody, and fluency (Strik and Cucchiarini, 1999; Chen et al., 2009; Cheng, 2011; Higgins et al., 2011), as well as language use features for vocabulary and grammar, and content features (Chen and Zechner, 2011; Xie et al., 2012). However, discourse-level features related to topic development have rarely been investigated in the context of automated speech scoring. This is despite the fact that an important criterion in the human scoring rubrics for speaking assessments is the evaluation of coherence, which refers to the conceptual relations between different units within a response. Methods for automatically assessing discourse coherence in text documents have been widely studied in the context of applications such as natural language generation, document summarization, and assessment of text reada"
N13-1101,N04-1024,0,0.0393273,"tween adjacent sentences, and calculated the entity transition probabilities on the document level. Pitler et al. (2010) provided a summary of the performance of several different types of features for automated coherence evaluation, such as cohesive devices, adjacent sentence similarity, Coh-Metrix (Graesser et al., 2004), word cooccurrence patterns, and entity-grid. In addition to studies on well-formed text, researchers have also addressed coherence modeling on text produced by language learners, which may contain many spelling and grammar errors. Utilizing LSA and Random Indexing methods, Higgins et al. (2004) measured the global 814 Proceedings of NAACL-HLT 2013, pages 814–819, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics coherence of students’ essays by calculating the semantic relatedness between sentences and the corresponding prompts. In addition, Burstein et. al (2010) combined entity-grid features with writing quality features produced by an automated assessment system of essays to predict the coherence scores of student essays. Recently, Yannakoudakis and Briscoe (2012) systematically analyzed a variety of coherence modeling methods within the framework"
N13-1101,P10-1056,0,0.0229389,"tion, and assessment of text readability. For example, Foltz et al. (1998) measured the overall coherence of a text by utilizing Latent Semantic Analysis (LSA) to calculate the semantic relatedness between adjacent sentences. Barzilay and Lee (2004) introduced an HMM-based model for the document-level analysis of topics and topic transitions. Barzilay and Lapata (2005; 2008) presented an approach to coherence modeling which focused on the entities in the text and their grammatical transitions between adjacent sentences, and calculated the entity transition probabilities on the document level. Pitler et al. (2010) provided a summary of the performance of several different types of features for automated coherence evaluation, such as cohesive devices, adjacent sentence similarity, Coh-Metrix (Graesser et al., 2004), word cooccurrence patterns, and entity-grid. In addition to studies on well-formed text, researchers have also addressed coherence modeling on text produced by language learners, which may contain many spelling and grammar errors. Utilizing LSA and Random Indexing methods, Higgins et al. (2004) measured the global 814 Proceedings of NAACL-HLT 2013, pages 814–819, c Atlanta, Georgia, 9–14 Jun"
N13-1101,N12-1011,1,0.839786,"nto developing automated assessment systems to automatically score spontaneous speech from nonnative speakers with the goals of reducing the burden on human raters, improving reliability, and generating feedback that can be used by language learners. Various features related to different aspects of speaking proficiency have been exploited, such as delivery features for pronunciation, prosody, and fluency (Strik and Cucchiarini, 1999; Chen et al., 2009; Cheng, 2011; Higgins et al., 2011), as well as language use features for vocabulary and grammar, and content features (Chen and Zechner, 2011; Xie et al., 2012). However, discourse-level features related to topic development have rarely been investigated in the context of automated speech scoring. This is despite the fact that an important criterion in the human scoring rubrics for speaking assessments is the evaluation of coherence, which refers to the conceptual relations between different units within a response. Methods for automatically assessing discourse coherence in text documents have been widely studied in the context of applications such as natural language generation, document summarization, and assessment of text readability. For example"
N13-1101,W12-2004,0,0.0900653,"Missing"
N13-1101,N09-1050,1,\N,Missing
N18-3008,W10-1013,0,0.0349693,"peech processing and NLP techniques have also been used to detect other types of non-scorable responses: language identification technology for non-English detection (Yoon and Higgins, 2011) and speaker recognition technology for automated impostor detection (Qian et al., 2016). “Banging on the keyboard” can be identified by analyzing part-ofspeech sequences and looking for ill-formed sequences (Higgins et al., 2006). have been developed based on question-specific content models, such as a standard vector space model (VSM) built for each question (Bernstein et al., 2000; Higgins et al., 2006; Louis and Higgins, 2010). For speaking tests eliciting highly or moderately restricted speech, filtering models based on features derived from ASR systems such as normalized confidence scores and language model (LM) scores can achieve good performance in identifying topic-related non-scorable responses (van Doremalen et al., 2009; Lo et al., 2010; Cheng and Shen, 2011). However, this approach is not appropriate for a speaking test that elicits unconstrained spontaneous speech. More recently, similar to techniques that have been applied in essay scoring, systems based on document similarity measures and topic detectio"
N18-3008,W11-1420,1,0.808429,"overall architecture of a generic automated scoring pipeline. Above the dotted line are the key stages in automated scoring. Below the dotted line are the possible additions to the pipeline to handle atypical inputs using filtering models (FMs). task on plagiarism detection. Wang et al. (2016) developed a spoken canned response detection system using similar techniques applied in essay plagiarism detection. In addition, various speech processing and NLP techniques have also been used to detect other types of non-scorable responses: language identification technology for non-English detection (Yoon and Higgins, 2011) and speaker recognition technology for automated impostor detection (Qian et al., 2016). “Banging on the keyboard” can be identified by analyzing part-ofspeech sequences and looking for ill-formed sequences (Higgins et al., 2006). have been developed based on question-specific content models, such as a standard vector space model (VSM) built for each question (Bernstein et al., 2000; Higgins et al., 2006; Louis and Higgins, 2010). For speaking tests eliciting highly or moderately restricted speech, filtering models based on features derived from ASR systems such as normalized confidence score"
P11-1073,W10-1010,0,0.244133,"et al. (2007). The speaking output is usually less clean than writing data (e.g., considering disfluencies such as false starts, repetitions, filled pauses etc.). There724 fore we may need to remove these disfluencies first before computing syntactic complexity features. Also, importantly, ASR output does not contain interpunctuation but both for sentential-based features as well as for parser-based features, the boundaries of clauses and sentences need to be known. For this purpose, we will use automated classifiers that are trained to predict clause and sentence boundaries, as described in Chen et al. (2010). With previous studies providing us a rich pool of complexity features, additionally we also develop features analogous to the ones from the literature, mostly by using different calculation methods. For instance, the frequency of Prepositional Phrases (PPs) is a feature from the literature, and we add some variants such as number of PPs per clause as a new feature to our extended feature set. 2.2 Devising the Initial Feature Set Through this literature review, we identified some important features that were frequently used in previous studies in both L2 speaking and writing, such as length o"
P11-1073,W08-0900,0,0.190527,"Missing"
P11-1073,levy-andrew-2006-tregex,0,0.0122418,"lauses per clause”, etc. Parse tree based features refer to features that are generated from parse trees and cannot be extracted from human annotated clauses directly. We first selected features showing high correlation to human assigned scores. In this process the CB features were computed from human labeled clause boundaries in transcripts for best accuracy, and PT features were calculated from using parsing and other tools because we did not have human parse tree annotations for our data. We used the Stanford Parser (Klein and Manning, 2003) in conjunction with the Stanford Tregex package (Levy and Andrew, 2006) which supports using rules to extract specific configurations from parse trees, in a package put together by Lu (Lu, 2011). When given a sentence, the Stanford Parser outputs its grammatical structure by grouping words (and phrases) in a tree structure and identifies grammatical roles of words and phrases. Tregex is a tree query tool that takes Stanford parser trees as input and queries the trees to find subtrees that meet specific rules written in Tregex syntax (Levy and Andrew, 2006). It uses relational operators regulated by Tregex, for example, “A << B” stands for “subtree A dominates sub"
P17-2041,W01-1605,0,0.730346,"used in automated discourse analysis and coherence measurement for non-native spoken responses, thereby improving the validity of the automated scoring systems. RST is a descriptive framework that has been widely used in the analysis of discourse organization of written texts (Taboada and Mann, 2006b), and has been applied to various natural language processing tasks, including language generation, text summarization, and machine translation (Taboada and Mann, 2006a). In particular, the availability of RST annotations on a selection of 385 Wall Street Journal articles from the Penn Treebank1 (Carlson et al., 2001) has facilitated RST-based discourse analysis of written texts, since it provides a standard benchmark for comparing the performance of different techniques for document-level discourse parsing (Joty et al., 2013; Feng and Hirst, 2014). Another important application of RST closely related to our research is the automated evaluation of discourse in student essays. For example, one study used features for each sentence in an essay to reflect the status of its parent node as well as its rhetorical relation based on automatically parsed RST trees with the goal of providing feedback to students abo"
P17-2041,W14-4916,0,0.193076,"monstrated the effectiveness of deep discourse structure in better differentiation of text coherence (Feng et al., 2014). Related work has also been conducted to annotate discourse relations in spoken language, which is produced and processed differently from written texts (Rehbein et al., 2016), and often lacks explicit discourse connectives that are more frequent in written language. Instead of the rootedtree structure that is employed in RST, the annotation scheme with shallow discourse structure and relations from the PDTB (Prasad et al., 2008) has been generally used for spoken language (Demirahin and Zeyrek, 2014; Stoyanchev and Bangalore, 2015). For example, Tonelli et al. adapted the PDTB annotation scheme to annotate discourse relations in spontaneous conversations in Italian (Tonelli et al., 2010) and Rehbein et al. compared two frameworks, PDTB and CCR (Cognitive approach to Coherence Relations) (Sanders et al., 1992), for the annotation of discourse relations in spoken language (Rehbein et al., 2016). In contrast to these previous studies, this study focuses on monologic spoken responses produced by non-native speakers within the context of a language proficiency assessment. A discourse annotati"
P17-2041,P14-1048,0,0.15192,"f discourse organization of written texts (Taboada and Mann, 2006b), and has been applied to various natural language processing tasks, including language generation, text summarization, and machine translation (Taboada and Mann, 2006a). In particular, the availability of RST annotations on a selection of 385 Wall Street Journal articles from the Penn Treebank1 (Carlson et al., 2001) has facilitated RST-based discourse analysis of written texts, since it provides a standard benchmark for comparing the performance of different techniques for document-level discourse parsing (Joty et al., 2013; Feng and Hirst, 2014). Another important application of RST closely related to our research is the automated evaluation of discourse in student essays. For example, one study used features for each sentence in an essay to reflect the status of its parent node as well as its rhetorical relation based on automatically parsed RST trees with the goal of providing feedback to students about the discourse structure in the essay (Burstein et al., 2003). Another study compared The availability of the Rhetorical Structure Theory (RST) Discourse Treebank has spurred substantial research into discourse analysis of written te"
P17-2041,tonelli-etal-2010-annotation,0,0.18225,"ken language, which is produced and processed differently from written texts (Rehbein et al., 2016), and often lacks explicit discourse connectives that are more frequent in written language. Instead of the rootedtree structure that is employed in RST, the annotation scheme with shallow discourse structure and relations from the PDTB (Prasad et al., 2008) has been generally used for spoken language (Demirahin and Zeyrek, 2014; Stoyanchev and Bangalore, 2015). For example, Tonelli et al. adapted the PDTB annotation scheme to annotate discourse relations in spontaneous conversations in Italian (Tonelli et al., 2010) and Rehbein et al. compared two frameworks, PDTB and CCR (Cognitive approach to Coherence Relations) (Sanders et al., 1992), for the annotation of discourse relations in spoken language (Rehbein et al., 2016). In contrast to these previous studies, this study focuses on monologic spoken responses produced by non-native speakers within the context of a language proficiency assessment. A discourse annotation scheme based on the RST framework was selected due to the fact that it can effectively demonstrate the deep hierarchical discourse structure across an entire response, rather than focusing"
P17-2041,C14-1089,0,0.119425,"g passage (N = 400 responses). The spoken responses were all manually transcribed using standard punctuation and capitalization. The average number of words per response is 104.4 (st. dev. = 34.4) and the average number of sentences is 5.5 (st. dev. = 2.1). features derived from deep hierarchical discourse relations based on RST parsing and features derived from shallow discourse relations based on Penn Discourse Treebank (PDTB) (Prasad et al., 2008) parsing in the task of essay scoring and demonstrated the effectiveness of deep discourse structure in better differentiation of text coherence (Feng et al., 2014). Related work has also been conducted to annotate discourse relations in spoken language, which is produced and processed differently from written texts (Rehbein et al., 2016), and often lacks explicit discourse connectives that are more frequent in written language. Instead of the rootedtree structure that is employed in RST, the annotation scheme with shallow discourse structure and relations from the PDTB (Prasad et al., 2008) has been generally used for spoken language (Demirahin and Zeyrek, 2014; Stoyanchev and Bangalore, 2015). For example, Tonelli et al. adapted the PDTB annotation sch"
P17-2041,N13-1101,1,0.701883,"ress the following three main aspects of speaking proficiency: delivery (pronunciation, fluency, prosody), language use (grammar and lexical choice), and topic development (content and coherence). In order to ensure a sufficient quantity of responses from each proficiency level, 25 responses were selected randomly from each of the 4 score points for each of the 6 test questions. The current study builds on a previous study that investigated approaches for modeling discourse coherence in non-native spontaneous speech (but which did not consider the hierarchical rhetorical structure of speech) (Wang et al., 2013). In that study, each spoken response in the same corpus that was used for the current study was provided with global discourse coherence scores. Two expert annotators (not drawn from the pool of expert human raters who provided the holistic scores) provided each response with a score on a scale of 1 to 3 based on the orthographic transcriptions of the spoken response. The three score points were defined as follows: 3 = highly coherent (contains no instances of confusing arguments or examples), 2 = somewhat coherent (contains some awkward points in which the speaker’s line of argument is uncle"
P17-2041,P13-1048,0,0.153675,"d in the analysis of discourse organization of written texts (Taboada and Mann, 2006b), and has been applied to various natural language processing tasks, including language generation, text summarization, and machine translation (Taboada and Mann, 2006a). In particular, the availability of RST annotations on a selection of 385 Wall Street Journal articles from the Penn Treebank1 (Carlson et al., 2001) has facilitated RST-based discourse analysis of written texts, since it provides a standard benchmark for comparing the performance of different techniques for document-level discourse parsing (Joty et al., 2013; Feng and Hirst, 2014). Another important application of RST closely related to our research is the automated evaluation of discourse in student essays. For example, one study used features for each sentence in an essay to reflect the status of its parent node as well as its rhetorical relation based on automatically parsed RST trees with the goal of providing feedback to students about the discourse structure in the essay (Burstein et al., 2003). Another study compared The availability of the Rhetorical Structure Theory (RST) Discourse Treebank has spurred substantial research into discourse"
P17-2041,W99-0307,0,0.50626,"ency score of 1. the EDU segmentation task, we first converted the segmentation sequences into 0/1 sequences: for each word in a response, 1 is assigned if a segment boundary exists after the word; otherwise, 0 is assigned. The inter-annotator agreement rate on the EDU segmentations of the 32 pilot samples (from stage 2) was κ = 0.876. On the hierarchical tree building task, inter-annotator agreement was evaluated on the levels of Span (assignment of discourse segment), Nuclearity (assignment of nucleus vs. satellite), and Relation (assignment of rhetorical relation) using κ, as described in (Marcu et al., 1999); on the 32 samples, the κ values are 0.861, 0.769, and 0.631 for the three levels, respectively. 3.3 Table 1: Human agreement on RST annotations in terms of κ and F1-Measure. Span Nuclearity Relation κ 0.848 0.766 0.653 F1-Measure 0.872 0.724 0.522 Table 2: The average number of awkward relations appearing in responses from each of the four proficiency score levels. 1 2 3 4 Annotator 1 3.2 1.1 1.1 0.3 Annotator 2 2.1 1.2 0.7 0.3 Formal Annotation arity, and Relation are 0.848, 0.766, and 0.653, respectively. Besides the κ evaluation, the standard ways of F1-Measure on three levels of Span, Nu"
P17-2041,prasad-etal-2008-penn,0,0.154078,"ding an opinion based on personal experience (N = 200 responses) and 2) Integrated questions: summarizing or discussing material provided in a reading and/or listening passage (N = 400 responses). The spoken responses were all manually transcribed using standard punctuation and capitalization. The average number of words per response is 104.4 (st. dev. = 34.4) and the average number of sentences is 5.5 (st. dev. = 2.1). features derived from deep hierarchical discourse relations based on RST parsing and features derived from shallow discourse relations based on Penn Discourse Treebank (PDTB) (Prasad et al., 2008) parsing in the task of essay scoring and demonstrated the effectiveness of deep discourse structure in better differentiation of text coherence (Feng et al., 2014). Related work has also been conducted to annotate discourse relations in spoken language, which is produced and processed differently from written texts (Rehbein et al., 2016), and often lacks explicit discourse connectives that are more frequent in written language. Instead of the rootedtree structure that is employed in RST, the annotation scheme with shallow discourse structure and relations from the PDTB (Prasad et al., 2008) h"
P17-2041,L16-1165,0,0.0940264,"104.4 (st. dev. = 34.4) and the average number of sentences is 5.5 (st. dev. = 2.1). features derived from deep hierarchical discourse relations based on RST parsing and features derived from shallow discourse relations based on Penn Discourse Treebank (PDTB) (Prasad et al., 2008) parsing in the task of essay scoring and demonstrated the effectiveness of deep discourse structure in better differentiation of text coherence (Feng et al., 2014). Related work has also been conducted to annotate discourse relations in spoken language, which is produced and processed differently from written texts (Rehbein et al., 2016), and often lacks explicit discourse connectives that are more frequent in written language. Instead of the rootedtree structure that is employed in RST, the annotation scheme with shallow discourse structure and relations from the PDTB (Prasad et al., 2008) has been generally used for spoken language (Demirahin and Zeyrek, 2014; Stoyanchev and Bangalore, 2015). For example, Tonelli et al. adapted the PDTB annotation scheme to annotate discourse relations in spontaneous conversations in Italian (Tonelli et al., 2010) and Rehbein et al. compared two frameworks, PDTB and CCR (Cognitive approach"
P98-2236,A97-1052,0,0.0173993,"currently underway in building WordNet resources for other European languages. • a mapper which tries to find the &quot;best match&quot; between the chunks found within a short clause and the subcat frames for the main verb in that clause 4 The major blocks of the system architecture are depicted in Figure I. We want to stress here that except for the development of the small POS grammar and the framemapper, the other components and resources were already present or quite simple to implement. There has also been significant work on (semi-)automatic induction of subcategorization frames (Manning, 1993; Briscoe and Carroll, 1997), such that even 3More details about the chunk parser can be found in (Zechner, 1997). 1449 Preliminary Experiments We performed some initial experiments using the SWBD transcripts as input to the system. These were POS tagged, preprocessed, segmented into short clauses, parsed in chunks using a POS based grammar, and finally, for each short clause, the frame-mapper matched all potential arguments of the verb against all possible subcategorization frames listed in the lemmata file we had precomputed from WordNet (see section 2). In total we had over 600000 short clauses, containing approximate"
P98-2236,W97-0800,0,0.277717,"Missing"
P98-2236,P93-1032,0,\N,Missing
P98-2236,W97-0801,0,\N,Missing
W03-0315,P91-1022,0,0.0683844,"ter-subject agreement score correlations. Pearson's correlation ranges from 0.53 up to 0.72 in our experiments. 1 Introduction In many instances, multilingual natural language systems like machine translation systems are developed and trained on parallel corpora. When faced with a different, unseen text genre, however, translation performance usually drops noticeably. One way to remedy this situation is to adapt and retrain the system parameters based on bilingual data from the same source or at least a closely related source. A bilingual sentence alignment program (Gale and Church, 1991, and Brown et al., 1991) is the crucial part in this adaptation procedure, in that it collects bilingual document pairs from the Internet, and identifies sentence pairs, which should have a high likelihood of being correct translations of each other. The set of identified bilingual parallel sentence vogel+@cs.cmu.edu ahw@cs.cmu.edu pairs is then added to the training set for parameter reestimation. As is well known, text mined from the Internet is very noisy. Even after careful html parsing and filtering for text size and language, the text from comparable html-page pairs still contains mismatches of content or non-p"
W03-0315,P93-1002,0,0.0803175,"Missing"
W03-0315,P91-1023,0,0.0549414,"s is in the range of the inter-subject agreement score correlations. Pearson's correlation ranges from 0.53 up to 0.72 in our experiments. 1 Introduction In many instances, multilingual natural language systems like machine translation systems are developed and trained on parallel corpora. When faced with a different, unseen text genre, however, translation performance usually drops noticeably. One way to remedy this situation is to adapt and retrain the system parameters based on bilingual data from the same source or at least a closely related source. A bilingual sentence alignment program (Gale and Church, 1991, and Brown et al., 1991) is the crucial part in this adaptation procedure, in that it collects bilingual document pairs from the Internet, and identifies sentence pairs, which should have a high likelihood of being correct translations of each other. The set of identified bilingual parallel sentence vogel+@cs.cmu.edu ahw@cs.cmu.edu pairs is then added to the training set for parameter reestimation. As is well known, text mined from the Internet is very noisy. Even after careful html parsing and filtering for text size and language, the text from comparable html-page pairs still contains misma"
W03-0315,W96-0201,0,0.0463513,"Missing"
W03-0315,J93-2003,0,0.0423788,"Missing"
W03-0315,P99-1068,0,0.0370012,"ty. We describe alignment experiments in section 5, focusing on the correlation between the alignment scores predicted by the sentence alignment models and by humans. Conclusions are given in section 6. 2 System of Mining Parallel Text One crucial component of statistical machine translation (SMT) system is the parallel text mining from Internet. Several processing modules are applied to collect, extract, convert, and clean the text from Internet. The components in our system include: • A web crawler, which collects potential parallel html documents based on link information following (Philip Resnik 1999); • A bilingual html parser (based on flex for efficiency), which is designed for both Chinese and English html documents. The paragraphs’ boundaries within the html structure are kept. • A character encoding detector, which judges if the Chinese html document is GB2312 encoding or BIG5 encoding. • An encoding converter, which converts the BIG5 documents to GB2312 encoding. • A language identifier to ensure that source and target documents are both of the proper language. (Noord’s Implementation). • A Chinese word segmenter, which parses the Chinese strings into Chinese words. • A document ali"
W03-0315,J93-1004,0,\N,Missing
W03-0315,P93-1001,0,\N,Missing
W03-0315,1999.mtsummit-1.79,0,\N,Missing
W08-0912,N06-1028,1,0.85119,"this system and its acoustic model and language model adaptation; the speech features computed based on the recognition output; and finally the scoring models based on multiple regression and classification trees. For both tasks, agreement measures between machine and human scores (correlation, kappa) are close to or reach inter-human agreements. 1 Introduction As demand for spoken language testing and cost of human scoring have increased in recent years, there is a growing interest in building both research and industrial systems for automatically scoring non-native speech (Bernstein, 1999, Zechner and Bejar, 2006, Zechner et al, 2007). However, past approaches have focused typically only on one type of spoken language, or on a range of types similar in linguistic entropy. Entropy in this context can be seen as a measure for how predictable the language in the expected spoken response is: Some tests, such as SET-10 (Bernstein 1999), are focused mostly on the lower entropy aspects of language, using tasks such as “reading” or “repetition”, where the expected sequence of words is highly predictable. Other assessments, such as the TOEFL® Practice Online Speaking test, on the other hand, focus on more spon"
W10-0708,D09-1030,0,0.0330079,"o its ability to provide multiple sources of information for a given task in a cost-effective way, several recent studies have combined multiple MTurk outputs for NLP annotation tasks. For example, one study involving annotation of emotions in text used average scores from up to 10 turkers to show the minimum number of MTurk annotations required to achieve performance comparable to experts (Snow et al., 2008). Another study used preference voting to combine up to 5 MTurk rankings of machine translation quality and showed that the resulting judgments approached expert interannotator agreement (Callison-Burch, 2009). These 53 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 53–56, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tasks, however, are much simpler than transcription. MTurk has been used extensively as a transcription provider, as is apparent from the success of a middleman site that act as an interface to MTurk for transcription tasks.1 However, to our knowledge, only one previous study has systematically evaluated the quality of MTurk transcriptions (Marge et al., to appear). This recent"
W10-0708,N10-1024,0,0.214945,"Missing"
W10-0708,D08-1027,0,0.0408943,"Missing"
W12-2010,W08-0912,1,0.77582,"ing process by normalization and regression analysis (Landauer et al., 2003). The IntelliMetric system uses a nonlinear and multidimensional modeling approach to reflect the complexity of the writing process as opposed to the general linear model (Dikli, 2006). Larkey and Croft (2003) employ Bayesian classifiers for modeling, which is a type of text categorization technique. It treats essay scoring as a text categorization task, the purpose of which is to classify essays into score categories based on content features (i.e., if the scores range from 1-4, then there are four score categories). Zechner and Xi (2008) report on experiments related to scoring of spontaneous speech responses where content vector analysis was used as one of several features in scoring models for two different item types. They found that while these content features performed reasonably well by themselves, they were not able to increase the overall scoring model performance over a baseline that did not use content features. This paper will use CVA as a baseline for our experiment and investigate two ontology-based approaches to enhance the content representation and improve content feature performance. 3 Data We use data from"
W12-2010,N04-3012,0,\N,Missing
W12-2021,W11-1405,1,0.783819,"ng system of spontaneous speech responses by non-native English speakers. Focusing on vocabulary sophistication, we estimate the difficulty of each word in the vocabulary based on its frequency in a reference corpus and assess the mean difficulty level of the vocabulary usage across the responses (vocabulary profile). A majority of the studies in automated speech scoring have focused on fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2011). More recently, Chen and Yoon (2011) and Chen and Zechner (2011) have measured syntactic competence in speech scoring. However, only a few have explored features related to vocabulary usage and they have been limited to type-token ratio (TTR) related features (e.g., Lu (2011)). In addition, Bernstein et al. (2010) developed vocabulary features that measure the similarity between the vocabulary in the test responses and the vocabulary in the pre-collected texts in the same topic. However, their features assessed content and topicality, not vocabulary usage. Three different classes of features were generated based on the words in"
W12-2021,P11-1073,1,0.505876,"speech responses by non-native English speakers. Focusing on vocabulary sophistication, we estimate the difficulty of each word in the vocabulary based on its frequency in a reference corpus and assess the mean difficulty level of the vocabulary usage across the responses (vocabulary profile). A majority of the studies in automated speech scoring have focused on fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2011). More recently, Chen and Yoon (2011) and Chen and Zechner (2011) have measured syntactic competence in speech scoring. However, only a few have explored features related to vocabulary usage and they have been limited to type-token ratio (TTR) related features (e.g., Lu (2011)). In addition, Bernstein et al. (2010) developed vocabulary features that measure the similarity between the vocabulary in the test responses and the vocabulary in the pre-collected texts in the same topic. However, their features assessed content and topicality, not vocabulary usage. Three different classes of features were generated based on the words in a spoken response: coverage-"
W12-2021,N12-1011,1,\N,Missing
W13-1709,P11-1073,1,0.75464,"9) used Latent Semantic Analysis (LSA) to score students’ answers by comparing them to domain-representative texts. Since LSA is based on the bag-of-words model, researchers have also tried to expand it by introducing additional information, such as part-of-speech (POS) tags (Kanejiya et al., 2003). In addition, research efforts have also been made to evaluate the content relatedness and correctness for spoken responses. For example, Xie et al. (2012) used LSA and Pairwise Mutual Information approaches to evaluate the content correctness of unrestricted spontaneous spoken responses. Moreover, Chen and Zechner (2011) explored features related to grammatical complexity in an automated speech scoring system. In order to address the moderately predictable speaking test items in the new ETLA, this paper presents several different types of features to score the content correctness of the elicited spoken responses. Following a series of experiments and comparisons, seven features from three content feature categories are selected and evaluated. 3 Data Sets and ASR System This study conducts experiments and evaluations based on two different data sets: (1) a small scale in-house data collection effort, which was"
W13-1709,E09-1065,0,0.0188484,"ential operational use and then evaluated on the pilot data. The paper is organized as follows: Section 2 provides an overview on related work; Section 3 describes the in-house data set, the pilot data and the ASR system; the developed features are presented in Section 4; Section 5 presents our experiments; we then discuss our findings in Section 6 and we conclude the paper in Section 7. 2 Related Work Related to the automated assessment of writing free-text, research to date has concentrated mainly on two tasks: (1) scoring of short answers (Mitchell et al., 2002; Leacock and Chodorow, 2003; Mohler and Mihalcea, 2009) and (2) scoring of essays (Foltz et al., 1999; Kanejiya et al., 2003; Attali and Burstein, 2006). For example, Leacock and Chodorow (2003) built an automated scoring system, c-rater™, to evaluate the short constructed or free-text responses, where the concepts given in test items were modeled, and the presence of these expected concepts in students’ answers would be detected. As for the evaluation of free-text essays, Attali and Burstein (2006) used a selected set of meaningful features to measure different constructed aspects of writing essays, such as grammar, usage, mechanics, style, organ"
W13-1709,P02-1040,0,0.096358,"ng the words that need to be included in a test response by test takers. Then a feature, num_keywords, can be used to examine how many keywords appear in a test response, which can be further normalized by the number of predefined keywords for each item, i.e., percent_keywords. In addition, as some keywords may be a phrase with multiple words, such as “page 55,” we can split all the keywords into single words and get another sub-keywords list. Then two corresponding features can be extracted as num_sub_keywords and percent_sub_keywords. BIII. Machine Translation Evaluation Metric (BLEU) BLEU (Papineni et al., 2002) is one of the most popular metrics for automatic evaluation of machine translation, where the score is calculated based on the modified n-gram precision. In this study, the BLEU score is introduced to evaluate the content quality of a test response, where three different gold standard reference corpora are extracted from the training set according to each score level. Similar to the edit distance and WER features described below, three BLEU scores are calculated by comparing them with reference responses from each score level (i.e., bleu_1, bleu_2 and bleu_3). We decide to use the following t"
W13-1709,N12-1011,1,0.862881,"chanics, style, organization, development, lexical complexity and prompt-specific vocabulary usage. In addition, the Intelligent Essay Assessor (Foltz et al., 1999) used Latent Semantic Analysis (LSA) to score students’ answers by comparing them to domain-representative texts. Since LSA is based on the bag-of-words model, researchers have also tried to expand it by introducing additional information, such as part-of-speech (POS) tags (Kanejiya et al., 2003). In addition, research efforts have also been made to evaluate the content relatedness and correctness for spoken responses. For example, Xie et al. (2012) used LSA and Pairwise Mutual Information approaches to evaluate the content correctness of unrestricted spontaneous spoken responses. Moreover, Chen and Zechner (2011) explored features related to grammatical complexity in an automated speech scoring system. In order to address the moderately predictable speaking test items in the new ETLA, this paper presents several different types of features to score the content correctness of the elicited spoken responses. Following a series of experiments and comparisons, seven features from three content feature categories are selected and evaluated. 3"
W13-1709,W03-0208,0,\N,Missing
W13-1721,W12-2010,1,0.664914,"equire a large amount of actual responses to train the models. In this paper, we propose such a method which uses the stimulus materials for each prompt contained in the assessment to evaluate the content in a spoken response. 157 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 157–162, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics 2 3 Related Work There has been little prior work concerning automated content scoring for spontaneous spoken responses (a few recent studies include (Xie et al., 2012) and (Chen and Zechner, 2012)); however, several approaches have been investigated for written responses. A standard approach for extended written responses (e.g., essays) is to compare the content in a given essay to the content in essays that have been provided with scores by human raters using similarity methods such as Content Vector Analysis (Attali and Burstein, 2006) and Latent Semantic Analysis (Foltz et al., 1999). This method thus requires a relatively large set of pre-scored responses for each test question in order to train the content models. For shorter written responses (e.g., short answer questions targeti"
W13-1721,N07-2013,0,0.653765,"extended written responses, it is less practical to make comparisons with model responses, due to the greater length and variability of the responses. However, another approach that does not require pre-scored responses is possible for test questions that have prompts with substantial amounts of information that should be included in the answer. In these cases, the similarity between the response and the prompt materials can be calculated, with the hypothesis that higher scoring responses will incorporate certain prompt materials more than lower scoring responses. This approach was taken by (Gurevich and Deane, 2007) which demonstrated that lower proficiency non-native essay writers tend to use more content from the reading passage, which is visually accessible and thus easier to comprehend, than the listening passage. The current study investigates a similar approach for spoken responses. 158 Data The data used in this study was drawn from TOEFL iBT, an international assessment of academic English proficiency for non-native speakers. For this study, we focus on a task from the assessment which elicits a 60 second spoken response from the test takers. In their response, the test takers are asked to use in"
W13-1721,E09-1065,0,0.0347817,"Vector Analysis (Attali and Burstein, 2006) and Latent Semantic Analysis (Foltz et al., 1999). This method thus requires a relatively large set of pre-scored responses for each test question in order to train the content models. For shorter written responses (e.g., short answer questions targeting factual content) approaches have been developed that compare the similarity between the content in a given response and a model correct answer, and thus do not necessarily require the collection of pre-scored responses. These approaches range from fully unsupervised text-to-text similarity measures (Mohler and Mihalcea, 2009) to systems that incorporate hand-crafted patterns identifying specific key concepts (Sukkarieh et al., 2004; Mitchell et al., 2002). For extended written responses, it is less practical to make comparisons with model responses, due to the greater length and variability of the responses. However, another approach that does not require pre-scored responses is possible for test questions that have prompts with substantial amounts of information that should be included in the answer. In these cases, the similarity between the response and the prompt materials can be calculated, with the hypothesi"
W13-1721,N12-1011,1,0.523069,"Missing"
W14-1809,N09-1050,1,0.831503,"adjusted ROUGE and the baselines was significant at α = 0.001. In line with previous results, the models based on manual transcriptions showed better agreement with human scores than the models based on ASR output. Manual 0.673 0.589 0.451 Table 4 shows that the addition of content metrics lead to relatively small increase in the performance of the integrated models. This is due to the fact that for most speakers different aspects of proficiency tend to be correlated. For example, more fluent speakers also achieve higher ROUGE scores (the correlation between ROUGE and pronunciation accuracy (Chen et al., 2009) is r = 0.62). As a result, a model which measures only one aspect of performance such as fluency may sometimes reach near optimal performance and adding further predictors leads to a relatively small gain. When interpreting these results, it is important to bear in mind that empirical performance is only Table 3: Correlation coefficients with human scores (Spearman’s ρ) for the entire training partition for the newly adjusted version of ROUGE and the baseline metrics. The table shows the results for metrics computed based on ASR and manual transcriptions. We then trained a standard linear reg"
W14-1809,W12-2010,1,0.844685,"n responses. Furthermore, spoken responses differ in many properties from written ones (Biber et al., 2004) and the validity of existing methods for assessing speech needs to be established before they can be used for operational scoring. Xie et al. (2012) presented experiments using content features on spontaneous-speech data based on vector-space models, latent semantic analysis, as well as point-wise mutual information. Some of these content features showed higher correlations with human scores than features measuring other aspects of speaking proficiency, such as fluency or pronunciation. Chen and Zechner (2012) also used a vector space model for the scoring of spontaneous speech, but extended it by using the ontological information contained in WordNet. Finally, Xiong et al. (2013) used a variety of approaches to capture the content of spontaneous responses from the same corpus that we are investigating in this paper. Approaches varied from computing the overlap between key words in the stimuli and responses to a more traditional vector space model based on content vector analysis. 3 3.1 Data and methodology Description of the corpus The study is based on a corpus of responses collected during the p"
W14-1809,P08-1094,0,0.0253265,"erent types of ROUGE varied between 0.81 for ROUGE -4 and 0.9 for ROUGE -1). Since automated scoring relies on the output of automatic speech recognition, all numerical results reported in the main text of this section are based on ASR output. The tables report the numbers for both ASR and manual transcriptions. Evaluation We computed the Spearman’s rank correlation between the metric and the holistic score assigned by the first rater to identify the best method of computing ROUGE and the optimal number of references. Performance of the metric may be affected by properties of the prompt (cf. (Nenkova and Louis, 2008)), therefore we first analyzed each prompt separately and then selected the variants that achieved the highest performance across all of the prompts. Since correlation coefficients are not normally distributed, we used several nonparametric methods to identify significant differences including non-parametric bootstrapping and non-parametric ANOVAs. These analyses were 4.1 Number and choice of reference responses Number of references. To identify the optimal number of references for each prompt and metrics, we first found Nbest , which had the highest correlation with human scores and then iden"
W14-1809,W12-2601,0,0.325639,"Service (ETS) Princeton, NJ 08541, USA aloukina@ets.org, kzechner@ets.org, lchen@ets.org Abstract rics such as ROUGE (Lin and Rey, 2004), which measures the n-gram overlap between the candidate summary and a set of reference summaries (see also Rankel et al. (2013) for historical background). ROUGE is a recall-oriented metric inspired by its precision-oriented counterpart BLEU, developed to evaluate machine translations (Papineni et al., 2002). Recent research in this area has been focused on identifying the most reliable variants of ROUGE and best practices in the application of the metric (Owczarzak et al., 2012; Rankel et al., 2013). These studies (reviewed in more detail in Section 2.1) showed that less commonly used variants of ROUGE may in fact be more consistent with human judgments, at least in the context of automatic summary evaluation. Beyond the research in automatic summarization systems, ROUGE has also been used to evaluate written summaries in the context of educational assessment. Madnani et al. (2013) showed that one of the variants of ROUGE, in combination with other metrics, performed consistently well for the automated scoring of written responses to summary tasks produced by middle"
W14-1809,P02-1040,0,0.0911241,"er (ASR) with constrained vocabulary (see Evanini and Wang (2013) for further details). To evaluate the effect of the errors that may have been introduced by the ASR system, all responses were ROU GEN = P Countoverlap (grn ) P Count(grn ) S∈RS grn ∈S P S∈RS grn ∈S (1) We used n-grams whereby n was in a range from 1 to 4 (ROUGE 1-4) and a combination of unigrams with skip-bigrams with maximum step of four words (ROUGE - SU 1-4). Finally, we also computed a combined measure ROUGE ALL which is the geometrical mean of ROUGE -1– ROUGE -4, computed by using the same smoothing procedure as for BLEU (Papineni et al., 2002). We used the cosine distance (CVA) between the response and reference summaries as a baseline metric as this metric is commonly used for evaluating document similarity in the context of language assessment. CVA was computed as the cosine distance between candidate responses and the same reference responses as used for the computation of ROUGE. All term frequencies were weighted using tf-idf where tf is the frequency of a term in a given response and idf is the inverse document frequency. idf frequencies were computed based on all of the responses in the corpus. Reference summaries. The refere"
W14-1809,P08-1054,0,0.0271741,"evaluate spoken summaries in the context of language assessment. The performance of automatic summarization systems is routinely evaluated using content met68 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 68–78, c Baltimore, Maryland USA, June 26, 2014. 2014 Association for Computational Linguistics for a review). ROUGE performed reasonably well for the evaluation of text summaries of spoken presentations (Hirohata et al., 2005), but was not correlated with the summary accuracy of summaries of meetings or conversations (although see (Penn and Zhu, 2008)). Most of this work was performed on extractive summaries produced by summarization systems that used multiple summaries to evaluate each system. In this study, we explore the application of ROUGE to the evaluation of abstractive summaries produced by students in a language assessment context with an aim of producing a separate evaluation for each summary. Furthermore, the fact that these are spoken responses adds an extra layer of complexity to the analysis, therefore the results of previous studies cannot directly be applied to this new context. maries, spoken summaries are abstractive and"
W14-1809,P13-2024,0,0.0220697,"Missing"
W14-1809,W04-1013,0,0.0733124,"reported that variants based on unigrams and skip-bigrams (ROUGE - SU 4) or bigrams alone (ROUGE -2) performed best. ROUGE -2 was also identified as the best variant more recently by Owczarzak et al. (2012). Rankel et al. (2013) found that linear combinations of these metrics with ROUGE based on longer n-grams are more accurate in finding significantly different systems. Previous work also explored various methods of text pre-processing prior to the computation of ROUGE , including stemming and the removal of stop-words, neither of which had any substantial effect on the performance of ROUGE (Lin and Rey, 2004; Owczarzak et al., 2012). Owczarzak et al. (2012) reported that the agreement with human judgments was, in fact, higher if the stop-words were retained. All applications discussed so far used ROUGE to evaluate the textual summarization of written texts. There have also been attempts to apply this metric to text summaries of speech data with mixed results (see Nenkova and McKeown (2011) 69 tical difficulties when new items are added to the tests: the computation of content metrics for each new item requires either a manual annotation or a relatively large number of reference responses. ROUGE a"
W14-1809,J13-2002,0,0.0457573,"ne distance between candidate responses and the same reference responses as used for the computation of ROUGE. All term frequencies were weighted using tf-idf where tf is the frequency of a term in a given response and idf is the inverse document frequency. idf frequencies were computed based on all of the responses in the corpus. Reference summaries. The reference summaries were selected from responses with the highest human rater final score (4). This approach is similar to using system outputs as pseudomodels for the evaluation of machine-translation or automatic-summarization systems (cf. Louis and Nenkova (2013)). It has also been successfully applied to the content assessment of written answers by Madnani et al. (2013) who used one randomly selected highly scored summary as a reference summary. Since previous work on summarization evaluation showed that multiple summaries increase the reliability of evaluations (Louis and Nenkova, 2013; Nenkova and McKeown, 2011), we tested 2 see http://www.ets.org/s/toefl junior/pdf/toefl junior comprehensive speaking scoring guides.pdf for the scoring rubrics 71 done using the data from the training partition of the corpus. We then evaluated how well the selected"
W14-1809,N12-1011,1,0.8772,"Missing"
W14-1809,W13-1722,0,0.111313,"Missing"
W14-1809,W01-0100,0,\N,Missing
W14-1809,W13-1707,1,\N,Missing
W14-1816,W11-1419,1,0.725816,"Missing"
W14-1816,W12-2021,1,0.900341,"intensity slope (Hoenig, 2002). In addition to the large majority of applications that elicit restricted speech, a small number of applications have also investigated automated scoring of non-native spontaneous speech, in order to more fully evaluate a speaker’s communicative competence (e.g., (Cucchiarini et al., 2002) and (Zechner et al., 2009)). In these systems, the same types of pronunciation, fluency, and prosody features can be extracted; furthermore, features related to additional aspects of a speaker’s proficiency in the non-native language can be extracted, such as vocabulary usage (Yoon et al., 2012), syntactic complexity (Bernstein et al., 2010a; Chen and Zechner, 2011), and topical content (Xie et al., 2012). As described in Section 1, the domain for the automated speaking assessment investigated in this study is teachers of EFL around the world. Based on the fact that many of the item types are designed to assess the test taker’s ability to productively use English constructions and linguistic units that commonly recur in English teaching environments, several of the item types elicit semi-restricted speech (see Table 1 below for a description of the different item types). These types"
W14-1816,W13-1709,1,0.8515,"tem and the linear regression scoring models. The amount of data and 136 human score distributions in each of these partitions are displayed in Table 2. 5 Methodology 4 The feature computation components of our speech scoring system compute more than 100 features based on a speaker’s response. They belong to the following broad dimensions of speaking proficiency: fluency, pronunciation, prosody, vocabulary usage, grammatical complexity and accuracy, and content accuracy (Zechner et al., 2009; Chen and Yoon, 2012; Chen et al., 2009; Zechner et al., 2011; Yoon et al., 2012; Yoon and Bhat, 2012; Zechner and Wang, 2013). After initial feature generation, we selected a set of about 10 features for each of the 8 item types, based on the following considerations1 (Zechner et al., 2009; Xi et al., 2008): 5.1 System Architecture The automated scoring system used for the teachers’ spoken language assessment consists of the following four components, which are invoked one after the other in a pipeline fashion (ETS SpeechRaterSM , (Zechner et al., 2009; Higgins et al., 2011)): • an automated speech recognizer, generating word hypotheses from input audio recordings of the test takers’ responses • a feature computatio"
W14-1816,N12-1011,1,0.882347,"Missing"
W14-1816,D12-1055,1,0.848659,"uation of the ASR system and the linear regression scoring models. The amount of data and 136 human score distributions in each of these partitions are displayed in Table 2. 5 Methodology 4 The feature computation components of our speech scoring system compute more than 100 features based on a speaker’s response. They belong to the following broad dimensions of speaking proficiency: fluency, pronunciation, prosody, vocabulary usage, grammatical complexity and accuracy, and content accuracy (Zechner et al., 2009; Chen and Yoon, 2012; Chen et al., 2009; Zechner et al., 2011; Yoon et al., 2012; Yoon and Bhat, 2012; Zechner and Wang, 2013). After initial feature generation, we selected a set of about 10 features for each of the 8 item types, based on the following considerations1 (Zechner et al., 2009; Xi et al., 2008): 5.1 System Architecture The automated scoring system used for the teachers’ spoken language assessment consists of the following four components, which are invoked one after the other in a pipeline fashion (ETS SpeechRaterSM , (Zechner et al., 2009; Higgins et al., 2011)): • an automated speech recognizer, generating word hypotheses from input audio recordings of the test takers’ respons"
W14-1816,N09-1050,1,\N,Missing
W14-1816,P11-1073,1,\N,Missing
W15-0602,P98-1032,0,0.0601576,"rical performance. 1 Introduction In this paper we compare different methods of selecting the best feature subset for scoring models used in the context of large-scale language assessments, with a particular look at the assessment of spoken responses produced by test-takers. The basic approach to automatically scoring written or spoken responses is to collect a training corpus of responses that are scored by human raters, use machine learning to estimate a model that maps response features to scores from this corpus , and then use this model to predict scores for unseen responses (Page, 1966; Burstein et al., 1998; Landauer et al., 2003; Eskenazi, 2009; Zechner et al., 2009; Bernstein et al., 2010). While this method is often quite effective in terms of producing scoring models that exhibit good agreement with human raters, it can lend itself to criticism from the educational ∗ Currently at Civis Analytics measurement community if it fails to address certain basic considerations for assessment design and scoring that are common practice in that field. For instance, Ramineni and Williamson (2013) argue that automated scoring not only has to be reliable (i.e., exhibiting a good empirical performance as d"
W15-0602,W14-1802,0,0.0591949,"Missing"
W15-0602,C98-1032,0,\N,Missing
W18-0501,P14-2064,1,0.919766,"performance. To monitor rater performance, testing programs sometimes use previously scored responses that are intermixed with the operational responses. These responses are selected from operational responses to represent exemplar cases of each score level and the scores are further reviewed by multiple raters to ensure their accuracy. In this paper we are examining the effect of using such “exemplar” responses for scoring model training and evaluation in the context of automated speech scoring. In particular, we aim to address the following research questions: In a series of papers, Beigman Klebanov and Beigman (2014; 2009; 2009) studied annotation noise in linguistic data, namely, a situation where some of the data is easy to judge, with clear-cut annotation/classification, whereas some of the data is harder to judge, yielding disagreements among raters. They show that in a binary classification task, the presence of annotation noise (hard to judge cases) in the evaluation data could skew benchmarking, especially in cases of small discrepancies between competing models. They also show that the presence of hard cases in the training data could compromise system performance on easyto-judge test cases, a ph"
W18-0501,W17-4609,1,0.89323,"Missing"
W18-0501,J08-3001,0,0.0316515,"Missing"
W18-0501,W15-0602,1,0.757219,"g on the question type and system used. This model achieved substantially higher performance on the EXEM - 5.2 Size of the training set To further evaluate whether training on a larger number of EXEMPLAR responses may have lead to better performance on the MAIN corpus, we re-trained the models using all responses pooled across the different question types. Such an approach has been previously used in other studies in situations where all types of questions are scored based on the same or similar rubrics and the scoring models do not include any questionspecific features (Higgins et al., 2011; Loukina et al., 2015). A substantial increase in the size of the training set to some extent compensates for loss of information about question-specific patterns. The models were evaluated by question type, as in the rest of this paper. To obtain the learning curves for different training sets, we trained all models using training sets of varying sizes from 1000 responses to the full training partition of a given corpus. For each N other than where N is the length of full corpus we trained models 5 times using 5 randomly sampled training sets. Figure 1 shows the learning curves for different combinations of traini"
W18-0501,P11-1067,0,0.275616,"Missing"
W18-0501,W17-1605,1,0.464752,"Missing"
W18-0501,N15-1152,0,0.384794,"orpus of such test taker responses with scores assigned by trained human raters, considered to be the “gold standard” for both training and evaluation of the automated scoring system (Page, 1966; Attali and Burstein, 2006; Bernstein et al., 2010; Williamson et al., 2012). Human raters follow certain agreed-upon scoring guidelines (“rubrics”) that define the characteristics of a 1 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1–12 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics 2008; Mart´ınez Alonso et al., 2015; Plank et al., 2014). performance. To monitor rater performance, testing programs sometimes use previously scored responses that are intermixed with the operational responses. These responses are selected from operational responses to represent exemplar cases of each score level and the scores are further reviewed by multiple raters to ensure their accuracy. In this paper we are examining the effect of using such “exemplar” responses for scoring model training and evaluation in the context of automated speech scoring. In particular, we aim to address the following research questions: In a ser"
W18-0501,E14-1078,0,0.0271219,"Missing"
W18-0501,W15-0615,0,0.059364,"Missing"
W19-2719,E17-1028,0,0.163631,"Missing"
W19-2719,W01-1605,0,0.192745,"connectives that are more frequent in written language. For example, RST has been used to analyze the semi-structured interviews of Alzheimer’s patients (Paulino and Sierra, 2017; Paulino et al., 2018). Previous Work RST is a descriptive framework that has been widely used in the analysis of the discourse organization of written texts (Taboada and Mann, 2006b) and has also been applied to various natural language processing tasks, including language generation, text summarization, and machine translation (Taboada and Mann, 2006a). In particular, the availability of the RST Discourse Treebank (Carlson et al., 2001), with annotations on a selection of 385 Wall Street Journal articles from the Penn Treebank2 , has facilitated RST-based discourse analysis of written texts, since it provides a standard benchmark for comparing the performance of different parsers. A wide range of techniques have However, the annotation scheme with shallow discourse structure and relations from the PDTB (Prasad et al., 2008) has been generally used for spoken language (Demirsahin and Zeyrek, 2014; Stoyanchev and Bangalore, 2015) instead of the rooted-tree structure that is employed in RST. For example, Tonelli et al. (2010) a"
W19-2719,P14-1002,0,0.019477,"tive spontaneous speech, which were at times unintelligible due to the lack of proficiency and transcription inaccuracy. 4 Automatic Parsing 4.1 Parser Training There has been a variety of research on documentlevel discourse parsing based on the RST Discourse Treebank, and multiple RST parsers are available as open source tools. In this study, since the focus of our research is not to investigate advanced techniques to improve the state-of-art in parsing, we employed a pre-existing open-source parser from Heilman and Sagae (2015)4 , which was implemented following the work of Sagae (2009) and Ji and Eisenstein (2014). It is a fast, transition-based parser and can process short documents such as news articles or essays in less Human Annotations Among the 600 annotations obtained in Wang et al. (2017a), 120 responses from 6 test questions (5 responses from each score level for each question) were double annotated. The standard evaluation method of F1 scores on three levels (span, nuclearity, and relation) (Marcu, 2000b) was used to evaluate the human agreement, where 4 Downloaded from https://github. com/EducationalTestingService/ discourse-parsing 3 Downloaded from http://www.isi.edu/ licensed-sw/RSTTool/i"
W19-2719,N09-1050,1,0.737625,"troduction The spread of English as the main global language for education and commerce is continuing, and there is a strong interest in developing assessment systems that can automatically score spontaneous speech from non-native speakers with the goals of reducing the burden on human raters, improving reliability, and generating feedback that can be used by language learners (Zechner et al., 2009; Higgins et al., 2011). Various features related to different aspects of speaking proficiency have been explored, such as features for pronunciation, prosody, and fluency (Cucchiarini et al., 2002; Chen et al., 2009; Cheng, 2011; Higgins et al., 2011), as well as features for vocabulary, • A larger annotated corpus consisting of 1440 non-native spontaneous spoken responses was obtained using an annotation scheme based on the RST framework. In addition to the previously annotated 600 responses (Wang et al., 2017a), annotations on additional 840 responses were obtained to enlarge the data set that can be used to train 153 Proceedings of Discourse Relation Parsing and Treebanking (DISRPT2019), pages 153–162 c Minneapolis, MN, June 6, 2019. 2019 Association for Computational Linguistics an automatic RST pars"
W19-2719,P13-1048,0,0.0663851,"Missing"
W19-2719,P11-1073,1,0.856733,"Missing"
W19-2719,J15-3002,0,0.203137,"Missing"
W19-2719,D16-1035,0,0.0324865,"Missing"
W19-2719,P14-1003,0,0.0524407,"Missing"
W19-2719,W14-4916,0,0.0231434,"on, text summarization, and machine translation (Taboada and Mann, 2006a). In particular, the availability of the RST Discourse Treebank (Carlson et al., 2001), with annotations on a selection of 385 Wall Street Journal articles from the Penn Treebank2 , has facilitated RST-based discourse analysis of written texts, since it provides a standard benchmark for comparing the performance of different parsers. A wide range of techniques have However, the annotation scheme with shallow discourse structure and relations from the PDTB (Prasad et al., 2008) has been generally used for spoken language (Demirsahin and Zeyrek, 2014; Stoyanchev and Bangalore, 2015) instead of the rooted-tree structure that is employed in RST. For example, Tonelli et al. (2010) adapted the PDTB annotation scheme to annotate discourse relations in spontaneous conversations in Italian, and Rehbein et al. (2016) compared two frameworks, PDTB and Cognitive approach to Coherence Relations (CCR) (Sanders et al., 1992), for the annotation of discourse relations in spoken language. 1 In this paper, all the reported results on the relation level use the full labels of both nuclearity and relation for evaluation. 2 https://catalog.ldc.upenn.edu/ LD"
W19-2719,D17-1133,0,0.023873,"Missing"
W19-2719,P14-1048,0,0.186129,"Missing"
W19-2719,J00-3005,0,0.330843,"to improve the state-of-art in parsing, we employed a pre-existing open-source parser from Heilman and Sagae (2015)4 , which was implemented following the work of Sagae (2009) and Ji and Eisenstein (2014). It is a fast, transition-based parser and can process short documents such as news articles or essays in less Human Annotations Among the 600 annotations obtained in Wang et al. (2017a), 120 responses from 6 test questions (5 responses from each score level for each question) were double annotated. The standard evaluation method of F1 scores on three levels (span, nuclearity, and relation) (Marcu, 2000b) was used to evaluate the human agreement, where 4 Downloaded from https://github. com/EducationalTestingService/ discourse-parsing 3 Downloaded from http://www.isi.edu/ licensed-sw/RSTTool/index.html 156 Figure 1: Example of an annotated RST tree on a response with a proficiency score of 1. Table 1: Average numbers of EDUs and word tokens (and their standard deviations) appearing in the RST Discourse Treebank (RST DT) and the annotated corpus of non-native spontaneous speech (RST SS). than a second. Since the ultimate goal is to introduce the discourse parser into an automated speech scorin"
W19-2719,C14-1089,0,0.0576495,"Missing"
W19-2719,D17-1136,0,0.313866,"Missing"
W19-2719,P17-2041,1,0.43986,"to explore effective means to automate the analysis of discourse and the measurement of coherence in non-native spoken responses, thereby improving the validity of an automated scoring system. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the most influential approaches for document-level discourse analysis. It can represent a document’s discourse structure using a hierarchical tree in which nodes are recursively linked with rhetorical relations and labeled with nucleus or satellite tags to depict the importance of the child nodes in a relation. In our previous study (Wang et al., 2017a), RST-based discourse annotations were obtained on a corpus of 600 spontaneous spoken responses provided by non-native English speakers in the context of an English speaking proficiency assessment. In this paper, we continued this line of research, and made further contributions as follows: This study aims to model the discourse structure of spontaneous spoken responses within the context of an assessment of English speaking proficiency for non-native speakers. Rhetorical Structure Theory (RST) has been commonly used in the analysis of discourse organization of written texts; however, limite"
W19-2719,W17-3605,0,0.0620641,"Missing"
W19-2719,N13-1101,1,0.7543,"2010) adapted the PDTB annotation scheme to annotate discourse relations in spontaneous conversations in Italian, and Rehbein et al. (2016) compared two frameworks, PDTB and Cognitive approach to Coherence Relations (CCR) (Sanders et al., 1992), for the annotation of discourse relations in spoken language. 1 In this paper, all the reported results on the relation level use the full labels of both nuclearity and relation for evaluation. 2 https://catalog.ldc.upenn.edu/ LDC2002T07 Regarding the measurement of discourse coherence in the automated assessment of spoken language, our previous work (Wang et al., 2013, 154 the spoken language assessment. The scoring rubrics address the following three main aspects of speaking proficiency: delivery (pronunciation, fluency, prosody), language use (grammar and lexical choice), and topic development (content and coherence). Responses were balanced for proficiency levels, i.e., 60 responses were included from each of the 4 score points from each of the 6 test questions. In addition to the holistic proficiency scores, the transcription of each spoken response in this corpus was also provided with a global discourse coherence score by two expert annotators (not d"
W19-2719,prasad-etal-2008-penn,0,0.463336,"ely related to our research is the automated evaluation of discourse in student essays. For example, one study used features for each sentence in an essay to reflect the status of its parent node as well as its rhetorical relation based on automatically parsed RST trees, with the goal of providing feedback to students about the discourse structure in their essay (Burstein et al., 2003). Another study compared features derived from deep hierarchical discourse relations based on RST trees with features derived from shallow discourse relations based on Penn Discourse Treebank (PDTB) annotations (Prasad et al., 2008) and demonstrated the positive impact of using deep discourse structures to evaluate text coherence (Feng et al., 2014). • A set of RST-based features were introduced to measure the discourse structure of nonnative spontaneous speech, where 1) an automatic speech recognizer (ASR) was used to transcribe the speech into text; 2) the aforementioned automatic parsers were applied to build RST trees based on the ASR output; 3) a set of features extracted from the automatic trees were explored, and the results show that these discourse features can predict holistic proficiency scores with an accurac"
W19-2719,P17-2029,0,0.0690365,"to explore effective means to automate the analysis of discourse and the measurement of coherence in non-native spoken responses, thereby improving the validity of an automated scoring system. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the most influential approaches for document-level discourse analysis. It can represent a document’s discourse structure using a hierarchical tree in which nodes are recursively linked with rhetorical relations and labeled with nucleus or satellite tags to depict the importance of the child nodes in a relation. In our previous study (Wang et al., 2017a), RST-based discourse annotations were obtained on a corpus of 600 spontaneous spoken responses provided by non-native English speakers in the context of an English speaking proficiency assessment. In this paper, we continued this line of research, and made further contributions as follows: This study aims to model the discourse structure of spontaneous spoken responses within the context of an assessment of English speaking proficiency for non-native speakers. Rhetorical Structure Theory (RST) has been commonly used in the analysis of discourse organization of written texts; however, limite"
W19-2719,L16-1165,0,0.0177089,"RST-based discourse analysis of written texts, since it provides a standard benchmark for comparing the performance of different parsers. A wide range of techniques have However, the annotation scheme with shallow discourse structure and relations from the PDTB (Prasad et al., 2008) has been generally used for spoken language (Demirsahin and Zeyrek, 2014; Stoyanchev and Bangalore, 2015) instead of the rooted-tree structure that is employed in RST. For example, Tonelli et al. (2010) adapted the PDTB annotation scheme to annotate discourse relations in spontaneous conversations in Italian, and Rehbein et al. (2016) compared two frameworks, PDTB and Cognitive approach to Coherence Relations (CCR) (Sanders et al., 1992), for the annotation of discourse relations in spoken language. 1 In this paper, all the reported results on the relation level use the full labels of both nuclearity and relation for evaluation. 2 https://catalog.ldc.upenn.edu/ LDC2002T07 Regarding the measurement of discourse coherence in the automated assessment of spoken language, our previous work (Wang et al., 2013, 154 the spoken language assessment. The scoring rubrics address the following three main aspects of speaking proficiency"
W19-2719,N12-1011,1,0.884192,"Missing"
W19-2719,W09-3813,0,0.0268883,"riptions of nonnative spontaneous speech, which were at times unintelligible due to the lack of proficiency and transcription inaccuracy. 4 Automatic Parsing 4.1 Parser Training There has been a variety of research on documentlevel discourse parsing based on the RST Discourse Treebank, and multiple RST parsers are available as open source tools. In this study, since the focus of our research is not to investigate advanced techniques to improve the state-of-art in parsing, we employed a pre-existing open-source parser from Heilman and Sagae (2015)4 , which was implemented following the work of Sagae (2009) and Ji and Eisenstein (2014). It is a fast, transition-based parser and can process short documents such as news articles or essays in less Human Annotations Among the 600 annotations obtained in Wang et al. (2017a), 120 responses from 6 test questions (5 responses from each score level for each question) were double annotated. The standard evaluation method of F1 scores on three levels (span, nuclearity, and relation) (Marcu, 2000b) was used to evaluate the human agreement, where 4 Downloaded from https://github. com/EducationalTestingService/ discourse-parsing 3 Downloaded from http://www.i"
W19-2719,D12-1055,0,0.0266336,"Missing"
W19-2719,W12-2021,1,0.885164,"Missing"
W19-2719,tonelli-etal-2010-annotation,0,0.0334103,"(Carlson et al., 2001), with annotations on a selection of 385 Wall Street Journal articles from the Penn Treebank2 , has facilitated RST-based discourse analysis of written texts, since it provides a standard benchmark for comparing the performance of different parsers. A wide range of techniques have However, the annotation scheme with shallow discourse structure and relations from the PDTB (Prasad et al., 2008) has been generally used for spoken language (Demirsahin and Zeyrek, 2014; Stoyanchev and Bangalore, 2015) instead of the rooted-tree structure that is employed in RST. For example, Tonelli et al. (2010) adapted the PDTB annotation scheme to annotate discourse relations in spontaneous conversations in Italian, and Rehbein et al. (2016) compared two frameworks, PDTB and Cognitive approach to Coherence Relations (CCR) (Sanders et al., 1992), for the annotation of discourse relations in spoken language. 1 In this paper, all the reported results on the relation level use the full labels of both nuclearity and relation for evaluation. 2 https://catalog.ldc.upenn.edu/ LDC2002T07 Regarding the measurement of discourse coherence in the automated assessment of spoken language, our previous work (Wang"
W19-4401,P16-2096,0,0.0571079,"ot immediately related to the construct (i.e., “constructirrelevant”) may indicate that the test is unfair (Xi, 2010; Zieky, 2016). Notably such “constructThe issues of algorithmic fairness and bias have recently featured prominently in many publications highlighting the fact that training the algorithms for maximum performance may often result in predictions that are biased against various groups (Kamiran and Calders, 2009; Kamishima et al., 2012; Luong et al., 2011; Zemel et al., 2013; Feldman et al., 2015; Friedler et al., 2016). Like any algorithm, NLP systems are not immune to such bias (Hovy and Spruit, 2016; Caliskan et al., 2017). These days it is hardly necessary to justify the importance of ensuring algorithmic fairness, especially in applications that can have a substantial impact on users’ lives. 1 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1–10 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics known outcome; (4) Conditional use accuracy equality: the prediction is equally accurate for all groups when conditioned on a predicted outcome; (5) Treatment equality: the ratio of false negatives and f"
W19-4401,W17-1605,1,0.652202,"Missing"
W19-4401,W17-5007,0,0.0409918,"during a large-scale assessment of English language speaking proficiency. For this assessment all test-takers answered 6 questions that elicited spontaneous speech. Depending on the question, the speakers were given 45 seconds or 1 minute to record their responses. We will focus on whether automated scoring might disadvantage test-takers depending on their native language (L1), a common concern in automated scoring contexts. Learners with different L1 might have different linguistic profile and it has been shown that it is possible to identify learner L1 from their written or spoken response (Malmasi et al., 2017). Therefore there is a danger that the scoring engine might inadvertently assign different scores to speakers of different L1 even when there is no difference in English proficiency, the actual construct measured by the test. In other words the system would introduce group-related construct-irrelevant differences. In an actual operational scenario there are many additional factors that can introduce bias to the performance of an automated scoring system: some L1s might be over- or under-represented in the data used for model training and evaluation; sometimes different versions of the test are"
W19-4432,W16-0514,0,0.0215391,"generator. Errors at the ASR stage may negatively affect the content features such that they are noisy and distorted to some extent. Secondly, and more importantly, spontaneous speech, unlike read speech, is highly variable, and particular aspects of content can be expressed in many different ways by different speakers. Consequently, relatively few studies have explored content of spontaneous spoken responses. Xie et al. (2012) and Cheng et al. (2014) assessed content using similarity scores between test responses and highly proficient sample responses, based on content vector analysis (CVA). Loukina and Cahill (2016) used a content-scoring engine based on many sparse features, such as unigrams and bigrams, trained on a large corpus of existing responses. These studies were based on traditional character or word ngrams. Recently, significant improvement in ASR systems, semantic modeling technology based on 3 Overview of the approach In order to address this gap, we developed an automated algorithm which provides feedback about content completeness for non-native speakers’ spontaneous speech. Distinct from previous content scoring approaches that look at correctness of overall content by calculating similar"
W19-4432,W14-1802,0,0.0368273,"Missing"
W19-4432,W16-0533,0,0.0295958,"ovative Use of NLP for Building Educational Applications, pages 306–315 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics more advanced deep-neural networks (DNN), and larger training data sets encouraged researchers in the automated scoring field to explore contentmodeling for spoken responses. For instance, Chen et al. (2018) and Qian et al. (2018) developed automated oral proficiency scoring models using diverse neural models and achieved comparable or superior performance to sophisticated linguistic feature-based systems. In addition, Yoon et al. (2018) and Rei and Cummins (2016) used similarity scores between the prompt texts and test responses based on word embeddings. Compared to the traditional word-matching based method, they have the advantage of capturing topical relevance that is not based on specific, identical words. However, these studies have focused only on scoring, and based on our knowledge, no study has explored content feedback for spontaneous speech. this study, we trained automated models to detect the absence of key points that are the core content expected in correct answers. Next, we discussed possible ways to generate content feedback based on t"
W19-4432,S13-2046,1,0.92334,"r algorithm first determines absence of individual key points. The absence of a key point signals an issue in the content completeness of a spoken response. Next, we provide a list of missing key points with feedback about how to improve content completeness to the speakers. Our approach is able to provide much more fine-grained and targeted feedback about the content of a response, as compared to a traditional holistic approach. In order to determine the absence of the key points, we calculated similarity scores between a spoken response and a key point using a short response scoring engine (Heilman and Madnani, 2013) and new word-embedding based features. The short response scoring engine generally requires a sizable amount of response data for each question to achieve a reliable performance. Collecting question-specific data is a difficult task. Thus, the word-embedding features, that do not require any sample responses for each question for the feature training, have a strong advantage for practical systems. We evaluated the algorithm in two different conditions (questions in the training data vs. questions not in the training data) and ex307 plored the impact of a question-specific training dataset. 4"
W19-4432,N12-1011,1,0.643843,"task for a variety of reasons. First, an automated speech recognition (ASR) system is used to generate an automated transcription of a spoken response as an input of the content feature generator. Errors at the ASR stage may negatively affect the content features such that they are noisy and distorted to some extent. Secondly, and more importantly, spontaneous speech, unlike read speech, is highly variable, and particular aspects of content can be expressed in many different ways by different speakers. Consequently, relatively few studies have explored content of spontaneous spoken responses. Xie et al. (2012) and Cheng et al. (2014) assessed content using similarity scores between test responses and highly proficient sample responses, based on content vector analysis (CVA). Loukina and Cahill (2016) used a content-scoring engine based on many sparse features, such as unigrams and bigrams, trained on a large corpus of existing responses. These studies were based on traditional character or word ngrams. Recently, significant improvement in ASR systems, semantic modeling technology based on 3 Overview of the approach In order to address this gap, we developed an automated algorithm which provides fee"
W19-4432,W18-4002,1,0.833025,"rteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 306–315 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics more advanced deep-neural networks (DNN), and larger training data sets encouraged researchers in the automated scoring field to explore contentmodeling for spoken responses. For instance, Chen et al. (2018) and Qian et al. (2018) developed automated oral proficiency scoring models using diverse neural models and achieved comparable or superior performance to sophisticated linguistic feature-based systems. In addition, Yoon et al. (2018) and Rei and Cummins (2016) used similarity scores between the prompt texts and test responses based on word embeddings. Compared to the traditional word-matching based method, they have the advantage of capturing topical relevance that is not based on specific, identical words. However, these studies have focused only on scoring, and based on our knowledge, no study has explored content feedback for spontaneous speech. this study, we trained automated models to detect the absence of key points that are the core content expected in correct answers. Next, we discussed possible ways to generate"
