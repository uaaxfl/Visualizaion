2005.eamt-1.3,J04-2003,0,0.0274549,"s ternatives offered for given source language words and constructions is generally smaller than what a translator can produce without much effort. While some approaches such as example-based MT or interlingual MT can cope better with structural differences and functional variants than other approaches, it is generally true that the more variation, the harder it is for the system to pick one which is appropriate in a given context. This is true also for statistical systems that generally perform better if source and target can be made more structurally similar before training starts (cf. e.g. Nießen and Ney, 2004). 2.3. Close translation as a goal Given this state of affairs it seems reasonable that MT should aim for reaching as far as possible towards the semantic end of the literalsemantic continuum. As a first step, I propose that the goal be set to close translation. This would be a desirable goal for a high-quality gisting system or a core system on which to develop domain-restricted systems. If this can be agreed, it should have some important effects on both training and evaluation, since the data one should use for training and evaluation should have been translated according to the requirement"
2005.eamt-1.3,C90-3048,0,0.0903383,"syntactic phrase and word token of the target must, with only few and systematic exceptions, be an ex15 Ahrenberg ponent of at least one source phrase or token; (3) Text level alignments belong to types, that can be instantiated in a variety of contexts. 3. Codifying close translation It is a fact that a translation is underdetermined by the source language text. For this reason some authors have argued for interactive architectures where the system asks the user for help whenever the information in the SL text is insufficient to determine a safe translation (e.g., Johnson & Whitelock, 1989; Somers et al., 1990), and others have argued that relevant world knowledge should be modelled to support decisions (e.g., Nirenburg et al. 1991). It may well be that the goals for MT, when developed for general (unknown) text, will have to be set at a low level, lower than this paper assumes. However, it is still of interest to know where that level is, and how it can be characterised in the terms of translation studies, given that we have access to more and more translation data to inform system development. 3.1. General selection criteria The Alignment Criterion in itself allows a wide range of possible transla"
2005.eamt-1.3,P02-1040,0,\N,Missing
2006.eamt-1.2,W02-1502,0,0.141929,"Semantics representations (MRS; Copestake, Flickinger, Sag, & Pollard, 2003) are used as interface structures. However, unlike the LOGON architecture, which uses different grammars based on different formalisms and linguistic theories, we see it as an advantage to use the same grammatical framework for both source and target languages, as this means that the same parser and generator can be used throughout. We use HPSG-like Typed Feature Structure Grammars as our framework because of the availability of the LKB workbench (Copestake, 2001) and the store of type definitions known as the Matrix (Bender, Flickinger, & Oepen, 2002), made available by the delph-in collaboration (Bond, Oepen, Siegel, Copestake, & Flickinger, 2005). In addition, we see it as desirable that grammars are designed on similar principles, so that solutions to translation problems can be coordinated between languages and implemented using a shared inventory of types. This makes the system more homogenous and facilitates the addition of new languages. We also believe that the development of practical applications benefits from the existence of a core system that provides a library of solutions to the translation problems that are likely to be en"
2006.eamt-1.2,1995.tmi-1.2,0,0.644555,"d as the core of a small MT system. Copestake, Flickinger, Malouf, Riehemann, & Sag (1995) describe how MRS can be used for translation. They suggest a design that is based on semantic transfer using MRS. The transfer component works on MRS to produce output that the target grammar can accept. It is possible that the transfer component can output more than one form, some of which may be unacceptable by the generator. When several forms are output they will be ordered by a control mechanism that is distinct from both the transfer component and the generator. The transfer component suggested by Copestake et al. (1995) is based on setting up symmetric and bidirectional transfer equivalences between each pair of languages. Their suggestion also allows interlingual predicates that are common for all languages such as negation. A large-scale project where semantic transfer with MRS is used is LOGON, which focus on translation between Norwegian and English (Oepen et al., 2004). The main architecture is: analysis of Norwegian to MRS using the Norwegian LFG grammar NorGram (Dyvik, 1999), MRS transfer as described above, and generation to English using the ERG. The LOGON system is unidirectional. It only translate"
2006.eamt-1.2,J94-4004,0,0.333457,"h the delph-in resources. Section 4 describes BiTSE, the bilingual grammar that is the core of our MT system. In section 5 we explain our treatment of several types of divergences. Section 6 contains a discussion on the merits and limits of our approach and section 7 contains the conclusion. 2 Verb Frame Divergences As part of this study we have investigated verb frame divergences (VFDs). A verb frame consists of a verb and its arguments. A verb frame divergence is when two verb frames with the same meaning have different structures. Some examples of this, based on the categories suggested by Dorr (1994), will be presented here. All examples in this article are taken from the Europarl corpus (Koehn, 2005). Some of the examples shown contain more than one type of divergence. (1) In that case, the matter turns out to be a national problem after all. Till sist kommer a¨ndock a¨rendet att visa sig vara ett nationellt problem. (2) This appears to be the case with the events which Mr Lomas reports in his question. Det tycks n¨amligen vara fallet med de fakta som herr Lomas f¨or p˚ a tal i sin fr˚ aga. (3) But that is precisely why we first need a clear strategy. Just d¨arf¨or a¨r till att b¨orja me"
2006.eamt-1.2,2005.mtsummit-papers.11,0,0.00562172,"system. In section 5 we explain our treatment of several types of divergences. Section 6 contains a discussion on the merits and limits of our approach and section 7 contains the conclusion. 2 Verb Frame Divergences As part of this study we have investigated verb frame divergences (VFDs). A verb frame consists of a verb and its arguments. A verb frame divergence is when two verb frames with the same meaning have different structures. Some examples of this, based on the categories suggested by Dorr (1994), will be presented here. All examples in this article are taken from the Europarl corpus (Koehn, 2005). Some of the examples shown contain more than one type of divergence. (1) In that case, the matter turns out to be a national problem after all. Till sist kommer a¨ndock a¨rendet att visa sig vara ett nationellt problem. (2) This appears to be the case with the events which Mr Lomas reports in his question. Det tycks n¨amligen vara fallet med de fakta som herr Lomas f¨or p˚ a tal i sin fr˚ aga. (3) But that is precisely why we first need a clear strategy. Just d¨arf¨or a¨r till att b¨orja med en klar strategi n¨odv¨andig h¨ar. In (1) “turns out to be” corresponds to “visa sig vara” (“show its"
2006.eamt-1.2,1991.mtsummit-papers.9,0,0.0584048,"interlinguas and the merits and drawbacks of interlingual approaches as compared to transfer approaches (e.g. Boitet, 1988; Nirenburg & Goldman, 1990). Basically a MRS relation is a place-holder for a concept with known argument structure which is associated with one or more linguistic expressions in a lexicon. Semantic relations such as hyponomy and antonomy, and even some semantic decomposition, could be added, but is not part of the current setup, though hyponymy could be dealt with within the type system. Domain knowledge, as used in knowledgebased interlingual MT such as the KANT system (Mitamura, Nyberg, & Carbonell, 1991), is also not handled. Similarities between two (or more) languages can be encoded in formal grammars in different ways. The Rosetta project (Rosetta, 1994) explored the idea of isomorphic grammars. Our framework does not require grammars to be isomorphic; the important thing is that they produce a common MRS for sentences that are translations of one another. In addition, the grammars are actually implemented as one bilingual (or multilingual) grammar, allowing types to be shared between languages. 3.1 MT using sources DELPH-IN reThere have been previous suggestions for MT using the delph-in"
2006.eamt-1.2,2004.tmi-1.2,0,0.0387312,"Missing"
2006.eamt-1.2,2005.mtsummit-osmtw.3,0,\N,Missing
2021.motra-1.6,P14-2048,0,0.0169725,"Missing"
2021.motra-1.6,ahrenberg-2017-comparing,1,0.923796,"the system can produce and to what extent it can apply those solutions accurately. As for the first part, it is related to taxonomies of what has variously been termed translation procedures (Vinay and Darbelnet, 1958/77) or translation relations (van Leuven-Zwart, 1989), though in this paper we will call them solution types. The second part is an analysis of what we can call debatable solutions or issues (Lommel et al., 2015). This analysis uses a linguistically-based taxonomy of issues, and has been done by the author only. It is however supported with evidence from the human translation. (Ahrenberg, 2017) concluded for a study on the same language pair and direction that ”... the MT is in many ways, such as length, information flow, and structure more similar to the source than the HT. More importantly, it exhibits a much more restricted repertoir of procedures, and its output is estimated to require about three edits per sentence”. Here, an edit is caused by an issue that was judged to require alteration. A specific aim of this paper is to see whether these conclusions are still valid. The data used for this study comes from the English and Swedish Parallel Universal Dependencies treebanks (Z"
2021.motra-1.6,J17-3002,0,0.0239154,"ngrams. (Nguyen-Son et al., 2017), found other features, such as word distributions, complex phrase constructions, and the occurrence of phrasal verbs to be helpful, in particular when combined with coherence features across sentences or within whole paragraphs. The data used in this study consist of isolated sentences, so we cannot employ coherence features. However, we can compare our approach to what can be gained from other features. There have also been studies aimed at automating, at least partly, the recognition of solution types, or divergencies as they are often called, for example (Deng and Xue, 2017; Zhai et al., 2019). This was an initial aim also of this study, but was abandoned for reasons that will be explained below. 3 Adjectives in English-Swedish translation Adjectives have very much the same behaviour in English and Swedish. They can be modifiers/attributes, predicatives, heads of arguments such as subjects and objects, conjuncts and be part of lexicalized phrases. All of these functions are actually found in the English source data. The distribution of these functions in the source data is shown in Table 1 together with simple examples to show what is meant. Function modifier pr"
ahrenberg-2010-alignment,holmqvist-2010-heuristic,0,\N,Missing
ahrenberg-2010-alignment,steinberger-etal-2006-jrc,0,\N,Missing
ahrenberg-2010-alignment,C00-2163,0,\N,Missing
ahrenberg-2010-alignment,W00-0901,0,\N,Missing
ahrenberg-2010-alignment,W08-0411,0,\N,Missing
ahrenberg-2010-alignment,W07-2441,1,\N,Missing
ahrenberg-2010-alignment,J03-1002,0,\N,Missing
ahrenberg-2010-alignment,W02-1039,0,\N,Missing
ahrenberg-2010-alignment,2005.mtsummit-papers.11,0,\N,Missing
ahrenberg-2017-comparing,P02-1040,0,\N,Missing
ahrenberg-2017-comparing,W15-0713,0,\N,Missing
ahrenberg-2017-comparing,W13-2510,0,\N,Missing
ahrenberg-etal-2000-evaluation,J93-1004,0,\N,Missing
ahrenberg-etal-2000-evaluation,C00-2163,0,\N,Missing
ahrenberg-etal-2000-evaluation,E93-1015,0,\N,Missing
ahrenberg-etal-2000-evaluation,J93-2003,0,\N,Missing
ahrenberg-etal-2000-evaluation,J90-2002,0,\N,Missing
ahrenberg-etal-2000-evaluation,P91-1022,0,\N,Missing
ahrenberg-etal-2000-evaluation,P98-1004,1,\N,Missing
ahrenberg-etal-2000-evaluation,C98-1004,1,\N,Missing
ahrenberg-etal-2000-evaluation,P00-1056,0,\N,Missing
C88-1003,E87-1024,1,0.758531,"Missing"
C88-1003,J86-2006,0,0.0252492,"Missing"
C88-1003,E87-1044,0,0.066664,"Missing"
C88-1003,P81-1028,0,0.0167409,"xicon. 1. B a c k g r o u n d a n d p u r p o s e In a working system, however, we are not merely interested in a decontextualised meaning of an expression, but in the content communicated in an utterance of an expression, which, as we know, depend on world knowledge and context in more or less subtle ways. A rather trivial fact is that we need to have an understanding of the context in order to find a referent for a referring expression. A more interesting fact is that we often need an understanding of context in order to get at the information which is relevant for determining the referent /Moore, 1981; Ahrenberg a 1987a,b; Pulman, 1987/. In knowledge-based natural language understanding systems the role of syntax is by no means self-evident. In the Yalean tradition /,~tchank & Riesbeck, 1981/ syntax has only played a minor role and whatever little syntactic information there is has been expressed in simple terms. Consequently, there is no grammar as such and syntactic conditions are freely intermixed with semantic conditions in the requests that drive the system forward/13irnbaum & Selfridge, 1981/. Similarly, in frame-based systems such as /Hayes, 1984/ the syntactic information is stated"
C88-1003,P84-1024,0,0.0734871,"Missing"
C88-1003,C86-1149,0,0.0607816,"Missing"
C90-2001,E87-1028,0,0.021844,"Missing"
C98-1004,C88-1016,0,0.0108735,"d, but wish to provide mechanisms that a user can easily adapt to new languages. The organisation of the paper is as follows: In section 2 we relate this approach to previous work, in section 3 we motivate and spell out our assumptions about the behaviour of lexical units in translation, in section 4 we present the basic features of the algorithm, and in section 5 we present results from an evaluation and try to compare these to the results of others. 2. Previous work Most algorithms for bilingual word alignment to date have been based on the probabilistic translation models first proposed by Brown et al. (1988, 1990), especially Model 1 and Model 2. These models explicitly exclude multi-word units from consideration 1. Melamed (1997b), however, proposes a method for the recognition of multiword compounds in bitexts that is based on the predictive value of a translation model. A trial translation model that treat certain multi-word sequences as units is compared with a base translation model that treats the same sequences as multiple single-word units. A drawback with Melamed&apos;s method is that compounds are defined relative to a given translation and not with respect to languageinternal criteria. Thu"
C98-1004,J90-2002,0,0.121967,"Missing"
C98-1004,C94-2178,0,0.00847334,"find linguistic units of the proper size. Kitamura and Matsumoto (1996) present results from aligning multi-word and single word expressions with a recall of 80 per cent if partially correct translations were included. Their method is iterative and is based on the use of the Dice coefficient. Smadja et. al (1996) also use the Dice i Model 3-5 includes multi-word units in one direction. 30 coefficient as their basis for aligning collocations between English and French. Their evaluation show results of 73 per cent accuracy (precision) on average. 3. U n d e r l y i n g a s s u m p t i o n s As Fung and Church (1994) we wish to estimate the bilingual lexicon directly. Unlike Fung and Church our texts are already aligned at sentence lcvel and the lexicon is viewed, not merely as word associations, but as associations between lexical units of the two languages. We assume that texts have structure at many different levels. At the most concrete level a text is simply a sequence of characters. At the next level a text is a sequence of word tokens, where word tokens are defined as sequences of alphanumeric character strings that are separated from one another by a finite set of delimiters such as spaces and pun"
C98-1004,W96-0107,0,0.0123431,"a base translation model that treats the same sequences as multiple single-word units. A drawback with Melamed&apos;s method is that compounds are defined relative to a given translation and not with respect to languageinternal criteria. Thus, if the method is used to construct a bilingual concordance, there is a risk that compounds and idioms that translate compositionally will not be found. Moreover, it is computationally expensive and, since it constructs compounds incrementally, adding one word at a time, requires many iterations and much processing to find linguistic units of the proper size. Kitamura and Matsumoto (1996) present results from aligning multi-word and single word expressions with a recall of 80 per cent if partially correct translations were included. Their method is iterative and is based on the use of the Dice coefficient. Smadja et. al (1996) also use the Dice i Model 3-5 includes multi-word units in one direction. 30 coefficient as their basis for aligning collocations between English and French. Their evaluation show results of 73 per cent accuracy (precision) on average. 3. U n d e r l y i n g a s s u m p t i o n s As Fung and Church (1994) we wish to estimate the bilingual lexicon directl"
C98-1004,1996.amta-1.15,0,0.0679167,"Missing"
C98-1004,P97-1063,0,0.0557206,"Missing"
C98-1004,W97-0311,0,0.100209,"ection 2 we relate this approach to previous work, in section 3 we motivate and spell out our assumptions about the behaviour of lexical units in translation, in section 4 we present the basic features of the algorithm, and in section 5 we present results from an evaluation and try to compare these to the results of others. 2. Previous work Most algorithms for bilingual word alignment to date have been based on the probabilistic translation models first proposed by Brown et al. (1988, 1990), especially Model 1 and Model 2. These models explicitly exclude multi-word units from consideration 1. Melamed (1997b), however, proposes a method for the recognition of multiword compounds in bitexts that is based on the predictive value of a translation model. A trial translation model that treat certain multi-word sequences as units is compared with a base translation model that treats the same sequences as multiple single-word units. A drawback with Melamed&apos;s method is that compounds are defined relative to a given translation and not with respect to languageinternal criteria. Thus, if the method is used to construct a bilingual concordance, there is a risk that compounds and idioms that translate compo"
C98-1004,A97-1050,0,0.0316456,"Missing"
C98-1004,J96-1001,0,0.124526,"Missing"
C98-1004,A94-1006,0,\N,Missing
E03-1086,ahrenberg-etal-2002-system,1,0.653858,"s, 2000; Ahrenberg et al., 2000a). However, the idea of improving the outcome of an automatic system, though quite common with sentence aligners and the creation of tree-banks (Marcus et al., 1993), 49 seems not to have been applied systematically to word alignment. Isahara and Haruno (2000) present a post-editing tool for sentence alignment that has been extended with functions for alignment of phrases and proper nouns. In the Cairo system (Smith and Jahr, 2000) a user can examine visualizations of the word alignments produced by a word aligner, but is not allowed to make changes to them. In Ahrenberg et al. (2002) an earlier version of the interactive linker was presented. This version had a more primitive interface and also lacked several of the resource and learning capabilities included in the current version. 3 I*Link — an interactive word aligner The current version of l*Link supports the following tasks :1 • Manual word alignment • Automatic proposals of token alignments • Reviewing and editing alignment proposals from the system in an orderly fashion, • Configuring the resources to be used by the system in a work session • Compiling reports and statistics from aligned files. 3.1 The I*Link works"
E03-1086,J93-2004,0,0.0334142,"el corpora to generate translational equivalences among word types. In addition to co-occurrence data, some systems employ linguistic knowledge of varying levels of sophistication (Melamed 2001, Ahrenberg et al., 2000b, Gaussier et al. 2000). Manual word alignment with the support of interactive tools has been used mainly for the creation of gold standards for evaluation purposes (e.g. Melamed, 2001; Veronis and Langlais, 2000; Ahrenberg et al., 2000a). However, the idea of improving the outcome of an automatic system, though quite common with sentence aligners and the creation of tree-banks (Marcus et al., 1993), 49 seems not to have been applied systematically to word alignment. Isahara and Haruno (2000) present a post-editing tool for sentence alignment that has been extended with functions for alignment of phrases and proper nouns. In the Cairo system (Smith and Jahr, 2000) a user can examine visualizations of the word alignments produced by a word aligner, but is not allowed to make changes to them. In Ahrenberg et al. (2002) an earlier version of the interactive linker was presented. This version had a more primitive interface and also lacked several of the resource and learning capabilities inc"
E03-1086,ahrenberg-etal-2000-evaluation,1,\N,Missing
E03-1086,smith-jahr-2000-cairo,0,\N,Missing
E03-1086,A97-1011,0,\N,Missing
E87-1024,C86-1149,0,0.210553,"Missing"
E87-1024,E85-1013,0,0.0279409,"cuted on the background system, and (ii) a meeting to be booked. They are labelled, say B1 and M1, and supplied with their descriptions. Moreover, both B1 and M1 are assigned topical status. The system is able to recognize information that it lacks for booking a meeting by comparing the information it has with a definition for a booking command. Having done this it may take the initiative and ask the user to supply that information, by outputting (2) above. In this case the next input from the user will be met with definite expectations, As a second example consider the case of PP-attachment. Wilks et al. (1985) argue (convincingly to my mind) that syntax generally fails to discriminate between alternative attachments. Instead they claim that correct interpretations can be made by a preferential approach on the basis of semantic information associated with the relevant verbs, nouns and prepositions. However, preferences based on general semantic evaluations are not sufficient either. Our knowledge of the actual discourse plays an important role. Consider (6), which taken in isolation is ambiguous since both meetings and cancellations are objects that ~happen ~ at definite times and therefore may be s"
E87-1024,P84-1047,0,\N,Missing
holmqvist-etal-2012-alignment,W09-0421,1,\N,Missing
holmqvist-etal-2012-alignment,J93-2003,0,\N,Missing
holmqvist-etal-2012-alignment,C04-1073,0,\N,Missing
holmqvist-etal-2012-alignment,W08-0406,0,\N,Missing
holmqvist-etal-2012-alignment,C08-1027,0,\N,Missing
holmqvist-etal-2012-alignment,P02-1040,0,\N,Missing
holmqvist-etal-2012-alignment,W08-0312,0,\N,Missing
holmqvist-etal-2012-alignment,W09-0401,0,\N,Missing
holmqvist-etal-2012-alignment,P07-2045,0,\N,Missing
holmqvist-etal-2012-alignment,P10-2033,0,\N,Missing
holmqvist-etal-2012-alignment,J03-1002,0,\N,Missing
holmqvist-etal-2012-alignment,P11-4010,1,\N,Missing
holmqvist-etal-2012-alignment,P05-1066,0,\N,Missing
holmqvist-etal-2012-alignment,W05-0908,0,\N,Missing
holmqvist-etal-2012-alignment,W11-1007,0,\N,Missing
holmqvist-etal-2012-alignment,2005.iwslt-1.8,0,\N,Missing
J13-4009,W05-0909,0,0.177447,"Missing"
J13-4009,D08-1078,0,0.0530341,"Missing"
J13-4009,C12-1022,0,0.0361736,"Missing"
J13-4009,2002.tmi-papers.3,0,0.0382296,"Missing"
J13-4009,P08-1009,0,0.0597835,"Missing"
J13-4009,P11-2031,0,0.0550678,"Missing"
J13-4009,W02-1001,0,0.0443174,"Missing"
J13-4009,J05-1003,0,0.0266713,"Missing"
J13-4009,A92-1018,0,0.48823,"Missing"
J13-4009,N09-1046,0,0.0911061,"Missing"
J13-4009,W06-3102,0,0.0630632,"Missing"
J13-4009,W09-0420,0,0.0373189,"Missing"
J13-4009,W07-2432,0,0.0591911,"Missing"
J13-4009,W10-1734,0,0.510195,"Missing"
J13-4009,W07-0723,1,0.89382,"Missing"
J13-4009,2005.mtsummit-papers.11,0,0.116679,"Missing"
J13-4009,W08-0318,0,0.0527943,"Missing"
J13-4009,D07-1091,0,0.146081,"Missing"
J13-4009,E03-1076,0,0.173848,"the desired compounds are missing in the training data or that they have not been aligned correctly. When a compound is the idiomatic word choice in the translation, systems often produce separate words, genitive or other alternative constructions, or translate only one part of the compound. For an SMT system to cope with the productivity of the phenomenon, any effective strategy should be able to correctly process compounds that have never been seen in the training data as such, although possibly their components have, either in isolation or within a different compound. Previous work (e.g., Koehn and Knight 2003) has shown that compound splitting improves translation from compounding languages into English. In this article we explore several aspects of the less-researched area of compound treatment for translation into such languages, using three Germanic languages (German, Swedish, and Danish) as examples.1 The assumption is that splitting compounds will also improve translation for this translation direction and lead to more natural translations. The strategy we adopt is to split compounds in the training data, and to merge them in the translation output. Our overall goal is to improve translation q"
J13-4009,W07-0734,0,0.0772725,"Missing"
J13-4009,P11-1140,0,0.0852879,"Missing"
J13-4009,C00-2162,0,0.320895,"Missing"
J13-4009,J04-2003,0,0.102576,"Missing"
J13-4009,P03-1021,0,0.153712,"Missing"
J13-4009,J03-1002,0,0.00859661,"Missing"
J13-4009,P02-1040,0,0.0885145,"Missing"
J13-4009,W05-0908,0,0.0723536,"Missing"
J13-4009,C08-1098,0,0.0270212,"Missing"
J13-4009,H05-1095,1,0.710001,"Missing"
J13-4009,sjobergh-kann-2004-finding,0,0.0641585,"Missing"
J13-4009,E09-3008,1,0.867101,"Missing"
J13-4009,W11-2129,1,0.753498,"Missing"
J13-4009,2008.eamt-1.25,1,0.869509,"Missing"
J13-4009,W08-0317,1,0.889497,"Missing"
J13-4009,A97-1011,0,0.155725,"Missing"
J13-4009,2007.mtsummit-papers.65,0,0.0650218,"Missing"
J13-4009,P07-2045,0,\N,Missing
J13-4009,P08-2039,0,\N,Missing
P98-1004,C88-1016,0,0.0108293,"h to provide mechanisms that a user can easily adapt to new languages. The organisation of the paper is as follows: In section 2 we relate this approach to previous work, in section 3 we motivate and spell out our assumptions about the behaviour of lexical units in translation, in section 4 we present the basic features of the algorithm, and in section 5 we present results from an evaluation and try to compare these to the results of others. 2. P r e v i o u s w o r k Most algorithms for bilingual word alignment to date have been based on the probabilistic translation models first proposed by Brown et al. (1988, 1990), especially Model I and Model 2. These models explicitly exclude multi-word units from consideration 1. Melamed (1997b), however, proposes a method for the recognition of multiword compounds in bitexts that is based on the predictive value of a translation model. A trial translation model that treat certain multi-word sequences as units is compared with a base translation model that treats the same sequences as multiple single-word units. A drawback with Melamed&apos;s method is that compounds are defined relative to a given translation and not with respect to languageinternal criteria. Thu"
P98-1004,J90-2002,0,0.116106,"Missing"
P98-1004,A94-1006,0,0.0149105,"Missing"
P98-1004,W96-0107,0,0.0117269,"a base translation model that treats the same sequences as multiple single-word units. A drawback with Melamed&apos;s method is that compounds are defined relative to a given translation and not with respect to languageinternal criteria. Thus, if the method is used to construct a bilingual concordance, there is a risk that compounds and idioms that translate compositionally will not be found. Moreover, it is computationally expensive and, since it constructs compounds incrementally, adding one word at a time, requires many iterations and much processing to find linguistic units of the proper size. Kitamura and Matsumoto (1996) present results from aligning multi-word and single word expressions with a recall of 80 per cent if partially correct translations were included. Their method is iterative and is based on the use of the Dice coefficient. Smadja et. al (1996) also use the Dice Model 3-5 includes multi-word units in one direction. 30 coefficient as their basis for aligning collocations between English and French. Their evaluation show results of 73 per cent accuracy (precision) on average. 3. Underlying assumptions As Fung and Church (1994) we wish to estimate the bilingual lexicon directly. Unlike Fung and Ch"
P98-1004,1996.amta-1.15,0,0.0702105,"Missing"
P98-1004,P97-1063,0,0.152651,"e relate this approach to previous work, in section 3 we motivate and spell out our assumptions about the behaviour of lexical units in translation, in section 4 we present the basic features of the algorithm, and in section 5 we present results from an evaluation and try to compare these to the results of others. 2. P r e v i o u s w o r k Most algorithms for bilingual word alignment to date have been based on the probabilistic translation models first proposed by Brown et al. (1988, 1990), especially Model I and Model 2. These models explicitly exclude multi-word units from consideration 1. Melamed (1997b), however, proposes a method for the recognition of multiword compounds in bitexts that is based on the predictive value of a translation model. A trial translation model that treat certain multi-word sequences as units is compared with a base translation model that treats the same sequences as multiple single-word units. A drawback with Melamed&apos;s method is that compounds are defined relative to a given translation and not with respect to languageinternal criteria. Thus, if the method is used to construct a bilingual concordance, there is a risk that compounds and idioms that translate compo"
P98-1004,W97-0311,0,0.164639,"e relate this approach to previous work, in section 3 we motivate and spell out our assumptions about the behaviour of lexical units in translation, in section 4 we present the basic features of the algorithm, and in section 5 we present results from an evaluation and try to compare these to the results of others. 2. P r e v i o u s w o r k Most algorithms for bilingual word alignment to date have been based on the probabilistic translation models first proposed by Brown et al. (1988, 1990), especially Model I and Model 2. These models explicitly exclude multi-word units from consideration 1. Melamed (1997b), however, proposes a method for the recognition of multiword compounds in bitexts that is based on the predictive value of a translation model. A trial translation model that treat certain multi-word sequences as units is compared with a base translation model that treats the same sequences as multiple single-word units. A drawback with Melamed&apos;s method is that compounds are defined relative to a given translation and not with respect to languageinternal criteria. Thus, if the method is used to construct a bilingual concordance, there is a risk that compounds and idioms that translate compo"
P98-1004,A97-1050,0,0.049782,"Missing"
P98-1004,J96-1001,0,0.195188,"Missing"
P98-1004,C94-2178,0,\N,Missing
stymne-ahrenberg-2010-using,1997.tmi-1.12,0,\N,Missing
stymne-ahrenberg-2010-using,W09-0402,0,\N,Missing
stymne-ahrenberg-2010-using,W07-0411,0,\N,Missing
stymne-ahrenberg-2010-using,W09-0441,0,\N,Missing
stymne-ahrenberg-2010-using,W07-2209,0,\N,Missing
stymne-ahrenberg-2010-using,W08-0332,0,\N,Missing
stymne-ahrenberg-2010-using,E09-3008,1,\N,Missing
stymne-ahrenberg-2010-using,1999.mtsummit-1.8,0,\N,Missing
stymne-ahrenberg-2010-using,P02-1040,0,\N,Missing
stymne-ahrenberg-2010-using,D07-1091,0,\N,Missing
stymne-ahrenberg-2010-using,W09-0401,0,\N,Missing
stymne-ahrenberg-2010-using,P07-2045,0,\N,Missing
stymne-ahrenberg-2010-using,W08-0317,1,\N,Missing
stymne-ahrenberg-2010-using,W07-0734,0,\N,Missing
stymne-ahrenberg-2010-using,2005.mtsummit-papers.11,0,\N,Missing
stymne-ahrenberg-2010-using,W99-1005,0,\N,Missing
stymne-ahrenberg-2010-using,P03-1021,0,\N,Missing
stymne-ahrenberg-2012-practice,elliott-etal-2004-fluency,0,\N,Missing
stymne-ahrenberg-2012-practice,P02-1040,0,\N,Missing
stymne-ahrenberg-2012-practice,P07-2045,0,\N,Missing
stymne-ahrenberg-2012-practice,P08-1087,0,\N,Missing
stymne-ahrenberg-2012-practice,W07-0718,0,\N,Missing
stymne-ahrenberg-2012-practice,J11-4004,0,\N,Missing
stymne-ahrenberg-2012-practice,P11-4010,1,\N,Missing
stymne-ahrenberg-2012-practice,W05-0908,0,\N,Missing
stymne-ahrenberg-2012-practice,2005.mtsummit-papers.11,0,\N,Missing
stymne-ahrenberg-2012-practice,vilar-etal-2006-error,0,\N,Missing
stymne-ahrenberg-2012-practice,2008.eamt-1.25,1,\N,Missing
stymne-ahrenberg-2012-practice,2010.eamt-1.12,0,\N,Missing
stymne-ahrenberg-2012-practice,2011.eamt-1.36,0,\N,Missing
W01-1701,J99-2004,0,0.0135519,"nstrained Lexical Transfer (SCLD. The paper is organised as follows: Sections 2 and 3 provide an overview of the framework. Section 4 presents an initial experiment of constructing a translation system from a translation corpus of the ATISdomain (Jonsson, 2000). 2. Supertags and superlinks 3. A translation model based on superlinks A superlink is, basically, a pair of supertags, where one element of the pair refers to the source language and the other element refers to the target language. A supertag is &quot;a rich description of a lexical item that impose complex constraints in a local context&quot; (Bangalore and Joshi, 1999: 237). While Bangalore and Joshi use the LTAG formalism in their work, the formalism of the supertags as well as their scope can be chosen in different ways. The tags used in our work so far have a more limited power of expression. We make a general distinction between inherent tags and relational tags. A relational tag includes an argument, while an inherent tag does not. The inherent tags used here bear strong resemblance to traditional POS-tags, but have in some of our experiments been augmented with semantic properties. Now, given the notion of a superlink we can give a characterization o"
W01-1701,J90-2002,0,0.130396,"Missing"
W01-1701,J93-2003,0,0.00998947,"Missing"
W07-0723,E03-1076,0,0.382824,"riments were performed on the Europarl data. With factored statistical machine translation, different levels of linguistic information can be taken into account during training of a statistical translation system and decoding. In our experiments we combined syntactic and morphological factors from an off-the-shelf parser with the factored translation framework in Moses (Moses, 2007). We wanted to test the following hypotheses: • Translation models based on lemmas will improve translation quality (Popovič and Ney, 2004) • Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) • Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). 2 The parser The parser, Machinese Syntax, is a commercially available dependency parser from Connexor Oy 1 . It provides each word with lemma, part-of-speech, morphological features and dependency relations (see Figure 1). In addition, the lemmas of compounds are marked by a ‘#’ separating the two parts of the compound. For the shared task we only used shallow linguistic information: lemma, partof-speech and morphology. The compound boundary identification was used to split nou"
W07-0723,J04-2003,0,0.0270794,"und boundary identification was used to split noun com1 Connexor Oy, http://www.connexor.com. 181 Proceedings of the Second Workshop on Statistical Machine Translation, pages 181–184, c Prague, June 2007. 2007 Association for Computational Linguistics pounds to make the German input more similar to English text. 1 Mit mit pm&gt;2 @PREMARK PREP 2 Blick blick advl&gt;10 @NH N MSC SG DAT 3 auf auf pm&gt;5 @PREMARK PREP Figure 1. Example of parser output We used the parser’s tokenization as given. Some common multiword units, such as ‘at all’ and ‘von heute’, are treated as single words by the parser (cf. Niessen and Ney, 2004). The German parser also splits contracted prepositions and determiners like ‘zum’ – ‘zu dem’ (“to the”). 3 System description For our experiments with Moses we basically followed the shared task baseline system setup to train our factored translation models. After training a statistical model, minimum error-rate tuning was performed to tune the model parameters. All experiments were performed on an AMD 64 Athlon 4000+ processor with 4 Gb of RAM and 32 bit Linux (Ubuntu). Since time as well as computer resources were limited we designed a model that we hoped would make the best use of all avai"
W07-0723,C00-2162,0,0.0395781,"-wrong Erbonkel Uncle dna Ref: Sugar daddy U-untranslated Schlussentwurf Schlussentwurf Ref: Final draft Table 3. Classification scheme with examples for compound translations Tot C 44 V 18 P 5 W 3 U 0 F 1 4 5 Tot 38 10 8 2 12 5 75 Table 4. Classification of 75 compounds from our second system and the baseline system Second system C 36 1 Baseline system V P W U F 1 3 3 1 9 2 1 5 3 2 1 2 184 Decompounding of nouns reduced the number of untranslated words, but there were still some left. Among these were cases that can be handled such as separable prefix verbs like ‘aufzeigten’ (“pointed out”) (Niessen and Ney, 2000) or adjective compounds such as ‘multidimensionale’ (“multi dimensional”). There were also some noun compounds left which indicates that we might need a better decompounding strategy than the one used by the parser (see e.g. Koehn and Knight, 2003). 4.2 Experiences and future plans With the computer equipment at our disposal, training of the models and tuning of the parameters turned out to be a very time-consuming task. For this reason, the number of system setups we could test was small, and much fewer than we had hoped for. Thus it is too early to draw any conclusions as regards our hypothe"
W07-0723,C04-1045,0,0.0230352,"er than our knowledge of German, making it easier to judge the quality of translation output. Experiments were performed on the Europarl data. With factored statistical machine translation, different levels of linguistic information can be taken into account during training of a statistical translation system and decoding. In our experiments we combined syntactic and morphological factors from an off-the-shelf parser with the factored translation framework in Moses (Moses, 2007). We wanted to test the following hypotheses: • Translation models based on lemmas will improve translation quality (Popovič and Ney, 2004) • Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) • Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). 2 The parser The parser, Machinese Syntax, is a commercially available dependency parser from Connexor Oy 1 . It provides each word with lemma, part-of-speech, morphological features and dependency relations (see Figure 1). In addition, the lemmas of compounds are marked by a ‘#’ separating the two parts of the compound. For the shared task we only used shallow linguistic informati"
W07-0723,W06-3108,0,0.0273706,"tic information can be taken into account during training of a statistical translation system and decoding. In our experiments we combined syntactic and morphological factors from an off-the-shelf parser with the factored translation framework in Moses (Moses, 2007). We wanted to test the following hypotheses: • Translation models based on lemmas will improve translation quality (Popovič and Ney, 2004) • Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) • Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). 2 The parser The parser, Machinese Syntax, is a commercially available dependency parser from Connexor Oy 1 . It provides each word with lemma, part-of-speech, morphological features and dependency relations (see Figure 1). In addition, the lemmas of compounds are marked by a ‘#’ separating the two parts of the compound. For the shared task we only used shallow linguistic information: lemma, partof-speech and morphology. The compound boundary identification was used to split noun com1 Connexor Oy, http://www.connexor.com. 181 Proceedings of the Second Workshop on Statistical Machine Translat"
W07-2441,W06-2705,0,0.114135,"of regular correspondence for subtrees and their images, we may consider extending it by adding explicit equivalence relations that express normal relations when translating from English to Swedish. 4 Related work Several projects for the creation of parallel treebanks have recently been launched. The FuSe project (Cyrus, 2006) annotates parts of the English and German sections of the Europarl corpus with regard to predicates and their arguments. LinES is different from FuSE in that it aims for complete alignments of segment pairs and (semi-)automatic derivation of shifts. The CroCo-project (Hansen-Schirra et al, 2006) also works with German and English but has a larger scope. Complex queries based on the annotation for many types of shifts can be formulated, though so far only with detailed knowledge of the XML-format and the details of the annotation. The SMULTRON corpus (Volk et al, 2006; Samuelsson and Volk, 2006) includes data from three languages (English, German, and Swedish). The annotation is based on phrase structure analyses. This project is primarily oriented towards machine translation and the recognition of translation equivalents that can “serve as translations outside the current sentence co"
W07-2441,W06-2717,0,0.164854,"Missing"
W07-2441,cyrus-2006-building,0,\N,Missing
W08-0317,P05-1066,0,0.218385,"Missing"
W08-0317,W07-0723,1,0.860427,"amework using factored models (Koehn et al., 2007). Furthermore, by parsing input sentences and restructuring based on the result to narrow the structural difference between source and target language, the current phrase-based models can be used more effectively (Collins et al., 2005). German differs structurally from English in several respects (see e.g. Collins et al., 2005). In this work we wanted to look at one particular aspect of restructuring, namely splitting of German compounds, and evaluate its effect in both translation directions, thus extending the initial experiments reported in Holmqvist et al. (2007). In addition, since For both English and German we used the part-ofspeech tagger TreeTagger (Schmid, 1994) to obtain POS-tags. The German POS-tags from TreeTagger were refined by adding morphological information from a commercial dependency parser, including case, number, gender, definiteness, and person for nouns, pronouns, verbs, adjectives and determiners in the cases where both tools agreed on the POS-tag. If they did not agree, the POS-tag from TreeTagger was chosen. This tag set seemed more suitable for SMT, with tags for proper names and foreign words which the commercial parser does n"
W08-0317,E03-1076,0,0.706555,"und (Langer, 1998). We have chosen to allow simple additions of letter(s) (-s, -n, -en, -nen, -es, -er, -ien) and simple truncations (-e, 135 Proceedings of the Third Workshop on Statistical Machine Translation, pages 135–138, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics -en, -n). Example of compounds with additions and truncations can be seen in (1). (1) a. Staatsfeind (Staat + Feind) public enemy b. Kirchhof (Kirche + Hof) graveyard 3.1 Splitting compounds Noun and adjective compounds are split by a modified version of the corpus-based method presented by Koehn and Knight (2003). First the German language model data is POS-tagged and used to calculate frequencies of all nouns, verbs, adjectives, adverbs and the negative particle. Then, for each noun and adjective all splits into these known words from the corpus, allowing filler additions and truncations, are considered, choosing the splitting option with the highest arithmetic mean1 of the frequencies of its parts. A length limit of each part was set to 4 characters. For adjectives we restrict the number of parts to maximum two, since they do not tend to have multiple parts as often as nouns. In addition we added a"
W08-0317,P07-2045,0,0.0229417,"ce model for German using morphologically rich parts-of-speech. It is shown that these additions lead to improved translations. 2 Part-of-speech and Morphology 1 Introduction Research in statistical machine translation (SMT) increasingly makes use of linguistic analysis in order to improve performance. By including abstract categories, such as lemmas and parts-of-speech (POS), in the models, it is argued that systems can become better at handling sentences for which training data at the word level is sparse. Such categories can be integrated in the statistical framework using factored models (Koehn et al., 2007). Furthermore, by parsing input sentences and restructuring based on the result to narrow the structural difference between source and target language, the current phrase-based models can be used more effectively (Collins et al., 2005). German differs structurally from English in several respects (see e.g. Collins et al., 2005). In this work we wanted to look at one particular aspect of restructuring, namely splitting of German compounds, and evaluate its effect in both translation directions, thus extending the initial experiments reported in Holmqvist et al. (2007). In addition, since For bo"
W08-0317,P02-1040,0,0.0797884,"comparison, we constructed a baseline according to the shared-task description, but with smaller tuning corpus, and the same sentence filtering for the translation model as in the submitted system, using only sentences of length 2-40. In addition we constructed a factored baseline system, with POS as an output factor and a sequence model for POS. Here we only used the original POS-tags from TreeTagger, no additional morphology was added for German. 137 De-En 19.54 20.16 20.61 En-De 14.31 14.37 14.77 Table 2: Bleu scores for News Commentary (nc-test2007) 5 Results Case-sensitive Bleu scores4 (Papineni et al., 2002) for the Europarl devtest set (test2007) are shown in table 1. We can see that the submitted system performs best, and that the factored baseline is better than the pure baseline, especially for translation into English. Bleu scores for News Commentary5 (nc-test2007) are shown in Table 2. Here we can also see that the submitted system is the best. As expected, Bleu is much lower on out-of-domain news text than on the Europarl development test set. 5.1 Compounds The quality of compound translations were analysed manually. The first 100 compounds that could be found by the splitting algorithm we"
W09-0421,W08-0312,0,0.11068,"st, Sara Stymne, Jody Foo and Lars Ahrenberg Department of Computer and Information Science Linköping University, Sweden {marho,sarst,jodfo,lah}@ida.liu.se Abstract the first method in addition to the processing of compounds and additional sequence models used by Stymne et al. (2008). Heuristics were used to generate true-cased versions of the translations that were submitted, as reported in section 6. In this paper we report case-insensitive Bleu scores (Papineni et al., 2002), unless otherwise stated, calculated with the NIST tool, and caseinsensitive Meteor-ranking scores, without WordNet (Agarwal and Lavie, 2008). We describe the LIU systems for EnglishGerman and German-English translation in the WMT09 shared task. We focus on two methods to improve the word alignment: (i) by applying Giza++ in a second phase to a reordered training corpus, where reordering is based on the alignments from the first phase, and (ii) by adding lexical data obtained as highprecision alignments from a different word aligner. These methods were studied in the context of a system that uses compound processing, a morphological sequence model for German, and a partof-speech sequence model for English. Both methods gave some im"
W09-0421,P04-1023,0,0.0501952,"Missing"
W09-0421,P02-1040,0,0.0760101,"Missing"
W09-0421,P06-1097,0,0.0596532,"Missing"
W09-0421,J07-3002,0,0.0219777,"ystem of the workshop1 . The translation system is a factored phrasebased translation system that uses the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. Minimum error rate training was used to tune the model feature weights (Och, 2003). Tuning was performed on the news-dev2009a set with 1025 sentences. All development testing was performed on the news-dev2009b set with 1026 sentences. Introduction It is an open question whether improved word alignment actually improves statistical MT. Fraser and Marcu (2007) found that improved alignments as measured by AER will not necessarily improve translation quality, whereas Ganchev et al. (2008) did improve translation quality on several language pairs by extending the alignment algorithm. For this year’s shared task we therefore studied the effects of improving word alignment in the context of our system for the WMT09 shared task. Two methods were tried: (i) applying Giza++ in a second phase to a reordered training corpus, where reordering is based on the alignments from the first phase, and (ii) adding lexical data obtained as high-precision alignments f"
W09-0421,P08-1112,0,0.0267503,"l., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. Minimum error rate training was used to tune the model feature weights (Och, 2003). Tuning was performed on the news-dev2009a set with 1025 sentences. All development testing was performed on the news-dev2009b set with 1026 sentences. Introduction It is an open question whether improved word alignment actually improves statistical MT. Fraser and Marcu (2007) found that improved alignments as measured by AER will not necessarily improve translation quality, whereas Ganchev et al. (2008) did improve translation quality on several language pairs by extending the alignment algorithm. For this year’s shared task we therefore studied the effects of improving word alignment in the context of our system for the WMT09 shared task. Two methods were tried: (i) applying Giza++ in a second phase to a reordered training corpus, where reordering is based on the alignments from the first phase, and (ii) adding lexical data obtained as high-precision alignments from a different word aligner. The submitted system includes 2.1 Sequence model based on part-of-speech and morphology The translat"
W09-0421,W08-0317,1,0.862537,"sion alignments from a different word aligner. These methods were studied in the context of a system that uses compound processing, a morphological sequence model for German, and a partof-speech sequence model for English. Both methods gave some improvements to translation quality as measured by Bleu and Meteor scores, though not consistently. All systems used both out-ofdomain and in-domain data as the mixed corpus had better scores in the baseline configuration. 1 2 Baseline system Our baseline system uses compound splitting, compound merging and part-ofspeech/morphological sequence models (Stymne et al., 2008). Except for these additions it is similar to the baseline system of the workshop1 . The translation system is a factored phrasebased translation system that uses the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. Minimum error rate training was used to tune the model feature weights (Och, 2003). Tuning was performed on the news-dev2009a set with 1025 sentences. All development testing was performed on the news-dev2009b set with 1026 sentences. Introduction It is an open question whethe"
W09-0421,E03-1076,0,0.0342188,"he Fourth Workshop on Statistical Machine Translation , pages 120–124, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 120 dependency parser2 . We used the extra factor in an additional sequence model which can improve agreement between words, and word order. For German this factor was also used for compound merging. 2.2 Corpus news-commentary09 Europarl news-train08 Table 1: Number of sentences in the corpora (after filtering) Compound processing Prior to training and translation, compound processing was performed using an empirical method based on (Koehn and Knight, 2003; Stymne, 2008). Words were split if they could be split into parts that occur in a monolingual corpus. We chose the split with the highest arithmetic mean of the corpus frequencies of compound parts. We split nouns, adjectives and verbs into parts that were content words or particles. A part had to be at least 3 characters in length and a stop list was used to avoid parts that often lead to errors, such as arische (Aryan) in konsularische (consular). Compound parts sometimes have special compound suffixes, which could be additions or truncations of letters, or combinations of these. We used t"
W09-0421,W07-0733,0,0.040885,"improved the results drastically, as shown in Table 2. The other experiments reported in this paper are based on the mixed+ system. Domain adaptation This year three training corpora were available, a small bilingual news commentary corpus, a reasonably large Europarl corpus, and a very large monolingual news corpus, see Table 1 for details. The bilingual data was filtered to remove sentences longer than 60 words. Because the German news training corpus contained a number of English sentences, this corpus was cleaned by removing sentences containing a number of common English words. Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data 4 Improved alignment by reordering Word alignment with Giza++ has been shown to improve from making the source and target language more similar, e.g., in terms of segmentation (Ma et al., 2007) or word order. We used the following simple procedure to improve alignment of the training corpus by reordering the words in one of the texts according to the 2 Machinese syntax, from Connexor Oy http://www. connexor.eu 121 Corpus Mixed+ Re-Src Re-Trg En⇒De Bleu Meteor 14.62 49.48 14.63 49.80 14.51 48.62 D"
W09-0421,E09-3008,1,0.826008,"ten lead to errors, such as arische (Aryan) in konsularische (consular). Compound parts sometimes have special compound suffixes, which could be additions or truncations of letters, or combinations of these. We used the top 10 suffixes from a corpus study of Langer (1998), and we also treated hyphens as suffixes of compound parts. Compound parts were given a special part-of-speech tag that matched the head word. For translation into German, compound parts were merged to form compounds, both during test and tuning. The merging is based on the special part-of-speech tag used for compound parts (Stymne, 2009). A token with this POS-tag is merged with the next token, either if the POS-tags match, or if it results in a known word. 3 German English 81,141 1,331,262 9,619,406 21,215,311 Corpus News com. Europarl Mixed Mixed+ En⇒De Bleu Meteor 12.13 47.01 12.92 47.27 12.91 47.96 14.62 49.48 De⇒En Bleu Meteor 17.21 36.08 18.53 37.65 18.76 37.69 19.92 38.18 Table 2: Results of domain adaptation from the news domain. We used the possibility to include several translation models in the Moses decoder by using multiple alternative decoding paths. We first trained systems on either bilingual news data or Euro"
W09-0421,P07-2045,0,0.0133413,"e model for English. Both methods gave some improvements to translation quality as measured by Bleu and Meteor scores, though not consistently. All systems used both out-ofdomain and in-domain data as the mixed corpus had better scores in the baseline configuration. 1 2 Baseline system Our baseline system uses compound splitting, compound merging and part-ofspeech/morphological sequence models (Stymne et al., 2008). Except for these additions it is similar to the baseline system of the workshop1 . The translation system is a factored phrasebased translation system that uses the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. Minimum error rate training was used to tune the model feature weights (Och, 2003). Tuning was performed on the news-dev2009a set with 1025 sentences. All development testing was performed on the news-dev2009b set with 1026 sentences. Introduction It is an open question whether improved word alignment actually improves statistical MT. Fraser and Marcu (2007) found that improved alignments as measured by AER will not necessarily improve translation quality, whereas Ganchev e"
W09-0421,P07-1039,0,0.0319336,"gual news corpus, see Table 1 for details. The bilingual data was filtered to remove sentences longer than 60 words. Because the German news training corpus contained a number of English sentences, this corpus was cleaned by removing sentences containing a number of common English words. Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data 4 Improved alignment by reordering Word alignment with Giza++ has been shown to improve from making the source and target language more similar, e.g., in terms of segmentation (Ma et al., 2007) or word order. We used the following simple procedure to improve alignment of the training corpus by reordering the words in one of the texts according to the 2 Machinese syntax, from Connexor Oy http://www. connexor.eu 121 Corpus Mixed+ Re-Src Re-Trg En⇒De Bleu Meteor 14.62 49.48 14.63 49.80 14.51 48.62 De⇒En Bleu Meteor 19.92 38.18 20.54 38.86 20.48 38.73 5 Augmenting the corpus with an extracted dictionary Previous research (Callison-Burch et al., 2004; Fraser and Marcu, 2006) has shown that including word aligned data during training can improve translation results. In our case we include"
W09-0421,W07-2456,1,0.870679,"s according to the 2 Machinese syntax, from Connexor Oy http://www. connexor.eu 121 Corpus Mixed+ Re-Src Re-Trg En⇒De Bleu Meteor 14.62 49.48 14.63 49.80 14.51 48.62 De⇒En Bleu Meteor 19.92 38.18 20.54 38.86 20.48 38.73 5 Augmenting the corpus with an extracted dictionary Previous research (Callison-Burch et al., 2004; Fraser and Marcu, 2006) has shown that including word aligned data during training can improve translation results. In our case we included a dictionary extracted from the news-commentary corpus during the word alignment. Using a method originally developed for term extraction (Merkel and Foo, 2007), the newscommentary09 corpus was grammatically annotated and aligned using a heuristic word aligner. Candidate dictionary entries were extracted from the alignments. In order to optimize the quality of the dictionary, dictionary entry candidates were ranked according to their Q-value, a metric specifically designed for aligned data (Merkel and Foo, 2007). The Q-value is based on the following statistics: Table 3: Results of reordering experiments word order in the other language: 1. Word align the corpus with Giza++. 2. Reorder the German words according to the order of the English words they"
W09-0421,J03-1002,0,0.0173665,"ality as measured by Bleu and Meteor scores, though not consistently. All systems used both out-ofdomain and in-domain data as the mixed corpus had better scores in the baseline configuration. 1 2 Baseline system Our baseline system uses compound splitting, compound merging and part-ofspeech/morphological sequence models (Stymne et al., 2008). Except for these additions it is similar to the baseline system of the workshop1 . The translation system is a factored phrasebased translation system that uses the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. Minimum error rate training was used to tune the model feature weights (Och, 2003). Tuning was performed on the news-dev2009a set with 1025 sentences. All development testing was performed on the news-dev2009b set with 1026 sentences. Introduction It is an open question whether improved word alignment actually improves statistical MT. Fraser and Marcu (2007) found that improved alignments as measured by AER will not necessarily improve translation quality, whereas Ganchev et al. (2008) did improve translation quality on several language pairs by"
W09-0421,P03-1021,0,0.00739997,"rpus had better scores in the baseline configuration. 1 2 Baseline system Our baseline system uses compound splitting, compound merging and part-ofspeech/morphological sequence models (Stymne et al., 2008). Except for these additions it is similar to the baseline system of the workshop1 . The translation system is a factored phrasebased translation system that uses the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. Minimum error rate training was used to tune the model feature weights (Och, 2003). Tuning was performed on the news-dev2009a set with 1025 sentences. All development testing was performed on the news-dev2009b set with 1026 sentences. Introduction It is an open question whether improved word alignment actually improves statistical MT. Fraser and Marcu (2007) found that improved alignments as measured by AER will not necessarily improve translation quality, whereas Ganchev et al. (2008) did improve translation quality on several language pairs by extending the alignment algorithm. For this year’s shared task we therefore studied the effects of improving word alignment in the"
W10-1727,W08-0312,0,0.0748074,"Missing"
W10-1727,J03-1002,0,0.0213145,"sis of the alignments from Giza++, and (ii) reorder by moving all verbs to the end of segments. In translation, out-ofvocabulary words were preprocessed in a knowledge-lite fashion to identify a likely equivalent. All three strategies were implemented for our English↔German system submitted to the WMT10 shared task. Combining them lead to improvements in both language directions. 1 2 Baseline System The 2010 Liu system is based on the PBSMT baseline system for the WMT shared translation task1 . We use the Moses toolkit (Koehn et al., 2007) for decoding and to train translation models, Giza++ (Och and Ney, 2003) for word alignment, and the SRILM toolkit (Stolcke, 2002) to train language models. The main difference to the WMT baseline is that the Liu system is trained on truecased data, as in Koehn et al. (2008), instead of lowercased data. This means that there is no need for a full recasing step after translation, instead we only need to uppercase the first word in each sentence. Introduction We present the Liu translation system for the constrained condition of the WMT10 shared translation task, between German and English in both directions. The system is based on the 2009 Liu submission (Holmqvist"
W10-1727,P02-1040,0,0.0778775,"Missing"
W10-1727,W05-0908,0,0.0138745,"u.se Abstract step, based on casing, stemming, and splitting of hyphenated compounds. In addition, we perform general compound splitting for German both before training and translation, which also reduces the OOV rate. All results in this article are for the development test set newstest2009, on truecased output. We report Bleu scores (Papineni et al., 2002) and Meteor ranking (without WordNet) scores (Agarwal and Lavie, 2008), using percent notation. We also used other metrics, but as they gave similar results they are not reported. For significance testing we used approximate randomization (Riezler and Maxwell, 2005), with p &lt; 0.05. In this paper we report on experiments with three preprocessing strategies for improving translation output in a statistical MT system. In training, two reordering strategies were studied: (i) reorder on the basis of the alignments from Giza++, and (ii) reorder by moving all verbs to the end of segments. In translation, out-ofvocabulary words were preprocessed in a knowledge-lite fashion to identify a likely equivalent. All three strategies were implemented for our English↔German system submitted to the WMT10 shared task. Combining them lead to improvements in both language di"
W10-1727,W09-0421,1,0.932571,"ey, 2003) for word alignment, and the SRILM toolkit (Stolcke, 2002) to train language models. The main difference to the WMT baseline is that the Liu system is trained on truecased data, as in Koehn et al. (2008), instead of lowercased data. This means that there is no need for a full recasing step after translation, instead we only need to uppercase the first word in each sentence. Introduction We present the Liu translation system for the constrained condition of the WMT10 shared translation task, between German and English in both directions. The system is based on the 2009 Liu submission (Holmqvist et al., 2009), that used compound processing, morphological sequence models, and improved alignment by reordering. This year we have focused on two issues: translation of verbs, which is problematic for translation between English and German since the verb placement is different with German verbs often being placed at the end of sentences; and OOVs, outof-vocabulary words, which are problematic for machine translation in general. Verb translation is targeted by trying to improve alignment, which we believe is a crucial step for verb translation since verbs that are far apart are often not aligned at all. W"
W10-1727,C08-1098,0,0.0647831,"Missing"
W10-1727,E03-1076,0,0.0501394,"ms instead of 7-grams for the morphological models. It was still impossible to train on the monolingual English news corpus, with nearly 50 million sentences, so we split that corpus into three equal size parts, and trained three models, that were interpolated with equal weights. 3 Bleu 13.42 13.85 14.24 Bleu 18.34 18.39 18.50 Meteor 38.13 37.86 38.47 Table 2: Results for morphological processing, German→English and agreement between words. For German the factor was also used for compound merging. Prior to training and translation, compound processing was performed, using an empirical method (Koehn and Knight, 2003; Stymne, 2008) that splits words if they can be split into parts that occur in a monolingual corpus, choosing the splitting option with the highest arithmetic mean of its part frequencies in the corpus. We split nouns, adjectives and verbs, into parts that are content words or particles. We imposed a length limit on parts of 3 characters for translation from German and of 6 characters for translation from English, and we had a stop list of parts that often led to errors, such as arische (Aryan) in konsularische (consular). We allowed 10 common letter changes (Langer, 1998) and hyphens at spli"
W10-1727,W07-0733,0,0.0247001,"he translation and reordering models were trained using the bilingual Europarl and news commentary corpora, which we concatenated. We used two sets of language models, one where we first trained two models on Europarl and news commentary, which we then interpolated 1 http://www.statmt.org/wmt10/baseline. html 183 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 183–188, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics System Baseline + morph + comp with more weight given to the news commentary, using weights from Koehn and Schroeder (2007). The second set of language models were trained on monolingual news data. For tuning we used every second sentence, in total 1025 sentences, of news-test2008. 2.2 Meteor 48.83 49.69 49.41 Table 1: Results for morphological processing, English→German Training with Limited Computational Resources System Baseline + morph + comp One challenge for us was to train the translation sytem with limited computational resources. We trained all systems on one Intel Core 2 CPU, 3.0Ghz, 16 Gb of RAM, 64 bit Linux (RedHat) machine. This constrained the possibilities of using the data provided by the workshop"
W10-1727,2008.eamt-1.25,1,0.927718,"orpus. We split nouns, adjectives and verbs, into parts that are content words or particles. We imposed a length limit on parts of 3 characters for translation from German and of 6 characters for translation from English, and we had a stop list of parts that often led to errors, such as arische (Aryan) in konsularische (consular). We allowed 10 common letter changes (Langer, 1998) and hyphens at split points. Compound parts were given a special part-of-speech tag that matches the head word. For translation into German, compound parts were merged into full compounds using a method described in Stymne and Holmqvist (2008), which is based on matching of the special part-of-speech tag for compound parts. A word with a compound POS-tag were merged with the next word, if their POS-tags were matching. Tables 1 and 2 show the results of the additional morphological processing. Adding the sequence models on morphologically enriched partof-speech tags gave a significant improvement for translation into German, but similar or worse results as the baseline for translation into English. This is not surprising, since German morphology is more complex than English morphology. The addition of compound processing significant"
W10-1727,W08-0318,0,0.0524119,"equivalent. All three strategies were implemented for our English↔German system submitted to the WMT10 shared task. Combining them lead to improvements in both language directions. 1 2 Baseline System The 2010 Liu system is based on the PBSMT baseline system for the WMT shared translation task1 . We use the Moses toolkit (Koehn et al., 2007) for decoding and to train translation models, Giza++ (Och and Ney, 2003) for word alignment, and the SRILM toolkit (Stolcke, 2002) to train language models. The main difference to the WMT baseline is that the Liu system is trained on truecased data, as in Koehn et al. (2008), instead of lowercased data. This means that there is no need for a full recasing step after translation, instead we only need to uppercase the first word in each sentence. Introduction We present the Liu translation system for the constrained condition of the WMT10 shared translation task, between German and English in both directions. The system is based on the 2009 Liu submission (Holmqvist et al., 2009), that used compound processing, morphological sequence models, and improved alignment by reordering. This year we have focused on two issues: translation of verbs, which is problematic for"
W10-1727,N04-1022,0,0.0215819,"reference The best and most technically well-equipped telephones, with a 3.5 mm jack for ordinary headphones, cost up to fifteen thousand crowns. Figure 1: Example of the effects of OOV processing for German→English System base + OOV base+verb + OOV + MBR Bleu 14.24 14.26 14.38 14.42 14.41 Meteor 49.41 49.43 49.72 49.75 49.77 6 For the final Liu shared task submission we used the base+verb+reorder+OOV system for German→English and the base+verb+OOV system for English→German, which had the best overall scores considering all metrics. To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004). In standard decoding, the top suggestion of the translation system is chosen as the system output. In MBR decoding the risk is spread by choosing the translation that is most similar to the N highest scoring translation suggestions from the system, with N = 100, as suggested in Koehn et al. (2008). MBR decoding gave hardly any changes in automatic scores, as shown in Tables 6 and 7. The final system was significantly better than the baseline in all cases, and significantly better than base on Meteor in both translation directions, and on Bleu for translation into English. Table 6: Results fo"
W10-1727,D07-1077,0,0.0206414,"logical sequence models. Overall, we believe that both compound splitting and morphology are useful; thus all experiments reported in the sequel are based on the baseline system with morphology models and compound splitting, which we will call base. 4 Meteor 49.41 49.58 49.22 49.72 49.39 Table 3: Results for improved alignment, English→German System base reorder verb base+verb base+verb+reorder Improved Alignment by Reordering Previous work has shown that translation quality can be improved by making the source language more similar to the target language, for instance in terms of word order (Wang et al., 2007; Xia and McCord, 2004). In order to harmonize the word order of the source and target sentence, they applied hand-crafted or automatically induced reordering rules to the source sentences of the training corpus. At decoding time, reordering rules were again applied to input sentences before translation. The positive effects of such methods seem to come from a combination of improved alignment and improved reordering during translation. In contrast, we focus on improving the word alignment by reordering the training corpus. The training corpus is reordered prior to word alignment with Giza++ ("
W10-1727,C04-1073,0,0.0651508,"dels. Overall, we believe that both compound splitting and morphology are useful; thus all experiments reported in the sequel are based on the baseline system with morphology models and compound splitting, which we will call base. 4 Meteor 49.41 49.58 49.22 49.72 49.39 Table 3: Results for improved alignment, English→German System base reorder verb base+verb base+verb+reorder Improved Alignment by Reordering Previous work has shown that translation quality can be improved by making the source language more similar to the target language, for instance in terms of word order (Wang et al., 2007; Xia and McCord, 2004). In order to harmonize the word order of the source and target sentence, they applied hand-crafted or automatically induced reordering rules to the source sentences of the training corpus. At decoding time, reordering rules were again applied to input sentences before translation. The positive effects of such methods seem to come from a combination of improved alignment and improved reordering during translation. In contrast, we focus on improving the word alignment by reordering the training corpus. The training corpus is reordered prior to word alignment with Giza++ (Och and Ney, 2003) and"
W10-1727,P09-1089,0,0.0283,"Missing"
W10-1727,P07-2045,0,\N,Missing
W10-2317,P02-1033,0,0.109297,"Missing"
W11-2147,W08-0312,0,0.0472087,"Missing"
W11-2147,P05-1066,0,0.22974,"Missing"
W11-2147,2010.iwslt-papers.6,0,0.0176481,"as measured by NIST and Meteor. For translation in the other direction, the differences on automatic metrics were very small. Still, we decided to use the hierarchical model in all our systems. 3 German–English For translation from German into English we focused on making the German source text more similar to English by removing redundant morphology and changing word order before training translation models. 3.1 Normalization We performed normalization of German words to remove distinctions that do not exist in English, such as case distinctions on nouns. This strategy is similar to that of El-Kahlout and Yvon (2010), but we used a slightly different set of transformations, that we thought better mirrored the English structure. For morphological tags we used RFTagger and for lemmas we used TreeTagger. The morphological transformations we performed were the following: • Nouns: – Replace with lemma+s if plural number – Replace with lemma otherwise • Verbs: – Replace with lemma if present tense, not third person singular – Replace with lemma+p if past tense Baseline +hier reo +normalization +source reordering + OOV proc. BLEU 21.01 20.94 20.85 21.06 21.22 NIST 6.2742 6.2800 6.2370 6.3082 6.3692 Meteor 41.32"
W11-2147,D08-1089,0,0.0171223,"mpound. For translation into German, compounds were 394 merged using the POS-merging strategy of Stymne (2009). A compound part in the translation output, identified by the special part-of-speech tags, was merged with the next word if that word had a matching part-of-speech tag. If the compound part was followed by the conjunction und (and), we added a hyphen to the part, to account for coordinated compounds. 2.3 Hierarchical reordering In our baseline system we experimented with two lexicalized reordering models. The standard model in Moses (Koehn et al., 2005), and the hierarchical model of Galley and Manning (2008). In both models the placement of a phrase is compared to that of the previous and/or next phrase. In the standard model up to three reorderings are distinguished, monotone, swap, and discontinuous. In the hierarchical model the discontinuous class can be further subdivided into two classes, left and right discontinuous. The hierarchical model further differs from the standard model in that it compares the order of the phrase with the next or previous block of phrases, not only with the next or previous single phrase. We investigated one configuration of each model. For the standard model we u"
W11-2147,W11-2123,0,0.0163588,"slation we also added morphological normalization, source side reordering, and processing of out-of-vocabulary words (OOVs). For English–German translation, we extracted word alignments with a supervised method and combined these alignments with Giza++ alignments in various Baseline System This years improvements were added to the LIU baseline system (Stymne et al., 2010). Our baseline is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al., 2007) for translation model training and decoding, GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights. In addition, the LIU baseline contains: • Compound processing, including compound splitting and for translation into German also compound merging • Part-of-speech and morphological sequence models All models were trained on truecased data. Translation and reordering models were trained using the bilingual Europarl and News Commentary corpora that were concatenated before training. We created two language models. The first model is a 5-gram model that we created by interpolating two language 393 P"
W11-2147,holmqvist-2010-heuristic,1,0.849719,"ee that while the normalization did not seem to have a positive effect on any metric, both source reordering and OOV processing led to small increases on all scores. 4 English–German For translation from English into German we attempted to improve the quality of the phrase table by adding new word alignments to the standard Giza++ alignments. 4.1 Phrase-based word alignment We experimented with different ways of combining word alignments from Giza++ with alignments created using phrase-based word alignment (PAL) which previously has been shown to improve alignment quality for English–Swedish (Holmqvist, 2010). The idea of phrase-based word alignment is to use word and part-of-speech sequence patterns from manual word alignments to align new texts. First, parallel phrases containing a source segment, a target segment and links between source and target words are extracted from word aligned texts (Figure 1). In the second step, these phrases are matched against new parallel text and if a matching phrase is found, word links from the phrase are added to the corresponding words in the new text. In order to increase the number of matching phrases and improve word alignment recall, words in the parallel"
W11-2147,E03-1076,0,0.0364454,"s were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs. We trained two sequence models for each system over this output factor and added them as features in our baseline system. The first sequence model is a 7-gram model interpolated from models of bilingual Europarl and News Commentary. The second model is a 6-gram model trained on monolingual News only. 2.2 Compound processing In both translation directions we split compounds, using a modified version of the corpus-based splitting method of Koehn and Knight (2003). We split nouns, verb, and adjective compounds into known parts that were content words or cardinal numbers, based on the arithmetic mean of the frequency of the parts in the training corpus. We allowed 10 common letter changes (Langer, 1998) and hyphens at split points. Compound parts were kept in their surface form and compound modifiers received a partof-speech tag based on that of the tag of the full compound. For translation into German, compounds were 394 merged using the POS-merging strategy of Stymne (2009). A compound part in the translation output, identified by the special part-of-"
W11-2147,2005.iwslt-1.8,0,0.024884,"-speech tag based on that of the tag of the full compound. For translation into German, compounds were 394 merged using the POS-merging strategy of Stymne (2009). A compound part in the translation output, identified by the special part-of-speech tags, was merged with the next word if that word had a matching part-of-speech tag. If the compound part was followed by the conjunction und (and), we added a hyphen to the part, to account for coordinated compounds. 2.3 Hierarchical reordering In our baseline system we experimented with two lexicalized reordering models. The standard model in Moses (Koehn et al., 2005), and the hierarchical model of Galley and Manning (2008). In both models the placement of a phrase is compared to that of the previous and/or next phrase. In the standard model up to three reorderings are distinguished, monotone, swap, and discontinuous. In the hierarchical model the discontinuous class can be further subdivided into two classes, left and right discontinuous. The hierarchical model further differs from the standard model in that it compares the order of the phrase with the next or previous block of phrases, not only with the next or previous single phrase. We investigated one"
W11-2147,J03-1002,0,0.00546384,"ce models, and a hierarchical reordering model. For German–English translation we also added morphological normalization, source side reordering, and processing of out-of-vocabulary words (OOVs). For English–German translation, we extracted word alignments with a supervised method and combined these alignments with Giza++ alignments in various Baseline System This years improvements were added to the LIU baseline system (Stymne et al., 2010). Our baseline is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al., 2007) for translation model training and decoding, GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights. In addition, the LIU baseline contains: • Compound processing, including compound splitting and for translation into German also compound merging • Part-of-speech and morphological sequence models All models were trained on truecased data. Translation and reordering models were trained using the bilingual Europarl and News Commentary corpora that were concatenated before training. We created two language models. The first model i"
W11-2147,P03-1021,0,0.0310854,"g, and processing of out-of-vocabulary words (OOVs). For English–German translation, we extracted word alignments with a supervised method and combined these alignments with Giza++ alignments in various Baseline System This years improvements were added to the LIU baseline system (Stymne et al., 2010). Our baseline is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al., 2007) for translation model training and decoding, GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights. In addition, the LIU baseline contains: • Compound processing, including compound splitting and for translation into German also compound merging • Part-of-speech and morphological sequence models All models were trained on truecased data. Translation and reordering models were trained using the bilingual Europarl and News Commentary corpora that were concatenated before training. We created two language models. The first model is a 5-gram model that we created by interpolating two language 393 Proceedings of the 6th Workshop on Statistical Machine Translation,"
W11-2147,P06-1146,0,0.0179087,"ks: 0-0 1-1 2-2 En: DT JJ NN De: ART ADJA N Links: 0-0 1-1 2-2 Figure 1: Examples of parallel phrases used in word alignment. Baseline +hier reo +pal-gdfa +pal-dual +pal-inter BLEU 16.16 16.06 16.14 15.71 15.92 NIST 6.2742 6.2800 5.6527 5.5735 5.6230 Meteor 50.89 51.25 51.10 50.43 50.73 Table 2: English–German translation results, results are cumulative except for the three alternative PALconfigurations. segments were replaced by POS/morphological tags from RFTagger. Alignment patterns were extracted from 1000 sentences in the manually word aligned sample of English–German Europarl texts from Pado and Lapata (2006). All parallel phrases were extracted from the word aligned texts, as when extracting a translation model. Parallel phrases that contain at least 3 words were generalized with POS tags to form word/POS patterns for alignment. A subset of these patterns, with high alignment precision (&gt; 0.80) on the 1000 sentences, were used to align the entire training corpus. We combined the new word alignments with the Giza++ alignments in two ways. In the first method, we used a symmetrization heuristic similar to grow-diag-final-and to combine three word alignments into one, the phrase-based alignment and"
W11-2147,P02-1040,0,0.0796483,"Missing"
W11-2147,C08-1098,0,0.0130567,"newstest2009 and all results reported in Tables 1 and 2 are based on a devtest set of 1244 sentences from newstest2010. 2.1 Sequence models with part-of-speech and morphology To improve target word order and agreement in the translation output, we added an extra output factor in our translation models consisting of tags with POS and morphological features. For English we used tags that were obtained by enriching POS tags from TreeTagger (Schmid, 1994) with additional morphological features such as number for determiners. For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs. We trained two sequence models for each system over this output factor and added them as features in our baseline system. The first sequence model is a 7-gram model interpolated from models of bilingual Europarl and News Commentary. The second model is a 6-gram model trained on monolingual News only. 2.2 Compound processing In both translation directions we split compounds, using a modified version of the corpus-based splitting method of Koehn and Knight (2003). We split nouns, verb, and ad"
W11-2147,W10-1727,1,0.79791,"ty of German compared to English, as well as dealing with previously unseen words. In both translation directions our systems include compound processing, morphological sequence models, and a hierarchical reordering model. For German–English translation we also added morphological normalization, source side reordering, and processing of out-of-vocabulary words (OOVs). For English–German translation, we extracted word alignments with a supervised method and combined these alignments with Giza++ alignments in various Baseline System This years improvements were added to the LIU baseline system (Stymne et al., 2010). Our baseline is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al., 2007) for translation model training and decoding, GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights. In addition, the LIU baseline contains: • Compound processing, including compound splitting and for translation into German also compound merging • Part-of-speech and morphological sequence models All models were trained on truecased data. Translation and reor"
W11-2147,E09-3008,1,0.833722,"ds, using a modified version of the corpus-based splitting method of Koehn and Knight (2003). We split nouns, verb, and adjective compounds into known parts that were content words or cardinal numbers, based on the arithmetic mean of the frequency of the parts in the training corpus. We allowed 10 common letter changes (Langer, 1998) and hyphens at split points. Compound parts were kept in their surface form and compound modifiers received a partof-speech tag based on that of the tag of the full compound. For translation into German, compounds were 394 merged using the POS-merging strategy of Stymne (2009). A compound part in the translation output, identified by the special part-of-speech tags, was merged with the next word if that word had a matching part-of-speech tag. If the compound part was followed by the conjunction und (and), we added a hyphen to the part, to account for coordinated compounds. 2.3 Hierarchical reordering In our baseline system we experimented with two lexicalized reordering models. The standard model in Moses (Koehn et al., 2005), and the hierarchical model of Galley and Manning (2008). In both models the placement of a phrase is compared to that of the previous and/or"
W11-2147,P07-2045,0,\N,Missing
W11-4615,E03-1086,1,0.86602,"ignment error rate of 0, which makes sense since both are perfectly valid human alignments. Table 4 also contains the result of evaluating each annotation with the other as reference which resulted in very low error rates. link or a null link. Words are the preferred unit of alignment and phrase alignments should be as small as possible. Training data alignment guidelines are similar to the Blinker guidelines although null links are used more often to avoid large phrase alignments. The training data was aligned by one annotator. 4.4 The manual alignments were produced using two tools, I*Link (Ahrenberg et al., 2003) and the Alpaco_sp editor2 (Lambert et al., 2005). I*Link is a a tool for interactive word alignment that simplifies the manual alignment process by suggesting alignments which the user can choose to either accept or reject. The user can also override suggested alignments and add new ones. This method of alignment is relatively fast but I*link can not distinguish between possible and sure links, and it does not allow alignment of discontinuous segments such as when the two parts of a particle verb låser upp (Eng. unlock) are separated by a pronoun as in låser du upp – you unlock. The Alpaco_sp"
W11-4615,W07-2441,1,0.934566,"allel syntactic trees that have manual alignments between corresponding words and phrases as well as between subtrees. The added effort of verifying syntactic structure and aligning subtrees makes treebanks even more labor-intensive to produce than alignment gold standards. However, word alignments from large parallel treebanks such as the English-Arabic treebank from LDC are also used to train and evaluate word alignment systems, (e.g., Gao et al., 2010). Currently, available resources for English– Swedish word alignment include two parallel treebanks, Smultron (Volk et al., 2009) and LinES (Ahrenberg, 2007). Smultron is a multi-lingual treebank consisting of 1500 sentences from three domains with subsentential alignments. The Smultron alignment guidelines are similar to our test data guidelines where two types of links are used, one for regular links and one for more fuzzy correspondences. LinES is an English–Swedish treebank containing 2400 sentences from four subcorpora. This treebank was primarily designed to investigate and measure the occurrence of translation shifts and the word alignments in LinES are sparse. Furthermore, LinES is not an open resource, but it can be queried through a web"
W11-4615,N06-1013,0,0.120673,"xts. Word alignment forms the basis of (phrasebased) statistical machine translation (PBSMT) but alignments are also used in other data-driven approaches to machine translation to extract bilingual dictionaries and learn translation rules. The task of identifying corresponding words in a parallel text is difficult and manual word alignment can be time-consuming. Unsupervised methods for automatic word alignment have dominated the machine translation field (Och and Ney, 2003), but an increasing amount of research is devoted to improving word alignment quality through supervised training (e.g., Ayan and Dorr, 2006; Blunsom and Cohn, 2006; Ittycheriah and Roukos, 2005). Supervised methods require a set of high quality alignments to train the parameters of a discriminative word alignment system. These alignments are often hand-made alignment gold standards. Gold standards are also an important resource for evaluation of word alignment accuracy. In this paper, we present an English–Swedish word alignment gold standard. It consists of 1164 sentence pairs divided into a training set and a test set. The training set was produced to be used as training data for supervised word alignment (Holmqvist, 2010) and"
W11-4615,P06-1009,0,0.0468688,"orms the basis of (phrasebased) statistical machine translation (PBSMT) but alignments are also used in other data-driven approaches to machine translation to extract bilingual dictionaries and learn translation rules. The task of identifying corresponding words in a parallel text is difficult and manual word alignment can be time-consuming. Unsupervised methods for automatic word alignment have dominated the machine translation field (Och and Ney, 2003), but an increasing amount of research is devoted to improving word alignment quality through supervised training (e.g., Ayan and Dorr, 2006; Blunsom and Cohn, 2006; Ittycheriah and Roukos, 2005). Supervised methods require a set of high quality alignments to train the parameters of a discriminative word alignment system. These alignments are often hand-made alignment gold standards. Gold standards are also an important resource for evaluation of word alignment accuracy. In this paper, we present an English–Swedish word alignment gold standard. It consists of 1164 sentence pairs divided into a training set and a test set. The training set was produced to be used as training data for supervised word alignment (Holmqvist, 2010) and the test set was created"
W11-4615,P06-1097,0,0.248623,"y can be made if the computed alignment lacks a sure link (6) and precision errors only when a computed link is not even a possible link in the reference (7). Sure links are by definition also possible. AER(A, P, S) = 1 − |S ∩ A |+ |P ∩ A| |S |+ |A| Recall(A, S) = |S ∩ A| |S| Precision(A, P) = |P ∩ A| |A| (5) (6) (7) 5.1 Word Alignment and SMT Researchers in statistical MT want to improve word alignment in order to produce better translations. However, several studies have shown that improvements in terms of AER often fail to result in improved translation quality, (e.g., Ayan and Dorr, 2006; Fraser and Marcu, 2006). Translation quality can be measured in terms of the Bleu metric (Papineni et al., 2001). One reason for this lack of correlation between intrinsic and extrinsic evaluation measures is that AER favours highprecision alignments. Fraser and Marcu (2006) found that although precision is important for translation systems trained on small corpora, the importance of recall increases as the amount of data grows and alignment quality improves. In a standard PBSMT system, word alignments control which phrases are extracted as possible translations. A sparse, high-precision alignment is more ambiguous"
W11-4615,P91-1023,0,0.601563,"Missing"
W11-4615,W10-1701,0,0.012989,"h (Lambert et al., 2005). For some language pairs, parallel resources have been developed in the form of parallel treebanks. Parallel treebanks consist of parallel syntactic trees that have manual alignments between corresponding words and phrases as well as between subtrees. The added effort of verifying syntactic structure and aligning subtrees makes treebanks even more labor-intensive to produce than alignment gold standards. However, word alignments from large parallel treebanks such as the English-Arabic treebank from LDC are also used to train and evaluate word alignment systems, (e.g., Gao et al., 2010). Currently, available resources for English– Swedish word alignment include two parallel treebanks, Smultron (Volk et al., 2009) and LinES (Ahrenberg, 2007). Smultron is a multi-lingual treebank consisting of 1500 sentences from three domains with subsentential alignments. The Smultron alignment guidelines are similar to our test data guidelines where two types of links are used, one for regular links and one for more fuzzy correspondences. LinES is an English–Swedish treebank containing 2400 sentences from four subcorpora. This treebank was primarily designed to investigate and measure the o"
W11-4615,holmqvist-2010-heuristic,1,0.854436,", Ayan and Dorr, 2006; Blunsom and Cohn, 2006; Ittycheriah and Roukos, 2005). Supervised methods require a set of high quality alignments to train the parameters of a discriminative word alignment system. These alignments are often hand-made alignment gold standards. Gold standards are also an important resource for evaluation of word alignment accuracy. In this paper, we present an English–Swedish word alignment gold standard. It consists of 1164 sentence pairs divided into a training set and a test set. The training set was produced to be used as training data for supervised word alignment (Holmqvist, 2010) and the test set was created for the purpose of word alignment evaluation. The test set alignments have confidence labels for ambiguous links in order to be able to calculate more fine-grained evaluation measures. The gold standard and alignment guidelines can be downloaded from http://www.ida.liu.se/˜nlplab/ges . Alignments are stored in NAACL format (Mihalcea and Pedersen, 2003). This paper is organized as follows. In Section 2 we review available gold standards for English– Swedish and compare them to our newly created resource. The selection of parallel texts is described in Section 3 and"
W11-4615,H05-1012,0,0.0206569,"ebased) statistical machine translation (PBSMT) but alignments are also used in other data-driven approaches to machine translation to extract bilingual dictionaries and learn translation rules. The task of identifying corresponding words in a parallel text is difficult and manual word alignment can be time-consuming. Unsupervised methods for automatic word alignment have dominated the machine translation field (Och and Ney, 2003), but an increasing amount of research is devoted to improving word alignment quality through supervised training (e.g., Ayan and Dorr, 2006; Blunsom and Cohn, 2006; Ittycheriah and Roukos, 2005). Supervised methods require a set of high quality alignments to train the parameters of a discriminative word alignment system. These alignments are often hand-made alignment gold standards. Gold standards are also an important resource for evaluation of word alignment accuracy. In this paper, we present an English–Swedish word alignment gold standard. It consists of 1164 sentence pairs divided into a training set and a test set. The training set was produced to be used as training data for supervised word alignment (Holmqvist, 2010) and the test set was created for the purpose of word alignm"
W11-4615,P07-2045,0,0.00528565,"Missing"
W11-4615,2005.mtsummit-papers.11,0,0.20886,"Missing"
W11-4615,2009.mtsummit-posters.12,0,0.116695,"ser and Marcu (2006) found that although precision is important for translation systems trained on small corpora, the importance of recall increases as the amount of data grows and alignment quality improves. In a standard PBSMT system, word alignments control which phrases are extracted as possible translations. A sparse, high-precision alignment is more ambiguous and phrase extraction heuristics will extract more alternative phrase translations. Especially for systems trained on small corpora the many alternative translations in the phrase table seem to be beneficial to translation quality (Lambert et al., 2009). The connection between word alignment and phrase extraction suggests that other alignment characteristics than alignment precision and recall might be important for extracting the right phrases. For instance, correctly aligned discontinuous phrases such as German particle verbs can prevent the extraction of useful phrases from embedded words and removing these (correct) links improved translation quality (Vilar et al., 2006). A better correlation between intrinsic measures of alignment quality and translation quality have been found by having a large proportion of S links in the reference (L"
W11-4615,macken-2010-annotation,0,0.0895537,"on 6 we use our gold standard reference alignment to compare intrinsic evaluation with extrinsic evaluation in a phrase-based statistical machine translation system. Finally, Section 7 contains conclusions and directions for future work. Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 106–113 A Gold Standard for English–Swedish Word Alignment 2 Related work Gold standards consisting of parallel text with manually annotated word alignments exist for several language pairs including English–French (Och and Ney, 2003), Dutch–English (Macken, 2010) and English–Spanish (Lambert et al., 2005). For some language pairs, parallel resources have been developed in the form of parallel treebanks. Parallel treebanks consist of parallel syntactic trees that have manual alignments between corresponding words and phrases as well as between subtrees. The added effort of verifying syntactic structure and aligning subtrees makes treebanks even more labor-intensive to produce than alignment gold standards. However, word alignments from large parallel treebanks such as the English-Arabic treebank from LDC are also used to train and evaluate word alignme"
W11-4615,W03-0301,0,0.316565,"we present an English–Swedish word alignment gold standard. It consists of 1164 sentence pairs divided into a training set and a test set. The training set was produced to be used as training data for supervised word alignment (Holmqvist, 2010) and the test set was created for the purpose of word alignment evaluation. The test set alignments have confidence labels for ambiguous links in order to be able to calculate more fine-grained evaluation measures. The gold standard and alignment guidelines can be downloaded from http://www.ida.liu.se/˜nlplab/ges . Alignments are stored in NAACL format (Mihalcea and Pedersen, 2003). This paper is organized as follows. In Section 2 we review available gold standards for English– Swedish and compare them to our newly created resource. The selection of parallel texts is described in Section 3 and the guidelines for manual word alignment are motivated and exemplified in Section 4. We then review recent research on word alignment evaluation in Section 5. In Section 6 we use our gold standard reference alignment to compare intrinsic evaluation with extrinsic evaluation in a phrase-based statistical machine translation system. Finally, Section 7 contains conclusions and direct"
W11-4615,J03-1002,0,0.20236,"parallel text is to perform word alignment and determine which words and phrases are translations of each other in the source and target texts. Word alignment forms the basis of (phrasebased) statistical machine translation (PBSMT) but alignments are also used in other data-driven approaches to machine translation to extract bilingual dictionaries and learn translation rules. The task of identifying corresponding words in a parallel text is difficult and manual word alignment can be time-consuming. Unsupervised methods for automatic word alignment have dominated the machine translation field (Och and Ney, 2003), but an increasing amount of research is devoted to improving word alignment quality through supervised training (e.g., Ayan and Dorr, 2006; Blunsom and Cohn, 2006; Ittycheriah and Roukos, 2005). Supervised methods require a set of high quality alignments to train the parameters of a discriminative word alignment system. These alignments are often hand-made alignment gold standards. Gold standards are also an important resource for evaluation of word alignment accuracy. In this paper, we present an English–Swedish word alignment gold standard. It consists of 1164 sentence pairs divided into a"
W11-4615,2001.mtsummit-papers.68,0,0.035819,"en a computed link is not even a possible link in the reference (7). Sure links are by definition also possible. AER(A, P, S) = 1 − |S ∩ A |+ |P ∩ A| |S |+ |A| Recall(A, S) = |S ∩ A| |S| Precision(A, P) = |P ∩ A| |A| (5) (6) (7) 5.1 Word Alignment and SMT Researchers in statistical MT want to improve word alignment in order to produce better translations. However, several studies have shown that improvements in terms of AER often fail to result in improved translation quality, (e.g., Ayan and Dorr, 2006; Fraser and Marcu, 2006). Translation quality can be measured in terms of the Bleu metric (Papineni et al., 2001). One reason for this lack of correlation between intrinsic and extrinsic evaluation measures is that AER favours highprecision alignments. Fraser and Marcu (2006) found that although precision is important for translation systems trained on small corpora, the importance of recall increases as the amount of data grows and alignment quality improves. In a standard PBSMT system, word alignments control which phrases are extracted as possible translations. A sparse, high-precision alignment is more ambiguous and phrase extraction heuristics will extract more alternative phrase translations. Espec"
W11-4615,2006.iwslt-papers.7,0,0.0617321,"Missing"
W13-5614,C94-2167,0,0.257351,"plications share the constructive nature of the activity, and the need to distinguish terms from non-terms, or, if we prefer domain-specific terms from general vocabulary (Justeson & Katz, 1995). The automatic term extraction process will usually be followed by a manual, sometimes computer-aided, process of validation. For this reason, the outputs of a term extraction process are better referred to as term candidates of which some, after the validation process, may be elevated to term status. Early work on term extraction focused on linguistic analysis and POS patterns (e.g. Bourigault, 1992; Ananiadou 1994) for identifying term patterns. Later, these methods have been complemented by using more statistical measures, especially for ranking and filtering of the results (e.g. Zhang 2008; Merkel & Foo 2007). Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 122 of 474] 2 The term extraction tool (IPhraxtor) The tool presented here is in a way a standard term extraction tool relying mainly on the linguistic data. In this experiment the Connexor Machinese Syntax software (Tapanainen & Järvinen, 1997) has been u"
W13-5614,el-hadi-etal-2006-terminological,0,0.0601346,"Missing"
W13-5614,R09-1055,0,0.0302093,"Missing"
W13-5614,A97-1011,0,0.0643701,"(e.g. Bourigault, 1992; Ananiadou 1994) for identifying term patterns. Later, these methods have been complemented by using more statistical measures, especially for ranking and filtering of the results (e.g. Zhang 2008; Merkel & Foo 2007). Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 122 of 474] 2 The term extraction tool (IPhraxtor) The tool presented here is in a way a standard term extraction tool relying mainly on the linguistic data. In this experiment the Connexor Machinese Syntax software (Tapanainen & Järvinen, 1997) has been used, but in principle any linguistic analyzer for any language could be used as long as the analysed texts conform to the xml dtd used. When texts are analyzed using the Connexor Machinese Syntax framework, information on various levels are produced such as base form, morpho-syntactic description (number, definiteness, case, etc.), syntactic function and dependency relation. The output data from the text analysis could look something like the following for the sentence “8. Increasing or decreasing the viscosity of the cement slurry.”: Input sentence: 8. Increasing or decreasing the"
W13-5614,zhang-etal-2008-comparative,0,0.013709,"ription of the corpus used in the evaluation as well as different configurations that were tested. Results for precision and recall are presented and discussed. The final section before the conclusions contains a description of how the extraction tool has been used in real world situations in the initial stages of creating term banks for Swedish government agencies. The paper ends with conclusions and a discussion of the results. 1.1 Background Automatic term extraction is a field within language technology that deals with “extraction of technical terms from domain-specific language corpora” (Zhang et al., 2008). All approaches within automatic term extraction involve some kind of text analysis, some method of selecting and filtering term candidates. Typically the text analysis stages requires some kind of parts-ofspeech analysis and lemmatization of inflected word forms, but it could also involve more complex analysis like dependency relations between tokens in the input text. Term exctraction may have different applications in areas such as construction of ontologies, document indexing, validation of translation memories, and even classical terminology work. All the different applications share the"
W13-5614,W07-2456,1,\N,Missing
W14-1805,N06-1014,0,0.138347,"Missing"
W14-1805,W13-1740,0,0.0197441,"tutoring system designed to help student translators learn to appreciate the distinction between literal and liberal translation”. Their system allows students to compare their own translations with reference translations and have them classified in terms of categories such as literal, semantic, and communicative. The comparisons are made one sentence at a time, using the Dice coefficient, i.e., by treating the sentences as bags of words. Our proposal, in contrast, uses more advanced computational linguistics tools and provides text level assessment based on word alignment. Michaud and McCoy (2013) describe a system and a study where the goal, as in our proposal, is to develop automatic support for translator training. They focus on the inverted TERp metric (Snover et al., 2009) for evaluation of student translations. TERp requires a reference translation but can represent the difference between a given translation and the reference in terms of editing operations such as insertion, deletion, change of word order and matches of different kinds. A weak positive correlation with instructor-based grades (using Pearson’s r) could be demonstrated in the study and the authors argue that TERp i"
W14-1805,P06-1065,0,0.0266337,"Missing"
W14-1805,P02-1040,0,0.0981465,"Missing"
W14-1805,W09-0441,0,0.0199404,"e their own translations with reference translations and have them classified in terms of categories such as literal, semantic, and communicative. The comparisons are made one sentence at a time, using the Dice coefficient, i.e., by treating the sentences as bags of words. Our proposal, in contrast, uses more advanced computational linguistics tools and provides text level assessment based on word alignment. Michaud and McCoy (2013) describe a system and a study where the goal, as in our proposal, is to develop automatic support for translator training. They focus on the inverted TERp metric (Snover et al., 2009) for evaluation of student translations. TERp requires a reference translation but can represent the difference between a given translation and the reference in terms of editing operations such as insertion, deletion, change of word order and matches of different kinds. A weak positive correlation with instructor-based grades (using Pearson’s r) could be demonstrated in the study and the authors argue that TERp is sufficiently reliable to provide feed-back to students in a tutoring environment. The main difference between their proposal and ours is that we start with a metric that has been dev"
W15-2103,W15-1821,0,0.117345,"Missing"
W15-2103,A97-1011,0,0.261012,"Missing"
W15-2103,E14-4028,0,0.0411566,"Missing"
W15-2103,de-marneffe-etal-2014-universal,0,0.0235083,"Missing"
W15-2103,petrov-etal-2012-universal,0,0.0448378,"rogram, and in section 5 we present and discuss the results. Section 6, finally, states the conclusions. 2 Related work Universal Dependencies is a project involving several research groups around the world with a common interest in treebank development, multilingual parsing and cross-lingual learning (Universal dependencies, 2015). The annotation scheme for dependency relations has its roots in universal Stanford dependencies (de Marneffe and Manning, 2008; de Marneffe et al., 2014) and the project also embraces a slightly extended version of the Google universal tag set for parts-of-speech (Petrov et al., 2012). At the time of writing treebanks using UD are available for download from the LINDAT/CLARIN Repository Home for 18 different languages (Agi´c et al., 2015). The first release of UD treebanks included six languages. Two of these, the ones for English and Swedish, were created by automatic conversion (McDonald et al., 2013). The English treebank used the Stanford parser (v1.6.8) on the WSJ section of the Penn treebank for this purpose. The Swedish Talbanken treebank was converted by a set of deterministic rules, and the outcome is claimed to have a high precision “due to the fine-grained label"
W15-2103,W08-1301,0,\N,Missing
W15-2103,W07-2441,1,\N,Missing
W17-0402,de-marneffe-etal-2014-universal,0,0.0214421,"Missing"
W17-0402,petrov-etal-2012-universal,0,0.0281867,"Missing"
W17-0402,L16-1262,0,\N,Missing
W19-8011,W13-3721,0,0.0391717,"Missing"
W19-8011,de-marneffe-etal-2014-universal,0,0.0719307,"Missing"
W19-8011,K17-3001,0,0.0573477,"Missing"
W85-0101,J86-2006,0,0.0423428,"Missing"
W85-0101,W83-0115,0,0.0414828,"Alternativa, mer performansinriktade parsningsmodeller diskutercis på olika ställen i Bresnan (1982a), t ex i artikeln Ford, Bresnan åc Kaplan (1982), där man också anstränger sig att visa att en kompetensgrammatik av LFG’s typ har ett förklaringsvärde för många performansfenomen. Ingen av dessa har dock implementerats. Proceedings of NODALIDA 1985 6 5. Morfologins roll i LF6. Det är ett välbekant faktum att morfologiska signaler är goda indikatorer på frasstruktur, något som för svenskans del tydligast demonstrerats av möjligheterna till korrekt morfologibaserad heuristisk parsning (se t ex Källgren (1984)). Det kan då vara intressant att konstatera att man inom LFG inte alls utnyttjar morfologiska signaler på detta sätt. Tar man hänsyn till denna egenskap hos morfologiska signaler vinner man också en hel del i parsningseffektivitet, i och med att antalet felaktiga genererade c-strukturer minskas betydligt. Den viktiga konsekvensen av detta är inte i och för sig det minskade antalet c-strukturer, utan det minskade antalet hypoteser rörande f-struktur. Det förutsätter dock en mindre förändring av grundkonceptet i LFG. De funktionella schemana är väsentligen av tre olika slag: morfosyntaktiska. I"
W93-0401,C90-2001,1,0.827229,"ed clusters, while augmenting the grammar with another device to handle word order regularities: the topological frame. The frames encode word order regularities that are valid for a class of constituents. They can basically be thought of as formalizations of the topological schemas used by Diderichsen (1962) and several other linguists working in his tradition. A cluster can similarly be seen as a sequence of constituents occuring within a specific position (or field) of a frame. For reasons of space the full motivations and implications of this proposal cannot be dealt with here, though see Ahrenberg (1990) for some of the motivations. Instead I will develop a small, illustrative grammar fragment to make the proposal more tangible. E lem en ts o f the gram m ar The language fragment used is small and simplified in many respects. What I propose is quite compatible with the general assumptions of HPSG, however, apart from the account of word order regularities; I assume that it is necessary to restrict the domain of word order rules in languages like Swedish and German to types. This is after all quite a natural assumption to make in a theory assuming grammars to be organized as type hierarchies."
W93-0401,P92-1026,0,0.020486,"<x&gt; DTRS:HEAD = [ v e r b ; PHON = 1, SYN: LOC: SUBCAT < x [n p ], y [n p ], z [p p ]&gt; ] DTRSiCDTRS: = <y[PHON = 2 ] , z [ PHON = 3 ]&gt; There are problems, however, for grammars relying on LP-rules as the sole means for stating word order constraints. Languages with discontinuous constituents, such as the Scandinavian languages, and especially German, pose difficulties. There have accordingly been many proposals to augment LP-rules in various ways. Reape (1989) proposes a more complex combinatoric operation, sequence union, which allows access to non-immediate daughters of a constituent, while Engelkamp et al. (1992) propose to widen the domain of LP-rules to what they call head-domains, i.e. sets of constituents consisting of a lexical head with all its complements and adjuncts. In this paper I propose instead to restrict the use of LP-rules to smaller domains, called clusters, while augmenting the grammar with another device to handle word order regularities: the topological frame. The frames encode word order regularities that are valid for a class of constituents. They can basically be thought of as formalizations of the topological schemas used by Diderichsen (1962) and several other linguists workin"
W93-0401,E89-1014,0,0.0281729,"alue of PHON is determined by analogs of the LP-rules in (3b). Proceedings of NODALIDA 1993, pages 9-17 (4 ) vp ; PHON = <1 2 3&gt; SYN: LOC: SUBCAT = <x&gt; DTRS:HEAD = [ v e r b ; PHON = 1, SYN: LOC: SUBCAT < x [n p ], y [n p ], z [p p ]&gt; ] DTRSiCDTRS: = <y[PHON = 2 ] , z [ PHON = 3 ]&gt; There are problems, however, for grammars relying on LP-rules as the sole means for stating word order constraints. Languages with discontinuous constituents, such as the Scandinavian languages, and especially German, pose difficulties. There have accordingly been many proposals to augment LP-rules in various ways. Reape (1989) proposes a more complex combinatoric operation, sequence union, which allows access to non-immediate daughters of a constituent, while Engelkamp et al. (1992) propose to widen the domain of LP-rules to what they call head-domains, i.e. sets of constituents consisting of a lexical head with all its complements and adjuncts. In this paper I propose instead to restrict the use of LP-rules to smaller domains, called clusters, while augmenting the grammar with another device to handle word order regularities: the topological frame. The frames encode word order regularities that are valid for a cla"
weiss-ahrenberg-2012-error,taylor-white-1998-predicting,0,\N,Missing
weiss-ahrenberg-2012-error,P10-1062,0,\N,Missing
weiss-ahrenberg-2012-error,2011.eamt-1.12,0,\N,Missing
weiss-ahrenberg-2012-error,vilar-etal-2006-error,0,\N,Missing
weiss-ahrenberg-2012-error,2011.eamt-1.36,0,\N,Missing
