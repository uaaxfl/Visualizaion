2008.eamt-1.23,W07-1512,0,0.216668,"Missing"
2008.eamt-1.23,J07-2003,0,0.195918,"Missing"
2008.eamt-1.23,P03-2041,0,0.109084,"Missing"
2008.eamt-1.23,graca-etal-2008-building,0,0.0944489,"Missing"
2008.eamt-1.23,C08-2026,1,0.848293,"Missing"
2008.eamt-1.23,J97-3002,0,0.559398,"Missing"
2010.eamt-1.5,W07-0414,0,0.0140965,"res parse failure rates for this subclass and the full class of ITGs. 1 Introduction The adequacy of grammar-based machine translation formalisms is sometimes empirically evaluated by running all-accepting grammars on large amounts of automatically aligned text (Zens and Ney, 2003). What is studied is called alignment capacity (Søgaard and Wu, 2009) or translation equivalence modeling (Zhang et al., 2008), i.e. a formalism’s ability to generate observed alignments or translation equivalences; and the study is closely related to the study of translation model search spaces (Zens and Ney, 2003; Dreyer et al., 2007). All-acccepting grammars are simply grammars that contain all possible rules that can be expressed in a formalism. A grammar generates an aligned sentence pair if it can generate the two sentences in such a way that all aligned words are genc 2010 European Association for Machine Translation. erated simultaneously (Wu, 1997). What is studied is thus: Can an all-accepting grammar generate the aligned sentence pairs observed in a text? The metric in these studies is parse failure rate (PFR) or its inverse, i.e. the number of sentence pairs that can be generated over the total number of sentence"
2010.eamt-1.5,J07-3002,0,0.0185377,"81.00 68.00 70.17 49.35 60.00 47.05 35.54 45.13 81.75 SW09 94.00 95.00 93.00 91.00 99.00 93.00 94.17 *89.63 *52.68 - LR 32.00 25.00 30.00 44.00 53.00 51.00 - Figure 4: Alignment reachability scores for NF-ITG, ITG and (an upper bound on) local reordering models, compared to results in Søgaard and Wu (2009). Sentence length cut-off given in parentheses. * means that results are incomparable to those in Søgaard and Wu (2009), because different cut-offs were used. also considerably better than that of the IBM models. 5 Conclusion The status of alignments in machine translation is widely debated (Fraser and Marcu, 2007), but so are other metrics such as BLEU. Alignment reachability is related to BLEU oracle computation (Dreyer et al., 2007), but in a very indirect way. Our studies nevertheless show that there are translational equivalences that are very hard to capture in computational search spaces. While the ITG translation search space seems better fit than many other models, as indicated by our experiment as well as experiments cited above, there are still many alignment configurations that are beyond reach. Capturing those configurations may not be necessary from a practical point of view, but it may ne"
2010.eamt-1.5,graca-etal-2008-building,0,0.125947,"all continuous translation units. 4 Experiments This section describes our experiments, incl. the data sets used, the metric used in evaluation and the results obtained on the data sets. 4.1 Data Sets The characteristics of the hand-aligned parallel texts used are presented in Figure 3. The parallel texts that involve Danish are part of the Copenhagen Dependency Treebank (BuchKromann et al., 2010), based on translations of the balanced Parole corpus, English-German is from Pado and Lapata (2006) (Europarl), and the six combinations of English, French, Portuguese and Spanish are documented in Graca et al. (2008) (Europarl). We use the 200 sentences standard training section of the Canadian Hansard data set for supervised word alignment. 4.2 Alignment reachability Similarly to Zens and Ney (2003), we use the inverse of PFR as metric. We refer to this below as alignment reachability, in analogy to translation reachability. Each experiment thus applies the above algorithm to a set of hand-aligned sentence pairs. The algorithm either reaches an alignment, which means that the alignment can be generated, or it does not, which mean that the alignment is beyond the expressive power of the formalism in quest"
2010.eamt-1.5,H05-1021,0,0.0609179,"Missing"
2010.eamt-1.5,P06-1146,0,0.291686,"Missing"
2010.eamt-1.5,W09-2303,1,0.851934,"Missing"
2010.eamt-1.5,P06-1123,0,0.5233,"Missing"
2010.eamt-1.5,J97-3002,0,0.530969,"and Wu, 2009) or translation equivalence modeling (Zhang et al., 2008), i.e. a formalism’s ability to generate observed alignments or translation equivalences; and the study is closely related to the study of translation model search spaces (Zens and Ney, 2003; Dreyer et al., 2007). All-acccepting grammars are simply grammars that contain all possible rules that can be expressed in a formalism. A grammar generates an aligned sentence pair if it can generate the two sentences in such a way that all aligned words are genc 2010 European Association for Machine Translation. erated simultaneously (Wu, 1997). What is studied is thus: Can an all-accepting grammar generate the aligned sentence pairs observed in a text? The metric in these studies is parse failure rate (PFR) or its inverse, i.e. the number of sentence pairs that can be generated over the total number of sentence pairs. Most alignment capacity studies use automatically aligned text, since hand-aligned text is hard to come by. Recently three important data sets have been released (Pad´o and Lapata, 2006; Graca et al., 2008; Buch-Kromann et al., 2010). Our experiments include 12 hand-aligned parallel texts of varying size. Our main con"
2010.eamt-1.5,P03-1019,0,0.660926,"ned by word alignments is one metric that has been used, but no one has studied parse failure rates of the full class of ITGs on representative hand aligned corpora. It has also been noted that ITGs in Chomsky normal form induce strictly less alignments than ITGs (Søgaard and Wu, 2009). This study is the first study that directly compares parse failure rates for this subclass and the full class of ITGs. 1 Introduction The adequacy of grammar-based machine translation formalisms is sometimes empirically evaluated by running all-accepting grammars on large amounts of automatically aligned text (Zens and Ney, 2003). What is studied is called alignment capacity (Søgaard and Wu, 2009) or translation equivalence modeling (Zhang et al., 2008), i.e. a formalism’s ability to generate observed alignments or translation equivalences; and the study is closely related to the study of translation model search spaces (Zens and Ney, 2003; Dreyer et al., 2007). All-acccepting grammars are simply grammars that contain all possible rules that can be expressed in a formalism. A grammar generates an aligned sentence pair if it can generate the two sentences in such a way that all aligned words are genc 2010 European Asso"
2010.eamt-1.5,C08-1136,0,0.110406,"Missing"
2020.acl-main.679,J90-1003,0,\N,Missing
2020.acl-main.679,Q15-1016,0,\N,Missing
2020.acl-main.679,D12-1071,0,\N,Missing
2020.acl-main.679,L16-1680,0,\N,Missing
2020.acl-main.679,K17-1004,0,\N,Missing
2020.acl-main.679,S18-2023,0,\N,Missing
2020.acl-main.679,L18-1239,0,\N,Missing
2020.acl-main.679,W18-5446,0,\N,Missing
2020.acl-main.679,Q19-1004,1,\N,Missing
2020.acl-main.679,P19-1334,0,\N,Missing
2020.acl-main.679,P19-1478,0,\N,Missing
2020.acl-main.679,N19-1419,0,\N,Missing
2020.acl-main.679,W19-4825,0,\N,Missing
2020.acl-main.679,W19-4820,0,\N,Missing
2020.acl-main.679,P19-1084,1,\N,Missing
2020.acl-main.679,W19-4828,0,\N,Missing
2020.acl-main.679,N19-1423,0,\N,Missing
2020.acl-main.679,D19-1109,0,\N,Missing
2020.acl-main.679,D19-1593,1,\N,Missing
2020.emnlp-main.209,J95-2003,0,0.397265,"lucination than violating grammatical constraints, and we acknowledge that in machine translation, as well as in language modeling, the difference concerning existing gender bias challenge datasets is less pronounced than with NLI and coreference resolution. Nevertheless, note that the model not only hallucinates a gender attribution, but also co-referentiality, making it relatively simple to construct semantically impossible examples, e.g., The mechanic needs his tools, but not his own tools. Furthermore, introducing a new referent without evidence also violates pragmatic economy principles (Grosz et al., 1995; Gardent and Webber, 2001). Google Translate incorrectly translates into a sentence with two reflexive pronouns (violating the semantic principle of bivalence). Experiments In this work, we are focused on highlighting a linguistic phenomenon that is useful for diagnosing gender bias, therefore we do not focus on an extensive comparison of model architectures; further work would be required to examine more models. We are interested in the gender associations that existing models make. Because of this, we take offthe-shelf translation models and language models. As there were not any state-of-t"
2020.emnlp-main.209,D18-1269,0,0.0290362,"al. (2017). We correlate our findings with these statistics as well as national statistics. NLI NLI is originally a three-way classification task. Given two sentences; a premise and a hypothesis, the system classifies the relation between them as entailment, contradiction, or neutral. Since ABC is only intended for diagnosing gender bias in off-the-shelf models, and not for training models, we only consider the entailment relation. If the premise contains a reflexive pronoun, the true class is entailment, and if the premise contains a masculine or feminine pronoun it is not entailment. XNLI (Conneau et al., 2018) is a manual translation of the English NLI data into 15 languages. Chinese and Russian are among them and we benchmark the model on the XNLI test set. Singh et al. (2019) extend the XNLI train set to a wider set of languages, including Danish and Swedish but there is not test set for benchmarking. We use 7 See also the footnote above on whether our machine translation examples diagnose model ’hallucinations’ or unambiguous prediction errors. 8 http://www.bls.gov/cps/cpsaat11.htm 2640 cross-lingual language model pre-training (XLI) (Conneau and Lample, 2019), i.e., we fine-tune on English NLI"
2020.emnlp-main.209,W19-5333,0,0.0187561,"s. Machine Translation For machine translation, we evaluate models for English→ {Danish, Russian, Swedish, Chinese} to assess how often they predict the non-gendered reflexive possessive pronouns when the source possessive pronoun is masculine versus feminine. For all languages, we report the performance of Google Translate. Additionally, for the languages where an off-the-shelf, near-stateof-the-art system was publicly available, we also report performance. For Chinese, we use the pretrained models provided by Sennrich et al. (2017) 10 (E-WMT). For Russian, we use the winner system of WMT19 (Ng et al., 2019), which is provided as part of the Fairseq toolkit (F-WMT).11 Coreference Resolution For coreference resolution, we are interested in whether the system violates grammatical rules by placing an anti-reflexive possesive pronoun in a cluster. We train coreference resolution models for Chinese and Russian using the model and code of Joshi et al. (2019). For Chinese, we use the Chinese version of Ontonotes as our training data, which is made up of about 1800 documents for training. For Russian, we use the RuCor corpus (Ju et al., 2014), which is small, containing only 181 documents total, but has"
2020.emnlp-main.209,L16-1262,0,0.0725493,"Missing"
2020.emnlp-main.209,Q18-1042,1,0.868346,"nificantly different because we only had statistics for 10 occupations. Language Modeling Correlations were stronger with national rather than U.S. statistics on average, but not significantly so. 6 Related Work The ABC dataset is not first to focus on pronouns and gender bias. The UD English-Pronouns23 (Munro, 2020), a manually constructed, genderbalanced benchmark of English sentences with pronouns, was motivated by the observation that the genitive pronoun hers only occurs three times in the English Universal Dependencies (Nivre et al., 2016). The gendered, ambiguous pronoun (GAP) dataset (Webster et al., 2018) is a coreference resolution dataset of human-annotated ambiguous pronoun-name examples from English Wikipedia. Prates et al. (2018) constructed a translation challenge dataset of simple sentences in gender-neutral languages such as Hungarian and Yoruba and English target sentences such as he/she is an engineer to estimate gender biases in machine translation. Both these challenge datasets focus on gender hallucinations, not unambiguous errors induced by gender bias. Some of our examples share similarities with the English WinoGender schema (Rudinger et al., 2018). Consider the following minim"
2020.emnlp-main.209,N18-2003,0,0.049056,"reflexive possessive pronouns: they always refer to the subject, and their anti-reflexive counterparts never do, so there is no grammatical ambiguity. The disadvantage with semantic disambiguation, we argue, is that it ultimately becomes a subjective competition of belief biases. It is generally impossible to perform CPR if you are dead, but special cases exist: (18) Dr Jones1 has turned into a zombie! He1 performed CPR on the passenger even though he1 was already dead. The ABC dataset evaluates to what extent gender bias leads to unambiguous NLP errors not based on semantic grounds. Finally, Zhao et al. (2018) also include English examples with reflexive pronouns that can be resolved on syntactic grounds, such as: (19) The secretary called the physician and told him about a new patient. This construction, however, is less interesting than the reflexive possessive pronominal construction, since in this case, pronouns are always coreferential with the object position, regardless of the pronoun. In sum, the ABC challenge dataset is, to the best of our knowledge, the first dataset to focus on cases where gender bias leads to unambiguous errors; it is also the first multilingual, multi-task gender bias"
2020.emnlp-main.209,N18-2002,0,0.11561,"Missing"
2020.emnlp-main.209,W17-4739,0,0.0132791,". For Danish and Swedish, we use the XLM-100 model, which we fine-tune for 28 epochs. Machine Translation For machine translation, we evaluate models for English→ {Danish, Russian, Swedish, Chinese} to assess how often they predict the non-gendered reflexive possessive pronouns when the source possessive pronoun is masculine versus feminine. For all languages, we report the performance of Google Translate. Additionally, for the languages where an off-the-shelf, near-stateof-the-art system was publicly available, we also report performance. For Chinese, we use the pretrained models provided by Sennrich et al. (2017) 10 (E-WMT). For Russian, we use the winner system of WMT19 (Ng et al., 2019), which is provided as part of the Fairseq toolkit (F-WMT).11 Coreference Resolution For coreference resolution, we are interested in whether the system violates grammatical rules by placing an anti-reflexive possesive pronoun in a cluster. We train coreference resolution models for Chinese and Russian using the model and code of Joshi et al. (2019). For Chinese, we use the Chinese version of Ontonotes as our training data, which is made up of about 1800 documents for training. For Russian, we use the RuCor corpus (Ju"
2020.emnlp-main.209,D19-6106,0,0.017528,"emise and a hypothesis, the system classifies the relation between them as entailment, contradiction, or neutral. Since ABC is only intended for diagnosing gender bias in off-the-shelf models, and not for training models, we only consider the entailment relation. If the premise contains a reflexive pronoun, the true class is entailment, and if the premise contains a masculine or feminine pronoun it is not entailment. XNLI (Conneau et al., 2018) is a manual translation of the English NLI data into 15 languages. Chinese and Russian are among them and we benchmark the model on the XNLI test set. Singh et al. (2019) extend the XNLI train set to a wider set of languages, including Danish and Swedish but there is not test set for benchmarking. We use 7 See also the footnote above on whether our machine translation examples diagnose model ’hallucinations’ or unambiguous prediction errors. 8 http://www.bls.gov/cps/cpsaat11.htm 2640 cross-lingual language model pre-training (XLI) (Conneau and Lample, 2019), i.e., we fine-tune on English NLI training data. For Chinese and Russian, we use a publicly available implementation9 of the XLM-15 model (Conneau and Lample, 2019) and fine-tune it using a batch size of 4"
2020.emnlp-main.220,W18-6002,0,0.0610584,"Missing"
2020.emnlp-main.220,W06-2920,0,0.180833,"have been pointed at in the literature that were not applicable to our experiments: Søgaard and Haulrich (2010) show that the perplexity of the derivation orders of a transition-based dependency parser, is also predictive of parser performance. They report Pearson’s ρ scores that are considerably higher than those we found. Their study suffers from two biases, though; one imposed by the transition-based parser and the other imposed by the language model used to calculate the perplexity. Moreover, the results they report, are for only the non-convertedd dependency treebanks in the CoNLL 2006 (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al., 2007) treebank releases. These treebanks form a very small set, providing limited statistical support, and, moreover, rely on very different linguistic formalisms and annotation guidelines, leading to very different levels of complexity of derivation. In other words, a comparison would be inconclusive because of the free parameters imposed by the language model and the transition oracle, and the fact that no code is publicly available. Also, their high correlation scores are unlikely to transfer to Universal Dependencies. 6 Discussion and Conclusion This paper su"
2020.emnlp-main.220,D19-1224,0,0.0126391,"oo surprising, since graph isomorphisms correlate with syntactic constructions, which in turn correlate with the occurrence of linguistic markers and tail linguistic phenomena.6 The observation that treebanks leak, quite dramatically, at the graph level, is not only interesting for explaining variance in parser performance. It also suggests a new and improved evaluation methodology: Since language is Zipfian, not only at the level of words, but at the level of phrases (Ha et al., 2002; Williams et al., 2015), standard evaluation methodology relying on random samples (Gorman and Bedrick, 2019; Dodge et al., 2019) is biased toward frequent phenomena. Evaluating only on non-isomorphic trees, i.e., leaving out graphs that have been seen at training time from the test sections of treebanks, would reduce this bias. We hope this is a factor that designers of future syntactic treebanks will take into account. It is an open question whether graph-level train-test leakage is predictive of performance in other sentence-level NLP tasks, i.e., whether the ratio of test sentences whose (predicted) syntactic dependency structure is identical to that of one of our training examples, correlates with state-of-the-art"
2020.emnlp-main.220,I11-1100,0,0.0884349,"Missing"
2020.emnlp-main.220,W01-0521,0,0.370282,"cross-validation explained variance and mean absolute error of a linear regression model with two features, as well as the baseline of just using a linear regression with treebank size as our only feature. whether the ratio of nouns and verbs over the total number of tokens in a sentence is predictive of parser performance. POS bigram perplexity Others have proposed to use the perplexity of a POS bigram language model trained on the treebank’s training section and applied to its test section, to predict parser performance (Coltekin and Rama, 2018; Berdicevskis et al., 2018). Domain divergence Gildea (2001) explore the effect of domain shifts on parsing performance and show that such shifts are often detrimental to the quality of parses. This issue has, since then, been explored in great detail in the domain adaptation literature, but here we simply note that treebanks with train-test divergences may appear harder to parse. In order to compute the impact of train-test divergence on state-of-the-art parsing results, we need to be able to compute it. Several proposals exist in the literature, including Jensen-Shannon divergence (Wu and Huang, 2016), Renyi divergence (Van Asch and Daelemans, 2010),"
2020.emnlp-main.220,P19-1267,0,0.0286561,"sult is perhaps 2768 not too surprising, since graph isomorphisms correlate with syntactic constructions, which in turn correlate with the occurrence of linguistic markers and tail linguistic phenomena.6 The observation that treebanks leak, quite dramatically, at the graph level, is not only interesting for explaining variance in parser performance. It also suggests a new and improved evaluation methodology: Since language is Zipfian, not only at the level of words, but at the level of phrases (Ha et al., 2002; Williams et al., 2015), standard evaluation methodology relying on random samples (Gorman and Bedrick, 2019; Dodge et al., 2019) is biased toward frequent phenomena. Evaluating only on non-isomorphic trees, i.e., leaving out graphs that have been seen at training time from the test sections of treebanks, would reduce this bias. We hope this is a factor that designers of future syntactic treebanks will take into account. It is an open question whether graph-level train-test leakage is predictive of performance in other sentence-level NLP tasks, i.e., whether the ratio of test sentences whose (predicted) syntactic dependency structure is identical to that of one of our training examples, correlates w"
2020.emnlp-main.220,C02-1117,0,0.126828,", treebank size aside, is the most predictive factor among those proposed, yet complementary. The result is perhaps 2768 not too surprising, since graph isomorphisms correlate with syntactic constructions, which in turn correlate with the occurrence of linguistic markers and tail linguistic phenomena.6 The observation that treebanks leak, quite dramatically, at the graph level, is not only interesting for explaining variance in parser performance. It also suggests a new and improved evaluation methodology: Since language is Zipfian, not only at the level of words, but at the level of phrases (Ha et al., 2002; Williams et al., 2015), standard evaluation methodology relying on random samples (Gorman and Bedrick, 2019; Dodge et al., 2019) is biased toward frequent phenomena. Evaluating only on non-isomorphic trees, i.e., leaving out graphs that have been seen at training time from the test sections of treebanks, would reduce this bias. We hope this is a factor that designers of future syntactic treebanks will take into account. It is an open question whether graph-level train-test leakage is predictive of performance in other sentence-level NLP tasks, i.e., whether the ratio of test sentences whose"
2020.emnlp-main.220,W04-1013,0,0.0245266,"and test splits. Note that Van Asch and Daelemans (2010) explicitly proposed quantifying domain divergence as a way of predicting performance, noting a linear correlation between the two. 4 Empirical Comparison of Factors We correlate the factors φ assumed to influence syntactic dependency parser performance with state-ofthe-art performance figures from the CoNLL 2018 shared task, i.e,. the performance of the best performing system per language.4 See the Appendix for the full statistics. While computing their Pearson’s ρ coefficients is standard methodology for validating performance metrics (Lin, 2004; Miculicich Werlen and Popescu-Belis, 2017) and has also been used to evaluate factors predicting system performance (Martin and Foltz, 2004; Søgaard and Haulrich, 2010), this is inadequate in our case: Many factors are potentially covariate, and we are, for example, not interested in factors that correlate strongly with treebank size, e.g., out-of-vocabulary rate or type-token ratio (Kettunen, 2014). Instead we compute the explained variance and mean absolute error of a linear regression model with treebank size and φ as input, i.e., ats + bφ + c with ts treebank size and a, b, c learned par"
2020.emnlp-main.220,J11-1007,0,0.732737,"thers written with the Universal Dependencies guidelines in mind; some, again, were developed by big teams, some by a single person. While protocol is hard to isolate and study – and while protocol may correlate both positively or negatively with parsing performance, i.e., it is easy to imagine a poorly designed treebank that is easy to parse – the protocol likely has a significant downstream effect on performance; which means we can only hope to explain some of the variance in the experiments below. 2 (Nivre et al., 2007), sentence length or average gold dependency length (in the test data) (McDonald and Nivre, 2011), and domain differences between training and test data (Foster et al., 2011). Training set size is undoubtedly a very strong predictor of parsing performance, but in this paper, overlap between unlabeled graphs in the training and test sections of a treebank is shown to be more predictive than any of the other factors. Specifically, we compute equivalence classes over unlabeled dependency graphs – directed or undirected – and compute the ratio of trees in the treebanks’ test sections that are isomorphic to graphs observed in the training section, i.e., the graph-level train-test leakage, and"
2020.emnlp-main.220,W17-4802,0,0.0198967,"that Van Asch and Daelemans (2010) explicitly proposed quantifying domain divergence as a way of predicting performance, noting a linear correlation between the two. 4 Empirical Comparison of Factors We correlate the factors φ assumed to influence syntactic dependency parser performance with state-ofthe-art performance figures from the CoNLL 2018 shared task, i.e,. the performance of the best performing system per language.4 See the Appendix for the full statistics. While computing their Pearson’s ρ coefficients is standard methodology for validating performance metrics (Lin, 2004; Miculicich Werlen and Popescu-Belis, 2017) and has also been used to evaluate factors predicting system performance (Martin and Foltz, 2004; Søgaard and Haulrich, 2010), this is inadequate in our case: Many factors are potentially covariate, and we are, for example, not interested in factors that correlate strongly with treebank size, e.g., out-of-vocabulary rate or type-token ratio (Kettunen, 2014). Instead we compute the explained variance and mean absolute error of a linear regression model with treebank size and φ as input, i.e., ats + bφ + c with ts treebank size and a, b, c learned parameters. We report explained variance and me"
2020.emnlp-main.220,W17-0411,0,0.112753,"search space of possible parses (McDonald and Nivre, 2011). This, for example, is why unsupervised dependency parsing has successfully relied on baby steps training (Spitkovsky et al., 2010). We correlate state-of-the-art parser performance with training set size and average test sentence length. Graph properties McDonald and Nivre (2011) discuss graph properties that seem to correlate with parsing performance. We include average dependency length in our experiments below, which we compute by simply dividing the total length of dependencies by word tokens in the test section. Open class ratio Nivre and Fang (2017) argue that open word classes (especially nouns and verbs) tend to be harder to attach than other parts of speech, and that languages with many of them will therefore be harder to parse. We therefore evaluate 2766 3 https://wals.info T WO FEATURES Factors Explained Variance Mean Error Treebank Size 0.014 0.082 +POS Bigram Perplexity +Morphology (WALS 21B) +Open Class Ratio +A-Distance +Dependency Length +Sentence Length 0.000 0.000 0.000 0.036 0.052 0.170 0.085 0.082 0.081 0.078 0.079 0.073 +UUG-I SO +DUG-I SO 0.222 0.228 0.072 0.071 Table 1: E MPIRICAL COMPARISON OF FACTORS. We report the thr"
2020.emnlp-main.220,W08-0504,0,0.721186,"report the full set of results with UUGs; both for exact computation of isomorphims with VF2, as well as for a heuristic simply matching a set of edge degrees. Treebank size It is trivially true that parser performance depends on treebank size, and it is unsurprising that the correlation is strong. Obviously, if the treebank does not contain any training data, supervised parsers will have to resort to blind guessing, and the more data they see, the less variance they have to resolve. That said, it is well established that increasing the size of a treebank often comes with diminishing returns (Sagae et al., 2008). Since treebank size is nevertheless trivially related to parsing performance, we correlate all other factors φ in combination with treebank size (see §4): Morphology Previous work has pointed to morphology as a source of lower parsing performance (Tsarfaty et al., 2013; Coltekin and Rama, 2018). In languages with rich morphology, many relations which are expressed implicitly by word order and adjacency in languages like English, are encoded in morphological affixes, which requires subwordlevel processing to detect (in the tail). Expressing functional information morphologically also allows f"
2020.emnlp-main.220,W10-2605,0,0.0819475,"Missing"
2020.emnlp-main.220,D19-1102,1,0.884949,"Missing"
2020.emnlp-main.220,P16-1029,0,0.0139976,"ma, 2018; Berdicevskis et al., 2018). Domain divergence Gildea (2001) explore the effect of domain shifts on parsing performance and show that such shifts are often detrimental to the quality of parses. This issue has, since then, been explored in great detail in the domain adaptation literature, but here we simply note that treebanks with train-test divergences may appear harder to parse. In order to compute the impact of train-test divergence on state-of-the-art parsing results, we need to be able to compute it. Several proposals exist in the literature, including Jensen-Shannon divergence (Wu and Huang, 2016), Renyi divergence (Van Asch and Daelemans, 2010), and Wasserstein distance (Shen et al., 2018). We choose to rely on A-distance (Kifer et al., 2004), since it is arguably the most popular divergence measure in domain adaptation, and since we can approximate it efficiently by the accuracy of a linear perceptron trained to discriminate between examples from the train and test splits. Note that Van Asch and Daelemans (2010) explicitly proposed quantifying domain divergence as a way of predicting performance, noting a linear correlation between the two. 4 Empirical Comparison of Factors We correl"
2020.emnlp-main.220,N10-1116,0,0.0234963,"in languages like English, are encoded in morphological affixes, which requires subwordlevel processing to detect (in the tail). Expressing functional information morphologically also allows for a high degree of word-order variation. In our experiments, we use the most predictive morphological feature in WALS3 and impute the missing values. Sentence length Parser performance unsurprisingly also depends on input length, i.e., the search space of possible parses (McDonald and Nivre, 2011). This, for example, is why unsupervised dependency parsing has successfully relied on baby steps training (Spitkovsky et al., 2010). We correlate state-of-the-art parser performance with training set size and average test sentence length. Graph properties McDonald and Nivre (2011) discuss graph properties that seem to correlate with parsing performance. We include average dependency length in our experiments below, which we compute by simply dividing the total length of dependencies by word tokens in the test section. Open class ratio Nivre and Fang (2017) argue that open word classes (especially nouns and verbs) tend to be harder to attach than other parts of speech, and that languages with many of them will therefore be"
2020.emnlp-main.220,J13-1003,0,0.0689997,"Missing"
2020.emnlp-main.257,W13-3520,0,0.254793,"which has so far been largely attributed only to inherent typological differences. In fact, the amount of data used to induce the monolingual embeddings is predictive of the quality of the aligned cross-lingual word embeddings, as evaluated on bilingual lexicon induction (BLI). Consider, for motivation, Figure 1; it shows the performance of a state-of-the-art alignment method— RCSLS with iterative normalisation (Zhang et al., 2019)—on mapping English embeddings onto embeddings in other languages, and its correlation (ρ = 0.72) with the size of the tokenised target language Polyglot Wikipedia (Al-Rfou et al., 2013). We investigate to what extent the amount of data available for some languages and corresponding training conditions provide a sufficient explanation for the variance in reported results; that is, whether it is the full story or not: The answer is ’almost’, 3178 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3178–3192, c November 16–20, 2020. 2020 Association for Computational Linguistics that is, its interplay with inherent typological differences does have a crucial impact on the ‘alignability’ of monolingual vector spaces. We first discuss cur"
2020.emnlp-main.257,D19-1328,1,0.761399,"IM; higher is better) computed across different AR and JA snapshots. 2019) whose neighbourhoods may be more isomorphic (Nakashole, 2018). We thus create new evaluation dictionaries for English–Spanish that consist of words in different frequency bins: we sample EN words for 300 translation pairs respectively from (i) the top 5k words of the full English Wikipedia (HFREQ); (ii) the interval [10k, 20k] (MFREQ); (iii) the interval [20k, 50k] (LFREQ). The entire dataset (ALL -FREQ; 900 pairs) consists of (i) + (ii) + (iii). We exclude named entities as they are over-represented in many test sets (Kementchedjhieva et al., 2019) and include nouns, verbs, adjectives, and adverbs in all three sets. All 900 words have been carefully manually translated and double-checked by a native Spanish speaker. There are no duplicates. We also report BLI results on the PanLex test lexicons (Vuli´c et al., 2019). For English–Spanish, we create training dictionaries of sizes 1k and 5k based on PanLex (Kamholz et al., 2014) following Vuli´c et al. (2019). We exclude all words from ALL -FREQ from the training set. For EN–JA / AR BLI experiments, we rely on the standard training and test dictionaries from the MUSE benchmark (Conneau et"
2020.emnlp-main.257,N13-1090,0,0.737461,"sv hu he vi de pl ja ru zh uk 20M lt 50M 100M 200M # of tokens in Wikipedia 500M Figure 1: Performance of a state-of-the-art BLI model mapping from English to a target language and the size of the target language Wikipedia are correlated. Linear fit shown as a blue line (log scale). Word embeddings have been argued to reflect how language users organise concepts (Mandera et al., 2017; Torabi Asr et al., 2018). The extent to which they really do so has been evaluated, e.g., using semantic word similarity and association norms (Hill et al., 2015; Gerz et al., 2016), and word analogy benchmarks (Mikolov et al., 2013c). If word embeddings reflect more or less languageindependent conceptual organisations, word embeddings in different languages can be expected to be near-isomorphic. Researchers have exploited this to learn linear transformations between such spaces (Mikolov et al., 2013a; Glavaˇs et al., 2019), which have been used to induce bilingual dictionaries, as well as to facilitate multilingual modeling and cross-lingual transfer (Ruder et al., 2019). In this paper, we show that near-isomorphism arises only with sufficient amounts of training. This is of practical interest for applications of linear"
2020.emnlp-main.257,D18-1047,0,0.147371,"For clarity, the corresponding isomorphism scores (and impact of training duration on isomorphism of vector spaces) over the same training snapshots for Spanish are shown in Figure 5. (c) We again report scores without and with self-learning on EN– AR / JA BLI evaluation sets from the MUSE benchmark with 1k seed translation pairs. The results with 5k seed pairs for EN–AR / JA are available in the appendix. Dashed lines without any marks show isomorphism scores (computed by RSIM; higher is better) computed across different AR and JA snapshots. 2019) whose neighbourhoods may be more isomorphic (Nakashole, 2018). We thus create new evaluation dictionaries for English–Spanish that consist of words in different frequency bins: we sample EN words for 300 translation pairs respectively from (i) the top 5k words of the full English Wikipedia (HFREQ); (ii) the interval [10k, 20k] (MFREQ); (iii) the interval [20k, 50k] (LFREQ). The entire dataset (ALL -FREQ; 900 pairs) consists of (i) + (ii) + (iii). We exclude named entities as they are over-represented in many test sets (Kementchedjhieva et al., 2019) and include nouns, verbs, adjectives, and adverbs in all three sets. All 900 words have been carefully ma"
2020.emnlp-main.257,D18-1268,0,0.0306659,"een a source and a target embedding space (Mikolov et al., 2013a). Such mapping-based approaches assume that the monolingual embedding spaces are isomorphic, i.e., that one can be transformed into the other via a linear transformation (Xing et al., 2015; Artetxe et al., 2018a). Recent unsupervised approaches rely even more strongly on this assumption: They assume that the structures of the embedding spaces are so similar that they can be aligned by minimising the distance between the transformed source language and the target language embedding space (Zhang et al., 2017; Conneau et al., 2018; Xu et al., 2018; Alvarez-Melis and Jaakkola, 2018; Hartmann et al., 2019). 2.1 Quantifying Isomorphism We employ measures that quantify isomorphism in three distinct ways—based on graphs, metric spaces, and vector similarity. Eigenvector similarity (Søgaard et al., 2018) Eigenvector similarity (EVS) estimates the degree of isomorphism based on properties of the nearest neighbour graphs of the two embedding spaces. We first length-normalise embeddings in both embedding spaces and compute the nearest neighbour graphs on a subset of the top most frequent N words. We then calculate the Laplacian matrices L1 and"
2020.emnlp-main.257,P16-1023,0,0.146189,"Missing"
2020.emnlp-main.257,J15-4004,0,\N,Missing
2020.emnlp-main.257,D14-1162,0,\N,Missing
2020.emnlp-main.257,kamholz-etal-2014-panlex,0,\N,Missing
2020.emnlp-main.257,N15-1104,0,\N,Missing
2020.emnlp-main.257,Q17-1010,0,\N,Missing
2020.emnlp-main.257,D16-1099,0,\N,Missing
2020.emnlp-main.257,P17-1042,0,\N,Missing
2020.emnlp-main.257,D17-1207,0,\N,Missing
2020.emnlp-main.257,D18-1056,1,\N,Missing
2020.emnlp-main.257,P19-1492,0,\N,Missing
2020.emnlp-main.257,P19-1018,0,\N,Missing
2020.emnlp-main.257,D19-1449,1,\N,Missing
2020.emnlp-main.257,D15-1243,0,\N,Missing
2020.emnlp-main.680,W19-4412,0,0.0106932,"te an inability to rely on language modelling in low errordensity domains. 5.2 Perplexity ratio Score differences for the R:SPELL error type seem to be driven by a different propensity of spelling errors being of a typographical vs. phonetical nature in the two datasets. versions of an input sentence and then deciding if any of the alternatives are preferable to the original version, based on language model probabilities. The authors use an n-gram language model, which we replace with GPT-2 (Radford et al., 2019) to see how a strong neural language model performs – this approach is similar to Alikaniotis and Raheja (2019). Hyperparameters are tuned for each dataset (see Appendix C for details). Table 7 displays the results on the different datasets. Recall and, in particular, precision is substantially lower on CWEB and AESW compared to other datasets. In general, scores are higher in domains with a higher proportion of errors and those containing edits which result in high perplexity improvements. In these cases systems can rely on a rough heuristic of replacing low probability sequences with high probability ones. However, in CWEB, where errors are fewer and more subtle, this leads to low precision, as perpl"
2020.emnlp-main.680,D19-1435,0,0.0222983,"Missing"
2020.emnlp-main.680,W18-0529,0,0.014767,"OTHER, R:SPELL and R:VERB. These are open class errors, where the error and correction can be quite different. It is therefore reasonable that differences in edits’ degree of semantic change and perplexity improvement across domains are particularly observed in these cases.22 Language Model Importance We also investigate the degree to which systems can rely on a strong internal language model representation when evaluated against different domains. We examine this by looking at the performance of a purely language model based GEC system over the different datasets. We build on the approach of Bryant and Briscoe (2018), using confusion sets to generate alternative 22 0.34 0.51 0.69 Table 8: Examples of false positives on the CWEB dataset that improve perplexity substantially – even more than the average gold edit in CWEB (0.86 perplexity ratio). Table 7: Scores of a language model based GEC system. The lower scores on CWEB and AESW indicate an inability to rely on language modelling in low errordensity domains. 5.2 Perplexity ratio Score differences for the R:SPELL error type seem to be driven by a different propensity of spelling errors being of a typographical vs. phonetical nature in the two datasets. ve"
2020.emnlp-main.680,W19-4406,0,0.288356,"e. We use the jusText5 tool to retrieve the content from HTML pages (removing boilerplate elements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English6 and incomplete sentences. To ensure diversity of the data, we also remove duplicate sentences. Among the million sentences gathered, we select paragraphs randomly. We split the data with respect to where they 4 https://commoncrawl.org/ https://github.com/miso-belica/ jusText 6 Using the langdetect package. 5 3 Total Test CWEB-S CWEB-G Total 2014)], student essays [W&I+LOCNESS (Bryant et al., 2019; Granger, 1998)] or target a specific domain [scientific writing; AESW (Daudaravicius et al., 2016)]. Supervised systems trained on specific domains are less likely to be as effective at correcting distinctive errors from other domains, as is the case for systems trained on learner data with different native languages (Chollampatt et al., 2016; Nadejde and Tetreault, 2019). The recent BEA 2019 shared task (Bryant et al., 2019) encouraged research in the use of low-resource and unsupervised approaches; however, evaluation primarily targeted the restricted domain of student essays. We show that"
2020.emnlp-main.680,P17-1074,0,0.113216,"ng of error corrections, as as there are often many different ways to correct a sentence (Bryant and Ng, 2015). Kappa is 0.39 and 0.44 for sponsored (CWEB-S) and generic website (CWEB-G) data respectively, and Table 3 presents how our agreement results compare to those of existing GEC datasets. The table also includes a number of other statistics, and the different datasets are further analyzed, compared and contrasted in Section 5. 7 8 The texts are tokenized using SpaCy9 and automatically labeled for error types (and converted into the M2 format) using the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017). Release For each dataset, we release a development and a test set: we propose a roughly equal division of the data into the two splits, which presents a fair amount of errors to evaluate on (see Table 2). To avoid copyright restrictions, we split the collected paragraphs into sentences and shuffle all sentences in order to break the original and coherent structure that would be needed to reproduce the copyrighted material. This approach has successfully been used in previous work for devising web-based corpora (Sch¨afer, 2015; Biemann et al., 2007). The data is available at https: //github.c"
2020.emnlp-main.680,P15-1068,0,0.133747,"minimum number of edits to make the text grammatical. During error annotation, the annotators have access to the entire paragraph in which a sentence belongs, therefore using the context of a sentence to help them in the correction. Examples of erroneous sentences from our data are shown in Table 1. Annotator agreement is calculated at the sentence level using Cohen’s Kappa, i.e. we calculate whether annotators agree on which sentences are erroneous. This approach is preferable to relying on exact matching of error corrections, as as there are often many different ways to correct a sentence (Bryant and Ng, 2015). Kappa is 0.39 and 0.44 for sponsored (CWEB-S) and generic website (CWEB-G) data respectively, and Table 3 presents how our agreement results compare to those of existing GEC datasets. The table also includes a number of other statistics, and the different datasets are further analyzed, compared and contrasted in Section 5. 7 8 The texts are tokenized using SpaCy9 and automatically labeled for error types (and converted into the M2 format) using the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017). Release For each dataset, we release a development and a test set: we propose a roughly"
2020.emnlp-main.680,D16-1195,0,0.0124626,"s gathered, we select paragraphs randomly. We split the data with respect to where they 4 https://commoncrawl.org/ https://github.com/miso-belica/ jusText 6 Using the langdetect package. 5 3 Total Test CWEB-S CWEB-G Total 2014)], student essays [W&I+LOCNESS (Bryant et al., 2019; Granger, 1998)] or target a specific domain [scientific writing; AESW (Daudaravicius et al., 2016)]. Supervised systems trained on specific domains are less likely to be as effective at correcting distinctive errors from other domains, as is the case for systems trained on learner data with different native languages (Chollampatt et al., 2016; Nadejde and Tetreault, 2019). The recent BEA 2019 shared task (Bryant et al., 2019) encouraged research in the use of low-resource and unsupervised approaches; however, evaluation primarily targeted the restricted domain of student essays. We show that when applied to data outside of the language learning domain, current state-of-the-art systems exhibit low precision due to a tendency to over-predict errors. Recent work tackled the domain adaptation problem, and released GEC benchmarks from Wikipedia data and online comments [GMEG Wiki+Yahoo (Napoles et al., 2019)]. However, these datasets p"
2020.emnlp-main.680,W13-1703,0,0.0688895,".77 41.89 37.57 32.26 36.00 14.05 21.34 13.24 23.00 13.88 21.58 17.27 19.97 15.75 20.28 16.91 19.98 33.08 26.97 31.29 8.78 14.29 9.67 18.91 8.94 14.98 5.73 10.80 8.78 15.11 6.15 11.43 PIE system 32.77 44.71 23.11 19.66 30.24 35.58 Table 5: Scores of two SOTA GEC systems on each domain. For both systems performance is substantially lower on CWEB than ESL domains. Scores are calculated against each individual annotator and averaged pre-trained on synthetic errors and fine-tuned on learner data from the train section of FCE (Yannakoudakis et al., 2011), Lang-8 (Mizumoto et al., 2011), and NUCLE (Dahlmeier et al., 2013) and for GEC-PSEUDODATA additionally on the W&I train split (Bryant et al., 2019). Performance is evaluated using the F0.5 metric calculated by ERRANT (Bryant et al., 2017).18 However, the more annotators a dataset has, the higher score a system will get on this data (Bryant and Ng, 2015). In order to perform a fair comparison of systems across datasets with a different number of annotators, we calculate the ERRANT score against each individual annotator and then take the average to get the final score. Evaluation results are presented in Table 5. Across all datasets, we observe lower scores w"
2020.emnlp-main.680,W16-0506,0,0.109906,"ements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English6 and incomplete sentences. To ensure diversity of the data, we also remove duplicate sentences. Among the million sentences gathered, we select paragraphs randomly. We split the data with respect to where they 4 https://commoncrawl.org/ https://github.com/miso-belica/ jusText 6 Using the langdetect package. 5 3 Total Test CWEB-S CWEB-G Total 2014)], student essays [W&I+LOCNESS (Bryant et al., 2019; Granger, 1998)] or target a specific domain [scientific writing; AESW (Daudaravicius et al., 2016)]. Supervised systems trained on specific domains are less likely to be as effective at correcting distinctive errors from other domains, as is the case for systems trained on learner data with different native languages (Chollampatt et al., 2016; Nadejde and Tetreault, 2019). The recent BEA 2019 shared task (Bryant et al., 2019) encouraged research in the use of low-resource and unsupervised approaches; however, evaluation primarily targeted the restricted domain of student essays. We show that when applied to data outside of the language learning domain, current state-of-the-art systems exhi"
2020.emnlp-main.680,N19-1423,0,0.00755079,"mprovements than datasets from more ad∆S ∆P 0.3 4 3 0.2 2 0.1 1 0 :P R: U N C OT T H E M R :D U :P ET U N R: CT SP EL R: L PR R: EP O RT R: H V ER B 0 ∆ Semantic Similarity (∆S) ∆ Perplexity Ratio (∆P ) ·10−2 M We limit our analysis to sentences containing exactly one edit, as we are interested in how individual edits change a sentence, regardless of how domains differ in amounts of erroneous sentences and in the number of edits per sentence (Table 3). Regarding 1), to measure the semantic change of a sentence after an edit is introduced, we use sentence embeddings generated by Sentence-BERT (Devlin et al., 2019) and calculate the cosine similarity between the original sentence and its corrected counterpart. Regarding 2), the degree of sentence improvement is calculated as the ratio of the perplexity of GPT-2 (Radford et al., 2019) on a sentence after and before it has been edited. Figure 4: Difference in semantic similarity and perplexity ratio between CWEB-S and FCE for the most frequent error types (M: missing; R: replace; U: unnecessary). vanced speakers. CWEB and AESW in particular stand out, with edits that largely retain the semantics of a sentence and that result in more subtle improvements. E"
2020.emnlp-main.680,D19-1119,0,0.063297,"sets). In particular, precision is improved (+20.8/+18.6 on CWEB-G/S) at the expense of recall (−6.4/−2.8 on CWEB-G/S). However, performance is still low compared to the language learning domain (F0.5 of at least 41), further indicating that there is scope for developing more robust and general-purpose, open-domain GEC systems. For the purpose of future benchmarking, Appendix B lists the system's ERRANT scores based on both annotators – as opposed to the average of individual annotator scores reported in Table 6. 19 www.github.com/chrisjbryant/errant 8471 We use the fine-tuning parameters of Kiyono et al. (2019). 5 Analysis FCE (PSEUDO) Wiki (PSEUDO) CWEB-G (PSEUDO) In order to assess the impact our new dataset can have on the GEC field, we carry out analyses to show 1) to what degree the domain of our data is different from existing GEC corpora, and how existing GEC systems are affected by the domain shift; and 2) that a factor behind the performance drop on CWEB data is the inability of systems to rely on a strong internal language model in low error density domains. 5.1 FCE (PIE) Wiki (PIE) CWEB-G (PIE) precision 60 40 20 Domain Shift Moving from error correction in learner texts to error correcti"
2020.emnlp-main.680,I11-1017,0,0.0219533,"13 35.94 47.09 52.81 34.13 23.02 43.77 41.89 37.57 32.26 36.00 14.05 21.34 13.24 23.00 13.88 21.58 17.27 19.97 15.75 20.28 16.91 19.98 33.08 26.97 31.29 8.78 14.29 9.67 18.91 8.94 14.98 5.73 10.80 8.78 15.11 6.15 11.43 PIE system 32.77 44.71 23.11 19.66 30.24 35.58 Table 5: Scores of two SOTA GEC systems on each domain. For both systems performance is substantially lower on CWEB than ESL domains. Scores are calculated against each individual annotator and averaged pre-trained on synthetic errors and fine-tuned on learner data from the train section of FCE (Yannakoudakis et al., 2011), Lang-8 (Mizumoto et al., 2011), and NUCLE (Dahlmeier et al., 2013) and for GEC-PSEUDODATA additionally on the W&I train split (Bryant et al., 2019). Performance is evaluated using the F0.5 metric calculated by ERRANT (Bryant et al., 2017).18 However, the more annotators a dataset has, the higher score a system will get on this data (Bryant and Ng, 2015). In order to perform a fair comparison of systems across datasets with a different number of annotators, we calculate the ERRANT score against each individual annotator and then take the average to get the final score. Evaluation results are presented in Table 5. Across all"
2020.emnlp-main.680,D19-5504,0,0.0150608,"graphs randomly. We split the data with respect to where they 4 https://commoncrawl.org/ https://github.com/miso-belica/ jusText 6 Using the langdetect package. 5 3 Total Test CWEB-S CWEB-G Total 2014)], student essays [W&I+LOCNESS (Bryant et al., 2019; Granger, 1998)] or target a specific domain [scientific writing; AESW (Daudaravicius et al., 2016)]. Supervised systems trained on specific domains are less likely to be as effective at correcting distinctive errors from other domains, as is the case for systems trained on learner data with different native languages (Chollampatt et al., 2016; Nadejde and Tetreault, 2019). The recent BEA 2019 shared task (Bryant et al., 2019) encouraged research in the use of low-resource and unsupervised approaches; however, evaluation primarily targeted the restricted domain of student essays. We show that when applied to data outside of the language learning domain, current state-of-the-art systems exhibit low precision due to a tendency to over-predict errors. Recent work tackled the domain adaptation problem, and released GEC benchmarks from Wikipedia data and online comments [GMEG Wiki+Yahoo (Napoles et al., 2019)]. However, these datasets present a high density of error"
2020.emnlp-main.680,Q19-1032,0,0.0408577,"Missing"
2020.emnlp-main.680,E17-2037,0,0.017618,"into sentences and shuffle all sentences in order to break the original and coherent structure that would be needed to reproduce the copyrighted material. This approach has successfully been used in previous work for devising web-based corpora (Sch¨afer, 2015; Biemann et al., 2007). The data is available at https: //github.com/SimonHFL/CWEB. 3 GEC Corpora We compare our data with existing GEC corpora which cover a range of domains and proficiency levels. Table 3 presents a number of different statistics and Table 4 their error-type frequencies.10 3.1 English as a second language (ESL) JFLEG (Napoles et al., 2017) The JHU FluencyExtended GUG corpus consists of sentences written by English language learners (with different proficiency levels and L1s) for the TOEFL® exam, 9 top-level domains: .gov, .edu, .mil, .int, and .museum. top-level domains: .com, .info, .net, .org. 10 8469 https://spacy.io/ See links to downloadable versions in Appendix A W&I JFLEG FCE 2.1 CoNLL14 A P UNCT V ERB OTHER D ET N OUN P REP S PELL A LL 147.7 233.5 295.6 180.7 167.7 107.1 242.5 112.3 176.7 138.3 149.1 105.4 113.8 107.8 1675.6 1084.9 65.5 200.5 158.1 134.9 116.8 92.7 26.0 244.8 300.0 237.3 159.1 139.8 137.2 79.3 LOCNESS B"
2020.emnlp-main.680,W14-1701,0,0.0302274,"1050.7 504.1 400.6 732.3 635.3 239.2 G S 48.9 23.4 31.6 20.9 19.6 15.6 3.8 48.7 13.1 21.0 19.7 12.8 9.8 2.4 208.9 147.2 Table 4: Number of error occurrences for the most frequent error types (per 10, 000 token). covering a range of topics. Texts have been corrected for grammatical errors and fluency. FCE (Yannakoudakis et al., 2011) consists of 1, 244 error corrected texts produced by learners taking the First Certificate in English exam, which assesses English at an upper-intermediate level. We use the data split made available for the BEA GEC shared task 2019 (Bryant et al., 2019). CoNLL14 (Ng et al., 2014) consists of (mostly argumentative) essays written by ESL learners from the National University of Singapore, which are annotated for grammatical errors by two native speakers of English. Write&Improve (W&I) (Bryant et al., 2019) Cambridge English Write & Improve (Yannakoudakis et al., 2018) is an online web platform that automatically provides diagnostic feedback to non-native English-language learners, including an overall language proficiency score based on the Common European Framework of Reference for Languages (CEFR).11 The W&I corpus contains 3, 600 texts across 3 different CEFR levels"
2020.emnlp-main.680,W17-5019,0,0.0334002,"Missing"
2020.emnlp-main.680,P11-1019,1,0.767761,"rse range of writing and constitute a major part of what people read and write on an everyday basis. This work highlights two major prevailing challenges of current approaches to GEC: domain adaptation and low precision in texts with low error density. Previous work has primarily targeted essaystyle text with high error density (see Figure 1); however, this lack of diversity means that it is not clear how systems perform on other domains and under different error distributions (Sakaguchi et al., 2017).2 Current publicly available datasets are restricted to non-native English essays [e.g. FCE (Yannakoudakis et al., 2011); CoNLL14 (Ng et al., 2 Leacock et al. (2010) highlighted the variations in the distribution of errors in non-native and native English writings. 8467 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8467–8478, c November 16–20, 2020. 2020 Association for Computational Linguistics Error type Example sentence V ERB :S VA They develop positive relationships with swimmers and members, and promotes promote programs in order to generate more participation. In a small agriculture agricultural town on the east side of Washington state State called Yakima."
2020.findings-emnlp.14,K15-1038,1,0.847444,"nally efficient NLP, e.g., document classification with a time budget (Xu et al., 2012; Nan et al., 2016; Nan and Saligrama, 2017). Moreover, as I show be1 Such data is readily available for normal-paced reading in the form of corpora such as the Dundee Corpus and the GECO Corpus: https://www2.ling.ohio-state. edu/golddundee/ and http://expsy.ugent.be/ downloads/geco/, respectively. These datasets have been used in machine learning experiments aimed at predicting fixations during reading (Nilsson and Nivre, 2009; Matthies and Søgaard, 2013), as well as as auxiliary data for various NLP tasks (Barrett and Søgaard, 2015; Klerke et al., 2016). Klerke et al. (2016), for example, show that jointly predicting fixations during reading is beneficial for a sentence compression model, trying to shorten and simplify input sentences. 148 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 148–153 c November 16 - 20, 2020. 2020 Association for Computational Linguistics low, neural speed reading architectures perform poorly compared to simple baseline approaches to fast document classification. Contributions In sum, this paper makes the following contributions: (a) I argue speed reading reduces"
2020.findings-emnlp.14,D18-1474,0,0.0775707,"ontributions: (a) I argue speed reading reduces to computationally efficient NLP, e.g., fast document classification. (b) I therefore present a heads-to-heads comparison of a state-of-the-art speed reading architecture to a simple n-grambased classifier. (c) Our simple n-gram-based classifier is shown to be significantly better and faster than the speed reading architecture. 2 Speed Reading Speed reading, as a machine learning task, was only introduced about three years ago, but has attracted a lot of attention (Yu et al., 2017; Johansen and Socher, 2017; Huang et al., 2017; Seo et al., 2018; Fu and Ma, 2018; Yu et al., 2018b; Hansen et al., 2019): All proposed models so far are extensions of recurrent neural networks for text classification or sequence labeling - mostly long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997) - that learn to either skip, skim or re-read words, jump elsewhere in the text or to make early predictions. As mentioned, none of the papers on neural speed reading, some of which are reviewed below, evaluate the extent to which they simulate human speed reading strategies. While the idea of human speed reading has intrigued modern society for decades – a"
2020.findings-emnlp.14,P18-1031,0,0.0430586,"Missing"
2020.findings-emnlp.14,W17-2631,0,0.0251227,"ification. Contributions In sum, this paper makes the following contributions: (a) I argue speed reading reduces to computationally efficient NLP, e.g., fast document classification. (b) I therefore present a heads-to-heads comparison of a state-of-the-art speed reading architecture to a simple n-grambased classifier. (c) Our simple n-gram-based classifier is shown to be significantly better and faster than the speed reading architecture. 2 Speed Reading Speed reading, as a machine learning task, was only introduced about three years ago, but has attracted a lot of attention (Yu et al., 2017; Johansen and Socher, 2017; Huang et al., 2017; Seo et al., 2018; Fu and Ma, 2018; Yu et al., 2018b; Hansen et al., 2019): All proposed models so far are extensions of recurrent neural networks for text classification or sequence labeling - mostly long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997) - that learn to either skip, skim or re-read words, jump elsewhere in the text or to make early predictions. As mentioned, none of the papers on neural speed reading, some of which are reviewed below, evaluate the extent to which they simulate human speed reading strategies. While the idea of human spe"
2020.findings-emnlp.14,N16-1179,1,0.825901,"document classification with a time budget (Xu et al., 2012; Nan et al., 2016; Nan and Saligrama, 2017). Moreover, as I show be1 Such data is readily available for normal-paced reading in the form of corpora such as the Dundee Corpus and the GECO Corpus: https://www2.ling.ohio-state. edu/golddundee/ and http://expsy.ugent.be/ downloads/geco/, respectively. These datasets have been used in machine learning experiments aimed at predicting fixations during reading (Nilsson and Nivre, 2009; Matthies and Søgaard, 2013), as well as as auxiliary data for various NLP tasks (Barrett and Søgaard, 2015; Klerke et al., 2016). Klerke et al. (2016), for example, show that jointly predicting fixations during reading is beneficial for a sentence compression model, trying to shorten and simplify input sentences. 148 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 148–153 c November 16 - 20, 2020. 2020 Association for Computational Linguistics low, neural speed reading architectures perform poorly compared to simple baseline approaches to fast document classification. Contributions In sum, this paper makes the following contributions: (a) I argue speed reading reduces to computationally eff"
2020.findings-emnlp.14,D13-1075,1,0.828595,"ing is therefore not a new task, but reduces to the well-known task of computationally efficient NLP, e.g., document classification with a time budget (Xu et al., 2012; Nan et al., 2016; Nan and Saligrama, 2017). Moreover, as I show be1 Such data is readily available for normal-paced reading in the form of corpora such as the Dundee Corpus and the GECO Corpus: https://www2.ling.ohio-state. edu/golddundee/ and http://expsy.ugent.be/ downloads/geco/, respectively. These datasets have been used in machine learning experiments aimed at predicting fixations during reading (Nilsson and Nivre, 2009; Matthies and Søgaard, 2013), as well as as auxiliary data for various NLP tasks (Barrett and Søgaard, 2015; Klerke et al., 2016). Klerke et al. (2016), for example, show that jointly predicting fixations during reading is beneficial for a sentence compression model, trying to shorten and simplify input sentences. 148 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 148–153 c November 16 - 20, 2020. 2020 Association for Computational Linguistics low, neural speed reading architectures perform poorly compared to simple baseline approaches to fast document classification. Contributions In sum, t"
2020.findings-emnlp.14,W09-1113,0,0.0459198,"tegies. Neural speed reading is therefore not a new task, but reduces to the well-known task of computationally efficient NLP, e.g., document classification with a time budget (Xu et al., 2012; Nan et al., 2016; Nan and Saligrama, 2017). Moreover, as I show be1 Such data is readily available for normal-paced reading in the form of corpora such as the Dundee Corpus and the GECO Corpus: https://www2.ling.ohio-state. edu/golddundee/ and http://expsy.ugent.be/ downloads/geco/, respectively. These datasets have been used in machine learning experiments aimed at predicting fixations during reading (Nilsson and Nivre, 2009; Matthies and Søgaard, 2013), as well as as auxiliary data for various NLP tasks (Barrett and Søgaard, 2015; Klerke et al., 2016). Klerke et al. (2016), for example, show that jointly predicting fixations during reading is beneficial for a sentence compression model, trying to shorten and simplify input sentences. 148 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 148–153 c November 16 - 20, 2020. 2020 Association for Computational Linguistics low, neural speed reading architectures perform poorly compared to simple baseline approaches to fast document classifica"
2020.findings-emnlp.14,D16-1013,0,0.05496,"Missing"
2020.findings-emnlp.14,P17-1172,0,0.0593735,"Missing"
2020.lrec-1.515,W19-0417,0,0.0222911,"nd Fung, 2009; Shi et al., 2016; Beloucif and Wu, 2018), discourse parsing (Mihaylov and Frank, 2016), question answering (Surdeanu et al., 2003; Moschitti et al., 2003; Shen and Lapata, 2007; Berant and Liang, 2014) and classifying reason and stance in online debates (Hasan and Ng, 2014). Frame-semantic parsers, however, are normally trained on manually annotated resources such as the FrameNet corpus (Baker et al., 1998) or the OntoNotes corpus (Pradhan and Xue, 2009; Weischedel et al., 2013). However, such annotations only exist for a small subset of the world’s languages. Some recent work (Aminian et al., 2019) solves this problem using annotation projection. Moreover, in contrast with previous methods, the authors have no supervision from lemmas, part-of-speech (POS) tags or syntactic parse trees. Language Sentences EN Angola borders Namibia to the south, Zambia to the east, the Democratic Republic of the Congo to the north-east. ES ´ Angola, es un pa´ıs ubicado al sur de Africa que tiene ´ fronteras con la Republica Democr´atica del Congo. DE Angola grenzt an Namibia, Sambia, die Republik Kongo, die Demokratische Republik Kongo und den Atlantischen Ozean. Table 1: An example of a Wikipedia sentenc"
2020.lrec-1.515,P98-1013,0,0.500242,"f information has been shown to improve multiple downstream tasks such as information extraction (Bastianelli et al., 2013), machine translation (Knight and Luk, 1994; Ueffing et al., 2007; Wu and Fung, 2009; Shi et al., 2016; Beloucif and Wu, 2018), discourse parsing (Mihaylov and Frank, 2016), question answering (Surdeanu et al., 2003; Moschitti et al., 2003; Shen and Lapata, 2007; Berant and Liang, 2014) and classifying reason and stance in online debates (Hasan and Ng, 2014). Frame-semantic parsers, however, are normally trained on manually annotated resources such as the FrameNet corpus (Baker et al., 1998) or the OntoNotes corpus (Pradhan and Xue, 2009; Weischedel et al., 2013). However, such annotations only exist for a small subset of the world’s languages. Some recent work (Aminian et al., 2019) solves this problem using annotation projection. Moreover, in contrast with previous methods, the authors have no supervision from lemmas, part-of-speech (POS) tags or syntactic parse trees. Language Sentences EN Angola borders Namibia to the south, Zambia to the east, the Democratic Republic of the Congo to the north-east. ES ´ Angola, es un pa´ıs ubicado al sur de Africa que tiene ´ fronteras con l"
2020.lrec-1.515,W13-3820,0,0.0281076,"ce could also be equally useful for semantic role labeling systems and other semantic parsing frameworks. The frames in frame-semantic parsing present extended predicate-argument structures that determine “who did what to whom”, “when”, “where” and “why”. For instance, in a sentence such as John gives Mary a computer in the morning, a frame-semantic parser may identify John, Mary, and computer as core arguments of the “giving” frame, and in the morning as a temporal, optional argument. This kind of information has been shown to improve multiple downstream tasks such as information extraction (Bastianelli et al., 2013), machine translation (Knight and Luk, 1994; Ueffing et al., 2007; Wu and Fung, 2009; Shi et al., 2016; Beloucif and Wu, 2018), discourse parsing (Mihaylov and Frank, 2016), question answering (Surdeanu et al., 2003; Moschitti et al., 2003; Shen and Lapata, 2007; Berant and Liang, 2014) and classifying reason and stance in online debates (Hasan and Ng, 2014). Frame-semantic parsers, however, are normally trained on manually annotated resources such as the FrameNet corpus (Baker et al., 1998) or the OntoNotes corpus (Pradhan and Xue, 2009; Weischedel et al., 2013). However, such annotations onl"
2020.lrec-1.515,P14-1133,0,0.0360405,"as John gives Mary a computer in the morning, a frame-semantic parser may identify John, Mary, and computer as core arguments of the “giving” frame, and in the morning as a temporal, optional argument. This kind of information has been shown to improve multiple downstream tasks such as information extraction (Bastianelli et al., 2013), machine translation (Knight and Luk, 1994; Ueffing et al., 2007; Wu and Fung, 2009; Shi et al., 2016; Beloucif and Wu, 2018), discourse parsing (Mihaylov and Frank, 2016), question answering (Surdeanu et al., 2003; Moschitti et al., 2003; Shen and Lapata, 2007; Berant and Liang, 2014) and classifying reason and stance in online debates (Hasan and Ng, 2014). Frame-semantic parsers, however, are normally trained on manually annotated resources such as the FrameNet corpus (Baker et al., 1998) or the OntoNotes corpus (Pradhan and Xue, 2009; Weischedel et al., 2013). However, such annotations only exist for a small subset of the world’s languages. Some recent work (Aminian et al., 2019) solves this problem using annotation projection. Moreover, in contrast with previous methods, the authors have no supervision from lemmas, part-of-speech (POS) tags or syntactic parse trees. Lan"
2020.lrec-1.515,J14-1002,0,0.0270876,"d a discriminative model for semantic role labeling using frame-semantics. (Thompson et al., 2003) proposed a generative model trained on FrameNet for shallow semantic parsing and frame identifications tasks. (Shi and Mihalcea, 2004) proposed to identify frames and their elements using a rule-based approach. (Johansson and Nugues, 2007) proposed a framesemantic structure extraction model based on syntactic dependency parsing; they also propose a method to strengthen FrameNet by automatically adding new units to its lexical database. More recent work on frame-semantic parsing includes SEMAFOR (Das et al., 2014; Kshirsagar et al., 2015), a frame-semantic parser for identifying and labeling the semantic arguments of a given predicate that evokes a specific FrameNet frame. (Ringgaard et al., 2017) use a modified version of OntoNotes (Pradhan and Xue, 2009; Weischedel et al., 2013) in their experiments, and we use a similar modification of the CoNLL 2009 corpora. That said, our parsing experiments are merely meant to illustrate the usefulness of the W IKI BANK resource. To the best of our knowledge, there are two similar works regarding the creation of automatic labeled resources for semantic role labe"
2020.lrec-1.515,S15-1029,0,0.0227054,"5), a frame-semantic parser for identifying and labeling the semantic arguments of a given predicate that evokes a specific FrameNet frame. (Ringgaard et al., 2017) use a modified version of OntoNotes (Pradhan and Xue, 2009; Weischedel et al., 2013) in their experiments, and we use a similar modification of the CoNLL 2009 corpora. That said, our parsing experiments are merely meant to illustrate the usefulness of the W IKI BANK resource. To the best of our knowledge, there are two similar works regarding the creation of automatic labeled resources for semantic role labeling. In the first one (Exner et al., 2015), the authors create a dataset using loosely parallel sentences from Wikipedia and transfer the predicate-argument structure from the source language (English) to the target language. The alignment is achieved using the Wikidata IDs, extracted using a dictionary and a named entity linker. In the second one (Hartmann et al., 2016), the approach is to use the distant supervision paradigm to transfer the labels from a Linked Lexical Resource, a combination of several resource like WordNet, FrameNet and Wikitionary, to a large unlabeled corpus (e.g. web pages). Both of these solution require addit"
2020.lrec-1.515,P00-1065,0,0.362272,"oNLL 2009 corpora and W IKI BANK. 5. Background Frame semantics (Fillmore, 1982; Baker et al., 1998; Swayamdipta et al., 2017) is the study of how linguistic forms affect frame knowledge, and how these frames thus could be integrated into an understanding of sentences and documents (Baker et al., 1998). Frame-semantic parsers typically rely on supervised learning to train complex mod4186 els on top of a syntactic parser, induced from manually annotated resources, such as FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2005). One of the earliest works on frame-semantic parsing is by (Gildea and Jurafsky, 2000), who proposed a discriminative model for semantic role labeling using frame-semantics. (Thompson et al., 2003) proposed a generative model trained on FrameNet for shallow semantic parsing and frame identifications tasks. (Shi and Mihalcea, 2004) proposed to identify frames and their elements using a rule-based approach. (Johansson and Nugues, 2007) proposed a framesemantic structure extraction model based on syntactic dependency parsing; they also propose a method to strengthen FrameNet by automatically adding new units to its lexical database. More recent work on frame-semantic parsing inclu"
2020.lrec-1.515,W09-1201,0,0.1037,"Missing"
2020.lrec-1.515,Q16-1015,0,0.0180117,"ora. That said, our parsing experiments are merely meant to illustrate the usefulness of the W IKI BANK resource. To the best of our knowledge, there are two similar works regarding the creation of automatic labeled resources for semantic role labeling. In the first one (Exner et al., 2015), the authors create a dataset using loosely parallel sentences from Wikipedia and transfer the predicate-argument structure from the source language (English) to the target language. The alignment is achieved using the Wikidata IDs, extracted using a dictionary and a named entity linker. In the second one (Hartmann et al., 2016), the approach is to use the distant supervision paradigm to transfer the labels from a Linked Lexical Resource, a combination of several resource like WordNet, FrameNet and Wikitionary, to a large unlabeled corpus (e.g. web pages). Both of these solution require additional resource which are not always available for low-resource languages. (Exner et al., 2015) dictionary creation requires a POS tagger, languagedependent rules, and entity databases; while (Hartmann et al., 2016) use two Linked Lexical Resource, Uby and SemLink, which support very few languages. 6. Conclusion We introduced W IK"
2020.lrec-1.515,D14-1083,0,0.020261,"ntify John, Mary, and computer as core arguments of the “giving” frame, and in the morning as a temporal, optional argument. This kind of information has been shown to improve multiple downstream tasks such as information extraction (Bastianelli et al., 2013), machine translation (Knight and Luk, 1994; Ueffing et al., 2007; Wu and Fung, 2009; Shi et al., 2016; Beloucif and Wu, 2018), discourse parsing (Mihaylov and Frank, 2016), question answering (Surdeanu et al., 2003; Moschitti et al., 2003; Shen and Lapata, 2007; Berant and Liang, 2014) and classifying reason and stance in online debates (Hasan and Ng, 2014). Frame-semantic parsers, however, are normally trained on manually annotated resources such as the FrameNet corpus (Baker et al., 1998) or the OntoNotes corpus (Pradhan and Xue, 2009; Weischedel et al., 2013). However, such annotations only exist for a small subset of the world’s languages. Some recent work (Aminian et al., 2019) solves this problem using annotation projection. Moreover, in contrast with previous methods, the authors have no supervision from lemmas, part-of-speech (POS) tags or syntactic parse trees. Language Sentences EN Angola borders Namibia to the south, Zambia to the eas"
2020.lrec-1.515,P17-1044,0,0.0286806,"14,288 9,988 14,555 0.33 0.98 Table 4: Number of correctly labeled arguments in W IK I BANK for 10K samples in English using a pretrained SRL model. not contain a verb. The last step is to automatically annotate the extracted sentences with frame-semantic labels. We assign the label ARG0 to the entity, V for the target in the property, and ARG1 to the value. If a sentence contains multiple verbs, we consider each verb annotation as a different example. Table 3 shows the number of sentences and examples extracted for each language. We used a pre-existing semantic role labeling model, DeepSRL (He et al., 2017), pretrained on CoNLL 2005, to see how well examples in W IKI BANK are annotated. For the evaluation we used a sample of 10K English sentences, and obtained the results shown in Table 4. We consider an argument label to be correct if the entity label from Wikidata is contained in the sequence extracted by DeepSRL. From Table 4, we note that the class ARG0 has a lower accuracy compared to ARG1. After further data analysis, we realized that this is due to the ARG0 containing names of locations, dates, and in general entities that are labeled as ARG1 by the DeepSRL model. 3.1. Baseline S LING (Ri"
2020.lrec-1.515,P16-1145,0,0.012194,"nnotated sentences. Examples in W IKI BANK consists of semantically labelled sentences, where each sentence has partial-semantic annotation for the predicate and its semantic arguments (see example in Table 2). W IKI BANK is derived directly from Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and Wikipedia. Wikidata is a collaborative knowledge base, containing triples (entity id, property id, value id) that define a type of relation holding between an entity and a value (which can also be an entity). Wikidata also contains labels, and aliases for the properties, entities, and values. Following (Hewlett et al., 2016), for each Wikidata item, we replace the IDs in each statement with the text label for properties and values that are entities, and with a human readable version for numeric values (e.g., time-stamp is converted into readable date), obtaining triples (entity, property, value). The extraction of sentences from Wikipedia is performed using distant supervision, as in (Levy et al., 2017). For each triple, we take the corresponding Wikipedia article for the entity then we extract the first sentence containing the entity, the property, and the value. To increase the amount of examples, we also use W"
2020.lrec-1.515,S07-1048,0,0.0715118,"rvised learning to train complex mod4186 els on top of a syntactic parser, induced from manually annotated resources, such as FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2005). One of the earliest works on frame-semantic parsing is by (Gildea and Jurafsky, 2000), who proposed a discriminative model for semantic role labeling using frame-semantics. (Thompson et al., 2003) proposed a generative model trained on FrameNet for shallow semantic parsing and frame identifications tasks. (Shi and Mihalcea, 2004) proposed to identify frames and their elements using a rule-based approach. (Johansson and Nugues, 2007) proposed a framesemantic structure extraction model based on syntactic dependency parsing; they also propose a method to strengthen FrameNet by automatically adding new units to its lexical database. More recent work on frame-semantic parsing includes SEMAFOR (Das et al., 2014; Kshirsagar et al., 2015), a frame-semantic parser for identifying and labeling the semantic arguments of a given predicate that evokes a specific FrameNet frame. (Ringgaard et al., 2017) use a modified version of OntoNotes (Pradhan and Xue, 2009; Weischedel et al., 2013) in their experiments, and we use a similar modif"
2020.lrec-1.515,Q17-1024,0,0.088187,"Missing"
2020.lrec-1.515,P15-2036,0,0.0202957,"model for semantic role labeling using frame-semantics. (Thompson et al., 2003) proposed a generative model trained on FrameNet for shallow semantic parsing and frame identifications tasks. (Shi and Mihalcea, 2004) proposed to identify frames and their elements using a rule-based approach. (Johansson and Nugues, 2007) proposed a framesemantic structure extraction model based on syntactic dependency parsing; they also propose a method to strengthen FrameNet by automatically adding new units to its lexical database. More recent work on frame-semantic parsing includes SEMAFOR (Das et al., 2014; Kshirsagar et al., 2015), a frame-semantic parser for identifying and labeling the semantic arguments of a given predicate that evokes a specific FrameNet frame. (Ringgaard et al., 2017) use a modified version of OntoNotes (Pradhan and Xue, 2009; Weischedel et al., 2013) in their experiments, and we use a similar modification of the CoNLL 2009 corpora. That said, our parsing experiments are merely meant to illustrate the usefulness of the W IKI BANK resource. To the best of our knowledge, there are two similar works regarding the creation of automatic labeled resources for semantic role labeling. In the first one (Ex"
2020.lrec-1.515,K17-1034,0,0.0507866,"Missing"
2020.lrec-1.515,K16-2014,0,0.016066,"gument structures that determine “who did what to whom”, “when”, “where” and “why”. For instance, in a sentence such as John gives Mary a computer in the morning, a frame-semantic parser may identify John, Mary, and computer as core arguments of the “giving” frame, and in the morning as a temporal, optional argument. This kind of information has been shown to improve multiple downstream tasks such as information extraction (Bastianelli et al., 2013), machine translation (Knight and Luk, 1994; Ueffing et al., 2007; Wu and Fung, 2009; Shi et al., 2016; Beloucif and Wu, 2018), discourse parsing (Mihaylov and Frank, 2016), question answering (Surdeanu et al., 2003; Moschitti et al., 2003; Shen and Lapata, 2007; Berant and Liang, 2014) and classifying reason and stance in online debates (Hasan and Ng, 2014). Frame-semantic parsers, however, are normally trained on manually annotated resources such as the FrameNet corpus (Baker et al., 1998) or the OntoNotes corpus (Pradhan and Xue, 2009; Weischedel et al., 2013). However, such annotations only exist for a small subset of the world’s languages. Some recent work (Aminian et al., 2019) solves this problem using annotation projection. Moreover, in contrast with pre"
2020.lrec-1.515,S14-2008,0,0.0396678,"Missing"
2020.lrec-1.515,J05-1004,0,0.764427,"g W IKI BANK as a bridge for crosslingual transfer. We facilitate cross-lingual transfer by using multilingual word embeddings (Lample et al., 2018) and, following (Johnson et al., 2017), by prepending all sentences with a language ID – as well as a task ID (CoNLL or W IKI BANK) for integrating the W IKI BANK sentences. Furthermore, we experiment with reducing the full label sets in the corpora to a common, simplified label set, to facilitate cross-lingual transfer. The simplified labels disregard thematic role information. For both setups, we normalize labels to be conform with the PropBank (Palmer et al., 2005) notation (e.g., A1 becomes ARG1). However, as shown in Figure 1, the experiments with the full label set have a slightly better accuracy than the ones with a simplified label set, so we will present only the results for the former. 3.3. Protocol We experiment with three languages, and subsamples of the training data of size 0 (zero-shot), 100, 500, 1000, and 2000, as well as the full training set, and also with and without cross-lingual transfer. The cross-lingual training setup consists of various combination of source languages, with both CoNLL and W IKI BANK, and the target language W IKI"
2020.lrec-1.515,N09-4006,0,0.30775,"also integrate this form of supervision into an off-the-shelf frame-semantic parser and allow cross-lingual transfer. Using Google’s S LING architecture, we show significant improvements on the English and Spanish CoNLL 2009 datasets, whether training on the full available datasets or small subsamples thereof. Keywords: cross-lingual frame semantic parsing, multilinguality, data augmentation 1. Introduction Shallow semantic parsing comes in many varieties, including frame-semantic parsing (T¨ackstr¨om et al., 2015; Ringgaard et al., 2017), semantic role labeling (SRL) (Surdeanu et al., 2008; Pradhan and Xue, 2009; Weischedel et al., 2013; Zhang et al., 2019), and semantic dependency parsing (Oepen et al., 2014). In this paper, we present W IK I BANK , a multilingual resource with partial semantic dependency structures projected from an existing knowledge base. W IKI BANK is created automatically, and used to augment pre-existing resources, or reduce the annotation effort for low-resource languages. We show how it can be used to improve an off-the-shelf frame-semantic parser, but this resource could also be equally useful for semantic role labeling systems and other semantic parsing frameworks. The fra"
2020.lrec-1.515,D07-1002,0,0.0402386,"ce, in a sentence such as John gives Mary a computer in the morning, a frame-semantic parser may identify John, Mary, and computer as core arguments of the “giving” frame, and in the morning as a temporal, optional argument. This kind of information has been shown to improve multiple downstream tasks such as information extraction (Bastianelli et al., 2013), machine translation (Knight and Luk, 1994; Ueffing et al., 2007; Wu and Fung, 2009; Shi et al., 2016; Beloucif and Wu, 2018), discourse parsing (Mihaylov and Frank, 2016), question answering (Surdeanu et al., 2003; Moschitti et al., 2003; Shen and Lapata, 2007; Berant and Liang, 2014) and classifying reason and stance in online debates (Hasan and Ng, 2014). Frame-semantic parsers, however, are normally trained on manually annotated resources such as the FrameNet corpus (Baker et al., 1998) or the OntoNotes corpus (Pradhan and Xue, 2009; Weischedel et al., 2013). However, such annotations only exist for a small subset of the world’s languages. Some recent work (Aminian et al., 2019) solves this problem using annotation projection. Moreover, in contrast with previous methods, the authors have no supervision from lemmas, part-of-speech (POS) tags or s"
2020.lrec-1.515,N04-3006,0,0.128831,"anding of sentences and documents (Baker et al., 1998). Frame-semantic parsers typically rely on supervised learning to train complex mod4186 els on top of a syntactic parser, induced from manually annotated resources, such as FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2005). One of the earliest works on frame-semantic parsing is by (Gildea and Jurafsky, 2000), who proposed a discriminative model for semantic role labeling using frame-semantics. (Thompson et al., 2003) proposed a generative model trained on FrameNet for shallow semantic parsing and frame identifications tasks. (Shi and Mihalcea, 2004) proposed to identify frames and their elements using a rule-based approach. (Johansson and Nugues, 2007) proposed a framesemantic structure extraction model based on syntactic dependency parsing; they also propose a method to strengthen FrameNet by automatically adding new units to its lexical database. More recent work on frame-semantic parsing includes SEMAFOR (Das et al., 2014; Kshirsagar et al., 2015), a frame-semantic parser for identifying and labeling the semantic arguments of a given predicate that evokes a specific FrameNet frame. (Ringgaard et al., 2017) use a modified version of On"
2020.lrec-1.515,P16-1212,0,0.0183078,"rames in frame-semantic parsing present extended predicate-argument structures that determine “who did what to whom”, “when”, “where” and “why”. For instance, in a sentence such as John gives Mary a computer in the morning, a frame-semantic parser may identify John, Mary, and computer as core arguments of the “giving” frame, and in the morning as a temporal, optional argument. This kind of information has been shown to improve multiple downstream tasks such as information extraction (Bastianelli et al., 2013), machine translation (Knight and Luk, 1994; Ueffing et al., 2007; Wu and Fung, 2009; Shi et al., 2016; Beloucif and Wu, 2018), discourse parsing (Mihaylov and Frank, 2016), question answering (Surdeanu et al., 2003; Moschitti et al., 2003; Shen and Lapata, 2007; Berant and Liang, 2014) and classifying reason and stance in online debates (Hasan and Ng, 2014). Frame-semantic parsers, however, are normally trained on manually annotated resources such as the FrameNet corpus (Baker et al., 1998) or the OntoNotes corpus (Pradhan and Xue, 2009; Weischedel et al., 2013). However, such annotations only exist for a small subset of the world’s languages. Some recent work (Aminian et al., 2019) solves th"
2020.lrec-1.515,P03-1002,0,0.222105,"to whom”, “when”, “where” and “why”. For instance, in a sentence such as John gives Mary a computer in the morning, a frame-semantic parser may identify John, Mary, and computer as core arguments of the “giving” frame, and in the morning as a temporal, optional argument. This kind of information has been shown to improve multiple downstream tasks such as information extraction (Bastianelli et al., 2013), machine translation (Knight and Luk, 1994; Ueffing et al., 2007; Wu and Fung, 2009; Shi et al., 2016; Beloucif and Wu, 2018), discourse parsing (Mihaylov and Frank, 2016), question answering (Surdeanu et al., 2003; Moschitti et al., 2003; Shen and Lapata, 2007; Berant and Liang, 2014) and classifying reason and stance in online debates (Hasan and Ng, 2014). Frame-semantic parsers, however, are normally trained on manually annotated resources such as the FrameNet corpus (Baker et al., 1998) or the OntoNotes corpus (Pradhan and Xue, 2009; Weischedel et al., 2013). However, such annotations only exist for a small subset of the world’s languages. Some recent work (Aminian et al., 2019) solves this problem using annotation projection. Moreover, in contrast with previous methods, the authors have no supervis"
2020.lrec-1.515,W08-2121,0,0.123828,"Missing"
2020.lrec-1.515,Q15-1003,0,0.0437515,"Missing"
2020.lrec-1.515,P07-1004,0,0.0432853,"other semantic parsing frameworks. The frames in frame-semantic parsing present extended predicate-argument structures that determine “who did what to whom”, “when”, “where” and “why”. For instance, in a sentence such as John gives Mary a computer in the morning, a frame-semantic parser may identify John, Mary, and computer as core arguments of the “giving” frame, and in the morning as a temporal, optional argument. This kind of information has been shown to improve multiple downstream tasks such as information extraction (Bastianelli et al., 2013), machine translation (Knight and Luk, 1994; Ueffing et al., 2007; Wu and Fung, 2009; Shi et al., 2016; Beloucif and Wu, 2018), discourse parsing (Mihaylov and Frank, 2016), question answering (Surdeanu et al., 2003; Moschitti et al., 2003; Shen and Lapata, 2007; Berant and Liang, 2014) and classifying reason and stance in online debates (Hasan and Ng, 2014). Frame-semantic parsers, however, are normally trained on manually annotated resources such as the FrameNet corpus (Baker et al., 1998) or the OntoNotes corpus (Pradhan and Xue, 2009; Weischedel et al., 2013). However, such annotations only exist for a small subset of the world’s languages. Some recent"
2020.lrec-1.515,N09-2004,0,0.0270265,"g frameworks. The frames in frame-semantic parsing present extended predicate-argument structures that determine “who did what to whom”, “when”, “where” and “why”. For instance, in a sentence such as John gives Mary a computer in the morning, a frame-semantic parser may identify John, Mary, and computer as core arguments of the “giving” frame, and in the morning as a temporal, optional argument. This kind of information has been shown to improve multiple downstream tasks such as information extraction (Bastianelli et al., 2013), machine translation (Knight and Luk, 1994; Ueffing et al., 2007; Wu and Fung, 2009; Shi et al., 2016; Beloucif and Wu, 2018), discourse parsing (Mihaylov and Frank, 2016), question answering (Surdeanu et al., 2003; Moschitti et al., 2003; Shen and Lapata, 2007; Berant and Liang, 2014) and classifying reason and stance in online debates (Hasan and Ng, 2014). Frame-semantic parsers, however, are normally trained on manually annotated resources such as the FrameNet corpus (Baker et al., 1998) or the OntoNotes corpus (Pradhan and Xue, 2009; Weischedel et al., 2013). However, such annotations only exist for a small subset of the world’s languages. Some recent work (Aminian et al"
2020.lrec-1.515,D19-1057,0,0.0158013,"off-the-shelf frame-semantic parser and allow cross-lingual transfer. Using Google’s S LING architecture, we show significant improvements on the English and Spanish CoNLL 2009 datasets, whether training on the full available datasets or small subsamples thereof. Keywords: cross-lingual frame semantic parsing, multilinguality, data augmentation 1. Introduction Shallow semantic parsing comes in many varieties, including frame-semantic parsing (T¨ackstr¨om et al., 2015; Ringgaard et al., 2017), semantic role labeling (SRL) (Surdeanu et al., 2008; Pradhan and Xue, 2009; Weischedel et al., 2013; Zhang et al., 2019), and semantic dependency parsing (Oepen et al., 2014). In this paper, we present W IK I BANK , a multilingual resource with partial semantic dependency structures projected from an existing knowledge base. W IKI BANK is created automatically, and used to augment pre-existing resources, or reduce the annotation effort for low-resource languages. We show how it can be used to improve an off-the-shelf frame-semantic parser, but this resource could also be equally useful for semantic role labeling systems and other semantic parsing frameworks. The frames in frame-semantic parsing present extended"
2020.lrec-1.565,C18-1139,0,0.0184988,"icts were caused by an annotator missing an entity but in Section 4.1. we outline some of the difficult cases. Table 2 shows the the final train set class distribution. We annotate NE using the four classes, LOC(ation), ORG(anisation), PER(son) and MISC(ellaneous), following the guidelines of the CoNLL-2003 NE annotation scheme (Tjong Kim Sang and De Meulder, 2003). Despite being a predecessor to the more extensive CoNLL-2012 NE annotation scheme, the CoNLL-2003 scheme is still actively used for annotation (Darwish, 2013; Fromreide et al., 2014) as well as for evaluation (Peters et al., 2018; Akbik et al., 2018). Below we summarize the annotation guidelines for the four classes. 2 https://github.com/ UniversalDependencies/UD_Danish-DDT 3 The Danish text in the Copenhagen Dependency Treebank is similar to the Danish Dependency Treebank. Figure 1: Normalized confusion matrix when comparing our validation and test set annotation to the annotation of Plank (2019). LOC includes locations like cities, roads and mountains, as well as both public and commercial places like specific buildings or meeting points, but also abstract places. PER consists of names of people, fictional characters, and animals. The n"
2020.lrec-1.565,N19-4010,0,0.0217249,"Missing"
2020.lrec-1.565,N19-1078,0,0.0210238,"Missing"
2020.lrec-1.565,bick-2004-named,0,0.204587,"rained models are made publicly available. Keywords: named entity recognition, resource, Danish, cross-lingual transfer 1. Introduction Named entity recognition (NER) is a crucial, yet challenging, component of a wide range of natural language processing applications, ranging from knowledge base population and other natural language understanding tasks to privacy protection systems. For Danish, there has not yet been a larger named entity (NE) annotated training set and until very recently; also no test set. Despite the lack of freely available training data, a few NER tools exist for Danish (Bick, 2004; Johannessen et al., 2005; Derczynski, 2019; Al-Rfou et al., 2015) but until Plank (2019), they have not been consistently benchmarked. This study found that Danish NER can benefit from transfer from English. In this paper, we present a new annotated resource for Danish NE, DaNE, which contains annotations of the Danish Universal Dependencies treebank (Johannsen et al., 2015). We describe the annotation process and evaluate the annotation quality intrinsically by looking at annotator agreement, and extrinsically by comparing to the recent NE annotation of the validation and test split of the"
2020.lrec-1.565,D19-5531,0,0.0121598,"ustness results of M -BERT and DA -BERT models on subsets of entities in the validation set and on a lowercased version of test set. F1 score averaged over five runs and calculated without MISC. Best result per class is boldfaced. evaluated on the capitalized train and test sets. However, the DA -BERT which is trained purely on lowercased text both in pre-training and fine-tuning does not rely on capitalization and thus has the highest performance on the lowercased test set. In order to use capitalization information without overfitting to these features, further data augmentation strategies (Bodapati et al., 2019) would be a promising direction for NER on less well-edited text such as usergenerated text or speech transcripts. 8. We have presented the largest Danish NE resource to date: DaNE. The data source is the Danish Universal Dependencies treebank. The resource is publicly available and will enable the training and evaluation of NER models and allows for tracking of progress for Danish NER. We have trained/fine-tuned models on DaNE and benchmarked the resource using state-of-the-art models including multilingual BERT and a recent Danish BERT model. 9. We observe that for unseen entities, MISC is m"
2020.lrec-1.565,P13-1153,0,0.0236832,"ed manually after calculating the inter-annotator agreement. The vast majority of conflicts were caused by an annotator missing an entity but in Section 4.1. we outline some of the difficult cases. Table 2 shows the the final train set class distribution. We annotate NE using the four classes, LOC(ation), ORG(anisation), PER(son) and MISC(ellaneous), following the guidelines of the CoNLL-2003 NE annotation scheme (Tjong Kim Sang and De Meulder, 2003). Despite being a predecessor to the more extensive CoNLL-2012 NE annotation scheme, the CoNLL-2003 scheme is still actively used for annotation (Darwish, 2013; Fromreide et al., 2014) as well as for evaluation (Peters et al., 2018; Akbik et al., 2018). Below we summarize the annotation guidelines for the four classes. 2 https://github.com/ UniversalDependencies/UD_Danish-DDT 3 The Danish text in the Copenhagen Dependency Treebank is similar to the Danish Dependency Treebank. Figure 1: Normalized confusion matrix when comparing our validation and test set annotation to the annotation of Plank (2019). LOC includes locations like cities, roads and mountains, as well as both public and commercial places like specific buildings or meeting points, but al"
2020.lrec-1.565,E14-2016,0,0.433113,"Missing"
2020.lrec-1.565,N19-1423,0,0.273698,"to the recent NE annotation of the validation and test split of the same resource (Plank, 2019). We benchmark our new resource with supervised state-of-the-art NE taggers, both off-the-shelf systems and models that are trained or fine-tuned on our train set. Our train set is smaller than available resources in other North-Germanic languages (Swedish, Norwegian (Bokm˚al and Nynorsk) and West-Germanic languages (English and Dutch). We, therefore, experiment with different ways of improving Danish NER using cross-lingual transfer from these languages using multilingual BERT (Pires et al., 2019; Devlin et al., 2019). Contributions We introduce the largest gold-annotated and publicly available Danish NE dataset. We train/finetune state-of-the-art NER systems on our train set as well ∗ as explore cross-lingual transfer from five related languages using multilingual BERT and benchmark these on our resource. All resources and trained models are made publicly available.1 2. Related work A few tools for Danish NER (Derczynski, 2019; Bick, 2004; Johannessen et al., 2005) have been presented. Plank (2019) recently annotated the Danish Universal Dependencies validation and test sets as well as 10,000 tokens from"
2020.lrec-1.565,fromreide-etal-2014-crowdsourcing,1,0.810632,"er calculating the inter-annotator agreement. The vast majority of conflicts were caused by an annotator missing an entity but in Section 4.1. we outline some of the difficult cases. Table 2 shows the the final train set class distribution. We annotate NE using the four classes, LOC(ation), ORG(anisation), PER(son) and MISC(ellaneous), following the guidelines of the CoNLL-2003 NE annotation scheme (Tjong Kim Sang and De Meulder, 2003). Despite being a predecessor to the more extensive CoNLL-2012 NE annotation scheme, the CoNLL-2003 scheme is still actively used for annotation (Darwish, 2013; Fromreide et al., 2014) as well as for evaluation (Peters et al., 2018; Akbik et al., 2018). Below we summarize the annotation guidelines for the four classes. 2 https://github.com/ UniversalDependencies/UD_Danish-DDT 3 The Danish text in the Copenhagen Dependency Treebank is similar to the Danish Dependency Treebank. Figure 1: Normalized confusion matrix when comparing our validation and test set annotation to the annotation of Plank (2019). LOC includes locations like cities, roads and mountains, as well as both public and commercial places like specific buildings or meeting points, but also abstract places. PER c"
2020.lrec-1.565,L18-1550,0,0.055272,"Missing"
2020.lrec-1.565,L18-1473,0,0.0127006,"d EuroParl (Tiedemann, 2012). The F LAIR embeddings are the extracted 1024dimensional hidden states of a biLSTM character-level language model. We train the F LAIR embeddings for five epochs with a batch size of 50. The F LAIR embeddings are concatenated with FastText embeddings (described in Section 5.2.1.). The models are trained for 150 epochs and a batch size of 32 with SGD optimization and an annealing learning rate starting at 0.1. We also train a F LAIR model where we concatenate the F LAIR embeddings and FastText with byte pair encoding (BPE) embeddings computed on Danish Wikipedia by Heinzerling and Strube (2018). 5.2.3. Danish BERT BERT is a transformer-based architecture that can be pretrained on a large corpus of raw text. Devlin et al. (2019) show that it requires only a small amount of fine-tuning of pre-trained BERT representations to obtain high performance on, e.g., NER. Recently, a Danish pre-trained BERT was made publicly available8 . The model is pretrained on Danish lowercased text from Common Crawl, Danish Wikipedia, OpenSubtitles (Lison and Tiedemann, 2016) and various online forums. For our BERT experiments, we use a public implementation of BERT9 along with the pre-trained weights for"
2020.lrec-1.565,2020.lrec-1.559,0,0.0448215,"Missing"
2020.lrec-1.565,N16-1030,0,0.0145086,"e link structure from Wikipedia. The model recognizes the three tags: LOC, ORG, and PER, but not MISC. We benchmark the model implemented in the P OLYGLOT framework4 DANER DANER5 (Derczynski, 2019) is a wrapper around the Stanford CoreNLP (Manning et al., 2014) using data from Derczynski et al. (2014) (not released). Like P OLYGLOT , this model recognizes LOC, ORG, and PER, but not MISC. 5.2. Models trained/fine-tuned on the dataset For all trained/fine-tuned models, we report average performance over five different random seeds. 5.2.1. biLSTM+CRF We train a bidirectional LSTM (biLSTM) model (Lample et al., 2016) using a public implementation6 . The model consists of a single-layer biLSTM (Hochreiter and Schmidhuber, 1997) with 200 hidden dimensions that takes a concatenation of character encoding and word embeddings as input. The character encoding is obtained by applying a biLSTM with 50 hidden dimensions on 25-dimensional character embeddings. The CRF uses the output from the biLSTM to make a prediction. The model is trained for 100 epochs with early stopping and a dropout of 0.5, a batch size of 10 and it uses SGD as optimizer with a learning rate of 0.01. For pre-trained word embeddings (+EM), we"
2020.lrec-1.565,P19-1301,0,0.0515998,"Missing"
2020.lrec-1.565,L16-1147,0,0.0213696,"where we concatenate the F LAIR embeddings and FastText with byte pair encoding (BPE) embeddings computed on Danish Wikipedia by Heinzerling and Strube (2018). 5.2.3. Danish BERT BERT is a transformer-based architecture that can be pretrained on a large corpus of raw text. Devlin et al. (2019) show that it requires only a small amount of fine-tuning of pre-trained BERT representations to obtain high performance on, e.g., NER. Recently, a Danish pre-trained BERT was made publicly available8 . The model is pretrained on Danish lowercased text from Common Crawl, Danish Wikipedia, OpenSubtitles (Lison and Tiedemann, 2016) and various online forums. For our BERT experiments, we use a public implementation of BERT9 along with the pre-trained weights for the Danish BERT (DA BERT). We fine-tune the BERT model on the DaNE train set for 5 epochs with a learning rate of 5 · 10−5 and a batch size of 8. Checkpoints are evaluated on the validation set every 50 iterations. 5.2.4. Multilingual BERT Using a BERT pretrained on text in 104 languages, has shown promising results for cross-lingual transfer for NER (Pires et al., 2019). We experiment with cross-lingual trans7 https://dumps.wikimedia.org/dawiki/ latest/ 8 https:"
2020.lrec-1.565,P14-5010,0,0.00555382,"Missing"
2020.lrec-1.565,N18-1202,0,0.0485861,"ast majority of conflicts were caused by an annotator missing an entity but in Section 4.1. we outline some of the difficult cases. Table 2 shows the the final train set class distribution. We annotate NE using the four classes, LOC(ation), ORG(anisation), PER(son) and MISC(ellaneous), following the guidelines of the CoNLL-2003 NE annotation scheme (Tjong Kim Sang and De Meulder, 2003). Despite being a predecessor to the more extensive CoNLL-2012 NE annotation scheme, the CoNLL-2003 scheme is still actively used for annotation (Darwish, 2013; Fromreide et al., 2014) as well as for evaluation (Peters et al., 2018; Akbik et al., 2018). Below we summarize the annotation guidelines for the four classes. 2 https://github.com/ UniversalDependencies/UD_Danish-DDT 3 The Danish text in the Copenhagen Dependency Treebank is similar to the Danish Dependency Treebank. Figure 1: Normalized confusion matrix when comparing our validation and test set annotation to the annotation of Plank (2019). LOC includes locations like cities, roads and mountains, as well as both public and commercial places like specific buildings or meeting points, but also abstract places. PER consists of names of people, fictional character"
2020.lrec-1.565,P19-1493,0,0.0663333,"sically by comparing to the recent NE annotation of the validation and test split of the same resource (Plank, 2019). We benchmark our new resource with supervised state-of-the-art NE taggers, both off-the-shelf systems and models that are trained or fine-tuned on our train set. Our train set is smaller than available resources in other North-Germanic languages (Swedish, Norwegian (Bokm˚al and Nynorsk) and West-Germanic languages (English and Dutch). We, therefore, experiment with different ways of improving Danish NER using cross-lingual transfer from these languages using multilingual BERT (Pires et al., 2019; Devlin et al., 2019). Contributions We introduce the largest gold-annotated and publicly available Danish NE dataset. We train/finetune state-of-the-art NER systems on our train set as well ∗ as explore cross-lingual transfer from five related languages using multilingual BERT and benchmark these on our resource. All resources and trained models are made publicly available.1 2. Related work A few tools for Danish NER (Derczynski, 2019; Bick, 2004; Johannessen et al., 2005) have been presented. Plank (2019) recently annotated the Danish Universal Dependencies validation and test sets as well"
2020.lrec-1.565,W19-6143,0,0.137541,"Danish, cross-lingual transfer 1. Introduction Named entity recognition (NER) is a crucial, yet challenging, component of a wide range of natural language processing applications, ranging from knowledge base population and other natural language understanding tasks to privacy protection systems. For Danish, there has not yet been a larger named entity (NE) annotated training set and until very recently; also no test set. Despite the lack of freely available training data, a few NER tools exist for Danish (Bick, 2004; Johannessen et al., 2005; Derczynski, 2019; Al-Rfou et al., 2015) but until Plank (2019), they have not been consistently benchmarked. This study found that Danish NER can benefit from transfer from English. In this paper, we present a new annotated resource for Danish NE, DaNE, which contains annotations of the Danish Universal Dependencies treebank (Johannsen et al., 2015). We describe the annotation process and evaluate the annotation quality intrinsically by looking at annotator agreement, and extrinsically by comparing to the recent NE annotation of the validation and test split of the same resource (Plank, 2019). We benchmark our new resource with supervised state-of-the-ar"
2020.lrec-1.565,2020.lrec-1.585,1,0.704135,"Missing"
2020.lrec-1.565,solberg-etal-2014-norwegian,0,0.110168,"dencies validation and test sets as well as 10,000 tokens from the train set, benchmarked existing models and explored the potentials for cross-lingual transfer from English to Danish. This study found that neural transfer is possible, and that a small amount of Danish NE training data helps crosslingual models. For the related North-Germanic languages, there are already NE resources. The large Stockholm-Ume˚a Corpus (SUC) (Nilsson Bj¨orkenstam and Bystr¨om, 2012) is also annotated with NE in SUC 2.0 (K¨allgren, 1998) and SUC ¨ 3.0 (Ostling, 2012). Recently, the Norwegian Dependency Treebank (Solberg et al., 2014) was also annotated with NE (NorNE) (Jørgensen et al., 2020). English, German, and Dutch are established languages in CoNLL shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for NER. Looking at other languages, state-of-the-art performance on NER has been achieved on a set of benchmark tasks, e.g., the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) on English and German by Akbik et al. (2018) using their proposed contextual string embeddings, denoted F LAIR embeddings. The results have been further improved by Akbik et al. (2019b) proposing a pooled version"
2020.lrec-1.565,P19-1527,0,0.0327751,"Missing"
2020.lrec-1.565,tiedemann-2012-parallel,0,0.0455954,"s-lingual experiments compared to DaNE. Measures below the line only concern the train split. 5.2.2. F LAIR The sequence tagging architecture implemented in the F LAIR framework (Akbik et al., 2019a) is a biLSTM+CRF based on Huang et al. (2015) with the option of passing concatenated embeddings of different types. Using contextual embeddings (F LAIR embeddings) in this architecture has shown state-of-the-art performance on NER tasks in other languages (Akbik et al., 2018; Akbik et al., 2019b). Therefore, we have pre-trained Danish F LAIR embeddings on Danish text from Wikipedia7 and EuroParl (Tiedemann, 2012). The F LAIR embeddings are the extracted 1024dimensional hidden states of a biLSTM character-level language model. We train the F LAIR embeddings for five epochs with a batch size of 50. The F LAIR embeddings are concatenated with FastText embeddings (described in Section 5.2.1.). The models are trained for 150 epochs and a batch size of 32 with SGD optimization and an annealing learning rate starting at 0.1. We also train a F LAIR model where we concatenate the F LAIR embeddings and FastText with byte pair encoding (BPE) embeddings computed on Danish Wikipedia by Heinzerling and Strube (2018"
2020.lrec-1.565,W03-0419,0,0.135972,"Missing"
2020.lrec-1.565,W02-2024,0,0.0820739,"Missing"
2020.lrec-1.9,N07-1011,0,0.0569267,"ks are scrubbed, we parse their original Wikipedia pages to get the entities. Lastly we remove all markups, references and lists from the documents. We collect a comprehensive list of English pronouns for linking. Some pronouns by their definition, almost never refer to entities. For example, (i) interrogative pronouns: ‘what’, ‘which’, etc., (ii) relative pronouns: ‘as’, ‘who’, etc., and (iii) indefinite pronouns: ‘anyone’, ‘many’, etc. For completeness, we do not remove these words from the list. We however allow the annotators to mark them specifically as No Reference. Mental models in NLP Culotta et al. (2007) present a probabilistic first-order logic approach to coreference resolution that implicitly relies on mental models. Peng et al. (2015) focus on hard Winogradstyle coreference problems and formulate coreference resolution as an Integer Linear Programming (ILP) to reason about likely models. Finkel and Manning (2008) also explore simple ILPs over simple first-order models for improving coreference resolution. They obtain improvements by focusing on enforcing transitivity of coreference links. In general, the use of first order models has a long history in NLP, rooted in formal semantics, goin"
2020.lrec-1.9,day-etal-2004-callisto,0,0.0310604,"comparable annotation experiments. Fig. 1 showcases a concrete example from the collected dataset. 2.1. Related Work Annotation interfaces The idea of easing the cognitive load of annotators by changing the way data is represented, is at the core of many papers on annotation interfaces. Early tools like MMAX2 (M¨uller and Strube, 2006) provide a clean user interface for annotators by highlighting mentions and connecting entity chains to visualize coreference along with helpful features like inter-annotator agreement checker, corpus querying, etc. Newer tools like WebAnno (Yimam et al., 2013; Day et al., 2004) ease the process of annotation by having support for flexible multi-layer annotations on a single document and also provide project management utilities. APLenty (Nghiem and Ananiadou, 2018) provides automatic annotations for easing annotator load and also has Contributions This paper makes a technical contribution, a conceptual contribution, and introduces a novel corpus annotated with coreference to the NLP community: (a) 74 an active learning component which makes the automatic annotations more accurate over time. For relieving annotator load, these tools form clusters of coreference such"
2020.lrec-1.9,P08-2012,0,0.0342454,"nouns: ‘what’, ‘which’, etc., (ii) relative pronouns: ‘as’, ‘who’, etc., and (iii) indefinite pronouns: ‘anyone’, ‘many’, etc. For completeness, we do not remove these words from the list. We however allow the annotators to mark them specifically as No Reference. Mental models in NLP Culotta et al. (2007) present a probabilistic first-order logic approach to coreference resolution that implicitly relies on mental models. Peng et al. (2015) focus on hard Winogradstyle coreference problems and formulate coreference resolution as an Integer Linear Programming (ILP) to reason about likely models. Finkel and Manning (2008) also explore simple ILPs over simple first-order models for improving coreference resolution. They obtain improvements by focusing on enforcing transitivity of coreference links. In general, the use of first order models has a long history in NLP, rooted in formal semantics, going back to Fregean semantics. Blackburn and Bos (2005), for example, present a comprehensive framework for solving NLP problems by building up first order models of discourses. 2.3. 3.2. Coreference datasets Annotation To test our hypothesis that model-based coreference annotations are faster to create and more coheren"
2020.lrec-1.9,L16-1021,0,0.0230458,"s 2 and 3 respectively. The tool takes in a pre-defined list of mentions (pronouns in our case) which are markable. The annotators can link only these words with coreferent entities. This reduces the cognitive load on the annotators. The annotation process for the two tasks is briefly described below. The main resource for English coreference resolution, also used in the CoNLL 2012 Shared Task, is OntoNotes (Pradhan et al., 2012). OntoNotes consists of data from multiple domains, ranging from newswire to broadcast conversations, and also contains annotations for Arabic and Chinese. WikiCoref (Ghaddar and Langlais, 2016) is a smaller resource with annotated sentences sampled from English Wikipedia. Our dataset includes paragraphs from all pages annotated in WikiCoref, for comparability with this annotation project. See §5 for discussion. These are the datasets used below, but alternatives exist: GAP (Webster et al., 2018) is another evaluation benchmark, also sampled from Wikipedia and focuses on addressing gender bias in coreference systems. Phrase Detectives (Poesio et al., 2013) gamifies the creation of anaphoric resources for Wikipedia pages, fiction and art history texts. Cohen et al. (2017) annotate jou"
2020.lrec-1.9,N18-2108,0,0.0134823,"annotation task. This effect is observed throughout the dataset. Also, in span annotation tasks, while some annotators link mention pronouns to the first occurrence of an entity, some link them to the latest occurrence, sometimes resulting in multiple clusters instead of one. By design, this is not the case in the grounded tasks. State-of-the-art We run our data through three state-of-the-art coreference resolution systems and report the average precision, recall and F1 scores of three standard metrics: MUC, B3 and CEAFe (Cai and Strube, 2010), in Table 2.2 While Clark and Manning (2016)3 and Lee et al. (2018) train on OntoNotes 5 to perform both mention detection and entity linking, Aralikatte et al. (2019) use a multi-task architecture for resolving coreference and ellipsis posed as reading comprehension, which is also trained on OntoNotes 5, but uses gold bracketing of the mentions and performs only entity linking.4 The results show that the dataset is hard even for the current state-of-the-art and thus a good resource to evaluate new research. 5.1. Comparison with WikiCoref WikiCoref has 30 annotated pages from English Wikipedia. Our dataset contains 200 documents of which 30 titles are the sam"
2020.lrec-1.9,D18-2019,0,0.017813,"annotators by changing the way data is represented, is at the core of many papers on annotation interfaces. Early tools like MMAX2 (M¨uller and Strube, 2006) provide a clean user interface for annotators by highlighting mentions and connecting entity chains to visualize coreference along with helpful features like inter-annotator agreement checker, corpus querying, etc. Newer tools like WebAnno (Yimam et al., 2013; Day et al., 2004) ease the process of annotation by having support for flexible multi-layer annotations on a single document and also provide project management utilities. APLenty (Nghiem and Ananiadou, 2018) provides automatic annotations for easing annotator load and also has Contributions This paper makes a technical contribution, a conceptual contribution, and introduces a novel corpus annotated with coreference to the NLP community: (a) 74 an active learning component which makes the automatic annotations more accurate over time. For relieving annotator load, these tools form clusters of coreference such that the annotator can choose to link a mention to one of these clusters. But this is possible only after the clusters are well-formed i.e. after some amount of annotation has taken place. On"
2020.lrec-1.9,N15-1082,0,0.0221076,"cuments. We collect a comprehensive list of English pronouns for linking. Some pronouns by their definition, almost never refer to entities. For example, (i) interrogative pronouns: ‘what’, ‘which’, etc., (ii) relative pronouns: ‘as’, ‘who’, etc., and (iii) indefinite pronouns: ‘anyone’, ‘many’, etc. For completeness, we do not remove these words from the list. We however allow the annotators to mark them specifically as No Reference. Mental models in NLP Culotta et al. (2007) present a probabilistic first-order logic approach to coreference resolution that implicitly relies on mental models. Peng et al. (2015) focus on hard Winogradstyle coreference problems and formulate coreference resolution as an Integer Linear Programming (ILP) to reason about likely models. Finkel and Manning (2008) also explore simple ILPs over simple first-order models for improving coreference resolution. They obtain improvements by focusing on enforcing transitivity of coreference links. In general, the use of first order models has a long history in NLP, rooted in formal semantics, going back to Fregean semantics. Blackburn and Bos (2005), for example, present a comprehensive framework for solving NLP problems by buildin"
2020.lrec-1.9,W12-4501,0,0.033276,"nter-annotator agreement and the other 70 were singly annotated. An annotation tool with two interfaces is built, one for each task, with slight differences between them as shown in Figures 2 and 3 respectively. The tool takes in a pre-defined list of mentions (pronouns in our case) which are markable. The annotators can link only these words with coreferent entities. This reduces the cognitive load on the annotators. The annotation process for the two tasks is briefly described below. The main resource for English coreference resolution, also used in the CoNLL 2012 Shared Task, is OntoNotes (Pradhan et al., 2012). OntoNotes consists of data from multiple domains, ranging from newswire to broadcast conversations, and also contains annotations for Arabic and Chinese. WikiCoref (Ghaddar and Langlais, 2016) is a smaller resource with annotated sentences sampled from English Wikipedia. Our dataset includes paragraphs from all pages annotated in WikiCoref, for comparability with this annotation project. See §5 for discussion. These are the datasets used below, but alternatives exist: GAP (Webster et al., 2018) is another evaluation benchmark, also sampled from Wikipedia and focuses on addressing gender bias"
2020.lrec-1.9,W10-4305,0,0.0379472,"the annotation for the grounded task is cleaner than that for the span annotation task. This effect is observed throughout the dataset. Also, in span annotation tasks, while some annotators link mention pronouns to the first occurrence of an entity, some link them to the latest occurrence, sometimes resulting in multiple clusters instead of one. By design, this is not the case in the grounded tasks. State-of-the-art We run our data through three state-of-the-art coreference resolution systems and report the average precision, recall and F1 scores of three standard metrics: MUC, B3 and CEAFe (Cai and Strube, 2010), in Table 2.2 While Clark and Manning (2016)3 and Lee et al. (2018) train on OntoNotes 5 to perform both mention detection and entity linking, Aralikatte et al. (2019) use a multi-task architecture for resolving coreference and ellipsis posed as reading comprehension, which is also trained on OntoNotes 5, but uses gold bracketing of the mentions and performs only entity linking.4 The results show that the dataset is hard even for the current state-of-the-art and thus a good resource to evaluate new research. 5.1. Comparison with WikiCoref WikiCoref has 30 annotated pages from English Wikipedi"
2020.lrec-1.9,D18-1241,0,0.0299347,"pore Janelle Ann Women’s Electoral Lobby ∅ Figure 1: Example of an annotation from the dataset. The technical contribution is a novel annotation methodology, where annotation is mediated through a model representation. We believe similar techniques can be developed for other NLP tasks; see §6 for discussion. (b) The conceptual contribution is a discussion of the importance of mental models in human language processing, and an argument for explicitly representing this level of representation in NLP models. (c) Our corpus consists of manually annotated sentences from English Wikipedia and QuAC (Choi et al., 2018). In addition to the model-based annotations, we also provide the coreference links obtained in our baseline experiments. (1) I knocked on the door of room 624. He wasn’t in. 2. The introduction of the referent of he in (1) is implied by the introduction of the entity room 624. In this paper, we present a new approach to annotating coreference that enables simple annotation of examples such as (1): Instead of asking an annotator to relate pronouns and previous spans of text, we ask the annotator to link pronouns and entities in document models. Moreover, we argue that model-based annotation re"
2020.lrec-1.9,D16-1245,0,0.0215418,"leaner than that for the span annotation task. This effect is observed throughout the dataset. Also, in span annotation tasks, while some annotators link mention pronouns to the first occurrence of an entity, some link them to the latest occurrence, sometimes resulting in multiple clusters instead of one. By design, this is not the case in the grounded tasks. State-of-the-art We run our data through three state-of-the-art coreference resolution systems and report the average precision, recall and F1 scores of three standard metrics: MUC, B3 and CEAFe (Cai and Strube, 2010), in Table 2.2 While Clark and Manning (2016)3 and Lee et al. (2018) train on OntoNotes 5 to perform both mention detection and entity linking, Aralikatte et al. (2019) use a multi-task architecture for resolving coreference and ellipsis posed as reading comprehension, which is also trained on OntoNotes 5, but uses gold bracketing of the mentions and performs only entity linking.4 The results show that the dataset is hard even for the current state-of-the-art and thus a good resource to evaluate new research. 5.1. Comparison with WikiCoref WikiCoref has 30 annotated pages from English Wikipedia. Our dataset contains 200 documents of whic"
2020.lrec-1.9,P13-4001,0,0.0796263,"Missing"
2021.acl-short.138,2020.udw-1.6,0,0.0630061,"Missing"
2021.acl-short.138,2020.iwpt-1.20,1,0.828267,"Missing"
2021.acl-short.138,D18-1312,0,0.0212604,"nce is well attested (Sagae et al., 2008; Falenska and C¸etino˘glu, 2017; Strzyz et al., 2019; Dehouck et al., 2020). Sentence length has also been observed to impact performance (McDonald and Nivre, 2011). One likely factor behind this is different sentence lengths having difference dependency distance distributions (Ferrer-i-Cancho and Liu, 2014) which in turn affects parsing as longer dependencies are typically harder to parse (Anderson and G´omez-Rodr´ıguez, 2020; Falenska et al., 2020). Others have offered explanations based on linguistic characteristics such as morphological complexity (Dehouck and Denis, 2018; C¸o¨ ltekin, 2020), part-of-speech bigram perplexity (Berdicevskis et al., 2018), and word order freedom (Gulordava and Merlo, 2016). The history of reproduction and replication in NLP is not so well established, with only a few studies in recent years, e.g. on Universal Dependency (UD) parsing (C¸o¨ ltekin, 2020) and on automatic essay scoring systems (Huber and C¸o¨ ltekin, 2020). Linear techniques, linear regression models or evaluating correlation coefficients are commonly used for statistical analyses of NLP systems. They have been used to model constituency parser performance (Ravi et"
2021.acl-short.138,2020.iwpt-1.4,0,0.0805913,"Missing"
2021.acl-short.138,Q16-1025,0,0.0219691,"th has also been observed to impact performance (McDonald and Nivre, 2011). One likely factor behind this is different sentence lengths having difference dependency distance distributions (Ferrer-i-Cancho and Liu, 2014) which in turn affects parsing as longer dependencies are typically harder to parse (Anderson and G´omez-Rodr´ıguez, 2020; Falenska et al., 2020). Others have offered explanations based on linguistic characteristics such as morphological complexity (Dehouck and Denis, 2018; C¸o¨ ltekin, 2020), part-of-speech bigram perplexity (Berdicevskis et al., 2018), and word order freedom (Gulordava and Merlo, 2016). The history of reproduction and replication in NLP is not so well established, with only a few studies in recent years, e.g. on Universal Dependency (UD) parsing (C¸o¨ ltekin, 2020) and on automatic essay scoring systems (Huber and C¸o¨ ltekin, 2020). Linear techniques, linear regression models or evaluating correlation coefficients are commonly used for statistical analyses of NLP systems. They have been used to model constituency parser performance (Ravi et al., 2008), to evaluate what affects annotation agreement (Bayerl and Paul, 2011), to investigate what impacts statistical MT systems"
2021.acl-short.138,C12-1063,0,0.0302733,"The history of reproduction and replication in NLP is not so well established, with only a few studies in recent years, e.g. on Universal Dependency (UD) parsing (C¸o¨ ltekin, 2020) and on automatic essay scoring systems (Huber and C¸o¨ ltekin, 2020). Linear techniques, linear regression models or evaluating correlation coefficients are commonly used for statistical analyses of NLP systems. They have been used to model constituency parser performance (Ravi et al., 2008), to evaluate what affects annotation agreement (Bayerl and Paul, 2011), to investigate what impacts statistical MT systems (Guzman and Vogel, 2012), what impacts performance on span identifying tasks (Papay et al., 2020), and many other examples. Therefore, it is likely that lessons drawn from this replication 1090 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 1090–1098 August 1–6, 2021. ©2021 Association for Computational Linguistics Training size + DUG + hLtest i All CoNLL18 Original UDPipe 1.2 UDPipe 2.0 CoNLL18 10 seeds UDPipe 1.2 UDPipe 2.0 0.014 0.228 0.195 0.100 0.061 0.169 0.060 0.097 0.146 -0"
2021.acl-short.138,2020.lrec-1.688,0,0.033096,"Missing"
2021.acl-short.138,J11-1007,0,0.033717,"be many crossovers between training and test data. DUG is also tied to the mean sentence length in the test data: smaller sentences are much more likely to 1 Note that in the treebanks used in this paper, namely Universal Dependencies, well-formed trees are enforced. Related Work There is a long history of investigating the causes of variance in parser performance. The effect of training data size on parser performance is well attested (Sagae et al., 2008; Falenska and C¸etino˘glu, 2017; Strzyz et al., 2019; Dehouck et al., 2020). Sentence length has also been observed to impact performance (McDonald and Nivre, 2011). One likely factor behind this is different sentence lengths having difference dependency distance distributions (Ferrer-i-Cancho and Liu, 2014) which in turn affects parsing as longer dependencies are typically harder to parse (Anderson and G´omez-Rodr´ıguez, 2020; Falenska et al., 2020). Others have offered explanations based on linguistic characteristics such as morphological complexity (Dehouck and Denis, 2018; C¸o¨ ltekin, 2020), part-of-speech bigram perplexity (Berdicevskis et al., 2018), and word order freedom (Gulordava and Merlo, 2016). The history of reproduction and replication in"
2021.acl-short.138,2020.emnlp-main.396,0,0.091548,"Missing"
2021.acl-short.138,D08-1093,0,0.144821,"Missing"
2021.acl-short.138,W08-0504,0,0.0298886,"erformance. However, DUG has two important covariants. The size of the training data impacts DUG because the smaller a treebank is, the less likely there will be many crossovers between training and test data. DUG is also tied to the mean sentence length in the test data: smaller sentences are much more likely to 1 Note that in the treebanks used in this paper, namely Universal Dependencies, well-formed trees are enforced. Related Work There is a long history of investigating the causes of variance in parser performance. The effect of training data size on parser performance is well attested (Sagae et al., 2008; Falenska and C¸etino˘glu, 2017; Strzyz et al., 2019; Dehouck et al., 2020). Sentence length has also been observed to impact performance (McDonald and Nivre, 2011). One likely factor behind this is different sentence lengths having difference dependency distance distributions (Ferrer-i-Cancho and Liu, 2014) which in turn affects parsing as longer dependencies are typically harder to parse (Anderson and G´omez-Rodr´ıguez, 2020; Falenska et al., 2020). Others have offered explanations based on linguistic characteristics such as morphological complexity (Dehouck and Denis, 2018; C¸o¨ ltekin, 20"
2021.acl-short.138,2020.emnlp-main.220,1,0.935198,"entences by length and find that only a small subset of sentences vary in performance with respect to graph isomorphism. Further, the correlation observed between parser performance and graph isomorphism in the wild disappears when controlling for covariants. However, in a controlled experiment, where covariants are kept fixed, we do observe a strong correlation. We suggest that conclusions drawn from statistical analyses like this need to be tempered and that controlled experiments can complement them by more readily teasing factors apart. 1 2 Introduction We undertake a replication study of Søgaard (2020) which introduced graph isomorphism (DUG - directed unlabelled graph isomorphism) as a means of explaining differences in parser performance across different treebanks. It measures the ratio of graphs1 in the test set that were also observed in the training data. It is intuitive that this would likely be related to parser performance. However, DUG has two important covariants. The size of the training data impacts DUG because the smaller a treebank is, the less likely there will be many crossovers between training and test data. DUG is also tied to the mean sentence length in the test data: sm"
2021.acl-short.138,L16-1680,0,0.0608886,"Missing"
2021.acl-short.138,N19-1077,1,0.891052,"Missing"
2021.acl-short.138,K18-2001,0,0.0211496,"Missing"
2021.americasnlp-1.28,P19-1310,0,0.019956,"Missing"
2021.americasnlp-1.28,D17-1209,0,0.0605819,"Missing"
2021.americasnlp-1.28,2020.emnlp-main.615,0,0.0224077,"chua, which we initially experimented on, this yielded a chrF score of 0.33 on the dev set vs. 0.27 with phrase-based MT, but we ran out of time to train models for the other languages. A post-hoc evaluation on the other languages failed to replicate this success, though. Potentially, the hyperparameter configuration is very sensitive to the language in question, or the amount of training data was not enough for the other languages (Quechua had by far the largest training set of all languages in the shared task). Language Model Prior We train NMT models using a language model prior, following Baziotis et al. (2020). This method allows us to make use of the additional monolingual data we gathered (cf. Sec. 2) within a neural MT framework, and we hoped that this would help the model to produce valid words in the target languages, i.e., reduce the “babbling” effect we saw in outputs like Example (1) above. We focused our efforts on the LSTM-based models provided by the authors8 rather than the transformer ones, since we believe that those should be easier to train in this extremely low-resource setting. Despite experimenting with different hyperparameters (including number and size of LSTM layers), we coul"
2021.americasnlp-1.28,P17-1080,0,0.0645998,"Missing"
2021.americasnlp-1.28,2020.lrec-1.320,0,0.182309,"Missing"
2021.americasnlp-1.28,2020.coling-main.351,0,0.738138,"Missing"
2021.americasnlp-1.28,galarreta-etal-2017-corpus,0,0.453103,"Missing"
2021.americasnlp-1.28,L16-1666,0,0.399567,"Missing"
2021.americasnlp-1.28,W18-2703,0,0.0232089,"the cross-attention weights between the encoder and decoders. Note that these weights need to be trained from scratch, as opposed to the other weights which are initialized from language modelling checkpoints. Back-translation In an attempt to improve the transformer-based models, we used the shared task data to train similar transformer-based models in the reverse direction, i.e. to Spanish, in order to back-translate the monolingual corpora (cf. Sec. 2). This would give us automatically translated Spanish outputs to use as the source side for additional training data (Sennrich et al., 2016; Hoang et al., 2018). Since monolingual data in Spanish—which was used to pre-train the decoder’s language model for this experiment—is abundant, we expected the machine-translated Spanish text to be of reasonably good quality. However, the models turned out to perform quite badly, with the resulting Spanish text being of very low quality and often very repetitive. We therefore decided to abandon this direction after preliminary experiments. Obviously, the random babbling baseline is not meant as an actual suggestion for a translation system—it literally does not “translate” anything. However, as the official sha"
2021.americasnlp-1.28,W18-1921,0,0.0178043,"rrault et al., 2020). In contrast, the AmericasNLP 2021 shared task (Mager et al., 2021) provided us with as lit• It was surprisingly hard to beat a standard tle as 3,883 sentence pairs (for Ashaninka), and phrase-based model, as evidenced not only with the exception of Quechua (125k pairs), all by our own failed attempts, but also by this languages had fewer than 30k sentence pairs. Addisystem taking third place on three languages tionally, many of these languages are polysynthetic, in the official evaluation (track 1). which is known to provide additional challenges for machine translation (Klavans et al., 2018; Mager • It is apparently challenging for many MT syset al., 2018b). tems to even produce well-formed outputs in We initially focused our efforts on two areas: the target languages, as our random babbling (i) obtaining more data, both parallel and monolinbaseline outperformed at least one other sysgual (Sec. 2); and (ii) exploring a range of different tem on nine of the languages, and even took neural machine translation techniques, particular fifth place out of 12 on Ashaninka (track 2). those specifically developed for low-resource scenarios, to find a promising system to build on and 2 Dat"
2021.americasnlp-1.28,P07-2045,0,0.0123734,"ries in Nahuatl (Gustavo et al., 2007), Raramuri (Arvizu Castillo, 2002), Hñähñu (Mondragón et al., 2002b), and Wixárika (Mondragón et al., 2002a). Finally, we also use the Bible translated to Quechua, Guarani, and Aymara. We extract the text for all of these resources with the Google OCR API.4 3 Models We first describe the two models we submitted: a standard phrase-based model (CoAStaL-1) and a random babbling baseline (CoAStaL-2). Other models that we experimented with but did not submit for evaluation are discussed later in Sec. 5. 3.1 We train a statistical phrase-based model with Moses (Koehn et al., 2007) using default settings, following the guidelines for training a baseline.5 We do minimal preprocessing: we use the provided cleaning script and rely on plain whitespace tokenization, with the only exception that we also insert spaces around square brackets. The language model is trained with 5-grams instead of 3-grams, as this improved the results very slightly on the development sets. We train a separate model for each language and use the respective development set for tuning before translating the test set. The models we submitted did, mistakenly, not make use of the additional parallel da"
2021.americasnlp-1.28,Q17-1026,0,0.0325612,"ystem—it literally does not “translate” anything. However, as the official shared task evaluation and Character-Level NMT Since many of the lanthe examples above show, it can serve as a useful guages in the shared task are polysynthetic, a “sanity check” for situations where the performance character-level model might be better suited here, of actual MT systems is so low that it is unclear as it can better learn morphology (Belinkov et al., whether they even acquired superficial knowledge 2017). We train fully character-level models folof character distributions in the target language. lowing Lee et al. (2017), which are based on com251 bining convolutional and recurrent layers.7 Finding a good hyperparameter configuration for this model proved very time-consuming; the best configuration we found modifies the original model by using half the number of units in the embedding layer and decoder layers (256 and 512, respectively). For Quechua, which we initially experimented on, this yielded a chrF score of 0.33 on the dev set vs. 0.27 with phrase-based MT, but we ran out of time to train models for the other languages. A post-hoc evaluation on the other languages failed to replicate this success, thou"
2021.americasnlp-1.28,W18-4808,0,0.0232811,"Missing"
2021.americasnlp-1.28,2020.lrec-1.497,0,0.030821,"Missing"
2021.americasnlp-1.28,2020.acl-main.156,0,0.0108852,"observable character trigrams (e.g. iro, tsi, ant). 5 Things that did not work Here we briefly describe other ideas that we pursued, but were unfortunately not successful with, so we did not submit any systems based on these techniques for evaluation. Pre-trained Transformers Following Rothe et al. (2020), we use an auto-encoding transformer as the encoder and an auto-regressive transformer as the decoder of a sequence-to-sequence model. Out of the several configurations we experimented with, the best performance was observed when the encoder is pre-trained on the Spanish OSCAR corpus (Ortiz Suárez et al., 2020) and the decoders are pre-trained on language-specific monolingual corpora collected from the web (cf. Sec. 2) along with the target files of the training data. However, the results were not on-par with the simpler models; averaging over all languages, we observed a chrF score of 0.12 on the dev sets, compared to 0.23 with the phrase-based model (cf. Sec. 3.1). We postulate that the training data was just not enough to train the cross-attention weights between the encoder and decoders. Note that these weights need to be trained from scratch, as opposed to the other weights which are initialize"
2021.americasnlp-1.28,L16-1144,0,0.144611,"Missing"
2021.americasnlp-1.28,2020.acl-demos.14,0,0.0155321,"ng with different hyperparameters (including number and size of LSTM layers), we could not exceed an average 0.16 chrF on the dev sets (compared to 0.23 with the phrase-based model). Graph Convolutional Encoders We experiment with graph convolutional encoders using the framework by Bastings et al. (2017). Thus, we train NMT systems that operate directly over graphs; in our case, syntactic annotations of the source sentences following the Universal Dependencies (UD) scheme (Nivre et al., 2020). We parsed the all the source sentences from training set provided by the task organizer with Stanza (Qi et al., 2020). We were initially motivated to follow this approach because UD annotation can provide extra information to the encoder to generate better translations, ideally with less data. Even though we tested several configurations, not even our best architecture—two 7 We use our own reimplementation of the authors’ code. https://github.com/cbaziotis/ lm-prior-for-nmt 8 layers of GCN encoder with 250 units, and LSTM decoder with 250 units, trained for 5 epochs, with a vocabulary of 5000 words in source and target— was able to outperform the random babbling system. We hypothesize that with this amount o"
2021.americasnlp-1.28,2020.tacl-1.18,0,0.0352311,"Missing"
2021.americasnlp-1.28,P16-1009,0,0.0305886,"st not enough to train the cross-attention weights between the encoder and decoders. Note that these weights need to be trained from scratch, as opposed to the other weights which are initialized from language modelling checkpoints. Back-translation In an attempt to improve the transformer-based models, we used the shared task data to train similar transformer-based models in the reverse direction, i.e. to Spanish, in order to back-translate the monolingual corpora (cf. Sec. 2). This would give us automatically translated Spanish outputs to use as the source side for additional training data (Sennrich et al., 2016; Hoang et al., 2018). Since monolingual data in Spanish—which was used to pre-train the decoder’s language model for this experiment—is abundant, we expected the machine-translated Spanish text to be of reasonably good quality. However, the models turned out to perform quite badly, with the resulting Spanish text being of very low quality and often very repetitive. We therefore decided to abandon this direction after preliminary experiments. Obviously, the random babbling baseline is not meant as an actual suggestion for a translation system—it literally does not “translate” anything. However"
2021.americasnlp-1.28,tiedemann-2012-parallel,0,0.0189834,"uld not overlap with the data provided by the shared task organizers: Aymara from JW300 (Agi´c and Vuli´c, 2019); Guarani from Tatoeba; and Nahuatl and Quechua from the Bible corpus by Christodouloupoulos and Steedman (2015). We note that for the Bible corpus, the Nahuatl portion is from a narrower dialectal region (NHG “Tetelcingo Nahuatl”) than the data in the shared task, and it also covers a different variant of Quechua (QUW “Kichwa” vs. QUY “Ayacucho Quechua”), but we hoped that in this extremely low-resource scenario, this would still prove useful. All datasets were obtained from OPUS1 (Tiedemann, 2012). Monolingual data Wikipedias exist for Aymara, Guaraní, Nahuatl, and Quechua. We use WikiExtractor (Attardi, 2015) to obtain text data from their respective dumps,2 then use a small set of regular expressions to clean them from XML tags and entities. This gives us between 28k and 100k lines of text per language. We obtain further monolingual data from several online sources in PDF format. For Nahuatl and Hñähñu, we use a book provided by the Mexican government;3 for Quechua, we use two books: The Little Prince (Saint-Exupéry, 2018) and Antonio Raimondi’s Once upon a time.. in Peru (Villacorta"
2021.blackboxnlp-1.40,C16-1332,0,0.030066,"sks tics have been theorised to capture a wide range Task 1: Predicting geo-coordinates We probe of information (Schütze, 1992). To evaluate this, a considerable body of literature has made use of se- LMs’ representation about whether they convey information about the actual position of the locamantic similarity, relatedness, and analogy datasets tion. To this extent, we train a model to predict (Agirre et al., 2009; Bruni et al., 2012; Baroni et al., 2014; Faruqui et al., 2014; Hill et al., 2015; GPS coordinates (latitude and longitude) given the location’s representation through language modDrozd et al., 2016; Abdou et al., 2018). Asking a broader question, Rubinstein et al. (2015) investi- els. We evaluate prediction error by computing the distance in kilometers between the predicted gated the types of semantic information which are GPS point and the expected point, and compute encoded by different classes of word embedding the average of error distances. Since we use linmodels, finding that taxonomic properties (such as 1 animacy) are well-modelled. In a similar direction, ear models (Lasso ) to predict geo-coordinates, we effectively evaluate the isomorphism of language Collell Talleda and Moen"
2021.blackboxnlp-1.40,D19-1006,0,0.0186463,"rd2Vec, but that larger (non-autoregressive) language models encode more information than their smaller counterparts, suggesting that such information is nevertheless available through higher-order co-occurrence statistics. 2 Related Work affordances (e.g. bananas can be eaten) but they do not to capture the more subtle interplay between the two. Studies investigating the geometry of word representations have focused on intrinsic dimensionality and subspaces (Yaghoobzadeh and Schütze, 2016; Coenen et al., 2019), embedding concentration in narrow cones and anisotropy (Mimno and Thompson, 2017; Ethayarajh, 2019), and comparisons to the geometry of cognitive measurements or perceptual spaces (Abnar et al., 2019; Abdou et al., 2021). In this work we investigate the degree of isomorphism between language model representations of geographic location names and their real-world counterparts. See Vuli´c et al. (2020) for a general discussion of isomorphism across language models, what explains this, and what it depends on. 3 Methodology We probe language model representations of city and country names. Below, we propose three classification/regression probing tasks, as an well as an analysis based on (relat"
2021.blackboxnlp-1.40,D19-1275,0,0.025655,"e for comparison. Task 3: Predicting neighboring countries Borders between countries are also an important geographic relation that is closely related to a country’s international policy. It also provides information about the continental situation ; indeed, a country with no land borders must be an island. To probe language models for neighboring relations, we train an classifier to predict whether two countries share a border or not, given the pair of representations. We report the probe accuracy. Neighboring relations are local and included here for comparison. 3.2 Control tasks and scores Hewitt and Liang (2019) have shown the importance of a control task to balance the probe accuracy. We construct our control tasks by randomly permuting the target variables and train the model on this randomly-permuted dataset. We repeat this control task 10 times and take the mean error/accuracy. Probe classifier selectivity For the country borders classification task, we define the probe selectivity as the difference between the probe accuracy on the original task and the accuracy on the control task (Hewitt and Liang, 2019). Probe error reduction Similarly, we define the probe error reduction, which measures how"
2021.blackboxnlp-1.40,J15-4004,0,0.0494542,"l as an analysis based on (relational) similarity. Word representations based on distributional statis3.1 Probing tasks tics have been theorised to capture a wide range Task 1: Predicting geo-coordinates We probe of information (Schütze, 1992). To evaluate this, a considerable body of literature has made use of se- LMs’ representation about whether they convey information about the actual position of the locamantic similarity, relatedness, and analogy datasets tion. To this extent, we train a model to predict (Agirre et al., 2009; Bruni et al., 2012; Baroni et al., 2014; Faruqui et al., 2014; Hill et al., 2015; GPS coordinates (latitude and longitude) given the location’s representation through language modDrozd et al., 2016; Abdou et al., 2018). Asking a broader question, Rubinstein et al. (2015) investi- els. We evaluate prediction error by computing the distance in kilometers between the predicted gated the types of semantic information which are GPS point and the expected point, and compute encoded by different classes of word embedding the average of error distances. Since we use linmodels, finding that taxonomic properties (such as 1 animacy) are well-modelled. In a similar direction, ear mod"
2021.blackboxnlp-1.40,2020.tacl-1.28,0,0.0345168,"present the distributional properties of words and connected by a bridge, often referred to as the phrases. bridge between Copenhagen and Malmö. Other It is well-documented that the representations of modern language models encode some syntac- cities belong to the same municipalities and their names therefore co-occur with the name of the tic (Tenney et al., 2019) and semantic knowledge municipality. Other cities were, for instance, poten((Reif et al., 2019)), as well as some real-world knowledge (Davison et al., 2019; Petroni et al., tially impacted by the same natural disaster or part 2019; Jiang et al., 2020; Roberts et al., 2020), i.e., of the same development project. By distilling thousands, or maybe hundreds of thousands, of such some knowledge base relations can be extracted directly from language models. Much of this in- co-occurrences, we conjecture that language models might be able to induce somewhat fine-grained formation was available in older language models maps of physical geometry. such as Word2Vec, too (Mikolov et al., 2013). The probes that have been designed for the above In this work, we study the extent to which geostudies, however, only probe for one-dimensional graphical inf"
2021.blackboxnlp-1.40,W17-2810,0,0.0575231,"Missing"
2021.blackboxnlp-1.40,D17-1308,0,0.0288009,"raphic information than Word2Vec, but that larger (non-autoregressive) language models encode more information than their smaller counterparts, suggesting that such information is nevertheless available through higher-order co-occurrence statistics. 2 Related Work affordances (e.g. bananas can be eaten) but they do not to capture the more subtle interplay between the two. Studies investigating the geometry of word representations have focused on intrinsic dimensionality and subspaces (Yaghoobzadeh and Schütze, 2016; Coenen et al., 2019), embedding concentration in narrow cones and anisotropy (Mimno and Thompson, 2017; Ethayarajh, 2019), and comparisons to the geometry of cognitive measurements or perceptual spaces (Abnar et al., 2019; Abdou et al., 2021). In this work we investigate the degree of isomorphism between language model representations of geographic location names and their real-world counterparts. See Vuli´c et al. (2020) for a general discussion of isomorphism across language models, what explains this, and what it depends on. 3 Methodology We probe language model representations of city and country names. Below, we propose three classification/regression probing tasks, as an well as an analy"
2021.blackboxnlp-1.40,D19-1250,0,0.0236561,"n, ear models (Lasso ) to predict geo-coordinates, we effectively evaluate the isomorphism of language Collell Talleda and Moens (2016) and Lucy and model representations to the physical world. Our Gauthier (2017) draw on semantic norm datasets target variable is two-dimensional, and we evaluate to test how well these models can encode a range non-local relations. of perceptual and conceptual features. In the context of the neural language models, Task 2: Predicting population sizes In the several recent works such as Davison et al. (2019) physical world, cities are characterized not only and Petroni et al. (2019) have attempted to extract by their location, but also by their size and populafactual and commonsense knowledge from them tion. There is indeed a major difference between by posing knowledge base triplets as close stateShanghai (27M people) and Worcester (101K peoments which are used to query the models. Most ple). Given the representation of a country name related to this work, Forbes et al. (2019) investior a city name, we train a probe to predict the gate whether LMs can learn physical commonsense population living in the location. We evaluate the through language. They find that LM repres"
2021.blackboxnlp-1.40,2020.emnlp-main.437,0,0.051279,"Missing"
2021.blackboxnlp-1.40,P15-2119,0,0.0249256,"ing geo-coordinates We probe of information (Schütze, 1992). To evaluate this, a considerable body of literature has made use of se- LMs’ representation about whether they convey information about the actual position of the locamantic similarity, relatedness, and analogy datasets tion. To this extent, we train a model to predict (Agirre et al., 2009; Bruni et al., 2012; Baroni et al., 2014; Faruqui et al., 2014; Hill et al., 2015; GPS coordinates (latitude and longitude) given the location’s representation through language modDrozd et al., 2016; Abdou et al., 2018). Asking a broader question, Rubinstein et al. (2015) investi- els. We evaluate prediction error by computing the distance in kilometers between the predicted gated the types of semantic information which are GPS point and the expected point, and compute encoded by different classes of word embedding the average of error distances. Since we use linmodels, finding that taxonomic properties (such as 1 animacy) are well-modelled. In a similar direction, ear models (Lasso ) to predict geo-coordinates, we effectively evaluate the isomorphism of language Collell Talleda and Moens (2016) and Lucy and model representations to the physical world. Our Gau"
2021.blackboxnlp-1.40,P18-1072,1,0.891315,"Missing"
2021.blackboxnlp-1.40,P19-1452,0,0.0245312,"ict words physical geometry? Higher-order co-occurrence in context, and as a side product, they learn to statistics can be surprisingly informative. Copencompress higher-order co-occurrence statistics to hagen and Malmö are in different countries, but represent the distributional properties of words and connected by a bridge, often referred to as the phrases. bridge between Copenhagen and Malmö. Other It is well-documented that the representations of modern language models encode some syntac- cities belong to the same municipalities and their names therefore co-occur with the name of the tic (Tenney et al., 2019) and semantic knowledge municipality. Other cities were, for instance, poten((Reif et al., 2019)), as well as some real-world knowledge (Davison et al., 2019; Petroni et al., tially impacted by the same natural disaster or part 2019; Jiang et al., 2020; Roberts et al., 2020), i.e., of the same development project. By distilling thousands, or maybe hundreds of thousands, of such some knowledge base relations can be extracted directly from language models. Much of this in- co-occurrences, we conjecture that language models might be able to induce somewhat fine-grained formation was available in"
2021.blackboxnlp-1.40,2020.emnlp-main.257,1,0.895154,"Missing"
2021.blackboxnlp-1.40,P16-1023,0,0.0489343,"Missing"
2021.bppf-1.2,W10-2927,0,0.0283533,"atively few examples of researchers identifying concrete guideline-related bias in benchmark datasets: Dickinson (2003) suggest that POS annotation in the English Penn Treebank is biased by the vagueness of the annotation guidelines in some respects. Friedrich et al. (2015) report a similar guideline-induced bias in the ACE datasets. Dandapat et al. (2009) discuss an interesting bias in a Bangla/Hindi POS-annotated corpus arising from a decision in the annotation guidelines to include two labels for when annotators were uncertain, but not specifying in detail how these labels were to be used. Goldberg and Elhadad (2010) define structural bias for dependency parsing and how it can be attributed to bias in individual datasets, among other factors, originating from their annotation schemes. Ibanez and Ohtani (2014) report a similar case, where ambiguity in how special categories were defined, led to bias in a corpus of Spanish learner errors. 0.75 0.88 Mo vie na me 0.8 0 Cit y x1 0.6 0.25 0 0.9 0.4 0.099 0 Movie name City Theater name 0.079 0.14 x2 Guideline 2 (Agent) 0 0.0 0.8 Th ea te rn am e Th ea ter na me 0.2 0.12 0.92 0 0.43 0.4 0.86 0.57 0 Theater name City Movie name 0.2 Mo vie na me Cit y x1 0.6 x2 0.0"
2021.bppf-1.2,D19-1662,1,0.781651,"s. Both corpora were collected by having a team of assistants interact with users in a Wizard-of-Oz (WoZ) set-up, i.e. a human plays the role of a digital assistant which engages a user in a conversation about their movie preferences. The assistants were given a set of guidelines in advance, as part of their training, and it is these guidelines that induce biases. In CCPE-M, it is the overwhelming use of the verb like (see Figure 5) and its trickle-down effects, we focus on; in Taskmaster-1, the order of Introduction Sample bias is a well-known problem in NLP – discussed from Marcus (1982) to Barrett et al. (2019) – and annotator bias has been discussed as far back as Ratnaparkhi (1996). This paper focuses on a different kind of bias that has received very little attention: guideline bias, i.e., the bias introduced by how our annotator guidelines are formulated. Annotation guidelines are used to train annotators, and guidelines are therefore in some sense intended to and designed to prime annotators. What we will refer to in our discussion of guideline bias, is rather the unintended biases that result from how guidelines are formulated, and the examples used in those guidelines. If a treebank annotatio"
2021.bppf-1.2,Q18-1041,0,0.0668362,"Missing"
2021.bppf-1.2,D19-1459,0,0.0244989,"entage of sentences with the word like in the CCPE-M annotation guidelines (Guidelines), the suggested questions to ask users, in the guidelines (Suggestions), (c) the actual first turns by the assistants (1st turn), and (d) the actual replies by the users (2nd turn). In all cases, more than half of the sentences contain the word like. We focus on two recently introduced datasets, the Coached Conversational Preference Elicitation corpus (CCPE-M) from Radlinski et al. (2019), related to the task of conversational recommendation (Christakopoulou et al., 2016; Li et al., 2018), and Taskmaster-1 (Byrne et al., 2019), which is a multipurpose, multi-domain dialogue dataset. CCPE-M consists of conversations about movie preferences, and the part of Taskmaster-1, we focus on here, conversations about theatre ticket reservations. Both corpora were collected by having a team of assistants interact with users in a Wizard-of-Oz (WoZ) set-up, i.e. a human plays the role of a digital assistant which engages a user in a conversation about their movie preferences. The assistants were given a set of guidelines in advance, as part of their training, and it is these guidelines that induce biases. In CCPE-M, it is the ov"
2021.bppf-1.2,Y14-1029,0,0.0240818,"ness of the annotation guidelines in some respects. Friedrich et al. (2015) report a similar guideline-induced bias in the ACE datasets. Dandapat et al. (2009) discuss an interesting bias in a Bangla/Hindi POS-annotated corpus arising from a decision in the annotation guidelines to include two labels for when annotators were uncertain, but not specifying in detail how these labels were to be used. Goldberg and Elhadad (2010) define structural bias for dependency parsing and how it can be attributed to bias in individual datasets, among other factors, originating from their annotation schemes. Ibanez and Ohtani (2014) report a similar case, where ambiguity in how special categories were defined, led to bias in a corpus of Spanish learner errors. 0.75 0.88 Mo vie na me 0.8 0 Cit y x1 0.6 0.25 0 0.9 0.4 0.099 0 Movie name City Theater name 0.079 0.14 x2 Guideline 2 (Agent) 0 0.0 0.8 Th ea te rn am e Th ea ter na me 0.2 0.12 0.92 0 0.43 0.4 0.86 0.57 0 Theater name City Movie name 0.2 Mo vie na me Cit y x1 0.6 x2 0.0 Figure 4: Probability that a guideline goal x1 is mentioned before another one x2 in an actual dialogue, given that x1 comes before x2 in the agent’s guideline. 5 In this work, we examined guidel"
2021.bppf-1.2,N18-1088,0,0.0656164,"Missing"
2021.bppf-1.2,P82-1033,0,0.251149,"icket reservations. Both corpora were collected by having a team of assistants interact with users in a Wizard-of-Oz (WoZ) set-up, i.e. a human plays the role of a digital assistant which engages a user in a conversation about their movie preferences. The assistants were given a set of guidelines in advance, as part of their training, and it is these guidelines that induce biases. In CCPE-M, it is the overwhelming use of the verb like (see Figure 5) and its trickle-down effects, we focus on; in Taskmaster-1, the order of Introduction Sample bias is a well-known problem in NLP – discussed from Marcus (1982) to Barrett et al. (2019) – and annotator bias has been discussed as far back as Ratnaparkhi (1996). This paper focuses on a different kind of bias that has received very little attention: guideline bias, i.e., the bias introduced by how our annotator guidelines are formulated. Annotation guidelines are used to train annotators, and guidelines are therefore in some sense intended to and designed to prime annotators. What we will refer to in our discussion of guideline bias, is rather the unintended biases that result from how guidelines are formulated, and the examples used in those guidelines"
2021.bppf-1.2,P13-1004,0,0.0365024,"rime the turkers, we compute a probability distribution over most of the verbs in the response vocabulary that are likely to be used to describe a general preference towards something. Figure 3 shows the results of the crowdsourced priming experiments. We can observe that when a specific priming word, such as like, is used, there is a significantly higher probability that the response from the user will contain that same word, illustrating that when keywords in guidelines are heavily overrepresented, the collected data will also reflect this bias. 11 Guideline 1 (Agent) applicable to another. Cohn and Specia (2013) explores how models can learn from annotator bias in a somewhat opposite scenario from ours, e.g. when annotators deviate from annotation guidelines and inject their own bias into the data, and by using multi-task learning to train annotator specific models, they improve performance by leveraging annotation (dis)agreements. There are, to the best of our knowledge, relatively few examples of researchers identifying concrete guideline-related bias in benchmark datasets: Dickinson (2003) suggest that POS annotation in the English Penn Treebank is biased by the vagueness of the annotation guideli"
2021.bppf-1.2,E14-1078,1,0.866795,"Missing"
2021.bppf-1.2,W19-5941,0,0.365314,"esented, biases the data collection. 1 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 eline Guid s ns estio Sugg 1st t A urn ( ) 2nd (U) turn Figure 1: The percentage of sentences with the word like in the CCPE-M annotation guidelines (Guidelines), the suggested questions to ask users, in the guidelines (Suggestions), (c) the actual first turns by the assistants (1st turn), and (d) the actual replies by the users (2nd turn). In all cases, more than half of the sentences contain the word like. We focus on two recently introduced datasets, the Coached Conversational Preference Elicitation corpus (CCPE-M) from Radlinski et al. (2019), related to the task of conversational recommendation (Christakopoulou et al., 2016; Li et al., 2018), and Taskmaster-1 (Byrne et al., 2019), which is a multipurpose, multi-domain dialogue dataset. CCPE-M consists of conversations about movie preferences, and the part of Taskmaster-1, we focus on here, conversations about theatre ticket reservations. Both corpora were collected by having a team of assistants interact with users in a Wizard-of-Oz (WoZ) set-up, i.e. a human plays the role of a digital assistant which engages a user in a conversation about their movie preferences. The assistants"
2021.bppf-1.2,W09-3002,0,0.0137695,"otators deviate from annotation guidelines and inject their own bias into the data, and by using multi-task learning to train annotator specific models, they improve performance by leveraging annotation (dis)agreements. There are, to the best of our knowledge, relatively few examples of researchers identifying concrete guideline-related bias in benchmark datasets: Dickinson (2003) suggest that POS annotation in the English Penn Treebank is biased by the vagueness of the annotation guidelines in some respects. Friedrich et al. (2015) report a similar guideline-induced bias in the ACE datasets. Dandapat et al. (2009) discuss an interesting bias in a Bangla/Hindi POS-annotated corpus arising from a decision in the annotation guidelines to include two labels for when annotators were uncertain, but not specifying in detail how these labels were to be used. Goldberg and Elhadad (2010) define structural bias for dependency parsing and how it can be attributed to bias in individual datasets, among other factors, originating from their annotation schemes. Ibanez and Ohtani (2014) report a similar case, where ambiguity in how special categories were defined, led to bias in a corpus of Spanish learner errors. 0.75"
2021.bppf-1.2,W96-0213,0,0.614134,"sers in a Wizard-of-Oz (WoZ) set-up, i.e. a human plays the role of a digital assistant which engages a user in a conversation about their movie preferences. The assistants were given a set of guidelines in advance, as part of their training, and it is these guidelines that induce biases. In CCPE-M, it is the overwhelming use of the verb like (see Figure 5) and its trickle-down effects, we focus on; in Taskmaster-1, the order of Introduction Sample bias is a well-known problem in NLP – discussed from Marcus (1982) to Barrett et al. (2019) – and annotator bias has been discussed as far back as Ratnaparkhi (1996). This paper focuses on a different kind of bias that has received very little attention: guideline bias, i.e., the bias introduced by how our annotator guidelines are formulated. Annotation guidelines are used to train annotators, and guidelines are therefore in some sense intended to and designed to prime annotators. What we will refer to in our discussion of guideline bias, is rather the unintended biases that result from how guidelines are formulated, and the examples used in those guidelines. If a treebank annotation guideline focuses overly on parasitic gap constructions, for example, in"
2021.bppf-1.2,E03-1068,0,0.135314,"resented, the collected data will also reflect this bias. 11 Guideline 1 (Agent) applicable to another. Cohn and Specia (2013) explores how models can learn from annotator bias in a somewhat opposite scenario from ours, e.g. when annotators deviate from annotation guidelines and inject their own bias into the data, and by using multi-task learning to train annotator specific models, they improve performance by leveraging annotation (dis)agreements. There are, to the best of our knowledge, relatively few examples of researchers identifying concrete guideline-related bias in benchmark datasets: Dickinson (2003) suggest that POS annotation in the English Penn Treebank is biased by the vagueness of the annotation guidelines in some respects. Friedrich et al. (2015) report a similar guideline-induced bias in the ACE datasets. Dandapat et al. (2009) discuss an interesting bias in a Bangla/Hindi POS-annotated corpus arising from a decision in the annotation guidelines to include two labels for when annotators were uncertain, but not specifying in detail how these labels were to be used. Goldberg and Elhadad (2010) define structural bias for dependency parsing and how it can be attributed to bias in indiv"
2021.bppf-1.2,W15-1603,0,0.0213744,"an learn from annotator bias in a somewhat opposite scenario from ours, e.g. when annotators deviate from annotation guidelines and inject their own bias into the data, and by using multi-task learning to train annotator specific models, they improve performance by leveraging annotation (dis)agreements. There are, to the best of our knowledge, relatively few examples of researchers identifying concrete guideline-related bias in benchmark datasets: Dickinson (2003) suggest that POS annotation in the English Penn Treebank is biased by the vagueness of the annotation guidelines in some respects. Friedrich et al. (2015) report a similar guideline-induced bias in the ACE datasets. Dandapat et al. (2009) discuss an interesting bias in a Bangla/Hindi POS-annotated corpus arising from a decision in the annotation guidelines to include two labels for when annotators were uncertain, but not specifying in detail how these labels were to be used. Goldberg and Elhadad (2010) define structural bias for dependency parsing and how it can be attributed to bias in individual datasets, among other factors, originating from their annotation schemes. Ibanez and Ohtani (2014) report a similar case, where ambiguity in how spec"
2021.bppf-1.2,D19-1107,0,0.0611504,"Missing"
2021.conll-1.19,2020.lrec-1.232,0,0.0239074,"ng towards the model’s inability to model the negation’s effect on sentence semantics. 2 adversarial examples by adding a tautology (and false is not true) to the end of every hypothesis in the MNLI data, leading to a drop in model accuracies caused by a large amount of false positives (FPs) for the neutral class, rather than an expected increase in FPs for the contradiction class. The authors hypothesize that this pattern stems from decreasing lexical overlap by adding the tautology, and as low lexical overlap is indicative for the neutral class, the model mispredicts the example as neutral. Aspillaga et al. (2020) confirm that also transformer-based models perform poorly on this negation stress test. On their challenge dataset for non-entailed subsequences based on the MNLI data, McCoy and Linzen (2019) find that models exploit a mismatch between negation cues in premise and hypothesis to predict non-entailment, and that removing unimportant negation cues decreases model accuracy to almost 0. Related Work The processing of negation has been investigated in several works, either by creating dedicated diagnostic datasets, or by investigating the phenomenon as part of a more general analysis. An overview"
2021.conll-1.19,W19-4802,0,0.0609567,"Missing"
2021.conll-1.19,2020.acl-main.463,0,0.0106065,"h syntactic negation of verbs, that requires lexical inference and reasoning skills. Again, models fine-tuned on the standard MNLI data perform poorly, but improve when fine-tuned on the target dataset. Instead of fine-tuning on an NLI task, Ettinger (2020) and Kassner and Schütze (2020) directly probe pre-trained encoders using a cloze language modeling task. They find that a language model makes the same predictions in negative and assertive contexts, but it is unclear to what extent we can expect a language model to learn the semantics of negation from an unsupervised pretraining task (see Bender and Koller (2020)). of linguistic phenomena in multiple languages, including features of sentence representations (Ravishankar et al., 2019b,a), tenses (Li and Wisniewski, 2021), gender bias (González et al., 2020), numerical understanding (Johnson et al., 2020), and lexical semantics (Vuli´c et al., 2020). 3 Approach Our goal is to verify if a model adequately processes negation: In the presence of negation, the model should make a correct prediction. The most naive way to look at this is to check model performance on examples that contain negations. However, even if the model correctly predicts such examples"
2021.conll-1.19,D16-1025,0,0.0194264,"Missing"
2021.conll-1.19,D15-1075,0,0.0434879,"existing NLI examples. The following works are interested in specific inference mechanisms, and artificially create data requiring the inference to be recognized correctly. Geiger et al. (2020) investigate if models can learn interactions between lexical entailment and negation, in particular the algorithm behind downward monotonicity (e.g., dance entails move, and not move entails not dance), and find that models cannot solve the task when fine-tuned on MNLI, but when fine-tuned on the challenge dataset. Their dataset is created by substituting single words in examples from the SNLI dataset (Bowman et al., 2015). In contrast, Richardson et al. (2020) use a template to build a probing dataset with syntactic negation of verbs, that requires lexical inference and reasoning skills. Again, models fine-tuned on the standard MNLI data perform poorly, but improve when fine-tuned on the target dataset. Instead of fine-tuning on an NLI task, Ettinger (2020) and Kassner and Schütze (2020) directly probe pre-trained encoders using a cloze language modeling task. They find that a language model makes the same predictions in negative and assertive contexts, but it is unclear to what extent we can expect a language"
2021.conll-1.19,D18-1269,0,0.0995387,"to shed light on models’ beled, as a result of manual inspection and editing. We use the benchmark to probe the negation processing capabilities, but the diagnostic negation-awareness of multilingual language datasets so far only cover English. With our work, models and find that models that correctly prewe extend the efforts of analyzing a model’s negadict examples with negation cues often fail to tion awareness to a multilingual setup, by deriving correctly predict their counter-examples withan analysis dataset from the multilingual XNLI out negation cues, even when the cues are irdataset (Conneau et al., 2018). relevant for semantic inference. The goal of our work is to provide multilingual datasets to determine if models adequately process 1 Introduction negation. We not only want to know if they make Negation is a fundamental concept of human cog- correct inferences in the presence of negation but nition, for asserting the falsity of a proposition also if correct inferences are a result of adequately (Heinemann, 2015). The linguistic markers of nega- modeling the semantics of the negation, or if they tion enable us, for example, to deny that events are a result of merely exploiting shallow heuris"
2021.conll-1.19,N13-1073,0,0.0137503,"rite; (ii) removing the negation leads to a contradictory statement; or (iii) translation quality is low. We then pair the verified sentences back into their original premise-hypothesis pairs, and re-label the resulting new NLI example.3 In Multilingual negation cues We extract a list of English (EN) negation cues from datasets annotated for the negation scope resolution task, in particular from the Sherlock dataset (Morante and Daelemans, 2012), as a starting point to automatically compile such lists for French ( FR), German (DE), and Bulgarian (BG). We apply the fast_align 2 alignment tool (Dyer et al., 2013) to word-align EnWe derive our minimal pairs from the test splits, removing examples for which the assigned gold label does not correglish and target language sentences sourced from to the majority label assigned by the five annotators. EuroParl (Tiedemann, 2012). Based on a many- spond 3 All annotations were done by native speakers of the reto-many alignment, we extract the twenty most spective languages and with expert knowledge of linguistics; common translations in the target language. The re- except for EN, for which the annotator is a C2 level speaker. 247 EN BG (Hossain et al. 2020) DE"
2021.conll-1.19,2020.tacl-1.3,0,0.272519,"edu.cn {ml,dh,yova,lukas.christian.nielsen,soegaard}@di.ku.dk Abstract et al., 2016), and question answering (Stali¯unait˙e and Iacobacci, 2020). Negation is one of the most fundamental To complement insights from anecdotal error concepts in human cognition and language, analysis, several diagnostic datasets have recently and several natural language inference (NLI) been designed to explicitly investigate the negation probes have been designed to investigate preprocessing capabilities of pretrained language modtrained language models’ ability to detect and els, either directly on the encoder (Ettinger, 2020; reason with negation. However, the existing probing datasets are limited to English Kassner and Schütze, 2020) or after fine-tuning only, and do not enable controlled probing the model for downstream tasks such as natural of performance in the absence or presence of language inference (NLI) (Naik et al., 2018; Kim negation. In response, we present a multiet al., 2019; Geiger et al., 2020; Richardson et al., lingual (English, Bulgarian, German, French 2020; Hossain et al., 2020b) or sentiment analyand Chinese) benchmark collection of NLI exsis (Li and Huang, 2009; Zhu et al., 2014). These amp"
2021.conll-1.19,W15-1301,0,0.0675638,"Missing"
2021.conll-1.19,2020.blackboxnlp-1.16,0,0.217661,"xplicitly investigate the negation probes have been designed to investigate preprocessing capabilities of pretrained language modtrained language models’ ability to detect and els, either directly on the encoder (Ettinger, 2020; reason with negation. However, the existing probing datasets are limited to English Kassner and Schütze, 2020) or after fine-tuning only, and do not enable controlled probing the model for downstream tasks such as natural of performance in the absence or presence of language inference (NLI) (Naik et al., 2018; Kim negation. In response, we present a multiet al., 2019; Geiger et al., 2020; Richardson et al., lingual (English, Bulgarian, German, French 2020; Hossain et al., 2020b) or sentiment analyand Chinese) benchmark collection of NLI exsis (Li and Huang, 2009; Zhu et al., 2014). These amples that are grammatical and correctly ladatasets have been useful to shed light on models’ beled, as a result of manual inspection and editing. We use the benchmark to probe the negation processing capabilities, but the diagnostic negation-awareness of multilingual language datasets so far only cover English. With our work, models and find that models that correctly prewe extend the effor"
2021.conll-1.19,S18-2023,0,0.0596339,"Missing"
2021.conll-1.19,W19-6205,0,0.0294929,"ard MNLI data perform poorly, but improve when fine-tuned on the target dataset. Instead of fine-tuning on an NLI task, Ettinger (2020) and Kassner and Schütze (2020) directly probe pre-trained encoders using a cloze language modeling task. They find that a language model makes the same predictions in negative and assertive contexts, but it is unclear to what extent we can expect a language model to learn the semantics of negation from an unsupervised pretraining task (see Bender and Koller (2020)). of linguistic phenomena in multiple languages, including features of sentence representations (Ravishankar et al., 2019b,a), tenses (Li and Wisniewski, 2021), gender bias (González et al., 2020), numerical understanding (Johnson et al., 2020), and lexical semantics (Vuli´c et al., 2020). 3 Approach Our goal is to verify if a model adequately processes negation: In the presence of negation, the model should make a correct prediction. The most naive way to look at this is to check model performance on examples that contain negations. However, even if the model correctly predicts such examples, it might do so for the wrong reasons: It might (1) ignore the negation completely or not properly model its effect on th"
2021.conll-1.19,W19-4318,0,0.0180193,"ard MNLI data perform poorly, but improve when fine-tuned on the target dataset. Instead of fine-tuning on an NLI task, Ettinger (2020) and Kassner and Schütze (2020) directly probe pre-trained encoders using a cloze language modeling task. They find that a language model makes the same predictions in negative and assertive contexts, but it is unclear to what extent we can expect a language model to learn the semantics of negation from an unsupervised pretraining task (see Bender and Koller (2020)). of linguistic phenomena in multiple languages, including features of sentence representations (Ravishankar et al., 2019b,a), tenses (Li and Wisniewski, 2021), gender bias (González et al., 2020), numerical understanding (Johnson et al., 2020), and lexical semantics (Vuli´c et al., 2020). 3 Approach Our goal is to verify if a model adequately processes negation: In the presence of negation, the model should make a correct prediction. The most naive way to look at this is to check model performance on examples that contain negations. However, even if the model correctly predicts such examples, it might do so for the wrong reasons: It might (1) ignore the negation completely or not properly model its effect on th"
2021.conll-1.19,2020.emnlp-main.573,0,0.08362,"Missing"
2021.conll-1.19,tiedemann-2012-parallel,0,0.00964589,"es We extract a list of English (EN) negation cues from datasets annotated for the negation scope resolution task, in particular from the Sherlock dataset (Morante and Daelemans, 2012), as a starting point to automatically compile such lists for French ( FR), German (DE), and Bulgarian (BG). We apply the fast_align 2 alignment tool (Dyer et al., 2013) to word-align EnWe derive our minimal pairs from the test splits, removing examples for which the assigned gold label does not correglish and target language sentences sourced from to the majority label assigned by the five annotators. EuroParl (Tiedemann, 2012). Based on a many- spond 3 All annotations were done by native speakers of the reto-many alignment, we extract the twenty most spective languages and with expert knowledge of linguistics; common translations in the target language. The re- except for EN, for which the annotator is a C2 level speaker. 247 EN BG (Hossain et al. 2020) DE not, no, never, nobody, without не (no/not), никога не (never), няма (doesn’t have/there isn’t/won’t), никой не (nobody), нямаше (няма, past tense) nicht (not), keine (no), nie (never), nichts (nothing), niemand (nobody) FR ZH (ours) ne pas (not), jamais (never),"
2021.conll-1.5,W19-7803,0,0.0683233,"linguistic insights (Parkvall, 2008). While these studies demonstrate that work on creoles is being done in a computational space, it is difficult to apply conclusions from them to NLP, because distinct empirical assumptions are made in these two research areas. Nigerian Pidgin English West Africa is one of the world’s most linguistically diverse places, with Nigeria alone having over 400 languages (Ufomata, 1999). Recent work to advance African NLP has led to the creation of several datasets in Nigerian Pidgin English (Agi´c and Vuli´c, 2019; Ogueji and Ahia, 2019; Ndubuisi-Obi et al., 2019; Caron et al., 2019; Oyewusi et al., 2021; Adelani et al., 2021; Oyewusi et al., 2020), which makes it particularly well-resourced in comparison to other creole languages. Nigerian Pidgin English, also referred to as simply Nigerian Pidgin, can further be understood as a member in the larger family of West African Pidgins, as many West African countries have their own unique variation of this creole, but all share influences from many of the same languages, such as Igbo, Hausa, and Yoruba. The first sizeable Nigerian Pidgin dataset comes from Agi´c and Vuli´c (2019), who collected parallel text from several maga"
2021.conll-1.5,W16-5801,0,0.0582036,"Missing"
2021.conll-1.5,2020.findings-emnlp.118,0,0.08238,"Missing"
2021.conll-1.5,2021.acl-long.131,0,0.0732692,"Missing"
2021.conll-1.5,2020.findings-emnlp.372,0,0.0382512,". Table 5: Over-parameterization experiments with M IXED -L ANGUAGE Nigerian Pidgin English data. Smaller sized models do not benefit DRO over ERM. Language Domain-1 Domain-2 PAD English Disaster Response Corpus Newswire 1.75 Haitian Creole Disaster Response Corpus Newswire 1.47 Over-parameterization Over-parameterization is known to be problematic for DRO (Sagawa et al., 2019). In order to investigate the role of over-parameterization in our experiments, we ran additional M IXED -L ANGUAGE experiments on Nigerian Pidgin English, with different sized BERT models, namely BERTTiny , BERTSmall (Jiao et al., 2020), and BERTBase . The results in Tab. 5 demonstrate that over-parameterization was not a leading cause for DRO failure, otherwise we would expect for smaller BERT versions to have relative better performance compared to the corresponding ERM runs. Instead, we see that standard BERT works fine for this task, and over-parameterization is not the cause of poor performance of DRO in our experiments. English Nigerian EWT-UD UNMT NUD NUD 1.04 1.28 Table 7: Proxy A-distance (PAD) scores on parallel (Haitian) or near-parallel (Nigerian) data. PAD is proportional to domain classification error; hence, l"
2021.conll-1.5,N16-1061,0,0.0150938,"velop baselines for machine translation of Nigerian Pidgin English (Ogueji and Ahia, 2019; Ahia and Ogueji, 2020). Furthermore, Ogueji and Ahia (2019) also introduced the first corpus of Nigerian Pidgin English to further facilitate machine translation from Nigerian Pidgin into English. Ndubuisi-Obi et al. (2019) also introduced a code-switching corpus of news articles and onDistributionally robust optimization Effectively learning to model and predict underrepresented subdistributions has always been a challenge in machine learning, e.g., when predicting rare classes, (Scheirer et al., 2013; Fei and Liu, 2016) or classes of examples from rare domains (Zheng et al., 2020) or minority groups (Hashimoto et al., 2018). Often, underrepresented data is ignored or learned poorly by the models (Feldman and Zhang, 2020), compared to their over-represented counterparts. Distributionally Robust Optimization (DRO) (Hashimoto et al., 2018; Sagawa et al., 2019) aims to minimize the loss on all sub-populations, rather than minimizing their average (Ben-Tal et al., 2013). DRO has been particularly useful in the domain of algorithmic fairness (Hashimoto et al., 60 line comments in both Nigerian Standard English and"
2021.conll-1.5,2020.acl-main.560,0,0.0266746,"that the difference persists in the absence of over-parameterization, and that drift is limited, confirming the relative stability of creole languages. 1 10M 1M 100K Figure 1: Creoles with a minimum of a hundred thousand speakers are shown here (Hawaiian Pidgin not pictured). Approximately 180 million creole speakers are represented in this map. Data extracted from https://en.wikipedia.org/ wiki/List_of_creole_languages. hundred million speakers of creole languages world wide (Fig. 1), with similar needs for technological assistance, and yet creoles are still largely absent from NLP research (Joshi et al., 2020). Haitian Creole, for example, has 9.6 million speakers as of today; Nigerian Pidgin English has 100 million speakers, and Singaporean Colloquial English (Singlish) has 3.5 million speakers. This paper sets out to collect existing resources for these three languages and provides language models for them. In doing so, we wish to take the nature of creole languages into account, not necessarily assuming that our best approaches to modeling non-creole language are also best for the creole languages. The nature of creole languages has been a matter of much debate in linguistics during the last dec"
2021.conll-1.5,W11-2146,0,0.0191431,"ers to Çetino˘glu et al. (2016) and Do˘gruöz et al. (2021) for an overview. 2018), but has also been found to boost performance on underrepresented domains in language modeling (Oren et al., 2019) and is generally applicable in situations with drift (Koh et al., 2021). 3 Creoles and Corpora While creole languages are spoken by hundreds of millions, and are often a lingua franca within a larger community, only a handful of resources exist for creoles presently. Some challenges to collecting data resources for creole languages can be a creole’s non-standardized orthography, e.g. Haitian Creole (Hewavitharana et al., 2011), or the specific contexts in which creoles are used – it may not always be used in official capacities for news, education, and official documents, even if the creoles are widely used in most other aspects of life (ShahSanghavi, 2017). This of course complicates data collection. In this work, we focus on the following creoles, as they each have diverse linguistic makeup and have some existing datasets: Computational research on language evolution Research on creoles is more common in the field of language evolution than in NLP. In particular, work on creoles in this field typically focuses on"
2021.conll-1.9,2020.conll-1.17,0,0.0310852,"mely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric. Figure 1: Right: Color orientation in 3d CIELAB space. Left: linear mapping from BERT (CC, see §2) color term embeddings to the CIELAB space. features of concrete and abstract concepts, such as objects’ attributes and affordances (Forbes et al., 2019b; Weir et al., 2020). Furthermore, the representational geometry of LMs has been found to naturally reflect human lexical similarity and relatedness judgements, as well as analogy relationships (Chronis and Erk, 2020). However, the extent to which these models reflect the structures that exist in humans’ perceptual world—such as the topology of visual perception (Chen, 1982), the structure of the color spectrum (Ennis and Zaidi, 2019; Provenzi, 2020), or of odour spaces (Rossiter, 1996; Chastrette, 1997)—is not well-understood. If LMs are indeed able to capture such topologies—in some domains, at least—it would mean that these structures are a) somehow reflected 1 Introduction in language and, thereby, encoded in the textual Without grounding or interaction with the world, training data on which models are"
2021.conll-1.9,D19-1109,0,0.0713743,"mal linguistic struc- and architectural inductive biases. To the extent ture (e.g., morphosyntax (Tenney et al., 2019)) they are not, the question becomes whether the inand semantic information (e.g., lexical similarity formation is not there in the data, or whether model (Reif et al., 2019a)). Beyond this, it has been sug- and training objective limitations are to blame. Cergested that text-only training data is enough for tainly, this latter point relates to an ongoing deLMs to also acquire factual and relational informa- bate regarding what exactly language models can tion about the world (Davison et al., 2019; Petroni be expected to learn from ungrounded form alone et al., 2019). This includes, for instance, some (Bender and Koller, 2020; Bisk et al., 2020; Merrill ∗ For correspondence: {abdou,soegaard}@di.ku.dk et al., 2021). While there have been many inter109 Using two methods of evaluating the structural alignment of colors in this space with textderived color term representations, we find significant correspondence. Analyzing the differences in alignment across the color spectrum, we find that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, sugges"
2021.conll-1.9,N19-1423,0,0.0118537,"terms are solicited through a free-naming task, resulting in 122 terms. Perceptual color space Following previous work (Regier et al., 2007; Zaslavsky et al., 2018; Chaabouni et al., 2021), we map colors to their corresponding points in the 3D CIELAB space, where the first dimension L expresses lightness, the second A expresses position between red and green, and the third B expresses the position between blue and yellow. Distances between colors in the space correspond to their perceptual difference. Language models Our analysis is conducted on three widely used language models (LMs): BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), both of which employ a masked language modelling objective, and ELECTRA (Clark et al., 4 http://www1.icsi.berkeley.edu/wcs/ images/jrus-20100531/wcs-chart-4x.png 110 Figure 2: Our experimental setup. In the center is a Munsell color chart. Each chip in the chart is represented in the CIELAB space (right) and has 51 color term annotations. Color term embeddings are extracted through various methods. In the Representation Similarity Analysis experiments, a corresponding color chip centroid is computed in the CIELAB space. In the Linear Mapping experiments, a colo"
2021.conll-1.9,2020.emnlp-main.395,0,0.0992173,"Missing"
2021.conll-1.9,D19-1065,0,0.135345,"ly and the extent to which models implicitly reflect topological structure that is grounded in world, such as perceptual structure, is unknown. To explore this question, we conduct a thorough case study on color. Namely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric. Figure 1: Right: Color orientation in 3d CIELAB space. Left: linear mapping from BERT (CC, see §2) color term embeddings to the CIELAB space. features of concrete and abstract concepts, such as objects’ attributes and affordances (Forbes et al., 2019b; Weir et al., 2020). Furthermore, the representational geometry of LMs has been found to naturally reflect human lexical similarity and relatedness judgements, as well as analogy relationships (Chronis and Erk, 2020). However, the extent to which these models reflect the structures that exist in humans’ perceptual world—such as the topology of visual perception (Chen, 1982), the structure of the color spectrum (Ennis and Zaidi, 2019; Provenzi, 2020), or of odour spaces (Rossiter, 1996; Chastrette, 1997)—is not well-understood. If LMs are indeed able to capture such topologies—in some domains"
2021.conll-1.9,2021.conll-1.7,0,0.0603838,"Missing"
2021.conll-1.9,D19-1275,0,0.0247702,"on (n = 6) indicates the alignability of the two spaces, given a linear transformation. Centroids corresponding to each Munsell color chip are computed in the color term embedding space via the weighted mean of the embeddings of the 51 terms used to label it. As in the RSA experiments, terms occurring less frequently than the cutoff (f = 100) are excluded. For evaluation, we compute the average (across splits and datapoints) proportion of explained variance as well as the ranking of a predicted color term embedding according to the Pearson distance (1 − r) to gold. Control task As proposed by Hewitt and Liang (2019), we construct a random control task for the linear mapping experiments, wherein we randomly swap each color chip’s CIELAB code for another. This is meant to break the mapping between the color chips and their corresponding terms. Control task results are reported as the mean of 10 different random re-mappings. We report probe selectivity, which is defined as the difference between proportion of explained variance in the standard experimental condition and in the control task (He8 We use the colormath Python package, setting illuminant to C, and assuming 2 degree standard observer. 112 NC Mode"
2021.conll-1.9,J15-4004,0,0.0464436,"tween a chip’s regression score (predicted color 11 chip code ranking) and its surprisal. We find signifLow entropy reflects frequent co-occurrence with a small icant Spearman’s rank correlation between lower subset of the vocabulary and high entropy the converse. 115 Results show that: 6 Related Work Distributional word representations have long been theorized to capture various types of information about the world (Schütze, 1992). Early work in this regard employed semantic similarity and relatedness datasets to measure alignment to human judgements (Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Rubinstein et al. (2015), however, question whether the distributional hypothesis is equally applicable to all types of semantic information, finding that taxonomic properties (such as animacy) are better modelled than attributive ones • deprel-ent and head-ent (but not (color, size, etc.). To a similar end, Lucy and Gaupos-ent) lead to a significantly improved thier (2017) analyze how well distributional repfit compared to the control predictors; we obresentations encode various aspects of grounded serve positive coefficients for both, indicating meaning. They investigate whether language m"
2021.conll-1.9,2020.emnlp-main.254,0,0.0995714,"Missing"
2021.conll-1.9,2021.ccl-1.108,0,0.019753,"Missing"
2021.conll-1.9,W17-2810,0,0.0434436,"Missing"
2021.conll-1.9,P15-2119,0,0.0251507,"ssion score (predicted color 11 chip code ranking) and its surprisal. We find signifLow entropy reflects frequent co-occurrence with a small icant Spearman’s rank correlation between lower subset of the vocabulary and high entropy the converse. 115 Results show that: 6 Related Work Distributional word representations have long been theorized to capture various types of information about the world (Schütze, 1992). Early work in this regard employed semantic similarity and relatedness datasets to measure alignment to human judgements (Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Rubinstein et al. (2015), however, question whether the distributional hypothesis is equally applicable to all types of semantic information, finding that taxonomic properties (such as animacy) are better modelled than attributive ones • deprel-ent and head-ent (but not (color, size, etc.). To a similar end, Lucy and Gaupos-ent) lead to a significantly improved thier (2017) analyze how well distributional repfit compared to the control predictors; we obresentations encode various aspects of grounded serve positive coefficients for both, indicating meaning. They investigate whether language modRSA score is higher for"
2021.conll-1.9,2020.lrec-1.497,0,0.0214251,"Missing"
2021.conll-1.9,D19-1250,0,0.0154348,"hey hold discussions on and modify a more diverse set of syntactic wall-collisions?”, finding that perceptual features heads. This suggests that occurring in a more are poorly modelled compared to encyclopedic and diverse set of contexts might be beneficial for robust representation learning, in correspon- taxonomic ones. More recently, several studies have asked related dence with the idea of sample diversity in the questions in the context of language models. For active learning literature (Brinker, 2003; Yang et al., 2015). pos-ent’s lack of significance, example, Davison et al. (2019) and Petroni et al. (2019) mine LMs for factual and commonsense on the other hand, indicates that the degree of knowledge by converting knowledge base triplets specification offered by the POS tagset might be too coarse to meaningfully differentiate be- into cloze statements that are used to query the models. In a similar vein, Forbes et al. (2019a) tween color terms, e.g. nouns can occur in a variety of DRELs such as subjects, objects, investigate LM representations’ encoding of oboblique modifiers (per the Universal Depende- ject properties (e.g., oranges are round), and affordances (e.g. oranges can be eaten), as we"
2021.conll-1.9,2020.coling-main.605,0,0.0440178,"Missing"
2021.conll-1.9,L16-1680,0,0.0122398,"r the CC configuration, broken down by Munsell color chip; (b) shows suprisal per chip. Circle colors reflect the modal color term assigned to the chips. What factors predict color space alignment? Given that LMs are trained exclusively on text corpora, we hypothesize that alignment between their embeddings and CIELAB is influenced by corpus usage statistics. To determine which factors could predict alignment score, we extract color term log frequency, part-of-speech tag (POS), dependency relation (DREL), and dependency tree head (HEAD) statistics for all color terms from a dependency-parsed (Straka et al., 2016) common crawl corpus. In addition to this, we compute, per color term, the entropy of its normalised PMI distribution (pmi-col, see §2) as a measure of collocation.11 We then fit a Linear Mixed Effects Model (Gałecki and Burzykowski, 2013) to the features listed above, with RSA score (Table 1) as the response variable, and model type as a random effect. We follow a multi-level step-wise model building sequence, where a baseline model is first fit with color term log frequency as a single fixed effect. A model which includes pmi-col as an additional fixed effect is then fit, and these two terms"
2021.conll-1.9,P19-1452,1,0.814956,"odour spaces (Rossiter, 1996; Chastrette, 1997)—is not well-understood. If LMs are indeed able to capture such topologies—in some domains, at least—it would mean that these structures are a) somehow reflected 1 Introduction in language and, thereby, encoded in the textual Without grounding or interaction with the world, training data on which models are trained, and b) language models (LMs) learn representations that learnable using models’ current training objectives encode various aspects of formal linguistic struc- and architectural inductive biases. To the extent ture (e.g., morphosyntax (Tenney et al., 2019)) they are not, the question becomes whether the inand semantic information (e.g., lexical similarity formation is not there in the data, or whether model (Reif et al., 2019a)). Beyond this, it has been sug- and training objective limitations are to blame. Cergested that text-only training data is enough for tainly, this latter point relates to an ongoing deLMs to also acquire factual and relational informa- bate regarding what exactly language models can tion about the world (Davison et al., 2019; Petroni be expected to learn from ungrounded form alone et al., 2019). This includes, for instan"
2021.conll-1.9,2020.emnlp-main.586,0,0.0814761,"Missing"
2021.crac-1.7,D19-1588,0,0.0170539,"s MUC, B 3 , and CEAFe. The best result per column is boldfaced. Coreference models We provide benchmark scores for two strong neural models using three different transformer representations. Lee et al. (2017) (L EE 2017) presented the first neural endto-end coreference model in which the spans are learned in the same training pass as the pairwise clustering. The pairwise clustering may produce globally inconsistent clusters, and Lee et al. (2018) (L EE 2018) presented a higher-order upgrade that did not only consider pairs of spans but a matrix of all spans to counter global inconsistencies. Joshi et al. (2019) showed that the model from Lee et al. (2018) performed even better on English data when using transformer-based models instead of the word embeddings of the original implementations. We follow their approach and try representations from three different pre-trained models: Danish BERT (DA BERT)4 and two multilingual, cased models: multilingual BERT (M BERT) (Devlin et al., 2019) and the base model of XLM-Roberta (XLM-R) (Conneau et al., 2020). Instead of the TensorFlow implementations released by Joshi et al. (2019), we use the PyTorch-based implementation from AllenNLP 1.3.0.5 with PyTorch ve"
2021.crac-1.7,2020.acl-main.747,0,0.0234922,"(2018) (L EE 2018) presented a higher-order upgrade that did not only consider pairs of spans but a matrix of all spans to counter global inconsistencies. Joshi et al. (2019) showed that the model from Lee et al. (2018) performed even better on English data when using transformer-based models instead of the word embeddings of the original implementations. We follow their approach and try representations from three different pre-trained models: Danish BERT (DA BERT)4 and two multilingual, cased models: multilingual BERT (M BERT) (Devlin et al., 2019) and the base model of XLM-Roberta (XLM-R) (Conneau et al., 2020). Instead of the TensorFlow implementations released by Joshi et al. (2019), we use the PyTorch-based implementation from AllenNLP 1.3.0.5 with PyTorch version 1.7.1. A description of the tuning process is in Appendix A. After model selection, we retrain the models for a maximum of 1200 epochs with early stopping and a patience of 10. 5 M ODEL https://github.com/botxo/nordic_bert https://github.com/allenai/allennlp 65 S ENTENCE E NTITY QID KG CONTEXT L ABEL The same sentence could be heard when Elvis had left the scene after a Las Vegas show. Elvis Q303 birth name Elvis Aaron Presley given nam"
2021.crac-1.7,2020.coling-main.583,1,0.853948,"Missing"
2021.crac-1.7,D17-1018,0,0.0295074,"l, 7,173 tokens were annotated with a QID. 2,193 unique QIDs were used. Coreference results The coreference benchmark results are presented in Table 2. Models based on the two multi-lingual, cased transformer models perform a lot better than the uncased DA BERT. The best model is L EE 2018 trained with XLM-R. 4 F1 Table 2: Coreference results: M(ention) R(ecall), average P(recision), R(ecall) and F1 across MUC, B 3 , and CEAFe. The best result per column is boldfaced. Coreference models We provide benchmark scores for two strong neural models using three different transformer representations. Lee et al. (2017) (L EE 2017) presented the first neural endto-end coreference model in which the spans are learned in the same training pass as the pairwise clustering. The pairwise clustering may produce globally inconsistent clusters, and Lee et al. (2018) (L EE 2018) presented a higher-order upgrade that did not only consider pairs of spans but a matrix of all spans to counter global inconsistencies. Joshi et al. (2019) showed that the model from Lee et al. (2018) performed even better on English data when using transformer-based models instead of the word embeddings of the original implementations. We fol"
2021.crac-1.7,N18-2108,0,0.0437791,"Missing"
2021.crac-1.7,W19-3502,0,0.0274694,"Missing"
2021.crac-1.7,W00-1007,0,0.367464,"lations into coreference clusters and the training and evaluation of coreference models on this data. To the best of our knowledge, these are the first publicly available neural coreference models for Danish. We also present a new entity linking annotation on the dataset using WikiData identifiers, a named entity disambiguation (NED) dataset, and a larger automatically created NED dataset enabling wikily supervised NED models. The entity linking annotation is benchmarked using a state-of-the-art neural entity disambiguation model. 1 2 Related work Danish anaphors have received some attention: Navarretta (2000) shows that Danish deictics are used in more contexts than the English ones and Houser et al. (2006) specifically discuss the use of verb phrase pronominalization in Danish. Danish has gendered possessive pronouns, but nongendered reflexive pronouns. This has made it useful as an unambiguous testbed for gender bias in natural language inference models, machine translation models, and language models (González et al., 2020). But automatic coreference resolution for Danish has received no attention, and there was no established evaluation set for this task. Linked resources such as Wikipedia ena"
2021.crac-1.7,2020.lrec-1.497,0,0.0547533,"Missing"
2021.crac-1.7,P17-1178,0,0.0266149,"lly discuss the use of verb phrase pronominalization in Danish. Danish has gendered possessive pronouns, but nongendered reflexive pronouns. This has made it useful as an unambiguous testbed for gender bias in natural language inference models, machine translation models, and language models (González et al., 2020). But automatic coreference resolution for Danish has received no attention, and there was no established evaluation set for this task. Linked resources such as Wikipedia enable multi-lingual entity linking/NED models and datasets, and Danish is often among the evaluation languages (Pan et al., 2017; McNamee et al., 2011). DBpedia Spotlight2 is the most recent entity linking system that also supports Danish. But due to this being a different task from NED, we can not compare our model to DBpedia Spotlight. Introduction The Danish Dependency Treebank (DDT) (BuchKromann, 2003) is a beneficial resource for Danish NLP that contains several annotation layers. Most of the layers were annotated as part of the Copenhagen Dependency Treebank project, but a conversion of the dependency syntax annotation into Universal dependencies (Johannsen et al., 2015; Nivre et al., 2020) and the addition of na"
2021.crac-1.7,W19-6143,1,0.77503,"recent entity linking system that also supports Danish. But due to this being a different task from NED, we can not compare our model to DBpedia Spotlight. Introduction The Danish Dependency Treebank (DDT) (BuchKromann, 2003) is a beneficial resource for Danish NLP that contains several annotation layers. Most of the layers were annotated as part of the Copenhagen Dependency Treebank project, but a conversion of the dependency syntax annotation into Universal dependencies (Johannsen et al., 2015; Nivre et al., 2020) and the addition of named entities annotation layers (Hvingelby et al., 2020; Plank, 2019; Plank et al., 2020) are newer additions. The partial coreference annotation has received no attention for NLP purposes despite being very well documented. This paper describes converting the coreference relations into coreference clusters and a new entity linking annotation with unique Wikidata item identification codes (QIDs) (Vrandeˇci´c, 2012) on the same data. Entity linking is the task of detecting mentions and matching the mentions to a knowledge base. The two annotation layers—coreference and entity linking—complement each other as two types of 1 https://github.com/alexandrainst/ danl"
2021.eacl-main.156,N19-1348,0,0.016878,"P to collect and annotate a text corpus – and split it into training, development and test data. These splits are often based on the order in which texts were published or sampled, and are referred to as ‘standard splits’. Gorman and Bedrick (2019) recently showed that system ranking results based on standard splits differ from results based on random splits and used this to argue in favor of using random splits. While perhaps less common, random splits are already used in probing (Elazar and Goldberg, 2018), interpretability (P¨orner et al., 2018), as well as core NLP tasks (Yu et al., 2019; Geva et al., 2019).1 Gorman and Bedrick (2019) focus on whether there is a significant performance difference δ 1 See also many of the tasks in the SemEval evaluation campaigns: http://alt.qcri.org/semeval2020/ Figure 1: Data splitting strategies. Each ball corresponds to a sentence represented in (two-dimensional) feature space. Blue (dark)/orange (bright) balls represent examples for training/test. Numbers represent sentence length. Heuristic splits can, e.g., be based on sentence length; adversarial splits maximize divergence. between systems S1 and S2 ; M(Gtest , S1 ) − M(Gtest , S2 ), in their notation. Th"
2021.eacl-main.156,P19-1267,0,0.3858,"nimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits. 1 Introduction It is common practice in NLP to collect and annotate a text corpus – and split it into training, development and test data. These splits are often based on the order in which texts were published or sampled, and are referred to as ‘standard splits’. Gorman and Bedrick (2019) recently showed that system ranking results based on standard splits differ from results based on random splits and used this to argue in favor of using random splits. While perhaps less common, random splits are already used in probing (Elazar and Goldberg, 2018), interpretability (P¨orner et al., 2018), as well as core NLP tasks (Yu et al., 2019; Geva et al., 2019).1 Gorman and Bedrick (2019) focus on whether there is a significant performance difference δ 1 See also many of the tasks in the SemEval evaluation campaigns: http://alt.qcri.org/semeval2020/ Figure 1: Data splitting strategies."
2021.eacl-main.156,J15-3006,0,0.029667,"Table 1: Data used in our experiments. *: We time slice the original data to create different samples. (if those exist), better for heuristic and adversarial splits, but error still tends to be higher on new (in-domain) samples; see Figure 1. Our results not only refute the hypothesis that δ can be estimated using random splits (Gorman and Bedrick, 2019),2 but also the covariate shift hypothesis (Shimodaira, 2000; Shah et al., 2020) that δ can be estimated using reweightings of the data. While biased splits are useful in the absence of multiple held-out samples, and have been proposed before (Karimi et al., 2015),3 they often overestimate performance in the wild. Our code is made publicly available at https://github.com/ google-research/google-research/tree/ master/talk_about_random_splits. 2 Experiments We consider 7 different NLP tasks: POS tagging (like Gorman and Bedrick (2019)), two sentence representation probing tasks, headline generation, translation quality estimation, emoji prediction, and news classification. We experiment with these tasks, because they a) are diverse, b) have not been subject to decades of community-wide overfitting (with the exception of POS tagging), and c) three of them"
2021.eacl-main.156,W17-3204,0,0.0318248,"Missing"
2021.eacl-main.156,W18-6210,1,0.865862,"Missing"
2021.eacl-main.156,D15-1166,0,0.00923165,"ochastic gradient-based optimizer (Kingma and Ba, 2015), a batch size of 200, and L2 penalty of strength α = 0.01. Headline generation We use the standard dataset for headline generation, derived from the Gigaword corpus (Napoles et al., 2012), as published by Rush et al. (2015). The task is to generate a headline from the first sentence of a news article. 5 https://catalog.ldc.upenn.edu/ LDC2013T19 6 github.com/jiesutd/NCRFpp 7 github.com/facebookresearch/SentEval 8 tinyurl.com/zyq3yvn Our architecture is a sequence-to-sequence model with stacked bi-directional LSTMs with dropout, attention (Luong et al., 2015) and beam decoding; the number of hidden units is 128; we do not pre-train. Different from Rush et al. (2015), we use subword units (Sennrich et al., 2016) to overcome the OOV problem and speed up training. The ROUGE scores we obtain on the standard splits are higher than those reported by Rush et al. (2015) and comparable to those of Nallapati et al. (2016), e.g., ROUGE-1 of 0.321. As our New sample, we reserve 20,000 sentence-headline pairs each from the first and second halves of 2004 for validation and testing; years 1998-2003 are used for training. For all the experiments we report the er"
2021.eacl-main.156,C18-1094,0,0.0545446,"Missing"
2021.eacl-main.156,J93-2004,0,0.0788297,"Missing"
2021.eacl-main.156,P18-1032,0,0.0568167,"Missing"
2021.eacl-main.156,2020.acl-main.680,0,0.0989141,"Missing"
2021.eacl-main.156,D15-1044,0,0.0134243,"n-English. We present a simple model that only considers the target sentence, but performs better than the best shared task systems: we train an MLP over a LASER sentence embedding (Schwenk et al., 2019) with the following hyperparameters: two hidden layers with 100 parameters each and ReLU activation functions, trained using the Adam stochastic gradient-based optimizer (Kingma and Ba, 2015), a batch size of 200, and L2 penalty of strength α = 0.01. Headline generation We use the standard dataset for headline generation, derived from the Gigaword corpus (Napoles et al., 2012), as published by Rush et al. (2015). The task is to generate a headline from the first sentence of a news article. 5 https://catalog.ldc.upenn.edu/ LDC2013T19 6 github.com/jiesutd/NCRFpp 7 github.com/facebookresearch/SentEval 8 tinyurl.com/zyq3yvn Our architecture is a sequence-to-sequence model with stacked bi-directional LSTMs with dropout, attention (Luong et al., 2015) and beam decoding; the number of hidden units is 128; we do not pre-train. Different from Rush et al. (2015), we use subword units (Sennrich et al., 2016) to overcome the OOV problem and speed up training. The ROUGE scores we obtain on the standard splits are"
2021.eacl-main.156,W17-5019,0,0.0546157,"Missing"
2021.eacl-main.156,P16-1162,0,0.0120284,"rd dataset for headline generation, derived from the Gigaword corpus (Napoles et al., 2012), as published by Rush et al. (2015). The task is to generate a headline from the first sentence of a news article. 5 https://catalog.ldc.upenn.edu/ LDC2013T19 6 github.com/jiesutd/NCRFpp 7 github.com/facebookresearch/SentEval 8 tinyurl.com/zyq3yvn Our architecture is a sequence-to-sequence model with stacked bi-directional LSTMs with dropout, attention (Luong et al., 2015) and beam decoding; the number of hidden units is 128; we do not pre-train. Different from Rush et al. (2015), we use subword units (Sennrich et al., 2016) to overcome the OOV problem and speed up training. The ROUGE scores we obtain on the standard splits are higher than those reported by Rush et al. (2015) and comparable to those of Nallapati et al. (2016), e.g., ROUGE-1 of 0.321. As our New sample, we reserve 20,000 sentence-headline pairs each from the first and second halves of 2004 for validation and testing; years 1998-2003 are used for training. For all the experiments we report the error reduction in ROUGE-2 of the model over the IDENTITY baseline, which simply copies the input sentence (other ROUGE values are reported in the §A.1). In"
2021.eacl-main.156,W12-3018,0,0.0613866,"Missing"
2021.eacl-main.156,N18-1101,0,0.0545901,"Missing"
2021.eacl-main.162,W09-1201,0,0.139366,"Missing"
2021.eacl-main.162,C10-2051,0,0.0416761,". Edmiston (2020) analyses morphological content in BERT-style models for five languages and finds that “[morphological] ambiguity is negatively correlated with performance on classification, and to a significant degree in many cases”, suggesting that morphology is still a significant source of error in these models. We go significantly beyond this work by studying a much larger set of morphological variables, across several architectures and tasks, and across up to 57 languages. Background Morphology is frequently identified as a source of error during qualitative evaluations of NLP systems. Honnibal et al. (2010) observe that inflectional variants cause problems for statistical CCG tagging due to training data sparseness, and explicit morphological analysis helps, even for English. For dependency parsing, Seeker and Kuhn (2013) identify case syncretism as a source of error propagation in data from Czech, German, and Hungarian. Tsarfaty 3 Datasets We collect datasets from shared tasks that (i) publish system outputs along with their gold annotations, (ii) span a variety of languages, and (iii) cover different NLP tasks. Based on these criteria, we pick datasets from the following shared tasks: 1 https:"
2021.eacl-main.162,Q13-1035,0,0.034633,"... Predict Random forest classifier Figure 1: Overview of our methodology: We map each token to a set of morphological features and, based on this representation, predict whether some NLP system (e.g., a dependency parser) was correct (3) or made an error (7) on that token. Introduction In error analysis, we often blame morphology (Nivre, 2007; Bender, 2009), i.e., the productive inflection and derivation of new word forms. Morphology has been argued to be a major source of error in syntactic parsing (Tsarfaty et al., 2020), semantic parsing (Sahin ¸ and Steedman, 2018), machine translation (Irvine et al., 2013; Burlot and Yvon, 2017) and a range of other tasks, in particular in morphologically complex languages (Bender, 2009; Søgaard et al., 2018; Tsarfaty et al., 2020). This paper presents a large-scale study showing that morphology is, as commonly conjectured, an important source of error across tasks, but somewhat surprisingly, that morphology is less predictive of errors in morphologically complex languages. English is a morphologically simple language, showing very limited inflection and expressing most concepts through syntactic structure instead; it is also the most-represented language at m"
2021.eacl-main.162,2020.acl-main.560,0,0.0174171,"et al., 2018; Tsarfaty et al., 2020). This paper presents a large-scale study showing that morphology is, as commonly conjectured, an important source of error across tasks, but somewhat surprisingly, that morphology is less predictive of errors in morphologically complex languages. English is a morphologically simple language, showing very limited inflection and expressing most concepts through syntactic structure instead; it is also the most-represented language at major natural language processing (NLP) venues and that with the largest amount of language resources available (Bender, 2011; Joshi et al., 2020). This makes it easy to ignore morphology when designing model architectures. As a consequence, we frequently observe that performance of NLP systems on morphologically more complex languages lags behind that for English (e.g. Czarnowska et al., 2019; Tsarfaty et al., 2020). Complex morphology leads to the occurrence of rare inflected word forms. Polish nouns, for example, can inflect for number and seven different cases; this makes it less likely that all of these inflected word forms appear in the training data for our NLP models. Consequently, a model that correctly handles imi˛e ‘name’ (NO"
2021.eacl-main.162,L18-1293,0,0.0280595,"for several morphological feature sets: e.g., ask can be either U : MOOD = IND or U : MOOD = IMP, depending on context; (ii) AMBIG POS specifies to what extent the universal part-of-speech tag of the token can differ based on context: e.g., book could be either U : POS = VERB or U : POS = NOUN; and (iii) AMBIG LEX specifies whether or not the token belongs to multiple lexemes: e.g., ruling is a form of both ‘(to) rule’ and ‘(the) ruling’. To determine these features for a given token, we use UDLexicons5 (Sagot, 2018); in case a language is not covered by UDLexicons, we fall back to UniMorph6 (Kirov et al., 2018). Finally, we define purely string-based features based on comparing the token with its lemma. We perform character-based string alignment using Edlib (Šoši´c and Šiki´c, 2017) and derive the following features: (i) EDIT = PRE and EDIT = SUF when there is an edit at the beginning or the end of the sequence, respectively; (ii) EDIT = IN when there is an edit in the middle of the sequence; and (iii) EDIT = FULL when there is no character alignment between the strings. These features are intended to approximate prefixation, suffixation, infixation or other word-internal processes, and suppletion,"
2021.eacl-main.162,L18-1292,0,0.0186295,"al lexical features: (i) SYNCRETIC specifies to what extent a token can be representative for several morphological feature sets: e.g., ask can be either U : MOOD = IND or U : MOOD = IMP, depending on context; (ii) AMBIG POS specifies to what extent the universal part-of-speech tag of the token can differ based on context: e.g., book could be either U : POS = VERB or U : POS = NOUN; and (iii) AMBIG LEX specifies whether or not the token belongs to multiple lexemes: e.g., ruling is a form of both ‘(to) rule’ and ‘(the) ruling’. To determine these features for a given token, we use UDLexicons5 (Sagot, 2018); in case a language is not covered by UDLexicons, we fall back to UniMorph6 (Kirov et al., 2018). Finally, we define purely string-based features based on comparing the token with its lemma. We perform character-based string alignment using Edlib (Šoši´c and Šiki´c, 2017) and derive the following features: (i) EDIT = PRE and EDIT = SUF when there is an edit at the beginning or the end of the sequence, respectively; (ii) EDIT = IN when there is an edit in the middle of the sequence; and (iii) EDIT = FULL when there is no character alignment between the strings. These features are intended to a"
2021.eacl-main.162,J13-1004,0,0.0313274,"in many cases”, suggesting that morphology is still a significant source of error in these models. We go significantly beyond this work by studying a much larger set of morphological variables, across several architectures and tasks, and across up to 57 languages. Background Morphology is frequently identified as a source of error during qualitative evaluations of NLP systems. Honnibal et al. (2010) observe that inflectional variants cause problems for statistical CCG tagging due to training data sparseness, and explicit morphological analysis helps, even for English. For dependency parsing, Seeker and Kuhn (2013) identify case syncretism as a source of error propagation in data from Czech, German, and Hungarian. Tsarfaty 3 Datasets We collect datasets from shared tasks that (i) publish system outputs along with their gold annotations, (ii) span a variety of languages, and (iii) cover different NLP tasks. Based on these criteria, we pick datasets from the following shared tasks: 1 https://github.com/coastalcph/ eacl2021-morpherror 1888 • SEM: CoNLL-2009 Shared Task on Semantic Dependencies (Hajiˇc et al., 2009), covering semantic role labeling for seven languages. • UDP: CoNLL-2018 Shared Task on Unive"
2021.eacl-main.162,P16-1162,0,0.0206364,"cted word forms. Polish nouns, for example, can inflect for number and seven different cases; this makes it less likely that all of these inflected word forms appear in the training data for our NLP models. Consequently, a model that correctly handles imi˛e ‘name’ (NOM . SG) might not have seen the less frequent form imionami (INST. PL), potentially resulting in errors. If the model has generally seen fewer words in instrumental case, this can lead to systematic errors on this class of inflections. Nowadays, many NLP systems use statistically learned subword units such as byte-pair encodings (Sennrich et al., 2016) or use characters as input representations, which could allow a system to generalize to individual affixes. However, in practice, these approaches are often found to be in1887 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1887–1900 April 19 - 23, 2021. ©2021 Association for Computational Linguistics sufficient at capturing morphological structure (Vania and Lopez, 2017; Bostrom and Durrett, 2020; Klein and Tsarfaty, 2020). Contributions In this study, we revisit two common conjectures about the role of morphology that are ma"
2021.eacl-main.162,P18-1072,1,0.837317,"Missing"
2021.eacl-main.162,K17-3009,0,0.0206232,". The full feature inventory is summarized in Table 1; what follows is a description of these features and how we derived them. Morphological features Our morphological feature inventory consists of (i) Universal Dependencies (UD) features, (ii) lexical features, and (iii) string-based features. UD features include the universal part-ofspeech (POS) category and the universal feature set as defined by Universal Dependencies; e.g. U : POS = VERB or U : TENSE = PAST.2 The UDP shared-task gold data already provides this annotation; for the other tasks, we obtain these features by running UDPipe3 (Straka and Straková, 2017) with the largest pre-trained model for the language in question.4 We complement this with the following additional lexical features: (i) SYNCRETIC specifies to what extent a token can be representative for several morphological feature sets: e.g., ask can be either U : MOOD = IND or U : MOOD = IMP, depending on context; (ii) AMBIG POS specifies to what extent the universal part-of-speech tag of the token can differ based on context: e.g., book could be either U : POS = VERB or U : POS = NOUN; and (iii) AMBIG LEX specifies whether or not the token belongs to multiple lexemes: e.g., ruling is a"
2021.eacl-main.264,Q19-1004,0,0.0209175,"O:   Hi = Attention QWiQ , KWiK , V WiV (2) MHA(Q, K, V ) = concat(H1 , H2 , ..., Hk )W O (3) Every layer also consists of a feed-forward network (FFN), consisting of two Dense layers with ReLU activation functions. For each layer, therefore, the output of MHA is passed through a LayerNorm with residual connections, passed through FFN, and then through another LayerNorm with residual connections. Searching for structure Often, the line of inquiry regarding interpretability in NLP has been concerned with extracting and analyzing linguistic information from neural network models of language (Belinkov and Glass, 2019). Recently, such investigations have targeted Transformer models (Hewitt and Manning, 2019; Rosa and Mareˇcek, 2019; Tenney et al., 2019), at least in part because the self-attention mechanism employed by these models offers a possible window into their inner workings. With large-scale machine translation and language models being openly distributed for experimentation, several researchers have wondered if self-attention is capable of representing syntactic structure, despite not being trained with any overt parsing objective. In pursuit of this question, Raganato et al. (2018) applied a maxim"
2021.eacl-main.264,2020.acl-main.493,0,0.0162277,"Design To examine the extent to which we can decode dependency trees from attention patterns, we run a tree decoding algorithm over mBERT’s attention heads — before and after fine-tuning via a parsing objective. We surmise that doing so will enable us to determine if attention can be interpreted as a reliable mechanism for capturing linguistic structure. 3.1 Model We employ mBERT1 in our experiments, which has been shown to perform well across a variety of NLP tasks (Hu et al., 2020; Kondratyuk and Straka, 2019a) and capture aspects of syntactic structure cross-lingually (Pires et al., 2019; Chi et al., 2020). mBERT features 12 layers with 768 hidden units and 12 attention heads, with a joint WordPiece sub-word vocabulary across languages. The model was trained on the concatenation of WikiDumps for the top 104 languages with the largest Wikipedias,where principled sampling was employed to enforce a balance between high- and 1 https://github.com/google-research/ bert 3033 low-resource languages. 3.2 Decoding Algorithm For decoding dependency trees, we follow Raganato et al. (2018) in applying the Chu-LiuEdmonds maximum spanning tree algorithm (Chu, 1965) to every layer/head combination available in"
2021.eacl-main.264,W19-4828,0,0.35935,"such attention matrices to the score matrices employed in arc-factored dependency parsing (McDonald et al., 2005a,b), a salient question concerning interpretability becomes: Can we expect some combination of these parameters to capture linguistic structure in the form of a dependency tree, especially if the model performs well on NLP tasks? If not, can we relax the expectation and examine the extent to which subcomponents of the linguistic structure, such as subject-verb relations, are represented? This prospect was first posed by Raganato et al. (2018) for MT encoders, and later explored by Clark et al. (2019) for BERT. Ultimately, the consensus of these and other studies (Voita et al., 2019; Htut et al., 2019; Limisiewicz et al., 2020) was that, while there appears to exist no “generalist” head responsible for extracting full dependency structures, standalone heads often specialize in capturing individual grammatical relations. Unfortunately, most of such studies focused their 3031 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3031–3045 April 19 - 23, 2021. ©2021 Association for Computational Linguistics experiments entirely on E"
2021.eacl-main.264,N19-1423,0,0.0343978,"tions track the same relations across languages? 3. How do attention patterns change after finetuning with explicit syntactic annotation? 4. Which components of the model are involved in these changes? In answering these questions, we believe we can shed further light on the (cross-)linguistic properties of Transformer-based language models, as well as address the question of attention patterns being a reliable representation of linguistic structure. 2 Attention as Structure Transformers The focus of the present study is mBERT, a multilingual variant of the exceedingly popular language model (Devlin et al., 2019). BERT is built upon the Transformer architecture (Vaswani et al., 2017b), which is a self-attentionbased encoder-decoder model (though only the encoder is relevant to our purposes). A Transformer takes a sequence of vectors x = [x1 , x2 , ...xn ] as input and applies a positional encoding to them, in order to retain the order of words in a sentence. These inputs are then transformed into query (Q), key (K), and value (V ) vectors via three separate linear transformations and passed to an attention mechanism. A single attention head computes scaled dot-product attention between K and Q, output"
2021.eacl-main.264,P81-1022,0,0.634003,"Missing"
2021.eacl-main.264,N19-1419,0,0.0240634,"O (3) Every layer also consists of a feed-forward network (FFN), consisting of two Dense layers with ReLU activation functions. For each layer, therefore, the output of MHA is passed through a LayerNorm with residual connections, passed through FFN, and then through another LayerNorm with residual connections. Searching for structure Often, the line of inquiry regarding interpretability in NLP has been concerned with extracting and analyzing linguistic information from neural network models of language (Belinkov and Glass, 2019). Recently, such investigations have targeted Transformer models (Hewitt and Manning, 2019; Rosa and Mareˇcek, 2019; Tenney et al., 2019), at least in part because the self-attention mechanism employed by these models offers a possible window into their inner workings. With large-scale machine translation and language models being openly distributed for experimentation, several researchers have wondered if self-attention is capable of representing syntactic structure, despite not being trained with any overt parsing objective. In pursuit of this question, Raganato et al. (2018) applied a maximum-spanning-tree algorithm over the attention weights of several trained MT models, compar"
2021.eacl-main.264,N19-1357,0,0.153929,"s entirely on English, which is typologically favored to succeed in such scenarios due to its rigid word order and lack of inflectional morphology. It remains to be seen whether the attention patterns of such models can capture structural features across typologically diverse languages, or if the reported experiments on English are a misrepresentation of local positional heuristics as such. Furthermore, though previous work has investigated how attention patterns might change after fine-tuning on different tasks (Htut et al., 2019), a recent debate about attention as an explanatory mechanism (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019) has cast the entire enterprise in doubt. Indeed, it remains to be seen whether fine-tuning on an explicit structured prediction task, e.g. dependency parsing, can force attention to represent the structure being learned, or if the patterns observed in pretrained models are not altered in any meaningful way. To address these issues, we investigate the prospect of extracting linguistic structure from the attention weights of multilingual Transformerbased language models. In light of the surveyed literature, our research questions are as follows: 1. Can we decode dep"
2021.eacl-main.264,P04-1061,0,0.0967521,"input to be the element-wise product of a given attention matrix and its transpose (A ◦ A&gt; ). We liken this to the joint probability of a head attending to its dependent and a dependent attending to its head, similarly to Limisiewicz et al. (2020). Per this point, we also follow Htut et al. (2019) in evaluating the decoded trees via Undirected Unlabeled Attachment Score (UUAS) — the percentage of undirected edges recovered correctly. Since we discount directionality, this is effectively a less strict measure than UAS, but one that has a long tradition in unsupervised dependency parsing since Klein and Manning (2004). 3.3 Data For our data, we employ the Parallel Universal Dependencies (PUD) treebanks, as collected in UD v2.4 (Nivre et al., 2019). PUD was first released as part of the CONLL 2017 shared task (Zeman et al., 2018), containing 1000 parallel sentences, which were (professionally) translated from English, German, French, Italian, and Spanish to 14 other languages. The sentences are taken from two domains, news and wikipedia, the latter implying some overlap with mBERT’s training data (though we did not investigate this). We include all PUD treebanks except Thai.2 3.4 the exact tree structure we"
2021.eacl-main.264,D19-1279,0,0.380977,"hat attention weights cannot provide even a plausible explanation for models relying on syntax. 3 Experimental Design To examine the extent to which we can decode dependency trees from attention patterns, we run a tree decoding algorithm over mBERT’s attention heads — before and after fine-tuning via a parsing objective. We surmise that doing so will enable us to determine if attention can be interpreted as a reliable mechanism for capturing linguistic structure. 3.1 Model We employ mBERT1 in our experiments, which has been shown to perform well across a variety of NLP tasks (Hu et al., 2020; Kondratyuk and Straka, 2019a) and capture aspects of syntactic structure cross-lingually (Pires et al., 2019; Chi et al., 2020). mBERT features 12 layers with 768 hidden units and 12 attention heads, with a joint WordPiece sub-word vocabulary across languages. The model was trained on the concatenation of WikiDumps for the top 104 languages with the largest Wikipedias,where principled sampling was employed to enforce a balance between high- and 1 https://github.com/google-research/ bert 3033 low-resource languages. 3.2 Decoding Algorithm For decoding dependency trees, we follow Raganato et al. (2018) in applying the Chu"
2021.eacl-main.264,D19-1445,0,0.0224834,"e-tuning the entire mBERT network, we conducted a series of experiments, wherein we updated only select components of model and left the remainder frozen. Most surprisingly, we observed that the Transformer parameters designed for composing the attention matrix, K and Q, were only modestly capable of guiding the attention towards resembling the dependency structure. In contrast, it was the Value (V ) parameters, which are used for computing a weighted sum over the KQproduced attention, that yielded the most faithful representations of the linguistic structure via attention. Though prior work (Kovaleva et al., 2019; Zhao and Bethard, 2020) seems to indicate that there is a lack of a substantial change in attention patterns after fine-tuning on syntax- and semantics-oriented classification tasks, the opposite effect has been observed with fine-tuning on negation scope resolution, where a more explanatory attention mechanism can be induced (Htut et al., 2019). Our results are similar to the latter, and we demonstrate that given explicit syntactic annotation, attention weights do end up storing more transparently decodable structure. It is, however, still unclear which sets of transformer parameters are be"
2021.eacl-main.264,2020.acl-main.375,1,0.840291,"to training on concatenated PUD sets, however, our results are not directly comparable/ 3038 mBERT’s internal representations may play a role. Indeed, as we hypothesized in Section 3.2, it could be the case that the composition of CJK characters into gold tokens for Chinese and Japanese may degrade the representations (and their corresponding attention) therein. Furthermore, for Japanese and Korean specifically, it has been observed that tokenization strategies employed by different treebanks could drastically influence the conclusions one may draw about their inherent hierarchical structure (Kulmizev et al., 2020). Turkish and French are admittedly more difficult to diagnose. Note, however, that we fine-tuned our model on a concatenation of all PUD treebanks. As such, any deviation from PUD’s annotation norms is therefore likely to be heavily penalised, by virtue of signal from other languages drowning out these differences. 6 Conclusion In this study, we revisited the prospect of decoding dependency trees from the self-attention patterns of Transformer-based language models. We elected to extend our experiments to 18 languages in order to gain better insight about how tree decoding accuracy might be a"
2021.eacl-main.264,2020.emnlp-main.363,1,0.839093,"UERY, now seems to show a UUAS drop almost uniformly. This is also true for the completely unfrozen encoder. Supervised Parsing In addition to decoding trees from attention matrices, we also measure supervised UAS/LAS on a held-out test set.5 Based on Figure 4, it is apparent that all settings result 4 The inner average is over all heads; the outer is over all languages. 5 Note that the test set in our scenario is from the actual, non-parallel language treebank; as such, we left Korean out of this comparison due to annotation differences. in generally the same UAS. This is somewhat expected; Lauscher et al. (2020) see better results on parsing with the entire encoder frozen, implying that the task is easy enough for a biaffine parser to learn, given frozen mBERT representations.6 The LAS distinction is, however, rather interesting: there is a marked difference between how important the dense layers are, as opposed to the attentive components. This is likely not reflected in our UUAS probe as, strictly speaking, labelling arcs is not equivalent to searching for structure in sentences, but more akin to classifying pre-identified structures. We also note that D ENSE appears to be better than N ONE on aver"
2021.eacl-main.264,2020.findings-emnlp.245,0,0.0378293,"Missing"
2021.eacl-main.264,P05-1012,0,0.234401,"owing that the attention learned by their models reflects expected cross-lingual idiosyncrasies between English and French, e.g., concerning word order. With self-attentive Transformers, interpretation becomes slightly more difficult, as attention is distributed across words within the input itself. This is further compounded by the use of multiple layers and heads, each combination of which yields its own alignment, representing a different (possibly redundant) view of the data. Given the similarity of such attention matrices to the score matrices employed in arc-factored dependency parsing (McDonald et al., 2005a,b), a salient question concerning interpretability becomes: Can we expect some combination of these parameters to capture linguistic structure in the form of a dependency tree, especially if the model performs well on NLP tasks? If not, can we relax the expectation and examine the extent to which subcomponents of the linguistic structure, such as subject-verb relations, are represented? This prospect was first posed by Raganato et al. (2018) for MT encoders, and later explored by Clark et al. (2019) for BERT. Ultimately, the consensus of these and other studies (Voita et al., 2019; Htut et a"
2021.eacl-main.264,H05-1066,0,0.53079,"Missing"
2021.eacl-main.264,2020.lrec-1.497,1,0.867817,"Missing"
2021.eacl-main.264,P19-1493,0,0.0214993,"ntax. 3 Experimental Design To examine the extent to which we can decode dependency trees from attention patterns, we run a tree decoding algorithm over mBERT’s attention heads — before and after fine-tuning via a parsing objective. We surmise that doing so will enable us to determine if attention can be interpreted as a reliable mechanism for capturing linguistic structure. 3.1 Model We employ mBERT1 in our experiments, which has been shown to perform well across a variety of NLP tasks (Hu et al., 2020; Kondratyuk and Straka, 2019a) and capture aspects of syntactic structure cross-lingually (Pires et al., 2019; Chi et al., 2020). mBERT features 12 layers with 768 hidden units and 12 attention heads, with a joint WordPiece sub-word vocabulary across languages. The model was trained on the concatenation of WikiDumps for the top 104 languages with the largest Wikipedias,where principled sampling was employed to enforce a balance between high- and 1 https://github.com/google-research/ bert 3033 low-resource languages. 3.2 Decoding Algorithm For decoding dependency trees, we follow Raganato et al. (2018) in applying the Chu-LiuEdmonds maximum spanning tree algorithm (Chu, 1965) to every layer/head combi"
2021.eacl-main.264,W18-5431,0,0.0260543,"Missing"
2021.eacl-main.264,P19-1282,0,0.0246737,"f whether it can be seen as a “faithful” explanation of model predictions has been subject to much recent debate. For example, Jain and Wallace (2019) present compelling arguments that attention does not offer a faithful explanation of predictions. Primarily, they demonstrate that there is little correlation between standard feature importance measures and attention weights. Furthermore, they contend that there exist counterfactual attention distributions, which are substantially different from learned attention weights but that do not alter a model’s predictions. Using a similar methodology, Serrano and Smith (2019) corroborate that attention does not provide an adequate account of an input component’s importance. In response to these findings, Wiegreffe and Pinter (2019) question the assumptions underlying such claims. Attention, they argue, is not a primitive, i.e., it cannot be detached from the rest of a model’s components as is done in the experiments of Jain and Wallace (2019). They propose a set of four analyses to test whether a given model’s attention mechanism can provide meaningful explanation and demonstrate that the alternative attention distributions found via adversarial training methods d"
2021.eacl-main.264,D18-1548,0,0.0596304,"Missing"
2021.eacl-main.264,K18-2001,1,0.819166,"to Limisiewicz et al. (2020). Per this point, we also follow Htut et al. (2019) in evaluating the decoded trees via Undirected Unlabeled Attachment Score (UUAS) — the percentage of undirected edges recovered correctly. Since we discount directionality, this is effectively a less strict measure than UAS, but one that has a long tradition in unsupervised dependency parsing since Klein and Manning (2004). 3.3 Data For our data, we employ the Parallel Universal Dependencies (PUD) treebanks, as collected in UD v2.4 (Nivre et al., 2019). PUD was first released as part of the CONLL 2017 shared task (Zeman et al., 2018), containing 1000 parallel sentences, which were (professionally) translated from English, German, French, Italian, and Spanish to 14 other languages. The sentences are taken from two domains, news and wikipedia, the latter implying some overlap with mBERT’s training data (though we did not investigate this). We include all PUD treebanks except Thai.2 3.4 the exact tree structure we aim to decode. To this end, we employ the graph-based decoding algorithm of the biaffine parser introduced by Dozat and Manning (2016). We replace the standard BiLSTM encoder for this parser with the entire mBERT n"
2021.eacl-main.264,2020.acl-main.429,0,0.0374127,"RT network, we conducted a series of experiments, wherein we updated only select components of model and left the remainder frozen. Most surprisingly, we observed that the Transformer parameters designed for composing the attention matrix, K and Q, were only modestly capable of guiding the attention towards resembling the dependency structure. In contrast, it was the Value (V ) parameters, which are used for computing a weighted sum over the KQproduced attention, that yielded the most faithful representations of the linguistic structure via attention. Though prior work (Kovaleva et al., 2019; Zhao and Bethard, 2020) seems to indicate that there is a lack of a substantial change in attention patterns after fine-tuning on syntax- and semantics-oriented classification tasks, the opposite effect has been observed with fine-tuning on negation scope resolution, where a more explanatory attention mechanism can be induced (Htut et al., 2019). Our results are similar to the latter, and we demonstrate that given explicit syntactic annotation, attention weights do end up storing more transparently decodable structure. It is, however, still unclear which sets of transformer parameters are best suited for learning th"
2021.eacl-main.264,D18-1412,0,0.0195081,"same fashion as described in Section 3.2. We surmise that, if attention heads are capable of tracking hierarchical relations between words in any capacity, it is precisely in this setting that this ability would be attested. In addition to this, we are interested in what individual components of the mBERT network are capable of steering attention patterns towards syntactic structure. We believe that addressing this question will help us not only in interpreting decisions made by BERT-based neural parsers, but also in aiding us developing syntax-aware models in general (Strubell et al., 2018; Swayamdipta et al., 2018). As such — beyond fine-tuning all parameters of the mBERT network (our basic setting) — we perform a series of ablation experiments wherein we update only one set of parameters per training cycle, e.g. the Query weights WiQ , and leave everything else frozen. This gives us a set of 6 models, which are described below. For each model, all non-BERT parser components are always left unfrozen. Fine-Tuning Details In addition to exploring pretrained mBERT’s attention weights, we are also interested in how attention might be guided by a training objective that learns 2 Thai is the only treebank tha"
2021.eacl-main.264,P19-1452,0,0.0637448,"Missing"
2021.eacl-main.264,P19-1580,0,0.0785703,"arsing (McDonald et al., 2005a,b), a salient question concerning interpretability becomes: Can we expect some combination of these parameters to capture linguistic structure in the form of a dependency tree, especially if the model performs well on NLP tasks? If not, can we relax the expectation and examine the extent to which subcomponents of the linguistic structure, such as subject-verb relations, are represented? This prospect was first posed by Raganato et al. (2018) for MT encoders, and later explored by Clark et al. (2019) for BERT. Ultimately, the consensus of these and other studies (Voita et al., 2019; Htut et al., 2019; Limisiewicz et al., 2020) was that, while there appears to exist no “generalist” head responsible for extracting full dependency structures, standalone heads often specialize in capturing individual grammatical relations. Unfortunately, most of such studies focused their 3031 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3031–3045 April 19 - 23, 2021. ©2021 Association for Computational Linguistics experiments entirely on English, which is typologically favored to succeed in such scenarios due to its rigi"
2021.eacl-main.264,D19-1002,0,0.112922,"hich is typologically favored to succeed in such scenarios due to its rigid word order and lack of inflectional morphology. It remains to be seen whether the attention patterns of such models can capture structural features across typologically diverse languages, or if the reported experiments on English are a misrepresentation of local positional heuristics as such. Furthermore, though previous work has investigated how attention patterns might change after fine-tuning on different tasks (Htut et al., 2019), a recent debate about attention as an explanatory mechanism (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019) has cast the entire enterprise in doubt. Indeed, it remains to be seen whether fine-tuning on an explicit structured prediction task, e.g. dependency parsing, can force attention to represent the structure being learned, or if the patterns observed in pretrained models are not altered in any meaningful way. To address these issues, we investigate the prospect of extracting linguistic structure from the attention weights of multilingual Transformerbased language models. In light of the surveyed literature, our research questions are as follows: 1. Can we decode dependency trees for some langua"
2021.eacl-main.68,P96-1047,0,0.621166,"llipsis The J OINT Sluice Ellipsis results improve modestly over the S INGLE -TASK Sluice Ellipsis results. This is noteworthy, since the added VP Ellipsis data is quite small compared to the size of the sluice data. These models consistently select an antecedent of the right syntactic form, which is normally a complete sentence. Many of the errors consist of empty outputs: S INGLE -TASK Sluice Ellipsis produces 58 empty outputs, while J OINT Sluice Ellipsis produces 63. Another source of error is discontiguous antecedents. It is not unusual for the gold antecedent to be a discontiguous span (Donecker, 1996), but our models are not permitted to produce such antecedents, so these cases will always be a source of error. All the systems have problems when the antecedent follows the ellipsis, as in the following example: I don’t know why, but they seem to need a story. We also compared the right and left periphery scores of sluices, and found better results predicting the right periphery: for S INGLE -TASK Sluice Ellipsis, there were 678 matches on the left edge, and 733 on the right edge; for J OINT Sluice 6 We also briefly discuss how coreference resolution benefits from synergies with ellipsis in"
2021.eacl-main.68,W09-3906,0,0.0375753,"ncept stock,&quot; he said. &quot;You either believe Seymour can do it again or you don&apos;t … Question: You either believe Seymour can do it again or you don&apos;t. Answer: believe Seymour can do it again Introduction Figure 1: Examples of Sluice Ellipsis and Verb Phrase Ellipsis, represented as “questions” about their associated contexts. Wh-phrases and auxiliary verbs are marked in red and elided phrases are marked in blue. Ellipsis resolution is a hard, open problem in NLP, and an important source of error in machine translation, question answering, and dialogue understanding (Vicedo and Ferr´andez, 2000; Dzikovska et al., 2009; Chung and Gildea, 2010; Macketanz et al., 2018; Petrn Bach Hansen and Sgaard, 2020). There are no large annotated text corpora for this phenomenon, even for English, and we only have annotations for a subset of the known ellipsis constructions. Since annotation is expensive and cumbersome, any synergies with existing NLP tasks could be useful and enable us to leverage auxiliary data when learning models for ellipsis resolution. This paper presents a simple yet strong approach to ellipsis resolution based on a straightforward observation, depicted in Figure 1, that ellipsis resolution can be"
2021.eacl-main.68,2020.acl-main.132,0,0.0284898,"e, Isabelle Augenstein, and Anders Søgaard. 2019. X-WikiRE: A large, multilingual resource for relation extraction as machine comprehension. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 265–274, Hong Kong, China. Association for Computational Linguistics. We are not the first to use question answering to redefine a set of tasks. Recently, He et al. (2015) showed that semantic role labeling annotations could be solicited by asking simple questions that implicitly target predicate-argument relations in a sentence. Parallel to our work, Hou (2020) cast bridging anaphora resolution as question answering based on context. Wu et al. (2020) and Li et al. (2020) also reformulate coreference resolution and named entity recognition as QA. In the realm of re-framing relation extraction as a QA problem, Levy et al. (2017) and Abdou et al. (2019) create monolingual and multilingual template based QA datasets respectively, which yield relation extraction models which were better at generalizing in the zero-shot setting. Extending this idea, McCann et al. (2018) introduced the DecaNLP challenge, which casts 10 core tasks in NLP as question-answeri"
2021.eacl-main.68,N18-2108,0,0.0532653,"Missing"
2021.eacl-main.68,W12-4501,0,0.0451033,"al., 2018), with a CNN encoder, and (iii) BERT (Devlin et al., 2019), with a (pretrained) transformer encoder. We use the three different models to show that the between-task synergies are relatively robust across architectures; but one architecture (BERT) is clearly superior to the others and will be the standard baseline we propose for future research.3 Coreference Resolution For coreference resolution, which we use as an auxiliary task, we train and evaluate on two corpora: (i) the English portion of the OntoNotes 5.02 corpus with the standard data split used in the CoNLL-2012 shared task (Pradhan et al., 2012), and (ii) the WikiCoref corpus (Ghaddar and Langlais, 2016), which contains annotations of 30 documents from the English Wikipedia. From this dataset, we use 22 documents for training, 4 documents for development, and 4 for testing. QA We also use SQuAD v1.1 (Rajpurkar et al., 2016) as an auxiliary reading comprehension dataset. 3 Note that there are many differences between these architectures; not only the encoder networks. The number of parameters differ, and BERT is pre-trained on large volumes of data. Our purpose here is not comparing strategies, but simply showing that synergies can be"
2021.eacl-main.68,K17-1034,0,0.0122311,"g, China. Association for Computational Linguistics. We are not the first to use question answering to redefine a set of tasks. Recently, He et al. (2015) showed that semantic role labeling annotations could be solicited by asking simple questions that implicitly target predicate-argument relations in a sentence. Parallel to our work, Hou (2020) cast bridging anaphora resolution as question answering based on context. Wu et al. (2020) and Li et al. (2020) also reformulate coreference resolution and named entity recognition as QA. In the realm of re-framing relation extraction as a QA problem, Levy et al. (2017) and Abdou et al. (2019) create monolingual and multilingual template based QA datasets respectively, which yield relation extraction models which were better at generalizing in the zero-shot setting. Extending this idea, McCann et al. (2018) introduced the DecaNLP challenge, which casts 10 core tasks in NLP as question-answering problems. Similar to our work, their architecture jointly learns across all of these tasks. DecaNLP includes pronoun resolution, a subset of coreference resolution, but it does so only on a small, hand-crafted dataset; it does not address ellipsis. Pranav Anand and Ji"
2021.eacl-main.68,D16-1264,0,0.0470955,"e others and will be the standard baseline we propose for future research.3 Coreference Resolution For coreference resolution, which we use as an auxiliary task, we train and evaluate on two corpora: (i) the English portion of the OntoNotes 5.02 corpus with the standard data split used in the CoNLL-2012 shared task (Pradhan et al., 2012), and (ii) the WikiCoref corpus (Ghaddar and Langlais, 2016), which contains annotations of 30 documents from the English Wikipedia. From this dataset, we use 22 documents for training, 4 documents for development, and 4 for testing. QA We also use SQuAD v1.1 (Rajpurkar et al., 2016) as an auxiliary reading comprehension dataset. 3 Note that there are many differences between these architectures; not only the encoder networks. The number of parameters differ, and BERT is pre-trained on large volumes of data. Our purpose here is not comparing strategies, but simply showing that synergies can be seen across all architectures. For more details, see Appendix B. 2 https://catalog.ldc.upenn.edu/ LDC2013T19 811 TASK Sluice Ellipsis VP Ellipsis S OTA S INGLE TASK 70.00 (Rønning et al., 2018) 72.89 (Zhang et al., 2019) J OINT D R QA QAN ET BERT D R QA QAN ET BERT 77.48 62.86 75.70"
2021.eacl-main.68,2020.acl-main.519,0,0.0258791,"traction as machine comprehension. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 265–274, Hong Kong, China. Association for Computational Linguistics. We are not the first to use question answering to redefine a set of tasks. Recently, He et al. (2015) showed that semantic role labeling annotations could be solicited by asking simple questions that implicitly target predicate-argument relations in a sentence. Parallel to our work, Hou (2020) cast bridging anaphora resolution as question answering based on context. Wu et al. (2020) and Li et al. (2020) also reformulate coreference resolution and named entity recognition as QA. In the realm of re-framing relation extraction as a QA problem, Levy et al. (2017) and Abdou et al. (2019) create monolingual and multilingual template based QA datasets respectively, which yield relation extraction models which were better at generalizing in the zero-shot setting. Extending this idea, McCann et al. (2018) introduced the DecaNLP challenge, which casts 10 core tasks in NLP as question-answering problems. Similar to our work, their architecture jointly learns across all of these tasks. DecaNLP includes"
2021.eacl-main.68,N18-2038,1,0.891584,"Missing"
2021.eacl-main.68,P00-1070,0,0.496067,"Missing"
2021.eacl-main.68,W18-6436,0,0.0275159,"ur can do it again or you don&apos;t … Question: You either believe Seymour can do it again or you don&apos;t. Answer: believe Seymour can do it again Introduction Figure 1: Examples of Sluice Ellipsis and Verb Phrase Ellipsis, represented as “questions” about their associated contexts. Wh-phrases and auxiliary verbs are marked in red and elided phrases are marked in blue. Ellipsis resolution is a hard, open problem in NLP, and an important source of error in machine translation, question answering, and dialogue understanding (Vicedo and Ferr´andez, 2000; Dzikovska et al., 2009; Chung and Gildea, 2010; Macketanz et al., 2018; Petrn Bach Hansen and Sgaard, 2020). There are no large annotated text corpora for this phenomenon, even for English, and we only have annotations for a subset of the known ellipsis constructions. Since annotation is expensive and cumbersome, any synergies with existing NLP tasks could be useful and enable us to leverage auxiliary data when learning models for ellipsis resolution. This paper presents a simple yet strong approach to ellipsis resolution based on a straightforward observation, depicted in Figure 1, that ellipsis resolution can be converted to a QA problem. Ellipsis and question"
2021.emnlp-main.363,Q19-1038,0,0.0547326,"Missing"
2021.emnlp-main.363,W19-3809,0,0.0184555,"the idea of retraining sentiment classifiers on round-trip-translated data. 1 Introduction It is both unethical and potentially illegal for document classification algorithms to perform significantly better for some groups in society than for others (Mehrabi et al., 2019). Many document classification technologies have, however, been shown to be sensitive to protected attributes such as gender and age (Mehrabi et al., 2019; Delobelle et al., 2020; Ferrer et al., 2020; Koh et al., 2021). This also holds for sentiment analysis (Johannsen et al., 2015; Hovy, 2015; Kiritchenko and Mohammad, 2018; Bhaskaran and Bhallamudi, 2019; Touileb et al., 2020). At the same time it is known that round-trip machine translation (Huang, 1990; Federmann et al., 2019) can be used to normalize text (Ling et al., 2013; Rabinovich et al., 2017; Prabhumoye et al., 2018). This could potentially remove group specific deviations from normal language. However, Stanovsky et al. (2019) found that machine translation is also prone to potentially introducing gender bias. Combined, this leaves it an open question whether round-trip translation can be used to reduce the sensitivity of document classifiers to protected attributes of the authors."
2021.emnlp-main.363,2021.wassa-1.6,0,0.0623398,"Missing"
2021.emnlp-main.363,D19-5503,0,0.0173993,"l for document classification algorithms to perform significantly better for some groups in society than for others (Mehrabi et al., 2019). Many document classification technologies have, however, been shown to be sensitive to protected attributes such as gender and age (Mehrabi et al., 2019; Delobelle et al., 2020; Ferrer et al., 2020; Koh et al., 2021). This also holds for sentiment analysis (Johannsen et al., 2015; Hovy, 2015; Kiritchenko and Mohammad, 2018; Bhaskaran and Bhallamudi, 2019; Touileb et al., 2020). At the same time it is known that round-trip machine translation (Huang, 1990; Federmann et al., 2019) can be used to normalize text (Ling et al., 2013; Rabinovich et al., 2017; Prabhumoye et al., 2018). This could potentially remove group specific deviations from normal language. However, Stanovsky et al. (2019) found that machine translation is also prone to potentially introducing gender bias. Combined, this leaves it an open question whether round-trip translation can be used to reduce the sensitivity of document classifiers to protected attributes of the authors. In this paper, we evaluate the effect of roundtrip translation on fairness using a representative ∗ Equal contributions. soegaa"
2021.emnlp-main.363,P15-1073,0,0.0366917,"Missing"
2021.emnlp-main.363,C90-3074,0,0.767021,"tially illegal for document classification algorithms to perform significantly better for some groups in society than for others (Mehrabi et al., 2019). Many document classification technologies have, however, been shown to be sensitive to protected attributes such as gender and age (Mehrabi et al., 2019; Delobelle et al., 2020; Ferrer et al., 2020; Koh et al., 2021). This also holds for sentiment analysis (Johannsen et al., 2015; Hovy, 2015; Kiritchenko and Mohammad, 2018; Bhaskaran and Bhallamudi, 2019; Touileb et al., 2020). At the same time it is known that round-trip machine translation (Huang, 1990; Federmann et al., 2019) can be used to normalize text (Ling et al., 2013; Rabinovich et al., 2017; Prabhumoye et al., 2018). This could potentially remove group specific deviations from normal language. However, Stanovsky et al. (2019) found that machine translation is also prone to potentially introducing gender bias. Combined, this leaves it an open question whether round-trip translation can be used to reduce the sensitivity of document classifiers to protected attributes of the authors. In this paper, we evaluate the effect of roundtrip translation on fairness using a representative ∗ Eq"
2021.emnlp-main.363,K15-1011,1,0.844762,"Missing"
2021.emnlp-main.363,N19-1423,0,0.0307709,"Missing"
2021.emnlp-main.363,S18-2005,0,0.019534,"en-group gaps). We also explore the idea of retraining sentiment classifiers on round-trip-translated data. 1 Introduction It is both unethical and potentially illegal for document classification algorithms to perform significantly better for some groups in society than for others (Mehrabi et al., 2019). Many document classification technologies have, however, been shown to be sensitive to protected attributes such as gender and age (Mehrabi et al., 2019; Delobelle et al., 2020; Ferrer et al., 2020; Koh et al., 2021). This also holds for sentiment analysis (Johannsen et al., 2015; Hovy, 2015; Kiritchenko and Mohammad, 2018; Bhaskaran and Bhallamudi, 2019; Touileb et al., 2020). At the same time it is known that round-trip machine translation (Huang, 1990; Federmann et al., 2019) can be used to normalize text (Ling et al., 2013; Rabinovich et al., 2017; Prabhumoye et al., 2018). This could potentially remove group specific deviations from normal language. However, Stanovsky et al. (2019) found that machine translation is also prone to potentially introducing gender bias. Combined, this leaves it an open question whether round-trip translation can be used to reduce the sensitivity of document classifiers to prote"
2021.emnlp-main.363,D13-1008,0,0.0282708,"nificantly better for some groups in society than for others (Mehrabi et al., 2019). Many document classification technologies have, however, been shown to be sensitive to protected attributes such as gender and age (Mehrabi et al., 2019; Delobelle et al., 2020; Ferrer et al., 2020; Koh et al., 2021). This also holds for sentiment analysis (Johannsen et al., 2015; Hovy, 2015; Kiritchenko and Mohammad, 2018; Bhaskaran and Bhallamudi, 2019; Touileb et al., 2020). At the same time it is known that round-trip machine translation (Huang, 1990; Federmann et al., 2019) can be used to normalize text (Ling et al., 2013; Rabinovich et al., 2017; Prabhumoye et al., 2018). This could potentially remove group specific deviations from normal language. However, Stanovsky et al. (2019) found that machine translation is also prone to potentially introducing gender bias. Combined, this leaves it an open question whether round-trip translation can be used to reduce the sensitivity of document classifiers to protected attributes of the authors. In this paper, we evaluate the effect of roundtrip translation on fairness using a representative ∗ Equal contributions. soegaard@di.ku.dk. Corresponding Table 1: The normalizi"
2021.emnlp-main.363,P18-1080,0,0.0275888,"than for others (Mehrabi et al., 2019). Many document classification technologies have, however, been shown to be sensitive to protected attributes such as gender and age (Mehrabi et al., 2019; Delobelle et al., 2020; Ferrer et al., 2020; Koh et al., 2021). This also holds for sentiment analysis (Johannsen et al., 2015; Hovy, 2015; Kiritchenko and Mohammad, 2018; Bhaskaran and Bhallamudi, 2019; Touileb et al., 2020). At the same time it is known that round-trip machine translation (Huang, 1990; Federmann et al., 2019) can be used to normalize text (Ling et al., 2013; Rabinovich et al., 2017; Prabhumoye et al., 2018). This could potentially remove group specific deviations from normal language. However, Stanovsky et al. (2019) found that machine translation is also prone to potentially introducing gender bias. Combined, this leaves it an open question whether round-trip translation can be used to reduce the sensitivity of document classifiers to protected attributes of the authors. In this paper, we evaluate the effect of roundtrip translation on fairness using a representative ∗ Equal contributions. soegaard@di.ku.dk. Corresponding Table 1: The normalizing effect of round-trip translation: translating En"
2021.emnlp-main.363,E17-1101,0,0.0177941,"or some groups in society than for others (Mehrabi et al., 2019). Many document classification technologies have, however, been shown to be sensitive to protected attributes such as gender and age (Mehrabi et al., 2019; Delobelle et al., 2020; Ferrer et al., 2020; Koh et al., 2021). This also holds for sentiment analysis (Johannsen et al., 2015; Hovy, 2015; Kiritchenko and Mohammad, 2018; Bhaskaran and Bhallamudi, 2019; Touileb et al., 2020). At the same time it is known that round-trip machine translation (Huang, 1990; Federmann et al., 2019) can be used to normalize text (Ling et al., 2013; Rabinovich et al., 2017; Prabhumoye et al., 2018). This could potentially remove group specific deviations from normal language. However, Stanovsky et al. (2019) found that machine translation is also prone to potentially introducing gender bias. Combined, this leaves it an open question whether round-trip translation can be used to reduce the sensitivity of document classifiers to protected attributes of the authors. In this paper, we evaluate the effect of roundtrip translation on fairness using a representative ∗ Equal contributions. soegaard@di.ku.dk. Corresponding Table 1: The normalizing effect of round-trip t"
2021.emnlp-main.363,N13-1077,1,0.857469,"Missing"
2021.emnlp-main.363,P19-1164,0,0.0271642,"e sensitive to protected attributes such as gender and age (Mehrabi et al., 2019; Delobelle et al., 2020; Ferrer et al., 2020; Koh et al., 2021). This also holds for sentiment analysis (Johannsen et al., 2015; Hovy, 2015; Kiritchenko and Mohammad, 2018; Bhaskaran and Bhallamudi, 2019; Touileb et al., 2020). At the same time it is known that round-trip machine translation (Huang, 1990; Federmann et al., 2019) can be used to normalize text (Ling et al., 2013; Rabinovich et al., 2017; Prabhumoye et al., 2018). This could potentially remove group specific deviations from normal language. However, Stanovsky et al. (2019) found that machine translation is also prone to potentially introducing gender bias. Combined, this leaves it an open question whether round-trip translation can be used to reduce the sensitivity of document classifiers to protected attributes of the authors. In this paper, we evaluate the effect of roundtrip translation on fairness using a representative ∗ Equal contributions. soegaard@di.ku.dk. Corresponding Table 1: The normalizing effect of round-trip translation: translating English to Spanish and back. corpus of Danish Trustpilot reviews, in which reviews are associated with self-report"
2021.emnlp-main.363,2020.gebnlp-1.11,0,0.0318053,"classifiers on round-trip-translated data. 1 Introduction It is both unethical and potentially illegal for document classification algorithms to perform significantly better for some groups in society than for others (Mehrabi et al., 2019). Many document classification technologies have, however, been shown to be sensitive to protected attributes such as gender and age (Mehrabi et al., 2019; Delobelle et al., 2020; Ferrer et al., 2020; Koh et al., 2021). This also holds for sentiment analysis (Johannsen et al., 2015; Hovy, 2015; Kiritchenko and Mohammad, 2018; Bhaskaran and Bhallamudi, 2019; Touileb et al., 2020). At the same time it is known that round-trip machine translation (Huang, 1990; Federmann et al., 2019) can be used to normalize text (Ling et al., 2013; Rabinovich et al., 2017; Prabhumoye et al., 2018). This could potentially remove group specific deviations from normal language. However, Stanovsky et al. (2019) found that machine translation is also prone to potentially introducing gender bias. Combined, this leaves it an open question whether round-trip translation can be used to reduce the sensitivity of document classifiers to protected attributes of the authors. In this paper, we evalu"
2021.emnlp-main.375,2021.naacl-main.184,0,0.0939797,"Missing"
2021.emnlp-main.375,2021.ccl-1.108,0,0.0951171,"Missing"
2021.emnlp-main.375,N18-2002,0,0.0478951,"Missing"
2021.emnlp-main.375,2020.emnlp-main.334,0,0.0169358,"pretrained language models represent concepts and demographic groups, and its lower part shows (May et al., 2019; Kurita et al., 2019), but this is, to the mean rank of each group across all models. the best of our knowledge, the first work on whose The correlations of different models’ rankings are language pretrained language models reflect, i.e., shown as a heat map in Figure 1 in the Appendix. what sociolects models align best with. Work on We make the following observation: The language personalized language modeling (Garimella et al., models systematically disfavor young non-white 2017; Welch et al., 2020) is loosely related, e.g., male speakers. Other groups that are poorly aligned Stoop and van den Bosch (2014) present interesting with language models include older white speakers. to work to make word prediction sociolect-aware, 5 showing significant keystroke savings by conditionBoth models remain the most fair under the standard deviation definition of fairness. ing on sociolect. 4584 Ethics Statement Our paper considers the sensitivity of pretrained language models to protected attributes. The data used in our experiments was collected using Amazon Mechanical Turk by researchers in psychol"
2021.emnlp-main.375,2020.emnlp-main.346,0,0.0369981,"t al., 2020) uses a jointly trained discriminator network to distinguish the masked tokens from candidates suggested by the generator, avoiding costly inference over the full vocabulary. We use the generator models, google/electra-small-generator and google/electra-large-generator, which are suitable for the cloze-style word prediction. 6. Finally, we also include instances of the unidirectional architecture proposed in (Radford et al., 2019) (GPT). Since GPT-3 (Brown et al., 2020) is not currently open source, we probe gpt2, gpt2-medium, gpt2-large and gpt2-xl models below. Metrics We follow Shin et al. (2020) in using precision (P@1) and mean reciprocal rank (MRR) 3 Model names listed in this paper are consistent with the name in Transformers packages (https://github.com/ huggingface/transformers). All models can be downloaded at https://huggingface.co/models 4582 to evaluate the extent to which pretrained language models are aligned with annotator preferences. Given a incomplete sentence v1 . . . vn __, and W = [w1 , w2 , · · · , wr ] the r-most frequent continuations of v1 . . . vn (within a group of human annotators). Our probed language model ranks candidate words by their model likelihood C ="
2021.emnlp-main.375,P19-1164,0,0.0218173,"augh (0.04) growl (0.04) move (0.03) rise (0.03) leave (0.08) fidget (0.04) argue (0.02) Table 1: An example of a cloze (fill-in-the-gap) task with human and model predictions. We evaluate the sociolectal biases of a range of pre-trained English language models. Unlike previous work on biases in pre-trained language models, we do not consider representational biases (Sun et al., 2019), but performance disparities; moreover, we do not consider downstream performance differences after fine-tuning for downstream tasks such as coreference resolution (Rudinger et al., 2018) or machine translation (Stanovsky et al., 2019), but performance differences across demographics of the language models themselves on cloze (fill-inthe-gap) problems. Since the cloze task is how these pre-trained language models are trained, we can evaluate models directly without introducing biases from probes or downstream tasks. Note that some sociolinguistic variables are salient (e.g., this→dis), others are not (Jaeger and Weatherholtz, 2016). One strategy to evaluate the robustness of pre-trained language models across groups would be to identify salient variables and evaluate language models in the context of those (Demszky et al.,"
2021.emnlp-main.375,E14-1034,0,0.0640466,"Missing"
2021.emnlp-main.375,P19-1159,0,0.0232972,"t violate regulations. After waiting three hours, Cal whined and started to ___. cry (0.50) squirm (0.05) yell (0.04) Human complain (0.11) pout (0.05) pace (0.03) run (0.12) cry (0.07) eat (0.03) Machine bark (0.08) pace (0.07) laugh (0.04) growl (0.04) move (0.03) rise (0.03) leave (0.08) fidget (0.04) argue (0.02) Table 1: An example of a cloze (fill-in-the-gap) task with human and model predictions. We evaluate the sociolectal biases of a range of pre-trained English language models. Unlike previous work on biases in pre-trained language models, we do not consider representational biases (Sun et al., 2019), but performance disparities; moreover, we do not consider downstream performance differences after fine-tuning for downstream tasks such as coreference resolution (Rudinger et al., 2018) or machine translation (Stanovsky et al., 2019), but performance differences across demographics of the language models themselves on cloze (fill-inthe-gap) problems. Since the cloze task is how these pre-trained language models are trained, we can evaluate models directly without introducing biases from probes or downstream tasks. Note that some sociolinguistic variables are salient (e.g., this→dis), others"
2021.emnlp-main.59,2020.acl-main.421,0,0.187639,"gual transfer. Pires et al. (2019), including untied positional encodings et al. (2019) show that although wordpiece over- (TUPE; Ke et al. (2020)) and relative positional lap tends to improve cross-lingual transfer perfor- encodings (Shaw et al., 2018; Huang et al., 2020), mance, even languages with different scripts (and none of these better facilitate cross-lingual comno shared subwords) may enable zero-shot trans- pression or sharing. In fact, multilingual language fer. Wu and Dredze (2019) report similar results on models trained with untied or relative positional a wider range of tasks. Artetxe et al. (2020) show encodings exhibit much worse cross-lingual perfor763 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 763–777 c November 7–11, 2021. 2021 Association for Computational Linguistics mance. We show that this is because sinusoidal embeddings facilitate compositionality, which we argue is particularly important for cross-lingual compression. We present a method for quantifying the compositionality of positional encodings, and find additional evidence for this hypothesis in word-position correlations and ablation studies. We are, to the best of our"
2021.emnlp-main.59,2020.acl-main.407,0,0.0251116,"at absolute representations converge to waveforms that represent sinusoids somewhat, while neither of the untied experiments do so (cf. Appendix B). We hypothesize that absolute representations converge to waveforms because of increased pressure for compositionality, being trained on struc5 Vaswani et al. (2017) do not explicitly mention compoturally different languages. To test this, we quantify sitionality, but only generalization across positions for fixed the extent to which the absolute, relative and untied offsets. Positional disentanglement is the flipside of compositionality, however (Chaabouni et al., 2020). encodings are compositional in the sense that there 768 est for the (simpler) EN + [ EN ] experiment – this is unsurprising, as EN + [ EN ] is still perceived as bilingual due to the shifted vocabulary. The structural similarity between the two, however, makes it easier to build compositional representations by relying on offsets, as the model only needs to learn to represent one language, structurally speaking. We observe a similar gap when comparing pretrained BERT models: bert-base-multilingual-cased exhibits more sinusoidal representations over a range of offsets, when compared to bert-b"
2021.emnlp-main.59,2020.acl-main.240,0,0.0240561,"g us a total of 7 k = 16 for the relative key model, but saw no difference in languages x 6 encoding methods x 3 seeds x 2 cor- results. 766 4 Evaluation We adopt Dufter and Schütze’s (2021) evaluation pipeline, evaluating each of our models at layers 0 and 8; we also describe a multilingual score, which is defined as the average accuracy for the retrieval and translation tasks, at layers 0 and 8. We also measure perplexity, both on the monolingual first half of the corpus, and on both halves combined. Note that true perplexities for masked language models are intractable (Wang and Cho, 2019; Salazar et al., 2020). We use a trivial approximation and calculate perplexity based on the prediction loss for each masked token; note that while these suffice for comparison purposes, they are not true perplexities and should not be taken as such outside the context of these experiments. We present our results (averaged out over fauxlanguages) in Figure 1, with full results in Appendix C. As expected, the more recent positional encodings are superior to sinusoidal or absolute positional encodings in the monolingual setting; but somewhat surprisingly, sinusoidal and absolute positional encodings are clearly outpe"
2021.emnlp-main.59,N18-2074,0,0.372643,"n compositionality and cross-lingual alignment. In other words, while sinusoidal positional encodings were originally designed for monolingual applications, they are particularly useful in multilingual language models. Anders Søgaard Department of Computer Science University of Copenhagen Denmark soegaard@di.ku.dk Sinusoidal Absolute TUPE TUPE(r) Relative(k) Relative(k/q) See §2 ((wi + pi )W Q,1 ) ((wj + pj )W K,1 )&gt; (xli W Q,l )(xlj W K,l )&gt; + (pi U Q )(pj U K )&gt; . . . +bj−i (xi W Q )(xj W K + aij )&gt; (xi W Q + aij ) (xj W K + aij )&gt; Vaswani et al. (2017) Devlin et al. (2019) Ke et al. (2020) Shaw et al. (2018) Huang et al. (2020) Table 1: We compare six positional encodings and their impact on cross-lingual generalization in multilingual language models that neither a shared vocabulary nor joint multilingual pre-training are necessary to train successful multilingual models. K et al. (2020) find that model depth is a contributor to transfer performance, but that reducing the number of self-attention heads does not have much of an effect. Our starting point is Dufter and Schütze (2021), who claim that a) multilingual compression is caused by forced parameter sharing across languages, and that b) pos"
2021.emnlp-main.624,D19-1481,0,0.0181815,"reduced to 512. With 5.2 turns per conversation on average in both CGA and CMV, we can encode 97 subword tokens on average per turn. 3.2 Training paradigms In this and subsequent sections, we use N to denote the number of turns in a conversation, f to denote the final turn in a conversation (which is always the potential site of derailment in CGA and CMV ), l to denote the label of this turn, such that 1=personal attack and 0=civil. We use t to denote any turn in a conversation that is not the final one. A conversation is therefore be formalized as ({t1 , ..., tN −1 }, f, l). Static training Chang and Danescu-NiculescuMizil (2019) train their models feeding all {t1 , ..., tN −1 } turns as input and making a predicIn this section we describe previous approaches to the task of forecasting derailment in online conver- tion. At inference time, the model is tested dynamically, i.e. feeding turn t1 as input and making a 1 We omit PreTox, a fully silver-standard data set, which prediction ˆl1 , then feeding turns {t1 , t2 } and makcontains excessive noise (Karan and Šnajder, 2019). ˆ 2 Toxicity scores are obtained with https://www. ing a prediction l2 , and so on until N − 1 predicperspectiveapi.com/ tions have been accumulat"
2021.emnlp-main.624,N19-1423,0,0.0275737,"we propose. 3.1 Model architectures Chang and Danescu-Niculescu-Mizil (2019) encode conversations with a recurrent hierarchical encoder (Sordoni et al., 2015). In the main model for the task, this encoder is followed by a classification head, making a binary prediction. The authors show that it is beneficial to pretrain the encoder in an encoder-decoder system trained to generate the next turn in a conversation. In their setup, only the first 80 tokens of a turn are considered, presumably for reasons of computational efficiency. Recently, pretrained transformer language encoders such as BERT (Devlin et al., 2019) have proven successful at various NLP tasks, so we test whether that would extend to forecasting derailment. BERT does not have a hierarchical structure, but relying on attention rather than recurrency it can better capture long-distance dependencies. We preserve information about the boundaries between turns by inserting a [SEP] token between them. One [CLS] token is further added to the start of the input and one [SEP] token to its end. To fit within BERT’s maximum sequence length, we crop each conversation down to 512 subword tokens—instead of doing the cropping with a hard limit per turn,"
2021.emnlp-main.624,2020.lrec-1.838,0,0.0238224,"Missing"
2021.emnlp-main.624,D18-1305,0,0.0180703,"r on the F1 metric, respectively. 7915 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7915–7919 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Resources for Forecasting Derailment In this section, we describe the two main resources available for the task of forecasting derailment.1 Conversations Gone Awry (CGA) CGA (Zhang et al., 2018) was built from Wikipedia Talk Page conversations. The initial seed of conversations is sampled from WikiConv based on an automatic measure of toxicity that ranges from 0 (no toxicity) to 1 (Hua et al., 2018).2 A conversation is included as a potential example of derailment if the N th comment in it has a toxicity score higher than 0.6 and all preceding comments have a score lower than 0.4. A conversation is included as a potential example of non-derailment if all comments in it score below 0.4. This initial seed is subsequently manually annotated (1) to confirm that the initial exchange is civil and (2) to determine whether after this initial exchange a personal attack occurs from one user towards another. Conversations with a personal attack are included in the dataset up to and including the tu"
2021.emnlp-main.624,W19-3514,0,0.0150956,"n a conversation that is not the final one. A conversation is therefore be formalized as ({t1 , ..., tN −1 }, f, l). Static training Chang and Danescu-NiculescuMizil (2019) train their models feeding all {t1 , ..., tN −1 } turns as input and making a predicIn this section we describe previous approaches to the task of forecasting derailment in online conver- tion. At inference time, the model is tested dynamically, i.e. feeding turn t1 as input and making a 1 We omit PreTox, a fully silver-standard data set, which prediction ˆl1 , then feeding turns {t1 , t2 } and makcontains excessive noise (Karan and Šnajder, 2019). ˆ 2 Toxicity scores are obtained with https://www. ing a prediction l2 , and so on until N − 1 predicperspectiveapi.com/ tions have been accumulated. The overall predicted 7916 3 Methods for Forecasting Derailment CGA CRAFT BERT·SC BERT·SC+ CMV Acc P R F1 Mean H Acc P R F1 Mean H 64.4 64.7 64.3 62.7 61.5 61.2 71.7 79.4 78.9 66.9 69.3 68.8 2.36 2.60 2.85 60.5 62.0 56.5 57.5 58.6 56.0 81.3 82.8 73.2 67.3 68.5 61.7 4.01 3.90 4.06 Table 2: Experimental results. + denotes dynamic training. H denotes the forecast horizon. label is obtained as ˆl = maxi=1..N −1 ˆli . Chang and Danescu-Niculescu-Miz"
2021.emnlp-main.624,R19-1154,0,0.0378477,"Missing"
2021.emnlp-main.624,P18-1125,0,0.106095,"n can be achieved at the cost of a small drop in F1; in a low-quality data setting, however, dynamic training propagates the noise and is highly detrimental to performance. Anders Søgaard University of Copenhagen soegaard@di.ku.dk A B A Where’s your source? I’m just looking for it. In other words, it’s all bullshit you pulled outta your ass, right? Table 1: A conversation derails into a personal attack. the first comment made by user A, and hope to get an early warning for the derailment coming up in two turns. This is indeed the inference procedure that was used in previous work on the task (Zhang et al., 2018; Chang and Danescu-Niculescu-Mizil, 2019). Training, on the other hand, was done statically in those works, i.e. by feeding all turns up to the personal attack as input and making a prediction based on that. This leads to a disparity between the training and inference procedures, where at training time the implicit task of the model is to predict whether the next turn contains a personal 1 Introduction attack, whereas at inference time its task is to predict whether any future turn will contain a personal The flow of a conversation can be interrupted when its participants turn to attacking ea"
2021.emnlp-main.649,D19-1662,1,0.813975,"senzweig, 2011), by design it mostly presents propositions that are consistent with our world views. Apart from an early attempt to model belief bias in argument analysis (McConachy et al., 1998), and one example of awareness of belief bias in crowdsourcing experiments (Chen et al., 2019), NLP has so far ignored belief biases.2 This is, in a way, surprising given the amount of research in other biases, including sample biases (Chaganty et al., 2017; Xu et al., 2020), reporting biases (Forbes and Choi, 2017; Shwartz and Choi, 2020), annotator biases (Geva et al., 2019), and demographic biases (Barrett et al., 2019; Meyer et al., 2020). 2 Machine Reading We briefly present the four (common, popular) machine reading models evaluated in §3 and §4: Bi-directional Attention Flow The bidirectional attention flow (BiDAF) architecture (Seo et al., 2018) comprises character and word embeddings, and uses a recurrent neural network (Hochreiter and Schmidhuber, 1997) to learn how to represent the context. A specialized attention flow layer couples query and context vectors to produce query-aware feature vectors for each word in the context that are then passed on to an output layer. BiDAF models trained on SQuAD a"
2021.emnlp-main.649,D17-1109,0,0.0278602,": While Wikipedia, the main source of data for many language models and most machine reading systems, includes descriptions of fiction, and occasional misinformation (Rosenzweig, 2011), by design it mostly presents propositions that are consistent with our world views. Apart from an early attempt to model belief bias in argument analysis (McConachy et al., 1998), and one example of awareness of belief bias in crowdsourcing experiments (Chen et al., 2019), NLP has so far ignored belief biases.2 This is, in a way, surprising given the amount of research in other biases, including sample biases (Chaganty et al., 2017; Xu et al., 2020), reporting biases (Forbes and Choi, 2017; Shwartz and Choi, 2020), annotator biases (Geva et al., 2019), and demographic biases (Barrett et al., 2019; Meyer et al., 2020). 2 Machine Reading We briefly present the four (common, popular) machine reading models evaluated in §3 and §4: Bi-directional Attention Flow The bidirectional attention flow (BiDAF) architecture (Seo et al., 2018) comprises character and word embeddings, and uses a recurrent neural network (Hochreiter and Schmidhuber, 1997) to learn how to represent the context. A specialized attention flow layer couples q"
2021.emnlp-main.649,N19-1053,0,0.0284371,"tors. The failure mode discussed in this paper is a direct consequence of our language models being trained on texts that align with our beliefs about the world: While Wikipedia, the main source of data for many language models and most machine reading systems, includes descriptions of fiction, and occasional misinformation (Rosenzweig, 2011), by design it mostly presents propositions that are consistent with our world views. Apart from an early attempt to model belief bias in argument analysis (McConachy et al., 1998), and one example of awareness of belief bias in crowdsourcing experiments (Chen et al., 2019), NLP has so far ignored belief biases.2 This is, in a way, surprising given the amount of research in other biases, including sample biases (Chaganty et al., 2017; Xu et al., 2020), reporting biases (Forbes and Choi, 2017; Shwartz and Choi, 2020), annotator biases (Geva et al., 2019), and demographic biases (Barrett et al., 2019; Meyer et al., 2020). 2 Machine Reading We briefly present the four (common, popular) machine reading models evaluated in §3 and §4: Bi-directional Attention Flow The bidirectional attention flow (BiDAF) architecture (Seo et al., 2018) comprises character and word emb"
2021.emnlp-main.649,N19-1423,0,0.0170405,"F models trained on SQuAD are known to be sensitive to syntactic and lexical ambiguities (Seo et al., 2018). I evaluate two versions of BiDAF: The simpler (BiDAF) relies on GloVe (Pennington et al., 2014) word embeddings; the slightly more sophisticated (ELMo-BiDAF) relies on ELMo contextualized embeddings (Peters et al., 2018), a log-bilinear regression model that combines the advantages of global matrix factorization and local context window methods. Transformer Question Answering This model (TransformerQA) is based on RoBERTa (Liu et al., 2019) and simply uses the architecture for SQuAD in Devlin et al. (2019). This model performs much better on SQuAD 2.0 than ELMo-BiDAF – with an error reduction of two thirds, i.e., 0.67 – but on par with ELMo-BIDAF on AUTO -L OCKE (§4). Numerically Aware QANet Our last model is an extension of the QANet architecture (Yu et al., 2018), presented in Dua et al. (2019). The QANet architecture is based on convolutions and self-attention, and NAQAnet, in addition, includes a classifier that predicts whether the answer is a count or an arithmetic expression, triggering a subsequent prediction of the specific numbers involved in the expression. The evaluated models were"
2021.emnlp-main.649,N19-1246,0,0.0501649,"Missing"
2021.emnlp-main.649,P17-1025,0,0.0173941,"e models and most machine reading systems, includes descriptions of fiction, and occasional misinformation (Rosenzweig, 2011), by design it mostly presents propositions that are consistent with our world views. Apart from an early attempt to model belief bias in argument analysis (McConachy et al., 1998), and one example of awareness of belief bias in crowdsourcing experiments (Chen et al., 2019), NLP has so far ignored belief biases.2 This is, in a way, surprising given the amount of research in other biases, including sample biases (Chaganty et al., 2017; Xu et al., 2020), reporting biases (Forbes and Choi, 2017; Shwartz and Choi, 2020), annotator biases (Geva et al., 2019), and demographic biases (Barrett et al., 2019; Meyer et al., 2020). 2 Machine Reading We briefly present the four (common, popular) machine reading models evaluated in §3 and §4: Bi-directional Attention Flow The bidirectional attention flow (BiDAF) architecture (Seo et al., 2018) comprises character and word embeddings, and uses a recurrent neural network (Hochreiter and Schmidhuber, 1997) to learn how to represent the context. A specialized attention flow layer couples query and context vectors to produce query-aware feature vec"
2021.emnlp-main.649,D19-1107,0,0.0536356,"Missing"
2021.emnlp-main.649,D17-1215,0,0.341758,". Both ELMo-BiDAF and NAQANet, for example, err on this simple machine reading problem in spite of the very short context (and thereby very limited set of potential answers). See Table 2 for hand-tailored examples used as templates in AUTO -L OCKE and more real-life examples. even the simple question of What is Washington? Instead of answering ’a number’, they consistently answered ’a city’. Introduction Reading comprehension models are biased in many ways: they often expect lexical overlap between answer and question (Schlegel et al., 2020), expect the answers to occur in specific positions (Jia and Liang, 2017), or expect answers to be named entities (Rondeau and Hazen, 2018). This paper considers belief bias (Sternberg and Leighton, 2004; Anderson and Hartzler, 2014) in the context of machine reading based on language models. Belief bias is a type of cognitive bias, defined in psychology as the tendency to evaluate a statement based on prior beliefs rather than its logical strength (Evans et al., 1983). In Figure 1, the answer (’Germany’) follows straightforwardly from the context (without inference), but machine reading models nevertheless err, presumably because the prediction (’Malaysia’) aligns"
2021.emnlp-main.649,2020.acl-main.698,0,0.0439547,"Missing"
2021.emnlp-main.649,W98-1212,0,0.201295,"ntexts (see Figure 1). In §4, I argue why our failure mode cannot be reduced to being about distractors. The failure mode discussed in this paper is a direct consequence of our language models being trained on texts that align with our beliefs about the world: While Wikipedia, the main source of data for many language models and most machine reading systems, includes descriptions of fiction, and occasional misinformation (Rosenzweig, 2011), by design it mostly presents propositions that are consistent with our world views. Apart from an early attempt to model belief bias in argument analysis (McConachy et al., 1998), and one example of awareness of belief bias in crowdsourcing experiments (Chen et al., 2019), NLP has so far ignored belief biases.2 This is, in a way, surprising given the amount of research in other biases, including sample biases (Chaganty et al., 2017; Xu et al., 2020), reporting biases (Forbes and Choi, 2017; Shwartz and Choi, 2020), annotator biases (Geva et al., 2019), and demographic biases (Barrett et al., 2019; Meyer et al., 2020). 2 Machine Reading We briefly present the four (common, popular) machine reading models evaluated in §3 and §4: Bi-directional Attention Flow The bidirec"
2021.emnlp-main.649,2020.lrec-1.796,0,0.027769,"sign it mostly presents propositions that are consistent with our world views. Apart from an early attempt to model belief bias in argument analysis (McConachy et al., 1998), and one example of awareness of belief bias in crowdsourcing experiments (Chen et al., 2019), NLP has so far ignored belief biases.2 This is, in a way, surprising given the amount of research in other biases, including sample biases (Chaganty et al., 2017; Xu et al., 2020), reporting biases (Forbes and Choi, 2017; Shwartz and Choi, 2020), annotator biases (Geva et al., 2019), and demographic biases (Barrett et al., 2019; Meyer et al., 2020). 2 Machine Reading We briefly present the four (common, popular) machine reading models evaluated in §3 and §4: Bi-directional Attention Flow The bidirectional attention flow (BiDAF) architecture (Seo et al., 2018) comprises character and word embeddings, and uses a recurrent neural network (Hochreiter and Schmidhuber, 1997) to learn how to represent the context. A specialized attention flow layer couples query and context vectors to produce query-aware feature vectors for each word in the context that are then passed on to an output layer. BiDAF models trained on SQuAD are known to be sensit"
2021.emnlp-main.649,2020.lrec-1.660,0,0.0155077,"ng. 1 Figure 1: Real-life example from Twitter (through http://metropho.rs). Both ELMo-BiDAF and NAQANet, for example, err on this simple machine reading problem in spite of the very short context (and thereby very limited set of potential answers). See Table 2 for hand-tailored examples used as templates in AUTO -L OCKE and more real-life examples. even the simple question of What is Washington? Instead of answering ’a number’, they consistently answered ’a city’. Introduction Reading comprehension models are biased in many ways: they often expect lexical overlap between answer and question (Schlegel et al., 2020), expect the answers to occur in specific positions (Jia and Liang, 2017), or expect answers to be named entities (Rondeau and Hazen, 2018). This paper considers belief bias (Sternberg and Leighton, 2004; Anderson and Hartzler, 2014) in the context of machine reading based on language models. Belief bias is a type of cognitive bias, defined in psychology as the tendency to evaluate a statement based on prior beliefs rather than its logical strength (Evans et al., 1983). In Figure 1, the answer (’Germany’) follows straightforwardly from the context (without inference), but machine reading model"
2021.emnlp-main.649,2020.coling-main.605,0,0.0413273,"ne reading systems, includes descriptions of fiction, and occasional misinformation (Rosenzweig, 2011), by design it mostly presents propositions that are consistent with our world views. Apart from an early attempt to model belief bias in argument analysis (McConachy et al., 1998), and one example of awareness of belief bias in crowdsourcing experiments (Chen et al., 2019), NLP has so far ignored belief biases.2 This is, in a way, surprising given the amount of research in other biases, including sample biases (Chaganty et al., 2017; Xu et al., 2020), reporting biases (Forbes and Choi, 2017; Shwartz and Choi, 2020), annotator biases (Geva et al., 2019), and demographic biases (Barrett et al., 2019; Meyer et al., 2020). 2 Machine Reading We briefly present the four (common, popular) machine reading models evaluated in §3 and §4: Bi-directional Attention Flow The bidirectional attention flow (BiDAF) architecture (Seo et al., 2018) comprises character and word embeddings, and uses a recurrent neural network (Hochreiter and Schmidhuber, 1997) to learn how to represent the context. A specialized attention flow layer couples query and context vectors to produce query-aware feature vectors for each word in the"
2021.emnlp-main.649,D14-1162,0,0.0888354,"ntion Flow The bidirectional attention flow (BiDAF) architecture (Seo et al., 2018) comprises character and word embeddings, and uses a recurrent neural network (Hochreiter and Schmidhuber, 1997) to learn how to represent the context. A specialized attention flow layer couples query and context vectors to produce query-aware feature vectors for each word in the context that are then passed on to an output layer. BiDAF models trained on SQuAD are known to be sensitive to syntactic and lexical ambiguities (Seo et al., 2018). I evaluate two versions of BiDAF: The simpler (BiDAF) relies on GloVe (Pennington et al., 2014) word embeddings; the slightly more sophisticated (ELMo-BiDAF) relies on ELMo contextualized embeddings (Peters et al., 2018), a log-bilinear regression model that combines the advantages of global matrix factorization and local context window methods. Transformer Question Answering This model (TransformerQA) is based on RoBERTa (Liu et al., 2019) and simply uses the architecture for SQuAD in Devlin et al. (2019). This model performs much better on SQuAD 2.0 than ELMo-BiDAF – with an error reduction of two thirds, i.e., 0.67 – but on par with ELMo-BIDAF on AUTO -L OCKE (§4). Numerically Aware"
2021.emnlp-main.649,2020.findings-emnlp.34,0,0.0165579,"main source of data for many language models and most machine reading systems, includes descriptions of fiction, and occasional misinformation (Rosenzweig, 2011), by design it mostly presents propositions that are consistent with our world views. Apart from an early attempt to model belief bias in argument analysis (McConachy et al., 1998), and one example of awareness of belief bias in crowdsourcing experiments (Chen et al., 2019), NLP has so far ignored belief biases.2 This is, in a way, surprising given the amount of research in other biases, including sample biases (Chaganty et al., 2017; Xu et al., 2020), reporting biases (Forbes and Choi, 2017; Shwartz and Choi, 2020), annotator biases (Geva et al., 2019), and demographic biases (Barrett et al., 2019; Meyer et al., 2020). 2 Machine Reading We briefly present the four (common, popular) machine reading models evaluated in §3 and §4: Bi-directional Attention Flow The bidirectional attention flow (BiDAF) architecture (Seo et al., 2018) comprises character and word embeddings, and uses a recurrent neural network (Hochreiter and Schmidhuber, 1997) to learn how to represent the context. A specialized attention flow layer couples query and context v"
2021.emnlp-main.649,N18-1202,0,0.0195562,"uses a recurrent neural network (Hochreiter and Schmidhuber, 1997) to learn how to represent the context. A specialized attention flow layer couples query and context vectors to produce query-aware feature vectors for each word in the context that are then passed on to an output layer. BiDAF models trained on SQuAD are known to be sensitive to syntactic and lexical ambiguities (Seo et al., 2018). I evaluate two versions of BiDAF: The simpler (BiDAF) relies on GloVe (Pennington et al., 2014) word embeddings; the slightly more sophisticated (ELMo-BiDAF) relies on ELMo contextualized embeddings (Peters et al., 2018), a log-bilinear regression model that combines the advantages of global matrix factorization and local context window methods. Transformer Question Answering This model (TransformerQA) is based on RoBERTa (Liu et al., 2019) and simply uses the architecture for SQuAD in Devlin et al. (2019). This model performs much better on SQuAD 2.0 than ELMo-BiDAF – with an error reduction of two thirds, i.e., 0.67 – but on par with ELMo-BIDAF on AUTO -L OCKE (§4). Numerically Aware QANet Our last model is an extension of the QANet architecture (Yu et al., 2018), presented in Dua et al. (2019). The QANet a"
2021.emnlp-main.649,W00-0603,0,0.423218,"answer context phrases were potential distractors, being distributionally similar to the answer phrase, contributed to error, but only made for a tiny fraction of the observed error. The main source of error is that the context does not align with common beliefs. 4 Discussion Machine Reading without Language Models Given the progress machine reading has seen with large-scale language models, it is hard to imagine a return to from-scratch training. Any such system would be sensitive to linguistic variation and out-ofvocabulary effects in the same way rule-based question answering systems were (Riloff and Thelen, 2000). How, then, can we build machine reading models that are less sensitive to belief biases? Obviously, we can create gold standard training data for machine reading models from fictional texts and disinformation, or we can use adversarial data augmentation techniques to create silver standard data that does not align with common beliefs. It is an open question, however, whether this is enough, or whether we need to design hybrid machine reading models that disentangle common sense reasoning and a more abstract and logical form of reasoning, in which it has no value whether our premises hold tru"
2021.emnlp-main.649,W18-2602,0,0.316895,"machine reading problem in spite of the very short context (and thereby very limited set of potential answers). See Table 2 for hand-tailored examples used as templates in AUTO -L OCKE and more real-life examples. even the simple question of What is Washington? Instead of answering ’a number’, they consistently answered ’a city’. Introduction Reading comprehension models are biased in many ways: they often expect lexical overlap between answer and question (Schlegel et al., 2020), expect the answers to occur in specific positions (Jia and Liang, 2017), or expect answers to be named entities (Rondeau and Hazen, 2018). This paper considers belief bias (Sternberg and Leighton, 2004; Anderson and Hartzler, 2014) in the context of machine reading based on language models. Belief bias is a type of cognitive bias, defined in psychology as the tendency to evaluate a statement based on prior beliefs rather than its logical strength (Evans et al., 1983). In Figure 1, the answer (’Germany’) follows straightforwardly from the context (without inference), but machine reading models nevertheless err, presumably because the prediction (’Malaysia’) aligns better with associations learned by language models. In another e"
2021.findings-acl.259,D19-1523,0,0.0378255,"Missing"
2021.findings-acl.259,N19-1062,0,0.0284057,"methods change when introducing such controls, pointing to the importance of accounting for belief bias in evaluation. 1 Figure 1: Evaluation protocols considered in this work Introduction Machine learning has become an integral part of our lives; from everyday use (e.g., search, translation, recommendations) to high-stake applications in healthcare, law, or transportation. However, its impact is controversial: neural models have been shown to make confident predictions relying on artifacts (McCoy et al., 2019; Wallace et al., 2019) and have shown to encode and amplify negative social biases (Manzini et al., 2019; Caliskan et al., 2017; May et al., 2019; Tan and Celis, 2019; Gonz´alez et al., 2020; Rudinger et al., 2018). Explainability aims to make model decisions transparent and predictable to humans; it serves as a tool for model diagnosis, detecting failure modes and biases, and more generally, to increase trust by providing transparency (Amershi et al., 2019). While automatic metrics have been proposed to evaluate various properties of explanations such as faithfulness, consistency and agreement with human explanations (Atanasova et al., 2020; Robnikˇ Sikonja and Bohanec, 2018; DeYoung et al., 20"
2021.findings-acl.259,N19-1063,0,0.0264298,", pointing to the importance of accounting for belief bias in evaluation. 1 Figure 1: Evaluation protocols considered in this work Introduction Machine learning has become an integral part of our lives; from everyday use (e.g., search, translation, recommendations) to high-stake applications in healthcare, law, or transportation. However, its impact is controversial: neural models have been shown to make confident predictions relying on artifacts (McCoy et al., 2019; Wallace et al., 2019) and have shown to encode and amplify negative social biases (Manzini et al., 2019; Caliskan et al., 2017; May et al., 2019; Tan and Celis, 2019; Gonz´alez et al., 2020; Rudinger et al., 2018). Explainability aims to make model decisions transparent and predictable to humans; it serves as a tool for model diagnosis, detecting failure modes and biases, and more generally, to increase trust by providing transparency (Amershi et al., 2019). While automatic metrics have been proposed to evaluate various properties of explanations such as faithfulness, consistency and agreement with human explanations (Atanasova et al., 2020; Robnikˇ Sikonja and Bohanec, 2018; DeYoung et al., 2020), these metrics do not inform us about"
2021.findings-acl.259,D17-1215,0,0.193288,"ntroducing a condition which can help account for belief bias effects: evaluating explainability methods on low-quality models, the predictions of which substantially differ from human beliefs. This means that in order to succeed in the task, humans cannot simply rely on their previous beliefs, therefore, helping us assess the ability of explanations in helping humans to realign their expectations of model behavior. The predictions of reading comprehension models can also be made different from human answers by introducing distractor sentences that fool machine reading models, but not humans (Jia and Liang, 2017). If in human forward prediction, participants predict the true answer rather than spans in the distractor sentences, this suggests participants may be relying on their belief biases. Best model selection. Ribeiro et al. (2016b) presented an evaluation of explainability methods for text classification, where explanations for decisions of two different models on the same instance are presented side by side, and humans decide which model is likely to generalize better. With some exceptions (Lertvittayakumjorn and Toni, 2019), there has not been much follow up work on this task, but this scenario"
2021.findings-acl.259,P19-1334,0,0.0208251,"els of varying quality and adversarial examples. We show that conclusions about the highest performing methods change when introducing such controls, pointing to the importance of accounting for belief bias in evaluation. 1 Figure 1: Evaluation protocols considered in this work Introduction Machine learning has become an integral part of our lives; from everyday use (e.g., search, translation, recommendations) to high-stake applications in healthcare, law, or transportation. However, its impact is controversial: neural models have been shown to make confident predictions relying on artifacts (McCoy et al., 2019; Wallace et al., 2019) and have shown to encode and amplify negative social biases (Manzini et al., 2019; Caliskan et al., 2017; May et al., 2019; Tan and Celis, 2019; Gonz´alez et al., 2020; Rudinger et al., 2018). Explainability aims to make model decisions transparent and predictable to humans; it serves as a tool for model diagnosis, detecting failure modes and biases, and more generally, to increase trust by providing transparency (Amershi et al., 2019). While automatic metrics have been proposed to evaluate various properties of explanations such as faithfulness, consistency and agreeme"
2021.findings-acl.259,2020.findings-emnlp.372,0,0.0143184,"3) Introducing model comparisons where relying on belief bias is not enough to obtain high performance (best model selection) 4 Experimental Setup This section introduces the general setup of the experiments, with details specific to each experimental paradigm described in section 5 and section 6. 4.1 Models We evaluate explanations produced by three BERTbased (Devlin et al., 2019) models: (a) a high performing model (H IGH): BERTbase, fine-tuned on SQuAD 2.0. This model is more aligned with human beliefs. (b) a medium performing model (M EDIUM): tinyBERT, a 6-layer distilled version of BERT (Jiao et al., 2020), fine-tuned on SQuAD 2.0. It performs about 20 F1 points below H IGH. This model somewhat aligns with human intuition, but performs significantly lower. (c) a low performing model (L OW): BERT-base, fine-tuned to always choose the first occurrence of the last word of the question. This system mimicks a rule-based system2 ; however, we evaluate gradient-based methods requiring a neural model. This model diverges significantly from human beliefs. 4.2 Data We use SQuAD 2.0 (Rajpurkar et al., 2018), a RC dataset consisting of 150k factoid question-answer pairs, with texts coming from Wikipedia ar"
2021.findings-acl.259,N18-1097,0,0.104444,"ansparency (Amershi et al., 2019). While automatic metrics have been proposed to evaluate various properties of explanations such as faithfulness, consistency and agreement with human explanations (Atanasova et al., 2020; Robnikˇ Sikonja and Bohanec, 2018; DeYoung et al., 2020), these metrics do not inform us about human interaction with explanations. Doshi-Velez and Kim (2017) suggested human forward prediction, a simulation task in which humans are given an input and an explanation, and their task is to predict the expected model output, regardless of the gold answer. Recent studies include Nguyen (2018); Lage et al. (2019); ?); Poursabzi-Sangdeh et al. (2021). Such protocols are widely used and can provide valuable insight into human understanding of explanations. However, prior work has not accounted for how humans’ prior beliefs (belief biases) interact with the evaluation; simulating model decisions becomes an easier task when the model being evaluated makes predictions which align with human expectations. We argue that not considering belief bias in such protocols may lead to misleading conclusions about which explainability methods perform best. Other protocols have evaluated participan"
2021.findings-acl.259,P18-2124,0,0.0199824,"h human beliefs. (b) a medium performing model (M EDIUM): tinyBERT, a 6-layer distilled version of BERT (Jiao et al., 2020), fine-tuned on SQuAD 2.0. It performs about 20 F1 points below H IGH. This model somewhat aligns with human intuition, but performs significantly lower. (c) a low performing model (L OW): BERT-base, fine-tuned to always choose the first occurrence of the last word of the question. This system mimicks a rule-based system2 ; however, we evaluate gradient-based methods requiring a neural model. This model diverges significantly from human beliefs. 4.2 Data We use SQuAD 2.0 (Rajpurkar et al., 2018), a RC dataset consisting of 150k factoid question-answer pairs, with texts coming from Wikipedia articles. We opt for this data as it contains short passages that can be read by humans in a short time. In the human forward prediction experiments, we refer to experiments using this data as ORIG. As described 2 This model achieves about 0.90 F1 for this task, but in the results we show its performance on the actual RC task in section 2, Wikipedia texts could by themselves induce people to rely on their belief bias, but this particular dataset allows us to also introduce controls for the bias: t"
2021.findings-acl.259,N16-3020,0,0.731205,". However, prior work has not accounted for how humans’ prior beliefs (belief biases) interact with the evaluation; simulating model decisions becomes an easier task when the model being evaluated makes predictions which align with human expectations. We argue that not considering belief bias in such protocols may lead to misleading conclusions about which explainability methods perform best. Other protocols have evaluated participant’s ability to select the best model based on explanations offered by different interpretability methods (e.g. decide which model would generalize ‘in the wild’) (Ribeiro et al., 2016a). However, comparisons have been made between a model which is clearly in line with human beliefs, and another which exploits spurious correlations diverging from human expec2930 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2930–2942 August 1–6, 2021. ©2021 Association for Computational Linguistics tations. When differences are less obvious, humans may not be able to leverage their belief biases, and conclusions may change. This paper, which includes evaluations for both of the previously mentioned tasks, closes an important gap: to the best of our knowle"
2021.findings-acl.259,D19-1221,0,0.0138765,"ty and adversarial examples. We show that conclusions about the highest performing methods change when introducing such controls, pointing to the importance of accounting for belief bias in evaluation. 1 Figure 1: Evaluation protocols considered in this work Introduction Machine learning has become an integral part of our lives; from everyday use (e.g., search, translation, recommendations) to high-stake applications in healthcare, law, or transportation. However, its impact is controversial: neural models have been shown to make confident predictions relying on artifacts (McCoy et al., 2019; Wallace et al., 2019) and have shown to encode and amplify negative social biases (Manzini et al., 2019; Caliskan et al., 2017; May et al., 2019; Tan and Celis, 2019; Gonz´alez et al., 2020; Rudinger et al., 2018). Explainability aims to make model decisions transparent and predictable to humans; it serves as a tool for model diagnosis, detecting failure modes and biases, and more generally, to increase trust by providing transparency (Amershi et al., 2019). While automatic metrics have been proposed to evaluate various properties of explanations such as faithfulness, consistency and agreement with human explanati"
2021.findings-acl.259,N18-2002,0,0.0656033,"Missing"
2021.findings-acl.284,2020.repl4nlp-1.18,0,0.0239494,"not significant (p ∼ 0.18) across our experiments. 2 Related Work Pruning neural networks The literature on pruning neural networks is decades old (Mozer and Smolensky, 1989; Cun et al., 1990; Hassibi and Stork, 1993), but has recently seen a resurgence with the all-encompassing success of neural networks and the need for small and fast on-device model inference (Han et al., 2015; Sze et al., 2017; Frankle and Carbin, 2018; Frankle et al., 2019). In NLP, specifically, pruning methods have been applied to recurrent neural networks (Desai et al., 2019; Yu et al., 2020), as well as transformers (Gordon et al., 2020; Brix et al., 2020; Prasanna et al., 2020; Chen et al., 2020; Sanh et al., 2020). Fairness in pruned models Measuring fairness in pruned models is an unexplored area. However, Paganini (2020) evaluates the fairness, i.e., the difference between the best- and worst-case groups, of lottery ticket-style weight pruning for digit recognition problems: Specifically, they retrain models for a fixed number of iterations using global unstructured pruning. In addition, they present a meta-regression study suggesting that underrepresented and more complex classes are most severely affected by pruning pr"
2021.findings-acl.284,2020.coling-main.92,0,0.0609336,"Missing"
2021.findings-acl.284,2020.emnlp-main.259,0,0.0227171,"periments. 2 Related Work Pruning neural networks The literature on pruning neural networks is decades old (Mozer and Smolensky, 1989; Cun et al., 1990; Hassibi and Stork, 1993), but has recently seen a resurgence with the all-encompassing success of neural networks and the need for small and fast on-device model inference (Han et al., 2015; Sze et al., 2017; Frankle and Carbin, 2018; Frankle et al., 2019). In NLP, specifically, pruning methods have been applied to recurrent neural networks (Desai et al., 2019; Yu et al., 2020), as well as transformers (Gordon et al., 2020; Brix et al., 2020; Prasanna et al., 2020; Chen et al., 2020; Sanh et al., 2020). Fairness in pruned models Measuring fairness in pruned models is an unexplored area. However, Paganini (2020) evaluates the fairness, i.e., the difference between the best- and worst-case groups, of lottery ticket-style weight pruning for digit recognition problems: Specifically, they retrain models for a fixed number of iterations using global unstructured pruning. In addition, they present a meta-regression study suggesting that underrepresented and more complex classes are most severely affected by pruning procedures. See Hooker et al. (2020) for rel"
2021.findings-acl.284,P13-2113,1,0.787481,"ning, but results are not consistent. The code for our experiments is available at https://github. com/vpetren/fairness_lottery. 1 Figure 1: Fairness Sensitivity to Pruning (FSP): the gradient of the linear fit of (the logarithm of) the pruning ratio to min-max group-level disparity. We use this to quantify the sensitivity of Rawlsian min-max fairness to weight pruning across architectures, pruning strategies and datasets. evidence rather than the most prominent independent variables. Sparse models do not have that luxury and are therefore more sensitive to shifts (Globerson and Roweis, 2006; Søgaard, 2013). Introduction Heavily pruning deep neural network models is a way of reducing inference cost for resourceconstrained environments, but does weight-pruning of deep neural networks increase their unfairness? Several recent papers suggest this (Paganini, 2020; Hooker et al., 2020), based on experiments from face and digit recognition, but does this also hold for natural language processing (NLP) models? Systematic biases may easily be exacerbated by pruning interventions in high-dimensional problems because of feature swamping effects (Sutton et al., 2006). Overparameterized deep neural networks"
2021.findings-acl.284,N06-1012,0,0.102999,"sitive to shifts (Globerson and Roweis, 2006; Søgaard, 2013). Introduction Heavily pruning deep neural network models is a way of reducing inference cost for resourceconstrained environments, but does weight-pruning of deep neural networks increase their unfairness? Several recent papers suggest this (Paganini, 2020; Hooker et al., 2020), based on experiments from face and digit recognition, but does this also hold for natural language processing (NLP) models? Systematic biases may easily be exacerbated by pruning interventions in high-dimensional problems because of feature swamping effects (Sutton et al., 2006). Overparameterized deep neural networks generalize well, in part because they can hedge their bets and rely on multitudes of weak We introduce a fairness sensitivity to pruning metric that measures how Rawlsian min-max fairness across demographic groups changes with weight pruning. We estimate this sensitivity by taking the gradient of the linear fit of the logarithm of the pruning ratio to min-max group-level disparity. We show that across four datasets, fairness sensitivity to pruning is similar for layer-wise and global pruning strategies (Frankle and Carbin, 2018), as well as for text cla"
2021.findings-acl.429,W99-0212,0,0.224141,"IC bias also exposes a limitation of theirs: while these models are advanced enough to use IC bias for their predictions, their interpretation of semantics is still fairly shallow. The lower-order signal coming from lexical semantics is given priority over the higher-order signal coming from the sentencelevel semantics. In the experiment presented in this section, this leads to a higher error rate on resolving pronoun antecedents in incongruent contexts, with potential impact on tasks that depend on co-reference resolution, e.g. document summarization (Azzam et al., 1999), question answering (Morton, 1999; Vicedo and Ferr´andez, 2000), and information extraction (Zelenko et al., 2004). 7 Conclusion From the comparison of six competitive PLMs, BERT , R o BERT a- L , GPT , GPT 2- M , ELECTRA and ELECTRA - L, we conclude that PLMs can exhibit IC bias much like humans do, but that different models do so to a different degree, with bidirectional models showing moderate to strong correlation to human judgements, and unidirectional models showing only a weak correlation. This ability of some PLMs has the unfortunate effect that it makes them prone to higher error rates in contexts of incongruent IC a"
2021.findings-acl.429,2020.emnlp-main.376,0,0.0339887,"from PLMs feed into systems for various complex tasks, typically improving performance. Many of the testing paradigms used in psycholinguistics lend themselves well to LM analysis as they rely on a textual stimulus and a lexical response. Linzen et al. (2016) were first to borrow from the psycholinguistic testing paradigm, in a study of the capabilities of LSTM-based models to resolve subject-verb number agreement. Goldberg (2019) adopted the psycholinguistic approach in an assessment of BERT (Devlin et al., 2019) on a number of syntactic tasks and found it to perform remarkably well on all. Hawkins et al. (2020) studied the ability of different LMs to capture human preferences as to the argument structure of English verbs. The analysis of semantic capabilities in LMs includes studies on negative polarity in LSTM LMs (Marvin and Linzen, 2018; Jumelet and Hupkes, 2018), reasoning based on higher-order linguistic skill (Talmor et al., 2019), arithmetic and compositional semantics (Stali¯unait˙e and Iacobacci, 2020), stereotypic tacit assumptions and lexical priming (Misra et al., 2020; Weir et al., 2020). Many of these studies look at recent PLMs and draw mixed conclusions about the level of semantics e"
2021.findings-acl.429,W18-5424,0,0.0166111,"016) were first to borrow from the psycholinguistic testing paradigm, in a study of the capabilities of LSTM-based models to resolve subject-verb number agreement. Goldberg (2019) adopted the psycholinguistic approach in an assessment of BERT (Devlin et al., 2019) on a number of syntactic tasks and found it to perform remarkably well on all. Hawkins et al. (2020) studied the ability of different LMs to capture human preferences as to the argument structure of English verbs. The analysis of semantic capabilities in LMs includes studies on negative polarity in LSTM LMs (Marvin and Linzen, 2018; Jumelet and Hupkes, 2018), reasoning based on higher-order linguistic skill (Talmor et al., 2019), arithmetic and compositional semantics (Stali¯unait˙e and Iacobacci, 2020), stereotypic tacit assumptions and lexical priming (Misra et al., 2020; Weir et al., 2020). Many of these studies look at recent PLMs and draw mixed conclusions about the level of semantics encoded by these models. Peters et al. (2018) and Tenney et al. (2019) observed that PLMs do encode some higher-order syntactic abstractions in the higher layers (whereas lower-order syntactic information is encoded in the lower layers). However, in a compariso"
2021.findings-acl.429,Q16-1037,0,0.0337067,"al conclusions about the shortcomings of language models, which seem to prioritise a low-level lexical pattern (when they are aware of it in the first place) over a higher-order contextual signal. 2 Related Work The study of the linguistic capacities of neural language models (LMs) has become especially relevant in current NLP research, where representations from PLMs feed into systems for various complex tasks, typically improving performance. Many of the testing paradigms used in psycholinguistics lend themselves well to LM analysis as they rely on a textual stimulus and a lexical response. Linzen et al. (2016) were first to borrow from the psycholinguistic testing paradigm, in a study of the capabilities of LSTM-based models to resolve subject-verb number agreement. Goldberg (2019) adopted the psycholinguistic approach in an assessment of BERT (Devlin et al., 2019) on a number of syntactic tasks and found it to perform remarkably well on all. Hawkins et al. (2020) studied the ability of different LMs to capture human preferences as to the argument structure of English verbs. The analysis of semantic capabilities in LMs includes studies on negative polarity in LSTM LMs (Marvin and Linzen, 2018; Jume"
2021.findings-acl.429,2021.ccl-1.108,0,0.0250799,"Missing"
2021.findings-acl.429,D18-1179,0,0.0212064,"different LMs to capture human preferences as to the argument structure of English verbs. The analysis of semantic capabilities in LMs includes studies on negative polarity in LSTM LMs (Marvin and Linzen, 2018; Jumelet and Hupkes, 2018), reasoning based on higher-order linguistic skill (Talmor et al., 2019), arithmetic and compositional semantics (Stali¯unait˙e and Iacobacci, 2020), stereotypic tacit assumptions and lexical priming (Misra et al., 2020; Weir et al., 2020). Many of these studies look at recent PLMs and draw mixed conclusions about the level of semantics encoded by these models. Peters et al. (2018) and Tenney et al. (2019) observed that PLMs do encode some higher-order syntactic abstractions in the higher layers (whereas lower-order syntactic information is encoded in the lower layers). However, in a comparison of contextualized and static word embeddings, Tenney et al. (2019) concluded that PLMs do not generally offer the same improvement with respect to semantics as they do for syntax. At the crossroad of semantic analysis and psycholinguistic approaches, Ettinger (2020) introduced a suite of six psycholinguistic diagnostics for the analysis of semantic awareness in LMs. The tasks wer"
2021.findings-acl.429,W19-6204,0,0.0257671,"Missing"
2021.findings-acl.429,2020.emnlp-main.573,0,0.0190633,"Missing"
2021.findings-acl.429,P19-1452,0,0.0389224,"human preferences as to the argument structure of English verbs. The analysis of semantic capabilities in LMs includes studies on negative polarity in LSTM LMs (Marvin and Linzen, 2018; Jumelet and Hupkes, 2018), reasoning based on higher-order linguistic skill (Talmor et al., 2019), arithmetic and compositional semantics (Stali¯unait˙e and Iacobacci, 2020), stereotypic tacit assumptions and lexical priming (Misra et al., 2020; Weir et al., 2020). Many of these studies look at recent PLMs and draw mixed conclusions about the level of semantics encoded by these models. Peters et al. (2018) and Tenney et al. (2019) observed that PLMs do encode some higher-order syntactic abstractions in the higher layers (whereas lower-order syntactic information is encoded in the lower layers). However, in a comparison of contextualized and static word embeddings, Tenney et al. (2019) concluded that PLMs do not generally offer the same improvement with respect to semantics as they do for syntax. At the crossroad of semantic analysis and psycholinguistic approaches, Ettinger (2020) introduced a suite of six psycholinguistic diagnostics for the analysis of semantic awareness in LMs. The tasks were selected based on a spe"
2021.findings-acl.429,D18-1151,0,0.0208563,"esponse. Linzen et al. (2016) were first to borrow from the psycholinguistic testing paradigm, in a study of the capabilities of LSTM-based models to resolve subject-verb number agreement. Goldberg (2019) adopted the psycholinguistic approach in an assessment of BERT (Devlin et al., 2019) on a number of syntactic tasks and found it to perform remarkably well on all. Hawkins et al. (2020) studied the ability of different LMs to capture human preferences as to the argument structure of English verbs. The analysis of semantic capabilities in LMs includes studies on negative polarity in LSTM LMs (Marvin and Linzen, 2018; Jumelet and Hupkes, 2018), reasoning based on higher-order linguistic skill (Talmor et al., 2019), arithmetic and compositional semantics (Stali¯unait˙e and Iacobacci, 2020), stereotypic tacit assumptions and lexical priming (Misra et al., 2020; Weir et al., 2020). Many of these studies look at recent PLMs and draw mixed conclusions about the level of semantics encoded by these models. Peters et al. (2018) and Tenney et al. (2019) observed that PLMs do encode some higher-order syntactic abstractions in the higher layers (whereas lower-order syntactic information is encoded in the lower layer"
2021.findings-acl.429,2020.emnlp-main.15,0,0.0109757,"differences ensue. We extract ‘decontextualized’ verb representation from the PLMs following the procedure described in Appendix G. Using those, we carry out two types of probes: an extrinsic one, where we train a linear regression model (LR) to map from a verb’s representation to its IC bias; and an intrinsic one, where we use linear discriminant analysis (LDA) to identify the single dimension in the verb representations that is most informative of IC bias.5 The benefit of the latter approach is that it does not add any newly trained parameters to the computation of the correlation (Torroba Hennigen et al., 2020). In both cases, the result is a vector of scalars (the values predicted by the LR, or the values of the selected dimension)—we measure the correlation between these values and human IC bias to determine how much of the latter can be recovered from the representations. To reduce overfitting, which is inevitable with 5 As LDA operates over a space of discrete labels, we convert the IC bias scores into 3 classes (&gt; 0, < 0, = 0). 4864 German mBERT BERT Figure 3: Ratio of S-bias and O-bias verbs in German (top) and Spanish (bottom). 305 datapoints in total and representations of 768 to 1024 dimens"
2021.findings-acl.429,2020.findings-emnlp.415,0,0.0170096,"sessment of BERT (Devlin et al., 2019) on a number of syntactic tasks and found it to perform remarkably well on all. Hawkins et al. (2020) studied the ability of different LMs to capture human preferences as to the argument structure of English verbs. The analysis of semantic capabilities in LMs includes studies on negative polarity in LSTM LMs (Marvin and Linzen, 2018; Jumelet and Hupkes, 2018), reasoning based on higher-order linguistic skill (Talmor et al., 2019), arithmetic and compositional semantics (Stali¯unait˙e and Iacobacci, 2020), stereotypic tacit assumptions and lexical priming (Misra et al., 2020; Weir et al., 2020). Many of these studies look at recent PLMs and draw mixed conclusions about the level of semantics encoded by these models. Peters et al. (2018) and Tenney et al. (2019) observed that PLMs do encode some higher-order syntactic abstractions in the higher layers (whereas lower-order syntactic information is encoded in the lower layers). However, in a comparison of contextualized and static word embeddings, Tenney et al. (2019) concluded that PLMs do not generally offer the same improvement with respect to semantics as they do for syntax. At the crossroad of semantic analysis"
2021.findings-acl.429,2020.emnlp-main.70,0,0.0126533,"est prompt. Ettinger (2020) suggests that LMs might be “tripped up” in such contexts if they are unable to accurately integrate all the available information—she indeed found that to be the case for role-based event prediction in BERT (Devlin et al., 2019), for example. The phenomenon we study, incongruency in causality signals, has been observed to trigger a similar response in humans (Van Berkum et al., 2007) and can thus be expected to also “trip up” LMs. Implicit causality bias was previously considered in PLM analysis by two works, both looking at how well unidirectional PLMs capture it. Upadhye et al. (2020) studied IC from the perspective of how different connectives between the main clause and the following clause (because, and as a result, full stop) affect the strength of the bias. While they did not find strong evidence for a correlation to human-based results in this respect, they did observe that in the context of connective because PLMs assigned lower probability to subjectreferring pronouns for an object-biasing verb as compared to a subject-biasing verb. Davis and van Schijndel (2020) observed that GPT2-XL (Radford et al., 2019) encodes some level of IC bias in its representations (meas"
2021.findings-acl.429,P00-1070,0,0.470351,"Missing"
2021.findings-acl.429,2020.emnlp-main.586,0,0.0455989,"Missing"
2021.findings-acl.429,W04-0704,0,0.0418804,"nced enough to use IC bias for their predictions, their interpretation of semantics is still fairly shallow. The lower-order signal coming from lexical semantics is given priority over the higher-order signal coming from the sentencelevel semantics. In the experiment presented in this section, this leads to a higher error rate on resolving pronoun antecedents in incongruent contexts, with potential impact on tasks that depend on co-reference resolution, e.g. document summarization (Azzam et al., 1999), question answering (Morton, 1999; Vicedo and Ferr´andez, 2000), and information extraction (Zelenko et al., 2004). 7 Conclusion From the comparison of six competitive PLMs, BERT , R o BERT a- L , GPT , GPT 2- M , ELECTRA and ELECTRA - L, we conclude that PLMs can exhibit IC bias much like humans do, but that different models do so to a different degree, with bidirectional models showing moderate to strong correlation to human judgements, and unidirectional models showing only a weak correlation. This ability of some PLMs has the unfortunate effect that it makes them prone to higher error rates in contexts of incongruent IC and EC signals, where the PLMs overly rely on IC bias. This finding adds to a grow"
2021.louhi-1.2,2020.acl-main.421,0,0.0344587,"nerated from that word receives the correct label. Pretrained Multilingual Encoder As pre-trained multilingual encoder, we use multilingual BERT, a transformer-based language model pre-trained with a masked language modeling and next sentence prediction objective on Wikipedia text in multiple languages.14 The encoder can easily be adapted to downstream classification tasks by putting a randomly initialized classification layer on top of the pre-trained encoder.15 mBERT has shown cross-lingual transfer abilities when finetuned on target data as well as in zero-shot setups (Wu and Dredze, 2019; Artetxe et al., 2020). Following the standard input scheme (Devlin et al., 2019), for all fine-tuning tasks, we add a special [CLS] token serving as an aggregated senSequence Classification For the sequence classification task, we use the [CLS] token as aggregated sentence representation and assign a label to the whole sequence by feeding the encoded [CLS] token representation through the output classification layer. 16 Khandelwal and Sawant (2020) found this method to work better than replacing the negation cue by the special token. Sergeeva et al. (2019) suggest to add a special cue embedding to the subtoken emb"
2021.louhi-1.2,2020.cl-1.5,0,0.0288974,"Missing"
2021.louhi-1.2,N19-1423,0,0.158165,"ld a negation resolution model that works on data in multiple languages. Negation scope resolution has also been considered for non-clinical text (Morante and Sporleder, 2012; Morante and Blanco, 2012), and several datasets spanning various domains and including a small amount of non-English languages are available (see Section 4). In this work, we investigate if and how these disparate data sources can serve as training resource to resolve negation scope in multiple languages in the clinical domain in a zeroshot setup. To enable transfer across languages we rely on multilingual BERT (mBERT) (Devlin et al., 2019), a multilingual pre-trained language encoder that has proven capable of zero-shot cross-lingual transfer in other tasks3 (Wu and Dredze, 2019), and has recently been applied for zero-shot transfer to French clinical text (Shaitarova et al., 2020). One challenge arising from the available scope resolution datasets is that they are annotated according to different annotation schemes (see Section 3.3), raising the question if and how they can be combined as a training resource (Barnes et al., 2020; Jim´enez-Zafra et al., 2020). We explore two strategies to handle disparities between the resource"
2021.louhi-1.2,2020.acl-main.698,0,0.041873,"Missing"
2021.louhi-1.2,W17-1808,0,0.046111,"Missing"
2021.louhi-1.2,2020.lrec-1.704,0,0.344469,"edical and clinical domains (Lee et al., 2020; Peng et al., 2019; Alsentzer et al., 2019) now produce state-of-the-art results for several downstream tasks, such language models do not sufficiently capture the semantics of negation (Ettinger, 2020; Kassner and Sch¨utze, 2020). One way of accessing negation information is to explicitly detect all negated words in a sentence, a task which is referred to as negation scope resolution (Morante et al., 2008). The task has recently been successfully approached by fine-tuning a pre-trained language model on labeled target data (Sergeeva et al., 2019; Khandelwal and Sawant, 2020). For the clinical domain however, we cannot rely on multilingual labeled target data to be available. This is partly due to data privacy issues, which lead to few publicly available datasets in the clinical domain (Chapman et al., 2011; Velupillai et al., 2018) 1 , and partly due to the fact that other languages are underrepresented compared to English in clinical NLP (N´ev´eol et al., 2018).2 In summary, when building a multilingual negation scope resolution system for clinical text, we are facing a lack of training data. Our approach is hence to use the available data as best as possible to"
2021.louhi-1.2,konstantinova-etal-2012-review,0,0.0493972,"Missing"
2021.louhi-1.2,2020.tacl-1.3,0,0.0212135,"ing (ML) models can improve performance for relation extraction (Chowdhury and Lavelli, 2013) and more general NLP tasks such as sentiment analysis (Barnes et al., 2020) and machine translation (Fancellu and Webber, 2014), which become increasingly popular for the clinical domain (Denecke and Deng, 2015). Even though pre-trained language models tailored to the biomedical and clinical domains (Lee et al., 2020; Peng et al., 2019; Alsentzer et al., 2019) now produce state-of-the-art results for several downstream tasks, such language models do not sufficiently capture the semantics of negation (Ettinger, 2020; Kassner and Sch¨utze, 2020). One way of accessing negation information is to explicitly detect all negated words in a sentence, a task which is referred to as negation scope resolution (Morante et al., 2008). The task has recently been successfully approached by fine-tuning a pre-trained language model on labeled target data (Sergeeva et al., 2019; Khandelwal and Sawant, 2020). For the clinical domain however, we cannot rely on multilingual labeled target data to be available. This is partly due to data privacy issues, which lead to few publicly available datasets in the clinical domain (Cha"
2021.louhi-1.2,W10-3108,0,0.0258092,"for clinical and biomedical text, as there is large interest in identifying negated concepts (Mutalik et al., 2001; Chapman et al., 2001; Mowery et al., 2012), e.g. negated medical events (Nawaz et al., 2013) or negated drug-drug interactions (Bokharaeian et al., 2016). Absence of symptoms or the fact that chemical re7 Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis, pages 7–18 April 19, 2021. ©2021 Association for Computational Linguistics actions are not observable are crucial knowledge in the clinical and biomedical domain (Elkin et al., 2005; Krallinger, 2010). This is reflected in Figure 1, which shows the percentage of negated sentences in clinical compared to non-clinical text. Explicitly integrating negation information into machine learning (ML) models can improve performance for relation extraction (Chowdhury and Lavelli, 2013) and more general NLP tasks such as sentiment analysis (Barnes et al., 2020) and machine translation (Fancellu and Webber, 2014), which become increasingly popular for the clinical domain (Denecke and Deng, 2015). Even though pre-trained language models tailored to the biomedical and clinical domains (Lee et al., 2020;"
2021.louhi-1.2,2020.iwpt-1.3,0,0.0129938,"if the concepts of interest are contained in the scope or not. With this setup, a system can be re-used across concepts, and does not have to be re-trained for a different downstream task. Also, as negation is inherent in text from any domain, this approach opens the possibility to learn to resolve negation scope in other domains, and transfer this knowledge to clinical text. new state-of-the-art results on the BIOSCOPE and SHERLOCK datasets. Barnes et al. (2020) deviate from the two-step sequence labeling approach and propose a sequence labeling model that detects cues and scope in one step. Kurtz et al. (2020) show that it is beneficial to frame the negation scope resolution task as a graph parsing problem. All the works listed above focus exclusively on English data, and only few works attempt to do multi- or cross-lingual negation scope resolution. Fancellu et al. (2018) were the first to present a zero-shot approach for negation scope resolution. They find that transfer from English to a Chinese version of the SHERLOCK corpus is possible, using an LSTM and a graph convolutional network (GCN) in combination with static crosslingual word embeddings. However, both their models rely on PoS-tags and"
2021.louhi-1.2,E17-2010,0,0.0124938,"(MTLn ) or with and additional event detection task (MTLn+e ). Best performance in the ZS setup is marked in gray. 5.2 Evaluation F1-score across the validation splits of the three clinical datasets. Validation scores for other dataset combinations can be found in our code repository. As baselines, we report in-domain performance of mBERT , which provides an upper baseline indicating the performance gain if annotated target data was available. We also include a punctuation baseline, as previous work has found that many test sentences are easy to label from cue token to next punctuation mark (Fancellu et al., 2017; Sergeeva et al., 2019). The punctuation baseline labels all tokens following the negation cue until the next punctuation mark18 as in scope. In all experiments, mBERT is fine-tuned with the default hyperparameters for sequence labeling tasks implemented in MT-DNN, which means that we use the adamax optimizer with a learning rate of 5e-5 and a batch size of 8. The maximum sequence length is set to 512.19 All models are trained for a maximum of 50 epochs, and the best model is selected according to F1-score on the BIOSCOPE validation split. We report two widely used evaluation metrics for nega"
2021.louhi-1.2,E14-1063,0,0.0292885,"ormation Analysis, pages 7–18 April 19, 2021. ©2021 Association for Computational Linguistics actions are not observable are crucial knowledge in the clinical and biomedical domain (Elkin et al., 2005; Krallinger, 2010). This is reflected in Figure 1, which shows the percentage of negated sentences in clinical compared to non-clinical text. Explicitly integrating negation information into machine learning (ML) models can improve performance for relation extraction (Chowdhury and Lavelli, 2013) and more general NLP tasks such as sentiment analysis (Barnes et al., 2020) and machine translation (Fancellu and Webber, 2014), which become increasingly popular for the clinical domain (Denecke and Deng, 2015). Even though pre-trained language models tailored to the biomedical and clinical domains (Lee et al., 2020; Peng et al., 2019; Alsentzer et al., 2019) now produce state-of-the-art results for several downstream tasks, such language models do not sufficiently capture the semantics of negation (Ettinger, 2020; Kassner and Sch¨utze, 2020). One way of accessing negation information is to explicitly detect all negated words in a sentence, a task which is referred to as negation scope resolution (Morante et al., 200"
2021.louhi-1.2,P19-1441,0,0.0215223,"ation information in datasets associated with tasks other than negation scope resolution, e.g. negated event detection. We use an MTL model with hard parameter sharing (Caruana, 1997; Collobert et al., 2011), i.e. all model parameters except for the task specific output layers are shared between the different tasks. The model is shown in Figure 4. Using multilingual BERT as shared multilingual encoder enables the model to learn from datasets in different languages, and to do zero-shot cross-lingual transfer at inference time. In particular, we run our experiments using the MT-DNN framework12 (Liu et al., 2019). Sherlock The SHERLOCK corpus (Morante and Daelemans, 2012) comprises two English Conan Doyle short stories. It was used in the 2012 Shared Task on negation scope resolution and is the most popular benchmark corpus. Here, syntactic, lexical and morphological negation cues are annotated. DDI The DDI corpus (Bokharaeian et al., 2014) is a collection of texts from the chemical and pharmaceutical database DrugBank, and English Medline abstracts. Sampling During training, we sample batches from a task i according to pi ∝ Ni , where Ni is the size of the dataset associated with task i.We implement"
2021.louhi-1.2,2020.lrec-1.708,0,0.0305228,"Missing"
2021.louhi-1.2,W19-5006,0,0.027712,". This is reflected in Figure 1, which shows the percentage of negated sentences in clinical compared to non-clinical text. Explicitly integrating negation information into machine learning (ML) models can improve performance for relation extraction (Chowdhury and Lavelli, 2013) and more general NLP tasks such as sentiment analysis (Barnes et al., 2020) and machine translation (Fancellu and Webber, 2014), which become increasingly popular for the clinical domain (Denecke and Deng, 2015). Even though pre-trained language models tailored to the biomedical and clinical domains (Lee et al., 2020; Peng et al., 2019; Alsentzer et al., 2019) now produce state-of-the-art results for several downstream tasks, such language models do not sufficiently capture the semantics of negation (Ettinger, 2020; Kassner and Sch¨utze, 2020). One way of accessing negation information is to explicitly detect all negated words in a sentence, a task which is referred to as negation scope resolution (Morante et al., 2008). The task has recently been successfully approached by fine-tuning a pre-trained language model on labeled target data (Sergeeva et al., 2019; Khandelwal and Sawant, 2020). For the clinical domain however, w"
2021.louhi-1.2,W11-4207,0,0.160194,"Missing"
2021.louhi-1.2,W17-1807,0,0.0229067,"s (Dalloux et al., 2019) is a collection of clinical trial protocols in French which were obtained from the registry of the Gustave Roussy hospital as well as the French National Cancer Institute. The corpus includes text parts about the patient inclusion criteria and the description of the procedure of the trials. Here, syntactic and lexical negation cues are annotated. In this corpus, there is no marked association between a negation cue and its scope, hence sentences with different negation cues have to be processed at once. Clinical Negation Scope Resolution Datasets IULA The IULA corpus (Marimon et al., 2017) is a collection of Spanish clinical records from several services of one of the main hospitals in Barcelona (Spain) (Marimon et al., 2017) and includes text from five sections of the electronic record: physical exploration, evolution, radiology, current process, and comparative explorations. Here, syntactic and lexical negation cues are annotated. Subjects are (almost) always excluded from 4.2 Other Negation Scope Resolution Datasets Bioscope The BIOSCOPE corpus (Vincze et al., 2008) is a collection of English biomedical and clinical texts and consists of three parts: abstracts 11 and full pa"
2021.louhi-1.2,S12-1035,0,0.177257,"le datasets in the clinical domain (Chapman et al., 2011; Velupillai et al., 2018) 1 , and partly due to the fact that other languages are underrepresented compared to English in clinical NLP (N´ev´eol et al., 2018).2 In summary, when building a multilingual negation scope resolution system for clinical text, we are facing a lack of training data. Our approach is hence to use the available data as best as possible to build a negation resolution model that works on data in multiple languages. Negation scope resolution has also been considered for non-clinical text (Morante and Sporleder, 2012; Morante and Blanco, 2012), and several datasets spanning various domains and including a small amount of non-English languages are available (see Section 4). In this work, we investigate if and how these disparate data sources can serve as training resource to resolve negation scope in multiple languages in the clinical domain in a zeroshot setup. To enable transfer across languages we rely on multilingual BERT (mBERT) (Devlin et al., 2019), a multilingual pre-trained language encoder that has proven capable of zero-shot cross-lingual transfer in other tasks3 (Wu and Dredze, 2019), and has recently been applied for ze"
2021.louhi-1.2,morante-daelemans-2012-conandoyle,0,0.0226676,"Missing"
2021.louhi-1.2,D08-1075,0,0.31185,"u and Webber, 2014), which become increasingly popular for the clinical domain (Denecke and Deng, 2015). Even though pre-trained language models tailored to the biomedical and clinical domains (Lee et al., 2020; Peng et al., 2019; Alsentzer et al., 2019) now produce state-of-the-art results for several downstream tasks, such language models do not sufficiently capture the semantics of negation (Ettinger, 2020; Kassner and Sch¨utze, 2020). One way of accessing negation information is to explicitly detect all negated words in a sentence, a task which is referred to as negation scope resolution (Morante et al., 2008). The task has recently been successfully approached by fine-tuning a pre-trained language model on labeled target data (Sergeeva et al., 2019; Khandelwal and Sawant, 2020). For the clinical domain however, we cannot rely on multilingual labeled target data to be available. This is partly due to data privacy issues, which lead to few publicly available datasets in the clinical domain (Chapman et al., 2011; Velupillai et al., 2018) 1 , and partly due to the fact that other languages are underrepresented compared to English in clinical NLP (N´ev´eol et al., 2018).2 In summary, when building a mu"
2021.louhi-1.2,J12-2001,0,0.0272898,"lead to few publicly available datasets in the clinical domain (Chapman et al., 2011; Velupillai et al., 2018) 1 , and partly due to the fact that other languages are underrepresented compared to English in clinical NLP (N´ev´eol et al., 2018).2 In summary, when building a multilingual negation scope resolution system for clinical text, we are facing a lack of training data. Our approach is hence to use the available data as best as possible to build a negation resolution model that works on data in multiple languages. Negation scope resolution has also been considered for non-clinical text (Morante and Sporleder, 2012; Morante and Blanco, 2012), and several datasets spanning various domains and including a small amount of non-English languages are available (see Section 4). In this work, we investigate if and how these disparate data sources can serve as training resource to resolve negation scope in multiple languages in the clinical domain in a zeroshot setup. To enable transfer across languages we rely on multilingual BERT (mBERT) (Devlin et al., 2019), a multilingual pre-trained language encoder that has proven capable of zero-shot cross-lingual transfer in other tasks3 (Wu and Dredze, 2019), and has r"
2021.louhi-1.2,W12-3806,0,0.0201335,"s a Zero-shot Setup We assume no training data for the clinical datasets and test in a zero-shot setup, after converting all annotations according to the steps described in Section 3.3. During training, we use per-token F1-scores on the development split of the BIOSCOPE corpus for model selection. To identify the combination of training datasets that are expected to perform best on the target datasets, 10 books, cars, computers, cookware, hotels, films, music, and phones 11 Difference and similarities between the tasks of negated event detection and negation scope resolution are discussed in (Stenetorp et al., 2012) 12 https://github.com/namisan/mt-dnn Following Stickland and Murray (2019) we set f = 0.8. We set E to the number of training epochs. 13 12 C ... O I I I I ... S C O ... Ouput Layer Task 1 I I I I S ... 1 Ouput Layer Task 2 Ouput Layer Task 3 Shared Encoder Multilingual BERT Shared Embedding Layer Task 1 [CLS] ... am Task 2 [CLS] ... find Task 3 [CLS] ... indicate [CUE] not [CUE] no the happy with reaction absence ... [SEP] between ... of @CONCEPT$ [SEP] [SEP] Figure 4: MTL model with a shared multilingual encoder for multilingual negation scope resolution. The multilingual embedding and enco"
2021.louhi-1.2,W12-2407,0,0.0776636,"Missing"
2021.louhi-1.2,W08-0606,0,0.202037,"Missing"
2021.louhi-1.2,D19-1077,0,0.128361,"xt (Morante and Sporleder, 2012; Morante and Blanco, 2012), and several datasets spanning various domains and including a small amount of non-English languages are available (see Section 4). In this work, we investigate if and how these disparate data sources can serve as training resource to resolve negation scope in multiple languages in the clinical domain in a zeroshot setup. To enable transfer across languages we rely on multilingual BERT (mBERT) (Devlin et al., 2019), a multilingual pre-trained language encoder that has proven capable of zero-shot cross-lingual transfer in other tasks3 (Wu and Dredze, 2019), and has recently been applied for zero-shot transfer to French clinical text (Shaitarova et al., 2020). One challenge arising from the available scope resolution datasets is that they are annotated according to different annotation schemes (see Section 3.3), raising the question if and how they can be combined as a training resource (Barnes et al., 2020; Jim´enez-Zafra et al., 2020). We explore two strategies to handle disparities between the resources, a simple concatenation of datasets after a partial conversion of annotations, and a multi-task learning (MTL) setup, where each dataset is h"
2021.mrl-1.3,P18-1073,0,0.0238576,"ations.5 To this end, we create a sparse graph containing for each subword, the top-k translations in each language, i.e., the top-k nearest neighbors in the projected space. We x3 x1 x2 x4 In Figure 4, we plot the average number of languages per clique, across different clique sizes. We see that most small cliques contain translation pairs, not merely synonym pairs in the same language. 3 https://github.com/codogogo/xling-eval Glavas et al. (2019) showed that by applying a bootstrapping algorithm it is possible to obtain similar performances with seed dictionaries containing only 1000 pairs. Artetxe et al. (2018) also discuss inducing dictionary seeds in a completely unsupervised manner. 5 We ran early experiments to verify that simple k-means clustering performs poorly. 4 Graph Partitioning In order to obtain final clusters of similar size, we apply the METIS algorithm on the sparse translation graph. The METIS algorithm was first introduced in 1995 (Karypis and 34 Figure 4: Average number of different languages in a clique, across different clique sizes Figure 5: Distribution of the sizes of the groups in the partition obtained with the (28500, 1.5) METIS algorithm. Most groups have size 8, close to"
2021.mrl-1.3,2020.emnlp-main.367,0,0.0326766,"ion is only partial, with some language-specific partitioning of the representational space (Libovický et al., 2019; Singh et al., 2019; Pires et al., 2019). Moreover, mBERT relies on a single, multilingual tokenizer to create its vocabulary (WordPiece, Schuster and Nakajima (2012)), a subword tokenizer based on an algorithm similar to BPE (Gage, 1994). The multilingual tokenizer lets subwords be shared across different languages, but the approach penalizes outlier languages, whose words end up being segmented according to statistics derived from high-resource languages (Pyysalo et al., 2020; Chung et al., 2020). In this study, we propose a novel method for building a multilingual vocabulary that is more fair to outlier languages: The core idea is to train mBERT-like language models on multilingual clusters of monolingual tokens derived from monolingual tokenizations into vocabularies of equal size. Related work Pyysalo et al. (2020) showed how training monolingual BERT models instead of a single multilingual one can improve performance for some languages even if the amount of training data decreases. A possible cause of this phenomenon could be identified in the better quality of tokenization that c"
2021.mrl-1.3,2020.acl-main.747,0,0.16228,"anguages: The core idea is to train mBERT-like language models on multilingual clusters of monolingual tokens derived from monolingual tokenizations into vocabularies of equal size. Related work Pyysalo et al. (2020) showed how training monolingual BERT models instead of a single multilingual one can improve performance for some languages even if the amount of training data decreases. A possible cause of this phenomenon could be identified in the better quality of tokenization that can be reached when a dedicated monolingual vocabulary is used. This thesis is also supported by the findings of Conneau et al. (2020) on the importance of vocabulary size to a multilingual language model’s performance. In order to overcome the issues deriving from the use of a single multilingual vocabulary, Chung et al. (2020) propose to cluster together similar languages and to learn vocabularies for language clusters. Their methods proves to be effective, with a significant reduction of maximum description length, and significant improvements on QA, NER and XNLI tasks. This suggests that multilingual model can benefit from the use of a vocabulary that more fairly represents all languages. In this paper, we take this appr"
2021.mrl-1.3,P19-1070,0,0.100422,"pemb.h-its.org/ This vocabulary size was chosen to be similar to the size of the monolingual BERT’s vocabulary, leaving the 30k most frequent tokens in each language. 2 33 Cross-lingual embeddings creation The subwords in the joint vocabulary were clustered by semantic similarity exploiting the already available BPEmb-embeddings. We started from the monolingual BPEmb embeddings of size d = 300, and projected them into a shared semantic space by mapping all vectors but the English ones into the English vector space. The mapping was performed using the scripts and seed dictionaries published by Glavas et al. (2019).3 In particular, we relied on the PROC algorithm to solve the Procrustes problem (Schönemann, 1966). Seed dictionaries were available for English-Russian and English-Finnish, containing 5000 training pairs and 2000 test pairs. Dictionaries for the remaining languages were derived from Google Translate and can be shared for resource purposes upon request.4 The mapping provides us with 300-dimensional multilingual embeddings, which we evaluate on a bilingual lexicon induction (BLI) task. For each source/target language pair, and for each word pair in the rtest dictionary, the top k translations"
2021.mrl-1.3,D19-6106,0,0.0214956,"with standard mBERT tokenization, across nine typologically diverse languages. We observe the same improvements across two different language model sizes. Introduction Since its release in 2018, multilingual BERT (mBERT) has been extensively analyzed. For instance, mBERT has been shown to facilitate zero-shot cross-lingual transfer, but transfer performance is significantly worse for typologically distant languages (Pires et al., 2019). This is because cross-lingual generalization is only partial, with some language-specific partitioning of the representational space (Libovický et al., 2019; Singh et al., 2019; Pires et al., 2019). Moreover, mBERT relies on a single, multilingual tokenizer to create its vocabulary (WordPiece, Schuster and Nakajima (2012)), a subword tokenizer based on an algorithm similar to BPE (Gage, 1994). The multilingual tokenizer lets subwords be shared across different languages, but the approach penalizes outlier languages, whose words end up being segmented according to statistics derived from high-resource languages (Pyysalo et al., 2020; Chung et al., 2020). In this study, we propose a novel method for building a multilingual vocabulary that is more fair to outlier langu"
2021.mrl-1.3,2021.naacl-main.40,0,0.0624985,"Missing"
2021.mrl-1.3,L18-1473,0,0.0894677,"Missing"
2021.mrl-1.3,W15-4324,1,0.818682,"sites.research.google/trc/ 14 The learning rate was adjusted to take into account the difference batch size (16 instead of 32). 5 Discussion and Future Work ICEBERT significantly improves over the baseline, across both the experimental settings and 38 model BaselineSMALL ICEBERTSMALL EM 1.80 4.74 F1 12.89 15.30 BaselineBASE ICEBERTBASE 10.96 11.36 21.49 23.18 mBERT 41.46 57.74 (Clark et al., 2021). It is beyond the scope of this paper to systematically compare all these different approaches. The idea of using sequences of token group IDs to represent input sentences was previously explored in Wulff and Søgaard (2015), who used finite state automata instead of clusters. 6 Table 2: Exact Match and F1 average scores across all languages different from English. On top of the baseline’s and the ICEBERT model’s performance, the scores obtained by the fully-trained mBERT model are also reported. Conclusion This study aimed to improve multilingual language models by training them on clusters of monolingual segments. The proposed approach yielded good quality clusters able to group semantically similar words and subwords across languages. Our clustering strategy led to improvements over standard segmentation and t"
2021.mrl-1.3,P19-1493,0,0.149565,"er our baseline, trained with standard mBERT tokenization, across nine typologically diverse languages. We observe the same improvements across two different language model sizes. Introduction Since its release in 2018, multilingual BERT (mBERT) has been extensively analyzed. For instance, mBERT has been shown to facilitate zero-shot cross-lingual transfer, but transfer performance is significantly worse for typologically distant languages (Pires et al., 2019). This is because cross-lingual generalization is only partial, with some language-specific partitioning of the representational space (Libovický et al., 2019; Singh et al., 2019; Pires et al., 2019). Moreover, mBERT relies on a single, multilingual tokenizer to create its vocabulary (WordPiece, Schuster and Nakajima (2012)), a subword tokenizer based on an algorithm similar to BPE (Gage, 1994). The multilingual tokenizer lets subwords be shared across different languages, but the approach penalizes outlier languages, whose words end up being segmented according to statistics derived from high-resource languages (Pyysalo et al., 2020; Chung et al., 2020). In this study, we propose a novel method for building a multilingual vocabulary that is more f"
2021.newsum-1.6,P16-1188,0,0.020364,"clear differences, with Hispanics finding TextRank significantly more informative and useful, and American Indians finding TextRank significantly more fluent. Interestingly, Hispanics exhibit significant differences across W OMEN and M EN, finding TextRank summaries of female biographies significantly more informative and useful than TextRank summaries of male biographies. 3 cating it is better than TextRank at extracting sentences with pronouns without breaking coreference chains. Referential clarity, e.g., dangling pronouns, is a known source of error in summarization (Pitler et al., 2010; Durrett et al., 2016). TextRank summaries are often preferred by A MERICAN I NDIAN and A SIAN, when they include negation. This is unsurprising, since negated sentences can often be very informative, and may seem more sophisticated in the context of machine-generated summaries. Negation is also a known source of error (Fiszman et al., 2006). In our data, however, this effect varies across subdemographics. Our main observation is that female and black participants under 30 prefer TextRank over MatchSum. What drives this? The main predictors in our logistic regression analysis are a) TextRank extracting the first se"
2021.newsum-1.6,W10-0722,0,0.0216006,"very different extractive summarization systems – TextRank (Mihalcea and Tarau, 2004) and MatchSum (Zhong et al., 2020). The groups are defined by the three protected attributes: gender, age, and race. While the systems are reported to perform very differently, we show that the system rankings induced by performance scores or user preferences differ across these groups of human summary authors and summary raters. We analyze what drives 1 We thereby challenge the widely held position that lay people cannot be used for summary evaluation, because they exhibit divergent views on summary quality (Gillick and Liu, 2010). We, in contrast, believe such variance is a product of social differences and something we need to worry about in NLP. ∗ The work was done while the author was at the University of Amsterdam. 51 Proceedings of the Third Workshop on New Frontiers in Summarization, pages 51–56 November 10, 2021. ©2021 Association for Computational Linguistics these differences and provide recommendations for future evaluations of summarization systems. 2 Gender ♀ ♂ Experiments ♀ We present two evaluations in this short paper: an automated scoring against human summaries (E XP. A) and a human rater study (E XP."
2021.newsum-1.6,D15-1013,0,0.0159944,"jh and Jurafsky, 2020).1 Summarization – the task of automatically generating brief summaries of longer documents or collections of documents – has, so it seems, seen a lot of progress recently. Progress, of course, is relative to how performance is measured. Generally, summarization systems are evaluated in two ways: by comparing machine-generated summaries to human summaries by text similarity metrics (Lin, 2004; Nenkova and Passonneau, 2004) or by human rater studies, in which participants are asked to rank system outputs. While using similarity metrics is controversial (Liu and Liu, 2008; Graham, 2015; Schluter, 2017), the standard way to evaluate summarization systems is a combination of both. Both comparison to human summaries and the use of human raters naturally involve human participants, and these participants are typically recruited in some way. In Liu and Liu (2008), for example, the human subjects are five undergraduate students in Computer Science. Undergraduate students in Computer Science are not necessarily representative of the population at large, however, or of the end users of the technologies we develop. In this work, we ask whether such sampling bias when Contributions W"
2021.newsum-1.6,W04-1013,0,0.108693,"fair if they do not put certain demographics at a disadvantage (Larson, 2017), and it is therefore crucial our benchmarks reflect preferences and judgments across those demographics (Ethayarajh and Jurafsky, 2020).1 Summarization – the task of automatically generating brief summaries of longer documents or collections of documents – has, so it seems, seen a lot of progress recently. Progress, of course, is relative to how performance is measured. Generally, summarization systems are evaluated in two ways: by comparing machine-generated summaries to human summaries by text similarity metrics (Lin, 2004; Nenkova and Passonneau, 2004) or by human rater studies, in which participants are asked to rank system outputs. While using similarity metrics is controversial (Liu and Liu, 2008; Graham, 2015; Schluter, 2017), the standard way to evaluate summarization systems is a combination of both. Both comparison to human summaries and the use of human raters naturally involve human participants, and these participants are typically recruited in some way. In Liu and Liu (2008), for example, the human subjects are five undergraduate students in Computer Science. Undergraduate students in Computer Scien"
2021.newsum-1.6,P08-2051,0,0.0473639,"ographics (Ethayarajh and Jurafsky, 2020).1 Summarization – the task of automatically generating brief summaries of longer documents or collections of documents – has, so it seems, seen a lot of progress recently. Progress, of course, is relative to how performance is measured. Generally, summarization systems are evaluated in two ways: by comparing machine-generated summaries to human summaries by text similarity metrics (Lin, 2004; Nenkova and Passonneau, 2004) or by human rater studies, in which participants are asked to rank system outputs. While using similarity metrics is controversial (Liu and Liu, 2008; Graham, 2015; Schluter, 2017), the standard way to evaluate summarization systems is a combination of both. Both comparison to human summaries and the use of human raters naturally involve human participants, and these participants are typically recruited in some way. In Liu and Liu (2008), for example, the human subjects are five undergraduate students in Computer Science. Undergraduate students in Computer Science are not necessarily representative of the population at large, however, or of the end users of the technologies we develop. In this work, we ask whether such sampling bias when C"
2021.newsum-1.6,P18-1128,0,0.0330922,"Missing"
2021.newsum-1.6,W04-3252,0,0.0722889,"these participants are typically recruited in some way. In Liu and Liu (2008), for example, the human subjects are five undergraduate students in Computer Science. Undergraduate students in Computer Science are not necessarily representative of the population at large, however, or of the end users of the technologies we develop. In this work, we ask whether such sampling bias when Contributions We present the, to the best of our knowledge, first in-detail evaluations of summarization systems across demographic groups, focusing on two very different extractive summarization systems – TextRank (Mihalcea and Tarau, 2004) and MatchSum (Zhong et al., 2020). The groups are defined by the three protected attributes: gender, age, and race. While the systems are reported to perform very differently, we show that the system rankings induced by performance scores or user preferences differ across these groups of human summary authors and summary raters. We analyze what drives 1 We thereby challenge the widely held position that lay people cannot be used for summary evaluation, because they exhibit divergent views on summary quality (Gillick and Liu, 2010). We, in contrast, believe such variance is a product of social"
2021.newsum-1.6,N04-1019,0,0.0745853,"ey do not put certain demographics at a disadvantage (Larson, 2017), and it is therefore crucial our benchmarks reflect preferences and judgments across those demographics (Ethayarajh and Jurafsky, 2020).1 Summarization – the task of automatically generating brief summaries of longer documents or collections of documents – has, so it seems, seen a lot of progress recently. Progress, of course, is relative to how performance is measured. Generally, summarization systems are evaluated in two ways: by comparing machine-generated summaries to human summaries by text similarity metrics (Lin, 2004; Nenkova and Passonneau, 2004) or by human rater studies, in which participants are asked to rank system outputs. While using similarity metrics is controversial (Liu and Liu, 2008; Graham, 2015; Schluter, 2017), the standard way to evaluate summarization systems is a combination of both. Both comparison to human summaries and the use of human raters naturally involve human participants, and these participants are typically recruited in some way. In Liu and Liu (2008), for example, the human subjects are five undergraduate students in Computer Science. Undergraduate students in Computer Science are not necessarily represen"
2021.newsum-1.6,P10-1056,0,0.0360376,"generally low, we see clear differences, with Hispanics finding TextRank significantly more informative and useful, and American Indians finding TextRank significantly more fluent. Interestingly, Hispanics exhibit significant differences across W OMEN and M EN, finding TextRank summaries of female biographies significantly more informative and useful than TextRank summaries of male biographies. 3 cating it is better than TextRank at extracting sentences with pronouns without breaking coreference chains. Referential clarity, e.g., dangling pronouns, is a known source of error in summarization (Pitler et al., 2010; Durrett et al., 2016). TextRank summaries are often preferred by A MERICAN I NDIAN and A SIAN, when they include negation. This is unsurprising, since negated sentences can often be very informative, and may seem more sophisticated in the context of machine-generated summaries. Negation is also a known source of error (Fiszman et al., 2006). In our data, however, this effect varies across subdemographics. Our main observation is that female and black participants under 30 prefer TextRank over MatchSum. What drives this? The main predictors in our logistic regression analysis are a) TextRank"
2021.newsum-1.6,E17-2007,0,0.0154433,"y, 2020).1 Summarization – the task of automatically generating brief summaries of longer documents or collections of documents – has, so it seems, seen a lot of progress recently. Progress, of course, is relative to how performance is measured. Generally, summarization systems are evaluated in two ways: by comparing machine-generated summaries to human summaries by text similarity metrics (Lin, 2004; Nenkova and Passonneau, 2004) or by human rater studies, in which participants are asked to rank system outputs. While using similarity metrics is controversial (Liu and Liu, 2008; Graham, 2015; Schluter, 2017), the standard way to evaluate summarization systems is a combination of both. Both comparison to human summaries and the use of human raters naturally involve human participants, and these participants are typically recruited in some way. In Liu and Liu (2008), for example, the human subjects are five undergraduate students in Computer Science. Undergraduate students in Computer Science are not necessarily representative of the population at large, however, or of the end users of the technologies we develop. In this work, we ask whether such sampling bias when Contributions We present the, to"
2021.newsum-1.6,P19-1628,0,0.0536209,"Missing"
2021.newsum-1.6,2020.acl-main.552,0,0.0486013,"ed in some way. In Liu and Liu (2008), for example, the human subjects are five undergraduate students in Computer Science. Undergraduate students in Computer Science are not necessarily representative of the population at large, however, or of the end users of the technologies we develop. In this work, we ask whether such sampling bias when Contributions We present the, to the best of our knowledge, first in-detail evaluations of summarization systems across demographic groups, focusing on two very different extractive summarization systems – TextRank (Mihalcea and Tarau, 2004) and MatchSum (Zhong et al., 2020). The groups are defined by the three protected attributes: gender, age, and race. While the systems are reported to perform very differently, we show that the system rankings induced by performance scores or user preferences differ across these groups of human summary authors and summary raters. We analyze what drives 1 We thereby challenge the widely held position that lay people cannot be used for summary evaluation, because they exhibit divergent views on summary quality (Gillick and Liu, 2010). We, in contrast, believe such variance is a product of social differences and something we need"
2021.starsem-1.25,D15-1085,0,0.0712215,"Missing"
2021.starsem-1.25,D17-1218,0,0.124697,"ly mostly on spurious correlations and only generalise within closely related topics, e.g., a model trained only on closed-class words and a few common open-class words outperforms a state-of-theart cross-topic model on distant target topics. 1 Figure 1: In human interaction, it is evident that relying on topic words for recognizing an argument is nonsensical. It is, nevertheless, what a BERT-based crosstopic argument mining model does. standard evaluation protocol is to evaluate argument mining systems across topics, i.e., on heldout topics, precisely to avoid over-fitting to a single topic (Daxenberger et al., 2017; Stab et al., 2018; Reimers et al., 2019). This study shows that despite this sensible cross-topic evaluation protocol, stateof-the-art systems nevertheless rely primarily on spurious correlations, e.g., guns (Figure 1). These spurious correlations transfer across some topics in popular benchmarks, but only because the topics are closely related. Introduction When a sentiment analysis model associates the word Shrek with positive sentiment (Sindhwani and Melville, 2008), it relies on a spurious correlation. While the movie Shrek was popular at the time the training data was sampled, this is u"
2021.starsem-1.25,N19-1423,0,0.013681,"sentences (23,467). We use binary labels (claim or not) and introduce a random 70-10-20 split. 4 Experimental setup We now describe our learning architecture, an almost out-of-the-box application of the MT-DNN architecture in Liu et al. (2019). It is a strong model that achieves a better performance than previously reported across the benchmarks. The MT-DNN model of Liu et al. (2019) combines the pre-trained BERT architecture with multitask learning. The model can be broken up into shared layers and task-specific layers. The shared layers are initialised with the pre-trained BERT base model (Devlin et al., 2019). We add a taskspecific output layer for each task and update all model parameters during training with AdaMax. The task-specific layers are logistic regression classifiers with softmax activation, minimising crossentropy loss functions for classification tasks or mean squared error for regression tasks. If we only have a single output layer, we refer to the architecture as single-task DNN (ST-DNN) rather than MT-DNN. We train all models over 10 epochs with a batch size of 5 for feasibility and otherwise use default hyperparameters. Following Stab et al. (2018), we iteratively combine the trai"
2021.starsem-1.25,2020.acl-main.385,0,0.0107018,"lexicon, a sentiment lexicon, an emotion lexicon and the Princeton WordNet8 ) in the attention mechanism of a BiLSTM, but evaluate this only in the context 8 https://wordnet.princeton.edu/ 270 Feature analysis in deep neural networks Feature analysis in deep neural networks is not straightforward but, by now, several approaches to attribute importance in deep neural networks to features or input tokens are available. One advantage of LIME is that it can be applied to any model posthoc. Other approaches for interpreting transformers, specifically, focus on inspections of the attention weights (Abnar and Zuidema, 2020; Vig, 2019) and vector norms (Kobayashi et al., 2020). Spurious correlations in text classification Landeiro and Culotta (2018) provide a thorough description of spurious correlations deriving from confounding factors in text classification and outline methods from social science of controlling for confounds. However, these methods require the confounding factors to be known, which is often not the case. This problem is tackled by Wang and Culotta (2020) who, in contrast, develop a computational method for distinguishing spurious from genuine correlations in text classification to adjust for"
2021.starsem-1.25,2020.emnlp-main.649,0,0.0278836,"Missing"
2021.starsem-1.25,2020.emnlp-main.574,0,0.0153279,"he Princeton WordNet8 ) in the attention mechanism of a BiLSTM, but evaluate this only in the context 8 https://wordnet.princeton.edu/ 270 Feature analysis in deep neural networks Feature analysis in deep neural networks is not straightforward but, by now, several approaches to attribute importance in deep neural networks to features or input tokens are available. One advantage of LIME is that it can be applied to any model posthoc. Other approaches for interpreting transformers, specifically, focus on inspections of the attention weights (Abnar and Zuidema, 2020; Vig, 2019) and vector norms (Kobayashi et al., 2020). Spurious correlations in text classification Landeiro and Culotta (2018) provide a thorough description of spurious correlations deriving from confounding factors in text classification and outline methods from social science of controlling for confounds. However, these methods require the confounding factors to be known, which is often not the case. This problem is tackled by Wang and Culotta (2020) who, in contrast, develop a computational method for distinguishing spurious from genuine correlations in text classification to adjust for the identified spurious features to improve model robu"
2021.starsem-1.25,D19-1425,0,0.0273182,"founds. However, these methods require the confounding factors to be known, which is often not the case. This problem is tackled by Wang and Culotta (2020) who, in contrast, develop a computational method for distinguishing spurious from genuine correlations in text classification to adjust for the identified spurious features to improve model robustness. They consider spurious correlations in sentiment classification and toxicity detection. McHardy et al. (2019) identified similar problems in sarcasm detection and suggested adversarial training to reduce sensitivity to spurious correlations. Kumar et al. (2019) present a similar method to avoid “topical confounds” in native language identification. MTL to regularize spurious correlations Tu et al. (2020) suggest multi-task learning increase robustness to spurious correlations. Multi-task learning has previously been shown to be an effective regularizer (Søgaard and Goldberg, 2016; Sener and Koltun, 2018), leading to better generalization to new domains (Cheng et al., 2015; Peng and Dredze, 2017). Jabbour et al. (2020), though, presents experiments in automated diagnosis of disease based on chest X-rays suggesting that multi-task learning is not alwa"
2021.starsem-1.25,C18-1176,0,0.197705,"similar problem in state-ofthe-art cross-topic argument mining systems. The task of argument mining is to recognise the existence of claims and premises in a text span. The All code will be publicly available at https:// github.com/terne/spurious_correlations_ in_argmin Contributions We present experiments with an out-of-the-box learning architecture for argument mining, yet with state-of-the-art performance, based on Microsoft’s MT-DNN library (Liu et al., 2019). We train models on the UKP Sentential Argument Mining Corpus (Stab et al., 2018), the IBM Debater Argument Search Engine Dataset (Levy et al., 2018), the Argument Extraction corpus (Swanson et al., 2015), and the Vaccination Corpus (Morante et al., 2020). We analyse the models with 263 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 263–277 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics respect to spurious correlations using the post-hoc interpretability tool LIME (Ribeiro et al., 2016) and we find that the models rely heavily on these. This analysis is the paper’s main contribution: In §5, we: a) evaluate our best-performing model on a small set of challenge e"
2021.starsem-1.25,W17-5110,0,0.0424717,"Missing"
2021.starsem-1.25,W19-4508,0,0.012586,"ge of cross-domain generalization in argument mining, finding that models performing best indomain may not be the ones performing best outof-domain, which they argue may in part be due to different notions of claims in the dataset development. Through experiments with different feature groups, such as embeddings, syntax or lexical features, they find lexical clues to be the “essence” of claims and that simple rules are important for cross-domain performance. Simple lexical clues are also found to be effective for argument mining in Levy et al. (2018), who create a claim lexicon, as well as in Lin et al. (2019) who investigate the effectiveness of integrating lexica (a claim lexicon, a sentiment lexicon, an emotion lexicon and the Princeton WordNet8 ) in the attention mechanism of a BiLSTM, but evaluate this only in the context 8 https://wordnet.princeton.edu/ 270 Feature analysis in deep neural networks Feature analysis in deep neural networks is not straightforward but, by now, several approaches to attribute importance in deep neural networks to features or input tokens are available. One advantage of LIME is that it can be applied to any model posthoc. Other approaches for interpreting transform"
2021.starsem-1.25,W19-4520,0,0.0136891,"detection can be used to see what fractions of demographic subgroups are in favor of or against some topic, argument mining can be used to identify the arguments made for and against policies in political discussions. What is an argument? An argument is made up of propositions (claims), which are statements that are either true or false. Traditionally, an argument must consist of at least two claims, with one being the conclusion (major claim) and at least one reason (premise) backing up that claim. Some argument annotation schemes ask annotators to label premises and major claims separately (Lindahl et al., 2019). Others simplify the task to identifying claim or claim-like sentences (Morante et al., 2020) or to whether sentences are claims supporting or opposing a particular idea or topic (Levy et al., 2018; Stab et al., 2018). The resources used in our experiments below are of the latter type: Sentences are labeled as arguments if they present evidence or reasoning in relation to a claim or topic and are refutable. The resources used in our experiments are annotated with arguments in the context of a particular topic, as well as the argument’s polarity, i.e., what is annotated relates to stance. The"
2021.starsem-1.25,P19-1441,0,0.302387,"zone, demoting spurious correlations is key to learning robust NLP models (Sutton et al., 2006; Søgaard, 2013; Tu et al., 2020). This paper studies a similar problem in state-ofthe-art cross-topic argument mining systems. The task of argument mining is to recognise the existence of claims and premises in a text span. The All code will be publicly available at https:// github.com/terne/spurious_correlations_ in_argmin Contributions We present experiments with an out-of-the-box learning architecture for argument mining, yet with state-of-the-art performance, based on Microsoft’s MT-DNN library (Liu et al., 2019). We train models on the UKP Sentential Argument Mining Corpus (Stab et al., 2018), the IBM Debater Argument Search Engine Dataset (Levy et al., 2018), the Argument Extraction corpus (Swanson et al., 2015), and the Vaccination Corpus (Morante et al., 2020). We analyse the models with 263 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 263–277 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics respect to spurious correlations using the post-hoc interpretability tool LIME (Ribeiro et al., 2016) and we find that the models"
2021.starsem-1.25,N19-1069,0,0.0276261,"scription of spurious correlations deriving from confounding factors in text classification and outline methods from social science of controlling for confounds. However, these methods require the confounding factors to be known, which is often not the case. This problem is tackled by Wang and Culotta (2020) who, in contrast, develop a computational method for distinguishing spurious from genuine correlations in text classification to adjust for the identified spurious features to improve model robustness. They consider spurious correlations in sentiment classification and toxicity detection. McHardy et al. (2019) identified similar problems in sarcasm detection and suggested adversarial training to reduce sensitivity to spurious correlations. Kumar et al. (2019) present a similar method to avoid “topical confounds” in native language identification. MTL to regularize spurious correlations Tu et al. (2020) suggest multi-task learning increase robustness to spurious correlations. Multi-task learning has previously been shown to be an effective regularizer (Søgaard and Goldberg, 2016; Sener and Koltun, 2018), leading to better generalization to new domains (Cheng et al., 2015; Peng and Dredze, 2017). Jab"
2021.starsem-1.25,P19-1054,0,0.182339,"Missing"
2021.starsem-1.25,N16-3020,0,0.0111545,"on Microsoft’s MT-DNN library (Liu et al., 2019). We train models on the UKP Sentential Argument Mining Corpus (Stab et al., 2018), the IBM Debater Argument Search Engine Dataset (Levy et al., 2018), the Argument Extraction corpus (Swanson et al., 2015), and the Vaccination Corpus (Morante et al., 2020). We analyse the models with 263 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 263–277 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics respect to spurious correlations using the post-hoc interpretability tool LIME (Ribeiro et al., 2016) and we find that the models rely heavily on these. This analysis is the paper’s main contribution: In §5, we: a) evaluate our best-performing model on a small set of challenge examples, which we make available, and which motivate our subsequent analyses; b) manually analyse how many of the words our models rely the most on are spurious correlations; c) evaluate how much weight our models attribute to open class words and whether multi-task training effectively moves emphasis to closed-class items that likely transfer better across topics; d) evaluate how much weight our models attribute to wo"
2021.starsem-1.25,P11-1097,0,0.0607977,"Missing"
2021.starsem-1.25,P13-2113,1,0.799361,"ely related. Introduction When a sentiment analysis model associates the word Shrek with positive sentiment (Sindhwani and Melville, 2008), it relies on a spurious correlation. While the movie Shrek was popular at the time the training data was sampled, this is unlikely to transfer across demographics, platforms and years. While there exists a continuum from sentiment words such as fantastic to spurious correlations such as Shrek, with words such as Hollywood or anticipation being perhaps in a grey zone, demoting spurious correlations is key to learning robust NLP models (Sutton et al., 2006; Søgaard, 2013; Tu et al., 2020). This paper studies a similar problem in state-ofthe-art cross-topic argument mining systems. The task of argument mining is to recognise the existence of claims and premises in a text span. The All code will be publicly available at https:// github.com/terne/spurious_correlations_ in_argmin Contributions We present experiments with an out-of-the-box learning architecture for argument mining, yet with state-of-the-art performance, based on Microsoft’s MT-DNN library (Liu et al., 2019). We train models on the UKP Sentential Argument Mining Corpus (Stab et al., 2018), the IBM"
2021.starsem-1.25,P16-2038,1,0.899724,"ords and for all topics, except cloning, there are many topic words among the top 20. Highfrequency words (as well as most argument words) are naturally ranked much lower after normalisation. Stance words are, of course, not spurious for our three-way classification problem, but a near disappearance of argument words in the normalized top 20 suggests our models are unlikely to capture low-frequency argument markers. c) How much weight do our models attribute to open class words, and does multi-task learning move emphasis to closed-class items? Multitask learning is a regularization technique (Søgaard and Goldberg, 2016; Liu et al., 2019) and may, as suggested by Tu et al. (2020), reduce the extent to which our models rely on spurious correlations, which tend to be open class words. To compare the weight attributed to open-class words, across single-task and multi-task models, we define a score reflecting the weight put on open class words in a sentence: For each word in the sentence, we consider the maximum LIME weight of the two weights towards the argument classes A RGUMENT AGAINST and A RGUMENT FOR. We then take the sum of LIME weights put on open class words, normalised by the total sum of weights, and"
2021.starsem-1.25,J17-3005,0,0.345605,"aper’s main contribution: In §5, we: a) evaluate our best-performing model on a small set of challenge examples, which we make available, and which motivate our subsequent analyses; b) manually analyse how many of the words our models rely the most on are spurious correlations; c) evaluate how much weight our models attribute to open class words and whether multi-task training effectively moves emphasis to closed-class items that likely transfer better across topics; d) evaluate how much weight our models attribute to words in a manually constructed claim indicator list (Morante et al., 2020; Stab and Gurevych, 2017), and whether multi-task training effectively moves emphasis to such claim indicators that likely transfer better across topics; and lastly e) evaluate the performance of models trained only on closedclass words or closed class and open class words that are shared across topics. Surprisingly, we find that models with access to only closed-class words, and a few common (topic-independent) open-class words, perform better across distant topics than our baseline, state-of-the-art models (Table 5). 2 Argument mining We first describe the task of argument mining, focusing, in particular, on the sub"
2021.starsem-1.25,2020.lrec-1.611,0,0.0227535,"Missing"
2021.starsem-1.25,D18-1402,0,0.245657,"elations and only generalise within closely related topics, e.g., a model trained only on closed-class words and a few common open-class words outperforms a state-of-theart cross-topic model on distant target topics. 1 Figure 1: In human interaction, it is evident that relying on topic words for recognizing an argument is nonsensical. It is, nevertheless, what a BERT-based crosstopic argument mining model does. standard evaluation protocol is to evaluate argument mining systems across topics, i.e., on heldout topics, precisely to avoid over-fitting to a single topic (Daxenberger et al., 2017; Stab et al., 2018; Reimers et al., 2019). This study shows that despite this sensible cross-topic evaluation protocol, stateof-the-art systems nevertheless rely primarily on spurious correlations, e.g., guns (Figure 1). These spurious correlations transfer across some topics in popular benchmarks, but only because the topics are closely related. Introduction When a sentiment analysis model associates the word Shrek with positive sentiment (Sindhwani and Melville, 2008), it relies on a spurious correlation. While the movie Shrek was popular at the time the training data was sampled, this is unlikely to transfer"
2021.starsem-1.25,W17-2612,0,0.0173123,"tion. McHardy et al. (2019) identified similar problems in sarcasm detection and suggested adversarial training to reduce sensitivity to spurious correlations. Kumar et al. (2019) present a similar method to avoid “topical confounds” in native language identification. MTL to regularize spurious correlations Tu et al. (2020) suggest multi-task learning increase robustness to spurious correlations. Multi-task learning has previously been shown to be an effective regularizer (Søgaard and Goldberg, 2016; Sener and Koltun, 2018), leading to better generalization to new domains (Cheng et al., 2015; Peng and Dredze, 2017). Jabbour et al. (2020), though, presents experiments in automated diagnosis of disease based on chest X-rays suggesting that multi-task learning is not always robust to spurious correlations. In our study, we expected multi-task learning to move emphasis to closed-class items and claim indicators and away from the spurious correlations that do not hold as general markers of claims and arguments across topics and domains. Still, our analysis of feature weights does not indicate that multi-task learning is effective to this end. 7 Conclusion We have shown that cross-topic evaluation of argument"
2021.starsem-1.25,N16-1107,0,0.0186273,"Missing"
2021.starsem-1.25,N06-1012,0,0.186959,"Missing"
2021.starsem-1.25,W15-4631,0,0.0258901,"ment mining systems. The task of argument mining is to recognise the existence of claims and premises in a text span. The All code will be publicly available at https:// github.com/terne/spurious_correlations_ in_argmin Contributions We present experiments with an out-of-the-box learning architecture for argument mining, yet with state-of-the-art performance, based on Microsoft’s MT-DNN library (Liu et al., 2019). We train models on the UKP Sentential Argument Mining Corpus (Stab et al., 2018), the IBM Debater Argument Search Engine Dataset (Levy et al., 2018), the Argument Extraction corpus (Swanson et al., 2015), and the Vaccination Corpus (Morante et al., 2020). We analyse the models with 263 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 263–277 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics respect to spurious correlations using the post-hoc interpretability tool LIME (Ribeiro et al., 2016) and we find that the models rely heavily on these. This analysis is the paper’s main contribution: In §5, we: a) evaluate our best-performing model on a small set of challenge examples, which we make available, and which motivate ou"
2021.starsem-1.25,2020.tacl-1.40,0,0.288964,"troduction When a sentiment analysis model associates the word Shrek with positive sentiment (Sindhwani and Melville, 2008), it relies on a spurious correlation. While the movie Shrek was popular at the time the training data was sampled, this is unlikely to transfer across demographics, platforms and years. While there exists a continuum from sentiment words such as fantastic to spurious correlations such as Shrek, with words such as Hollywood or anticipation being perhaps in a grey zone, demoting spurious correlations is key to learning robust NLP models (Sutton et al., 2006; Søgaard, 2013; Tu et al., 2020). This paper studies a similar problem in state-ofthe-art cross-topic argument mining systems. The task of argument mining is to recognise the existence of claims and premises in a text span. The All code will be publicly available at https:// github.com/terne/spurious_correlations_ in_argmin Contributions We present experiments with an out-of-the-box learning architecture for argument mining, yet with state-of-the-art performance, based on Microsoft’s MT-DNN library (Liu et al., 2019). We train models on the UKP Sentential Argument Mining Corpus (Stab et al., 2018), the IBM Debater Argument S"
2021.starsem-1.25,P19-3007,0,0.0154297,"con, an emotion lexicon and the Princeton WordNet8 ) in the attention mechanism of a BiLSTM, but evaluate this only in the context 8 https://wordnet.princeton.edu/ 270 Feature analysis in deep neural networks Feature analysis in deep neural networks is not straightforward but, by now, several approaches to attribute importance in deep neural networks to features or input tokens are available. One advantage of LIME is that it can be applied to any model posthoc. Other approaches for interpreting transformers, specifically, focus on inspections of the attention weights (Abnar and Zuidema, 2020; Vig, 2019) and vector norms (Kobayashi et al., 2020). Spurious correlations in text classification Landeiro and Culotta (2018) provide a thorough description of spurious correlations deriving from confounding factors in text classification and outline methods from social science of controlling for confounds. However, these methods require the confounding factors to be known, which is often not the case. This problem is tackled by Wang and Culotta (2020) who, in contrast, develop a computational method for distinguishing spurious from genuine correlations in text classification to adjust for the identifi"
2021.starsem-1.25,walker-etal-2012-corpus,0,0.0241653,"Missing"
2021.starsem-1.25,2020.findings-emnlp.308,0,0.0611641,"at arguments require the author to present evidence or reasoning for or against the topic. Spurious correlations of arguments Arguments for or against a policy typically refer to different concepts. Take, for example, discussions of minimum wage and the terms living wages and jobs. Since these terms are frequent in arguments for and against minimum wage, they will be predictive of arguments (in discussions of minimum wage). Still, mentions of the terms are not themselves markers of arguments, but simply spurious correlations of arguments. We use the same definition of spurious correlations as Wang and Culotta (2020), mainly that a relationship between a term and a label is spurious if one cannot expect the term to be a determining factor for assigning the label.1 Examples of the contrary are terms such as if and because (and to some degree stance terms), which one can reasonably expect to be determining factors for an argument to exist (and therefore to be stable across topics and time). 3 Datasets The UKP Sentential Argument Mining Corpus (UKP) (Stab et al., 2018) contains 25,492 sentences spanning eight controversial topics (abortion, cloning, death penalty, gun control, marijuana legalization, school"
2021.wat-1.22,D18-1530,1,0.828232,"f digitization has been a bottleneck hindering any meaningful progress towards automatic translation systems. This has changed recently, at least for monolingual data, with the curation of digital libraries like GRETIL8 and DCS9 . Currently, the largest freely available repository of translations are for The Bhagavadgita (Prabhakar et al., 2000) and The R¯am¯ayana (Geervani et al., 1989). However, labeled datasets for other tasks, like the ones proposed in (Kulkarni, 2013; Bhardwaj et al., 2018; Krishnan et al., 2020) have resulted in parsers (Krishna et al., 2020, 2021) and sandhi splitters (Aralikatte et al., 2018; Krishnan and Kulkarni, 2020) which are pre-cursors to modular translation systems. Though there have been attempts at building Sanskrit translation tools (Bharati and Kulkarni, 2009), they are mostly rule-based and rely on manual intervention. We hope that the availability of the Itih¯asa corpus pushes the domain towards endto-end systems. 8 http://gretil.sub.uni-goettingen.de/ gretil.html 9 http://www.sanskrit-linguistics.org/ dcs/index.php 6 Conclusion In this work, we introduce Itih¯asa, a large-scale dataset containing more than 93,000 pairs of Sanskrit shlokas and their English translat"
2021.wat-1.22,2009.freeopmt-1.2,0,0.0623893,"ation of digital libraries like GRETIL8 and DCS9 . Currently, the largest freely available repository of translations are for The Bhagavadgita (Prabhakar et al., 2000) and The R¯am¯ayana (Geervani et al., 1989). However, labeled datasets for other tasks, like the ones proposed in (Kulkarni, 2013; Bhardwaj et al., 2018; Krishnan et al., 2020) have resulted in parsers (Krishna et al., 2020, 2021) and sandhi splitters (Aralikatte et al., 2018; Krishnan and Kulkarni, 2020) which are pre-cursors to modular translation systems. Though there have been attempts at building Sanskrit translation tools (Bharati and Kulkarni, 2009), they are mostly rule-based and rely on manual intervention. We hope that the availability of the Itih¯asa corpus pushes the domain towards endto-end systems. 8 http://gretil.sub.uni-goettingen.de/ gretil.html 9 http://www.sanskrit-linguistics.org/ dcs/index.php 6 Conclusion In this work, we introduce Itih¯asa, a large-scale dataset containing more than 93,000 pairs of Sanskrit shlokas and their English translations from The R¯am¯ayana and The Mah¯abh¯arata. First, we detail the extraction process which includes an automated OCR phase and a manual alignment phase. Next, we analyze the dataset"
2021.wat-1.22,L18-1712,0,0.0648864,"Missing"
2021.wat-1.22,W18-6111,0,0.0209163,"line is ignored. In general, print errors are corrected as much as possible, subjective errors are retained for originality, and other types of errors are corrected when encountered. 6 This was a time-consuming process and the first author inspected the output manually over the course of one year. 7 It was not feasible for the authors to correct every error, especially the lexical ones. The most common error that exists in the corpus is the swapping of e and c. For example, ‘thcir’ instead of ‘their’. Though these errors can easily be corrected using automated tools like the one proposed in (Boyd, 2018), it is out-of-scope of this paper and is left for future work. Train Chapters Shlokas Chapters Shlokas Chapters Shlokas Dev. Test R¯amayana 514 42 86 15,834 1,115 2,422 Mah¯abh¯arata 1,688 139 283 59,327 5,033 9,299 Overall 2202 181 369 75,161 6,148 11,721 Total 642 19,371 2,110 73,659 2,752 93,030 Table 1: Size of training, development, and test sets. 3 Analysis In total, we extract 19,371 translation pairs from 642 chapters of The R¯am¯ayana and 73,659 translation pairs from 2,110 chapters of The Mah¯abh¯arata. It should be noted that these numbers do not correspond to the number of shlokas"
2021.wat-1.22,C12-1062,0,0.0174373,"viz., The R¯am¯ayana and The Mah¯abh¯arata. We first describe the motivation behind the curation of such a dataset and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this corpus and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.1 1 Introduction Sanskrit is one of the oldest languages in the world and most Indo-European languages are influenced by it (Beekes, 1995). There are about 30 million pieces of Sanskrit literature available to us today (Goyal et al., 2012), most of which have not been digitized. Among those that have been, few have been translated. The main reason for this is the lack of expertise and funding. An automatic translation system would not only aid and accelerate this process, but it would also help in democratizing the knowledge, history, and culture present in this literature. In this work, we present Itih¯asa, a large-scale Sanskrit-English translation corpus consisting of more than 93,000 shlokas and their translations. Itih¯asa, literally meaning ‘it happened this way’ is a collection of historical records of important events i"
2021.wat-1.22,2005.mtsummit-papers.11,0,0.152061,"Itih¯asa, with the best models scoring between 7-8 BLEU points. This indicates the complex nature of the dataset (see §3 for a detailed analysis of the dataset and its vocabulary). Motivation The main motivation behind this work is to provide an impetus for the Indic NLP community to build better translation systems for Sanskrit. Additionally, since The R¯am¯ayana and The Mah¯abh¯arata are so pervasive in Indian culture, and have been translated to all major Indian languages, there is a possibility of creating an n-way parallel corpus with Sanskrit as the pivot language, similar to Europarl (Koehn, 2005) and PMIndia (Haddow and Kirefu, 2020) datasets. The existence of Sanskrit-English parallel data has other advantages as well. Due to Sanskrit being a morphologically rich, agglutinative, and highly inflexive, complex concepts can be expressed in compact forms by combining individual words through Sandhi and Samasa.2 This also enables a speaker to potentially create an infinite number of unique words in Sanskrit. Having a parallel corpus can help us induce word translations through bilingual dictionary induction (Søgaard et al., 2018). It also allows us to use English as a surrogate language f"
2021.wat-1.22,P07-2045,0,0.0131653,"Missing"
2021.wat-1.22,2020.emnlp-main.388,0,0.0181067,"ike Ganguli (1883) have been published, the lack of digitization has been a bottleneck hindering any meaningful progress towards automatic translation systems. This has changed recently, at least for monolingual data, with the curation of digital libraries like GRETIL8 and DCS9 . Currently, the largest freely available repository of translations are for The Bhagavadgita (Prabhakar et al., 2000) and The R¯am¯ayana (Geervani et al., 1989). However, labeled datasets for other tasks, like the ones proposed in (Kulkarni, 2013; Bhardwaj et al., 2018; Krishnan et al., 2020) have resulted in parsers (Krishna et al., 2020, 2021) and sandhi splitters (Aralikatte et al., 2018; Krishnan and Kulkarni, 2020) which are pre-cursors to modular translation systems. Though there have been attempts at building Sanskrit translation tools (Bharati and Kulkarni, 2009), they are mostly rule-based and rely on manual intervention. We hope that the availability of the Itih¯asa corpus pushes the domain towards endto-end systems. 8 http://gretil.sub.uni-goettingen.de/ gretil.html 9 http://www.sanskrit-linguistics.org/ dcs/index.php 6 Conclusion In this work, we introduce Itih¯asa, a large-scale dataset containing more than 93,000"
2021.wat-1.22,W13-3718,0,0.0242224,"M¨uller, 1866; MonierWilliams, 1899). Over the years, though notable translation works like Ganguli (1883) have been published, the lack of digitization has been a bottleneck hindering any meaningful progress towards automatic translation systems. This has changed recently, at least for monolingual data, with the curation of digital libraries like GRETIL8 and DCS9 . Currently, the largest freely available repository of translations are for The Bhagavadgita (Prabhakar et al., 2000) and The R¯am¯ayana (Geervani et al., 1989). However, labeled datasets for other tasks, like the ones proposed in (Kulkarni, 2013; Bhardwaj et al., 2018; Krishnan et al., 2020) have resulted in parsers (Krishna et al., 2020, 2021) and sandhi splitters (Aralikatte et al., 2018; Krishnan and Kulkarni, 2020) which are pre-cursors to modular translation systems. Though there have been attempts at building Sanskrit translation tools (Bharati and Kulkarni, 2009), they are mostly rule-based and rely on manual intervention. We hope that the availability of the Itih¯asa corpus pushes the domain towards endto-end systems. 8 http://gretil.sub.uni-goettingen.de/ gretil.html 9 http://www.sanskrit-linguistics.org/ dcs/index.php 6 Con"
2021.wat-1.22,P02-1040,0,0.109083,"Missing"
2021.wat-1.22,P16-1162,0,0.0267965,"Missing"
2021.wat-1.22,2006.amta-papers.25,0,0.199483,"Missing"
2021.wat-1.22,P18-1072,1,0.865103,"Missing"
2021.wat-1.24,P19-1310,0,0.0464951,"Missing"
2021.wat-1.24,D17-1209,0,0.0536477,"Missing"
2021.wat-1.24,E06-1032,0,0.163401,"l would become more accurate if it is trained for a longer period or with more compute. To establish whether an increase in BLEU scores corresponds to an increase in the fluency and faithfulness of the translations, we manually annotate 50 Hindi and Kannada test predictions from the best model to find that the increase in both cases is marginal. In the 20 additional training hours, the fluency and faithfulness increased by 0.005 and 0.01 respectively which suggests that BLEU may not be the best metric to quantify the goodness of translation systems, as shown in works like Zhang et al. (2004); Callison-Burch et al. (2006). 6 Conclusion In this work, we show that it is possible to get competitive translation results using a single GPU for a limited amount of time by carefully selecting and training large pre-trained encoder-decoder models. We also show that we can train models which have more than 109 trainable parameters using the latest advances in GPU resource optimization. Finally, through a small empirical study, we find that while longer training can increase BLEU scores, it may not increase their fluency and faithfulness. Acknowledgements We thank the reviewers for their valuable feedback. Rahul Aralikat"
2021.wat-1.24,D14-1179,0,0.016331,"Missing"
2021.wat-1.24,N19-1423,0,0.076397,"9) Wiki Titles (2021) ALT (2016) IITB 3.0 (2018) NLPC (2020) UFAL EnTam (2012) Uka Tarsadia (2019) MTEnglish2Odia (2018) OdiEnCorp 2.0 (2020) Introduction Machine Translation is one of the few tasks in NLP which has the luxury of data. Due to the efforts of the community over the last two decades (Koehn, 2005; Tiedemann, 2012, 2020), most major languages of the world have millions of translated sentence pairs with English. With the introduction of sequence to sequence models (Sutskever et al., 2014; Cho et al., 2014), transformers (Vaswani et al., 2017), and large pre-trained language models (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019), the accuracy of machine translation models has almost risen to that of humans (Wu et al., 2016). Yet, the ability to train such models is limited by the availability of compute. Today’s state-of-the-art models are trained by industry research labs, using large compute infrastructure which is usually unavailable or unaffordable to others. Such training is also shown to have large carbon footprints (Strubell et al., 2019). In this work, we show that competitive translation performance can be achieved even with limited resources. We fi"
2021.wat-1.24,2005.mtsummit-papers.11,0,0.068299,"gle GPU for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the WAT 2021 leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of current translation metrics. 1 Bible-uedin (2014) OpenSubtitles (2016) WikiMatrix (2019) Wiki Titles (2021) ALT (2016) IITB 3.0 (2018) NLPC (2020) UFAL EnTam (2012) Uka Tarsadia (2019) MTEnglish2Odia (2018) OdiEnCorp 2.0 (2020) Introduction Machine Translation is one of the few tasks in NLP which has the luxury of data. Due to the efforts of the community over the last two decades (Koehn, 2005; Tiedemann, 2012, 2020), most major languages of the world have millions of translated sentence pairs with English. With the introduction of sequence to sequence models (Sutskever et al., 2014; Cho et al., 2014), transformers (Vaswani et al., 2017), and large pre-trained language models (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019), the accuracy of machine translation models has almost risen to that of humans (Wu et al., 2016). Yet, the ability to train such models is limited by the availability of compute. Today’s state-of-the-art models are trained by indu"
2021.wat-1.24,P07-2045,0,0.0128504,"´c and Vuli´c, 2019) has alignment issues, where a part of the translation is moved to the next line, thereby starting a chain of misalignments, as shown in Fig. 1. We manually annotate 100 translations for fluency and faithfulness on a scale of 0-5 and get a score of 4.01 for fluency and 3.54 for faithfulness. 3 Models We train four types of models: (i) a phrase-based statistical model, (ii) a graph-to-text model, (iii) a sequence-to-sequence model, and (iv) a text-to-text model. Brief descriptions of the models are given below. 3.1 Moses We train a statistical phrase-based model with Moses (Koehn et al., 2007) using default settings, following the guidelines for training a baseline.2 We prune words that occur less than three times in the corpus and use the same tokenizer as for the other models and de-tokenize predictions before evaluating. We train a separate model for each language pair and use the respective development set 2 http://www.statmt.org/moses/?n=Moses. Baseline for tuning before translating the test set. 3.2 GRAPH - TO - TEXT model We also train a graph2seq model with a GCN (Kipf and Welling, 2016) encoder and LSTM decoder. In addition to text, we input the source syntax trees obtaine"
2021.wat-1.24,L18-1548,0,0.0697831,"Missing"
2021.wat-1.24,2021.ccl-1.108,0,0.031246,"Missing"
2021.wat-1.24,P02-1040,0,0.113797,"meters which do not fit on our 24GB GPU, even if trained with mixed-precision and a batch size of one. Therefore, we resort to optimizer state and gradient partitioning with ZeRO (Rajbhandari et al., 2020). ZeRO is a zero-redundancy optimizer that offloads some computations and memory to the host’s CPU and provides a better GPU management system that uses smart allocation methods to reduce memory fragmentation. For more details, see Rasley et al. (2020). With these modifications, we train the model with a learning rate of 3 × 10−5 . All other hyper-parameters remain unchanged. 4 Results BLEU (Papineni et al., 2002), which are standard metrics to evaluate translations. We train two variants of all models: (i) only on the PMI corpus, and (ii) on the full training data. The English to Indic results are shown in Tab. 2 and the Indic to English results, in Tab. 3.4 m2m100 We first benchmark the performance of the Many-to-Many multilingual model (m2m100; Fan et al., 2020) which is trained on non-English centric translation. It can translate to and from all Indic languages in our task, except Telugu. As expected, with no finetuning, both the small (418M parameters) and large (1.2B parameters) models perform po"
2021.wat-1.24,2020.wildre-1.3,0,0.0793484,"Missing"
2021.wat-1.24,W12-5611,0,0.0720034,"Missing"
2021.wat-1.24,2020.tacl-1.18,0,0.0341832,"use pretrained transformer-based language models to initialize the encoder and decoder of our seq2seq models. For English, we use standard uncased BERTBase (Devlin et al., 2019) and for Indic languages, we use MuRIL (Khanuja et al., 2021). MuRIL’s architecture is similar to BERT and is pre-trained on 17 Indic languages including all ten required for our translation task. It is pre-trained on publicly available corpora from Wikipedia and Common Crawl. It also uses automatically translated and transliterated data for pre-training. We add cross-attention between the encoder and decoder following Rothe et al. (2020). The model has 375M trainable parameters. When the decoder is multilingual, we follow previous works and force a language identifier as the BOS token. We use a learning rate of 5 × 10−5 and a batch size of 12. We truncate sequences to a maximum length of 128 and use a cosine learning rate scheduler with a warmup of 10,000 steps. We denote our models as BERT 2 MURIL and MURIL 2 BERT when translating from and to En206 Model m2m100 (418M) m2m100 (1.2B) Bn Gu Hi Kn Ml Mr Or Pa Ta Te chrF bleu chrF bleu chrF bleu chrF bleu chrF bleu chrF bleu chrF bleu chrF bleu chrF bleu chrF bleu 0.30 2.98 0.11"
2021.wat-1.24,2021.eacl-main.115,0,0.0773519,"Missing"
2021.wat-1.24,P19-1355,0,0.0225546,"utskever et al., 2014; Cho et al., 2014), transformers (Vaswani et al., 2017), and large pre-trained language models (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019), the accuracy of machine translation models has almost risen to that of humans (Wu et al., 2016). Yet, the ability to train such models is limited by the availability of compute. Today’s state-of-the-art models are trained by industry research labs, using large compute infrastructure which is usually unavailable or unaffordable to others. Such training is also shown to have large carbon footprints (Strubell et al., 2019). In this work, we show that competitive translation performance can be achieved even with limited resources. We first train a statistical MT system that does not require GPUs, as a baseline. Next, we run inference on the best publicly available pre-trained models to benchmark their performance. Finally, we train graph2seq, seq2seq, and text2text models, Language(s) BN , GU , HI , ML , MR , OR , PA , TA , TE BN , GU , HI , KN , ML , MR , PA , TA , TE BN , GU , HI , KN , ML , MR , PA , TA , TE BN , GU , HI , KN , ML , MR , OR , PA , TA , TE GU , HI , KN , ML , MR , TE BN , HI , ML , TA , TE HI"
2021.wat-1.24,L16-1249,0,0.0460581,"Missing"
2021.wat-1.24,tiedemann-2012-parallel,0,0.0128437,"maximum of 100 hours and get within 4-5 BLEU points of the top submission on the WAT 2021 leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of current translation metrics. 1 Bible-uedin (2014) OpenSubtitles (2016) WikiMatrix (2019) Wiki Titles (2021) ALT (2016) IITB 3.0 (2018) NLPC (2020) UFAL EnTam (2012) Uka Tarsadia (2019) MTEnglish2Odia (2018) OdiEnCorp 2.0 (2020) Introduction Machine Translation is one of the few tasks in NLP which has the luxury of data. Due to the efforts of the community over the last two decades (Koehn, 2005; Tiedemann, 2012, 2020), most major languages of the world have millions of translated sentence pairs with English. With the introduction of sequence to sequence models (Sutskever et al., 2014; Cho et al., 2014), transformers (Vaswani et al., 2017), and large pre-trained language models (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019), the accuracy of machine translation models has almost risen to that of humans (Wu et al., 2016). Yet, the ability to train such models is limited by the availability of compute. Today’s state-of-the-art models are trained by industry research lab"
2021.wat-1.24,L16-1559,0,0.0351865,"Missing"
2021.wat-1.24,2020.wmt-1.139,0,0.0488995,"Missing"
2021.wat-1.24,zhang-etal-2004-interpreting,0,0.0190398,"y shows that the model would become more accurate if it is trained for a longer period or with more compute. To establish whether an increase in BLEU scores corresponds to an increase in the fluency and faithfulness of the translations, we manually annotate 50 Hindi and Kannada test predictions from the best model to find that the increase in both cases is marginal. In the 20 additional training hours, the fluency and faithfulness increased by 0.005 and 0.01 respectively which suggests that BLEU may not be the best metric to quantify the goodness of translation systems, as shown in works like Zhang et al. (2004); Callison-Burch et al. (2006). 6 Conclusion In this work, we show that it is possible to get competitive translation results using a single GPU for a limited amount of time by carefully selecting and training large pre-trained encoder-decoder models. We also show that we can train models which have more than 109 trainable parameters using the latest advances in GPU resource optimization. Finally, through a small empirical study, we find that while longer training can increase BLEU scores, it may not increase their fluency and faithfulness. Acknowledgements We thank the reviewers for their val"
2021.wnut-1.14,W09-1206,0,0.183534,"ept of conventional associations, which in many ways are Memory babysat the abstract concept of Reasonindistinguishable from common sense (Trinh and ing. The literal reading represents an unlikely state 114 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 114–119 November 11, 2021. ©2021 Association for Computational Linguistics names countries abstract random-nouns random-strings      Mary Sweden Memory   apples   asdfabv       Tom     Senegal Reasoning babysat       oranges   sadlfkj      Ref Model     Björkelund et al. (2009) Stanovsky et al. (2018) Shi and Lin (2019) MST/MIRA LSTM/GloVe BERT Figure 2: Examples of transitive sentences with person names, country names, abstract nouns, (randomly chosen) plural common nouns, or random strings as arguments. Person names, and to some degree country names (which are often personified (Wang, 2020)), align with expectations of animacy. of affairs, since abstract concepts generally do not have the capacity of babysitting. Obviously, this does not prevent language users from uttering the sentence, and it is, for most of us, not hard to make sense of it: The sentence, for ex"
2021.wnut-1.14,I11-1021,0,0.0112385,"ural parsers, there is no correlation, however. We ran a syntactic parser (Dozat et al., 2017) on our three-word sentences and correlated errors across lemmata. We observed a small, but negative correlation between error rates. 4 Fine-Grained Error Analysis Multitude of Errors Our first observation is that across all verb lemmata, the parser in Shi and Lin (2019) produces many different output trees, depending on the argument word forms. For some lemmata, the error distribution is near-uniform across 15-20 outputs. It is well-established in SRL that infrequent contexts lead to low confidence (Chen et al., 2011), explaining why common sense bias leads to a multitude of errors. Morphosyntactic Ambiguity While parsing errors do not correlate with errors of Shi and Lin (2019) (§3), the SRL system seems to be sensitive to part-of-speech ambiguity. It errs, for example, on Insomnia trips jaywalking, but not on Insomnia tripped jaywalking, presumably because trips is (on its own) ambiguous.8 Sensitivity to such ambiguities disappears when aligning with common sense: The parser does not err on Mary trips John or Mary likes jaywalking. The same ambiguity leads to error in London trips John., but not in 8 Thi"
2021.wnut-1.14,N19-1423,0,0.158842,"and also correlate with demographics, since groups differ in how much they engage with counterfactual and fictitious contents. Semantic role labeling (SRL) refers to a shallow semantic dependency parsing that returns predicate-argument structures for input sentences; see Figure 1. Modern-day SRL systems, like most other NLP technologies, rely heavily on largescale language models. Such language models We compare the errors of a modern, competitive are extremely useful for generalizing to out-of- SRL system (Shi and Lin, 2019), based on BERT vocabulary items, making subtle syntactic distinc- (Devlin et al., 2019), and show how it, unlike eartions, and for capturing a range of lexical ambigui- lier SRL systems, suffers from common sense bias: ties; but they also introduce notable biases. When confronted with sentences that, when read Previous work has shown that SRL systems literally, express unlikely states of affairs, it can exhibit demographic biases (Zhao et al., 2017); ignore obvious cues and produce false predicatewe focus on a form of belief bias (Sternberg and argument structures even for very simple sentences. Leighton, 2004), which we will refer to as common The sentence in Figure 1, for exam"
2021.wnut-1.14,K17-3002,0,0.0135149,"nk that, in line with previous error analyses of SRL systems (He et al., 2017; Strubell et al., 2018), the error distribution can be explained by syntactic ambiguities and resulting syntactic errors. This, perhaps unsurprisingly, turns out to reliably explain the error distribution observed with Björkelund et al. (2009). See Figure 3 for the syntactic parse on Memory babysits Reasoning, on which the log-linear parser fails to deliver any SRL analysis, interpreting the threeword sentence as a nominal compound. For the neural parsers, there is no correlation, however. We ran a syntactic parser (Dozat et al., 2017) on our three-word sentences and correlated errors across lemmata. We observed a small, but negative correlation between error rates. 4 Fine-Grained Error Analysis Multitude of Errors Our first observation is that across all verb lemmata, the parser in Shi and Lin (2019) produces many different output trees, depending on the argument word forms. For some lemmata, the error distribution is near-uniform across 15-20 outputs. It is well-established in SRL that infrequent contexts lead to low confidence (Chen et al., 2011), explaining why common sense bias leads to a multitude of errors. Morphosyn"
2021.wnut-1.14,P17-1044,0,0.18438,"r experiments show that while SRL performance numbers have gone up dramatically in recent years, parsers seem to have become more sensitive to it. F1 0.803 0.823 0.888 Table 1: The three SRL systems used below and their performance on the CoNLL 2005 benchmark tic categories lead to errors for what verbs; and (d) how the BERT-based system suffers from common sense bias. Finally, we create a 1000-sentence challenge dataset for probing SRL for common sense bias. Our error analyses paint a complementary, yet entirely different picture of what SRL systems can and cannot, compared to previous work (He et al., 2017; Strubell et al., 2018), which has focused on long-distance dependencies and the need for syntax. 2 Semantic Role Labeling Systems Björkelund et al. (2009) combine three logistic regression classifiers with beam search and a global reranker: the first classifier identifies predicates, the second their arguments, and the third labels the semantic dependencies between predicates and their arguments. The system relies on a POS tagger and a syntactic dependency parser to generate features for the classifiers. This system had the secondbest performance in the CoNLL 2009 Shared Task. Stanovsky et a"
2021.wnut-1.14,N06-2015,0,0.0352131,"onsists of comparing performance across different types of subjects and objects and comprises examples such as those in Figure 2. The arguments exhibit various degrees of animacy associations, aligning more or less with common sense expectations. We obtain the names from the NAMES library,1 the country names from WorldMap,2 the abstract nouns from YourDictionary,3 and common nouns from the Princeton WordNet.4 We assume a correct semantic parse associates subject with A0 and object with A1 (of the predicate introduced by the verb). This is obviously not true for all verbs (Palmer et al., 2005; Hovy et al., 2006). In Table 2, we list verbs that frequently associate subjects and objects with other arguments (fails and calls), as well as verbs that are very ambiguous and easily mistaken for nouns (trips and tips). Both phenomena are reflected in the distribution of analyses for Shi and Lin (2019). While much can be said about these verbs, our main contribution here is highlighting the role of common sense bias in some SRL parsers, and we thus focus on verbs where we can safely assume a A0 V A1 reading is correct (such as babyslams and babysits).5 Error analysis results are presented in Table 3. If 1 htt"
2021.wnut-1.14,C08-1084,0,0.0159408,"ude of errors. Morphosyntactic Ambiguity While parsing errors do not correlate with errors of Shi and Lin (2019) (§3), the SRL system seems to be sensitive to part-of-speech ambiguity. It errs, for example, on Insomnia trips jaywalking, but not on Insomnia tripped jaywalking, presumably because trips is (on its own) ambiguous.8 Sensitivity to such ambiguities disappears when aligning with common sense: The parser does not err on Mary trips John or Mary likes jaywalking. The same ambiguity leads to error in London trips John., but not in 8 This is orthogonal to the ambiguity of jaywalking; see Padó et al. (2008) for the analysis of nominal predicates. C OMTE: A Test of Common Sense Bias Our challenge dataset9 C OMTE consists of 1,000 simple, three-word sentences with the same gold analysis: the second word is the predicate, the first word its A0 , the last word its A1 . The predicates are sampled at random from a list of six carefully selected verbs (see §3) that select for animate subjects and objects and consistently prefer these to be A0 and A1 . As before, we combine the verbs with names, countries, abstract nouns, plural common nouns, and random strings. The sentences were simply the first 1,000"
2021.wnut-1.14,W19-4020,0,0.0510903,"Missing"
2021.wnut-1.14,J05-1004,0,0.298283,"The error analysis consists of comparing performance across different types of subjects and objects and comprises examples such as those in Figure 2. The arguments exhibit various degrees of animacy associations, aligning more or less with common sense expectations. We obtain the names from the NAMES library,1 the country names from WorldMap,2 the abstract nouns from YourDictionary,3 and common nouns from the Princeton WordNet.4 We assume a correct semantic parse associates subject with A0 and object with A1 (of the predicate introduced by the verb). This is obviously not true for all verbs (Palmer et al., 2005; Hovy et al., 2006). In Table 2, we list verbs that frequently associate subjects and objects with other arguments (fails and calls), as well as verbs that are very ambiguous and easily mistaken for nouns (trips and tips). Both phenomena are reflected in the distribution of analyses for Shi and Lin (2019). While much can be said about these verbs, our main contribution here is highlighting the role of common sense bias in some SRL parsers, and we thus focus on verbs where we can safely assume a A0 V A1 reading is correct (such as babyslams and babysits).5 Error analysis results are presented"
2021.wnut-1.14,D14-1162,0,0.0913291,"e need for syntax. 2 Semantic Role Labeling Systems Björkelund et al. (2009) combine three logistic regression classifiers with beam search and a global reranker: the first classifier identifies predicates, the second their arguments, and the third labels the semantic dependencies between predicates and their arguments. The system relies on a POS tagger and a syntactic dependency parser to generate features for the classifiers. This system had the secondbest performance in the CoNLL 2009 Shared Task. Stanovsky et al. (2018) rely on a standard recurrent architecture. They use GloVe embeddings (Pennington et al., 2014), in conjunction with embeddings from a POS tagger, and stack bidirectional LSTM layers (Hochreiter and Schmidhuber, 1997) on top of the embedding layer. The representation at each time-step is passed to a classifier, which directly predicts the output label for that time-step. Unlike Björkelund et al. (2009), they do not rely on search over possible output combinations. Shi and Lin (2019) also do not rely on search, but reduce SRL to two-stage sequence labeling, both stages pretrained with BERT-large (Devlin et al., 2019); first identifying predicates, then arguments, while conditioning on th"
2021.wnut-1.14,N18-1081,0,0.0925665,"iations, which in many ways are Memory babysat the abstract concept of Reasonindistinguishable from common sense (Trinh and ing. The literal reading represents an unlikely state 114 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 114–119 November 11, 2021. ©2021 Association for Computational Linguistics names countries abstract random-nouns random-strings      Mary Sweden Memory   apples   asdfabv       Tom     Senegal Reasoning babysat       oranges   sadlfkj      Ref Model     Björkelund et al. (2009) Stanovsky et al. (2018) Shi and Lin (2019) MST/MIRA LSTM/GloVe BERT Figure 2: Examples of transitive sentences with person names, country names, abstract nouns, (randomly chosen) plural common nouns, or random strings as arguments. Person names, and to some degree country names (which are often personified (Wang, 2020)), align with expectations of animacy. of affairs, since abstract concepts generally do not have the capacity of babysitting. Obviously, this does not prevent language users from uttering the sentence, and it is, for most of us, not hard to make sense of it: The sentence, for example, could mean someth"
2021.wnut-1.14,D18-1548,0,0.0359277,"Missing"
2021.wnut-1.14,D17-1323,0,0.0282772,"language models. Such language models We compare the errors of a modern, competitive are extremely useful for generalizing to out-of- SRL system (Shi and Lin, 2019), based on BERT vocabulary items, making subtle syntactic distinc- (Devlin et al., 2019), and show how it, unlike eartions, and for capturing a range of lexical ambigui- lier SRL systems, suffers from common sense bias: ties; but they also introduce notable biases. When confronted with sentences that, when read Previous work has shown that SRL systems literally, express unlikely states of affairs, it can exhibit demographic biases (Zhao et al., 2017); ignore obvious cues and produce false predicatewe focus on a form of belief bias (Sternberg and argument structures even for very simple sentences. Leighton, 2004), which we will refer to as common The sentence in Figure 1, for example, can be unsense bias, reflecting how language models encode derstood as expressing that the abstract concept of conventional associations, which in many ways are Memory babysat the abstract concept of Reasonindistinguishable from common sense (Trinh and ing. The literal reading represents an unlikely state 114 Proceedings of the 2021 EMNLP Workshop W-NUT: The"
C08-2025,J07-4003,0,0.0319832,"characterized in terms of an untraditional modifiIt is shown how weighted context-free grammars can be used to recognize languages beyond their weak generative capacity by a one-step constant time extension of standard recognition algorithms. 1 Introduction Weighted context-free grammars (WCFGs) are used to disambiguate strings and thus filter out subsets of the tree languages of the underlying context-free grammars (CFGs). Weights can either be used as probabilities, i.e. higher weights are preferred, or as penalities, i.e. lower weights are preferred. The first convention, also followed by Smith and Johnson (2007), is followed here. The subsets of the tree languages that consist of the heaviest tree for each yield are called the Viterbi tree languages. String languages are the yields of tree languages, and Viterbi string languages are the yields of Viterbi tree languages. Infante-Lopez and de Rijke (2006) show that the Viterbi tree languages strictly extend the tree languages. The idea explored in this paper is simple. If trees must have particular weights for their yields to be recognized, weights can be used to encode non-local dependencies. Technically, the {r1 , . . . , rn }-language is defined as"
C08-2026,W01-1807,0,0.460589,"wn that (2,2)-BRCGs induce inside-out alignments (Wu, 1997) and cross-serial discontinuous translation units (CDTUs); both phenomena can be shown to occur frequently in many hand-aligned parallel corpora. A CYK-style parsing algorithm is introduced, and induction from aligment structures is briefly discussed. Range concatenation grammars (RCG) (Boullier, 1998) mainly attracted attention in the formal language community, since they recognize exactly the polynomial time recognizable languages, but recently they have been argued to be useful for data-driven parsing too (Maier and Søgaard, 2008). Bertsch and Nederhof (2001) present the only work to our knowledge on using RCGs for translation. Both Bertsch and Nederhof (2001) and Maier and Søgaard (2008), however, only make use of so-called simple RCGs, known to be equivalent to linear context-free rewrite systems (LCFRSs) (Weir, 1988; Boullier, 1998). Our strict extension of ITGs, on the other hand, makes use of the ability to copy substrings in RCG derivations; one of the things that makes RCGs strictly more expressive than LCFRSs. Copying enables us to recognize the intersection of any two translations that we can recognize and induce the union c 2008. License"
C08-2026,P06-1123,0,0.280248,"Missing"
C08-2026,J97-3002,0,0.390104,"1 ) → C(cX1 ) → D(dX1 ) → → → → → S0 (X1 , Y1 )S0′ (X1 , Y1 ) S1 (X1 , Y1 )D(X2 ) S1 (X1 , Y1 ) B(X1 )C(Y1 )D(Y2 ) → S1′ (X2 , Y1 )A(X1 ) → S1′ (X1 , Y1 ) → C(X1 )A(Y1 )B(Y2 ) A(X1 ) A(ǫ) → ǫ B(X1 ) B(ǫ) → ǫ C(X1 ) C(ǫ) → ǫ D(X1 ) D(ǫ) → ǫ Note that L(G) = {han bm cn dm , (ab)n (cd)m i | m, n ≥ 0}. 104 Since the component grammars in ITGs are context-free, Example 1.4 shows that there is at least one translation not recognizable by ITGs that is recognized by a (2,2)-BRCG; {an bm cn dm | m, n ≥ 0} is known to be non-context-free. ITGs translate into simple (2,2)-BRCGs in the following way; see Wu (1997) for a definition of ITGs. The left column is ITG production rules; the right column their translations in simple (2,2)-BRCGs. A → [BC] A → hBCi A→e|f A→e|ǫ A→ǫ|f A(X1 X2 , Y1 Y2 ) → B(X1 , Y1 )C(X2 , Y2 ) A(X1 X2 , Y1 Y2 ) → B(X1 , Y2 )C(X2 , Y1 ) A(e, f ) → ǫ A(e, ǫ) → ǫ A(ǫ, f ) → ǫ It follows immediately that Theorem 1.5. (2,2)-BRCGs are strictly more expressive than ITGs. 2 Alignment capacity Zens and Ney (2003) identify a class of alignment structures that cannot be induced by ITGs, but that can be induced by a number of similar synchronous grammar formalisms, e.g. synchronous tree subst"
C08-2026,P03-2041,0,0.161474,"Missing"
C08-2026,graca-etal-2008-building,0,0.143498,"Missing"
C08-2026,P03-1019,0,0.201207,"(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. of any two alignment structures that we can induce. Our extension of ITGs in fact introduces two things: (i) A clause may introduce any number of terminals. This enables us to induce multiword translation units. (ii) A clause may copy a substring, i.e. a clause can associate two or more nonterminals A1 , . . . An with the same substring and thereby check if the substring is in the intersection of the languages of the subgrammars with start predicate names A1 , . . . An . The first point is motivated by studies such as Zens and Ney (2003) and simply reflects that in order to induce multiword translation units in this kind of synchronous grammars, it is useful to be able to introduce multiple terminals simultaneously. The second point gives us a handle on context-sensitivity. It means that (2,2)-BRCGs can define translations such as {han bm cn dm , an bm dm cn i |m, n ≥ 0}, i.e. a translation of cross-serial dependencies into nested ones; but it also means that (2,2)-BRCGs induce a larger class of alignment structures. In fact the set of alignment structures that can be induced is closed under union, i.e. any alignment structur"
C08-2026,P04-1084,0,0.299418,"Missing"
C08-2026,C90-3045,0,0.409928,"Missing"
C08-2026,W90-0102,0,\N,Missing
C10-1120,W06-2920,0,0.0961583,"t-of-domain labeled data. The two parsers parse the in-domain labeled data (reversed, in the case of the SVMbased parser). Identical analyses are added to the original training set. The first parser is retrained and used to parse the test data. In sum, the authors do one round of co-training with the following selection criterion: If the two parsers produce the same dependency structures for a sentence, the dependency structure is added to the labeled data. This criterion is also the selection criterion in tri-training. 3 Experiments 3.1 Data We use five datasets from the CONLL-X Shared Task (Buchholz and Marsi, 2006).1 Lemmas and morphological features (FEATS) are ignored, since we only add POS and CPOS tags to unlabeled data. For German and Swedish, we use 100,000 sentences from the Leipzig Corpora Collection (Biemann et al., 2007) as unlabeled data. For Danish, Dutch, and Portuguese we use 100,000 sentences from the Europarl corpus (Koehn, 2005). The data characteristics are provided in Figure 2. The unlabeled data were POS tagged using the freely available SVMTool (Gimenez and Marquez, 2004) (model 4, leftright-left). 3.2 Algorithm Once our data has been prepared, we train the stacked dependency parser"
C10-1120,W09-4631,0,0.0639475,"in semisupervised learning scenarios, and which can later be combined into dependency trees using parsing algorithms for arc-factored dependency parsing. Our approach thus combines ensemblebased methods and semi-supervised learning. Ensemble-based methods such as stacked learning are used to reduce the instability of classifiers, to average out their errors and to combine the strengths of diverse learning algorithms. Ensemble-based methods have attracted a lot of attention in dependency parsing recently (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Martins et al., 2008; Fishel and Nivre, 2009; Surdeanu and Manning, 2010). Nivre and McDonald (2008) were first to introduce stacking in the context of dependency parsing. Semi-supervised learning is typically motivated by data sparseness. For many classification tasks in natural language processing, labeled data can be in short supply but unlabeled data is more readily available. Semisupervised methods exploit unlabeled data in addition to labeled data to improve performance on classification tasks. If the predictions of a learner l on unlabeled data are used to improve a learner l′ in semi-supervised learning, the robustness of learni"
C10-1120,gimenez-marquez-2004-svmtool,0,0.0414743,"he selection criterion in tri-training. 3 Experiments 3.1 Data We use five datasets from the CONLL-X Shared Task (Buchholz and Marsi, 2006).1 Lemmas and morphological features (FEATS) are ignored, since we only add POS and CPOS tags to unlabeled data. For German and Swedish, we use 100,000 sentences from the Leipzig Corpora Collection (Biemann et al., 2007) as unlabeled data. For Danish, Dutch, and Portuguese we use 100,000 sentences from the Europarl corpus (Koehn, 2005). The data characteristics are provided in Figure 2. The unlabeled data were POS tagged using the freely available SVMTool (Gimenez and Marquez, 2004) (model 4, leftright-left). 3.2 Algorithm Once our data has been prepared, we train the stacked dependency parsers and use them to label training data for our classifiers (∼4,000 tokens), our test data and our unlabeled data. This gives us three sets of predictions for each of the three data sets. Using the features described in Sect. 2.4 we then construct data for training our two triads of classifiers (for attachment and dependency relations). The entire architecture can be depicted as in Figure 3. We first stack three dependency parsers as described in Martins et al. (2008). We then stack t"
C10-1120,W09-3829,0,0.0127125,"ependency tree that reflects the maximum number of votes. They also showed that binning the vote over part-of-speech tags led to further improvements. This set-up was adopted by Hall et al. (2007) in the best performing system in the CONLL 2007 Shared Task. Fishel and Nivre (2009) later experimented with binning the vote on other features with modest improvements. Semi-supervised dependency parsing has only recently been explored, and failures have been more frequent than successes. There are, however, noteable exceptions such as Koo et al. (2008), Wang et al. (2008), Suzuki et al. (2009) and Sagae and Gordon (2009). The semi-supervised methods employed in these experiments are very different from more traditional scenarios such as self-training and cotraining. Two approaches (Koo et al., 2008; Sagae and Gordon, 2009) use clusters obtained from large amounts of unlabeled data to augment their labeled data by introducing new features, and two approaches (Wang et al., 2008; Suzuki et al., 2009) combine probability distributions obtained from labeled data with probability distributions obtained from unlabeled data. Successes with self-training and co-training are rare, and several authors report negative re"
C10-1120,N06-2033,0,0.178615,"multinomial variables, i.e. attachment and labeling decisions, which are easier to manage in semisupervised learning scenarios, and which can later be combined into dependency trees using parsing algorithms for arc-factored dependency parsing. Our approach thus combines ensemblebased methods and semi-supervised learning. Ensemble-based methods such as stacked learning are used to reduce the instability of classifiers, to average out their errors and to combine the strengths of diverse learning algorithms. Ensemble-based methods have attracted a lot of attention in dependency parsing recently (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Martins et al., 2008; Fishel and Nivre, 2009; Surdeanu and Manning, 2010). Nivre and McDonald (2008) were first to introduce stacking in the context of dependency parsing. Semi-supervised learning is typically motivated by data sparseness. For many classification tasks in natural language processing, labeled data can be in short supply but unlabeled data is more readily available. Semisupervised methods exploit unlabeled data in addition to labeled data to improve performance on classification tasks. If the predictions of a learner l on unlabeled"
C10-1120,D07-1111,0,0.583561,"itraining to two classification problems, attachment and labeling. The attachment classifier’s weights are used for arc-factored dependency parsing, and the labeling classifier’s weights are then used to label the dependency tree delivered by the parser. Semi-supervised dependency parsing has attracted a lot of attention recently (Koo et al., 2008; Wang et al., 2008; Suzuki et al., 2009), but there has, to the best of our knowledge, been no previous attempts to apply tri-training or related combinations of ensemble-based and semisupervised methods to any of these tasks, except for the work of Sagae and Tsujii (2007) discussed in Sect. 2.6. However, tri-training has been applied to Chinese chunking (Chen et al., 2006), question classification (Nguyen et al., 2008) and POS tagging (Søgaard, 2010). We compare generalized tri-training to other semi-supervised learning algorithms, incl. selftraining, the original tri-training algorithm based on bootstrap samples (Li and Zhou, 2005), co-forests (Li and Zhou, 2007) and semisupervised support vector machines (Sindhwani and Keerthi, 2006). Sect. 2 introduces dependency parsing and stacked learning. Stacked learning is generalized to dependency parsing, and previo"
C10-1120,2005.mtsummit-papers.11,0,0.00441984,"s produce the same dependency structures for a sentence, the dependency structure is added to the labeled data. This criterion is also the selection criterion in tri-training. 3 Experiments 3.1 Data We use five datasets from the CONLL-X Shared Task (Buchholz and Marsi, 2006).1 Lemmas and morphological features (FEATS) are ignored, since we only add POS and CPOS tags to unlabeled data. For German and Swedish, we use 100,000 sentences from the Leipzig Corpora Collection (Biemann et al., 2007) as unlabeled data. For Danish, Dutch, and Portuguese we use 100,000 sentences from the Europarl corpus (Koehn, 2005). The data characteristics are provided in Figure 2. The unlabeled data were POS tagged using the freely available SVMTool (Gimenez and Marquez, 2004) (model 4, leftright-left). 3.2 Algorithm Once our data has been prepared, we train the stacked dependency parsers and use them to label training data for our classifiers (∼4,000 tokens), our test data and our unlabeled data. This gives us three sets of predictions for each of the three data sets. Using the features described in Sect. 2.4 we then construct data for training our two triads of classifiers (for attachment and dependency relations)."
C10-1120,P08-1068,0,0.275865,"ing scenarios; in particular they embed ensembles in a form of co-training that is shown to maintain the diversity of the ensemble over time. Milidiu and Duarte (2009) generalize boosting at start to semi-supervised learning. This paper applies a generalization of tritraining to two classification problems, attachment and labeling. The attachment classifier’s weights are used for arc-factored dependency parsing, and the labeling classifier’s weights are then used to label the dependency tree delivered by the parser. Semi-supervised dependency parsing has attracted a lot of attention recently (Koo et al., 2008; Wang et al., 2008; Suzuki et al., 2009), but there has, to the best of our knowledge, been no previous attempts to apply tri-training or related combinations of ensemble-based and semisupervised methods to any of these tasks, except for the work of Sagae and Tsujii (2007) discussed in Sect. 2.6. However, tri-training has been applied to Chinese chunking (Chen et al., 2006), question classification (Nguyen et al., 2008) and POS tagging (Søgaard, 2010). We compare generalized tri-training to other semi-supervised learning algorithms, incl. selftraining, the original tri-training algorithm base"
C10-1120,P10-2038,1,0.833604,"then used to label the dependency tree delivered by the parser. Semi-supervised dependency parsing has attracted a lot of attention recently (Koo et al., 2008; Wang et al., 2008; Suzuki et al., 2009), but there has, to the best of our knowledge, been no previous attempts to apply tri-training or related combinations of ensemble-based and semisupervised methods to any of these tasks, except for the work of Sagae and Tsujii (2007) discussed in Sect. 2.6. However, tri-training has been applied to Chinese chunking (Chen et al., 2006), question classification (Nguyen et al., 2008) and POS tagging (Søgaard, 2010). We compare generalized tri-training to other semi-supervised learning algorithms, incl. selftraining, the original tri-training algorithm based on bootstrap samples (Li and Zhou, 2005), co-forests (Li and Zhou, 2007) and semisupervised support vector machines (Sindhwani and Keerthi, 2006). Sect. 2 introduces dependency parsing and stacked learning. Stacked learning is generalized to dependency parsing, and previous work is briefly surveyed. We then describe how stacked dependency parsers can be further stacked as input for two end classifiers that can be combined to produce dependency struct"
C10-1120,N06-1020,0,0.116802,"cenarios such as self-training and cotraining. Two approaches (Koo et al., 2008; Sagae and Gordon, 2009) use clusters obtained from large amounts of unlabeled data to augment their labeled data by introducing new features, and two approaches (Wang et al., 2008; Suzuki et al., 2009) combine probability distributions obtained from labeled data with probability distributions obtained from unlabeled data. Successes with self-training and co-training are rare, and several authors report negative results, e.g. Spreyer and Kuhn (2009). A noteable exception in constituent-based parsing is the work of McClosky et al. (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. Sagae and Tsujii (2007) participated in (and won) the CONLL 2007 Shared Task on domain adaptation. They first trained a maximum entropy-based transition-based dependency parser on the out-of-domain labeled data and an SVM-based transition-based dependency parser on the reversed out-of-domain labeled data. The two parsers parse the in-domain labeled data (reversed, in the case of the SVMbased parser). Identical analyses are added to the original training set. The first parser is retrained and used to"
C10-1120,H05-1066,0,0.111051,"Missing"
C10-1120,W09-1104,0,0.0553813,"upervised methods employed in these experiments are very different from more traditional scenarios such as self-training and cotraining. Two approaches (Koo et al., 2008; Sagae and Gordon, 2009) use clusters obtained from large amounts of unlabeled data to augment their labeled data by introducing new features, and two approaches (Wang et al., 2008; Suzuki et al., 2009) combine probability distributions obtained from labeled data with probability distributions obtained from unlabeled data. Successes with self-training and co-training are rare, and several authors report negative results, e.g. Spreyer and Kuhn (2009). A noteable exception in constituent-based parsing is the work of McClosky et al. (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. Sagae and Tsujii (2007) participated in (and won) the CONLL 2007 Shared Task on domain adaptation. They first trained a maximum entropy-based transition-based dependency parser on the out-of-domain labeled data and an SVM-based transition-based dependency parser on the reversed out-of-domain labeled data. The two parsers parse the in-domain labeled data (reversed, in the case of the SVMbased parser). Identical a"
C10-1120,N10-1091,0,0.0324686,"ng scenarios, and which can later be combined into dependency trees using parsing algorithms for arc-factored dependency parsing. Our approach thus combines ensemblebased methods and semi-supervised learning. Ensemble-based methods such as stacked learning are used to reduce the instability of classifiers, to average out their errors and to combine the strengths of diverse learning algorithms. Ensemble-based methods have attracted a lot of attention in dependency parsing recently (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Martins et al., 2008; Fishel and Nivre, 2009; Surdeanu and Manning, 2010). Nivre and McDonald (2008) were first to introduce stacking in the context of dependency parsing. Semi-supervised learning is typically motivated by data sparseness. For many classification tasks in natural language processing, labeled data can be in short supply but unlabeled data is more readily available. Semisupervised methods exploit unlabeled data in addition to labeled data to improve performance on classification tasks. If the predictions of a learner l on unlabeled data are used to improve a learner l′ in semi-supervised learning, the robustness of learning will depend on the stabili"
C10-1120,P08-1061,0,0.164938,"particular they embed ensembles in a form of co-training that is shown to maintain the diversity of the ensemble over time. Milidiu and Duarte (2009) generalize boosting at start to semi-supervised learning. This paper applies a generalization of tritraining to two classification problems, attachment and labeling. The attachment classifier’s weights are used for arc-factored dependency parsing, and the labeling classifier’s weights are then used to label the dependency tree delivered by the parser. Semi-supervised dependency parsing has attracted a lot of attention recently (Koo et al., 2008; Wang et al., 2008; Suzuki et al., 2009), but there has, to the best of our knowledge, been no previous attempts to apply tri-training or related combinations of ensemble-based and semisupervised methods to any of these tasks, except for the work of Sagae and Tsujii (2007) discussed in Sect. 2.6. However, tri-training has been applied to Chinese chunking (Chen et al., 2006), question classification (Nguyen et al., 2008) and POS tagging (Søgaard, 2010). We compare generalized tri-training to other semi-supervised learning algorithms, incl. selftraining, the original tri-training algorithm based on bootstrap samp"
C10-1120,W05-1518,0,0.141,"Missing"
C10-1120,W09-3838,0,0.0374209,"Missing"
C10-1120,P08-1108,0,0.232678,"labeling decisions, which are easier to manage in semisupervised learning scenarios, and which can later be combined into dependency trees using parsing algorithms for arc-factored dependency parsing. Our approach thus combines ensemblebased methods and semi-supervised learning. Ensemble-based methods such as stacked learning are used to reduce the instability of classifiers, to average out their errors and to combine the strengths of diverse learning algorithms. Ensemble-based methods have attracted a lot of attention in dependency parsing recently (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Martins et al., 2008; Fishel and Nivre, 2009; Surdeanu and Manning, 2010). Nivre and McDonald (2008) were first to introduce stacking in the context of dependency parsing. Semi-supervised learning is typically motivated by data sparseness. For many classification tasks in natural language processing, labeled data can be in short supply but unlabeled data is more readily available. Semisupervised methods exploit unlabeled data in addition to labeled data to improve performance on classification tasks. If the predictions of a learner l on unlabeled data are used to improve a learner l′ in semi"
C10-1120,D07-1097,0,\N,Missing
C10-1120,D08-1017,0,\N,Missing
C12-2114,P11-2071,0,0.0190913,"on (Dredze et al., 2010; McClosky et al., 2010), but this still requires that we know the possible domains in advance and are able to relate each instance to one of them, and in many cases we do not. If the possible target domains are not known in advance, the transfer learning problem reduces to the problem of learning robust models that are as insensitive as possible to domain shifts. This is the problem considered in this paper. One of the main reasons for performance drops when evaluating supervised NLP models on out-of-domain data is out-of-vocabulary (OOV) effects (Blitzer et al., 2007; Daumé and Jagarlamudi, 2011). Several techniques for reducing OOV effects have been introduced in the literature, including spelling expansion, morphological expansion, dictionary term expansion, proper name transliteration, correlation analysis, and word clustering (Blitzer et al., 2007; Habash, 2008; Turian et al., 2010; Daumé and Jagarlamudi, 2011), but most of these techniques still leave us with a lot of ""empty dimensions"", i.e. features that are always 0 in the test data. While these features are not uninstantiated in the sense of missing values, we will nevertheless refer to OOV effects as removing dimensions from"
C12-2114,D10-1057,0,0.0473415,"rget domain is unknown. The assumption that a large pool of unlabeled data is available from a relatively homogeneous target domain holds only if the target domain is known in advance. In a lot of applications of NLP, this is not the case. When we design publicly available software such as the Stanford Parser, or when we set up online services such as Google Translate, we do not know much about the input in advance. A user will apply the Stanford Parser to any kind of text from any textual domain and expect it to do well.1 Recent work has extended domain adaptation with domain identification (Dredze et al., 2010; McClosky et al., 2010), but this still requires that we know the possible domains in advance and are able to relate each instance to one of them, and in many cases we do not. If the possible target domains are not known in advance, the transfer learning problem reduces to the problem of learning robust models that are as insensitive as possible to domain shifts. This is the problem considered in this paper. One of the main reasons for performance drops when evaluating supervised NLP models on out-of-domain data is out-of-vocabulary (OOV) effects (Blitzer et al., 2007; Daumé and Jagarlamudi,"
C12-2114,P08-2015,0,0.0311467,"uces to the problem of learning robust models that are as insensitive as possible to domain shifts. This is the problem considered in this paper. One of the main reasons for performance drops when evaluating supervised NLP models on out-of-domain data is out-of-vocabulary (OOV) effects (Blitzer et al., 2007; Daumé and Jagarlamudi, 2011). Several techniques for reducing OOV effects have been introduced in the literature, including spelling expansion, morphological expansion, dictionary term expansion, proper name transliteration, correlation analysis, and word clustering (Blitzer et al., 2007; Habash, 2008; Turian et al., 2010; Daumé and Jagarlamudi, 2011), but most of these techniques still leave us with a lot of ""empty dimensions"", i.e. features that are always 0 in the test data. While these features are not uninstantiated in the sense of missing values, we will nevertheless refer to OOV effects as removing dimensions from our datasets, since a subset of dimensions become uninformative as we leave our source domain. This is a potential source of error, since the best decision boundary in n dimensions is not necessarily the best boundary in m < n dimensions. If we remove dimensions, our optim"
C12-2114,N10-1004,0,0.0166792,"n. The assumption that a large pool of unlabeled data is available from a relatively homogeneous target domain holds only if the target domain is known in advance. In a lot of applications of NLP, this is not the case. When we design publicly available software such as the Stanford Parser, or when we set up online services such as Google Translate, we do not know much about the input in advance. A user will apply the Stanford Parser to any kind of text from any textual domain and expect it to do well.1 Recent work has extended domain adaptation with domain identification (Dredze et al., 2010; McClosky et al., 2010), but this still requires that we know the possible domains in advance and are able to relate each instance to one of them, and in many cases we do not. If the possible target domains are not known in advance, the transfer learning problem reduces to the problem of learning robust models that are as insensitive as possible to domain shifts. This is the problem considered in this paper. One of the main reasons for performance drops when evaluating supervised NLP models on out-of-domain data is out-of-vocabulary (OOV) effects (Blitzer et al., 2007; Daumé and Jagarlamudi, 2011). Several technique"
C12-2114,P05-1003,0,0.0251612,"Missing"
C12-2114,N06-1012,0,0.138533,"Missing"
C12-2114,P10-1040,0,0.0466825,"oblem of learning robust models that are as insensitive as possible to domain shifts. This is the problem considered in this paper. One of the main reasons for performance drops when evaluating supervised NLP models on out-of-domain data is out-of-vocabulary (OOV) effects (Blitzer et al., 2007; Daumé and Jagarlamudi, 2011). Several techniques for reducing OOV effects have been introduced in the literature, including spelling expansion, morphological expansion, dictionary term expansion, proper name transliteration, correlation analysis, and word clustering (Blitzer et al., 2007; Habash, 2008; Turian et al., 2010; Daumé and Jagarlamudi, 2011), but most of these techniques still leave us with a lot of ""empty dimensions"", i.e. features that are always 0 in the test data. While these features are not uninstantiated in the sense of missing values, we will nevertheless refer to OOV effects as removing dimensions from our datasets, since a subset of dimensions become uninformative as we leave our source domain. This is a potential source of error, since the best decision boundary in n dimensions is not necessarily the best boundary in m < n dimensions. If we remove dimensions, our optimal decision boundarie"
C12-2114,P07-1056,0,\N,Missing
C12-2115,P10-1131,0,0.0547669,"rameters to the parsing model. In our experiments we consider some possible extensions of this simple model. The extensions are discussed in the following subsections: 4.1 Language-level weighted learning Intuitively some languages are more relevant as source languages for some language than others. While results in the literature show that good source languages may be geographically distant and unrelated to the target language (e.g. Arabic and Danish (Søgaard, 2011)), genealogically related languages should in general be better source languages for each other. This idea was first explored by Berg-Kirkpatrick and Klein (2010) who used genealogical relations to impose contraints on models in multi-lingual grammar inductions. In the experiments below we use a language genealogy to take a weighted average of the models obtained from our set of source languages. Each model is weighted by 4 − d where d is the distance between the source language and a node dominating the target language node in a genealogical tree (see Figure 2). If two languages belong to the same subfamily such as the Western Germanic languages Dutch and German, d = 1, but for Dutch to Greek, for example, d = 3. We also try using a typological databa"
C12-2115,W06-2920,0,0.0790066,"ity per word given a language model trained on a corpus sampled from domain D. 1185 Indo-Eur. Germ. North Germ. West Germ. Da, Sw Du, En Ge Hellenic Italic Slavic Latin Gr Ca, It, Po, Sp Bu, Cz, Sl Figure 2: Language genealogy. 5 Experiments Our parser is a modification of the publicly available implementation of the easy-first parser.1 In our main experiments, we used the English feature model as is with no modifications. This is not entirely meaningful as the feature model refers to POS tags specific to the English Penn Treebank (PTB) (see Discussion). We used the datasets from the CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al., 2007) shared tasks with standard train-test splits, but mapped all POS tags into Google’s universal tag set (Petrov et al., 2011). See the shared task descriptions for dataset characteristics. We used a publicly available language genealogy2 and a publicly available database of typological properties3 to obtain our weights. See Figure 2 for the linguistic genealogy. In the typological table, we disregarded phonological properties (properties 1–20) when computing Hamming distances between languages. We also report results obtained using voting. These results are o"
C12-2115,D11-1005,0,0.155064,"ing text that differs in domain or genre from the available treebank(s). However, there are still many languages for which no treebanks exist, and for which we therefore do not have parsers available. Unsupervised parsing has seen considerable progress over the last ten years (Gelling et al., 2012), but recently several authors have demonstrated that better results can be achieved transferring linguistic knowledge from treebanks from other languages rather than inducing this knowledge from unannotated text (Zeman and Resnik, 2008; Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). Some of these authors have projected syntactic structures across word aligned parallel text (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; McDonald et al., 2011), while others have used a much simpler technique sometimes referred to as delexicalized transfer (Zeman and Resnik, 2008; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). This work presents a new approach to delexicalized transfer and explores possible improvements. Sect. 2 covers related work on delexicalize"
C12-2115,D12-1001,0,0.130543,"elexicalized transfer averaging across several languages. They also were the first to explicitly introduce the idea of using multiple source languages as a kind of regularization. Finally, Cohen et al. (2011) used the delexicalized transfer models to initialize unsupervised parameter estimation on unlabeled target data. Naseem et al. (2012) subsequently explored a more complex transfer model where only hierarchical information is transferred directly to reflect that languages have very different word orders. Täckström et al. (2012) augment delexicalized transfer with bilingual clusters, while Durrett et al. (2012) use a bilingual dictionary to project lexical features. Täckström (2012) used self-training to supply the bilingual word clusters with monolingual clusters, but evaluated the idea on named entity recognition rather than cross-language parser adaptation. 3 Easy-first transition-based parsing with averaged perceptron The parser we apply in this study is a non-directional easy-first transition-based dependency parser (Goldberg and Elhadad, 2010). The parser consecutively applies one of two actions, ATTACH LEFT(i) and ATTACH RIGHT(i), to a list of partial structures initialized as the words in th"
C12-2115,W12-1909,0,0.0169523,"ogies such as question-answering, machine translation between distant languages, and sentiment analysis. State-of-the-art parsers provide accurate syntactic analyses in languages for which annotated resources known as treebanks exist, although significant and sometimes prohibitive performance drops are observed when parsing text that differs in domain or genre from the available treebank(s). However, there are still many languages for which no treebanks exist, and for which we therefore do not have parsers available. Unsupervised parsing has seen considerable progress over the last ten years (Gelling et al., 2012), but recently several authors have demonstrated that better results can be achieved transferring linguistic knowledge from treebanks from other languages rather than inducing this knowledge from unannotated text (Zeman and Resnik, 2008; Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). Some of these authors have projected syntactic structures across word aligned parallel text (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; McDonald et al., 2011), while others have used a much simpler techniq"
C12-2115,N10-1115,0,0.279702,"parallel text (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; McDonald et al., 2011), while others have used a much simpler technique sometimes referred to as delexicalized transfer (Zeman and Resnik, 2008; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). This work presents a new approach to delexicalized transfer and explores possible improvements. Sect. 2 covers related work on delexicalized transfer for cross-language parser adaptation. We explore delexicalized transfer in the context of easy-first transition-based dependency parsing (Goldberg and Elhadad, 2010), which is introduced in Sect. 3. Sect. 4 introduces our implementation of delexicalized transfer which differs from other approaches by doing model averaging of under-fitted models rather than concatenating data when learning from multiple source languages. Sect. 4 also introduces the possible improvements of this model we explore in our experiments. Sect. 5 and 6 present the experiments and results. 2 Cross-language adaptation with delexicalized transfer Delexicalized transfer refers to a simple idea first introduced in Zeman and Resnik (2008). In unsupervised parsing, you hope to learn that"
C12-2115,P07-1034,0,0.0453831,"is from the Bulgarian treebank, and the right one from the German. However, the left structure contains many edges that also occur in the right structure, e.g. from root to verb, from verb to noun, and from adposition to noun. A dependency parser can learn such dependencies are likely in Bulgarian, but apply this knowledge when parsing German. Independently of each other, three papers revisited the idea of delexicalized transfer in 2011. Søgaard (2011) used the tag set mappings in Zeman and Resnik (2008), but also used instance weighting to do a form of outlier detection. In a way similar to Jiang and Zhai (2007) he did not make use of the actual weights, but simply used all labeled instances with weights greater than some fixed threshold. McDonald et al. (2011) used the more recent tag set mappings by Petrov et al. (2011), explored combinations of delexicalized transfer and structure projection (Smith and Eisner, 2009; Spreyer and Kuhn, 2009) and were able to improve delexicalized transfer averaging across several languages. They also were the first to explicitly introduce the idea of using multiple source languages as a kind of regularization. Finally, Cohen et al. (2011) used the delexicalized tran"
C12-2115,N10-1004,0,0.0343022,"e 3: Results, incl. language-level weighting (gtree and typology), unsupervised model selection (pr), instance-weighted extensions (w–) and a comparison with recent work. AV is macro-average, and AV M is macro-average on the 8 languages used in McDonald et al. (2011). The voted systems (a) and (b) take non-weighted votes over the systems in the above rows. advanced methods in the future. Since we average over languages - which in a domain adaptation setting corresponds to distinct source domains - our model is very similar to multi-source domain adaptation models that use mixtures of experts (McClosky et al., 2010; Spinello and Arras, 2012). In future work we would also like to explore the idea of using smoothness assumptions in the target domain to select models based on source languages (Gao et al., 2008). Another option would be to view multi-source language learning as multi-task learning and apply a multi-task perceptron learning algorithm (Cavallanti et al., 2008) rather than just averaged perceptron learning. On a more technical note, as already mentioned, readers familiar with the easy-first parser may wonder what we did with the POS-tag specific features in the English feature model distribute"
C12-2115,D11-1006,0,0.643654,"s in domain or genre from the available treebank(s). However, there are still many languages for which no treebanks exist, and for which we therefore do not have parsers available. Unsupervised parsing has seen considerable progress over the last ten years (Gelling et al., 2012), but recently several authors have demonstrated that better results can be achieved transferring linguistic knowledge from treebanks from other languages rather than inducing this knowledge from unannotated text (Zeman and Resnik, 2008; Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). Some of these authors have projected syntactic structures across word aligned parallel text (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; McDonald et al., 2011), while others have used a much simpler technique sometimes referred to as delexicalized transfer (Zeman and Resnik, 2008; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). This work presents a new approach to delexicalized transfer and explores possible improvements. Sect. 2 covers related work on delexicalized transfer for cross-la"
C12-2115,P12-1066,0,0.707303,"k(s). However, there are still many languages for which no treebanks exist, and for which we therefore do not have parsers available. Unsupervised parsing has seen considerable progress over the last ten years (Gelling et al., 2012), but recently several authors have demonstrated that better results can be achieved transferring linguistic knowledge from treebanks from other languages rather than inducing this knowledge from unannotated text (Zeman and Resnik, 2008; Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). Some of these authors have projected syntactic structures across word aligned parallel text (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; McDonald et al., 2011), while others have used a much simpler technique sometimes referred to as delexicalized transfer (Zeman and Resnik, 2008; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). This work presents a new approach to delexicalized transfer and explores possible improvements. Sect. 2 covers related work on delexicalized transfer for cross-language parser adaptation. We explore delexical"
C12-2115,D10-1120,0,0.0671532,"tenating data when learning from multiple source languages. Sect. 4 also introduces the possible improvements of this model we explore in our experiments. Sect. 5 and 6 present the experiments and results. 2 Cross-language adaptation with delexicalized transfer Delexicalized transfer refers to a simple idea first introduced in Zeman and Resnik (2008). In unsupervised parsing, you hope to learn that nouns tend to attach to verbs, determiners tend to attach to nouns, adverbs to verbs, etc. Many of these tendencies are cross-linguistic tendencies, a fact also exploited in unsupervised parsing by Naseem et al. (2010) and Søgaard (2012). Since this knowledge is reflected in most treebanks, can’t we extract this knowledge from a treebank in one language and apply the resulting model to another language for which we do not have a treebank? There are certainly differences between distant languages in, for example, how likely adjectives are to modify adverbs rather than nouns, but as mentioned in McDonald et al. (2011) using multiple source languages may reduce such biases on average. Zeman and Resnik (2008), in their seminal paper, considered a pair of closely related languages, namely Danish and Swedish. The"
C12-2115,N06-2033,0,0.335304,"andard train-test splits, but mapped all POS tags into Google’s universal tag set (Petrov et al., 2011). See the shared task descriptions for dataset characteristics. We used a publicly available language genealogy2 and a publicly available database of typological properties3 to obtain our weights. See Figure 2 for the linguistic genealogy. In the typological table, we disregarded phonological properties (properties 1–20) when computing Hamming distances between languages. We also report results obtained using voting. These results are obtained using the reparsing technique first described in Sagae and Lavie (2006) with our various weighted parsers as committee members. 6 Results We note that our macro-average results are the best reported results for a fully delexicalized model, i.e. without lexical projections or bilingual word clusters. That being said recent results show that using cross-language projection or bilingual clustering to obtain lexical knowledge is beneficial, and we will explore different ways of augmenting the models presented here with such knowledge. 7 Discussion We have tried to prevent over-fitting doing only a single pass over the data. While the averaged perceptron is less prone"
C12-2115,P07-1096,0,0.0208535,"as the words in the sentence. Each action connects the heads of two neighboring structures, making one the head of the other. The dependent partial structure is removed from the list. The parsing algorithm is obviously projective. The next action is chosen by a score function score(ACTION)(i) that assigns a weight to all pairs of actions and locations. The scoring function ideally ranks possible actions from easy to hard. The scoring function is learned from data using a variant of the averaged perceptron learning algorithm (Freund and Schapire, 1999; Collins, 2002) similar to the one used in Shen et al. (2007). While the ordering from easy to hard is not known in advance, the ordering is implicitly learned by decreasing weights associated with invalid actions and increasing weights associated with the currently highest scoring valid action. The major advantage of using easy-first parsing is the efficient O (n log n) parsing algorithm, but training is also a lot faster than with comparable dependency parsers (Goldberg and Elhadad, 2010); e.g. training an experiment for Spanish-Italian on a Macbook Pro takes less than a 1183 minute. The easy-first learning algorithm is used throughout our experiments"
C12-2115,D09-1086,0,0.256168,"sometimes prohibitive performance drops are observed when parsing text that differs in domain or genre from the available treebank(s). However, there are still many languages for which no treebanks exist, and for which we therefore do not have parsers available. Unsupervised parsing has seen considerable progress over the last ten years (Gelling et al., 2012), but recently several authors have demonstrated that better results can be achieved transferring linguistic knowledge from treebanks from other languages rather than inducing this knowledge from unannotated text (Zeman and Resnik, 2008; Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). Some of these authors have projected syntactic structures across word aligned parallel text (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; McDonald et al., 2011), while others have used a much simpler technique sometimes referred to as delexicalized transfer (Zeman and Resnik, 2008; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). This work presents a new approach to delexicalized transfer and explores possibl"
C12-2115,P11-2120,1,0.930721,"erved when parsing text that differs in domain or genre from the available treebank(s). However, there are still many languages for which no treebanks exist, and for which we therefore do not have parsers available. Unsupervised parsing has seen considerable progress over the last ten years (Gelling et al., 2012), but recently several authors have demonstrated that better results can be achieved transferring linguistic knowledge from treebanks from other languages rather than inducing this knowledge from unannotated text (Zeman and Resnik, 2008; Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). Some of these authors have projected syntactic structures across word aligned parallel text (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; McDonald et al., 2011), while others have used a much simpler technique sometimes referred to as delexicalized transfer (Zeman and Resnik, 2008; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). This work presents a new approach to delexicalized transfer and explores possible improvements. Sect. 2 covers related"
C12-2115,W11-2906,1,0.845186,"sentences with highest perplexity per word according to a target language model. We go beyond Søgaard (2011) by learning from weighted data, where each weight estimates the relevance of a labeled sentence by the perplexity by word of the corresponding POS sequence in a target language model over its perplexity by word in a source language model. In weighted perceptron learning (Cavallanti et al., 2006), we make the learning rate dependent on the current instance, using the following update rule on xn : wi+1 ← wi + βn α( yn − sign(wi · xn ))xn (1) where β1 , . . . , βn are importance weights. Søgaard and Haulrich (2011) present an application of an importance weighted version of the MIRA algorithm (Crammer and Singer, 2003) and apply it to dependency parsing. Huang et al. (2007) present an instance-weighted learning algorithm for support vector machines. Under the assumption that differences between source and target distributions are due to sample bias only (which is clearly not the case here) we should weight a data point by its probability in the target domain over its probability in the source domain (Shimodaira, 2000), but since it is not possible to estimate densities in our case, we resort to a heuris"
C12-2115,W09-1104,0,0.266029,"erformance drops are observed when parsing text that differs in domain or genre from the available treebank(s). However, there are still many languages for which no treebanks exist, and for which we therefore do not have parsers available. Unsupervised parsing has seen considerable progress over the last ten years (Gelling et al., 2012), but recently several authors have demonstrated that better results can be achieved transferring linguistic knowledge from treebanks from other languages rather than inducing this knowledge from unannotated text (Zeman and Resnik, 2008; Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). Some of these authors have projected syntactic structures across word aligned parallel text (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; McDonald et al., 2011), while others have used a much simpler technique sometimes referred to as delexicalized transfer (Zeman and Resnik, 2008; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). This work presents a new approach to delexicalized transfer and explores possible improvements. Sect. 2"
C12-2115,W12-1908,0,0.0295718,"irst to explicitly introduce the idea of using multiple source languages as a kind of regularization. Finally, Cohen et al. (2011) used the delexicalized transfer models to initialize unsupervised parameter estimation on unlabeled target data. Naseem et al. (2012) subsequently explored a more complex transfer model where only hierarchical information is transferred directly to reflect that languages have very different word orders. Täckström et al. (2012) augment delexicalized transfer with bilingual clusters, while Durrett et al. (2012) use a bilingual dictionary to project lexical features. Täckström (2012) used self-training to supply the bilingual word clusters with monolingual clusters, but evaluated the idea on named entity recognition rather than cross-language parser adaptation. 3 Easy-first transition-based parsing with averaged perceptron The parser we apply in this study is a non-directional easy-first transition-based dependency parser (Goldberg and Elhadad, 2010). The parser consecutively applies one of two actions, ATTACH LEFT(i) and ATTACH RIGHT(i), to a list of partial structures initialized as the words in the sentence. Each action connects the heads of two neighboring structures,"
C12-2115,N12-1052,0,0.163834,"om the available treebank(s). However, there are still many languages for which no treebanks exist, and for which we therefore do not have parsers available. Unsupervised parsing has seen considerable progress over the last ten years (Gelling et al., 2012), but recently several authors have demonstrated that better results can be achieved transferring linguistic knowledge from treebanks from other languages rather than inducing this knowledge from unannotated text (Zeman and Resnik, 2008; Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). Some of these authors have projected syntactic structures across word aligned parallel text (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; McDonald et al., 2011), while others have used a much simpler technique sometimes referred to as delexicalized transfer (Zeman and Resnik, 2008; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). This work presents a new approach to delexicalized transfer and explores possible improvements. Sect. 2 covers related work on delexicalized transfer for cross-language parser adaptation"
C12-2115,I08-3008,0,0.393564,"although significant and sometimes prohibitive performance drops are observed when parsing text that differs in domain or genre from the available treebank(s). However, there are still many languages for which no treebanks exist, and for which we therefore do not have parsers available. Unsupervised parsing has seen considerable progress over the last ten years (Gelling et al., 2012), but recently several authors have demonstrated that better results can be achieved transferring linguistic knowledge from treebanks from other languages rather than inducing this knowledge from unannotated text (Zeman and Resnik, 2008; Smith and Eisner, 2009; Spreyer and Kuhn, 2009; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). Some of these authors have projected syntactic structures across word aligned parallel text (Smith and Eisner, 2009; Spreyer and Kuhn, 2009; McDonald et al., 2011), while others have used a much simpler technique sometimes referred to as delexicalized transfer (Zeman and Resnik, 2008; Søgaard, 2011; Cohen et al., 2011; McDonald et al., 2011; Täckström et al., 2012; Naseem et al., 2012). This work presents a new approach to delexicalized trans"
C12-2115,petrov-etal-2012-universal,0,\N,Missing
C12-2115,D07-1096,0,\N,Missing
C14-1168,I13-1041,0,0.0122948,"Missing"
C14-1168,P11-1040,0,0.0233001,"Missing"
C14-1168,D07-1074,0,0.00844979,"tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching of snippets and search queries, giving priority to n-gram matches with larger n. Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less spelling variation than tweets. In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama and Torisawa, 2007; Cucerzan, 2007). R¨ud et al. (2011) consider using search engines for distant supervision of NER of search queries. Their set-up is very similar to Ganchev et al. (2012), except they do not use click-through data. They use the search engine snippets to generate feature representations rather than projections. Want et al. (2013) also use distant supervision for NER, i.e., Wikipedia page view counts, 1790 applying their model to Twitter data, but their results are considerably below the state of the art. Also, their source of supervision is not linked to the individual tweets in the way mentioned websites are."
C14-1168,P11-1061,0,0.03304,", 2013; Derczynski et al., 2013). Strictly supervised approaches to analyzing Twitter has the weakness that labeled data quickly becomes unrepresentative of what people write on Twitter. This paper presents results using no in-domain labeled data that are significantly better than several offthe-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled data to reach new highs across several data sets. Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings (Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T¨ackstr¨om et al., 2013). Our type constraints in POS tagging come from tag dictionaries, but also from linked websites. The idea of using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev et al. (2012) for search query tagging. Ganchev et al. (2012), considering the problem of POS tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching"
C14-1168,W10-2608,0,0.0382342,"Missing"
C14-1168,R13-1026,0,0.0929452,"oriously hard to analyze (Foster et al., 2011; Eisenstein, 2013; Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, fragmented or mixed language, etc. Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and suffer on other (out-of-sample) Twitter data sets. This bias can, at least in theory, be corrected by lea"
C14-1168,N13-1037,0,0.0133621,"Missing"
C14-1168,W10-0713,0,0.0396482,"the universal POS tag set by Petrov et al. (2012). The data sets also differ in a few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong. Instead of making annotations more consistent across data sets, we follow Ritter et al. (2011) in using a few post-processing rules to deterministically assign Twitter-specific symbols to their correct tags. The major difference between the NER data sets is whether Twitter user accounts are annotated as PER. We follow Finin et al. (2010) in doing so. Unlabeled data We downloaded 200k tweet-website pairs from the Twitter search API over a period of one week in August 2013 by searching for tweets that contain the string http and downloading the content of the websites they linked to. We filter out duplicate tweets and restrict ourselves to websites that contain more than one sentence (after removing boilerplate text, scripts, HTML, etc).13 We also require website and tweet to have at least one matching word that is not a stopword (as defined by the NLTK stopword list).14 Finally we restrict ourselves to pairs where the website"
C14-1168,P05-1045,0,0.0126701,"(2011), in particular features for word tokens, a set of features that check for the presence of hyphens, digits, single quotes, upper/lowercase, 3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2i for i ∈ 1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is publicly available.5 We use a pool size of 1000 tweets. We experimented with other pool sizes {500,2000} showing similar performance. The number of iterations i is set on the development data. For NER on websites, we use the Stanford NER system (Finkel et al., 2005)6 with POS tags from the L APOS tagger (Tsuruoka et al., 2011).7 For POS we found it to be superior to use the current POS model for re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-line NER tagging rather than retagging the websites in every iteration. 3.2 Data In our experiments, we consider two scenarios, sometimes referred to as unsupervised and semisupervised domain adaptation (DA), respectively (Daum´e et al., 2010; Plank, 2011). In unsupervised DA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from bo"
C14-1168,I11-1100,0,0.0448938,"Missing"
C14-1168,fromreide-etal-2014-crowdsourcing,1,0.118987,"red-task 9 LDC2011T03. 10 http://www.clips.ua.ac.be/conll2003/ner/ 11 http://www.isi.edu/publications/licensed-sw/mace/ 5 1786 et al. (2011). For NER, we simply use the parameters from our POS tagging experiments and thus do not assume to have access to further development data. For both POS tagging and NER, we have three test sets. For POS tagging, the ones used in Foster et al. (2011) (F OSTER -T EST) and Ritter et al. (2011) (R ITTER -T EST),12 as well as the one presented in Hovy et al. (2014) (H OVY-T EST). For NER, we use the data set from Ritter et al. (2011) and the two data sets from Fromreide et al. (2014) as test sets. One is a manual correction of a held-out portion of F ININ -T RAIN, named F ININ -T EST; the other one is referred to as F ROMREIDE -T EST. Since the different POS corpora use different tag sets, we map all of them corpora onto the universal POS tag set by Petrov et al. (2012). The data sets also differ in a few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong. Instead of making annotations more consistent across data sets, we follow Rit"
C14-1168,P12-2047,1,0.432581,"that are significantly better than several offthe-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled data to reach new highs across several data sets. Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings (Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T¨ackstr¨om et al., 2013). Our type constraints in POS tagging come from tag dictionaries, but also from linked websites. The idea of using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev et al. (2012) for search query tagging. Ganchev et al. (2012), considering the problem of POS tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching of snippets and search queries, giving priority to n-gram matches with larger n. Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less spelling variation than tweets. In NER, it is common to use gazet"
C14-1168,P11-2008,0,0.147708,"Missing"
C14-1168,N13-1132,1,0.602877,"n our experiments, we use the SANCL shared task8 splits of the OntoNotes 4.0 distribution of the WSJ newswire annotations as newswire training data for POS tagging.9 For NER, we use the CoNLL 2003 data sets of annotated newswire from the Reuters corpus.10 The in-domain training POS data comes from Gimpel et al. (2011), and the in-domain NER data comes from Finin et al. (2010) (F ININ -T RAIN). These data sets are added to the newswire sets when doing semi-supervised DA. Note that for NER, we thus do not rely on expertannotated Twitter data, but rely on crowdsourced annotations. We use MACE11 (Hovy et al., 2013) to resolve inter-annotator conflicts between turkers (50 iterations, 10 restarts, no confidence threshold). We believe relying on crowdsourced annotations makes our set-up more robust across different samples of Twitter data. Development and test data. We use several evaluation sets for both tasks to prevent overfitting to a specific sample. We use the (out-of-sample) development data sets from Ritter et al. (2011) and Foster 4 http://www.chokkan.org/software/crfsuite/ http://www.ark.cs.cmu.edu/TweetNLP/ 6 http://http://nlp.stanford.edu/software/CRF-NER.shtml 7 http://www.logos.ic.i.u-tokyo.a"
C14-1168,hovy-etal-2014-pos,1,0.895368,"Foster et al., 2011; Eisenstein, 2013; Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, fragmented or mixed language, etc. Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and suffer on other (out-of-sample) Twitter data sets. This bias can, at least in theory, be corrected by learning from additiona"
C14-1168,P07-1034,0,0.0634483,"cross different samples of tweets than existing approaches. We consider both the scenario where a small sample of labeled Twitter data is available, and the scenario where only newswire data is available. Training on a mixture of out-of-domain (WSJ) and in-domain (Twitter) data as well as unlabeled data, we get the best reported results in the literature for both POS tagging and NER on Twitter. Our tagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd 2 Tagging with not-so-distant supervision We assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), population drift (Hand, 2006), or by our sample size simply being too small. To correct this bias, we want to use unlabeled Twitter data. It is well-known that semi-supervised learning algorithms such as self-training sometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009). This paper presents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data and not-so-distant supervision. More specifically, the idea is to use hyperlinks to condition tagging decisions in tweets on a richer linguistic context than what is available in th"
C14-1168,P11-1016,0,0.0902879,"Missing"
C14-1168,D07-1073,0,0.0341914,"sidering the problem of POS tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching of snippets and search queries, giving priority to n-gram matches with larger n. Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less spelling variation than tweets. In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama and Torisawa, 2007; Cucerzan, 2007). R¨ud et al. (2011) consider using search engines for distant supervision of NER of search queries. Their set-up is very similar to Ganchev et al. (2012), except they do not use click-through data. They use the search engine snippets to generate feature representations rather than projections. Want et al. (2013) also use distant supervision for NER, i.e., Wikipedia page view counts, 1790 applying their model to Twitter data, but their results are considerably below the state of the art. Also, their source of supervision is not linked to the individual tweets in the way mentio"
C14-1168,D12-1127,0,0.0865368,"Missing"
C14-1168,N06-1020,0,0.0511572,"l as unlabeled data, we get the best reported results in the literature for both POS tagging and NER on Twitter. Our tagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd 2 Tagging with not-so-distant supervision We assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), population drift (Hand, 2006), or by our sample size simply being too small. To correct this bias, we want to use unlabeled Twitter data. It is well-known that semi-supervised learning algorithms such as self-training sometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009). This paper presents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data and not-so-distant supervision. More specifically, the idea is to use hyperlinks to condition tagging decisions in tweets on a richer linguistic context than what is available in the tweets. This semi-supervised approach gives state-of-the-art performance across available Twitter POS and NER data sets. The overall semi-supervised learning algorithm is presented in Figure 1. The aim is to correct model bias by predicting tag sequences on small pools of unlabeled"
C14-1168,P09-1113,0,0.139295,"his is the hypothesis we explore in this paper. We present a semi-supervised learning method that does not require additional labeled in-domain data to correct sample bias, but rather leverages pools of unlabeled Twitter data. However, since taggers trained on newswire perform poorly on Twitter data, we need additional guidance when utilizing the unlabeled data. This paper proposes distant supervision to help our models learn from unlabeled data. Distant supervision is a weakly supervised learning paradigm, where a knowledge resource is exploited to gather (possible noisy) training instances (Mintz et al., 2009). Our basic idea is to can use linguistic analysis of linked websites as a novel kind of distant supervision for learning how to analyze tweets. We explore standard sources of distant supervision, such as Wiktionary for POS tagging, but we also propose to use the linked websites of tweets with URLs as supervision. The intuition is that we can use websites to provide a richer linguistic context for our tagging decisions. We exploit the fact that tweets with URLs provide a one-to-one map between an unlabeled instance and the source of supervision, making this This work is licensed under a Creati"
C14-1168,N13-1039,0,0.158057,"Missing"
C14-1168,petrov-etal-2012-universal,1,0.678346,"gging and NER, we have three test sets. For POS tagging, the ones used in Foster et al. (2011) (F OSTER -T EST) and Ritter et al. (2011) (R ITTER -T EST),12 as well as the one presented in Hovy et al. (2014) (H OVY-T EST). For NER, we use the data set from Ritter et al. (2011) and the two data sets from Fromreide et al. (2014) as test sets. One is a manual correction of a held-out portion of F ININ -T RAIN, named F ININ -T EST; the other one is referred to as F ROMREIDE -T EST. Since the different POS corpora use different tag sets, we map all of them corpora onto the universal POS tag set by Petrov et al. (2012). The data sets also differ in a few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong. Instead of making annotations more consistent across data sets, we follow Ritter et al. (2011) in using a few post-processing rules to deterministically assign Twitter-specific symbols to their correct tags. The major difference between the NER data sets is whether Twitter user accounts are annotated as PER. We follow Finin et al. (2010) in doing so. Unlabeled data W"
C14-1168,N10-1021,0,0.0862976,"Missing"
C14-1168,D11-1141,0,0.836453,"ze named entities. Tweets, however, are notoriously hard to analyze (Foster et al., 2011; Eisenstein, 2013; Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, fragmented or mixed language, etc. Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and suffer on other (out-of-sample) Twitter data sets. This b"
C14-1168,P11-1097,0,0.0422016,"Missing"
C14-1168,Q13-1001,1,0.89785,"Missing"
C14-1168,W11-0328,0,0.0169234,"tures that check for the presence of hyphens, digits, single quotes, upper/lowercase, 3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2i for i ∈ 1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is publicly available.5 We use a pool size of 1000 tweets. We experimented with other pool sizes {500,2000} showing similar performance. The number of iterations i is set on the development data. For NER on websites, we use the Stanford NER system (Finkel et al., 2005)6 with POS tags from the L APOS tagger (Tsuruoka et al., 2011).7 For POS we found it to be superior to use the current POS model for re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-line NER tagging rather than retagging the websites in every iteration. 3.2 Data In our experiments, we consider two scenarios, sometimes referred to as unsupervised and semisupervised domain adaptation (DA), respectively (Daum´e et al., 2010; Plank, 2011). In unsupervised DA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from both domains, besides unlabeled target data, but the amount of l"
C14-1168,W03-0419,0,\N,Missing
C14-1168,D10-1002,0,\N,Missing
C14-3005,W06-1615,0,0.0791074,"tion or instance weighting (Wang et al., 2013). In language technology, the bias correction problem is harder. In the case of elections, you have a single output variable and various demographic observed variables. All values taken by discrete variables at test time can be assumed to have been observed, and all values observed at training time can be assumed to be seen at test time. In language technology, we typically have several features only seen in training data and several features only seen in test data. The latter observation has led to interest in bridging unseen words to known ones (Blitzer et al., 2006; Turian et al., 2010), while the former has led to the development of learning algorithms that prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associated with less predictive, correlated features from being updated. Note, however, that post-stratification (Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Royall, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization of post-stratificiation, has received some interest in language technology (Jiang and Zhai,"
C14-3005,E03-1068,0,0.0275044,"taggers for English Twitter as our running example. Label Bias In most annotation projects, there is an initial stage, where the project managers compare annotators’ performance, compute agreement scores, select reliable annotators, adjudicate, and elaborate on annotation guidelines, if necessary. Such procedures are considered necessary to correct for the individual biases of the annotators (label bias). However, this is typically only for the first batches of data, and it is well-known that even some of the most widely used annotated corpora (such as the Penn Treebank) contain many errors (Dickinson and Meurers, 2003) in the form of inconsistent annotations of the same n-grams. Obviously, using non-expert annotators, e.g., through crowd-sourcing platforms, increase the label bias considerably. One way to reduce this bias involves collecting several annotations for each datapoint and averaging over them, which is often feasible because of the low cost of non-expert annotation. This is called majority voting and is analogous to using ensembles of models to obtain more robust systems. In the tutorial we discuss alternatives to averaging over annotators, incl., using EM to estimate annotator confidence (Hovy e"
C14-3005,I11-1100,0,0.0552098,"Missing"
C14-3005,N13-1132,1,0.821754,"2003) in the form of inconsistent annotations of the same n-grams. Obviously, using non-expert annotators, e.g., through crowd-sourcing platforms, increase the label bias considerably. One way to reduce this bias involves collecting several annotations for each datapoint and averaging over them, which is often feasible because of the low cost of non-expert annotation. This is called majority voting and is analogous to using ensembles of models to obtain more robust systems. In the tutorial we discuss alternatives to averaging over annotators, incl., using EM to estimate annotator confidence (Hovy et al., 2013), and joint learning of annotator competence and model parameters (Raykar and Yu, 2012). Bias in Ground Truth In annotation projects, we use inter-annotator agreement measures and annotation guidelines to ensure consistent annotations. However, annotation guidelines often make linguistically debatable and even somewhat arbitrary decisions, and inter-annotator agreement is often less than perfect. Some annotators, for example, may annotate socialin social media as a noun, others may annotate it as an adjective. In this part of the tutorial, we discuss how to correct for the bias introduced by a"
C14-3005,P07-1034,0,0.0177436,"er et al., 2006; Turian et al., 2010), while the former has led to the development of learning algorithms that prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associated with less predictive, correlated features from being updated. Note, however, that post-stratification (Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Royall, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization of post-stratificiation, has received some interest in language technology (Jiang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 11 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 11–13, Dublin, Ireland, August 23-29 2014. approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; McClosky et al., 2"
C14-3005,N10-1004,0,0.0227936,"ang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 11 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 11–13, Dublin, Ireland, August 23-29 2014. approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; McClosky et al., 2010; Chen et al., 2011). Selection bias introduces a bias in P (X). Note that, in theory, this should not hurt discriminative algorithms trying to estimate P (Y |X), without estimating P (X), but in practice it still does. The inductive bias of our algorithms and the size our samples make our models sensitive to selection bias (Zadrozny, 2004). Predictive approaches try to correct this bias by adding more (pseudo-labeled) data to the training sample, while post-stratification and instance weighting reweigh the data to make P (X) similar to the distribution observed in the population. As mentioned"
C14-3005,C14-1168,1,0.824408,"make P (X) similar to the distribution observed in the population. As mentioned, this will never solve the problem with unseen features, since you cannot up-weigh a null feature. Semi-supervised learning can correct modest selection bias, but if the domain gap is too wide, our initial predictions in the target domain will be poor, and semi-supervised learning is likely to increase bias rather than decrease it. However, recent work has shown that semi-supervised learning can be combined with distant supervision and correct bias in cases where semi-supervised learning algorithms typically fail (Plank et al., 2014). In the tutorial we illustrate these different approaches to selection bias correction, with discriminative learning of POS taggers for English Twitter as our running example. Label Bias In most annotation projects, there is an initial stage, where the project managers compare annotators’ performance, compute agreement scores, select reliable annotators, adjudicate, and elaborate on annotation guidelines, if necessary. Such procedures are considered necessary to correct for the individual biases of the annotators (label bias). However, this is typically only for the first batches of data, and"
C14-3005,P07-1078,0,0.0272566,"has received some interest in language technology (Jiang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 11 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 11–13, Dublin, Ireland, August 23-29 2014. approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; McClosky et al., 2010; Chen et al., 2011). Selection bias introduces a bias in P (X). Note that, in theory, this should not hurt discriminative algorithms trying to estimate P (Y |X), without estimating P (X), but in practice it still does. The inductive bias of our algorithms and the size our samples make our models sensitive to selection bias (Zadrozny, 2004). Predictive approaches try to correct this bias by adding more (pseudo-labeled) data to the training sample, while post-stratification and instance weighting reweigh the data to make P (X) similar to the distri"
C14-3005,D07-1111,0,0.0319065,"language technology (Jiang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 11 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 11–13, Dublin, Ireland, August 23-29 2014. approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; McClosky et al., 2010; Chen et al., 2011). Selection bias introduces a bias in P (X). Note that, in theory, this should not hurt discriminative algorithms trying to estimate P (Y |X), without estimating P (X), but in practice it still does. The inductive bias of our algorithms and the size our samples make our models sensitive to selection bias (Zadrozny, 2004). Predictive approaches try to correct this bias by adding more (pseudo-labeled) data to the training sample, while post-stratification and instance weighting reweigh the data to make P (X) similar to the distribution observed in the p"
C14-3005,N06-1012,0,0.0210595,"ave a single output variable and various demographic observed variables. All values taken by discrete variables at test time can be assumed to have been observed, and all values observed at training time can be assumed to be seen at test time. In language technology, we typically have several features only seen in training data and several features only seen in test data. The latter observation has led to interest in bridging unseen words to known ones (Blitzer et al., 2006; Turian et al., 2010), while the former has led to the development of learning algorithms that prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associated with less predictive, correlated features from being updated. Note, however, that post-stratification (Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Royall, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization of post-stratificiation, has received some interest in language technology (Jiang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a"
C14-3005,P10-1040,0,0.0127431,"ting (Wang et al., 2013). In language technology, the bias correction problem is harder. In the case of elections, you have a single output variable and various demographic observed variables. All values taken by discrete variables at test time can be assumed to have been observed, and all values observed at training time can be assumed to be seen at test time. In language technology, we typically have several features only seen in training data and several features only seen in test data. The latter observation has led to interest in bridging unseen words to known ones (Blitzer et al., 2006; Turian et al., 2010), while the former has led to the development of learning algorithms that prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associated with less predictive, correlated features from being updated. Note, however, that post-stratification (Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Royall, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization of post-stratificiation, has received some interest in language technology (Jiang and Zhai, 2007; Foster et al.,"
C16-1013,W13-2711,0,0.103565,"et language (Piotrowski, 2012). Training data for supervised learning of spelling normalization is typically scarce in the historical domain. Furthermore, dialectal influences and even individual preferences of an author can have a huge impact on the spelling characteristics in a particular text, meaning that even training data from other corpora of the same language and time period cannot always be reliably used. Algorithms have often been developed with this fact in mind, e.g. by being based on some form of phonetic, graphematic, or semantic similarity measure (Jurish, 2010; Bollmann, 2012; Amoia and Martinez, 2013). On the other hand, neural networks – and particularly deep networks with several hidden layers – are assumed to work best when trained on large amounts of data. It is therefore not clear whether neural networks are a good choice for this particular domain. We frame spelling normalization as a character-based sequence labeling task, and explore the suitability of a deep bi-directional long short-term memory model (bi-LSTM) in this setting. By basing our model on individual characters as input, along with performing some basic preprocessing (e.g., downcasing all characters), we keep the vocabu"
C16-1013,W14-4012,0,0.0386818,"Missing"
C16-1013,J81-4005,0,0.71273,"Missing"
C16-1013,N16-1179,1,0.84316,"Missing"
C16-1013,W13-2409,0,0.239502,"Missing"
C16-1013,W09-0304,0,0.0823543,"Missing"
C16-1126,K15-1038,1,0.798957,"S) correlations largely transfer across English and French. This means that we can replicate previous studies on gaze-based PoS tagging for French, but also that we can use English gaze data to assist the induction of French NLP models. 1 Introduction The eye movements during normal, skilled reading are known to reflect the processing load associated with reading. Recently, eye movement data has been integrated into natural language processing models for weakly supervised part-of-speech (PoS) induction (Barrett et al., 2016), sentence compression (Klerke et al., 2016), supervised PoS tagging (Barrett and Søgaard, 2015a), and supervised parsing (Barrett and Søgaard, 2015b). Barrett et al. (2016) used eye movements from the English portion of a large eye tracking corpus, the Dundee corpus (Kennedy et al., 2003), for weakly supervised PoS induction for English, obtaining significant improvements over a baseline without gaze features. They used a second-order hidden Markov Model, which was type-constrained by Wiktionary dictionaries for their experiments. These results suggest an approach to weakly supervised PoS induction using only a dictionary and eye movement data. Such an approach would be applicable for"
C16-1126,W15-2401,1,0.851881,"S) correlations largely transfer across English and French. This means that we can replicate previous studies on gaze-based PoS tagging for French, but also that we can use English gaze data to assist the induction of French NLP models. 1 Introduction The eye movements during normal, skilled reading are known to reflect the processing load associated with reading. Recently, eye movement data has been integrated into natural language processing models for weakly supervised part-of-speech (PoS) induction (Barrett et al., 2016), sentence compression (Klerke et al., 2016), supervised PoS tagging (Barrett and Søgaard, 2015a), and supervised parsing (Barrett and Søgaard, 2015b). Barrett et al. (2016) used eye movements from the English portion of a large eye tracking corpus, the Dundee corpus (Kennedy et al., 2003), for weakly supervised PoS induction for English, obtaining significant improvements over a baseline without gaze features. They used a second-order hidden Markov Model, which was type-constrained by Wiktionary dictionaries for their experiments. These results suggest an approach to weakly supervised PoS induction using only a dictionary and eye movement data. Such an approach would be applicable for"
C16-1126,P16-2094,1,0.755469,"es have been limited to English, however. This study shows that gaze and part of speech (PoS) correlations largely transfer across English and French. This means that we can replicate previous studies on gaze-based PoS tagging for French, but also that we can use English gaze data to assist the induction of French NLP models. 1 Introduction The eye movements during normal, skilled reading are known to reflect the processing load associated with reading. Recently, eye movement data has been integrated into natural language processing models for weakly supervised part-of-speech (PoS) induction (Barrett et al., 2016), sentence compression (Klerke et al., 2016), supervised PoS tagging (Barrett and Søgaard, 2015a), and supervised parsing (Barrett and Søgaard, 2015b). Barrett et al. (2016) used eye movements from the English portion of a large eye tracking corpus, the Dundee corpus (Kennedy et al., 2003), for weakly supervised PoS induction for English, obtaining significant improvements over a baseline without gaze features. They used a second-order hidden Markov Model, which was type-constrained by Wiktionary dictionaries for their experiments. These results suggest an approach to weakly supervised PoS ind"
C16-1126,N10-1083,0,0.022573,"features as well. 4 Experiment We replicate the experimental setup of Barrett et al. (2016), which used the best model from Li et al. (2012), a second-order hidden Markov model with maximum entropy emissions (SHMM-ME) constrained by Wiktionary tags such that emissions are confined to the allowed PoS tags of the Wiktionary given that the token exists in the Wiktionary. Li et al. (2012) report considerable improvements from the Wiktionary contraint when comparing to unsupervised methods. The second-order model includes transition probabilities from the antecedent state like a first order model (Berg-Kirkpatrick et al., 2010) as well as from the second-order antecedent state. We use the original implementation of Li et al. and we also include a subset of their word-level features, viz., four features detecting hyphens, numerals, punctuation and capitalization. We leave out the three 2 http://www.natcorp.ox.ac.uk http://www.lexique.org 4 http://www.speech.cs.cmu.edu/SLM/toolkit.html 3 1335 suffix features from Li et al.’s basic feature model, as these features do not transfer across languages. These features were also included by Barrett et al. (2016). We use the English Wiktionary dumps made available by Li et al."
C16-1126,W11-2123,0,0.0261518,"Missing"
C16-1126,N16-1179,1,0.774922,"s study shows that gaze and part of speech (PoS) correlations largely transfer across English and French. This means that we can replicate previous studies on gaze-based PoS tagging for French, but also that we can use English gaze data to assist the induction of French NLP models. 1 Introduction The eye movements during normal, skilled reading are known to reflect the processing load associated with reading. Recently, eye movement data has been integrated into natural language processing models for weakly supervised part-of-speech (PoS) induction (Barrett et al., 2016), sentence compression (Klerke et al., 2016), supervised PoS tagging (Barrett and Søgaard, 2015a), and supervised parsing (Barrett and Søgaard, 2015b). Barrett et al. (2016) used eye movements from the English portion of a large eye tracking corpus, the Dundee corpus (Kennedy et al., 2003), for weakly supervised PoS induction for English, obtaining significant improvements over a baseline without gaze features. They used a second-order hidden Markov Model, which was type-constrained by Wiktionary dictionaries for their experiments. These results suggest an approach to weakly supervised PoS induction using only a dictionary and eye movem"
C16-1126,D12-1127,0,0.0361907,"Missing"
C16-1126,D14-1187,0,0.0753916,"Missing"
C16-1179,W13-3520,0,0.0189266,"Missing"
C16-1179,D15-1263,0,0.225908,"f discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consider the question of whether the information in PDTB can be used to improve RST discourse parsers. Discourse parsing is known to be a hard task (Stede, 2011). It involves several complex and interacting factors, touching upon all layers of lingu"
C16-1179,W01-1605,0,0.0602111,". [The gain on the sale couldn’t be estimated] [until the “tax treatment has been determined.”] b. [On Friday, Datuk Daim added spice to an otherwise unremarkable address on Malaysia’s proposed budget for 1990] [by ordering the Kuala Lumpur Stock Exchange “to take appropriate action immediately” to cut its links with the Stock Exchange of Singapore.] Different theories of discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse"
C16-1179,J81-4005,0,0.703759,"Missing"
C16-1179,P12-1007,0,0.031503,"building. Joty et al. (2012) (TSP) built a two-stage parsing system, training separate sequential models (CRF) for the intra and the inter-sentential levels. These models jointly learn the relation and the structure, and a CKY-like algorithm is used to find the optimal tree. Feng and Hirst (2014) noticed the inefficiency of TSP and proposed a greedy approach inspired by 1910 HILDA but using CRF as local models for the inter- and intra-sententials levels, allowing to take into account sequential dependencies. Last studies also focused on the issue of building a good representation of the data. Feng and Hirst (2012) introduced linguistic features, mostly syntactic and contextual ones. Ji and Eisenstein (2014b) (DPLP) proposed to learn jointly the representation and the task, more precisely a projection matrix that maps the bag-of-words representation of the discourse units into a new vector space. This idea is promising, but a drawback could be the limited amount of data available in the RST-DT, an issue even more crucial for other languages. Discourse parsing has proven useful for many applications (Taboada and Mann, 2006), ranging from summarization (Daum´e III and Marcu, 2009; Thione et al., 2004; Spo"
C16-1179,P14-1048,0,0.329829,"spice to an otherwise unremarkable address on Malaysia’s proposed budget for 1990] [by ordering the Kuala Lumpur Stock Exchange “to take appropriate action immediately” to cut its links with the Stock Exchange of Singapore.] Different theories of discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consi"
C16-1179,N04-1024,0,0.0346199,"hetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consider the question of whether the information in PDTB can be used to improve RST discourse parsers. Discourse parsing is known to be a hard task (Stede, 2011). It involves several complex and interacting factors, touching upon all layers of linguistic analysis, from syntax, semantics up to p"
C16-1179,N06-2015,0,0.0358279,"NS-E LABORATION ( SN-ATTRIBUTION (1) ( NS-E LABORATION (2)(3) ) (4) ) ) (5) ) b. ( NS-E LABORATION ( SN-ATTRIBUTION (1) ( NS-E LABORATION (2)(3) ) (4) ) (5) ) 3 Auxiliary tasks We consider two types of auxiliary tasks: first, tasks derived from the RST-DT (multi-view), that is dependency encoding of the trees and additional auxiliary tasks derived from the main one; second, we consider tasks derived from additional data, namely, the Penn Discourse Treebank (Prasad et al., 2008), Timebank (Pustejovsky et al., 2003; Pustejovsky et al., 2005), Factbank (Saur´ı and Pustejovsky, 2009), Ontonotes (Hovy et al., 2006) and the Santa Barbara corpus of spoken American English (Du Bois, 2000). All the auxiliary tasks are, as the main one, document-level sequence prediction tasks. In Table 1 we report the number of documents and single labels for each task. We hypothesize that such auxiliary information is useful to address data sparsity for RST discourse parsing. 4 Using the implementation available in NLTK. 1905 NN-T EXTUAL O RGANIZATION NN-L IST NN-S AME U NIT NN-L IST 1 6 NS-E LABORATION 2 3 7 NS-E LABORATION 4 =⇒ 5 EDU 1 EDU 2 EDU 3 EDU 4 EDU 5 EDU 6 EDU 7 root -1 NN-L IST -2 NN-S AME U NIT -1 NS-E LABORAT"
C16-1179,P14-1002,0,0.749887,"unremarkable address on Malaysia’s proposed budget for 1990] [by ordering the Kuala Lumpur Stock Exchange “to take appropriate action immediately” to cut its links with the Stock Exchange of Singapore.] Different theories of discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consider the question of wheth"
C16-1179,D12-1083,0,0.147778,", Datuk Daim added spice to an otherwise unremarkable address on Malaysia’s proposed budget for 1990] [by ordering the Kuala Lumpur Stock Exchange “to take appropriate action immediately” to cut its links with the Stock Exchange of Singapore.] Different theories of discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among"
C16-1179,N16-1179,1,0.383709,"e first to propose using bi-LSTMs for tree structure prediction problems. Zhou and Xu (2015), for example, use bi-LSTMs to produce semantic role labelling structures. Zhang et al. (2015) did the same for relation extraction. None of them considered multi-task learning architectures, however. Multi-task learning in deep networks was first introduced by Caruana (1993), who did multi-task learning by doing parameter sharing across several deep networks, letting them share hidden layers. The same technique was used by Collobert et al. (2011) for various NLP tasks, and for sentence compression in (Klerke et al., 2016). Hierarchical multi-task bi-LSTMs have been previously used for part-of-speech tagging (Plank et al., 2016). 8 Conclusion and future work We presented the first experiments exploiting different views of the data and related tasks to improve textlevel discourse parsing. We presented a hierarchical bi-LSTM model allowing to leverage information from various sequence prediction tasks (multi-task learning) that achieves a new state-of-the-art performance on unlabeled text-level discourse parsing, and competitive performance in predicting nuclearity and discourse relations. For relation prediction"
C16-1179,P13-1047,0,0.0338202,"t al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010), sentiment analysis (Bhatia et al., 2015) or essay scoring (Burstein et al., 2003; Higgins et al., 2004). However, the range of applications and the improvement allowed are for now limited by the low performance of the existing discourse parsers. We are not aware of other studies trying to combine various encodings of the RST-DT trees or to leverage relevant information through multi-task learning to improve discourse parsing. To the best of our knowledge, multi-task learning has only been used for discourse relation classification (Lan et al., 2013) on the Penn Discourse Treebank to combine implicit and explicit data. We are not the first to propose using bi-LSTMs for tree structure prediction problems. Zhou and Xu (2015), for example, use bi-LSTMs to produce semantic role labelling structures. Zhang et al. (2015) did the same for relation extraction. None of them considered multi-task learning architectures, however. Multi-task learning in deep networks was first introduced by Caruana (1993), who did multi-task learning by doing parameter sharing across several deep networks, letting them share hidden layers. The same technique was used"
C16-1179,C04-1048,0,0.0973187,"Missing"
C16-1179,P14-1003,0,0.0415062,"-L IST 1 6 NS-E LABORATION 2 3 7 NS-E LABORATION 4 =⇒ 5 EDU 1 EDU 2 EDU 3 EDU 4 EDU 5 EDU 6 EDU 7 root -1 NN-L IST -2 NN-S AME U NIT -1 NS-E LABORATION -1 NS-E LABORATION -5 NN-T EXTUAL O RGANIZATION -1 NN-L IST Figure 3: From RST-DT discourse trees to dependency sequence labels. The numbers indicate the position of the head of the EDU, e.g. EDU 2 and EDU 3 have the root EDU 1 as head. 3.1 Building other views of the RST-DT trees Binary dependencies We first use a representation of the RST-DT trees as binary dependencies (RSTDep). We roughly do the same transformation as (Muller et al., 2012; Li et al., 2014) but contrary to the latter, we choose as root the nucleus of the root node of the tree rather than the first EDU of the document. More precisely, we associate each node with its saliency set as defined in (Marcu, 1997): The nucleus is the salient EDU of a relation, and the nuclei can go up in the tree with possibly several nuclei in the saliency set of a node. Like Li et al. (2014), we replace all multi-nuclear relations (NN) by mono-nuclear ones choosing the left DU as the nucleus (NS). We thus have only one nucleus in each saliency set. Figure 3 illustrates the conversion of an RST tree int"
C16-1179,D09-1036,0,0.016877,"Missing"
C16-1179,W10-4327,0,0.142625,"Different theories of discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consider the question of whether the information in PDTB can be used to improve RST discourse parsers. Discourse parsing is known to be a hard task (Stede, 2011). It involves several complex and interacting factors, touching upo"
C16-1179,J00-3005,0,0.563392,"uning We used a development set of 25 documents randomly chosen among the training set. We optimized the number of passes p over the data (p ∈ [10, 60]), the value of the Gaussian noise (σ ∈ {0.0, 0.2}), the number of hidden dimensions (d ∈ {200, 400}), the number of stacked layers (h ∈ {1, 2, 3, 4, 5}), and the auxiliary tasks to be included and combined. In the end, we report results using 2 feed-forward layers with 128 dimensions, a Gaussian noise with sigma of 0.2, 200 hidden dimensions, 20 passes over the data, 2 layers and Polyglot embeddings (Al-Rfou et al., 2013)8 . Metrics Following (Marcu, 2000b) and most subsequent work, output trees are evaluated against gold trees in terms of how similar they bracket the EDUs (Span), how often they agree about nuclei when predicting a true bracket (Nuclearity), and in terms of the relation label, i.e., the overlap between the shared brackets between predicted and gold trees (Relation).9 These scores are analogous to labeled and unlabeled syntactic parser evaluation metrics. The exact definitions of the three metrics are: • Span: This metric is the unlabeled F1 over gold and predicted trees, and identical to the PARSEVAL metric in syntactic parsin"
C16-1179,J93-2004,0,0.0630102,"Missing"
C16-1179,C12-1115,0,0.0322083,"IST NN-S AME U NIT NN-L IST 1 6 NS-E LABORATION 2 3 7 NS-E LABORATION 4 =⇒ 5 EDU 1 EDU 2 EDU 3 EDU 4 EDU 5 EDU 6 EDU 7 root -1 NN-L IST -2 NN-S AME U NIT -1 NS-E LABORATION -1 NS-E LABORATION -5 NN-T EXTUAL O RGANIZATION -1 NN-L IST Figure 3: From RST-DT discourse trees to dependency sequence labels. The numbers indicate the position of the head of the EDU, e.g. EDU 2 and EDU 3 have the root EDU 1 as head. 3.1 Building other views of the RST-DT trees Binary dependencies We first use a representation of the RST-DT trees as binary dependencies (RSTDep). We roughly do the same transformation as (Muller et al., 2012; Li et al., 2014) but contrary to the latter, we choose as root the nucleus of the root node of the tree rather than the first EDU of the document. More precisely, we associate each node with its saliency set as defined in (Marcu, 1997): The nucleus is the salient EDU of a relation, and the nuclei can go up in the tree with possibly several nuclei in the saliency set of a node. Like Li et al. (2014), we replace all multi-nuclear relations (NN) by mono-nuclear ones choosing the left DU as the nucleus (NS). We thus have only one nucleus in each saliency set. Figure 3 illustrates the conversion"
C16-1179,P09-1077,0,0.0189492,"Missing"
C16-1179,P16-2067,1,0.830709,"e bi-LSTMs to produce semantic role labelling structures. Zhang et al. (2015) did the same for relation extraction. None of them considered multi-task learning architectures, however. Multi-task learning in deep networks was first introduced by Caruana (1993), who did multi-task learning by doing parameter sharing across several deep networks, letting them share hidden layers. The same technique was used by Collobert et al. (2011) for various NLP tasks, and for sentence compression in (Klerke et al., 2016). Hierarchical multi-task bi-LSTMs have been previously used for part-of-speech tagging (Plank et al., 2016). 8 Conclusion and future work We presented the first experiments exploiting different views of the data and related tasks to improve textlevel discourse parsing. We presented a hierarchical bi-LSTM model allowing to leverage information from various sequence prediction tasks (multi-task learning) that achieves a new state-of-the-art performance on unlabeled text-level discourse parsing, and competitive performance in predicting nuclearity and discourse relations. For relation prediction, future work includes adding additional information at the sentence level, such as syntactic information us"
C16-1179,prasad-etal-2008-penn,0,0.0947831,"This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consider the question of whether the information in PDTB can be used to improve RST discourse parsers. Discourse parsing is known to be a hard task (Stede, 2011). It involves several complex and interacting factors, touching upon all layers of linguistic analysis, from syntax, semantics up to pragmatics. Consequently, also annotation is complex and time consuming, and hence available annotated corpora are sparse"
C16-1179,E14-1068,0,0.0350995,"Missing"
C16-1179,H05-1033,0,0.0635365,"ock Exchange of Singapore.] Different theories of discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consider the question of whether the information in PDTB can be used to improve RST discourse parsers. Discourse parsing is known to be a hard task (Stede, 2011). It involves several complex and interacting f"
C16-1179,W04-1009,0,0.0875429,"Missing"
C16-1179,W12-1623,0,0.028001,"Missing"
C16-1179,Y15-1009,0,0.00549823,"ance of the existing discourse parsers. We are not aware of other studies trying to combine various encodings of the RST-DT trees or to leverage relevant information through multi-task learning to improve discourse parsing. To the best of our knowledge, multi-task learning has only been used for discourse relation classification (Lan et al., 2013) on the Penn Discourse Treebank to combine implicit and explicit data. We are not the first to propose using bi-LSTMs for tree structure prediction problems. Zhou and Xu (2015), for example, use bi-LSTMs to produce semantic role labelling structures. Zhang et al. (2015) did the same for relation extraction. None of them considered multi-task learning architectures, however. Multi-task learning in deep networks was first introduced by Caruana (1993), who did multi-task learning by doing parameter sharing across several deep networks, letting them share hidden layers. The same technique was used by Collobert et al. (2011) for various NLP tasks, and for sentence compression in (Klerke et al., 2016). Hierarchical multi-task bi-LSTMs have been previously used for part-of-speech tagging (Plank et al., 2016). 8 Conclusion and future work We presented the first expe"
C16-1179,P15-1109,0,0.127731,"s of two stacked layers. For multi-task learning, each task is associated with a specific output layer, whereas the inner layers – the stacked LSTMs – are shared across the tasks. At training time, we randomly sample data points from target or auxiliary tasks and do forward predictions. In the backward pass, we modify the weights of the shared layers and the task-specific outer layer. Except for the outer layer, the target task model is thus regularized by the induction of auxiliary models. Bi-LSTMs have already been used for syntactic chunking (Huang et al., 2015) and semantic role labeling (Zhou and Xu, 2015), as well as other tasks. Our model differs from most of these models in being a hierarchical model, composing word embeddings into sentence embeddings that are the inputs of a bigger bi-LSTM model. This means our model can also be initialized by pre-trained word embeddings. We implemented our recurrent network in CNN/pycnn,7 fixing the random seed. We use standard SGD for learning our model parameters. 5 Experiments Data The RST-DT contains 385 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993), with 347 documents for training and 38 for testing in the split used in pre"
C16-1179,Q15-1024,0,\N,Missing
C16-1179,P02-1057,0,\N,Missing
C18-1021,W18-0503,1,0.839312,"ch that amendments to and maintenance of the latter need only be implemented on the server side. Lexi is currently limited to performing lexical simplification. Note, however, that this is merely a limitation of the backend system, which only implements a lexical simplification system for now. From the frontend perspective, however, there are no limitations as to the nature and length of the simplified items in a text, and extending Lexi to support higher-level modes of simplification simply amounts to 2 An alternative to traditional, one-size-fits-all approaches has recently been proposed by Bingel et al. (2018), who use eye-tracking measures to induce personalized models to predict misreadings in children with reading difficulties. 248 Figure 1: Lexical simplification pipeline as identified by Paetzold and Specia (2015). The simplification workflow consists of identifying simplification targets, i.e. words that pose a challenge to the reader. In the generation step, possible alternatives for each target are retrieved, which are then filtered in the selection step, eliminating words that do not fit the context. In the ranking step, the system finally orders the candidates by simplicity. implementing"
C18-1021,P11-2087,0,0.0259131,"arget group has its own simplification needs, and there is considerable variation as to how well the specifics of what makes a text difficult is defined for each group and simplification strategy. While difficult items in a text may be identified more easily and generally for problems such as resolving pronoun reference, questions such as what makes a French word difficult for a native speaker of Japanese, or what dyslexic children consider a difficult character combination or an overly long sentence, are much harder to answer. Nevertheless, there is a vast body of work (Yatskar et al., 2010; Biran et al., 2011; Horn et al., 2014) that ventures to build very general-purpose simplification models from simplification corpora such as the Simple English Wikipedia corpus (Coster and Kauchak, 2011), which has been edited by amateurs without explicit regard to a specific audience, and with rather vague guidelines as to what constitutes difficult or simple language. Other work in simplification attempts to answer the above questions by inducing models from specifically compiled datasets, which for instance may have been collected by surveying specific target groups and asking them to indicate difficult mate"
C18-1021,P11-2117,0,0.424489,"users. As a first step towards personalized simplification, we propose a framework for adaptive lexical simplification and introduce Lexi, a free open-source and easily extensible tool for adaptive, personalized text simplification. Lexi is easily installed as a browser extension, enabling easy access to the service for its users. 1 Introduction Many a research paper on text simplification starts out by sketching the problem of text simplification as rewriting a text such that it becomes easier to read, changing or removing as little of its informational content as possible (Zhu et al., 2010; Coster and Kauchak, 2011; De Belder and Moens, 2010; Paetzold and Specia, 2015; Bingel and Søgaard, 2016). Such a statement may describe the essence of simplification as a research task, but it hides the fact that it is not always easy to decide what is easy for a particular user. This paper discusses why we need custom-tailored simplifications for individual users, and argues that previous research on non-adaptive text simplification has been too generic to unfold the full potential of text simplification. Even when limiting ourselves to lexical substitution, i.e. the task of reducing the complexity of a document by"
C18-1021,W14-1215,0,0.345672,"pursued, often focusing on lexical (Tweissi, 1998) but also sentence-level simplification (Liu and Matsumoto, 2016). Other notable groups that have been specifically targeted in text simplification research include dyslexics (Rello et al., 2013), and the aphasic (Carroll et al., 1998), for whom particularly long words and sentences, but also certain surface forms such as specific character combinations, may pose difficulties. People on the autism spectrum have also been addressed, with the focus lying on reducing the amount of figurative expressions in a text or reducing syntactic complexity (Evans et al., 2014). Reading beginners (both children and adults) are another group with very particular needs, and text simplification This paper is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 245 Proceedings of the 27th International Conference on Computational Linguistics, pages 245–258 Santa Fe, New Mexico, USA, August 20-26, 2018. research has tried to provide this group with methods to reduce the amount of high-register language and non-frequent words (De Belder and Moens, 2010). Evidently, each target group has its"
C18-1021,P15-2011,0,0.154431,"asks), such that data points sampled from users can quickly have an impact on a generic base model.2 3.1.2 Adaptive Substitution Ranking Substitution Ranking has received relatively little attention in the community compared to CWI. Most lexical simplifiers rank candidates using unsupervised approaches. The earliest example is the approach of Carroll et al. (1998), who rank candidates according to their Kucera-Francis coefficients, which are calculated based on frequencies extracted from the Brown corpus (Rudell, 1993). Other unsupervised approaches, such as those of Ligozat et al. (2012) and Glavaš and Štajner (2015), go a step further and use metrics that incorporate multiple aspects of word complexity, including context-aware features such as n-gram frequencies and language model probabilities. But even though unsupervised rankers perform well in the task, they are incapable of learning from data, which makes them unsuitable for adaptive SR. Our approach to adaptive SR is similar to our approach to adaptive CWI, namely to train an initial model over manually produced simplicity rankings, then continuously update them with new knowledge as Lexi users provide feedback on the simplifications they receive."
C18-1021,P14-2075,0,0.0420329,"own simplification needs, and there is considerable variation as to how well the specifics of what makes a text difficult is defined for each group and simplification strategy. While difficult items in a text may be identified more easily and generally for problems such as resolving pronoun reference, questions such as what makes a French word difficult for a native speaker of Japanese, or what dyslexic children consider a difficult character combination or an overly long sentence, are much harder to answer. Nevertheless, there is a vast body of work (Yatskar et al., 2010; Biran et al., 2011; Horn et al., 2014) that ventures to build very general-purpose simplification models from simplification corpora such as the Simple English Wikipedia corpus (Coster and Kauchak, 2011), which has been edited by amateurs without explicit regard to a specific audience, and with rather vague guidelines as to what constitutes difficult or simple language. Other work in simplification attempts to answer the above questions by inducing models from specifically compiled datasets, which for instance may have been collected by surveying specific target groups and asking them to indicate difficult material in a text. Yet"
C18-1021,S12-1068,0,0.173203,"in the mentioned shared tasks), such that data points sampled from users can quickly have an impact on a generic base model.2 3.1.2 Adaptive Substitution Ranking Substitution Ranking has received relatively little attention in the community compared to CWI. Most lexical simplifiers rank candidates using unsupervised approaches. The earliest example is the approach of Carroll et al. (1998), who rank candidates according to their Kucera-Francis coefficients, which are calculated based on frequencies extracted from the Brown corpus (Rudell, 1993). Other unsupervised approaches, such as those of Ligozat et al. (2012) and Glavaš and Štajner (2015), go a step further and use metrics that incorporate multiple aspects of word complexity, including context-aware features such as n-gram frequencies and language model probabilities. But even though unsupervised rankers perform well in the task, they are incapable of learning from data, which makes them unsuitable for adaptive SR. Our approach to adaptive SR is similar to our approach to adaptive CWI, namely to train an initial model over manually produced simplicity rankings, then continuously update them with new knowledge as Lexi users provide feedback on the"
C18-1021,W16-4901,0,0.023613,"f text simplification, including resources across languages, see Siddharthan (2014), Shardlow (2014b) and Collins-Thompson (2014). 1.1 There is no one-size-fits-all solution to text simplification Text simplification is a diverse task, or perhaps rather a family of tasks, with a number of different target audiences that different papers and research projects have focused on. Among the most prominent target audiences are foreign language learners, for whom various approaches to simplifying text have been pursued, often focusing on lexical (Tweissi, 1998) but also sentence-level simplification (Liu and Matsumoto, 2016). Other notable groups that have been specifically targeted in text simplification research include dyslexics (Rello et al., 2013), and the aphasic (Carroll et al., 1998), for whom particularly long words and sentences, but also certain surface forms such as specific character combinations, may pose difficulties. People on the autism spectrum have also been addressed, with the focus lying on reducing the amount of figurative expressions in a text or reducing syntactic complexity (Evans et al., 2014). Reading beginners (both children and adults) are another group with very particular needs, and"
C18-1021,P15-4015,1,0.947247,"ation, we propose a framework for adaptive lexical simplification and introduce Lexi, a free open-source and easily extensible tool for adaptive, personalized text simplification. Lexi is easily installed as a browser extension, enabling easy access to the service for its users. 1 Introduction Many a research paper on text simplification starts out by sketching the problem of text simplification as rewriting a text such that it becomes easier to read, changing or removing as little of its informational content as possible (Zhu et al., 2010; Coster and Kauchak, 2011; De Belder and Moens, 2010; Paetzold and Specia, 2015; Bingel and Søgaard, 2016). Such a statement may describe the essence of simplification as a research task, but it hides the fact that it is not always easy to decide what is easy for a particular user. This paper discusses why we need custom-tailored simplifications for individual users, and argues that previous research on non-adaptive text simplification has been too generic to unfold the full potential of text simplification. Even when limiting ourselves to lexical substitution, i.e. the task of reducing the complexity of a document by replacing difficult words with easier-to-read synonym"
C18-1021,C16-2017,1,0.879594,"k (2006), who present HAPPI, a web platform that allows users to request simplified versions of words, as well as other “memory jogging” pieces of information, such as related images. Another example is the work of Azab et al. (2015), who present a web platform that allows users to select words they do not comprehend, then presents them with synonyms in order to facilitate comprehension. Notice that their approach does not simplify the selected complex words directly, it simply shows semantically equivalent alternatives that could be within the vocabulary known by the user. The recent work of Paetzold and Specia (2016a) describes Anita, yet another web platform of this kind. It allows users to select complex words and then request a simplified version, related images, synonyms, definitions and translations. Paetzold and Specia (2016a) claim that their approach outputs customized simplifications depending on the user’s profile, and evolves as users provide feedback on the output produced. However, they provide no details of the approach they use to do so, nor do they present any results showcasing its effectiveness. Therefore not counting Paetzold and Specia (2016a) as work in personalized simplification, w"
C18-1021,W16-4912,0,0.0339028,"k (2006), who present HAPPI, a web platform that allows users to request simplified versions of words, as well as other “memory jogging” pieces of information, such as related images. Another example is the work of Azab et al. (2015), who present a web platform that allows users to select words they do not comprehend, then presents them with synonyms in order to facilitate comprehension. Notice that their approach does not simplify the selected complex words directly, it simply shows semantically equivalent alternatives that could be within the vocabulary known by the user. The recent work of Paetzold and Specia (2016a) describes Anita, yet another web platform of this kind. It allows users to select complex words and then request a simplified version, related images, synonyms, definitions and translations. Paetzold and Specia (2016a) claim that their approach outputs customized simplifications depending on the user’s profile, and evolves as users provide feedback on the output produced. However, they provide no details of the approach they use to do so, nor do they present any results showcasing its effectiveness. Therefore not counting Paetzold and Specia (2016a) as work in personalized simplification, w"
C18-1021,E17-2006,1,0.831883,"to train an initial model over manually produced simplicity rankings, then continuously update them with new knowledge as Lexi users provide feedback on the simplifications they receive. The feedback in this scenario is composed of a complex word in context, a simplification produced by Lexi, and a binary rank provided by the user determining which word (complex or simplification) makes the sentence easier to understand. For that purpose, we need a supervised model that (i) supports online learning so that it can be efficiently updated after each session, and (ii) can learn from binary ranks. Paetzold and Specia (2017) offer some intuition on how this can be done. They exploit the fact that one can decompose a sequence of elements {e1 , e2 , ..., en } with ranks {r1 , r2 , ..., rn } into a matrix m ∈ Rn×n , such that m(i, j) = f (ri , rj ), and function f (ri , rj ) estimates a value that describes the relationship between the ranks of elements ei and ej . For example, f could be described as:   1 if ri < rj f (ri , rj ) −1 if ri > rj  0 otherwise (2) The ranker of Paetzold and Specia (2017) uses a deep multi-layer perceptron that predicts each value of m individually. It takes as input feature represent"
C18-1021,shardlow-2014-open,0,0.480113,"acing difficult words with easier-to-read synonyms, we see plenty of evidence that, for instance, dyslexics are highly individual in what material is deemed easy and complex (Ziegler et al., 2008). Lexi, which we introduce in this paper, is a free, open-source and easily extensible tool for adaptively learning what items specific users find difficult, using this information to provide better (lexical) simplification. Our system initially serves Danish, but is easily extended to further languages. For surveys of text simplification, including resources across languages, see Siddharthan (2014), Shardlow (2014b) and Collins-Thompson (2014). 1.1 There is no one-size-fits-all solution to text simplification Text simplification is a diverse task, or perhaps rather a family of tasks, with a number of different target audiences that different papers and research projects have focused on. Among the most prominent target audiences are foreign language learners, for whom various approaches to simplifying text have been pursued, often focusing on lexical (Tweissi, 1998) but also sentence-level simplification (Liu and Matsumoto, 2016). Other notable groups that have been specifically targeted in text simplif"
C18-1021,N10-1056,0,0.0358474,"10). Evidently, each target group has its own simplification needs, and there is considerable variation as to how well the specifics of what makes a text difficult is defined for each group and simplification strategy. While difficult items in a text may be identified more easily and generally for problems such as resolving pronoun reference, questions such as what makes a French word difficult for a native speaker of Japanese, or what dyslexic children consider a difficult character combination or an overly long sentence, are much harder to answer. Nevertheless, there is a vast body of work (Yatskar et al., 2010; Biran et al., 2011; Horn et al., 2014) that ventures to build very general-purpose simplification models from simplification corpora such as the Simple English Wikipedia corpus (Coster and Kauchak, 2011), which has been edited by amateurs without explicit regard to a specific audience, and with rather vague guidelines as to what constitutes difficult or simple language. Other work in simplification attempts to answer the above questions by inducing models from specifically compiled datasets, which for instance may have been collected by surveying specific target groups and asking them to ind"
C18-1021,W18-0507,1,0.863339,"tivity. 3.1 Adaptivity in the lexical simplification pipeline Lexical simplification, i.e. replacing single words with simpler synonyms, classically employs a pipeline approach illustrated in Figure 1 (Shardlow, 2014a; Paetzold and Specia, 2015). This pipeline consists of a four-step process, the first step of which is to identify simplification targets, i.e. words that the model believes will pose a difficulty for the user. This step is called Complex Word Identification (CWI) and has received a great deal of attention in the community, including two shared tasks (Paetzold and Specia, 2016b; Yimam et al., 2018). In a second step, known as Substitution Generation, synonyms are retrieved as candidate replacements for the target These are then filtered to match the context, resolving word sense ambiguities or stylistic mismatches, in Substitution Selection. Finally, those filtered candidate are ranked in order of simplicity in what is known as Substitution Ranking (SR). Out of these four steps, we consider CWI and SR as the most natural ones to make adaptive, whereas generation and selecting candidates can be regarded as relatively independent from a specific user. In order to implement adaptivity, we"
C18-1021,C10-1152,0,0.0475755,"needs of specific users. As a first step towards personalized simplification, we propose a framework for adaptive lexical simplification and introduce Lexi, a free open-source and easily extensible tool for adaptive, personalized text simplification. Lexi is easily installed as a browser extension, enabling easy access to the service for its users. 1 Introduction Many a research paper on text simplification starts out by sketching the problem of text simplification as rewriting a text such that it becomes easier to read, changing or removing as little of its informational content as possible (Zhu et al., 2010; Coster and Kauchak, 2011; De Belder and Moens, 2010; Paetzold and Specia, 2015; Bingel and Søgaard, 2016). Such a statement may describe the essence of simplification as a research task, but it hides the fact that it is not always easy to decide what is easy for a particular user. This paper discusses why we need custom-tailored simplifications for individual users, and argues that previous research on non-adaptive text simplification has been too generic to unfold the full potential of text simplification. Even when limiting ourselves to lexical substitution, i.e. the task of reducing the c"
D13-1075,W09-1113,0,\N,Missing
D13-1075,W12-4905,0,\N,Missing
D13-1154,D12-1118,0,0.0308542,", regular expressions. The problem with using regular expressions as features is of course that even with a finite vocabRelated work Musat et al. (2012) design a collaborative two-player game for sentiment annotation and collecting a sentiment lexicon. One player guesses the sentiment of a text and picks a word from it that is representative of its sentiment. The other player also provides a guess observing only this word. If the two guesses agree, both players get a point. The idea of gamifying the problem of finding good representations goes beyond crowdsourcing, but is not considered here. Boyd-Graber et al. (2012) crowdsource the feature weighting problem, but using standard representations. The work most similar to ours is probably Tamuz et al. (2011), who learn a ’crowd kernel’ by asking annotators to rate examples by similarity, providing an embedding that promotes feature combinations deemed relative when measuring similarity. 1476 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1476–1480, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics S TACKOVERFLOW TASTE A PPEARANCE n 97,519 152,390 152,331 P (1) 0.5013"
D13-1154,P98-1032,0,0.0324384,"easing popular in NLP, this is, to the best of our knowledge, the first attempt to crowdsource the problem of finding good representations. 1.1 1 Introduction Finding good representations of classification problems is often glossed over in the literature. Several authors have emphasized the need to pay more attention to finding such representations (Wagstaff, 2012; Domingos, 2012), but in document classification most research still uses n-gram representations. This paper considers two document classification problems where such representations seem inadequate. The problems are answer scoring (Burstein et al., 1998), on data from stackoverflow.com, and multi-attribute sentiment analysis (McAuley et al., 2012). We argue that in order to adequately represent such problems we need discontinuous features, i.e., regular expressions. The problem with using regular expressions as features is of course that even with a finite vocabRelated work Musat et al. (2012) design a collaborative two-player game for sentiment annotation and collecting a sentiment lexicon. One player guesses the sentiment of a text and picks a word from it that is representative of its sentiment. The other player also provides a guess obser"
D13-1154,W13-1703,0,0.0166052,"btaining 1,500 HITs for each dataset. The annotators were presented with each text, a review or an answer, twice: once as running text, once word-by-word with bullets to tick off words. The annotators were instructed to tick off words or phrases that they found predictive of the text’s sentiment or answer quality. They were not informed about the class of the text. We chose this annotation task, because it is relatively easy for annotators to mark spans of text with a particular attribute. This set-up has been used in other applications, including NER (Finin et al., 2010) and error detection (Dahlmeier et al., 2013). The annotators were constrained to tick off at least three words, including one closed class item (closed class items were colored differently). Finally, we only used annotators with a track record of providing high-quality annotations in previous tasks. It was clear from the average time spent by annotators that annotating S TACK OVERFLOW was harder than annotating the Ratebeer datasets. The average time spent on a Ratebeer HIT was 44s, while for S TACKOVERFLOW it was 3m:8s. The mean number of words ticked off 4 S TACKOVERF TASTE A PPEARANCE BoW 0.655 0.798 0.758 HI 0.654 0.797 0.760 Exp 0."
D13-1154,W10-0713,0,0.0328579,"rs each, using Amazon Mechanical Turk,4 obtaining 1,500 HITs for each dataset. The annotators were presented with each text, a review or an answer, twice: once as running text, once word-by-word with bullets to tick off words. The annotators were instructed to tick off words or phrases that they found predictive of the text’s sentiment or answer quality. They were not informed about the class of the text. We chose this annotation task, because it is relatively easy for annotators to mark spans of text with a particular attribute. This set-up has been used in other applications, including NER (Finin et al., 2010) and error detection (Dahlmeier et al., 2013). The annotators were constrained to tick off at least three words, including one closed class item (closed class items were colored differently). Finally, we only used annotators with a track record of providing high-quality annotations in previous tasks. It was clear from the average time spent by annotators that annotating S TACK OVERFLOW was harder than annotating the Ratebeer datasets. The average time spent on a Ratebeer HIT was 44s, while for S TACKOVERFLOW it was 3m:8s. The mean number of words ticked off 4 S TACKOVERF TASTE A PPEARANCE BoW"
D13-1154,W12-4001,0,0.0199017,"s (Wagstaff, 2012; Domingos, 2012), but in document classification most research still uses n-gram representations. This paper considers two document classification problems where such representations seem inadequate. The problems are answer scoring (Burstein et al., 1998), on data from stackoverflow.com, and multi-attribute sentiment analysis (McAuley et al., 2012). We argue that in order to adequately represent such problems we need discontinuous features, i.e., regular expressions. The problem with using regular expressions as features is of course that even with a finite vocabRelated work Musat et al. (2012) design a collaborative two-player game for sentiment annotation and collecting a sentiment lexicon. One player guesses the sentiment of a text and picks a word from it that is representative of its sentiment. The other player also provides a guess observing only this word. If the two guesses agree, both players get a point. The idea of gamifying the problem of finding good representations goes beyond crowdsourcing, but is not considered here. Boyd-Graber et al. (2012) crowdsource the feature weighting problem, but using standard representations. The work most similar to ours is probably Tamuz"
D13-1154,D09-1035,0,0.0141763,"nsidering ngram models. An n-gram extracts features from a sliding window (of size n) over the text. We call this model BoW(N = n). Our BoW(N = 1) model takes word forms as features, and there are obviously more advanced ways of automatically combining such features. Kernel representations We experimented with applying an approximate feature map for the additive χ2 -kernel. We used two sample steps, resulting in 4N + 1 features. See Vedaldi and Zimmerman (2011) for details. Deep features We also ran denoising autoencoders (Pascal et al., 2008), previously applied to a wide range of NLP tasks (Ranganath et al., 2009; Socher et al., 2011; Chen et al., 2012), with 2N nodes in the middle layer to obtain a deep representation of our datasets from χ2 -BoW input. The network was trained for 15 epochs. We set the drop-out rate to 0.0 and 0.3. Summary of feature sets The feature sets – BoW, www.mturk.com 1478 StackOverflow Taste 90 90 74 Appearance −BoW −BoW + HI χ −BoW + Exp χ2 −BoW + AMT χ2 χ2 2 72 85 85 68 66 64 60 102 N 80 75 −BoW −BoW + HI −BoW + Exp 2 χ −BoW + AMT χ2 62 Accuracy Accuracy Accuracy 70 χ2 χ2 χ2 70 103 75 −BoW −BoW + HI −BoW + Exp 2 χ −BoW + AMT χ2 χ2 102 N 80 70 103 102 N 103 Figure 1: Result"
D13-1154,C98-1032,0,\N,Missing
D14-1104,D10-1044,0,0.0285571,", but nevertheless different distribution. The problem of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic assumption, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervised domain adaptation s"
D14-1104,P07-1034,0,0.525385,"sampled from a related, but nevertheless different distribution. The problem of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic assumption, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervise"
D14-1104,C08-1071,0,0.0756699,"Missing"
D14-1104,P13-1147,1,0.933854,"fferent distribution. The problem of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic assumption, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervised domain adaptation scenario, obtaining mixed res"
D14-1104,E14-1078,1,0.836428,"sion and transition probabilities across the various domains. 99 Cortes et al. (2010) show that importance weighting potentially leads to over-fitting, but propose to use quantiles to obtain more robust weight functions. The idea is to rank all weights and obtain q quantiles. If a data point x is weighted by w, and w lies in the ith quantile of the ranking (i ≤ q), x is weighted by the average weight of data points in the ith quantile. The weighted structured perceptron (§3) used in the experiments below was recently used for a different problem, namely for correcting for bias in annotations (Plank et al., 2014). wsj answers reviews emails weblogs newsgroups 98 ● 97 ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 93 94 95 96 ● Related work 92 2 0 Most prior work on importance weighting use a domain classifier, i.e., train a classifier to discriminate between source and target instances (Søgaard and Haulrich, 2011; Plank and Moschitti, 2013) (y ∈ {s, t}). For instance, Søgaard and Haulrich (2011) train a n-gram text classifier and Plank and Moschitti (2013) a tree-kernel based classifier on relation extraction instances. In these studies, Pˆ (t|x) is used as an approximation of Pt (x) Ps (x) , following Zadrozn"
D14-1104,Q14-1002,0,0.0267413,"Missing"
D14-1104,W11-2906,1,0.915587,"em of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic assumption, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervised domain adaptation scenario, obtaining mixed results. These two papers assume co"
D14-1104,Q13-1001,0,0.0257099,"Missing"
D14-1104,W02-1001,0,\N,Missing
D14-1104,petrov-etal-2012-universal,0,\N,Missing
D14-1104,W06-1615,0,\N,Missing
D15-1245,acs-2014-pivot,0,0.0535652,"Missing"
D15-1245,P14-1136,0,0.113418,"Missing"
D15-1245,W07-2416,0,0.0467962,"Missing"
D15-1245,D07-1002,0,0.0208932,"e-semantic parsing is the task of automatically finding semantically salient targets in text, disambiguating the targets by assigning a sense (frame) to them, identifying their arguments, and labeling these arguments with appropriate roles. The F RAME N ET 1.5 lexicon1 provides a fixed repository of semantic frames and roles, which we use in the experiments below. Several learning and parsing algorithms have been developed for frame-semantic analysis (Johansson and Nugues, 2007; Das et al., 2014; Täckström et al., 2015), and frame semantics has been successfully applied to question-answering (Shen and Lapata, 2007), information extraction (Surdeanu et al., 2003) and knowledge extraction (Søgaard et al., 2015b). 1 https://framenet.icsi.berkeley.edu/ Contributions This paper makes the following three contributions. We present a new multilingual frame-annotated corpus covering five topics, two domains (Wikipedia and Twitter), and nine languages. We implement a simplified version of the frame-semantic parser introduced in Das et al. (2014). Finally, we show how to modify this parser to learn any-language frame-semantic parsing models using inter-lingual word embeddings (Søgaard et al., 2015a). 2 Data annota"
D15-1245,P03-1002,0,0.0606614,"finding semantically salient targets in text, disambiguating the targets by assigning a sense (frame) to them, identifying their arguments, and labeling these arguments with appropriate roles. The F RAME N ET 1.5 lexicon1 provides a fixed repository of semantic frames and roles, which we use in the experiments below. Several learning and parsing algorithms have been developed for frame-semantic analysis (Johansson and Nugues, 2007; Das et al., 2014; Täckström et al., 2015), and frame semantics has been successfully applied to question-answering (Shen and Lapata, 2007), information extraction (Surdeanu et al., 2003) and knowledge extraction (Søgaard et al., 2015b). 1 https://framenet.icsi.berkeley.edu/ Contributions This paper makes the following three contributions. We present a new multilingual frame-annotated corpus covering five topics, two domains (Wikipedia and Twitter), and nine languages. We implement a simplified version of the frame-semantic parser introduced in Das et al. (2014). Finally, we show how to modify this parser to learn any-language frame-semantic parsing models using inter-lingual word embeddings (Søgaard et al., 2015a). 2 Data annotation Figure 1 depicts a F RAME N ET 1.5 frame-se"
D15-1245,Q15-1003,0,0.0331545,"Missing"
D15-1245,P12-1068,0,0.0523384,"Missing"
D15-1245,J14-1002,0,\N,Missing
D15-1245,P15-1165,1,\N,Missing
D17-1169,S16-1173,0,0.0136269,"Missing"
D17-1169,W16-6208,0,0.0155262,"Missing"
D17-1169,P82-1020,0,0.578107,"Missing"
D17-1169,D16-1104,0,0.0336131,"[Deriu] .51 [Deriu] .62 .75 .51 .85 .88 .54 .87 .92 .58 .88 .93 .58 SCv1 SCv2-GEN F1 F1 .63 [Joshi] .72 [Joshi] .67 .71 .65 .71 .68 .74 .69 .75 Dataset than the pretrained convolutional neural network. These results are therefore excluded. For sarcasm detection we use the sarcasm dataset version 1 and 2 from the Internet Argument Corpus (Walker et al., 2012). Note that results presented on these benchmarks in e.g. Oraby et al. (2016) are not directly comparable as only a subset of the data is available online.3 A state-of-the-art baseline is found by modeling the embedding-based features from Joshi et al. (2016) alongside unigrams, bigrams and trigrams with an SVM. GoogleNews word2vec embeddings (Mikolov et al., 2013) are used for computing the embedding-based features. A hyperparameter search for regularization parameters is carried out using cross-validation. Note that the sarcasm dataset version 2 contains both a quoted text and a sarcastic response, but to keep the models identical across the datasets only the response is used. For training we use the Adam optimizer (Kingma and Ba, 2015) with gradient clipping of the norm to 1. Learning rate is set to 1E−3 for training of all new layers and 1E−4"
D17-1169,W14-1304,0,0.0624187,"Missing"
D17-1169,S12-1033,0,0.0105017,"various NLP tasks1 . 2 Softmax Attention 1 x 2304 BiLSTM T x 1024 BiLSTM T x 1024 Embedding T x 256 Text Related work Using emotional expressions as noisy labels in text to counter scarcity of labels is not a new idea (Read, 2005; Go et al., 2009). Originally, binarized emoticons were used as noisy labels, but later also hashtags and emojis have been used. To our knowledge, previous research has always manually specified which emotional category each emotional expression belong to. Prior work has used theories of emotion such as Ekman’s six basic emotions and Plutchik’s eight basic emotions (Mohammad, 2012; Suttles and Ide, 2013). Such manual categorization requires an understanding of the emotional content of each expression, which is difficult and time-consuming for sophisticated combinations of emotional content. Moreover, any manual selection and categorization is prone to misinterpretations and may omit important details regarding usage. In contrast, our approach requires no prior knowledge of the corpus and can capture diverse usage of 64 types of emojis (see Table 1 for examples and Figure 3 for how the model implicitly groups emojis). Another way of automatically interpreting the emotio"
D17-1169,S16-1001,0,0.0863668,"Missing"
D17-1169,W16-3604,0,0.0226707,"lits are split by us (with splits publicly available). Data used for hyperparameter tuning is taken from the training set. Identifier Study Task Domain Classes Ntrain Ntest SE0714 Olympic PsychExp (Strapparava and Mihalcea, 2007) (Sintsova et al., 2013) (Wallbott and Scherer, 1986) Emotion Emotion Emotion Headlines Tweets Experiences 3 4 7 250 250 1000 1000 709 6480 SS-Twitter SS-Youtube SE1604 (Thelwall et al., 2012) (Thelwall et al., 2012) (Nakov et al., 2016) Sentiment Sentiment Sentiment Tweets Video Comments Tweets 2 2 3 1000 1000 7155 1113 1142 31986 SCv1 SCv2-GEN (Walker et al., 2012) (Oraby et al., 2016) Sarcasm Sarcasm Debate Forums Debate Forums 2 2 1000 1000 995 2260 Table 5: Comparison across benchmark datasets. Reported values are averages across five runs. Variations refer to transfer learning approaches in §3.3 with ‘new’ being a model trained without pretraining. Measure State of the art DeepMoji (new) DeepMoji (full) DeepMoji (last) DeepMoji (chain-thaw) SE0714 Olympic PsychExp F1 F1 F1 .34 [Buechel] .50 [Buechel] .45 [Buechel] .21 .43 .32 .31 .50 .42 .36 .61 .56 .37 .61 .57 SS-Twitter SS-Youtube SE1604 Acc Acc Acc .82 [Deriu] .86 [Deriu] .51 [Deriu] .62 .75 .51 .85 .88 .54 .87 .92 ."
D17-1169,P05-2008,0,0.0146313,"ovements over the stateof-the-art within a range of tasks: emotion, sarcasm and sentiment detection. We present multiple analyses on the effect of pretraining, including results that show that the diversity of our emoji set is important for the transfer learning potential of our model. Our pretrained DeepMoji model is released with the hope that other researchers can use it for various NLP tasks1 . 2 Softmax Attention 1 x 2304 BiLSTM T x 1024 BiLSTM T x 1024 Embedding T x 256 Text Related work Using emotional expressions as noisy labels in text to counter scarcity of labels is not a new idea (Read, 2005; Go et al., 2009). Originally, binarized emoticons were used as noisy labels, but later also hashtags and emojis have been used. To our knowledge, previous research has always manually specified which emotional category each emotional expression belong to. Prior work has used theories of emotion such as Ekman’s six basic emotions and Plutchik’s eight basic emotions (Mohammad, 2012; Suttles and Ide, 2013). Such manual categorization requires an understanding of the emotional content of each expression, which is difficult and time-consuming for sophisticated combinations of emotional content. M"
D17-1169,W13-1603,0,0.0657179,"Missing"
D17-1169,P14-2070,0,0.0207286,"Missing"
D17-1169,N16-1174,0,0.0510416,"use two bidirectional LSTM layers with 1024 hidden units in each (512 in each direction). Finally, an attention layer that take all of these layers as input using skip-connections is used (see Figure 1 for an illustration). The attention mechanism lets the model decide the importance of each word for the prediction task by weighing them when constructing the representation of the text. For instance, a word such as ‘amazing’ is likely to be very informative of the emotional meaning of a text and it should thus be treated accordingly. We use a simple approach inspired by (Bahdanau et al., 2014; Yang et al., 2016) with a single parameter pr. input channel: et = ht wa exp(et ) at = PT i=1 exp(ei ) v= T X ai hi i=1 Here ht is the representation of the word at time step t and wa is the weight matrix for the attention layer. The attention importance scores for each time step, at , are obtained by multiplying the representations with the weight matrix and then normalizing to construct a probability distribution over the words. Lastly, the representation vector for the text, v, is found by a weighted summation over all the time steps using the attention importance scores as weights. This representation vecto"
D17-1169,S07-1013,0,0.305705,"Missing"
D17-1169,P14-1146,0,0.276493,"ble of convolutional neural networks that are pretrained on a private dataset of tweets with emoticons, making it difficult to replicate (Deriu et al., 2016). Instead we pretrain a model with the hyperparameters of the largest model in their ensemble on the positive/negative emoticon dataset from Go et al. (2009). Using this pretraining as an initialization we finetune the model on the target tasks using early stopping on a validation set to determine the amount of training. We also implemented the SentimentSpecific Word Embedding (SSWE) using the embeddings available on the authors’ website (Tang et al., 2014), but found that it performed worse 1619 Table 4: Description of benchmark datasets. Datasets without pre-existing training/test splits are split by us (with splits publicly available). Data used for hyperparameter tuning is taken from the training set. Identifier Study Task Domain Classes Ntrain Ntest SE0714 Olympic PsychExp (Strapparava and Mihalcea, 2007) (Sintsova et al., 2013) (Wallbott and Scherer, 1986) Emotion Emotion Emotion Headlines Tweets Experiences 3 4 7 250 250 1000 1000 709 6480 SS-Twitter SS-Youtube SE1604 (Thelwall et al., 2012) (Thelwall et al., 2012) (Nakov et al., 2016) Se"
D17-1169,walker-etal-2012-corpus,0,0.022912,"sting training/test splits are split by us (with splits publicly available). Data used for hyperparameter tuning is taken from the training set. Identifier Study Task Domain Classes Ntrain Ntest SE0714 Olympic PsychExp (Strapparava and Mihalcea, 2007) (Sintsova et al., 2013) (Wallbott and Scherer, 1986) Emotion Emotion Emotion Headlines Tweets Experiences 3 4 7 250 250 1000 1000 709 6480 SS-Twitter SS-Youtube SE1604 (Thelwall et al., 2012) (Thelwall et al., 2012) (Nakov et al., 2016) Sentiment Sentiment Sentiment Tweets Video Comments Tweets 2 2 3 1000 1000 7155 1113 1142 31986 SCv1 SCv2-GEN (Walker et al., 2012) (Oraby et al., 2016) Sarcasm Sarcasm Debate Forums Debate Forums 2 2 1000 1000 995 2260 Table 5: Comparison across benchmark datasets. Reported values are averages across five runs. Variations refer to transfer learning approaches in §3.3 with ‘new’ being a model trained without pretraining. Measure State of the art DeepMoji (new) DeepMoji (full) DeepMoji (last) DeepMoji (chain-thaw) SE0714 Olympic PsychExp F1 F1 F1 .34 [Buechel] .50 [Buechel] .45 [Buechel] .21 .43 .32 .31 .50 .42 .36 .61 .56 .37 .61 .57 SS-Twitter SS-Youtube SE1604 Acc Acc Acc .82 [Deriu] .86 [Deriu] .51 [Deriu] .62 .75 .51"
D17-1169,W10-3111,0,0.0193783,"Missing"
D17-1258,afantenos-etal-2010-learning,0,0.081493,"r the English RST-DT, we present two performance metrics: • F1 for intra-sentential boundaries only (see Section 7.1), in order to be comparable with state-of-the-art systems; (1) [But maintaining the key components (. . .)]1 [– a stable exchange rate and high levels of imports –]2 [will consume enormous amounts (. . .).]3 We follow previous work on treating this as three segments, but note that this may not be the optimal solution. It introduces a bias: while most of the EDUs are full clauses, EDU 1 and 3 are fragments. Other designs are possible, especially a multi-label setting as done in (Afantenos et al., 2010) for a corpus annotated within the Segmented Discourse Representation Theory (Asher and Lascarides, 2003). While it seems relevant to deal with this issue during segmentation rather than using a pseudo-relation, it introduces new issues (i.e. the final structures are not trees anymore). We thus leave this question for future work. 3.2 Sentence vs document-level segmentation Most of the existing work on discourse segmentation always assume a gold segmentation of the sentences: since an EDU boundary never crosses • and F1 for all EDU boundaries, in order to set up a document-level baseline (see"
D17-1258,P17-2037,1,0.724562,"son and Marcu, 2001). Consequently, existing discourse segmenters heavily rely on information derived from constituent trees usually following the Penn Treebank (PTB) (Marcus et al., 1993) guidelines. Nevertheless constituent trees are not easily available for any language. Finally, even for English, using predicted trees leads to a large drop in per2432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2432–2442 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics formance for discourse segmentation. Recently, Braud et al. (2017) proposed the first cross-lingual and cross-domain experiments for discourse segmentation, relying only on words and Part-of-Speech (POS) tags (morpho-syntactic level). However, they focus on document-level discourse segmentation – preventing from a comparison with previous work –, and they did not include any syntactic information. In this paper, we significantly extend their work by investigating the use of syntactic information, reporting results with various sets of features at the sentence level – varying the settings between gold and predicted, and fine-grained vs coarse grained informat"
D17-1258,W01-1605,0,0.600741,"Missing"
D17-1258,P82-1020,0,0.83024,"Missing"
D17-1258,P14-1002,0,0.116058,"Missing"
D17-1258,J15-3002,0,0.272284,"Missing"
D17-1258,P13-1048,0,0.0375165,"Missing"
D17-1258,C04-1048,0,0.864647,"Missing"
D17-1258,D14-1220,0,0.0316103,"Missing"
D17-1258,J93-2004,0,0.0582786,"Missing"
D17-1258,N03-1030,0,0.692488,"014). For Brazilian Portuguese (Pt-DT), we merged four corpora (Cardoso et al., 2011; Collovini et al., 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al., 2015; Braud et al., 2017). Table 1 summarizes statistics about the data. 6 6.1 Experiments Evaluation For English, on the En-DT, evaluation for discourse segmentation has been done under different conditions. First, all previous systems were evaluated on the same set of 38 documents that initially contains 991 sentences – and more precisely on each sentence of this set for intra-sentential results. However, Soricut and Marcu (2003) do not consider sentences that are not exactly spanned by a discourse subtree (keeping only 941 sentences in the test set), and Sporleder and Lapata (2005) only keep the sentences that contain intra-sentential EDUs (608 sentences). Since we want to give results at the document level, – with the sentence boundaries being predicted as the other EDU boundaries –, there is no reason to remove any sentences. We thus keep all the 991 sentences in the test set as done in (Fisher and Roark, 2007; Xuan Bach et al., 2012) at the sentence level, and in (Braud et al., 2017) at the document level. For the"
D17-1258,H05-1033,0,0.647,", 2004) as done in (Maziero et al., 2015; Braud et al., 2017). Table 1 summarizes statistics about the data. 6 6.1 Experiments Evaluation For English, on the En-DT, evaluation for discourse segmentation has been done under different conditions. First, all previous systems were evaluated on the same set of 38 documents that initially contains 991 sentences – and more precisely on each sentence of this set for intra-sentential results. However, Soricut and Marcu (2003) do not consider sentences that are not exactly spanned by a discourse subtree (keeping only 941 sentences in the test set), and Sporleder and Lapata (2005) only keep the sentences that contain intra-sentential EDUs (608 sentences). Since we want to give results at the document level, – with the sentence boundaries being predicted as the other EDU boundaries –, there is no reason to remove any sentences. We thus keep all the 991 sentences in the test set as done in (Fisher and Roark, 2007; Xuan Bach et al., 2012) at the sentence level, and in (Braud et al., 2017) at the document level. For the other corpora (see Section 5), we either use the official test set (Es-DT, 84 documents) or build a test set containing 38 documents chosen randomly. Secon"
D17-1258,W04-0213,0,0.0345579,"T (from now on called En-DT), the most widely used corpus for this task. This corpus is composed of Wall Street Journal articles, it has been annotated over the Penn Treebank. We also report performance on the SFU review corpus6 (En-SFU-DT) 2436 6 https://www.sfu.ca/˜mtaboada containing product reviews, and on the instructional corpus (En-Instr-DT) (Subba and Di Eugenio, 2009) built on instruction manuals.7 We also evaluate our model across languages. For Spanish, we report performance on the corpus (Es-DT) presented in (da Cunha et al., 2011),8 . For German, we use the Postdam corpus (DeDT) (Stede, 2004; Stede and Neumann, 2014). For Brazilian Portuguese (Pt-DT), we merged four corpora (Cardoso et al., 2011; Collovini et al., 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al., 2015; Braud et al., 2017). Table 1 summarizes statistics about the data. 6 6.1 Experiments Evaluation For English, on the En-DT, evaluation for discourse segmentation has been done under different conditions. First, all previous systems were evaluated on the same set of 38 documents that initially contains 991 sentences – and more precisely on each sentence of this set for intra-sentent"
D17-1258,stede-neumann-2014-potsdam,0,0.255988,"n called En-DT), the most widely used corpus for this task. This corpus is composed of Wall Street Journal articles, it has been annotated over the Penn Treebank. We also report performance on the SFU review corpus6 (En-SFU-DT) 2436 6 https://www.sfu.ca/˜mtaboada containing product reviews, and on the instructional corpus (En-Instr-DT) (Subba and Di Eugenio, 2009) built on instruction manuals.7 We also evaluate our model across languages. For Spanish, we report performance on the corpus (Es-DT) presented in (da Cunha et al., 2011),8 . For German, we use the Postdam corpus (DeDT) (Stede, 2004; Stede and Neumann, 2014). For Brazilian Portuguese (Pt-DT), we merged four corpora (Cardoso et al., 2011; Collovini et al., 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al., 2015; Braud et al., 2017). Table 1 summarizes statistics about the data. 6 6.1 Experiments Evaluation For English, on the En-DT, evaluation for discourse segmentation has been done under different conditions. First, all previous systems were evaluated on the same set of 38 documents that initially contains 991 sentences – and more precisely on each sentence of this set for intra-sentential results. However, Sori"
D17-1258,L16-1680,0,0.0326641,"Missing"
D17-1258,N09-1064,0,0.0719242,"Missing"
D17-1258,P09-2020,0,0.637863,"Missing"
D17-1258,W12-1623,0,0.486911,"Missing"
D18-1042,D16-1250,0,0.30206,"om a one-to-many alignment a, rather than a matching m. Why a Reinterpretation? The reinterpretation of Artetxe et al. (2017) as a probabilistic model yields a clear analytical comparison between our method and theirs. The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce. 6 Experiments We first conduct experiments on bilingual dictionary induction and cross-lingual word similarity on three standard language pairs, English–Italian, English–German, and English–Finnish. 462 Mikolov et al. (2013c) Xing et al. (2015) Zhang et al. (2016) Artetxe et al. (2016) Artetxe et al. (2017) Ours (1:1) Ours (1:1, rank constr.) 5,000 English–Italian 25 num iden 5,000 34.93 36.87 36.73 39.27 39.67 41.00 42.47 00.00 0.00 0.07 0.07 37.27 39.63 41.13 1.87 27.13 28.07 31.07 39.97 41.07 41.80 35.00 41.27 40.80 41.87 40.87 42.60 41.93 0.00 0.13 0.27 0.40 39.40 40.47 41.40 English–German 25 num iden 0.00 0.07 0.13 0.13 39.60 42.40 42.40 0.07 0.53 0.87 0.73 40.27 42.60 41.93 19.20 38.13 38.27 41.53 40.67 43.20 41.47 5,000 25.91 28.23 28.16 30.62 28.72 29.78 28.23 English–Finnish 25 num iden 0.00 0.07 0.14 0.21 28.16 0.07 27.04 0.00 0.56 0.42 0.77 26.47 3.02 27.60 7.02"
D18-1042,P17-1042,0,0.0577102,"g a novel discriminative latent-variable model that outperforms previous work. Our proposed model is a bridge between current state-of-the-art methods in bilingual lexicon induction that take advantage of word embeddings, e.g., the embeddings induced by Mikolov et al. (2013b)’s skip-gram objective, and older ideas in the literature that build explicit probabilistic models for the task. We propose a discriminative probability model, inspired by Irvine and Callison-Burch (2013), infused with the bipartite matching dictionary prior of Haghighi et al. (2008). However, like more recent approaches (Artetxe et al., 2017), our model operates directly over pretrained word embeddings, induces a joint cross-lingual embedding space, and scales to large vocabulary sizes. To train our model, we derive a generalized expectationmaximization algorithm (EM; Neal and Hinton, 1998) and employ an efficient matching algorithm. Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over sever"
D18-1042,J93-2003,0,0.197772,"and Hinton, 1998) and employ an efficient matching algorithm. Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of Artetxe et al. (2017) as a latent-variable model with an IBM Model 1–style (Brown et al., 1993) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages’ respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017). 2 Background: Bilingual Lexicon Induction and Word Embeddings Bilingual lexicon induction2 is the task of finding word-level translations betwee"
D18-1042,P15-2001,0,0.0227503,"h Søgaard et al. (2018), we additionally use a dictionary of identically spelled strings in both vocabularies. Table 2: Spearman correlations on English–Italian and English–German cross-lingual word similarity datasets. 6.1 Experimental Details Datasets For bilingual dictionary induction, we use the English–Italian dataset by Dinu et al. (2015) and the English–German and English–Finnish datasets by Artetxe et al. (2017). For cross-lingual word similarity, we use the RG-65 and WordSim353 cross-lingual datasets for English–German and the WordSim-353 cross-lingual dataset for English– Italian by Camacho-Collados et al. (2015). Monolingual Embeddings We follow Artetxe et al. (2017) and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 billion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl). Implementation details Similar to Artetxe et al. (2017), we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 × 10−6 between succeeding iterations. Unless stated other"
D18-1042,W95-0114,0,0.626272,"ilingual lexicon and (bilingual) dictionary synonymously. On the other hand, unmodified lexicon always refers to a word list in a single language. 458 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 458–468 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics equivalent, so the pair Hund–dog should be an entry in a German–English bilingual lexicon. The task itself comes in a variety of flavors. We consider a version of the task that only relies on monolingual corpora in the tradition of Rapp (1995) and Fung (1995). In other words, the goal is to produce a bilingual lexicon primarily from unannotated raw text in each of the respective languages. Importantly, we avoid reliance on bitext, i.e. corpora with parallel sentences that are known translations of each other, e.g., EuroParl (Koehn, 2005). The bitext assumption is quite common in the literature; see Ruder et al. (2018, Table 2) for a survey. Additionally, we will assume the existence of a small seed set of word-level translations obtained from a dictionary; we also experiment with seed sets obtained from heuristics that do not rely on the existence"
D18-1042,P08-1088,0,0.333996,"ova KementchedjhievaZ Anders SøgaardZ @Insight Research Centre, National University of Ireland, Galway, Ireland HAylien Ltd., Dublin, Ireland SThe Computer Laboratory, University of Cambridge, Cambridge, UK PDepartment of Computer Science, Johns Hopkins University, Baltimore, USA ZDepartment of Computer Science, University of Copenhagen, Copenhagen, Denmark sebastian@ruder.io,ryan.cotterell@jhu.com,{yova|soegaard}@di.ku.dk Abstract We introduce a novel discriminative latentvariable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embeddingbased approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.1 1 Introduction Is there a more fundamental bilingual linguistic resource than a dictionary? The task of bilingual lexicon induction seeks to create a dictionary in a datadriven manner di"
D18-1042,N13-1056,0,0.0178519,"ntiev et al., 2012) to cross-lingual named entity recognition (Mayhew et al., 2017). In this work, we offer a probabilistic twist on the task, developing a novel discriminative latent-variable model that outperforms previous work. Our proposed model is a bridge between current state-of-the-art methods in bilingual lexicon induction that take advantage of word embeddings, e.g., the embeddings induced by Mikolov et al. (2013b)’s skip-gram objective, and older ideas in the literature that build explicit probabilistic models for the task. We propose a discriminative probability model, inspired by Irvine and Callison-Burch (2013), infused with the bipartite matching dictionary prior of Haghighi et al. (2008). However, like more recent approaches (Artetxe et al., 2017), our model operates directly over pretrained word embeddings, induces a joint cross-lingual embedding space, and scales to large vocabulary sizes. To train our model, we derive a generalized expectationmaximization algorithm (EM; Neal and Hinton, 1998) and employ an efficient matching algorithm. Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bil"
D18-1042,K18-1021,1,0.770121,"nking neighbor lists. Lazaridou et al. (2015) proposed a max-marging objective as a solution, while more recent approaches proposed to modify the nearest neighbor retrieval by inverting the softmax (Smi, 2017) or scaling the similarity values (Conneau et al., 2018). 9 Table 5: Example translations for German-English. (2017) in German and seek their nearest neighbours in the English embedding space. P@1 over the German-English test set is 36.38 and 39.18 for Artetxe et al. (2017) and our method respectively. We show examples where nearest neighbours of the methods differ in Table 5. Similar to Kementchedjhieva et al. (2018), we find that morphologically related words are often the source of mistakes. Other common sources of mistakes in this dataset are names that are translated to different names and nearly synonymous words being predicted. Both of these sources indicate that while the learned alignment is generally good, it is often not sufficiently precise. 8 Conclusion We have presented a novel latent-variable model for bilingual lexicon induction, building on the work of Artetxe et al. (2017). Our model combines the prior over bipartite matchings inspired by Haghighi et al. (2008) and the discriminative, rat"
D18-1042,E12-1014,0,0.0237578,"ally helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.1 1 Introduction Is there a more fundamental bilingual linguistic resource than a dictionary? The task of bilingual lexicon induction seeks to create a dictionary in a datadriven manner directly from monolingual corpora in the respective languages and, perhaps, a small seed set of translations. From a practical point of view, bilingual dictionaries have found uses in a myriad of NLP tasks ranging from machine translation (Klementiev et al., 2012) to cross-lingual named entity recognition (Mayhew et al., 2017). In this work, we offer a probabilistic twist on the task, developing a novel discriminative latent-variable model that outperforms previous work. Our proposed model is a bridge between current state-of-the-art methods in bilingual lexicon induction that take advantage of word embeddings, e.g., the embeddings induced by Mikolov et al. (2013b)’s skip-gram objective, and older ideas in the literature that build explicit probabilistic models for the task. We propose a discriminative probability model, inspired by Irvine and Callison"
D18-1042,2005.mtsummit-papers.11,0,0.0965587,"1 - November 4, 2018. 2018 Association for Computational Linguistics equivalent, so the pair Hund–dog should be an entry in a German–English bilingual lexicon. The task itself comes in a variety of flavors. We consider a version of the task that only relies on monolingual corpora in the tradition of Rapp (1995) and Fung (1995). In other words, the goal is to produce a bilingual lexicon primarily from unannotated raw text in each of the respective languages. Importantly, we avoid reliance on bitext, i.e. corpora with parallel sentences that are known translations of each other, e.g., EuroParl (Koehn, 2005). The bitext assumption is quite common in the literature; see Ruder et al. (2018, Table 2) for a survey. Additionally, we will assume the existence of a small seed set of word-level translations obtained from a dictionary; we also experiment with seed sets obtained from heuristics that do not rely on the existence of linguistic resources. m richtig bird Vogel mother trinken drink Mutter right Wurzel eye werfen ash usrc rot utrg Figure 1: Partial lexicons of German and English shown as a 2.1 bipartite graph. German is the target language and English is the source language. The ntrg = 7 German"
D18-1042,P15-1027,0,0.605353,"results for our approach in comparison to the baselines in Figure 2 for English–Italian using a 5,000 word seed lexicon across vocabularies consisting of different 12 Other recent improvements such as symmetric reweighting (Artetxe et al., 2018) are orthogonal to our method, which is why we do not explicitly compare to them here. 13 Note that results are not directly comparable to (Conneau et al., 2018) due to the use of embeddings trained on different monolingual corpora (WaCKy vs. Wikipedia). Hubness problem We analyze empirically whether the prior helps with the hubness problem. Following Lazaridou et al. (2015), we define the hubness Nk (y) at k of a target word y as follows: Nk (y) = |{x ∈ Q |y ∈ NNk (x, G)}| (14) where Q is a set of query source language words and NNk (x, G) denotes the k nearest neighbors 464 14 We only use the words in the 5,000 word seed lexicon that are contained in the n most frequent words. We do not show results for the 25 word seed lexicon and numerals as they are not contained in the smallest n of most frequent words. (a) English–Italian (b) English–German (c) English–Finnish Figure 3: Bilingual dictionary induction results of our method with different priors using a 5,00"
D18-1042,D17-1269,0,0.108922,"Missing"
D18-1042,P95-1050,0,0.637006,"paper, we use bilingual lexicon and (bilingual) dictionary synonymously. On the other hand, unmodified lexicon always refers to a word list in a single language. 458 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 458–468 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics equivalent, so the pair Hund–dog should be an entry in a German–English bilingual lexicon. The task itself comes in a variety of flavors. We consider a version of the task that only relies on monolingual corpora in the tradition of Rapp (1995) and Fung (1995). In other words, the goal is to produce a bilingual lexicon primarily from unannotated raw text in each of the respective languages. Importantly, we avoid reliance on bitext, i.e. corpora with parallel sentences that are known translations of each other, e.g., EuroParl (Koehn, 2005). The bitext assumption is quite common in the literature; see Ruder et al. (2018, Table 2) for a survey. Additionally, we will assume the existence of a small seed set of word-level translations obtained from a dictionary; we also experiment with seed sets obtained from heuristics that do not rely"
D18-1042,N12-1087,0,0.0176416,"tep. Viterbi EM estimates the parameters by alternating between the two steps until convergence. We give the complete pseudocode in Algorithm 1. 4.1 Viterbi E-Step The E-step asks us to compute the posterior of latent bipartite matchings p(m |S, T ). Computation of this distribution, however, is intractable as it would require a sum over all bipartite matchings, which is #P-hard (Valiant, 1979). Tricks from combinatorial optimization make it possible to maximize over all bipartite matchings in polynomial time. Thus, we fall back on the Viterbi approximation for the E-step (Brown et al., 1993; Samdani et al., 2012). The derivation will follow Haghighi et al. (2008). In order to compute m? = argmax log pθ (m |S, T ) Finding a Maximal Bipartite Matching We frame finding an optimal one-to-one alignment between nsrc source and ntrg words as a combinatorial optimization problem, specifically, a linear assignment problem (LAP; Bertsimas and Tsitsiklis, 1997). In its original formulation, the LAP requires assigning a number of agents (source words) to a number of tasks (target words) at a cost that varies based on each assignment. An optimal solution assigns each source word to exactly one target word and vice"
D18-1042,P18-1072,1,0.88512,"Missing"
D18-1042,P16-1024,0,0.136192,"Missing"
D18-1042,N15-1104,0,0.514311,"source cannot be used more than once.4 (ii) There exists an orthogonal transformation, after which the embedding spaces are more or less equivalent. Assumption (i) may be true for related languages, but is likely false for morphologically rich languages that have a many-to-many relationship between the words in their respective lexicons. We propose to ameliorate this using a rank constraint that only considers the top n most frequent words in both lexicons for matching in §6. In addition, we experiment with priors that express different matchings in §7. As for assumption (ii), previous work (Xing et al., 2015; Artetxe et al., 2017) has achieved some success using an orthogonal transformation; recently, however, Søgaard et al. (2018) demonstrated that monolingual embedding spaces are not approximately isomorphic and that there is a complex relationship between word form and meaning, which is only inadequately modeled by current approaches, which for example cannot model polysemy. Nevertheless, we will show that imbuing our model with these assumptions helps empirically in §6, giving them practical utility. Why it Works: The Hubness Problem Why should we expect the bipartite matching prior to help,"
D18-1042,N16-1156,0,0.309433,"Sa and Ta formed from a one-to-many alignment a, rather than a matching m. Why a Reinterpretation? The reinterpretation of Artetxe et al. (2017) as a probabilistic model yields a clear analytical comparison between our method and theirs. The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce. 6 Experiments We first conduct experiments on bilingual dictionary induction and cross-lingual word similarity on three standard language pairs, English–Italian, English–German, and English–Finnish. 462 Mikolov et al. (2013c) Xing et al. (2015) Zhang et al. (2016) Artetxe et al. (2016) Artetxe et al. (2017) Ours (1:1) Ours (1:1, rank constr.) 5,000 English–Italian 25 num iden 5,000 34.93 36.87 36.73 39.27 39.67 41.00 42.47 00.00 0.00 0.07 0.07 37.27 39.63 41.13 1.87 27.13 28.07 31.07 39.97 41.07 41.80 35.00 41.27 40.80 41.87 40.87 42.60 41.93 0.00 0.13 0.27 0.40 39.40 40.47 41.40 English–German 25 num iden 0.00 0.07 0.13 0.13 39.60 42.40 42.40 0.07 0.53 0.87 0.73 40.27 42.60 41.93 19.20 38.13 38.27 41.53 40.67 43.20 41.47 5,000 25.91 28.23 28.16 30.62 28.72 29.78 28.23 English–Finnish 25 num iden 0.00 0.07 0.14 0.21 28.16 0.07 27.04 0.00 0.56 0.42 0.77"
D18-1056,D14-1162,0,0.0888872,"a linear generator are (Ω, w). They are obtained by solving the following min-max problem: min max E[log(Dw (X)) + log(1 − Dw (gΩ (Z)))] Ω w (1) 2.3 which reduces to min JS (PX |PΩ ) Ω Below we summarize some previous findings about the geometry of monolingual embeddings (Mimno and Thompson, 2017), and add some new observations. We discuss five embedding algorithms: SVD on positive PMI matrices (Hyperwords-SVD) (Levy et al., 2015), skip-gram negative sampling applied to co-occurrence matrices (Hyperwords-SGNS) (Levy et al., 2015), continuous bag-of-words (CBOW) (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). To analyze the geometry of our monolingual embeddings in space, we report average inner product to mean vector; see Mimno and Thompson (2017) for details. (2) Ω is initialized as the identity matrix I. If G wins the game against an ideal discriminator on a very large number of samples, then F (the source vector space) and ΩE (with E being the target vector space) can be shown to be close in Jensen-Shannon divergence, and thus the model has learned the true distribution. This result, referring to the distributions of the data, pdata , and the distributi"
D18-1056,P18-1072,1,0.878029,"Missing"
D18-1056,P17-1179,0,0.06774,"Missing"
D18-1056,Q17-1010,0,0.35089,"obtained by solving the following min-max problem: min max E[log(Dw (X)) + log(1 − Dw (gΩ (Z)))] Ω w (1) 2.3 which reduces to min JS (PX |PΩ ) Ω Below we summarize some previous findings about the geometry of monolingual embeddings (Mimno and Thompson, 2017), and add some new observations. We discuss five embedding algorithms: SVD on positive PMI matrices (Hyperwords-SVD) (Levy et al., 2015), skip-gram negative sampling applied to co-occurrence matrices (Hyperwords-SGNS) (Levy et al., 2015), continuous bag-of-words (CBOW) (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). To analyze the geometry of our monolingual embeddings in space, we report average inner product to mean vector; see Mimno and Thompson (2017) for details. (2) Ω is initialized as the identity matrix I. If G wins the game against an ideal discriminator on a very large number of samples, then F (the source vector space) and ΩE (with E being the target vector space) can be shown to be close in Jensen-Shannon divergence, and thus the model has learned the true distribution. This result, referring to the distributions of the data, pdata , and the distribution, pg , G is sampling from, is from Goo"
D18-1056,Q15-1016,0,0.0579077,"Missing"
D18-1515,D16-1084,1,0.852568,"k-specific classification layer for each output. The hyper-parameters, after doing grid search, optimizing performance on the validation data, are given in Figure 2. LSTM baseline We compare our MLP ranker to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) model. It takes two sequences inputs: sequence 1 and sequence 2, and a stack of three bidirectional LSTM layers, which encode sequence 1 and sequence 2, respectively. The outputs are then concatenated, to enable representing the differences between the two sequences. Instead of relying only on this presentation (Bowman et al., 2015; Augenstein et al., 2016), we also concatenate our distance features and feed everything into our MLP ranker described above. 3 Datasets For our experiments, we use data from SemEval shared tasks, but we also take advantage of potential synergies with other existing datasets for classification of sentence pairs. Below we present the datasets used for our main and auxiliary tasks. We provide some summary statistics for each dataset in Table 3. SemEval 2016 and 2017 As our main dataset we use the queries from SemEval’s subtask B which consists of an original query and 10 possibly related queries. As an auxiliary task, w"
D18-1515,N18-1172,1,0.831957,"CONTRADICTION or NEUTRAL , given a hypothesis and a premise. We use the MNLI dataset as opposed to the SNLI data (Bowman et al., 2015; Nangia et al., 2017), since it contains different genres. Our model is not built to be a strong NLI system; we use the similarity between premise and hypothesis as a weak signal to improve the generalization on our main task. Fake News Challenge The Fake News Challenge2 (FNC) was introduced to combat misleading and false information online. This task has been used before in a multi-task setting as a way to utilize general information about pairwise relations (Augenstein et al., 2018). Formally, the FNC task consists in, given a headline and the body of 2 http://www.fakenewschallenge.org/ text which can be from the same news article or not, classify the stance of the body of text relative to what is claimed in the headline. There are four labels: • AGREES: The body of the article is in agreement with the headline • D ISAGREES: The body of the article is in disagreement with the headline • D ISCUSSES: The body of the article does not take a position • U NRELATED: the body of the article discusses a different topic We include fake news detection as a weak auxiliary signal th"
D18-1515,P98-1013,0,0.0965072,"imple model introduces a new strong baseline which is particularly useful when there is a lack of labeled data. Acknowledgments Table 3: We perform an ablation test, where we remove one feature at a time and report performance on development data of our single-task baseline. We observe that our baseline suffers most from removing the Euclidean distance over trigrams and the cosine similarity over unigrams. Note also that the Jaccard index over unigrams seems to carry a strong signal, albeit a very simple feature. distributed representations of words, knowledge graphs and frames from FrameNet (Baker et al., 1998) as some of their features, and used SVMs for ranking. For a more direct comparison, we also train a more expressive model than the simple MTLbased model we propose. This architecture is based on bi-directional LSTMs (Hochreiter and Schmidhuber, 1997). For this model, we input sequences of embedded words (using pre-trained word embeddings) from each query into independent BiLSTM blocks and output a vector representation for each query. We then concatenate the vector representations with the similarity features from our MTL model and feed it into a dense layer and a classification layer. This w"
D18-1515,S16-1138,0,0.0570048,"Missing"
D18-1515,D15-1075,0,0.242462,"r and, finally, a task-specific classification layer for each output. The hyper-parameters, after doing grid search, optimizing performance on the validation data, are given in Figure 2. LSTM baseline We compare our MLP ranker to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) model. It takes two sequences inputs: sequence 1 and sequence 2, and a stack of three bidirectional LSTM layers, which encode sequence 1 and sequence 2, respectively. The outputs are then concatenated, to enable representing the differences between the two sequences. Instead of relying only on this presentation (Bowman et al., 2015; Augenstein et al., 2016), we also concatenate our distance features and feed everything into our MLP ranker described above. 3 Datasets For our experiments, we use data from SemEval shared tasks, but we also take advantage of potential synergies with other existing datasets for classification of sentence pairs. Below we present the datasets used for our main and auxiliary tasks. We provide some summary statistics for each dataset in Table 3. SemEval 2016 and 2017 As our main dataset we use the queries from SemEval’s subtask B which consists of an original query and 10 possibly related querie"
D18-1515,S16-1172,0,0.0263758,"he SemEval shared tasks on CQA, several authors used complex recurrent and convolutional neural network architectures (Severyn and Moschitti, 2015; Barr´on-Cedeno et al., 2016). For example, Barr´on-Cedeno et al. used a convolutional neural network in combination with feature vectors representing lexical, syntactic, and semantic similarity as well as tree kernels. Their performance was slightly lower than the best system (SemEval-Best for 2016 in Table 1). The best system used lexical and semantic similarity measures in combination with a ranking model based on support vector machines (SVMs) (Filice et al., 2016; Franco-Salvador et al., 2016). Both systems are harder to implement and train than the model we propose here. For SemEval-17, FrancoSalvador et al. (2016), the winning team used 4813 F EATURE R EMOVED MAP ACC C OSINE unigram trigram embedding 69.25 69.93 69.11 74.99 76.28 76.40 M ANHATTAN unigram trigram embedding 70.33 69.29 66.90 76.71 76.28 75.28 B HATTACHARYA unigram trigram embedding 70.83 71.14 71.72 75.85 77.50 77.28 E UCLIDEAN unigram trigram embedding 71.43 65.55 70.60 76.57 76.14 75.57 JACCARD unigram trigram 67.41 69.27 75.70 75.98 ward neural architecture is appropriate for a sma"
D18-1515,S17-2003,0,0.0474685,"Missing"
D18-1515,S15-2047,0,0.077163,"Missing"
D18-1515,W17-5301,0,0.0217855,"baselines and the best SemEval systems. We show results for three auxiliary tasks, Question-Comment relevancy prediction, Fake News detection and Natural Language Inference. The asterisks for the MTL results represent the significance of the improvements over the STL systems with ** representing a p-value of < 0.01 and * representing a p-value between 0.01 and 0.05 Natural Language Inference Natural Language Inference (NLI), consists in predicting EN TAILMENT, CONTRADICTION or NEUTRAL , given a hypothesis and a premise. We use the MNLI dataset as opposed to the SNLI data (Bowman et al., 2015; Nangia et al., 2017), since it contains different genres. Our model is not built to be a strong NLI system; we use the similarity between premise and hypothesis as a weak signal to improve the generalization on our main task. Fake News Challenge The Fake News Challenge2 (FNC) was introduced to combat misleading and false information online. This task has been used before in a multi-task setting as a way to utilize general information about pairwise relations (Augenstein et al., 2018). Formally, the FNC task consists in, given a headline and the body of 2 http://www.fakenewschallenge.org/ text which can be from th"
D18-1543,N18-1083,1,0.545746,"a parser across languages in the context of a probabilistic parser. Options we do not explore here are learning the architecture jointly with optimizing the task objective (Misra et al., 2016; Ruder et al., 2017), or learning an architecture search model that predicts an architecture based on the properties of datasets, typically with reinforcement learning (Zoph and Le, 2017; Wong and Gesmundo, 2018; Liang et al., 2018). We also do not explore the option of sharing selectively based on more ﬁne-grained typological information about languages, which related work has indicated could be useful (Bjerva and Augenstein, 2018). Rather, we stick to sharing between languages of the same language families. The strategies explored here do not exhaust the space of possible parameter sharing strategies. For example, we completely ignore soft sharing based on mean-constrained regularisation (Duong et al., 2015). 8 Conclusions We present evaluations of 27 parameter sharing strategies for the Uppsala parser across 10 languages, representing ﬁve language pairs from ﬁve different language families. We repeated the experiment with pairs of unrelated languages. We made several observations: (a) Generally, multitask learning hel"
D18-1543,P15-1166,0,0.0238529,"evaluated on development data); B EST and W ORST are the overall best and worst sharing strategy across languages; C HAR shares only the character-based LSTM parameters; W ORD shares only the word-based LSTM parameters; A LL shares all parameters. refers to hard sharing, ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not sharing. � (soft sharing of word parameters, hard sharing of the rest) improves parsing accuracy when training on related languages, and is especially useful in the low resource case. Similar effects have been observed in machine translation (Dong et al., 2015; Johnson et al., 2017), for example. Most studies have only explored a small number of parameter sharing strategies, however. Vilares et al. (2016) evaluate parsing with hard parameter sharing for 100 language pairs with a statistical parser. Naseem et al. (2012) proposed to selectively share subsets of a parser across languages in the context of a probabilistic parser. Options we do not explore here are learning the architecture jointly with optimizing the task objective (Misra et al., 2016; Ruder et al., 2017), or learning an architecture search model that predicts an architecture based on"
D18-1543,P15-2139,0,0.475239,"alidation data. This model is linguistically motivated and obtains signiﬁcant improvements over a mono-lingually trained baseline. We also ﬁnd that sharing transition classiﬁer parameters helps when training a parser on unrelated language pairs, but we ﬁnd that, in the case of unrelated languages, sharing too many parameters does not help. 1 Introduction The idea of sharing parameters between parsers of related languages goes back to early work in crosslingual adaptation (Zeman and Resnik, 2008), and the idea has recently received a lot of interest in the context of neural dependency parsers (Duong et al., 2015; Ammar et al., 2016; Susanto and Lu, 2017). Modern neural dependency parsers, however, use different sets of parameters for representation and scoring, and it is not clear what parameters it is best to share. The Universal Dependencies (UD) project (Nivre et al., 2016), which is seeking to harmonize the annotation of dependency treebanks across ∗ Work carried out during a stay at the University of Copenhagen. languages, has seen a steady increase in languages that have a treebank in a common standard. Many of these languages are low resource and have small UD treebanks. It seems interesting t"
D18-1543,Q16-1023,0,0.0441809,"rs is tuned. The novel architecture signiﬁcantly outperforms our monolingual baseline on our set of 10 languages. We additionally investigate parameter sharing of unrelated languages. 2 The Uppsala dependency parser The Uppsala parser (de Lhoneux et al., 2017a,b) consists of three sets of parameters; the parameters of the character-based LSTM, those of the word-based LSTM, and the parameters of the MLP that predicts transitions. The character-based LSTM produces representations for the wordbased LSTM, which produces representations for the MLP. The Uppsala parser is a transition-based parser (Kiperwasser and Goldberg, 2016), adapted to the Universal Dependencies (UD) scheme,1 and using the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a static-dynamic oracle, as described in de Lhoneux et al. (2017b). The S WAP 1 http://universaldependencies.org/ transition is used to generate non-projective dependency trees (Nivre, 2009). For an input sentence of length n with words w1 , . . . , wn , the parser creates a sequence of vectors x1:n , where the vector xi representing wi is the concatenation of a word embedding and the ﬁnal state of the character-based LSTM after proce"
D18-1543,P11-1068,0,0.107639,"Missing"
D18-1543,K17-3022,1,0.895862,"Missing"
D18-1543,W17-6314,1,0.900687,"Missing"
D18-1543,P12-1066,0,0.340049,"d sharing, ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not sharing. � (soft sharing of word parameters, hard sharing of the rest) improves parsing accuracy when training on related languages, and is especially useful in the low resource case. Similar effects have been observed in machine translation (Dong et al., 2015; Johnson et al., 2017), for example. Most studies have only explored a small number of parameter sharing strategies, however. Vilares et al. (2016) evaluate parsing with hard parameter sharing for 100 language pairs with a statistical parser. Naseem et al. (2012) proposed to selectively share subsets of a parser across languages in the context of a probabilistic parser. Options we do not explore here are learning the architecture jointly with optimizing the task objective (Misra et al., 2016; Ruder et al., 2017), or learning an architecture search model that predicts an architecture based on the properties of datasets, typically with reinforcement learning (Zoph and Le, 2017; Wong and Gesmundo, 2018; Liang et al., 2018). We also do not explore the option of sharing selectively based on more ﬁne-grained typological information about languages, which re"
D18-1543,P09-1040,0,0.040354,"STM, and the parameters of the MLP that predicts transitions. The character-based LSTM produces representations for the wordbased LSTM, which produces representations for the MLP. The Uppsala parser is a transition-based parser (Kiperwasser and Goldberg, 2016), adapted to the Universal Dependencies (UD) scheme,1 and using the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a static-dynamic oracle, as described in de Lhoneux et al. (2017b). The S WAP 1 http://universaldependencies.org/ transition is used to generate non-projective dependency trees (Nivre, 2009). For an input sentence of length n with words w1 , . . . , wn , the parser creates a sequence of vectors x1:n , where the vector xi representing wi is the concatenation of a word embedding and the ﬁnal state of the character-based LSTM after processing the characters of wi . The character vector ch(wi ) is obtained by running a (bi-directional) LSTM over the characters chj (1 ≤ j ≤ m) of wi . Each input element is represented by the word-level, bi-directional LSTM, as a vector vi = B I L STM(x1:n , i). For each conﬁguration, the feature extractor concatenates the LSTM representations of core"
D18-1543,K18-2011,1,0.882793,"Missing"
D18-1543,P17-2007,0,0.118529,"ally motivated and obtains signiﬁcant improvements over a mono-lingually trained baseline. We also ﬁnd that sharing transition classiﬁer parameters helps when training a parser on unrelated language pairs, but we ﬁnd that, in the case of unrelated languages, sharing too many parameters does not help. 1 Introduction The idea of sharing parameters between parsers of related languages goes back to early work in crosslingual adaptation (Zeman and Resnik, 2008), and the idea has recently received a lot of interest in the context of neural dependency parsers (Duong et al., 2015; Ammar et al., 2016; Susanto and Lu, 2017). Modern neural dependency parsers, however, use different sets of parameters for representation and scoring, and it is not clear what parameters it is best to share. The Universal Dependencies (UD) project (Nivre et al., 2016), which is seeking to harmonize the annotation of dependency treebanks across ∗ Work carried out during a stay at the University of Copenhagen. languages, has seen a steady increase in languages that have a treebank in a common standard. Many of these languages are low resource and have small UD treebanks. It seems interesting to ﬁnd out ways to leverage the wealth of in"
D18-1543,P16-2069,0,0.0948361,"Missing"
D18-1543,I08-3008,0,0.140566,"n classiﬁer is shared, and the sharing of word and character parameters is controlled by a parameter that can be tuned on validation data. This model is linguistically motivated and obtains signiﬁcant improvements over a mono-lingually trained baseline. We also ﬁnd that sharing transition classiﬁer parameters helps when training a parser on unrelated language pairs, but we ﬁnd that, in the case of unrelated languages, sharing too many parameters does not help. 1 Introduction The idea of sharing parameters between parsers of related languages goes back to early work in crosslingual adaptation (Zeman and Resnik, 2008), and the idea has recently received a lot of interest in the context of neural dependency parsers (Duong et al., 2015; Ammar et al., 2016; Susanto and Lu, 2017). Modern neural dependency parsers, however, use different sets of parameters for representation and scoring, and it is not clear what parameters it is best to share. The Universal Dependencies (UD) project (Nivre et al., 2016), which is seeking to harmonize the annotation of dependency treebanks across ∗ Work carried out during a stay at the University of Copenhagen. languages, has seen a steady increase in languages that have a treeb"
D18-1543,W14-4606,0,0.0296745,"0.5 0.8 0.1 0.3 -0.1 1.0 1.4 1.0 0.8 78.8 78.2 0.6 � � ID ID ✗ � � ID ✗ ✗ ✗ � � ID ✗ av. Table 3: LAS on the test sets of the best of 9 sharing strategies and the monolingual baseline. δ is the difference between O URS AND M ONO . 6 Unrelated languages We repeated the same set of experiments with unrelated language pairs. We hypothesise that parameter sharing between unrelated language pairs will be less useful in general than with related language pairs. However, it can still be useful, it has been shown previously that unrelated languages can beneﬁt from being trained jointly. For example, Lynn et al. (2014) have shown that Indonesian was surprisingly particularly useful for Irish. The results are presented in Table 4. The table only presents part of the results, the rest can be found in the supplementary material. As expected, there is much less to be gained from sharing parameters between unrelated pairs. However, it is possible to improve the monolingual baseline by sharing some of the parameters. In general, sharing the MLP is still a helpful thing to do. It is most helpful to share the MLP and optionally one of the two other sets of parameters. Results are close to the monolingual baseline w"
D19-1102,W17-0401,0,0.157942,"Missing"
D19-1102,K17-2002,0,0.0571064,"Missing"
D19-1102,Q17-1010,0,0.0186991,") for Galician and Uppsala (Smith et al., 2018) for Kazakh. rank shows our best model position in the shared task ranking for each treebank. a strong baseline, in a case when we have access to pre-trained word embeddings, for the source and/or the target languages. We treat a pre-trained word embedding as an external embedding, and concatenate it with the other representations, i.e., modifying Eq. 3 to xi = [ew (wi ); ep (wi ); ec (wi ); lk ], where ep (wi ) represents a pre-trained word embedding of wi , which we update during training. We use the pre-trained monolingual fastText embeddings (Bojanowski et al., 2017).9 We concatenate the source and target pre-trained word embeddings.10 For our experiments with transliteration (§2.4), we transliterate the entries of both the source and the target pre-trained word embeddings. To see how our best approach (i.e., cross-lingual model with MORPH augmentation) compares with the current state-of-the-art models, we compare it to the recent results from CoNLL 2018 shared task. Training state-of-the-art models may require lots of engineering and data resources. Our goal, however, is not to achieve the best performance, but rather to systematically investigate how fa"
D19-1102,P17-2090,0,0.0294277,"ng substantial research (Tiedemann and Agic, 2016; Agi´c, 2017; Rosa and Mareˇcek, 2018), along with the VarDial and the CoNLL UD shared tasks (Zampieri et al., 2017; Zeman et al., 2017, 2018). But low-resource parsing is still difficult. The organizers of the CoNLL 2018 UD shared task (Zeman et al., 2018) report that, in general, results on the task’s nine low-resource treebanks “are extremely low and the outputs are hardly useful for Each of these scenarios requires different approaches. Data augmentation is applicable in all scenarios, and has proven useful for low-resource NLP in general (Fadaee et al., 2017; Bergmanis et al., 2017; Sahin and Steedman, 2018). Transfer learning via cross-lingual training is applicable in scenarios 2 and 3. Finally, transliteration may be useful in scenario 3. To keep our scenarios as realistic as possible, we assume that no taggers are available since this would entail substantial annotation. Therefore, our neural parsing models must learn to parse from words or characters—that is, they must be lexicalized—even though there may be little shared vocabulary between source and target treebanks. While this may intuitively seem to make crosslingual training difficult,"
D19-1102,W17-6303,0,0.143051,"Missing"
D19-1102,N18-1108,0,0.0200389,"nstituent order (rotation), which may benefit languages with flexible word order and rich morphology. Some of our low-resource languages have these properties—while North Sámi has a fixed word order (SVO), Galician and Kazakh have relatively free word order. All three languages use case marking on nouns, so word order may not be as important for correct attachment. Both rotation and cropping can produce many trees. We use the default parameters given in (Sahin and Steedman, 2018). 1106 2.2 Data augmentation by nonce sentence generation (Nonce) Our next data augmentation method is adapted from Gulordava et al. (2018). The main idea is to create nonce sentences by replacing some of the words which have the same syntactic annotations. For each training sentence, we replace each content word—nouns, verbs, or adjective— with an alternative word having the same universal POS, morphological features, and dependency label.2 Specifically, for each content word, we first stochastically choose whether to replace it; then, if we have chosen to replace it, we uniformly sample the replacement word type meeting the corresponding constraints. For instance, given a sentence “He borrowed a book from the library.”, we can"
D19-1102,P18-1031,0,0.0209436,"iteration for other language pairs. While we have not exhausted all the possible techniques (e.g., use of external resources (Rasooli and Collins, 2017; Rosa and Mareˇcek, 2018), predicted POS (Ammar et al., 2016), multiple source treebanks (Lim et al., 2018; Stymne et al., 2018), among others), we show that simple methods which leverage the linguistic annotations in the treebank can improve low-resource parsing. Future work might explore different augmentation methods, such as the use of synthetic source treebanks (Wang and Eisner, 2018) or contextualized language model (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018) for scoring the augmented data (e.g., using perplexity). Finally, while the techniques presented in this paper might be applicable to other low-resource languages, we want to also highlight the importance of understanding the characteristics of languages being studied. For example, we showed that although North Sami and Finnish do not share vocabulary, cross-lingual training is still helpful because they share similar syntactic structures. Different language pairs might benefit from other types of similarity (e.g., morphological) and investigating this would be another i"
D19-1102,D11-1006,0,0.328528,"Missing"
D19-1102,P09-1040,0,0.0248166,"facilitating different pronunciations.4 We map these to their basic Latin counterparts, e.g., ‘c¸’ to ‘c’. For Kazakh, we use a simple dictionary created by a Kazakh computational linguist to map each Cyrillic letter to the basic Latin alphabet.5 . Experimental Setup Dependency Parsing Model We use the Uppsala parser, a transition-based neural dependency parser (de Lhoneux et al., 2017a,b; Kiperwasser and Goldberg, 2016). The parser uses an arc-hybrid transition system (Kuhlmann et al., 2011), extended with a static-dynamic oracle and SWAP transition to allow non-projective dependency trees (Nivre, 2009). Let w = w0 , . . . , w|w |be an input sentence of length |w |and let w0 represent an artificial ROOT token. We create a vector representation for each input token wi by concatenating (; ) its word embedding, ew (wi ) and its character-based word embedding, ec (wi ): xi = [ew (wi ); ec (wi )] (1) Here, ec (wi ) is the output of a character-level bidirectional LSTM (biLSTM) encoder run over the characters of wi (Ling et al., 2015); this makes the model fully open-vocabulary, since it can produce representations for any character sequence. We then obtain a context-sensitive encoding hi using a"
D19-1102,Q16-1023,0,0.0419216,"g from Turkish is straightforward. Its alphabet consists of 29 letters, 23 of which are in basic Latin. The other six letters, ‘c¸’,‘˘g’, ‘ı’, ‘¨o’, ‘s¸’, and ‘¨u’, add diacritics to basic Latin characters, facilitating different pronunciations.4 We map these to their basic Latin counterparts, e.g., ‘c¸’ to ‘c’. For Kazakh, we use a simple dictionary created by a Kazakh computational linguist to map each Cyrillic letter to the basic Latin alphabet.5 . Experimental Setup Dependency Parsing Model We use the Uppsala parser, a transition-based neural dependency parser (de Lhoneux et al., 2017a,b; Kiperwasser and Goldberg, 2016). The parser uses an arc-hybrid transition system (Kuhlmann et al., 2011), extended with a static-dynamic oracle and SWAP transition to allow non-projective dependency trees (Nivre, 2009). Let w = w0 , . . . , w|w |be an input sentence of length |w |and let w0 represent an artificial ROOT token. We create a vector representation for each input token wi by concatenating (; ) its word embedding, ew (wi ) and its character-based word embedding, ec (wi ): xi = [ew (wi ); ec (wi )] (1) Here, ec (wi ) is the output of a character-level bidirectional LSTM (biLSTM) encoder run over the characters of w"
D19-1102,P11-1068,0,0.0331551,"h are in basic Latin. The other six letters, ‘c¸’,‘˘g’, ‘ı’, ‘¨o’, ‘s¸’, and ‘¨u’, add diacritics to basic Latin characters, facilitating different pronunciations.4 We map these to their basic Latin counterparts, e.g., ‘c¸’ to ‘c’. For Kazakh, we use a simple dictionary created by a Kazakh computational linguist to map each Cyrillic letter to the basic Latin alphabet.5 . Experimental Setup Dependency Parsing Model We use the Uppsala parser, a transition-based neural dependency parser (de Lhoneux et al., 2017a,b; Kiperwasser and Goldberg, 2016). The parser uses an arc-hybrid transition system (Kuhlmann et al., 2011), extended with a static-dynamic oracle and SWAP transition to allow non-projective dependency trees (Nivre, 2009). Let w = w0 , . . . , w|w |be an input sentence of length |w |and let w0 represent an artificial ROOT token. We create a vector representation for each input token wi by concatenating (; ) its word embedding, ew (wi ) and its character-based word embedding, ec (wi ): xi = [ew (wi ); ec (wi )] (1) Here, ec (wi ) is the output of a character-level bidirectional LSTM (biLSTM) encoder run over the characters of wi (Ling et al., 2015); this makes the model fully open-vocabulary, since"
D19-1102,D18-1543,1,0.871399,"Missing"
D19-1102,K17-3022,0,0.078654,"Missing"
D19-1102,W17-6314,0,0.0552242,"Missing"
D19-1102,L18-1352,0,0.0225797,"t in the extremely low-resource setting, data augmentation improves parsing performance both in monolingual and cross-lingual settings. We also show that transfer learning is possible with lexicalized parsers. In addition, we show that transfer learning between two languages with different writing systems is possible, and future work should consider transliteration for other language pairs. While we have not exhausted all the possible techniques (e.g., use of external resources (Rasooli and Collins, 2017; Rosa and Mareˇcek, 2018), predicted POS (Ammar et al., 2016), multiple source treebanks (Lim et al., 2018; Stymne et al., 2018), among others), we show that simple methods which leverage the linguistic annotations in the treebank can improve low-resource parsing. Future work might explore different augmentation methods, such as the use of synthetic source treebanks (Wang and Eisner, 2018) or contextualized language model (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018) for scoring the augmented data (e.g., using perplexity). Finally, while the techniques presented in this paper might be applicable to other low-resource languages, we want to also highlight the importance of under"
D19-1102,D15-1176,0,0.0155681,"he parser uses an arc-hybrid transition system (Kuhlmann et al., 2011), extended with a static-dynamic oracle and SWAP transition to allow non-projective dependency trees (Nivre, 2009). Let w = w0 , . . . , w|w |be an input sentence of length |w |and let w0 represent an artificial ROOT token. We create a vector representation for each input token wi by concatenating (; ) its word embedding, ew (wi ) and its character-based word embedding, ec (wi ): xi = [ew (wi ); ec (wi )] (1) Here, ec (wi ) is the output of a character-level bidirectional LSTM (biLSTM) encoder run over the characters of wi (Ling et al., 2015); this makes the model fully open-vocabulary, since it can produce representations for any character sequence. We then obtain a context-sensitive encoding hi using a word-level biLSTM encoder: hi = [LSTMf (x0:i ); LSTMb (x|w|:i )] (2) We then create a configuration by concatenating the encoding of a fixed number of words on the top of the stack and the beginning of the buffer. Given 3 Another possible pivot is phonemes (Tsvetkov et al., 2016). We leave this as future work. 4 https://www.omniglot.com/writing/turkish.htm 5 The mapping from Kazakh Cyrilic into basic Latin alphabet is provided in"
D19-1102,N18-1202,0,0.0147329,"hould consider transliteration for other language pairs. While we have not exhausted all the possible techniques (e.g., use of external resources (Rasooli and Collins, 2017; Rosa and Mareˇcek, 2018), predicted POS (Ammar et al., 2016), multiple source treebanks (Lim et al., 2018; Stymne et al., 2018), among others), we show that simple methods which leverage the linguistic annotations in the treebank can improve low-resource parsing. Future work might explore different augmentation methods, such as the use of synthetic source treebanks (Wang and Eisner, 2018) or contextualized language model (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018) for scoring the augmented data (e.g., using perplexity). Finally, while the techniques presented in this paper might be applicable to other low-resource languages, we want to also highlight the importance of understanding the characteristics of languages being studied. For example, we showed that although North Sami and Finnish do not share vocabulary, cross-lingual training is still helpful because they share similar syntactic structures. Different language pairs might benefit from other types of similarity (e.g., morphological) and investigating"
D19-1102,Q17-1020,0,0.0416122,"embeddings. 6 Conclusions In this paper, we investigated various low-resource parsing scenarios. We demonstrate that in the extremely low-resource setting, data augmentation improves parsing performance both in monolingual and cross-lingual settings. We also show that transfer learning is possible with lexicalized parsers. In addition, we show that transfer learning between two languages with different writing systems is possible, and future work should consider transliteration for other language pairs. While we have not exhausted all the possible techniques (e.g., use of external resources (Rasooli and Collins, 2017; Rosa and Mareˇcek, 2018), predicted POS (Ammar et al., 2016), multiple source treebanks (Lim et al., 2018; Stymne et al., 2018), among others), we show that simple methods which leverage the linguistic annotations in the treebank can improve low-resource parsing. Future work might explore different augmentation methods, such as the use of synthetic source treebanks (Wang and Eisner, 2018) or contextualized language model (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018) for scoring the augmented data (e.g., using perplexity). Finally, while the techniques presented in this p"
D19-1102,K18-2019,0,0.0491895,"Missing"
D19-1102,D18-1545,0,0.35838,"2016; Agi´c, 2017; Rosa and Mareˇcek, 2018), along with the VarDial and the CoNLL UD shared tasks (Zampieri et al., 2017; Zeman et al., 2017, 2018). But low-resource parsing is still difficult. The organizers of the CoNLL 2018 UD shared task (Zeman et al., 2018) report that, in general, results on the task’s nine low-resource treebanks “are extremely low and the outputs are hardly useful for Each of these scenarios requires different approaches. Data augmentation is applicable in all scenarios, and has proven useful for low-resource NLP in general (Fadaee et al., 2017; Bergmanis et al., 2017; Sahin and Steedman, 2018). Transfer learning via cross-lingual training is applicable in scenarios 2 and 3. Finally, transliteration may be useful in scenario 3. To keep our scenarios as realistic as possible, we assume that no taggers are available since this would entail substantial annotation. Therefore, our neural parsing models must learn to parse from words or characters—that is, they must be lexicalized—even though there may be little shared vocabulary between source and target treebanks. While this may intuitively seem to make crosslingual training difficult, recent results have shown that lexical parameter sh"
D19-1102,D16-1159,0,0.054827,"between the two languages. Let cj be an embedding of character cj in a token wi from the treebank of language k, and let lk be the language embedding. For sharing on characters, we concatenate character and language embedding: [cj ; lk ] for input to the character-level biLSTM. Similarly, for input to the word-level biLSTM, we concatenate the language embedding to the word embedding, modifying Eq. 1 to xi = [ew (wi ); ec (wi ); lk ] (3) We use the default hyperparameters of de Lhoneux et al. (2018) in our experiments. We fine-tune each model by training it further only on the target treebank (Shi et al., 2016). We use early stopping based on Label Attachment Score (LAS) on development set. 3.3 Datasets We use Universal Dependencies (UD) treebanks version 2.2 (Nivre et al., 2018). None of our target treebanks have a development set, so we generate new train/dev splits by 50:50 (Table 1). Having large development sets allow us to perform better analysis for this study. 4 +Morph +Nonce 1128 564 141 7636 3838 854 4934 2700 661 Table 2: Number of North Sámi training sentences. Table 1: Train/dev split used for each treebank. 3.2 original Parsing North Sámi North Sámi is our largest low-resource treebank"
D19-1102,K18-2011,0,0.0414506,"Missing"
D19-1102,P11-2120,1,0.837952,"ifferent writing system, transliteration into a shared orthographic spaces is also very helpful. 1 1. What can we do with only a very small target treebank for a low-resource language? 2. What can we do if we also have a source treebank for a related high-resource language? 3. What if the source and target treebanks do not share a writing system? Introduction Large annotated treebanks are available for only a tiny fraction of the world’s languages, and there is a wealth of literature on strategies for parsing with few resources (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). A popular approach is to train a parser on a related high-resource language and adapt it to the low-resource language. This approach benefits from the availability of Universal Dependencies (UD; Nivre et al., 2016), prompting substantial research (Tiedemann and Agic, 2016; Agi´c, 2017; Rosa and Mareˇcek, 2018), along with the VarDial and the CoNLL UD shared tasks (Zampieri et al., 2017; Zeman et al., 2017, 2018). But low-resource parsing is still difficult. The organizers of the CoNLL 2018 UD shared task (Zeman et al., 2018) report that, in general, results on the task’s nine low-resource tr"
D19-1102,K17-3009,0,0.0168418,"transliterate the entries of both the source and the target pre-trained word embeddings. To see how our best approach (i.e., cross-lingual model with MORPH augmentation) compares with the current state-of-the-art models, we compare it to the recent results from CoNLL 2018 shared task. Training state-of-the-art models may require lots of engineering and data resources. Our goal, however, is not to achieve the best performance, but rather to systematically investigate how far simple approaches can take us. We report performance of the following: (1) the shared task baseline model (UDPipe v1.2; Straka and Straková, 2017) and (2) the best system for each treebank, (3) our best approach, and (4) a cross-lingual model with fastText embeddings. Table 8 presents the overall comparison on the test sets. For each treebank, we apply the same sentence segmentation and tokenization used by each best system.11 We see that our approach outperforms the baseline models on both languages. For Kazakh, our model (with transliteration) achieves a competitive LAS (28.23), which would be the second position in the shared task ranking. As comparison, the best system for Kazakh (Smith et al., 2018) trained a multitreebank model wi"
D19-1102,P18-2098,0,0.0525577,"Missing"
D19-1102,N16-1161,0,0.0181971,"ec (wi ): xi = [ew (wi ); ec (wi )] (1) Here, ec (wi ) is the output of a character-level bidirectional LSTM (biLSTM) encoder run over the characters of wi (Ling et al., 2015); this makes the model fully open-vocabulary, since it can produce representations for any character sequence. We then obtain a context-sensitive encoding hi using a word-level biLSTM encoder: hi = [LSTMf (x0:i ); LSTMb (x|w|:i )] (2) We then create a configuration by concatenating the encoding of a fixed number of words on the top of the stack and the beginning of the buffer. Given 3 Another possible pivot is phonemes (Tsvetkov et al., 2016). We leave this as future work. 4 https://www.omniglot.com/writing/turkish.htm 5 The mapping from Kazakh Cyrilic into basic Latin alphabet is provided in Appendix B 1107 Language Treebank ID train dev. test Finnish North Sámi fi_tdt sme_giella 14981 1128 1875 1129 1555 865 Portuguese Galician pt_bosque gl_treegal 8329 300 560 300 477 400 Turkish Kazakh tr_imst kk_ktb 3685 15 975 16 975 1047 T100 T50 T10 this configuration, we predict a transition and its arc label using a multi-layer perceptron (MLP). More details of the core parser can be found in de Lhoneux et al. (2017a,b). Parameter sharin"
D19-1102,K17-3001,0,0.0490395,"Missing"
D19-1102,I08-3008,0,0.117116,"nd (3) when the high-resource treebank uses a different writing system, transliteration into a shared orthographic spaces is also very helpful. 1 1. What can we do with only a very small target treebank for a low-resource language? 2. What can we do if we also have a source treebank for a related high-resource language? 3. What if the source and target treebanks do not share a writing system? Introduction Large annotated treebanks are available for only a tiny fraction of the world’s languages, and there is a wealth of literature on strategies for parsing with few resources (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). A popular approach is to train a parser on a related high-resource language and adapt it to the low-resource language. This approach benefits from the availability of Universal Dependencies (UD; Nivre et al., 2016), prompting substantial research (Tiedemann and Agic, 2016; Agi´c, 2017; Rosa and Mareˇcek, 2018), along with the VarDial and the CoNLL UD shared tasks (Zampieri et al., 2017; Zeman et al., 2017, 2018). But low-resource parsing is still difficult. The organizers of the CoNLL 2018 UD shared task (Zeman et al., 2018) report that, in general, res"
D19-1102,D18-1163,0,0.0645882,"different writing systems is possible, and future work should consider transliteration for other language pairs. While we have not exhausted all the possible techniques (e.g., use of external resources (Rasooli and Collins, 2017; Rosa and Mareˇcek, 2018), predicted POS (Ammar et al., 2016), multiple source treebanks (Lim et al., 2018; Stymne et al., 2018), among others), we show that simple methods which leverage the linguistic annotations in the treebank can improve low-resource parsing. Future work might explore different augmentation methods, such as the use of synthetic source treebanks (Wang and Eisner, 2018) or contextualized language model (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018) for scoring the augmented data (e.g., using perplexity). Finally, while the techniques presented in this paper might be applicable to other low-resource languages, we want to also highlight the importance of understanding the characteristics of languages being studied. For example, we showed that although North Sami and Finnish do not share vocabulary, cross-lingual training is still helpful because they share similar syntactic structures. Different language pairs might benefit from other types"
D19-1102,W17-1201,0,0.0677401,"Missing"
D19-1118,P15-1034,0,0.033357,"The three rewards focus on different aspects of relationships between entities, giving complimentary views of what entities are related. Similar to Verga et al. (2016), we parameterize candidate relation phrases with a BiLSTM (Graves and Schmidhuber, 2005), and use pretrained Wikidata BigGraph embeddings (Lerer et al., 2019) as the entity representations. We apply a one-layer MLP on the concatenated representations to get the reward value. Updating the coreference resolver Each resolved document is converted into n subjectrelation-object (SRO) triples by an open information retrieval system (Angeli et al., 2015). Each triple ti is then scored using a reward function to obtain a reward ri for i ∈ {1, . . . , n}. The final document-level reward is the normalized sum of the individual rewards as shown in Equation 1, where Rh is a moving window containing the previous h = 100 normalized reward values. P ri − mean(Rh ) R= i (1) stddev(Rh ) Since R is not differentiable with respect to the coreference resolver’s parameters, we use pol2 https://www.wikidata.org icy gradient training to update the coreference resolver. We select the best action according to the current policy, using random exploration of the"
D19-1118,D14-1165,0,0.0243206,"fed by the Iranian leadership. On March 19, Obama continued his outreach to the Muslim world, releasing a New Year’s video message to the people and government of Iran. This attempt was rebuffed by the Iranian leadership. Figure 3: Mention detection and linking examples by the baseline system from Lee et al. (2018), and the best performing fine-tuned system (Coref-Distill). Mentions of the same color are linked to form a coreference cluster. tasks other than knowledge base construction.8 Knowledge bases Knowledge bases have been leveraged across multiple tasks across NLP (Bordes et al., 2011; Chang et al., 2014; Lin et al., 2015; Toutanova et al., 2015; Yang and Mitchell, 2017). Specifically for coreference resolution, Prokofyev et al. (2015) implement a resolver that ensures semantic relatedness of resulting coreference clusters by leveraging Semantic Web annotations. Their work incorporates knowledge graph information only in the final stage of the resolver’s pipeline, and not during training. In contrast, our work augments information from the knowledge base directly into the training pipeline. Also, they use DBpedia (Auer et al., 2007) as the ontology. Although both Wikidata and DBpedia are desi"
D19-1118,D16-1245,0,0.0220654,"oreferent mentions, thereby improving the identification of long distance coreference chains. Zhang et al. (2019) improve pronoun coreference resolution by 2.2 F1 using linguistic features (gender, animacy and plurality) and a frequency based predicateargument selection preference as external knowledge. Emami et al. (2018) incorporate knowledge into coreference resolution by means of information retrieval, finding sentences that are syntactically similar to a given instance, and improving F1 by 0.16. Reinforcement learning RL has been used for many NLP tasks, including coreference resolution (Clark and Manning, 2016) and relation extraction (Zeng et al., 2018). Clark and Manning (2016) use RL to improve coreference resolution by optimizing their mention ranking model and directly use the standard evaluation metrics as the rewards. We, on the other hand, perform end-to-end optimization by rewarding the model’s consistency with real world knowledge using relation extraction. To our knowledge, we are the first to use consistency with world knowledge as a reward for Baseline system Fine-tuned system According to the library&apos;s publications, it is the largest academic library in the southern hemisphere. The uni"
D19-1118,N18-4004,0,0.0873735,"and Ma, 2017; Meng and Rumshisky, 2018), Lee et al. (2017) were the first to propose an end-to-end resolver which did not rely on hand-crafted rules or a syntactic parser. Extending this work, Lee et al. (2018) introduced a novel attention mechanism for iteratively ranking spans of candidate coreferent mentions, thereby improving the identification of long distance coreference chains. Zhang et al. (2019) improve pronoun coreference resolution by 2.2 F1 using linguistic features (gender, animacy and plurality) and a frequency based predicateargument selection preference as external knowledge. Emami et al. (2018) incorporate knowledge into coreference resolution by means of information retrieval, finding sentences that are syntactically similar to a given instance, and improving F1 by 0.16. Reinforcement learning RL has been used for many NLP tasks, including coreference resolution (Clark and Manning, 2016) and relation extraction (Zeng et al., 2018). Clark and Manning (2016) use RL to improve coreference resolution by optimizing their mention ranking model and directly use the standard evaluation metrics as the rewards. We, on the other hand, perform end-to-end optimization by rewarding the model’s c"
D19-1118,L16-1021,0,0.100015,"ence resolution model (Lee et al., 2018) as our baseline coreference resolver.3 This model extends Lee et al. (2017) with coarse-to-fine inference and ELMo pretrained embeddings (Peters et al., 2018). 3 https://github.com/kentonl/e2e-coref System Data Accuracy F1 score RE-KG RE-Text RE-Joint 12M 2M 60K 0.64 0.71 0.58 0.78 0.83 0.73 RE-Distill — 0.78 0.88 Table 1: Training data size, accuracy and F1 scores of the reward models on the 200,000 validation triples. Data We use the standard training, validation, and test splits from the English OntoNotes.4 We also evaluate on the English WikiCoref (Ghaddar and Langlais, 2016), with a validation and test split of 10 and 20 documents respectively. Reward model training We use data from English Wikipedia and Wikidata to train our three reward models. For training RE-KG, we sample 1 million Wikidata triples, and expand them to 12 million triples by replacing relation phrases with their aliases. For RE-Text, we pass the summary paragraphs from 50,000 random Wikipedia pages to Stanford’s OpenIE extractor (Manning et al., 2014), creating 2 million triples. For RE-Joint, we only use Wikipedia triples that are grounded in Wikidata, resulting in 60,000 triples.5 We further"
D19-1118,D17-1018,0,0.202098,", 2, 3} do Cd = entity clusters with θn d0 = resolve d with Cd T = obtain OpenIE triples for d0 r = rewardn (d0 ) gˆk = policy gradient for θn with reward r θnk+1 = θnk + αk gˆk end for end for end while Distilled policy θ∗ = θ1 +θ32 +θ3 Sample k documents Dk for d ∈ Dk do d0 = resolve d with Cd T = obtain OpenIE triples for d0 r = reward∗ (d0 ) gˆk = policy gradient for θ∗ with reward r θ∗k+1 = θ∗k + αk gˆk end for return Distilled policy θ∗ 3 Experiments We use a state-of-the-art neural coreference resolution model (Lee et al., 2018) as our baseline coreference resolver.3 This model extends Lee et al. (2017) with coarse-to-fine inference and ELMo pretrained embeddings (Peters et al., 2018). 3 https://github.com/kentonl/e2e-coref System Data Accuracy F1 score RE-KG RE-Text RE-Joint 12M 2M 60K 0.64 0.71 0.58 0.78 0.83 0.73 RE-Distill — 0.78 0.88 Table 1: Training data size, accuracy and F1 scores of the reward models on the 200,000 validation triples. Data We use the standard training, validation, and test splits from the English OntoNotes.4 We also evaluate on the English WikiCoref (Ghaddar and Langlais, 2016), with a validation and test split of 10 and 20 documents respectively. Reward model trai"
D19-1118,N18-2108,0,0.125707,"Missing"
D19-1118,D15-1082,0,0.0351766,"eadership. On March 19, Obama continued his outreach to the Muslim world, releasing a New Year’s video message to the people and government of Iran. This attempt was rebuffed by the Iranian leadership. Figure 3: Mention detection and linking examples by the baseline system from Lee et al. (2018), and the best performing fine-tuned system (Coref-Distill). Mentions of the same color are linked to form a coreference cluster. tasks other than knowledge base construction.8 Knowledge bases Knowledge bases have been leveraged across multiple tasks across NLP (Bordes et al., 2011; Chang et al., 2014; Lin et al., 2015; Toutanova et al., 2015; Yang and Mitchell, 2017). Specifically for coreference resolution, Prokofyev et al. (2015) implement a resolver that ensures semantic relatedness of resulting coreference clusters by leveraging Semantic Web annotations. Their work incorporates knowledge graph information only in the final stage of the resolver’s pipeline, and not during training. In contrast, our work augments information from the knowledge base directly into the training pipeline. Also, they use DBpedia (Auer et al., 2007) as the ontology. Although both Wikidata and DBpedia are designed to support wo"
D19-1118,P14-5010,0,0.00365763,"les. Data We use the standard training, validation, and test splits from the English OntoNotes.4 We also evaluate on the English WikiCoref (Ghaddar and Langlais, 2016), with a validation and test split of 10 and 20 documents respectively. Reward model training We use data from English Wikipedia and Wikidata to train our three reward models. For training RE-KG, we sample 1 million Wikidata triples, and expand them to 12 million triples by replacing relation phrases with their aliases. For RE-Text, we pass the summary paragraphs from 50,000 random Wikipedia pages to Stanford’s OpenIE extractor (Manning et al., 2014), creating 2 million triples. For RE-Joint, we only use Wikipedia triples that are grounded in Wikidata, resulting in 60,000 triples.5 We further sample 200,000 triples from Wikidata and Wikipedia for validation, and train the reward models with early stopping based on the F1 score of their predictions. Evaluation All models are evaluated using the standard CoNLL metric, which is the average F1 score over MUC, CEAFe, and B 3 (Denis and Baldridge, 2009). 4 Results Since the quality of our reward models is essential to the performance of the coreference resolver adaptations, we first report the"
D19-1118,D14-1221,0,0.0284311,"solvers for producing triples that are found in knowledge bases. Since relation extraction systems can rely on different forms of supervision and be biased in different ways, we obtain the best performance, improving over the state of the art, using multi-task reinforcement learning. 1 Introduction Coreference annotations are costly and difficult to obtain, since trained annotators with sufficient world knowledge are necessary for reliable annotations. This paper presents a way to simulate annotators using reinforcement learning. To motivate our approach, we rely on the following example from Martschat and Strube (2014, colors added to mark entity mentions): (1) [Lynyrd . . . . . . . . .Skynyrd] . . . . . . . . . 1 was formed in Florida2 . Other bands from [the Sunshine State]2 include Fireflight and :::::::: Marilyn :::::::: Manson. Martschat and Strube (2014) cite the association between Florida and the Sunshine State as an example of a common source of name-name recall error for state-of-the-art coreference resolution systems. The challenge is that the two names co-occur relatively infrequently and are unlikely to do so in a moderate-sized, manually annotated training corpus. A state-of-the-art system ma"
D19-1118,C18-1004,0,0.024306,"el. Interestingly, we do not see this type of eventive noun phrase linking either in OntoNotes or in the predictions of the baseline model. This phenomenon, however, also has a sideeffect of producing singleton clusters and spurious linking, which adversely affect the recall. On the OntoNotes test data, while the average precision of the best performing fine-tuned model is higher than the baseline (75.62 vs. 73.80), a drop in recall (70.75 vs. 71.34) causes the final F1 score to only marginally improve. 6 Related Work Coreference resolution Among neural coreference resolvers (Wu and Ma, 2017; Meng and Rumshisky, 2018), Lee et al. (2017) were the first to propose an end-to-end resolver which did not rely on hand-crafted rules or a syntactic parser. Extending this work, Lee et al. (2018) introduced a novel attention mechanism for iteratively ranking spans of candidate coreferent mentions, thereby improving the identification of long distance coreference chains. Zhang et al. (2019) improve pronoun coreference resolution by 2.2 F1 using linguistic features (gender, animacy and plurality) and a frequency based predicateargument selection preference as external knowledge. Emami et al. (2018) incorporate knowledg"
D19-1118,N18-1202,0,0.0222163,"Missing"
D19-1118,N13-1008,0,0.300966,"es us with more evidence for resolving coreference such as (1). We propose a training strategy (Figure 1) in which we pass on the predictions of a neural coreference resolver to an open relation extraction (OpenRE) system, matching relations extracted from resolved sentences with a knowledge base. We show how checking the produced relationships for consistency against the knowledge base produces a reward that is, indirectly, a signal about the quality of the coreference resolution. In order to generalize this signal beyond the coverage of the knowledge base, we train a Universal Schema model (Riedel et al., 2013) and use its confidence as our reward function. With this reward function, RE-KG RE-Text RE-Joint Document NA Bach was a German composer. He is known for instrumental compositions such as the Art of Fugue Bach was a German composer. He is known for instrumental compositions such as the Art of Fugue OpenRE NA (Bach, born in, Germany) (Bach, occupation, Composer) (He, composition, Art of Fugue) (Bach, born in, Germany) (Bach, occupation, Composer) (He, composition, Art of Fugue) NA ✗ (Bach, born in, Germany) ✓(Bach, occupation, Composer) ✗ (He, composition, Art of Fugue) (Bach, born in, Germany)"
D19-1118,D15-1174,0,0.0292741,"h 19, Obama continued his outreach to the Muslim world, releasing a New Year’s video message to the people and government of Iran. This attempt was rebuffed by the Iranian leadership. Figure 3: Mention detection and linking examples by the baseline system from Lee et al. (2018), and the best performing fine-tuned system (Coref-Distill). Mentions of the same color are linked to form a coreference cluster. tasks other than knowledge base construction.8 Knowledge bases Knowledge bases have been leveraged across multiple tasks across NLP (Bordes et al., 2011; Chang et al., 2014; Lin et al., 2015; Toutanova et al., 2015; Yang and Mitchell, 2017). Specifically for coreference resolution, Prokofyev et al. (2015) implement a resolver that ensures semantic relatedness of resulting coreference clusters by leveraging Semantic Web annotations. Their work incorporates knowledge graph information only in the final stage of the resolver’s pipeline, and not during training. In contrast, our work augments information from the knowledge base directly into the training pipeline. Also, they use DBpedia (Auer et al., 2007) as the ontology. Although both Wikidata and DBpedia are designed to support working with Wikipedia art"
D19-1118,N16-1103,0,0.0234297,"a models (Riedel et al., 2013; Verga and McCallum, 2016), resulting in three reward functions (Figure 2): RE-KG (Knowledge Graph Universal Schema) is trained to predict whether two 1 https://www.wikipedia.org entities are linked in Wikidata2 ; RE-Text (Textbased Universal Schema) is trained to predict whether two entities co-occur in Wikipedia; and RE-Joint (Joint Universal Schema) is trained to predict whether two entities are linked and cooccur. The three rewards focus on different aspects of relationships between entities, giving complimentary views of what entities are related. Similar to Verga et al. (2016), we parameterize candidate relation phrases with a BiLSTM (Graves and Schmidhuber, 2005), and use pretrained Wikidata BigGraph embeddings (Lerer et al., 2019) as the entity representations. We apply a one-layer MLP on the concatenated representations to get the reward value. Updating the coreference resolver Each resolved document is converted into n subjectrelation-object (SRO) triples by an open information retrieval system (Angeli et al., 2015). Each triple ti is then scored using a reward function to obtain a reward ri for i ∈ {1, . . . , n}. The final document-level reward is the normali"
D19-1118,W16-1312,0,0.0140193,"reference resolver for being consistent with world knowledge, we propose a simple training strategy based on relation extraction: (i) Sample a Wikipedia1 document at random, (ii) Replace mentions with their antecedents using a coreference resolver, (iii) Apply an offthe-shelf openRE system to each rewritten document, (iv) Score relationships that include coreferent mentions using Universal Schema, and (v) Use the score as a reward for training the coreference resolvers. Reward functions To model consistency with world knowledge, we train different Universal Schema models (Riedel et al., 2013; Verga and McCallum, 2016), resulting in three reward functions (Figure 2): RE-KG (Knowledge Graph Universal Schema) is trained to predict whether two 1 https://www.wikipedia.org entities are linked in Wikidata2 ; RE-Text (Textbased Universal Schema) is trained to predict whether two entities co-occur in Wikipedia; and RE-Joint (Joint Universal Schema) is trained to predict whether two entities are linked and cooccur. The three rewards focus on different aspects of relationships between entities, giving complimentary views of what entities are related. Similar to Verga et al. (2016), we parameterize candidate relation"
D19-1118,P17-1132,0,0.0205605,"s outreach to the Muslim world, releasing a New Year’s video message to the people and government of Iran. This attempt was rebuffed by the Iranian leadership. Figure 3: Mention detection and linking examples by the baseline system from Lee et al. (2018), and the best performing fine-tuned system (Coref-Distill). Mentions of the same color are linked to form a coreference cluster. tasks other than knowledge base construction.8 Knowledge bases Knowledge bases have been leveraged across multiple tasks across NLP (Bordes et al., 2011; Chang et al., 2014; Lin et al., 2015; Toutanova et al., 2015; Yang and Mitchell, 2017). Specifically for coreference resolution, Prokofyev et al. (2015) implement a resolver that ensures semantic relatedness of resulting coreference clusters by leveraging Semantic Web annotations. Their work incorporates knowledge graph information only in the final stage of the resolver’s pipeline, and not during training. In contrast, our work augments information from the knowledge base directly into the training pipeline. Also, they use DBpedia (Auer et al., 2007) as the ontology. Although both Wikidata and DBpedia are designed to support working with Wikipedia articles, DBpedia can be cons"
D19-1118,N19-1093,0,0.162475,"Missing"
D19-1328,P17-1042,0,0.36426,"erformance on BDI in these works is evaluated by verifying the system-retrieved translations for a source word against a set of goldstandard targets. The metric used is Precision at k (P@k), which measures how often the set of k top predictions contains one of the gold-standard targets, i.e. what is the ratio of True Positives to the sum of True Positives and False Positives. 1 Available at https://github.com/coastalcph/MUSE_dicos Data All systems listed above report results on one or both of two test sets: the MUSE test sets Conneau et al. (2018) and/or the Dinu test sets (Dinu et al., 2015; Artetxe et al., 2017). Similarly to MUSE, the Dinu dataset was compiled automatically (from Europarl word-alignments), but it only covers four languages. Due to the bigger size of MUSE (110 language pairs), we deem its impact larger and focus our study entirely on it. 3 Annotation-based observations In order to gain insights into the linguistic composition of the MUSE dictionaries, we employ annotators fluent in German, Danish, Bulgarian, Arabic and Hindi (hereafter, DE , DA , BG , AR , HI) to annotate the entire dictionaries from English to one of these languages (hereafter, from-EN) and the entire dictionaries f"
D19-1328,P18-1073,0,0.0543932,"sults on MUSE, and to account for the problems presented here through manual verification and analysis of the results. We share our part-of-speech annotations, such that future work can use this resource for analysis purposes. 1 2 Bilingual Dictionary Induction Improvements on BDI mostly stem from developments in the space of cross-lingual embeddings, which use BDI for intrinsic evaluation. Systems Five influential recent systems for cross-lingual embeddings are MUSE (Conneau et al., 2018), which can be supervised (MUSE-S) or unsupervised (MUSE-U); VecMap, which also can be supervised (VM-S) (Artetxe et al., 2018a) or unsupervised (VM-U) (Artetxe et al., 2018b); and RCSLS (Joulin et al., 2018), a supervised system (RCSLS), which scores best on BDI out of the five. We refer the reader to the respective publications for a general description of the systems. Metrics Performance on BDI in these works is evaluated by verifying the system-retrieved translations for a source word against a set of goldstandard targets. The metric used is Precision at k (P@k), which measures how often the set of k top predictions contains one of the gold-standard targets, i.e. what is the ratio of True Positives to the sum of"
D19-1328,Q17-1010,0,0.059709,"14 percent, respectively. Notice, however, that in the case of the MUSE data, the ratio is even more skewed in favour of nouns over the other two categories. The large number of proper nouns in the dictionaries seems even more problematic. Proper nouns are considered to have no lexical meaning, but rather just a referential function (Pierini, 2008). Personal names usually refer to a specific referent in a given context, but they can, in general, be attributed to different referents across different contexts, and they are almost univer2 For all experiments, we use the pretrained embeddings of Bojanowski et al. (2017), trained on Wikipedia. 3 The numbers were similar across from-EN dictionaries. 3337 NOUN VERB AD PNOUN Overall 60 40 Figure 1: Precision of RCSLS by POS tag on to-EN data. sally interchangeable in any given context. Some personal names and most place and organization names may have a unique referent, e.g. Barack Obama, Wisconsin, Skype, but these names still do not carry a sense, their referent is resolved through access to encyclopedic knowledge (Pierini, 2008). Considering that the pretrained embeddings which we use were trained on Wikipedia, we can expect that such encyclopedic information"
D19-1328,W16-2506,0,0.0247554,"ual words. The task has been widely used for intrinsic evaluation of cross-lingual embedding algorithms, which aim to map two languages into the same embedding space, for transfer learning purposes (Klementiev et al., 2012). Recently, Glavas et al. (2019) reported limited evidence in support of this practice—they found that cross-lingual embeddings optimized for a BDI evaluation metric were not necessarily better on downstream tasks. Here, we study BDI evaluation in itself, as has Anders Søgaard University of Copenhagen soegaard@di.ku.dk been done for other evaluation methods in the past (cf. Faruqui et al., 2016’s work on word similarity), with concerning findings about its reliability. A massive dataset of 110 bilingual dictionaries, known as the MUSE dataset, was introduced in early 2018 along with a strong baseline (Conneau et al., 2018). Subsets of the MUSE dictionaries have been used for model comparison in the evaluation of numerous cross-lingual embedding systems developed since (cf. Grave et al., 2018; Jawanpuria et al., 2018; Hoshen and Wolf, 2018a,b; Wada and Iwata, 2018; Joulin et al., 2018). Even though the field has been very active, progress has been incremental for most language pairs."
D19-1328,D16-1235,0,0.0496147,"Missing"
D19-1328,P19-1070,0,0.194478,"Missing"
D19-1328,D18-1330,0,0.18719,"University of Copenhagen soegaard@di.ku.dk been done for other evaluation methods in the past (cf. Faruqui et al., 2016’s work on word similarity), with concerning findings about its reliability. A massive dataset of 110 bilingual dictionaries, known as the MUSE dataset, was introduced in early 2018 along with a strong baseline (Conneau et al., 2018). Subsets of the MUSE dictionaries have been used for model comparison in the evaluation of numerous cross-lingual embedding systems developed since (cf. Grave et al., 2018; Jawanpuria et al., 2018; Hoshen and Wolf, 2018a,b; Wada and Iwata, 2018; Joulin et al., 2018). Even though the field has been very active, progress has been incremental for most language pairs. Moreover, there have been very few attempts at a linguistically-informed error analysis of BDI performance as measured on MUSE (cf. Kementchedjhieva et al., 2018). This is problematic for two reasons: on one hand, most systems greatly vary in their approach and architecture, so it is difficult to identify the source of the reported performance gains; on the other hand, the MUSE dataset was compiled automatically, with no manual post-processing to clean up noise, so the real impact of the perfor"
D19-1328,K18-1021,1,0.833385,"s the MUSE dataset, was introduced in early 2018 along with a strong baseline (Conneau et al., 2018). Subsets of the MUSE dictionaries have been used for model comparison in the evaluation of numerous cross-lingual embedding systems developed since (cf. Grave et al., 2018; Jawanpuria et al., 2018; Hoshen and Wolf, 2018a,b; Wada and Iwata, 2018; Joulin et al., 2018). Even though the field has been very active, progress has been incremental for most language pairs. Moreover, there have been very few attempts at a linguistically-informed error analysis of BDI performance as measured on MUSE (cf. Kementchedjhieva et al., 2018). This is problematic for two reasons: on one hand, most systems greatly vary in their approach and architecture, so it is difficult to identify the source of the reported performance gains; on the other hand, the MUSE dataset was compiled automatically, with no manual post-processing to clean up noise, so the real impact of the performance gains is unclear. In this work, we study the composition and quality of the MUSE data for five diverse languages: German, Danish, Bulgarian, Arabic and Hindi. A manual part-of-speech annotation of the test sets for these languages reveals a strikingly high"
D19-1328,L18-1293,0,0.026431,"Missing"
D19-1328,C12-1089,0,0.0870796,"Missing"
D19-1328,N03-1033,0,0.0205816,"Missing"
D19-1593,W19-4820,0,0.0363764,"nterpretable NLP in recent years. For example, Chrupała et al. (2017) employed RSA as a means of correlating encoder representations of speech, text, and images in a post-hoc analysis of a multi-task neural pipeline. Similarly, Bouchacourt and Baroni (2018) used the framework to measure the similarity between input image embeddings and the representations of the same image by an agent in an language game setting. More recently, Chrupała and Alishahi (2019) correlated activation patterns of sentence encoders with symbolic representations, such as syntax trees. Lastly, similar to our work here, Abnar et al. (2019) proposed an extension to RSA that enables the comparison of a single model in the face of isolated, changing parameters, and employed this metric along with RSA to correlate NLP models’ and human brains’ respective representations of language. We hope to position our work among this brief survey and further demonstrate the flexibility of RSA across several levels of abstraction. 2 Representational Similarity Analysis RSA was proposed by Kriegeskorte et al. (2008) as a method of relating the different representational modalities employed in neuroscientific studies. Due to the lack of correspon"
D19-1593,I17-1001,0,0.061084,"Missing"
D19-1593,P18-2003,0,0.0301616,"Missing"
D19-1593,D18-1119,0,0.0800662,"nomena are separable based on the provided representations, they do not tell us about the overall geometry of the representational spaces. RSA, on the other hand, provides a basis for higher-order comparisons between spaces of representations, and a way to visualise and quantify the extent to which they are isomorphic. Indeed, RSA has seen a modest introduction within interpretable NLP in recent years. For example, Chrupała et al. (2017) employed RSA as a means of correlating encoder representations of speech, text, and images in a post-hoc analysis of a multi-task neural pipeline. Similarly, Bouchacourt and Baroni (2018) used the framework to measure the similarity between input image embeddings and the representations of the same image by an agent in an language game setting. More recently, Chrupała and Alishahi (2019) correlated activation patterns of sentence encoders with symbolic representations, such as syntax trees. Lastly, similar to our work here, Abnar et al. (2019) proposed an extension to RSA that enables the comparison of a single model in the face of isolated, changing parameters, and employed this metric along with RSA to correlate NLP models’ and human brains’ respective representations of lan"
D19-1593,P19-1283,0,0.0436797,"sons between spaces of representations, and a way to visualise and quantify the extent to which they are isomorphic. Indeed, RSA has seen a modest introduction within interpretable NLP in recent years. For example, Chrupała et al. (2017) employed RSA as a means of correlating encoder representations of speech, text, and images in a post-hoc analysis of a multi-task neural pipeline. Similarly, Bouchacourt and Baroni (2018) used the framework to measure the similarity between input image embeddings and the representations of the same image by an agent in an language game setting. More recently, Chrupała and Alishahi (2019) correlated activation patterns of sentence encoders with symbolic representations, such as syntax trees. Lastly, similar to our work here, Abnar et al. (2019) proposed an extension to RSA that enables the comparison of a single model in the face of isolated, changing parameters, and employed this metric along with RSA to correlate NLP models’ and human brains’ respective representations of language. We hope to position our work among this brief survey and further demonstrate the flexibility of RSA across several levels of abstraction. 2 Representational Similarity Analysis RSA was proposed by"
D19-1593,P17-1057,0,0.169026,"Missing"
D19-1593,D17-1070,0,0.526849,"representations within models’ internal layers and between different models’ layers. 5838 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5838–5845, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Understanding and analysing language encoders In recent years, some prominent efforts towards interpreting neural networks for NLP have included: developing suites that evaluate network representations through performance on downstream tasks (Conneau et al., 2017a; Wang et al., 2018; McCann et al., 2018); analyzing network predictions on carefully curated datasets (Linzen et al., 2016; Marvin and Linzen, 2018; Gulordava et al., 2018; Loula et al., 2018; Dasgupta et al., 2018; Tenney et al., 2018); and employing diagnostic classifiers to assess whether certain classes of information are encoded in a model’s (intermediate) representations (Adi et al., 2016; Chrupała et al., 2017; Hupkes et al., 2017; Belinkov et al., 2017). While these approaches provide valuable insights into how neural networks process a large variety of phenomena, they rely on decodi"
D19-1593,N18-1108,0,0.0204398,"essing and the 9th International Joint Conference on Natural Language Processing, pages 5838–5845, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Understanding and analysing language encoders In recent years, some prominent efforts towards interpreting neural networks for NLP have included: developing suites that evaluate network representations through performance on downstream tasks (Conneau et al., 2017a; Wang et al., 2018; McCann et al., 2018); analyzing network predictions on carefully curated datasets (Linzen et al., 2016; Marvin and Linzen, 2018; Gulordava et al., 2018; Loula et al., 2018; Dasgupta et al., 2018; Tenney et al., 2018); and employing diagnostic classifiers to assess whether certain classes of information are encoded in a model’s (intermediate) representations (Adi et al., 2016; Chrupała et al., 2017; Hupkes et al., 2017; Belinkov et al., 2017). While these approaches provide valuable insights into how neural networks process a large variety of phenomena, they rely on decoding accuracy as a probe for encoded linguistic information. If properly biased, this means that they can detect whether information is encoded in a representation or not. How"
D19-1593,D16-1009,0,0.0524819,"Missing"
D19-1593,Q16-1037,0,0.0357367,"on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5838–5845, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Understanding and analysing language encoders In recent years, some prominent efforts towards interpreting neural networks for NLP have included: developing suites that evaluate network representations through performance on downstream tasks (Conneau et al., 2017a; Wang et al., 2018; McCann et al., 2018); analyzing network predictions on carefully curated datasets (Linzen et al., 2016; Marvin and Linzen, 2018; Gulordava et al., 2018; Loula et al., 2018; Dasgupta et al., 2018; Tenney et al., 2018); and employing diagnostic classifiers to assess whether certain classes of information are encoded in a model’s (intermediate) representations (Adi et al., 2016; Chrupała et al., 2017; Hupkes et al., 2017; Belinkov et al., 2017). While these approaches provide valuable insights into how neural networks process a large variety of phenomena, they rely on decoding accuracy as a probe for encoded linguistic information. If properly biased, this means that they can detect whether infor"
D19-1593,W18-5413,0,0.0228363,"national Joint Conference on Natural Language Processing, pages 5838–5845, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Understanding and analysing language encoders In recent years, some prominent efforts towards interpreting neural networks for NLP have included: developing suites that evaluate network representations through performance on downstream tasks (Conneau et al., 2017a; Wang et al., 2018; McCann et al., 2018); analyzing network predictions on carefully curated datasets (Linzen et al., 2016; Marvin and Linzen, 2018; Gulordava et al., 2018; Loula et al., 2018; Dasgupta et al., 2018; Tenney et al., 2018); and employing diagnostic classifiers to assess whether certain classes of information are encoded in a model’s (intermediate) representations (Adi et al., 2016; Chrupała et al., 2017; Hupkes et al., 2017; Belinkov et al., 2017). While these approaches provide valuable insights into how neural networks process a large variety of phenomena, they rely on decoding accuracy as a probe for encoded linguistic information. If properly biased, this means that they can detect whether information is encoded in a representation or not. However, they do not al"
D19-1593,D18-1151,0,0.0135415,"in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5838–5845, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Understanding and analysing language encoders In recent years, some prominent efforts towards interpreting neural networks for NLP have included: developing suites that evaluate network representations through performance on downstream tasks (Conneau et al., 2017a; Wang et al., 2018; McCann et al., 2018); analyzing network predictions on carefully curated datasets (Linzen et al., 2016; Marvin and Linzen, 2018; Gulordava et al., 2018; Loula et al., 2018; Dasgupta et al., 2018; Tenney et al., 2018); and employing diagnostic classifiers to assess whether certain classes of information are encoded in a model’s (intermediate) representations (Adi et al., 2016; Chrupała et al., 2017; Hupkes et al., 2017; Belinkov et al., 2017). While these approaches provide valuable insights into how neural networks process a large variety of phenomena, they rely on decoding accuracy as a probe for encoded linguistic information. If properly biased, this means that they can detect whether information is encoded in a re"
D19-1593,D13-1075,1,0.824891,"confirm the hypothesis that the sentences that are most challenging for humans to process, are the sentences (a) the layers of BERT disagree most on among themselves; and (b) that ELMo and BERT disagree most on, indicating that there may be common factors which affect human processing difficulty and result in disagreement between layers. By Layer disagreement we refer to the expression 1 − VCorrLi −Lj . It is important to note that these encoders are trained with a language modelling objective, unlike models where reading behaviour is explicitly modelled (Hahn and Keller, 2016) or predicted (Matthies and Søgaard, 2013). Indeed, the similarities here emerge naturally as a function of the task being performed. This can be seen as analogous to the case of similarities observed between neural networks trained to perform object recognition and spatio-temporal cortical dynamics (Cichy et al., 2016). Syntactic complexity Figure 2 shows that, for all combinations of BERT layers, total fixation time and Yngve scores have strong negative and positive correlations (respectively) with layer disagreement. Furthermore, we observe that disagreement between middle layers seems to show the strongest correlation with Yngve s"
D19-1593,N18-1202,0,0.0563779,"istic features which affect processing difficulty. For each of the following the result is also a vector of length 2, 368 where each cell corresponds to a sentence: a. the average word log frequency per sentence extracted from the British National Corpus (Leech, 1992), VlogF req. . b. the average number of senses per word per sentence extracted from WordNet (Miller, 1995), VwordSense . c. Yngve scores, a standard measure of syntactic complexity based on cognitive load (Yngve, 1960) , VY ngve . Pretrained encoders We conduct our analysis on pretrained BERT-large (Devlin et al., 2018) and ELMo (Peters et al., 2018), two widely employed contextual sentence encoders. To obtain a representation of a sentence from a given layer L, we perform mean-pooling over the time-steps which correspond to the words of a sentence, obtaining a vector representation of the sentence. Meanpooling is a common approach for obtaining vector representations of sentences for downstream tasks (Peters et al., 2018; Conneau et al., 2017b). We refer to ELMo’s lowest layer as E1, BERT’s 11th layer as B11, etc. RDMs We construct an RDM (see §2) for each contextual encoder’s layers. Each RDM is a 2, 368 × 2, 368 matrix which represents"
D19-1593,P19-1452,0,0.0127339,"o distinguish layers as either “adjacent” or “non-adjacent”. Considering these two factors as three- and two-leveled independent variables respectively, we conduct a two-way analysis of variance. The analysis reveals that the effect of group is significant at F (3, 275) = 78.47, p < 0.0001, with “low” (µ = 0.65, σ = 0.08), “middle” (µ = 0.84, σ = 0.03), “high” (µ = 0.80, σ = 0.05), and “out” (µ = 0.80, σ = 0.05). Neither the effect of adjacency nor its interaction with group proved to be significant. This can be seen as (modest) support for the findings of previous work (Blevins et al., 2018; Tenney et al., 2019): namely, that the intermediate layers of neural language models encode the Conclusion We presented a framework for analyzing neural network representations (RSA) that allowed us to relate human sentence processing data with language encoder representations. In experiments conducted on two widely used encoders, our findings show that sentences which are difficult for humans to process have more divergent representations both intra-encoder and between different encoders. Furthermore, we lend modest support to the intuition that a model’s middle layers encode comparatively more syntax. Our frame"
D19-1593,W18-5446,1,0.822711,"models’ internal layers and between different models’ layers. 5838 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5838–5845, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Understanding and analysing language encoders In recent years, some prominent efforts towards interpreting neural networks for NLP have included: developing suites that evaluate network representations through performance on downstream tasks (Conneau et al., 2017a; Wang et al., 2018; McCann et al., 2018); analyzing network predictions on carefully curated datasets (Linzen et al., 2016; Marvin and Linzen, 2018; Gulordava et al., 2018; Loula et al., 2018; Dasgupta et al., 2018; Tenney et al., 2018); and employing diagnostic classifiers to assess whether certain classes of information are encoded in a model’s (intermediate) representations (Adi et al., 2016; Chrupała et al., 2017; Hupkes et al., 2017; Belinkov et al., 2017). While these approaches provide valuable insights into how neural networks process a large variety of phenomena, they rely on decoding accuracy as a pro"
D19-1662,D11-1120,0,0.0547334,"), with 5 a neutral middle value. The mean for sentences by male authors selected without adversarial training is 4.68 and for females 5.69. With adversarial training, this mean is 4.83. The mean of sentences extracted from the model trained with adversarial training is closest to the neutral value 5, but independent ttests show that the differences between all classes are insignificant (p > 0.05). Therefore, the results show that humans had difficulties determining the gender of the author. This is contrary to the findings of Flekova et al. (2016), but is similar by the unaveraged results of Burger et al. (2011). This indicates that the data did not exhibit (m)any obvious predictors of gender or age. In addition to this study, we also use this data to visualize what our diagnostic classifiers focus on. For this purpose, we use the models from Section 2, trained on the full PAN 16 T WIT dataset, and perform feature analysis using uptraining: Us6332 AGE (a) G ENDER −A DV (b) G ENDER +A DV G ENDER T EST SET -A DV +A DV -A DV +A DV PAN 16 T WIT 67.86* 53.45* 56.79* 53.74* PAN 16 R AND 50.13 50.32 49.50 50.00 100 AUTH T WIT – CF T WITTER – – – 50.92 51.63* 52.57 50.74* B LOGS R EVIEWS SOME 55.12* 50.26 50"
D19-1662,D18-1002,1,0.376033,"Elazar2 , Desmond Elliott1 , Anders Søgaard1 1 University of Copenhagen, 2 Bar-Ilan University {mjb,yova,de,soegaard}@di.ku.dk,yanaiela@gmail.com Abstract source languages in multi-lingual machine translation (Xie et al., 2017). Elazar and Goldberg (2018) argue, however, that adversarial learning does not fully remove sensitive demographic traits from the data representations. This conclusion is based on the observation that a diagnostic classifier trained over the supposedly debiased data representations could still predict gender, age and race above chance level in their experimental setup. Elazar and Goldberg (2018) showed that protected attributes can be extracted from the representations of a debiased neural network for mention detection at above-chance levels, by evaluating a diagnostic classifier on a heldout subsample of the data it was trained on. We revisit their experiments and conduct a series of follow-up experiments showing that, in fact, the diagnostic classifier generalizes poorly to both new in-domain samples and new domains, indicating that it relies on correlations specific to their particular data sample. We further show that a diagnostic classifier trained on the biased baseline neural"
D19-1662,P16-1080,0,0.0569942,"Missing"
D19-1662,P18-2005,0,0.0336493,"as in our models. 1 Introduction Several approaches have been proposed to learn classifiers that are invariant (unbiased with respect) to protected attributes: cost-sensitive (Agarwal et al., 2018), regularization-based (Bechavod and Ligett, 2017), and adversarial (Ganin and Lempitsky, 2015). In the adversarial approach, a model learns representations x that should be predictive for a main task y and oblivious to a protected attribute z. Adversarial training has been used to learn data representations that are invariant to demographic attributes (Raff and Sylvester, 2018; Beutel et al., 2017; Li et al., 2018), as well as representations invariant to domain differences (Ganin and Lempitsky, 2015), clean or noisy speech (Sriram et al., 2018), and invariant to differences between In general, diagnostic classifiers are trained on data representations to predict the protected demographic attributes in question as well as possible, i.e., the classifier picks up on any correlations, strong or weak, between data representations and the demographic classes. Correlations come in different flavours: P REVALENT: Certain features are indicative of gender in most contexts, e.g., the distribution of phrases like"
D19-1662,W19-4825,0,0.0272155,"), but we report results on exactly the same development split as well as on the new held-out test split. PAN 16 T WIT is balanced using undersampling with respect to main task and demographic attribute (gender and age respectively) which is why there are separate datasets for G ENDER and AGE. Our main observation here is that training, development and test splits and random subsamples of one sample of data. Using random subsamples this way is common in machine learning, including bias detection studies (Elazar and Goldberg, 2018; Zhao et al., 2019) and probing studies (Ravfogel et al., 2018; Lin et al., 2019), but is known to overestimate performance (Globerson and Roweis, 2016), in particular for highdimensional problems. Replication We start by replicating the experiment of Elazar and Goldberg (2018) using their code on PAN 16 T WIT with their data splits. The main task is predicting the mentions of other Twitter users after removing all user names in the tweets. The protected demographic attributes are age and gender, both with binary targets. Our development results (main and diagnostic classifier) which are comparable to Elazar and Goldberg (2018) are reported in Table 6 in the Appendix; test"
D19-1662,W18-5412,0,0.0189565,"azar and Goldberg (2018), but we report results on exactly the same development split as well as on the new held-out test split. PAN 16 T WIT is balanced using undersampling with respect to main task and demographic attribute (gender and age respectively) which is why there are separate datasets for G ENDER and AGE. Our main observation here is that training, development and test splits and random subsamples of one sample of data. Using random subsamples this way is common in machine learning, including bias detection studies (Elazar and Goldberg, 2018; Zhao et al., 2019) and probing studies (Ravfogel et al., 2018; Lin et al., 2019), but is known to overestimate performance (Globerson and Roweis, 2016), in particular for highdimensional problems. Replication We start by replicating the experiment of Elazar and Goldberg (2018) using their code on PAN 16 T WIT with their data splits. The main task is predicting the mentions of other Twitter users after removing all user names in the tweets. The protected demographic attributes are age and gender, both with binary targets. Our development results (main and diagnostic classifier) which are comparable to Elazar and Goldberg (2018) are reported in Table 6 in"
D19-1662,N19-1064,0,0.0305459,"wer sentences in our train split than Elazar and Goldberg (2018), but we report results on exactly the same development split as well as on the new held-out test split. PAN 16 T WIT is balanced using undersampling with respect to main task and demographic attribute (gender and age respectively) which is why there are separate datasets for G ENDER and AGE. Our main observation here is that training, development and test splits and random subsamples of one sample of data. Using random subsamples this way is common in machine learning, including bias detection studies (Elazar and Goldberg, 2018; Zhao et al., 2019) and probing studies (Ravfogel et al., 2018; Lin et al., 2019), but is known to overestimate performance (Globerson and Roweis, 2016), in particular for highdimensional problems. Replication We start by replicating the experiment of Elazar and Goldberg (2018) using their code on PAN 16 T WIT with their data splits. The main task is predicting the mentions of other Twitter users after removing all user names in the tweets. The protected demographic attributes are age and gender, both with binary targets. Our development results (main and diagnostic classifier) which are comparable to Elazar and"
D19-6112,N18-1008,0,0.0206736,"learning (MTL) strategies to improve accuracy. We found that sharing more components between main and auxiliary tasks is usually better, and autoencoding generally provides the most benefit for our task. Analysis showed that this is mainly because MTL helps the model learn that most characters should stay the same, and that its beneficial effect vanishes as the size of the training set increases. While our models did not beat the nonneural models of Bollmann (2019), we believe our work still provides interesting insights into the impact of MTL for low-resource scenarios. all languages, while Anastasopoulos and Chiang (2018) compare both parallel and cascading model configurations. A different MTL approach is to share all parts of a model, but prepend a task-specific symbol to the input string to enable it to learn task-specific features (cf. Sec. 3.5). Milde et al. (2017) use this approach for grapheme-to-phoneme conversion; Kann et al. (2017) apply it to morphological paradigm completion. Auxiliary tasks for MTL For which auxiliary task(s) to use (Sec 3.2), few systematic studies exist. Most approaches use tasks that are deemed to be related to the main task—e.g., combining machine translation with syntactic pa"
D19-6112,P16-1038,0,0.0662781,"Missing"
D19-6112,E17-2026,1,0.845066,"odel, but prepend a task-specific symbol to the input string to enable it to learn task-specific features (cf. Sec. 3.5). Milde et al. (2017) use this approach for grapheme-to-phoneme conversion; Kann et al. (2017) apply it to morphological paradigm completion. Auxiliary tasks for MTL For which auxiliary task(s) to use (Sec 3.2), few systematic studies exist. Most approaches use tasks that are deemed to be related to the main task—e.g., combining machine translation with syntactic parsing (Kiperwasser and Ballesteros, 2018)—and justify their choice by the effectiveness of the resulting model. Bingel and Søgaard (2017) analyze beneficial task relations for MTL in more detail, but only consider sequence labelling tasks. For zero-shot learning (Sec. 3.5), we use an architecture very similar to Johnson et al. (2016), also used for grapheme-tophoneme mapping in Peters et al. (2017). 6 Acknowledgments We would like to thank the anonymous reviewers of this as well as previous iterations of this paper for several helpful comments. This research has received funding from the European Research Council under the ERC Starting Grant LOWLANDS No. 313695. Marcel Bollmann was partly funded from the European Union’s Horizo"
D19-6112,P15-1166,0,0.0258519,"languages, taken from Bollmann (2019). Table 1 gives an overview of the languages and the size of the development set, which we use for evaluation. 2 3 Multi-task learning Multi-task learning (MTL) is a technique to improve generalization by training a model jointly on a set of related tasks. We follow the common approach of hard parameter sharing suggested by Caruana (1993), in which certain parts of a model architecture are shared across all tasks, while others are kept distinct for each one. Such approaches have been applied successfully to a variety of problems, e.g., machine translation (Dong et al., 2015), sequence labelling (Yang et al., 2016; Peng and Dredze, 2017), or discourse parsing (Braud et al., 2016). Model architecture We use a standard attentional encoder–decoder architecture (Bahdanau et al., 2014) with words as 3 input sequences and characters as input symbols. Following the majority of previous work on this topic (cf. Sec. 5), we limit ourselves to word-byword normalization, ignoring problems of contextual ambiguity. Our model consists of the following parts (which we will also refer to using the bolded letters): • Source embedding layer: transforms input characters into dense ve"
D19-6112,L16-1169,0,0.0657935,"Missing"
D19-6112,N19-1389,1,0.868577,"s used for the main task. We do this by training MTL models (using all three auxiliary tasks, as in Sec. 3.1) with varying amounts of historical training data, ranging from 100 tokens to 50,000 tokens. Different 6 The same is true, of course, for the size of the auxiliary datasets. We try to balance out this factor by balancing the training updates as described in Sec. 3 “Training details”, but we also note that we do not observe a correlation between auxiliary dataset size and its effectiveness for MTL in Fig. 3. 107 Dataset Single Multi-task Dataset Autoenc Lemma g2p A LL 3 Best in (a) from Bollmann (2019) Norma SMT NMT DEA DER EN ES HU IS PT SLB SLG SV 54.84 56.72 66.95 74.68 42.44 63.40 72.23 74.06 86.34 69.98 56.41 65.05 76.94 77.87 40.39 67.31 76.28 74.44 87.86 70.29 56.55 63.79 73.84 76.97 40.49 67.02 73.89 74.54 86.15 72.34 55.99 60.25 68.72 78.45 40.07 66.31 74.27 75.59 87.40 65.97 56.52 64.49 72.01 79.09 38.64 68.51 75.55 74.39 89.45 73.05 DEA DER EN ES HU IS PT SLB SLG SV 56.55 65.05 76.94 79.09 42.44 68.51 76.28 75.59 89.45 73.05 61.27 73.62 84.53 86.21 55.75 70.86 82.94 78.97 84.36 74.54 58.60 75.04 83.81 85.89 53.00 72.30 82.00 82.90 90.00 78.51 52.74 60.61 66.93 76.32 40.52 62.80 7"
D19-6112,N16-1101,0,0.0665338,"Missing"
D19-6112,W18-4510,0,0.0982905,"Missing"
D19-6112,P17-1031,1,0.565064,"time, using 4 Whenever possible, we used the dumps provided by the Polyglot project: https://sites.google.com/ site/rmyeid/projects/polyglot Since an Icelandic text dump was not available from Polyglot, we generated one ourselves using the Cirrus Extractor: https://github.com/attardi/wikiextractor All dumps were cleaned from punctuation marks. 2 The datasets are available from: https://github.com/coastalcph/histnorm 3 Our implementation uses the XNMT toolkit (Neubig et al., 2018, https://github.com/neulab/xnmt). 105 (2016) to map words (i.e., sequences of graphemes) to sequences of phonemes. Bollmann et al. (2017) previously showed that this task can improve historical normalization, possibly because changes in spelling are often motivated by phonological processes, an assumption also made by other normalization systems (Porta et al., 2013; Etxeberria et al., 2016). SEADP SEATD SADP SEATP SED ETDP SETP SETD ⋯ No MTL ⋯ AT ST EP ATD • Lemmatization. We use the UniMorph 5 dataset (Kirov et al., 2018) to learn mappings from inflected word forms to their lemmas. This task is similar to normalization in that it maps a set of different word forms to a single target form, which typically bears a high resemblan"
D19-6112,W18-3403,1,0.6726,"Science, University of Copenhagen Institute of Computational Linguistics, University of Zurich marcel@di.ku.dk, korchagina@ifi.uzh.ch, soegaard@di.ku.dk ♢ Abstract learning strategies (a) to leverage whatever supervision is available for the language in question (few-shot learning), or (b) to do away with the need for supervision in the target language altogether (zero-shot learning). Bollmann et al. (2017) previously showed that multi-task learning with grapheme-to-phoneme conversion as an auxiliary task improves a sequence-to-sequence model for historical text normalization of German texts; Bollmann et al. (2018) showed that multi-task learning is particularly helpful in low-resource scenarios. We consider three auxiliary tasks in our experiments— grapheme-to-phoneme mapping, autoencoding, and lemmatization—and focus on extremely lowresource settings. Our paper makes several contributions: Historical text normalization often relies on small training datasets. Recent work has shown that multi-task learning can lead to significant improvements by exploiting synergies with related datasets, but there has been no systematic study of different multitask learning architectures. This paper evaluates 63 multi"
D19-6112,Q17-1024,0,0.0694754,"Missing"
D19-6112,C16-1179,1,0.824294,"elopment set, which we use for evaluation. 2 3 Multi-task learning Multi-task learning (MTL) is a technique to improve generalization by training a model jointly on a set of related tasks. We follow the common approach of hard parameter sharing suggested by Caruana (1993), in which certain parts of a model architecture are shared across all tasks, while others are kept distinct for each one. Such approaches have been applied successfully to a variety of problems, e.g., machine translation (Dong et al., 2015), sequence labelling (Yang et al., 2016; Peng and Dredze, 2017), or discourse parsing (Braud et al., 2016). Model architecture We use a standard attentional encoder–decoder architecture (Bahdanau et al., 2014) with words as 3 input sequences and characters as input symbols. Following the majority of previous work on this topic (cf. Sec. 5), we limit ourselves to word-byword normalization, ignoring problems of contextual ambiguity. Our model consists of the following parts (which we will also refer to using the bolded letters): • Source embedding layer: transforms input characters into dense vectors. Auxiliary tasks We experiment with the following auxiliary tasks: • Encoder: a single bidirectional"
D19-6112,P17-1182,0,0.0490262,"Missing"
D19-6112,W13-5617,0,0.0715634,"Missing"
D19-6112,Q18-1017,0,0.0243662,"e both parallel and cascading model configurations. A different MTL approach is to share all parts of a model, but prepend a task-specific symbol to the input string to enable it to learn task-specific features (cf. Sec. 3.5). Milde et al. (2017) use this approach for grapheme-to-phoneme conversion; Kann et al. (2017) apply it to morphological paradigm completion. Auxiliary tasks for MTL For which auxiliary task(s) to use (Sec 3.2), few systematic studies exist. Most approaches use tasks that are deemed to be related to the main task—e.g., combining machine translation with syntactic parsing (Kiperwasser and Ballesteros, 2018)—and justify their choice by the effectiveness of the resulting model. Bingel and Søgaard (2017) analyze beneficial task relations for MTL in more detail, but only consider sequence labelling tasks. For zero-shot learning (Sec. 3.5), we use an architecture very similar to Johnson et al. (2016), also used for grapheme-tophoneme mapping in Peters et al. (2017). 6 Acknowledgments We would like to thank the anonymous reviewers of this as well as previous iterations of this paper for several helpful comments. This research has received funding from the European Research Council under the ERC Starti"
D19-6112,L18-1293,0,0.052869,"Missing"
D19-6112,W17-0504,1,0.838813,"these documents amenable to search by today’s scholars, processable by NLP tools, and accessible to lay people. Many historical documents were written in the absence of standard spelling conventions, and annotated datasets are rare and small, making automatic normalization a challenging task (cf. Piotrowski, 2012; Bollmann, 2018). In this paper, we experiment with datasets in eight different languages: English, German, Hungarian, Icelandic, Portuguese, Slovene, Spanish, and Swedish. We use a standard neural sequenceto-sequence model, which has been shown to be competitive for this task (e.g., Korchagina, 2017; Bollmann, 2018; Tang et al., 2018). Our main focus is on analyzing the usefulness of multi-task (b) We show that in few-shot learning scenarios (ca. 1,000 tokens), multi-task learning leads to robust, significant gains over a state-of1 the-art, single-task baseline. (c) We are, to the best of our knowledge, the first to consider zero-shot historical text normalization, and we show significant improvements over the simple, but relatively strong, identity baseline. While our focus is on the specific task of historical text normalization, we believe that our results can be of interest to anyone"
D19-6112,N18-2113,0,0.0369219,"Missing"
D19-6112,C18-1112,0,0.0201319,"by today’s scholars, processable by NLP tools, and accessible to lay people. Many historical documents were written in the absence of standard spelling conventions, and annotated datasets are rare and small, making automatic normalization a challenging task (cf. Piotrowski, 2012; Bollmann, 2018). In this paper, we experiment with datasets in eight different languages: English, German, Hungarian, Icelandic, Portuguese, Slovene, Spanish, and Swedish. We use a standard neural sequenceto-sequence model, which has been shown to be competitive for this task (e.g., Korchagina, 2017; Bollmann, 2018; Tang et al., 2018). Our main focus is on analyzing the usefulness of multi-task (b) We show that in few-shot learning scenarios (ca. 1,000 tokens), multi-task learning leads to robust, significant gains over a state-of1 the-art, single-task baseline. (c) We are, to the best of our knowledge, the first to consider zero-shot historical text normalization, and we show significant improvements over the simple, but relatively strong, identity baseline. While our focus is on the specific task of historical text normalization, we believe that our results can be of interest to anyone looking to apply multi-task learnin"
D19-6112,W18-1818,0,0.0166033,"mapping (g2p). This task uses the data by Deri and Knight • Decoder: a single LSTM that decodes the encoded sequence one character at a time, using 4 Whenever possible, we used the dumps provided by the Polyglot project: https://sites.google.com/ site/rmyeid/projects/polyglot Since an Icelandic text dump was not available from Polyglot, we generated one ourselves using the Cirrus Extractor: https://github.com/attardi/wikiextractor All dumps were cleaned from punctuation marks. 2 The datasets are available from: https://github.com/coastalcph/histnorm 3 Our implementation uses the XNMT toolkit (Neubig et al., 2018, https://github.com/neulab/xnmt). 105 (2016) to map words (i.e., sequences of graphemes) to sequences of phonemes. Bollmann et al. (2017) previously showed that this task can improve historical normalization, possibly because changes in spelling are often motivated by phonological processes, an assumption also made by other normalization systems (Porta et al., 2013; Etxeberria et al., 2016). SEADP SEATD SADP SEATP SED ETDP SETP SETD ⋯ No MTL ⋯ AT ST EP ATD • Lemmatization. We use the UniMorph 5 dataset (Kirov et al., 2018) to learn mappings from inflected word forms to their lemmas. This task"
D19-6112,W17-2612,0,0.0216968,"rview of the languages and the size of the development set, which we use for evaluation. 2 3 Multi-task learning Multi-task learning (MTL) is a technique to improve generalization by training a model jointly on a set of related tasks. We follow the common approach of hard parameter sharing suggested by Caruana (1993), in which certain parts of a model architecture are shared across all tasks, while others are kept distinct for each one. Such approaches have been applied successfully to a variety of problems, e.g., machine translation (Dong et al., 2015), sequence labelling (Yang et al., 2016; Peng and Dredze, 2017), or discourse parsing (Braud et al., 2016). Model architecture We use a standard attentional encoder–decoder architecture (Bahdanau et al., 2014) with words as 3 input sequences and characters as input symbols. Following the majority of previous work on this topic (cf. Sec. 5), we limit ourselves to word-byword normalization, ignoring problems of contextual ambiguity. Our model consists of the following parts (which we will also refer to using the bolded letters): • Source embedding layer: transforms input characters into dense vectors. Auxiliary tasks We experiment with the following auxilia"
D19-6112,W17-5403,0,0.0684356,"Missing"
D19-6130,L18-1614,0,0.080506,"Missing"
D19-6130,P16-1145,0,0.0607532,"Missing"
D19-6130,Q17-1010,0,0.0479708,"Missing"
D19-6130,D18-1456,0,0.0281005,"Missing"
D19-6130,P17-1171,0,0.0517213,"Missing"
D19-6130,K17-1034,0,0.0289666,"Missing"
D19-6130,P16-1105,0,0.0502904,"Missing"
D19-6130,D18-1269,0,0.0592708,"Missing"
D19-6130,W18-5511,0,0.0327858,"Missing"
D19-6130,D14-1162,0,0.0823085,"Missing"
D19-6130,P18-2124,0,0.0491031,"Missing"
D19-6130,D11-1142,0,0.194172,"Missing"
D19-6130,N13-1008,0,0.115232,"Missing"
D19-6130,N15-1151,0,0.0572468,"Missing"
D19-6130,N15-1118,0,0.0751173,"Missing"
D19-6130,P18-1156,1,0.892562,"Missing"
D19-6130,W17-2623,0,0.0644235,"Missing"
D19-6130,N16-1103,0,0.0476727,"Missing"
D19-6130,P05-1053,0,0.179761,"Missing"
D19-6130,N07-4013,0,\N,Missing
D19-6130,S17-2001,0,\N,Missing
E14-1078,J09-4005,0,0.0425861,"han others, and which is thus not reflected in learned predictors. We incorporate the annotator uncertainty on certain labels by measuring annotator agreement and use it in the modified loss function of a structured perceptron. We show that this approach works well independent of regularization, both on in-sample and out-of-sample data. Moreover, when evaluating the models trained with our loss function on downstream tasks, we observe improvements on two different tasks. Our results suggest that we need to pay more attention to annotator confidence when training predictors. In a similar vein, Klebanov and Beigman (2009) divide the instance space into easy and hard cases, i.e. easy cases are reliably annotated, whereas items that are hard show confusion and disagreement. Hard cases are assumed to be annotated by individual annotator’s coin-flips, and thus cannot be assumed to be uniformly distributed (Klebanov and Beigman, 2009). They show that learning with annotator noise can have deteriorating effect at test time, and thus propose to remove hard cases, both at test time (Klebanov and Beigman, 2009) and training time (Beigman and Klebanov, 2009). Acknowledgements We would like to thank the anonymous reviewe"
E14-1078,W02-1001,0,0.0419919,"and A2 another, and vice versa. We experiment with both agreement scores (F 1 and confusion matrix probabilities) to augment the loss function in our learner. The next section describes this modification in detail. 4 γ(yj , yi )) = 1 − P ({A1 (X), A2 (X)} = {yj , yi }) In both loss functions, a lower gamma value means that the tags are more likely to be confused by a pair of annotators. In this case, the update is smaller. In contrast, the learner incurs greater loss when easy tags are confused. It is straight-forward to extend these costsensitive loss functions to the structured perceptron (Collins, 2002). In Figure 4, we provide the pseudocode for the cost-sensitive structured online learning algorithm. We refer to the cost-sensitive structured learners as F 1- and CM-weighted below. Inter-annotator agreement loss We briefly introduce the cost-sensitive perceptron classifier. Consider the weighted perceptron loss on our ith example hxi , yi i (with learning rate α = 1), Lw (hxi , yi i): γ(sign(w · xi ), yi ) max(0, −yi w · xi ) 5 Experiments In our main experiments, we use structured perceptron (Collins, 2002) with random corruptions In a non-cost-sensitive classifier, the weight function γ(y"
E14-1078,P11-1061,0,0.00446396,"o disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP"
E14-1078,R13-1026,0,0.0146309,"Missing"
E14-1078,N13-1070,1,0.0361792,"2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features (Elming et al., 2013). Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (Schwartz et al., 2011; Tsarfaty et al., 2012), as well as Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators 742 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics R ITTER -T RAIN and G IMPEL -T RAIN and evaluate them on the remain"
E14-1078,J93-2004,0,0.0539354,"ussion on October 10th re the aftermath of #seanref . . . While linguists will agree that in is a preposition, and panel discussion a compound noun, they are likely to disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections o"
E14-1078,W10-0713,0,0.111643,"Missing"
E14-1078,I11-1100,0,0.0198995,"Missing"
E14-1078,N13-1039,0,0.0226252,"Missing"
E14-1078,fromreide-etal-2014-crowdsourcing,1,0.835032,", we do not have access to carefully annotated Twitter data for training, but rely on the crowdsourced annotations described in Finin et al. (2010). We use the concatenation of the CoNLL 2003 training split of annotated data from the Reuters corpus and the Finin data for training, as in this case training on the union resulted in a model that is substantially better than training on any of the individual data sets. For evaluation, we have three Twitter data set. We use the recently published data set from the MSM 2013 challenge (29k tokens)6 , the data set of Ritter et al. (2011) used also by Fromheide et al. (2014) (46k tokens), as well as an in-house annotated data set (20k tokens) (Fromheide et al., 2014). F1: BL CM R ITTER 78.20 78.30 MSM 82.25 82.00 7 Related work Cost-sensitive learning takes costs, such as misclassification cost, into consideration. That is, each instance that is not classified correctly during the learning process may contribute differently to the overall error. Geibel and Wysotzki (2003) introduce instance-dependent cost values for the perceptron algorithm and apply it to a set of binary classification problems. We focus here on structured problems and propose cost-sensitive lea"
E14-1078,petrov-etal-2012-universal,0,0.0301893,"l is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on sy"
E14-1078,J08-3001,0,0.0420123,"red and the predicted tag is in the same class as the gold tag, a loss σ occurred, otherwise it counts as full cost. In contrast to our approach, they let the learner focus on the more difficult cases by occurring a bigger loss when the predicted POS tag I N -H OUSE 82.58 82.77 Table 3: Downstream results for named entity recognition (F1 scores). Table 3 shows the result of using our POS models in downstream NER evaluation. Here we observe mixed results. The cost-sensitive model is 5 http://www.ark.cs.cmu.edu/TweetNLP/ http://oak.dcs.shef.ac.uk/msm2013/ie_ challenge/ 6 748 dom but systematic (Reidsma and Carletta, 2008). However, rather than training on subsets of data or training separate models – which all implicitly assume that there is a large amount of training data available – we propose to integrate inter-annotator biases directly into the loss function. Regarding measurements for agreements, several scores have been suggested in the literature. Apart from the simple agreement measure, which records how often annotators choose the same value for an item, there are several statistics that qualify this measure by adjusting for other factors, such as Cohen’s κ (Cohen and others, 1960), the G-index score"
E14-1078,P11-2008,0,0.080131,"Missing"
E14-1078,W08-1203,0,0.405608,"Missing"
E14-1078,D11-1141,0,0.0112468,"s (Owoputi et al., 2013).5 For NER, we do not have access to carefully annotated Twitter data for training, but rely on the crowdsourced annotations described in Finin et al. (2010). We use the concatenation of the CoNLL 2003 training split of annotated data from the Reuters corpus and the Finin data for training, as in this case training on the union resulted in a model that is substantially better than training on any of the individual data sets. For evaluation, we have three Twitter data set. We use the recently published data set from the MSM 2013 challenge (29k tokens)6 , the data set of Ritter et al. (2011) used also by Fromheide et al. (2014) (46k tokens), as well as an in-house annotated data set (20k tokens) (Fromheide et al., 2014). F1: BL CM R ITTER 78.20 78.30 MSM 82.25 82.00 7 Related work Cost-sensitive learning takes costs, such as misclassification cost, into consideration. That is, each instance that is not classified correctly during the learning process may contribute differently to the overall error. Geibel and Wysotzki (2003) introduce instance-dependent cost values for the perceptron algorithm and apply it to a set of binary classification problems. We focus here on structured pr"
E14-1078,P11-1067,0,0.0210056,"entences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features (Elming et al., 2013). Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (Schwartz et al., 2011; Tsarfaty et al., 2012), as well as Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators 742 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics R ITTER -T RAIN and G IMPEL -T RAIN and evaluate them on the remaining data, the dev and test set provided by Foster et al. (2011) as well as an inhouse annotated data set of 3k tokens (see bel"
E14-1078,N03-1028,0,0.0984998,"Missing"
E14-1078,P13-2113,1,0.933032,"we incorporate the uncertainty exhibited by annotators in the training of our model. We measure inter-annotator agreement on small samples of data, then incorporate this in the loss function of a structured learner to reflect the confidence we can put in the annotations. This provides us with cost-sensitive online learning algorithms for inducing models from annotated data that take inter-annotator agreement into consideration. Specifically, we use online structured perceptron with drop-out, which has previously been applied to POS tagging and is known to be robust across samples and domains (Søgaard, 2013a). We incorporate the inter-annotator agreement in the loss function either as inter-annotator F 1-scores or as the confusion probability between annotators (see Section 3 below for a more detailed description). We use a small amounts of doublyannotated Twitter data to estimate F 1-scores and confusion probabilities, and incorporate them during training via a modified loss function. Specifically, we use POS annotations made by two annotators on a set of 500 newly sampled tweets to estimate our agreement scores, and train models on existing Twitter data sets (described below). We evaluate the"
E14-1078,N13-1077,1,0.937341,"we incorporate the uncertainty exhibited by annotators in the training of our model. We measure inter-annotator agreement on small samples of data, then incorporate this in the loss function of a structured learner to reflect the confidence we can put in the annotations. This provides us with cost-sensitive online learning algorithms for inducing models from annotated data that take inter-annotator agreement into consideration. Specifically, we use online structured perceptron with drop-out, which has previously been applied to POS tagging and is known to be robust across samples and domains (Søgaard, 2013a). We incorporate the inter-annotator agreement in the loss function either as inter-annotator F 1-scores or as the confusion probability between annotators (see Section 3 below for a more detailed description). We use a small amounts of doublyannotated Twitter data to estimate F 1-scores and confusion probabilities, and incorporate them during training via a modified loss function. Specifically, we use POS annotations made by two annotators on a set of 500 newly sampled tweets to estimate our agreement scores, and train models on existing Twitter data sets (described below). We evaluate the"
E14-1078,N13-1013,0,0.0467928,"Missing"
E14-1078,P12-1108,0,0.0190218,"ssification problems. We focus here on structured problems and propose cost-sensitive learning for POS tagging using the structured perceptron algorithm. In a similar spirit, Higashiyama et al. (2013) applied cost-sensitive learning to the structured perceptron for an entity recognition task in the medical domain. They consider the distance between the predicted and true label sequence smoothed by a parameter that they estimate on a development set. This means that the entire sequence is scored at once, while we update on a per-label basis. The work most related to ours is the recent study of Song et al. (2012). They suggest that some errors made by a POS tagger are more serious than others, especially for downstream tasks. They devise a hierarchy of POS tags for the Penn treebank tag set (e.g. the class NOUN contains NN, NNS, NNP, NNPS and CD) and use that in an SVM learner. They modify the Hinge loss that can take on three values: 0, σ, 1. If an error occurred and the predicted tag is in the same class as the gold tag, a loss σ occurred, otherwise it counts as full cost. In contrast to our approach, they let the learner focus on the more difficult cases by occurring a bigger loss when the predicte"
E14-1078,E12-1006,0,0.022199,"in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features (Elming et al., 2013). Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (Schwartz et al., 2011; Tsarfaty et al., 2012), as well as Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators 742 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics R ITTER -T RAIN and G IMPEL -T RAIN and evaluate them on the remaining data, the dev and test set provided by Foster et al. (2011) as well as an inhouse annotated data set of 3k tokens (see below). when learning predi"
E14-1078,I08-3008,0,0.00695885,"on a compound noun, they are likely to disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effect"
E14-1078,P09-1032,0,\N,Missing
E14-1078,hovy-etal-2014-pos,1,\N,Missing
E14-1078,P13-2017,0,\N,Missing
E17-1021,Q16-1022,1,0.903255,"Missing"
E17-1021,Q16-1023,0,0.0347548,"periments with neural networks for dependency parsing have focused mostly on learning higherorder scoring functions and creating efficient feature representations, with the notable exception of Fonseca et al. (2015). In their paper, a convolutional neural network is used to evaluate local edge scores based on global information. In Zhang and Zhao (2015) and Pei et al. (2015), neural networks are used to simultaneously evaluate first-order and higher-order scores for graph-based parsing, demonstrating good results. Bidirectional LSTM-models have been successfully applied to feature generation (Kiperwasser and Goldberg, 2016). Such LSTM-based features could in future work be employed and trained in conjunction with Tensor-LSTM, incorporating global information both in parsing and in featurization. An extension of LSTM to tensor-structured data has been explored in Graves et al. (2007), and further improved upon in Kalchbrenner et al. (2015) in the form of GridLSTM. Our approach is similar, but simpler and computationally more efficient as no within-layer connections between the first and the second axes of the tensor are required. Annotation projection for dependency parsing has been explored in a number of papers"
E17-1021,P15-2139,0,0.0377278,"tricted to biased data such as the Bible. In Agi´c et al. (2016) this problem is addressed, and a parser is constructed which utilizes averaging over edge posteriors for many source languages to compensate for low-quality projected data. Our work builds upon their contribution by constructing a more flexible parser which can bypass a source of bias in their projected labels, and we therefore compared our results directly to theirs. Annotation projection procedures for crosslingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015). In Guo et al. (2015), distributed, language-independent feature representations are used to train shared parsers. Zhang and Barzilay (2015) introduce a tensor-based feature representation capable of incorporating prior knowledge about feature interactions learned from source languages. In Duong et al. (2015), a neural network parser is built wherein higher-level layers are shared between languages. Finally, Rasooli and Collins (2015) leverage dense information in high-quality sentence translations to improve performance. Their work can be seen as opposite to ours"
E17-1021,E17-1072,1,0.821891,"ed parser of McDonald et al. (2011), and the approach of Agi´c et al. (2016) using TurboParser. To demonstrate the effectiveness of circumventing the decoding step, we conduct the cross-lingual evaluation of Tensor-LSTM using cross entropy loss with early decoding, and using mean squared loss with late decoding. 4.1 30 Model selection and training Our features consist of 500-dimensional word embeddings trained on translations of the Bible. The word embeddings were trained using skipgram with negative sampling on a word-by-sentence PMI matrix induced from the Edinburgh Bible Corpus, following (Levy et al., 2017). Our embeddings are not trainable, but fixed representations throughout the learning process. Unknown tokens were represented by zero-vectors. We combined the word embeddings with onehot-encodings of POS-tags, projected across word alignments following the method of Agi´c et al. (2016). To verify the value of the POS-features, we conducted preliminary experiments on English development data. When including POS224 We experimented with either 5000 or 10000 randomly sampled sentences. There are two motivating factors behind this subsampling. First, while the Bible in general consists of about 30"
E17-1021,C14-1075,0,0.0299446,"both in parsing and in featurization. An extension of LSTM to tensor-structured data has been explored in Graves et al. (2007), and further improved upon in Kalchbrenner et al. (2015) in the form of GridLSTM. Our approach is similar, but simpler and computationally more efficient as no within-layer connections between the first and the second axes of the tensor are required. Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages. Li et al. (2014) follows the method of Hwa et al. (2005) and adds a probabilistic target-language classifier to deter7 Conclusion We have introduced a novel algorithm for graphbased dependency parsing based on an extension of sequence-LSTM to the more general TensorLSTM. We have shown how the parser with a cross entropy loss function performs comparably to state of the art for monolingual parsing. Furthermore, we have demonstrated that the flexibility of our parser enables learning from non wellformed data and from the output of other parsers. Using this property, we have applied our parser to a cross-lingual"
E17-1021,P14-1126,0,0.350352,"Missing"
E17-1021,W15-1508,0,0.0585039,"Missing"
E17-1021,D10-1004,0,0.070246,"Missing"
E17-1021,H05-1066,0,0.293013,"Missing"
E17-1021,P15-1119,0,0.102101,"unrealistic as parallel resources may be restricted to biased data such as the Bible. In Agi´c et al. (2016) this problem is addressed, and a parser is constructed which utilizes averaging over edge posteriors for many source languages to compensate for low-quality projected data. Our work builds upon their contribution by constructing a more flexible parser which can bypass a source of bias in their projected labels, and we therefore compared our results directly to theirs. Annotation projection procedures for crosslingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015). In Guo et al. (2015), distributed, language-independent feature representations are used to train shared parsers. Zhang and Barzilay (2015) introduce a tensor-based feature representation capable of incorporating prior knowledge about feature interactions learned from source languages. In Duong et al. (2015), a neural network parser is built wherein higher-level layers are shared between languages. Finally, Rasooli and Collins (2015) leverage dense information in high-quality sentence translations to improve performanc"
E17-1021,P13-2017,0,0.11056,"Missing"
E17-1021,D15-1213,0,0.113093,"allel resources may be restricted to biased data such as the Bible. In Agi´c et al. (2016) this problem is addressed, and a parser is constructed which utilizes averaging over edge posteriors for many source languages to compensate for low-quality projected data. Our work builds upon their contribution by constructing a more flexible parser which can bypass a source of bias in their projected labels, and we therefore compared our results directly to theirs. Annotation projection procedures for crosslingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015). In Guo et al. (2015), distributed, language-independent feature representations are used to train shared parsers. Zhang and Barzilay (2015) introduce a tensor-based feature representation capable of incorporating prior knowledge about feature interactions learned from source languages. In Duong et al. (2015), a neural network parser is built wherein higher-level layers are shared between languages. Finally, Rasooli and Collins (2015) leverage dense information in high-quality sentence translations to improve performance. Their work can be seen"
E17-1021,Y15-1014,0,0.0288649,"on top of dependency parsing could lead to interesting results. Finally, extensions of the Tensor-LSTM function to deeper models, wider models, or more connected models as seen in e.g. Kalchbrenner et al. (2015) may yield further performance gains. 6 Related Work Experiments with neural networks for dependency parsing have focused mostly on learning higherorder scoring functions and creating efficient feature representations, with the notable exception of Fonseca et al. (2015). In their paper, a convolutional neural network is used to evaluate local edge scores based on global information. In Zhang and Zhao (2015) and Pei et al. (2015), neural networks are used to simultaneously evaluate first-order and higher-order scores for graph-based parsing, demonstrating good results. Bidirectional LSTM-models have been successfully applied to feature generation (Kiperwasser and Goldberg, 2016). Such LSTM-based features could in future work be employed and trained in conjunction with Tensor-LSTM, incorporating global information both in parsing and in featurization. An extension of LSTM to tensor-structured data has been explored in Graves et al. (2007), and further improved upon in Kalchbrenner et al. (2015) in"
E17-1021,P15-1031,0,0.0258057,"ng could lead to interesting results. Finally, extensions of the Tensor-LSTM function to deeper models, wider models, or more connected models as seen in e.g. Kalchbrenner et al. (2015) may yield further performance gains. 6 Related Work Experiments with neural networks for dependency parsing have focused mostly on learning higherorder scoring functions and creating efficient feature representations, with the notable exception of Fonseca et al. (2015). In their paper, a convolutional neural network is used to evaluate local edge scores based on global information. In Zhang and Zhao (2015) and Pei et al. (2015), neural networks are used to simultaneously evaluate first-order and higher-order scores for graph-based parsing, demonstrating good results. Bidirectional LSTM-models have been successfully applied to feature generation (Kiperwasser and Goldberg, 2016). Such LSTM-based features could in future work be employed and trained in conjunction with Tensor-LSTM, incorporating global information both in parsing and in featurization. An extension of LSTM to tensor-structured data has been explored in Graves et al. (2007), and further improved upon in Kalchbrenner et al. (2015) in the form of GridLSTM."
E17-1021,D15-1039,0,0.2402,"ta such as the Bible. In Agi´c et al. (2016) this problem is addressed, and a parser is constructed which utilizes averaging over edge posteriors for many source languages to compensate for low-quality projected data. Our work builds upon their contribution by constructing a more flexible parser which can bypass a source of bias in their projected labels, and we therefore compared our results directly to theirs. Annotation projection procedures for crosslingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015). In Guo et al. (2015), distributed, language-independent feature representations are used to train shared parsers. Zhang and Barzilay (2015) introduce a tensor-based feature representation capable of incorporating prior knowledge about feature interactions learned from source languages. In Duong et al. (2015), a neural network parser is built wherein higher-level layers are shared between languages. Finally, Rasooli and Collins (2015) leverage dense information in high-quality sentence translations to improve performance. Their work can be seen as opposite to ours – whereas Rasooli and Collin"
E17-1021,C14-1175,0,0.0349455,"ture work be employed and trained in conjunction with Tensor-LSTM, incorporating global information both in parsing and in featurization. An extension of LSTM to tensor-structured data has been explored in Graves et al. (2007), and further improved upon in Kalchbrenner et al. (2015) in the form of GridLSTM. Our approach is similar, but simpler and computationally more efficient as no within-layer connections between the first and the second axes of the tensor are required. Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages. Li et al. (2014) follows the method of Hwa et al. (2005) and adds a probabilistic target-language classifier to deter7 Conclusion We have introduced a novel algorithm for graphbased dependency parsing based on an extension of sequence-LSTM to the more general TensorLSTM. We have shown how the parser with a cross entropy loss function performs comparably to state of the art for monolingual parsing. Furthermore, we have demonstrated that the flexibility of our parser enables learning from non wellformed data and f"
E17-1021,W15-2137,0,0.052179,"and trained in conjunction with Tensor-LSTM, incorporating global information both in parsing and in featurization. An extension of LSTM to tensor-structured data has been explored in Graves et al. (2007), and further improved upon in Kalchbrenner et al. (2015) in the form of GridLSTM. Our approach is similar, but simpler and computationally more efficient as no within-layer connections between the first and the second axes of the tensor are required. Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages. Li et al. (2014) follows the method of Hwa et al. (2005) and adds a probabilistic target-language classifier to deter7 Conclusion We have introduced a novel algorithm for graphbased dependency parsing based on an extension of sequence-LSTM to the more general TensorLSTM. We have shown how the parser with a cross entropy loss function performs comparably to state of the art for monolingual parsing. Furthermore, we have demonstrated that the flexibility of our parser enables learning from non wellformed data and from the output of oth"
E17-1022,W15-5301,1,0.894616,"Missing"
E17-1022,P15-2044,1,0.884906,"Missing"
E17-1022,W09-0106,0,0.0296839,"t such a formalism lends itself more naturally to a simple and linguistically sound rulebased approach to cross-lingual parsing. In this paper we present such an approach. Our system is a dependency parser that requires no training, and relies solely on explicit part-ofspeech (POS) constraints that UD imposes. In particular, UD prescribes that trees are single-rooted, and that function words like adpositions, auxiliaries, and determiners are always dependents of content words, while other formalisms might treat them as heads (De Marneffe et al., 2014). We ascribe our work to the viewpoints of Bender (2009) about the incorporation of linguistic knowledge in language-independent systems. We propose UDP, the first training-free parser for Universal Dependencies (UD). Our algorithm is based on PageRank and a small set of head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters"
E17-1022,A00-1031,0,0.0296335,"ord forms. If there is more than one treebank per language, we use the treebank that has the 5.2 The resulting trees always pass the validation script in github.com/UniversalDependencies/tools. They also had a special connection to some extremists They - also had - • • • • a special connection - some extremists • • - to • • • • • - - Evaluation setup Our system relies solely on POS tags. To estimate the quality degradation of our system under non-gold POS scenarios, we evaluate UDP on two alternative scenarios. The first is predicted POS (UDPP ), where we tag the respective test set with TnT (Brants, 2000) trained on each language’s training set. The second is a naive typeconstrained two-POS tag scenario (UDPN ), and approximates a lower bound. We give each word either CONTENT or FUNCTION tag, depending on the word’s frequency. The 100 most frequent words of the input test section receive the FUNC TION tag. 4 −→ Baseline • • • • • - Table 4: Matrix representation of the directed graph for the words in the sentence. 234 Finally, we compare our parser UDP to a supervised cross-lingual system (MSD). It is a multisource delexicalized transfer parser, referred to as multi-dir in the original paper b"
E17-1022,P11-1061,0,0.0305356,"ing, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies,"
E17-1022,de-marneffe-etal-2014-universal,0,0.0875731,"Missing"
E17-1022,W12-1909,0,0.0167797,"head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters and is distinctly robust to domain change across languages. 1 Introduction Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns, which would not be an error if the test data used a DP analysis (Abney, 1987). Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences. The Universal Dependencies (UD) project (Nivre et al., 2015; Nivre et al., 2016) offers a dependency formalism that aims at providing a consistent represent"
E17-1022,P10-2036,0,0.0744508,"roach that takes a fresh angle on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer ann"
E17-1022,P16-2091,1,0.781898,"Missing"
E17-1022,P06-1063,0,0.117258,"Missing"
E17-1022,P04-1061,0,0.494345,"eRank and a small set of head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters and is distinctly robust to domain change across languages. 1 Introduction Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns, which would not be an error if the test data used a DP analysis (Abney, 1987). Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences. The Universal Dependencies (UD) project (Nivre et al., 2015; Nivre et al., 2016) offers a dependency formalism that aims at providing"
E17-1022,P14-1126,0,0.0366931,"et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extended the approach by using multiple sources, requiring uniform POS and dependency representations (McDonald et al., 2013). Both model transfer and annotation projection rely on a large number of presumptions to derive their compet"
E17-1022,P13-2109,0,0.0770734,"Missing"
E17-1022,D11-1006,0,0.500127,"y few parameters and is distinctly robust to domain change across languages. 1 Introduction Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns, which would not be an error if the test data used a DP analysis (Abney, 1987). Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences. The Universal Dependencies (UD) project (Nivre et al., 2015; Nivre et al., 2016) offers a dependency formalism that aims at providing a consistent representation across languages, while enforcing a few hard constraints. The arrival of such treebanks, expanded and improved on a regular basis, provides a new milestone for crosslingual dependency parsing research (McDonald et al., 2013). Contributions We introduce, to the best of our knowledge, the first unsupervised rule-based dependency parser for Universal Dependencies"
E17-1022,W10-2105,1,0.869888,"Missing"
E17-1022,D15-1039,0,0.0523361,"ereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extended the approach by using multiple sources, requiring uniform POS and dependency representations (McDonald et al., 2013). Both model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and"
E17-1022,N10-1116,0,0.0314682,"hey typically do not exploit constraints placed on linguistic structures through a formalism, and they do so by design. With the emergence of UD as the practical standard for multilingual POS and syntactic dependency annotation, we argue for an approach that takes a fresh angle on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et"
E17-1022,D10-1120,0,0.0928531,"on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource"
E17-1022,W10-2902,0,0.029288,"hey typically do not exploit constraints placed on linguistic structures through a formalism, and they do so by design. With the emergence of UD as the practical standard for multilingual POS and syntactic dependency annotation, we argue for an approach that takes a fresh angle on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et"
E17-1022,W12-1910,1,0.850332,"ed on the fly at runtime. We refer henceforth to our UD parser as UDP. 231 3.1 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: PageRank setup Our system uses the PageRank (PR) algorithm (Page et al., 1999) to estimate the relevance of the content words of a sentence. PR uses a random walk to estimate which nodes in the graph are more likely to be visited often, and thus, it gives higher rank to nodes with more incoming edges, as well as to nodes connected to those. Using PR to score word relevance requires an effective graphbuilding strategy. We have experimented with the strategies by Søgaard (2012b), such as words being connected to adjacent words, but our system fares best strictly using the dependency rules in Table 1 to build the graph. UD trees are often very flat, and a highly connected graph yields a PR distribution that is closer to uniform, thereby removing some of the difference of word relevance. We build a multigraph of all words in the sentence covered by the head-dependent rules in Table 1, giving each word an incoming edge for each eligible dependent, i.e., ADV depends on ADJ and VERB . This strategy does not always yield connected graphs, and we use a teleport probabilit"
E17-1022,C14-1175,0,0.0824867,"Missing"
E17-1022,H01-1035,0,0.0694204,"on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 20"
E17-1022,I08-3008,0,0.275845,"s. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extended the approach by using multiple sources, requiring uniform POS and dependency representations (McDonald et al., 2013). Both model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and large, these presumptions are unrealistic and exclusive to a group of very closely related, resource-rich IndoEuropean lan"
E17-1022,Q16-1022,1,\N,Missing
E17-1028,S15-1016,0,0.0421837,"of each file) and the previous principle. Our pre-processing step leads to corpora with bracketed files representing directly the RST trees (as in Figure 1) with stand-off annotation of the text of the EDUs. Note that, even if harmonized, the corpora are not parallel, making it hard to use them to study 10 12 http://ixa2.si.ehu.es/diskurtsoa/en/ https://www.sfu.ca/˜mtaboada/ research/SFU_Review_Corpus.html Recall that in the original format, the relation is not annotated on the parent node but on the children. 13 S being a satellite, N a nucleus and R a relation. 11 295 discussed (Roze, 2013; Benamara and Taboada, 2015). We decided on the following mapping, considering the properties of the relations and the classes: parenthetical – used to give “additional details” – is mapped to E LABORATION, conjunction – similar to a list with only two elements – to J OINT, justify – similar to Explanationargumentative – and motivation – quite similar to reason and grouped with evidence in (Benamara and Taboada, 2015) – to E XPLANATION, preparation – presenting preliminary information, increasing the readiness to read the nucleus – to BACK GROUND , and unconditional and unless – linked to condition – to C ONDITION. Final"
E17-1028,D15-1263,0,0.136735,"which texts are analyzed as constituency trees, such as the one in Figure 1. This theory guided the annotation of the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English, from which several textlevel discourse parsers have been induced (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Li et al., 2014; Ji and Eisenstein, 2014). Such parsers have proven to be useful for various downstream applications (Daum´e III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015). There are discourse treebanks for other languages than English, including Spanish, German, Basque, Dutch, and Brazilian Portuguese. However, most research experimenting with these languages has focused on rule-based systems (Pardo and Nunes, 2008; Maziero et al., 2011) or has been limited to intra-sentential relations (Maziero et al., 2015). Moreover, all discourse corpora are limited in size, since annotation is complex and time consuming. This data sparsity makes learning hard, especially considering that discourse parsing involves several complex and interacting factors, ranging from synt"
E17-1028,D15-1262,1,0.852758,"n the development set of the target language. Setting (1) provides performance when no data are available at all in the target language, while (2) aims at evaluating if one can expect improvements by simply combining all the available data. When combining the corpora, we cannot ignore lexical information as it has been done for syntactic parsing with delexicalized models (McDonald et al., 2011). Discourse parsing is a semantic task, at least when it comes to predict a rhetorical relation between two spans of text, and information from words have proven to be crucial (Rutherford and Xue, 2014; Braud and Denis, 2015). We thus include word features using bilingual dictionaries – i.e. translating the words used as features into a single language (English) –, or through crosslingual word embeddings as proposed in (Guo et al., 2015) for dependency parsing. More precisely, we used the cross-lingual word representations presented in (Levy et al., 2017) that allow multi-source learning and have proven useful for POS tagging but also more semantic-oriented tasks, such as dependency parsing and document classification. tains the unparsed DUs. A parsing configuration is a couple hS, Bi. In the initial configuration"
E17-1028,N04-1024,0,0.0581995,"ical Structure Theory (RST) (Mann and Thompson, 1988) is a prominent linguistic theory of discourse structures, in which texts are analyzed as constituency trees, such as the one in Figure 1. This theory guided the annotation of the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English, from which several textlevel discourse parsers have been induced (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Li et al., 2014; Ji and Eisenstein, 2014). Such parsers have proven to be useful for various downstream applications (Daum´e III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015). There are discourse treebanks for other languages than English, including Spanish, German, Basque, Dutch, and Brazilian Portuguese. However, most research experimenting with these languages has focused on rule-based systems (Pardo and Nunes, 2008; Maziero et al., 2011) or has been limited to intra-sentential relations (Maziero et al., 2015). Moreover, all discourse corpora are limited in size, since annotation is complex and time consuming. This data sparsity makes learning hard"
E17-1028,W01-1605,0,0.948583,"sh, (b) a harmonization of discourse treebanks across languages, enabling us to present (c) what to the best of our knowledge are the first experiments on crosslingual discourse parsing. 1 NN-C OMPARISON 1 3 2 Figure 1: Tree for the structure covering the segments 1 to 3 in document 1384 in the English RST Discourse Treebank. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is a prominent linguistic theory of discourse structures, in which texts are analyzed as constituency trees, such as the one in Figure 1. This theory guided the annotation of the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English, from which several textlevel discourse parsers have been induced (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Li et al., 2014; Ji and Eisenstein, 2014). Such parsers have proven to be useful for various downstream applications (Daum´e III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015). There are discourse treebanks for other languages than English, including Spanish, German, Basque, Dutch, and Brazilian Portuguese. However, most rese"
E17-1028,P82-1020,0,0.854695,"Missing"
E17-1028,D14-1082,0,0.0478971,"Missing"
E17-1028,P16-1017,1,0.906824,"Missing"
E17-1028,P14-1002,0,0.679219,"ing. 1 NN-C OMPARISON 1 3 2 Figure 1: Tree for the structure covering the segments 1 to 3 in document 1384 in the English RST Discourse Treebank. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is a prominent linguistic theory of discourse structures, in which texts are analyzed as constituency trees, such as the one in Figure 1. This theory guided the annotation of the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English, from which several textlevel discourse parsers have been induced (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Li et al., 2014; Ji and Eisenstein, 2014). Such parsers have proven to be useful for various downstream applications (Daum´e III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015). There are discourse treebanks for other languages than English, including Spanish, German, Basque, Dutch, and Brazilian Portuguese. However, most research experimenting with these languages has focused on rule-based systems (Pardo and Nunes, 2008; Maziero et al., 2011) or has been limited to intra-sentential relations (Maziero et al."
E17-1028,D12-1083,0,0.136783,"Missing"
E17-1028,P13-1048,0,0.401067,"Missing"
E17-1028,W11-0401,0,0.351318,"Missing"
E17-1028,E17-1072,1,0.867822,"parsing with delexicalized models (McDonald et al., 2011). Discourse parsing is a semantic task, at least when it comes to predict a rhetorical relation between two spans of text, and information from words have proven to be crucial (Rutherford and Xue, 2014; Braud and Denis, 2015). We thus include word features using bilingual dictionaries – i.e. translating the words used as features into a single language (English) –, or through crosslingual word embeddings as proposed in (Guo et al., 2015) for dependency parsing. More precisely, we used the cross-lingual word representations presented in (Levy et al., 2017) that allow multi-source learning and have proven useful for POS tagging but also more semantic-oriented tasks, such as dependency parsing and document classification. tains the unparsed DUs. A parsing configuration is a couple hS, Bi. In the initial configuration, S is empty and B contains the whole document. The parser iteratively applies actions to the current configuration, in order to derive new configurations until it reaches a final state, i.e. a parsing configuration where B is empty and S contains a single element (the root of the tree). The actions are defined as follows: • S HIFT po"
E17-1028,D14-1220,0,0.791819,"al discourse parsing. 1 NN-C OMPARISON 1 3 2 Figure 1: Tree for the structure covering the segments 1 to 3 in document 1384 in the English RST Discourse Treebank. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is a prominent linguistic theory of discourse structures, in which texts are analyzed as constituency trees, such as the one in Figure 1. This theory guided the annotation of the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English, from which several textlevel discourse parsers have been induced (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Li et al., 2014; Ji and Eisenstein, 2014). Such parsers have proven to be useful for various downstream applications (Daum´e III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015). There are discourse treebanks for other languages than English, including Spanish, German, Basque, Dutch, and Brazilian Portuguese. However, most research experimenting with these languages has focused on rule-based systems (Pardo and Nunes, 2008; Maziero et al., 2011) or has been limited to intra-sentential"
E17-1028,W10-4327,0,0.0437059,"urse structures, in which texts are analyzed as constituency trees, such as the one in Figure 1. This theory guided the annotation of the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English, from which several textlevel discourse parsers have been induced (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Li et al., 2014; Ji and Eisenstein, 2014). Such parsers have proven to be useful for various downstream applications (Daum´e III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015). There are discourse treebanks for other languages than English, including Spanish, German, Basque, Dutch, and Brazilian Portuguese. However, most research experimenting with these languages has focused on rule-based systems (Pardo and Nunes, 2008; Maziero et al., 2011) or has been limited to intra-sentential relations (Maziero et al., 2015). Moreover, all discourse corpora are limited in size, since annotation is complex and time consuming. This data sparsity makes learning hard, especially considering that discourse parsing involves several complex and interacting fact"
E17-1028,P12-1007,0,0.0405559,"al models (CRF) for the intra- and the inter-sentential levels. These models jointly learn the relation and the structure, and a CKY-like algorithm is used to find the optimal tree. Feng and Hirst (2014) use CRFs only as local models for the inter- and intra-sententials levels. For Brazilian Portuguese, for example, the first system, called DiZer (Pardo and Nunes, 2008; Maziero et al., 2011), was also rule-based, but there has been some work on using classification of intra-sentential relations (Maziero et al., 2015). Recently studies have focused on building good representations of the data. Feng and Hirst (2012) introduced linguistic features, mostly syntactic and contextual ones. Li et al. (2014) used a recursive neural network that builds a representation for each clause based on the syntactic tree, and then apply two classifiers as in Hernault et al. (2010). This leads to the best performing system for unlabeled structure (85.0 in F1 ). The system presented by Ji and Eisenstein (2014, DPLP) jointly learns the representation and the task: a large marNuclearity A DU is either a nucleus or a satellite, the nucleus being the most important part of the relation (i.e. of the text), while the satellite c"
E17-1028,P14-1048,0,0.676212,"eriments on crosslingual discourse parsing. 1 NN-C OMPARISON 1 3 2 Figure 1: Tree for the structure covering the segments 1 to 3 in document 1384 in the English RST Discourse Treebank. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is a prominent linguistic theory of discourse structures, in which texts are analyzed as constituency trees, such as the one in Figure 1. This theory guided the annotation of the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English, from which several textlevel discourse parsers have been induced (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Li et al., 2014; Ji and Eisenstein, 2014). Such parsers have proven to be useful for various downstream applications (Daum´e III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015). There are discourse treebanks for other languages than English, including Spanish, German, Basque, Dutch, and Brazilian Portuguese. However, most research experimenting with these languages has focused on rule-based systems (Pardo and Nunes, 2008; Maziero et al., 2011) or has been limited to"
E17-1028,W97-0713,0,0.738306,"ellite-nucleus depending on the relative order of the spans), or multi-nuclear. Some relations can be either monoor multi-nuclear, such as consequence or evaluation in the RST-DT. Binary trees In the original RST framework, each relation is associated with an application scheme that defines the nuclearity of the DUs (mono- or multi-nuclear relation), and the number of DUs linked. Among the six schemes, two correspond to a link between more than two DUs, either a nucleus shared between two mono-nuclear relations (e.g. motivation and enablement) or a relation linking several nuclei (e.g. list). Marcu (1997) proposed to simplify the representation to 293 Corpus #Doc #Trees #Words #Rel #Lab #EDU max/min/avg #CDU En-DT Pt-DT Es-DTa De-DT Nl-DT Eu-DT 385 330 266 174 80 88 385 329 266 173 80 85 206, 300 135, 820 69, 787 32, 274 27, 920 27, 982 56 32 29 30 31 31 110 58 43 46 51 50 21,789 12,573 4,019 2,790 2,345 2,396 304/2/56.6 187/3/38.2 77/2/11.5 24/10/16.1 47/14/29.3 68/3/28.2 21,404 12,244 3,671 2,617 2,265 2,311 Table 1: Number of documents (#Doc), trees (#Trees, less than #Doc when we were unable to parse a document, see Section 4.2), words (#Words, see Section 6), relations (#Rel, originally),"
E17-1028,P15-1119,0,0.0281023,"the available data. When combining the corpora, we cannot ignore lexical information as it has been done for syntactic parsing with delexicalized models (McDonald et al., 2011). Discourse parsing is a semantic task, at least when it comes to predict a rhetorical relation between two spans of text, and information from words have proven to be crucial (Rutherford and Xue, 2014; Braud and Denis, 2015). We thus include word features using bilingual dictionaries – i.e. translating the words used as features into a single language (English) –, or through crosslingual word embeddings as proposed in (Guo et al., 2015) for dependency parsing. More precisely, we used the cross-lingual word representations presented in (Levy et al., 2017) that allow multi-source learning and have proven useful for POS tagging but also more semantic-oriented tasks, such as dependency parsing and document classification. tains the unparsed DUs. A parsing configuration is a couple hS, Bi. In the initial configuration, S is empty and B contains the whole document. The parser iteratively applies actions to the current configuration, in order to derive new configurations until it reaches a final state, i.e. a parsing configuration"
E17-1028,J00-3005,0,0.91226,"n building a discourse structure, the text is first segmented into elementary discourse units (EDU), mostly clauses. EDUs are the smallest discourse units (DUs). Discourse relations are then used to build DUs, recursively. A non-elementary DU is called a complex discourse unit (CDU). The structure of a document is the set of linked DUs. In this paper, we focus on the Rhetorical Structure Theory (RST), a theoretical framework proposed by Mann and Thompson (1988). Related Work The first text-level discourse parsers were developed for English, relying mainly on hand-crafted rules and heuristics (Marcu, 2000a; Carlson et al., 2001). Hernault et al. (2010, HILDA) greedily use SVM classifiers to make attachment and labeling decisions, building up a discourse tree. Joty et al. (2012, TSP) build a two-stage parsing system, training separate sequential models (CRF) for the intra- and the inter-sentential levels. These models jointly learn the relation and the structure, and a CKY-like algorithm is used to find the optimal tree. Feng and Hirst (2014) use CRFs only as local models for the inter- and intra-sententials levels. For Brazilian Portuguese, for example, the first system, called DiZer (Pardo an"
E17-1028,D11-1006,0,0.0244819,"arget and the others as sources. More precisely, we will evaluate two settings: (1) training and optimizing only on the available source data; (2) training on all available data, including target ones if any, and optimizing on the development set of the target language. Setting (1) provides performance when no data are available at all in the target language, while (2) aims at evaluating if one can expect improvements by simply combining all the available data. When combining the corpora, we cannot ignore lexical information as it has been done for syntactic parsing with delexicalized models (McDonald et al., 2011). Discourse parsing is a semantic task, at least when it comes to predict a rhetorical relation between two spans of text, and information from words have proven to be crucial (Rutherford and Xue, 2014; Braud and Denis, 2015). We thus include word features using bilingual dictionaries – i.e. translating the words used as features into a single language (English) –, or through crosslingual word embeddings as proposed in (Guo et al., 2015) for dependency parsing. More precisely, we used the cross-lingual word representations presented in (Levy et al., 2017) that allow multi-source learning and h"
E17-1028,P09-1077,0,0.172059,"en of the two CDUs on the top of the stack, and adding as a feature the representation built for these two CDUs lead to important improvements. log P (a(i) |c(i) ; θ) i=1 where θ is the set of all parameters, including embedding matrices. We optimized this objective with the averaged stochastic gradient descent algorithm (Polyak and Juditsky, 1992). At inference time, we used beamsearch to find the best-scoring tree. 5.2 Features Cross-lingual Discourse Parsing Lexical features We use the first three words and the last word along with their POS, features that have proven useful for discourse (Pitler et al., 2009), and the words in the head set (Sagae, 2009) Our first experiments are strictly monolingual, and they are intended to give state-of-the-art performance in a fully supervised setting. We consider 297 – i.e. words whose head in the dependency graph is not in the EDU –, here limited to the first three.16 This head set contains the head of the sentence (in general, the main event), or words linked to the main clause when the segment does not contain the head (especially, discourse connectives that are subordinating or coordinating conjunctions could be found there). The words are the boundaries c"
E17-1028,redeker-etal-2012-multi,0,0.401472,"Missing"
E17-1028,E14-1068,0,0.0268096,"s if any, and optimizing on the development set of the target language. Setting (1) provides performance when no data are available at all in the target language, while (2) aims at evaluating if one can expect improvements by simply combining all the available data. When combining the corpora, we cannot ignore lexical information as it has been done for syntactic parsing with delexicalized models (McDonald et al., 2011). Discourse parsing is a semantic task, at least when it comes to predict a rhetorical relation between two spans of text, and information from words have proven to be crucial (Rutherford and Xue, 2014; Braud and Denis, 2015). We thus include word features using bilingual dictionaries – i.e. translating the words used as features into a single language (English) –, or through crosslingual word embeddings as proposed in (Guo et al., 2015) for dependency parsing. More precisely, we used the cross-lingual word representations presented in (Levy et al., 2017) that allow multi-source learning and have proven useful for POS tagging but also more semantic-oriented tasks, such as dependency parsing and document classification. tains the unparsed DUs. A parsing configuration is a couple hS, Bi. In t"
E17-1028,W05-1513,0,0.0416354,"ocus on the harder step of tree building. 5.1 Description of the Parser We used the syntactic parser described in Coavoux and Crabb´e (2016), in the static oracle setting. We chose this parser because it can take pre-trained embeddings as input and, more importantly, because it was designed for morphologically rich languages and thus takes as input not only tokens and POS tags, but any token attribute that is then mapped to a real-valued vector, which allows the use of complex features. The parser is a transition-based constituent parser that uses a lexicalized shift-reduce transition system (Sagae and Lavie, 2005). The transition system is based on two data structures – a stack (S) stores partial trees and a queue (B) con14 The full mapping is provided in Appendix A. http://www.sfu.ca/rst/01intro/ definitions.html 15 296 that we need at least 100 documents to build a monolingual model, since we already keep around 65 documents for test and development. We then evaluate multi-source transfer methods, considering one language as the target and the others as sources. More precisely, we will evaluate two settings: (1) training and optimizing only on the available source data; (2) training on all available"
E17-1028,W09-3813,0,0.583276,"x layer. To generate a set of training examples {a(i) , c(i) }N i=1 , we used the static oracle to extract the gold sequence of actions and configurations for each tree in the corpus. The objective function of the parser is the negative log-likelihood of gold actions given corresponding configurations: L(θ) = − N X 5.3 As in previous studies, we used features representing the two EDUs on the top of the stack and the EDU on the queue. If the stack contains CDUs, we use the nuclearity principle to choose the head EDU, converting multi-nuclear relations into nucleus-satellite ones as done since (Sagae, 2009). However, we found that using these information also for the left and right children of the two CDUs on the top of the stack, and adding as a feature the representation built for these two CDUs lead to important improvements. log P (a(i) |c(i) ; θ) i=1 where θ is the set of all parameters, including embedding matrices. We optimized this objective with the averaged stochastic gradient descent algorithm (Polyak and Juditsky, 1992). At inference time, we used beamsearch to find the best-scoring tree. 5.2 Features Cross-lingual Discourse Parsing Lexical features We use the first three words and t"
E17-1028,N03-1030,0,0.51686,", 2009). 4.2 Binarization All the corpora contain non-binary trees that we map to binary ones. In the En-DT, common cases of non-binarity are nodes whose daughters all hold the same multi-nuclear relation – indicating that this relation spans multiple DUs, e.g. list.12 In rare cases, the children are two satellites and a nucleus – indicating that the nucleus is shared by the satellites. These configurations are the ones described in (Marcu, 1997) (see Section 3), and choosing right or left-branching leads to a similar interpretation. For the En-DT, rightbranching is the chosen strategy since (Soricut and Marcu, 2003). We found more diverse cases in the other corpora, and, for some of them, right-branching is impossible. It is the case when the daughters are one nucleus (annotated with “Span”, only indicating that this node spans several EDUs) and more than two satellites holding different relations – i.e. the nucleus is shared by all the relations. More precisely, the issue arises when the last two children are satellites. Using right-branching, we end with a node with two satellites as daughters, and thus a ill-formed tree. In order to keep as often as possible the “right-branching by default” strategy,"
E17-1028,H05-1033,0,0.112524,"son, 1988) is a prominent linguistic theory of discourse structures, in which texts are analyzed as constituency trees, such as the one in Figure 1. This theory guided the annotation of the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English, from which several textlevel discourse parsers have been induced (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Li et al., 2014; Ji and Eisenstein, 2014). Such parsers have proven to be useful for various downstream applications (Daum´e III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015). There are discourse treebanks for other languages than English, including Spanish, German, Basque, Dutch, and Brazilian Portuguese. However, most research experimenting with these languages has focused on rule-based systems (Pardo and Nunes, 2008; Maziero et al., 2011) or has been limited to intra-sentential relations (Maziero et al., 2015). Moreover, all discourse corpora are limited in size, since annotation is complex and time consuming. This data sparsity makes learning hard, especially considering that discourse parsing i"
E17-1028,stede-neumann-2014-potsdam,0,0.363408,"Missing"
E17-1028,W04-0213,0,0.164241,"Missing"
E17-1028,W12-5607,0,0.0600095,"Missing"
E17-1028,N09-1064,0,0.272392,"Missing"
E17-1028,W04-1009,0,0.08038,"Missing"
E17-1072,Q16-1031,0,0.026374,".3869 .4119 .1364 .2408 .1280 .1877 .1403 .1791 .2299 .2759 .2207 .2598 0.2830 0 Multilingual SID-SGNS .4433 .4632 .4893 .5015 .4047 .4151 .4091 .4302 .2989 .3049 .2514 .2753 .2737 .3195 .2404 .2945 .3304 .3893 .3509 .3868 .4058 .4376 .1605 .3082 .1591 .2584 .1448 .2403 .2482 .3372 .2437 .3080 0.3289 22 Table 3: The performance of SID-SGNS compared to state-of-the-art cross-lingual embedding methods and traditional alignment methods. advantage, SID-SGNS performs significantly better than the other methods combined.6 This result is similar in vein to recent findings in the parsing literature (Ammar et al., 2016; Guo et al., 2016), where multi-lingual transfer was shown to improve upon bilingual transfer. In absolute terms, Multilingual SID-SGNS’s performance is still very low. However, this experiment demonstrates that one way of making significant improvement in cross-lingual embeddings is by considering additional sources of information, such as the multi-lingual signal demonstrated here. We hypothesize that, regardless of the algorithmic approach, relying solely on sentence IDs from bilingual parallel corpora will probably not be able to improve much beyond IBM Model-1. 6 Data Paradigms In §2, we"
E17-1072,J93-2003,0,0.156922,"oss-lingual similarity. Another important delineation of this work is that we focus on algorithms that rely on sentence-aligned data; in part, because these algorithms are particularly interesting for low-resource languages, but also to make our analysis and comparison with alignment algorithms more focused. We observe that the top performing embedding algorithms share the same underlying feature space – sentence IDs – while their different algorithmic approaches seem to have a negligible impact on performance. We also notice that several statistical alignment algorithms, such as IBM Model-1 (Brown et al., 1993), operate under the same data assumptions. Specifically, we find that using the translation probabilities learnt by Model-1 as the cross-lingual similarity function (in place of the commonly-used cosine similarity between word embeddings) performs on-par with state-of-the-art cross-lingual embeddings on word alignment and bilingual dictionary induction tasks. In other words, as long as the similarity function is based on the sentence ID feature space and the embedding/alignment algorithm itself is not too na¨ıve, the actual difference in performance between different approaches is marginal. Th"
E17-1072,E14-1049,0,0.0924706,"nce. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account. 1 Introduction Cross-lingual word embedding algorithms try to represent the vocabularies of two or more languages in one common continuous vector space. These vectors can be used to improve monolingual word similarity (Faruqui and Dyer, 2014) or support cross-lingual transfer (Gouws and Søgaard, 2015). In this work, we focus on the second (cross-lingual) aspect of these embeddings, and try to determine what makes some embedding approaches better than others on a set of ∗ These authors contributed equally to this work. 765 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 765–774, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics generalize it, resulting in an embedding model that is as effective as Model-1 and o"
E17-1072,N15-1157,1,0.39673,"els between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account. 1 Introduction Cross-lingual word embedding algorithms try to represent the vocabularies of two or more languages in one common continuous vector space. These vectors can be used to improve monolingual word similarity (Faruqui and Dyer, 2014) or support cross-lingual transfer (Gouws and Søgaard, 2015). In this work, we focus on the second (cross-lingual) aspect of these embeddings, and try to determine what makes some embedding approaches better than others on a set of ∗ These authors contributed equally to this work. 765 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 765–774, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics generalize it, resulting in an embedding model that is as effective as Model-1 and other sophisticated state-of-the-art embedding methods, but t"
E17-1072,graca-etal-2008-building,0,0.0204464,"nt-aligned data, and can be built in a similar manner: create a pseudo-bilingual sentence from each aligned sentence, and for each word in question, consider all the other words in this sentence as its features. BilBOWA (Gouws et al., 2015) also uses a similar set of features, but restricts the source language words to those that appeared within a certain distance Benchmarks We measure the quality of each embedding using both manually annotated word alignment datasets and bilingual dictionaries. We use 16 manually annotated word alignment datasets – Hansards3 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) – as well as 16 bilingual dictionaries from Wiktionary. 2 homepages.inf.ed.ac.uk/s0787820/ bible/ 3 www.isi.edu/natural-language/ download/hansard/ 767 tuning hyperparameters for truly low-resource languages. In the word alignment benchmark, each word in a given source language sentence is aligned with the most similar target language word from the target language sentence – this is exactly the same greedy decoding algorithm that is implemented in IBM Model-1 (Brown et al., 1993). If a sour"
E17-1072,J03-1002,0,0.116258,"similarity function (in place of the commonly-used cosine similarity between word embeddings) performs on-par with state-of-the-art cross-lingual embeddings on word alignment and bilingual dictionary induction tasks. In other words, as long as the similarity function is based on the sentence ID feature space and the embedding/alignment algorithm itself is not too na¨ıve, the actual difference in performance between different approaches is marginal. This leads us to revisit another statistical alignment algorithm from the literature that uses the same sentence-based signal – the Dice aligner (Och and Ney, 2003). We first observe that the vanilla Dice aligner is significantly outperformed by the Model-1 aligner. We then recast Dice as the dot-product between two word vectors (based on the sentence ID feature space), which allows us to While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remain vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment alg"
E17-1072,P15-1165,1,0.0916604,"Missing"
E17-1072,W11-4615,0,0.0169403,"bilingual sentence from each aligned sentence, and for each word in question, consider all the other words in this sentence as its features. BilBOWA (Gouws et al., 2015) also uses a similar set of features, but restricts the source language words to those that appeared within a certain distance Benchmarks We measure the quality of each embedding using both manually annotated word alignment datasets and bilingual dictionaries. We use 16 manually annotated word alignment datasets – Hansards3 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) – as well as 16 bilingual dictionaries from Wiktionary. 2 homepages.inf.ed.ac.uk/s0787820/ bible/ 3 www.isi.edu/natural-language/ download/hansard/ 767 tuning hyperparameters for truly low-resource languages. In the word alignment benchmark, each word in a given source language sentence is aligned with the most similar target language word from the target language sentence – this is exactly the same greedy decoding algorithm that is implemented in IBM Model-1 (Brown et al., 1993). If a source language word is out of vocabulary, it is not aligned with anything, whereas ta"
E17-1072,C12-1089,0,0.0182685,"show that a generalization of one of these, the Dice aligner, is a very strong baseline for crosslingual word embedding algorithms, performing better than several state-of-the-art algorithms, especially when exploiting a multi-lingual signal. Our code and data are publicly available.1 Previous approaches to cross-lingual word embeddings can be divided into three categories, according to assumptions on the training data. The first category assumes word-level alignments, in the form of bilingual dictionaries (Mikolov et al., 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014). Sizable bilingual dictionaries are not available for many language pairs, and the quality of automatic word alignment greatly affects the quality of the embeddings. It is also unclear whether the embedding process provides significant added value beyond the initial word alignments (Zou et al., 2013). We Algorithms that rely on sentence-aligned data typically create intermediate sentence representations from each sentence’s constituent words. Hermann and Blunsom (2014) proposed a deep neural model, BiCVM, which compared the two sentence representatio"
E17-1072,2005.mtsummit-papers.11,0,0.0150526,"Missing"
E17-1072,J10-4005,0,0.075475,"Missing"
E17-1072,P16-1157,0,0.0326699,"-based model over the same features. 1 bitbucket.org/omerlevy/xling_ embeddings We study which factors determine the success of cross-lingual word embedding algorithms 2 Background: Cross-lingual Embeddings 766 that use sentence-aligned data, and evaluate them against baselines from the statistical machine translation literature that incorporate the same data assumptions. We go on to generalize one of these, the Dice aligner, showing that one variant is a much stronger baseline for cross-lingual word embedding algorithms than standard baselines. Finally, we would like to point out the work of Upadhyay et al. (2016), who studied how different data assumptions affect embedding quality in both monolingual and cross-lingual tasks. Our work focuses on one specific data assumption (sentencelevel alignments) and only on cross-lingual usage. This more restricted setting allows us to: (a) compare embeddings to alignment algorithms, (b) decouple the feature space from the algorithm, and make a more specific observation about the contribution of each component to the end result. In that sense, our findings complement those of Upadhyay et al. (2016). from the word in question, and defines a slightly different inter"
E17-1072,W14-1613,0,0.0112876,"alignment algorithms that also rely on sentence ID signals. We show that a generalization of one of these, the Dice aligner, is a very strong baseline for crosslingual word embedding algorithms, performing better than several state-of-the-art algorithms, especially when exploiting a multi-lingual signal. Our code and data are publicly available.1 Previous approaches to cross-lingual word embeddings can be divided into three categories, according to assumptions on the training data. The first category assumes word-level alignments, in the form of bilingual dictionaries (Mikolov et al., 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014). Sizable bilingual dictionaries are not available for many language pairs, and the quality of automatic word alignment greatly affects the quality of the embeddings. It is also unclear whether the embedding process provides significant added value beyond the initial word alignments (Zou et al., 2013). We Algorithms that rely on sentence-aligned data typically create intermediate sentence representations from each sentence’s constituent words. Hermann and Blunsom (2014) proposed a deep"
E17-1072,Q15-1016,1,0.0780471,"Missing"
E17-1072,W15-1521,0,0.0257223,"oth source and target language sentences as the same intermediate sentence vector. Recently, a simpler model, BilBOWA (Gouws et al., 2015), showed similar performance without using a hidden sentencerepresentation layer, giving it a dramatic speed advantage over its predecessors. BilBOWA is essentially an extension of skip-grams with negative sampling (SGNS) (Mikolov et al., 2013b), which simultaneously optimizes each word’s similarity to its inter-lingual context (words that appeared in the aligned target language sentence) and its intra-lingual context (as in the original monolingual model). Luong et al. (2015) proposed a similar SGNS-based model over the same features. 1 bitbucket.org/omerlevy/xling_ embeddings We study which factors determine the success of cross-lingual word embedding algorithms 2 Background: Cross-lingual Embeddings 766 that use sentence-aligned data, and evaluate them against baselines from the statistical machine translation literature that incorporate the same data assumptions. We go on to generalize one of these, the Dice aligner, showing that one variant is a much stronger baseline for cross-lingual word embedding algorithms than standard baselines. Finally, we would like t"
E17-1072,D13-1141,0,0.065972,"on of one of these, the Dice aligner, is a very strong baseline for crosslingual word embedding algorithms, performing better than several state-of-the-art algorithms, especially when exploiting a multi-lingual signal. Our code and data are publicly available.1 Previous approaches to cross-lingual word embeddings can be divided into three categories, according to assumptions on the training data. The first category assumes word-level alignments, in the form of bilingual dictionaries (Mikolov et al., 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014). Sizable bilingual dictionaries are not available for many language pairs, and the quality of automatic word alignment greatly affects the quality of the embeddings. It is also unclear whether the embedding process provides significant added value beyond the initial word alignments (Zou et al., 2013). We Algorithms that rely on sentence-aligned data typically create intermediate sentence representations from each sentence’s constituent words. Hermann and Blunsom (2014) proposed a deep neural model, BiCVM, which compared the two sentence representations at the final la"
E17-1072,W03-0301,0,0.0358193,"ilar manner: create a pseudo-bilingual sentence from each aligned sentence, and for each word in question, consider all the other words in this sentence as its features. BilBOWA (Gouws et al., 2015) also uses a similar set of features, but restricts the source language words to those that appeared within a certain distance Benchmarks We measure the quality of each embedding using both manually annotated word alignment datasets and bilingual dictionaries. We use 16 manually annotated word alignment datasets – Hansards3 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) – as well as 16 bilingual dictionaries from Wiktionary. 2 homepages.inf.ed.ac.uk/s0787820/ bible/ 3 www.isi.edu/natural-language/ download/hansard/ 767 tuning hyperparameters for truly low-resource languages. In the word alignment benchmark, each word in a given source language sentence is aligned with the most similar target language word from the target language sentence – this is exactly the same greedy decoding algorithm that is implemented in IBM Model-1 (Brown et al., 1993). If a source language word is out of vocabulary, it is not al"
E17-1072,cakmak-etal-2012-word,0,\N,Missing
E17-2026,N15-1177,0,0.023236,"ration in the multi-task setup. The relative gains and losses from MTL over the single-task models (see Table 1) are presented in Figure 1, showing improvements in 40 out of 90 cases. We see that chunking and high-level semantic tagging generally contribute most to other tasks, while hyperlinks do not significantly improve any other task. On the receiving end, we see that multiword and hyperlink detection seem to profit most from several auxiliary tasks. Symbiotic relationships are formed, e.g., by POS and CCG-tagging, or MWE and compression. 8. MWE Detection (MWE) We use the Streusle corpus (Schneider and Smith, 2015) to learn to identify multi-word expressions (on my own, cope with). 9. Super-sense tagging (SEM) We use the standard splits for the Semcor dataset, predicting coarse-grained semantic types of nouns and verbs (super-senses). 10. Super-sense Tagging (STR) As for the MWE task, we use the Streusle corpus, jointly predicting brackets and coarse-grained semantic types of the multi-word expressions. 4 Number of training sentences. The number of labels. Type/token ratio in training data. Percentage of training words not in GloVe vectors. Entropy of theP label distribution. 2 1/2 ] , where ||X||F = ["
E17-2026,P16-2038,1,0.752942,"ts pushing state of the art in various tasks, but preliminary theoretical findings guarantee that multi-task learning works under various conditions. Some approaches to multi-task learning are, for example, known to work when the tasks share optimal hypothesis classes (Baxter, 2000) or are drawn from related sample generating distributions (BenDavid and Borberly, 2003). In NLP, multi-task learning typically involves very heterogeneous tasks. However, while great improvements have been reported (Luong et al., 2016; Klerke et al., 2016), results are also often mixed (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Mart´ınez Alonso and Plank, 2017), and theoretical guarantees no longer apply. The question what task relations guarantee gains or make gains likely in NLP remains open. ∗ 2 Related work In the context of structured prediction in NLP, there has been very little work on the conditions under which MTL works. Luong et al. (2016) suggest that it is important that the auxiliary data does not outsize the target data, while Benton et al. (2017) suggest that multi-task learning is particularly effective when we only have access to small amounts of target data. Mart´ınez Alonso and Plank (2017) prese"
E17-2026,P10-1130,0,0.0847229,"Missing"
E17-2026,E17-1015,0,0.0418864,"ever, while great improvements have been reported (Luong et al., 2016; Klerke et al., 2016), results are also often mixed (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Mart´ınez Alonso and Plank, 2017), and theoretical guarantees no longer apply. The question what task relations guarantee gains or make gains likely in NLP remains open. ∗ 2 Related work In the context of structured prediction in NLP, there has been very little work on the conditions under which MTL works. Luong et al. (2016) suggest that it is important that the auxiliary data does not outsize the target data, while Benton et al. (2017) suggest that multi-task learning is particularly effective when we only have access to small amounts of target data. Mart´ınez Alonso and Plank (2017) present a study on different task combinations with dedicated main and auxiliary tasks. Their findings suggest, among others, that success depends on how uniformly the auxiliary task labels are distributed. Mou et al. (2016) investigate multi-task learning and its relation to transfer learning, and under which conditions these work between a set of sentence classification tasks. Their main finding with respect to multi-task learning is that suc"
E17-2026,C16-1013,1,0.84782,"asks. In our MTL setup, a training step consists of uniformly drawing a training task, then sampling a random batch of 32 examples from the task’s training data. Every training step thus works on exactly one task, and optimizes the task-specific projection and the shared parameters using Adadelta. As already mentioned, we keep hyper-parameters fixed across single-task and multi-task settings, making our results only applicable to the scenario where one wants to know whether MTL works in the current parameter setting (Collobert and Weston, 2008; Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollman and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017). depends largely on “how similar in semantics the source and target datasets are”, and that it generally bears close resemblance to transfer learning in the effect it has on model performance. 3 Multi-task Learning While there are many approaches to multi-task learning, hard parameter sharing in deep neural networks (Caruana, 1993) has become extremely popular in recent years. Its greatest advantages over other methods include (i) that it is known to be an efficient regularizer, theoretically (Baxter, 2000), as well as in pra"
E17-2026,C16-1179,1,0.19216,"consists of uniformly drawing a training task, then sampling a random batch of 32 examples from the task’s training data. Every training step thus works on exactly one task, and optimizes the task-specific projection and the shared parameters using Adadelta. As already mentioned, we keep hyper-parameters fixed across single-task and multi-task settings, making our results only applicable to the scenario where one wants to know whether MTL works in the current parameter setting (Collobert and Weston, 2008; Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollman and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017). depends largely on “how similar in semantics the source and target datasets are”, and that it generally bears close resemblance to transfer learning in the effect it has on model performance. 3 Multi-task Learning While there are many approaches to multi-task learning, hard parameter sharing in deep neural networks (Caruana, 1993) has become extremely popular in recent years. Its greatest advantages over other methods include (i) that it is known to be an efficient regularizer, theoretically (Baxter, 2000), as well as in practice (Søgaard and Goldberg, 2016"
E17-2026,D13-1155,0,0.0215032,"Missing"
E17-2026,J07-3004,0,0.0123429,"s of the models to be identical, each task-specific model is still left with wiggle room to model heterogeneous tasks, but the expressivity is very limited, as evidenced by the inability of such networks to fit random noise (Søgaard and Goldberg, 2016). 3.1 3.2 Tasks In our experiments below, we consider the following ten NLP tasks, with one dataset for each task. Characteristics of the datasets that we use are summarized in Table 1. 1. CCG Tagging (CCG) is a sequence tagging problem that assigns a logical type to every token. We use the standard splits for CCG super-tagging from the CCGBank (Hockenmaier and Steedman, 2007). 2. Chunking (CHU) identifies continuous spans of tokens that form syntactic units such as noun phrases or verb phrases. We use the standard splits for syntactic chunking from the English Penn Treebank (Marcus et al., 1993). Models Recent work on multi-task learning of NLP models has focused on sequence labeling with recurrent neural networks (Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollman and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017), although sequence-to-sequence models have been shown to profit from MTL as 3. Sentence Compression (COM) We u"
E17-2026,N16-1179,1,0.525561,"le the induction of more robust models. The main driver has been empirical results pushing state of the art in various tasks, but preliminary theoretical findings guarantee that multi-task learning works under various conditions. Some approaches to multi-task learning are, for example, known to work when the tasks share optimal hypothesis classes (Baxter, 2000) or are drawn from related sample generating distributions (BenDavid and Borberly, 2003). In NLP, multi-task learning typically involves very heterogeneous tasks. However, while great improvements have been reported (Luong et al., 2016; Klerke et al., 2016), results are also often mixed (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Mart´ınez Alonso and Plank, 2017), and theoretical guarantees no longer apply. The question what task relations guarantee gains or make gains likely in NLP remains open. ∗ 2 Related work In the context of structured prediction in NLP, there has been very little work on the conditions under which MTL works. Luong et al. (2016) suggest that it is important that the auxiliary data does not outsize the target data, while Benton et al. (2017) suggest that multi-task learning is particularly effective when we onl"
E17-2026,J93-2004,0,0.0601855,"d Goldberg, 2016). 3.1 3.2 Tasks In our experiments below, we consider the following ten NLP tasks, with one dataset for each task. Characteristics of the datasets that we use are summarized in Table 1. 1. CCG Tagging (CCG) is a sequence tagging problem that assigns a logical type to every token. We use the standard splits for CCG super-tagging from the CCGBank (Hockenmaier and Steedman, 2007). 2. Chunking (CHU) identifies continuous spans of tokens that form syntactic units such as noun phrases or verb phrases. We use the standard splits for syntactic chunking from the English Penn Treebank (Marcus et al., 1993). Models Recent work on multi-task learning of NLP models has focused on sequence labeling with recurrent neural networks (Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollman and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017), although sequence-to-sequence models have been shown to profit from MTL as 3. Sentence Compression (COM) We use the publicly available subset of the Google Compression dataset (Filippova and Altun, 2013), which has token-level annotations of word deletions. 165 Task CCG CHU COM FNT POS HYP KEY MWE SEM STR Size 39,604 8,936 9,600 3,"
E17-2026,E17-1005,0,0.631185,"tasks, but preliminary theoretical findings guarantee that multi-task learning works under various conditions. Some approaches to multi-task learning are, for example, known to work when the tasks share optimal hypothesis classes (Baxter, 2000) or are drawn from related sample generating distributions (BenDavid and Borberly, 2003). In NLP, multi-task learning typically involves very heterogeneous tasks. However, while great improvements have been reported (Luong et al., 2016; Klerke et al., 2016), results are also often mixed (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Mart´ınez Alonso and Plank, 2017), and theoretical guarantees no longer apply. The question what task relations guarantee gains or make gains likely in NLP remains open. ∗ 2 Related work In the context of structured prediction in NLP, there has been very little work on the conditions under which MTL works. Luong et al. (2016) suggest that it is important that the auxiliary data does not outsize the target data, while Benton et al. (2017) suggest that multi-task learning is particularly effective when we only have access to small amounts of target data. Mart´ınez Alonso and Plank (2017) present a study on different task combin"
E17-2026,D16-1046,0,0.0386758,"t of structured prediction in NLP, there has been very little work on the conditions under which MTL works. Luong et al. (2016) suggest that it is important that the auxiliary data does not outsize the target data, while Benton et al. (2017) suggest that multi-task learning is particularly effective when we only have access to small amounts of target data. Mart´ınez Alonso and Plank (2017) present a study on different task combinations with dedicated main and auxiliary tasks. Their findings suggest, among others, that success depends on how uniformly the auxiliary task labels are distributed. Mou et al. (2016) investigate multi-task learning and its relation to transfer learning, and under which conditions these work between a set of sentence classification tasks. Their main finding with respect to multi-task learning is that success Both authors contributed to the paper in equal parts. 164 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 164–169, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics well (Luong et al., 2016). Our multi-task learning architecture is similar to the"
E17-2026,C16-1059,0,0.0676471,"raining step consists of uniformly drawing a training task, then sampling a random batch of 32 examples from the task’s training data. Every training step thus works on exactly one task, and optimizes the task-specific projection and the shared parameters using Adadelta. As already mentioned, we keep hyper-parameters fixed across single-task and multi-task settings, making our results only applicable to the scenario where one wants to know whether MTL works in the current parameter setting (Collobert and Weston, 2008; Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollman and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017). depends largely on “how similar in semantics the source and target datasets are”, and that it generally bears close resemblance to transfer learning in the effect it has on model performance. 3 Multi-task Learning While there are many approaches to multi-task learning, hard parameter sharing in deep neural networks (Caruana, 1993) has become extremely popular in recent years. Its greatest advantages over other methods include (i) that it is known to be an efficient regularizer, theoretically (Baxter, 2000), as well as in practice (Søgaar"
E17-2040,P15-2044,1,0.928871,"Missing"
E17-2040,Q16-1022,1,0.909051,"Missing"
E17-2040,petrov-etal-2012-universal,0,0.0285098,"We express the quality of predicted rankings using precision (P@1) and Kendall’s τb statistic (Knight, 1966). Data. We train and test our taggers on data from UD version 1.2 (Nivre et al., 2015). We intersect this collection with the dictionaries we make available for this experiment: 9 of the Wiktionaries come from Li et al. (2012), and we collect 16 new on top of that. Thus, we experiment with a total of 25 languages from the UD. We refer to the 9 languages of Li et al. (2012) as development languages. To make the Wiktionaries and the UD data compatible, we map all POS tags to the tagset by Petrov et al. (2012). We estimate the frequencies for the +freq variants of the soft metrics by using the multilingual Bible corpus by Christodouloupoulos and Steedman (2014) and the Watchtower corpus (Agi´c et al., 2016) combined. We translate the English Wiktionary from Li et al. (2012) by using bilingual dictionaries from Wiktionary to obtain Dtrans for 20 languages.3 precision(D, G) = ∑i=1 ∣D∣ ∣{Di ∩Gi }∣ ∣{t∈Di }∣ recall(D, G) = ∑i=1 ∣D∣ ∣{Di ∩Gi }∣ ∣{t∈Gi }∣ Namely, for each word wi covered by both D and G, we check how many tags Di and Gi intersect, and then use the intersection to estimate dictionary prec"
E17-2040,P13-2109,0,0.0384735,"Missing"
E17-2040,K15-1033,1,0.885938,"Missing"
E17-2040,P13-2017,0,0.0419349,"Missing"
E17-2040,P16-2067,1,0.880248,"Missing"
E17-2040,A00-1031,0,0.765935,"Missing"
E17-2040,N13-1014,0,0.131467,"Missing"
E17-2040,D13-1032,0,0.0602914,"Missing"
E17-2040,P15-2111,0,0.0278218,"Missing"
E17-2040,D12-1127,0,\N,Missing
fromreide-etal-2014-crowdsourcing,N13-1037,0,\N,Missing
fromreide-etal-2014-crowdsourcing,P11-1097,0,\N,Missing
fromreide-etal-2014-crowdsourcing,P11-1037,0,\N,Missing
fromreide-etal-2014-crowdsourcing,D11-1141,0,\N,Missing
fromreide-etal-2014-crowdsourcing,P10-1040,0,\N,Missing
fromreide-etal-2014-crowdsourcing,N13-1132,1,\N,Missing
fromreide-etal-2014-crowdsourcing,W10-0713,0,\N,Missing
hovy-etal-2014-pos,zeman-2008-reusable,0,\N,Missing
hovy-etal-2014-pos,J08-3001,0,\N,Missing
hovy-etal-2014-pos,I11-1100,0,\N,Missing
hovy-etal-2014-pos,N13-1037,0,\N,Missing
hovy-etal-2014-pos,R13-1026,0,\N,Missing
hovy-etal-2014-pos,petrov-etal-2012-universal,0,\N,Missing
hovy-etal-2014-pos,D11-1141,0,\N,Missing
hovy-etal-2014-pos,P11-2008,0,\N,Missing
hovy-etal-2014-pos,N13-1039,0,\N,Missing
I13-1132,W10-2102,0,0.0219285,"oups of functionally related words. Examples of categories are transition words (213), which is a nondisambiguated superset of the discourse markers, phrases that introduce examples (49), comparisons (66), and contrast (6). Numbers in parenthesis indicates how many words there are in each category. For each category we count the number of token occurrences and the number of types.2 Feature sets Below we describe our six groups of features. Previous studies have shown that most of these features are correlated with answer quality, see (Jeon et al., 2006; Zhou et al., 2012; Harper et al., 2008; Su et al., 2010; Aji and Agichtein, 2010). 4 Importance sampling Discourse We use the discourse marker disambiguation classifier of Pitler and Nenkova (2009) to identify discourse uses. We have features which count the number of times each discourse marker appears. The cQA sites contain abundant training data, but the sites are diverse and heteregoneous. We hypothesize that training our models on similar threads from different domains will improve our models considerably. We measure similarity with Length This group has four features that measure the length of the answer in tokens and sen2 The word lists are"
I13-1132,W10-0501,0,0.0257886,"lly related words. Examples of categories are transition words (213), which is a nondisambiguated superset of the discourse markers, phrases that introduce examples (49), comparisons (66), and contrast (6). Numbers in parenthesis indicates how many words there are in each category. For each category we count the number of token occurrences and the number of types.2 Feature sets Below we describe our six groups of features. Previous studies have shown that most of these features are correlated with answer quality, see (Jeon et al., 2006; Zhou et al., 2012; Harper et al., 2008; Su et al., 2010; Aji and Agichtein, 2010). 4 Importance sampling Discourse We use the discourse marker disambiguation classifier of Pitler and Nenkova (2009) to identify discourse uses. We have features which count the number of times each discourse marker appears. The cQA sites contain abundant training data, but the sites are diverse and heteregoneous. We hypothesize that training our models on similar threads from different domains will improve our models considerably. We measure similarity with Length This group has four features that measure the length of the answer in tokens and sen2 The word lists are distributed as a part of"
I13-1132,P09-2004,0,0.0129791,"iscourse markers, phrases that introduce examples (49), comparisons (66), and contrast (6). Numbers in parenthesis indicates how many words there are in each category. For each category we count the number of token occurrences and the number of types.2 Feature sets Below we describe our six groups of features. Previous studies have shown that most of these features are correlated with answer quality, see (Jeon et al., 2006; Zhou et al., 2012; Harper et al., 2008; Su et al., 2010; Aji and Agichtein, 2010). 4 Importance sampling Discourse We use the discourse marker disambiguation classifier of Pitler and Nenkova (2009) to identify discourse uses. We have features which count the number of times each discourse marker appears. The cQA sites contain abundant training data, but the sites are diverse and heteregoneous. We hypothesize that training our models on similar threads from different domains will improve our models considerably. We measure similarity with Length This group has four features that measure the length of the answer in tokens and sen2 The word lists are distributed as a part of the LightSIDE essay assessment software package found at http: //lightsidelabs.com/ 1 We use the August 2012 dump fr"
I13-1134,D11-1068,0,0.0403809,"Missing"
I13-1134,J07-4002,0,0.0564636,"Missing"
I13-1134,D11-1118,0,0.0466583,"Missing"
I13-1134,W04-1009,0,0.0234271,"Missing"
I13-1134,C12-1162,0,0.0221296,"Missing"
I13-1134,I11-1170,0,0.0349705,"Missing"
I13-1134,W10-4326,0,0.0412357,"Missing"
I13-1134,P11-1100,0,0.12548,"Missing"
I13-1134,W12-0117,0,0.0415051,"Missing"
I13-1134,P09-2004,0,0.299646,"nnectives follows a power law such that the majority of occurrences comes from relatively few but highly frequent connective types. If we do not take into account the uneven sizes of the categories, our performance figure ends up saying very little about how well we are doing on most of the connectives, because it is being dominated by the performance on a few high-frequency items. In this paper we look in more detail on the evaluation of the discourse connective disambiguation task, in particular how two commonly used feature models perform on individual discourse connectives. The models are Pitler and Nenkova (2009) (P&N), and its extension by Lin et al. (2010) (Lin). Motivated by our findings we advocate the use of macro-averaging as a necessary supplement to micro-averaging. Additionally, we perform our experiments in a more realistic setting where acccess to oracle gold-standard annotations is not assumed. The observed performance drop from oracle to predicted parses leads us to propose a new model, which approximates the syntactical information of the parse trees with part-of-speech tags. Although these features are less powerful in theory, the model has comparable macro-average performance in realis"
I13-1134,prasad-etal-2008-penn,0,0.221781,"Missing"
I13-1134,W12-3205,0,\N,Missing
K15-1011,Y00-1004,0,0.701769,"vertheless soon; presently; shortly a good deal; lots; very much M F F F F M M M F F Adjectives 6 5 4 fantastic; wondrous; wonderful inexpensive; cheap; economic amazing; awesome; marvelous tinny; bum; cheap happy best (quality) professional pretty easy; convenient; simple okay; o.k.; all right F M F M F M M F F M Table 6: Gender: equivalence classes in BabelNet 7 Related Work Sociolinguistic studies investigate the relation between a speaker’s linguistic choices and socio-economic variables. This includes regional origin (Schmidt and Herrgen, 2001; Nerbonne, 2003; Wieling et al., 2011), age (Barke, 2000; Barbieri, 2008; Rickford and Price, 2013), gender (Holmes, 1997; Rickford and Price, 2013), social class (Labov, 1964; Milroy and Milroy, 1992; Macaulay, 2001; Macaulay, 2002), and ethnicity (Carter, 2013; Rickford and Price, 2013). We focus on age and gender in this work. Corpus-based studies of variation have largely been conducted either by testing for the presence or absence of a set of pre-defined words (Pennebaker et al., 2001; Pennebaker et al., 2003), or by analysis of the unigram distribution (Barbieri, 2008). This approach restricts the findings to the phenomena defined in the hypo"
K15-1011,P13-2109,0,0.0221923,"Missing"
K15-1011,D13-1114,0,0.0287284,"However, Twitter is not well-suited for the study of syntactic variation for two reasons. First, the limited length of the posts compels the users to adopt a terse style that leaves out many grammatical markers. As a consequence, performance of syntactic parsers is prohibitive for linguistic analysis in this domain. Second, Twitter provides little meta-information about the users, except for regional origin and time of posting. Existing work has thus been restricted to these demographic variables. One line of research has focused on predictive models for age and gender (Alowibdi et al., 2013; Ciot et al., 2013) to add meta-data on Twitter, but again, error rates are too high for use in sociolinguistic hypothesis testing. We use a new source of data, namely the user Most computational sociolinguistics studies have focused on phonological and lexical variation. We present the first large-scale study of syntactic variation among demographic groups (age and gender) across several languages. We harvest data from online user-review sites and parse it with universal dependencies. We show that several age and gender-specific variations hold across languages, for example that women are more likely to use VP"
K15-1011,E14-1011,0,0.0624453,"phrase job advertisements in a gender-neutral way.1 While their tool addresses lexical variation, our results indicate that linguistic differences extend to the syntactic level. Previous work on demographic variation in both sociolinguistics and NLP has begun to rely on corpora from social media, most prominently Twitter. Twitter offers a sufficiently large data source with broad coverage (albeit limited to users with access to social media). Indeed, results show that this resource reflects the phonological and morpholexical variation of spoken language (Eisenstein, 2013b; Eisenstein, 2013a; Doyle, 2014). However, Twitter is not well-suited for the study of syntactic variation for two reasons. First, the limited length of the posts compels the users to adopt a terse style that leaves out many grammatical markers. As a consequence, performance of syntactic parsers is prohibitive for linguistic analysis in this domain. Second, Twitter provides little meta-information about the users, except for regional origin and time of posting. Existing work has thus been restricted to these demographic variables. One line of research has focused on predictive models for age and gender (Alowibdi et al., 2013"
K15-1011,P11-1137,0,0.0631603,"01; Macaulay, 2002), and ethnicity (Carter, 2013; Rickford and Price, 2013). We focus on age and gender in this work. Corpus-based studies of variation have largely been conducted either by testing for the presence or absence of a set of pre-defined words (Pennebaker et al., 2001; Pennebaker et al., 2003), or by analysis of the unigram distribution (Barbieri, 2008). This approach restricts the findings to the phenomena defined in the hypothesis, in this case the word list used. In contrast, our approach works beyond the lexical level, is data-driven and thus unconstrained by prior hypotheses. Eisenstein et al. (2011) use multi-output 109 Langs. BABEL N ET class bitual be, null genitive marking, etc. Our study is different from his in using full syntactic analyses, studying variation across age and gender rather than ethnicity, and in studying syntactic variation across several languages. Highest Adverbs 5 4 3 actually; really; in fact truly; genuinely; really however; nevertheless however merely; simply; just reasonably; moderately; fairly very in truth; really very; really; real very; really; real <35 <35 <35 <35 <35 <35 >45 <35 >45 <35 8 Syntax has been identified as an important factor in language vari"
K15-1011,W13-1102,0,0.0803442,"mpany Textio introduced a tool to help phrase job advertisements in a gender-neutral way.1 While their tool addresses lexical variation, our results indicate that linguistic differences extend to the syntactic level. Previous work on demographic variation in both sociolinguistics and NLP has begun to rely on corpora from social media, most prominently Twitter. Twitter offers a sufficiently large data source with broad coverage (albeit limited to users with access to social media). Indeed, results show that this resource reflects the phonological and morpholexical variation of spoken language (Eisenstein, 2013b; Eisenstein, 2013a; Doyle, 2014). However, Twitter is not well-suited for the study of syntactic variation for two reasons. First, the limited length of the posts compels the users to adopt a terse style that leaves out many grammatical markers. As a consequence, performance of syntactic parsers is prohibitive for linguistic analysis in this domain. Second, Twitter provides little meta-information about the users, except for regional origin and time of posting. Existing work has thus been restricted to these demographic variables. One line of research has focused on predictive models for age"
K15-1011,P12-1066,0,0.020615,"ial web data, say Twitter. Expected parse performance can be estimated from the SANCL 2012 shared task on dependency parsing of web data (Petrov and McDonald, 2012). The best result on the review domain there was 83.86 LAS and 88.31 UAS, close to the average over all web domains (83.45 LAS and 87.62 UAS). From the parses, we extract all subtrees of up to three tokens (treelets). We do not distinguish between right- and left-branching relations: the representation is basically a “bag of relations”. The purpose of this is to increase comparability across languages with different word orderings (Naseem et al., 2012). A onetoken treelet is simply the POS tag of the token, e.g. N OUN or V ERB. A two-token treelet is a typed relation between head and dependent, e.g. N SUBJ V ERB−−−−→N OUN. Treelets of three tokens have two possible structures. Either the head directly dominates two tokens, or the tokens are linked together in a chain, as shown below: nsubj dobj NOUN . . . VERB . . . NOUN 3.1 poss Second, we perform feature selection using L1 randomized logistic regression models, with age or gender as target variable, and the treelets as input features. However, direct feature selection with L1 regularized"
K15-1011,N13-1037,0,0.0153699,"mpany Textio introduced a tool to help phrase job advertisements in a gender-neutral way.1 While their tool addresses lexical variation, our results indicate that linguistic differences extend to the syntactic level. Previous work on demographic variation in both sociolinguistics and NLP has begun to rely on corpora from social media, most prominently Twitter. Twitter offers a sufficiently large data source with broad coverage (albeit limited to users with access to social media). Indeed, results show that this resource reflects the phonological and morpholexical variation of spoken language (Eisenstein, 2013b; Eisenstein, 2013a; Doyle, 2014). However, Twitter is not well-suited for the study of syntactic variation for two reasons. First, the limited length of the posts compels the users to adopt a terse style that leaves out many grammatical markers. As a consequence, performance of syntactic parsers is prohibitive for linguistic analysis in this domain. Second, Twitter provides little meta-information about the users, except for regional origin and time of posting. Existing work has thus been restricted to these demographic variables. One line of research has focused on predictive models for age"
K15-1011,P10-1023,0,0.00872615,"Highest Adverbs In German, it is mostly used to express comparisons 5 (1) in Ordnung (alright) (2) am Tag (on the day) 6 BABEL N ET class 4 Semantic variation within syntactic categories Given that a number of the indicative features are single treelets (POS tags), we wondered whether there are certain semantic categories that fill these slots. Since we work across several languages, we are looking for semantically equivalent classes. We collect the most significant adjectives and adverbs for each gender for each language and map the words to all of their possible lexical groups in BabelNet (Navigli and Ponzetto, 2010). This creates lexical equivalence classes. Table 6 shows the results. We purposefully exclude nouns and verbs here, as there is too much variation to detect any patterns. The number of languages that share lexical items from the same BabelNet class is typically smaller than the number of languages that share a treelet. Nevertheless, we observe certain patterns. The results for gender are presented in Table 6. For adverbs, the division seems to be about intensity: men use more downtoners (approximately; almost; still), while women use more intensifiers (actually; really; truly; quite; lots). T"
K15-1011,P15-2079,1,0.188287,"Missing"
K15-1011,P15-1073,1,0.673266,"Missing"
K15-1011,W15-4302,1,0.492093,"Missing"
K15-1011,E14-3004,0,0.0769129,"Missing"
K15-1011,D13-1187,0,0.366374,"Missing"
K15-1011,P13-2017,0,\N,Missing
K15-1033,E06-1040,0,0.0477065,"ng et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automatic measures such as labeled and u"
K15-1033,P02-1040,0,0.100561,"appropriate downstream tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automa"
K15-1033,W06-2920,0,0.0844944,"0 sentences for each of the 5 languages) annotated with human judgments for the preferred automatically parsed dependency tree, enabling further research in this direction. 2 UCP = LCP = We evaluate seven dependency parsing metrics, described in this section. Given a labeled gold tree G = hV, EG , lG (·)i and a labeled predicted tree P = hV, EP , lP (·)i, let E ⊂ V × V be the set of directed edges from dependents to heads, and let l : V × V → L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). LAS = 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. |{e |e ∈ EG ∩ EP }| |V | |{e |lG (e) = lP (e), e ∈ EG ∩ EP }| |V | LA = |{v |Vverb , cG (v) = cP (v) ∧ lG (v, ·) = lP (v, ·)}| |Vverb | For the"
K15-1033,W07-0718,0,0.188994,"e human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automatic measures such as labeled and unlabeled attachment scores is"
K15-1033,P11-1067,0,0.0697005,"Missing"
K15-1033,C96-1058,0,0.12707,"ly parsed dependency tree, enabling further research in this direction. 2 UCP = LCP = We evaluate seven dependency parsing metrics, described in this section. Given a labeled gold tree G = hV, EG , lG (·)i and a labeled predicted tree P = hV, EP , lP (·)i, let E ⊂ V × V be the set of directed edges from dependents to heads, and let l : V × V → L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). LAS = 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. |{e |e ∈ EG ∩ EP }| |V | |{e |lG (e) = lP (e), e ∈ EG ∩ EP }| |V | LA = |{v |Vverb , cG (v) = cP (v) ∧ lG (v, ·) = lP (v, ·)}| |Vverb | For the final figure of seven different parsing metrics, on top of the previous five, in our expe"
K15-1033,C12-1147,0,0.117595,"r NLP tasks. Also, inter-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. 315 Proceedings of the 19th Conference on Computationa"
K15-1033,N13-1070,1,0.846825,"ges: Croatian, Danish, English, German, and Spanish. For the human judgments, we asked professional linguists with dependency annotation experience to judge which of two parsers produced the better parse. Our stance here is that, insofar experts are able to annotate dependency trees, they are also able to determine the quality of a predicted syntactic structure, which we can in turn use to evaluate parser evaluation metrics. Even though downstream evaluation is critical in assessing the usefulness of parses, it also presents non-trivial challenges in choosing the appropriate downstream tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter,"
K15-1033,D11-1036,0,0.220614,"r-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. 315 Proceedings of the 19th Conference on Computational Language Learning, pag"
K15-1033,N13-1132,0,0.022444,"78 .437 .250* .469 .404 .230* .195* .297 .331 .232 .318 .323 .171 .223 .466 .453 .310 .501 .331 .120* .190* .540 .397 .467 .446 .405* .120* .143* .457 .425 .324* .448 .361* .126* .195* Table 4: Correlations between human judgments and metrics (micro avg). * means significantly different from LAS ρ using Fisher’s z-transform. Bold: highest correlation per language. correlated, e.g., LAS and LA, and UAS and NED, but some exhibit very low correlation coefficients. Next we study correlations with human judgments (Table 4). In order to aggregate over the annotations, we use an item-response model (Hovy et al., 2013). The correlations are relatively weak compared to similar findings for other NLP tasks. For instance, ROUGE-1 (Lin, 2004) correlates strongly with perceived summary quality, with a coefficient of 0.99. The same holds for BLEU and human judgments of machine translation quality (Papineni et al., 2002). We find that, overall, LAS is the metric that correlates best with human judgments. It is closely followed by UAS, which does not differ significantly from LAS, albeit the correlations for UAS are slightly lower on average. NED is in turn highly correlated with UAS. The correlations for the predi"
K15-1033,E12-1006,0,0.0277816,"Missing"
K15-1033,W04-1013,0,0.191111,"tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automatic measure"
K15-1033,P05-1012,0,0.0845214,". |{v |v ∈ V, lG (v, ·) = lP (v, ·)}| |V | Data In our experiments we use data from five languages: The English (en), German (de) and Spanish (es) treebanks from the Universal Dependencies (UD v1.0) project (Nivre et al., 2015), the Copenhagen Dependency Treebank (da) (BuchKromann, 2003), and the Croatian Dependency Treebank (hr) (Agi´c and Merkler, 2013). We keep the original POS tags for all datasets (17 tags in case of UD, 13 tags for Croatian, and 23 for Danish). Data characteristics are in Table 1. For the parsing systems, we follow McDonald and Nivre (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two deWe include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set o"
K15-1033,P05-1013,0,0.0676584,"a from five languages: The English (en), German (de) and Spanish (es) treebanks from the Universal Dependencies (UD v1.0) project (Nivre et al., 2015), the Copenhagen Dependency Treebank (da) (BuchKromann, 2003), and the Croatian Dependency Treebank (hr) (Agi´c and Merkler, 2013). We keep the original POS tags for all datasets (17 tags in case of UD, 13 tags for Croatian, and 23 for Danish). Data characteristics are in Table 1. For the parsing systems, we follow McDonald and Nivre (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two deWe include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set of nodes Vverb ) and checks whether all its core arguments match, i.e., all outg"
K15-1033,W04-2407,0,0.0333814,"dependency parsing metrics, described in this section. Given a labeled gold tree G = hV, EG , lG (·)i and a labeled predicted tree P = hV, EP , lP (·)i, let E ⊂ V × V be the set of directed edges from dependents to heads, and let l : V × V → L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). LAS = 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. |{e |e ∈ EG ∩ EP }| |V | |{e |lG (e) = lP (e), e ∈ EG ∩ EP }| |V | LA = |{v |Vverb , cG (v) = cP (v) ∧ lG (v, ·) = lP (v, ·)}| |Vverb | For the final figure of seven different parsing metrics, on top of the previous five, in our experiments we also include the neutral edge direction metric (NED) (Schwartz et al., 2011), and tree edit distan"
K15-1033,C10-1094,0,0.118629,"cs fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. 315 Proceedings of the 19th Conference on Computational Language Learning, pages 315–320, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Contributions We present i)"
K15-1033,S15-2153,0,0.0389892,"e (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two deWe include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set of nodes Vverb ) and checks whether all its core arguments match, i.e., all outgoing dependency edges except for punctuation. Since LCP is a very strict metric, we also evaluate UCP, its unlabeled variant. Given a function cX (v) that retrieves the set of child nodes of a node v from a tree X, we first define UCP as follows, and then incorporate the label matching for LCP: 1 The dataset is publicly available at https:// bitbucket.org/lowlands/release 2 http://alt.qcri.org/semeval2015/ 3 316 http://www.tsarfaty.com/unipar/ L ANG PARSER LAS UAS LA NED TED"
K15-1033,S14-2008,0,\N,Missing
K15-1038,P12-2047,0,0.0910684,"Missing"
K15-1038,D13-1075,1,0.770236,"ta, and evaluated on out-ofdomain test data; training on four domains, testing on one. For the gaze features, instead of using token gaze features, we first built a lexicon with average word type statistics from the training data. We normalize the gaze matrix by dividing with its standard deviation. This is the normalizer in Turian et al. (2010) with σ = 1.0. We condition on the gaze features of the current word, only. We compare performance using gaze features to using only word frequency, estimating from the (unlabeled) English Web Treebank corpus, and word length (F REQ L EN). Related work Matthies and Søgaard (2013) present results that suggest that individual variation among (academically trained) subjects’ reading behavior was not a greater source of error than variation within subjects, showing that it is possible to predict fixations across readers. Our work relates to such work, studying the robustness of reading models across domains and readers, but it also relates in spirit to research on using weak supervision in NLP, e.g., work on using HTML markup to improve dependency parsers (Spitkovsky, 2013) or using clickthrough data to improve POS taggers (Ganchev et al., 2012). 5 Conclusions We have sho"
K15-1038,W09-1113,0,0.388637,"linguistic annotation by hand. Our results show that gaze features do discriminate between most pairs of syntactic categories, and we show how we can use this to annotate words with part of speech across domains, when tag dictionaries enable us to narrow down the set of potential categories. 1 Introduction 2 Eye movements during reading is a wellestablished proxy for cognitive processing, and it is well-known that readers are more likely to fixate on words from open syntactic categories (verbs, nouns, adjectives) than on closed category items like prepositions and conjunctions (Rayner, 1998; Nilsson and Nivre, 2009). Generally, readers seem to be most likely to fixate and re-fixate on nouns (Furtner et al., 2009). If reading behavior is affected by syntactic category, maybe reading behavior can, conversely, also tell us about the syntax of words in context. This paper investigates to what extent gaze data can be used to predict syntactic categories. We show that gaze data can effectively be used to discriminate between a wide range of part of speech Experiment In our experiment, 10 subjects read syntactically annotated sentences from five domains. Data The data consists of 250 sentences: 50 sentences (mi"
K15-1038,N13-1039,0,0.0535656,"Missing"
K15-1038,petrov-etal-2012-universal,0,0.0612512,"Missing"
K15-1038,Q13-1001,0,0.0391576,"Missing"
K15-1038,P10-1040,0,0.00764834,"ace 4 trained a type-constrained (averaged) perceptron model with drop-out and a standard feature model (from Owoputi et al. (2013)) augmented with the above gaze features. The POS tagger was trained on a very small seed of data (200 sentences), doing 20 passes over the data, and evaluated on out-ofdomain test data; training on four domains, testing on one. For the gaze features, instead of using token gaze features, we first built a lexicon with average word type statistics from the training data. We normalize the gaze matrix by dividing with its standard deviation. This is the normalizer in Turian et al. (2010) with σ = 1.0. We condition on the gaze features of the current word, only. We compare performance using gaze features to using only word frequency, estimating from the (unlabeled) English Web Treebank corpus, and word length (F REQ L EN). Related work Matthies and Søgaard (2013) present results that suggest that individual variation among (academically trained) subjects’ reading behavior was not a greater source of error than variation within subjects, showing that it is possible to predict fixations across readers. Our work relates to such work, studying the robustness of reading models acro"
K15-1038,W02-1001,0,\N,Missing
K15-1038,I11-1100,0,\N,Missing
K15-1038,D12-1063,0,\N,Missing
K18-1021,W16-1614,0,0.151595,"Missing"
K18-1021,E14-1049,0,0.504024,"vector spaces indeed reported superior performance with linear models as compared to non-linear neural approaches (Mikolov et al., 2013). The relative success of the simple linear approach can be explained in terms of isomorphism across monolingual semantic spaces,1 an idea that receives support from cognitive science (Youn et al., 1999). Word vector spaces are not perfectly isomorphic, however, as shown by Søgaard et al. (2018), who use a Laplacian graph similarity metric to measure this property. In this work, we show that projecting both source and target vector spaces into a third space (Faruqui and Dyer, 2014), using a variant of PA known as Generalized Procrustes Analysis (Gower, 1975), makes it easier to learn the alignment between two word vector spaces, as compared to the single linear transform used in Conneau et al. (2018). Most recent approaches to bilingual dictionary induction find a linear alignment between the word vector spaces of two languages. We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of expressivity, makes it easier to learn approximate alignments. Our modified approach also allows for support"
K18-1021,N15-1028,0,0.0274487,"nality, as studied in Mimno and Thompson (2017), increases with corpus size, so we would expect embedding spaces learned from corpora comparable in size, to also be more similar in shape. 6 Related work Bilingual embeddings Many diverse crosslingual word embedding models have been proposed (Ruder et al., 2018). The most popular kind learns a linear transformation from source to target language space (Mikolov et al., 2013). In most recent work, this mapping is constrained to be orthogonal and solved using Procrustes Analysis (Xing et al., 2015; Artetxe et al., 2017, 2018; Conneau et al., 2018; Lu et al., 2015). The approach most similar to ours, Faruqui and Dyer (2014), uses canonical correlation analysis (CCA) to project both source and target language spaces into a third, joint space. In this setup, similarly to GPA, the third space is iteratively updated, such that at timestep t, it is a product of the two language spaces as transformed by the mapping learned at timestep t − 1. The objective that drives the updates of the mapping matrices is to maximize the correlation between the projected embeddings of translational equivalents (where the latter are taken from a gold-standard seed dictionary)."
K18-1021,D17-1308,0,0.0306298,"w-resource languages do not necessarily have lower Procrustes fits than high-resource ones (compare Estonian and Finnish, for example), the gap between the Procrustes fit and GPA precision is on average much higher within low-resource languages than within high-resource ones (52.4610 compared to 25.47, respectively). This finding is in line with the common understanding that the quality of distributional word vectors depends on the amount of data available—we can infer from these results that suboptimal embeddings results in suboptimal cross-lingual alignments. 5.3 rectionality, as studied in Mimno and Thompson (2017), increases with corpus size, so we would expect embedding spaces learned from corpora comparable in size, to also be more similar in shape. 6 Related work Bilingual embeddings Many diverse crosslingual word embedding models have been proposed (Ruder et al., 2018). The most popular kind learns a linear transformation from source to target language space (Mikolov et al., 2013). In most recent work, this mapping is constrained to be orthogonal and solved using Procrustes Analysis (Xing et al., 2015; Artetxe et al., 2017, 2018; Conneau et al., 2018; Lu et al., 2015). The approach most similar to"
K18-1021,P17-1042,0,0.683002,"ment between the word vector spaces of two languages. We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of expressivity, makes it easier to learn approximate alignments. Our modified approach also allows for supporting languages to be included in the alignment process, to obtain an even better performance in low resource settings. 1 Introduction Several papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction (BDI) (Barone, 2016; Artetxe et al., 2017; Zhang et al., 2017; Conneau et al., 2018; Søgaard et al., 2018), the task of identifying translational equivalents across two languages. These approaches cast BDI as a problem of aligning monolingual word embeddings. Pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties (for an adversarial approach, see Conneau et al. (2018)). Alternatively, weak supervision can be provided in the form of numerals (Artetxe et al., 2017) or identically spelled words (Søgaard et al., 2018). Successful unsupervised o"
K18-1021,P18-1072,1,0.852776,"Missing"
K18-1021,D17-1270,0,0.108811,"Missing"
K18-1021,N15-1104,0,0.607491,"GPA, which aligns both source and target spaces with a third, latent space, constructed by averaging over the two language spaces. (1) thus minimizing the trace between each two corresponding rows of the transformed space T E and F . We build E and F based on a seed dictionary of N entries, such that each pair of corresponding rows in E and F , (en , fn ) for n = 1, . . . , N consists of the embeddings of a translational pair of words. In order to preserve the monolingual quality of the transformed embeddings, it is beneficial to use an orthogonal matrix T for cross-lingual mapping purposes (Xing et al., 2015; Artetxe et al., 2017). Conveniently, the orthogonal Procrustes problem has an analytical solution, based on Singular Value Decomposition (SVD): F > E = U ΣV > T = V U> 3 For an analytical solution to GPA, we compute the average of the embedding matrices E1...k after transformation by T1...k : G=k {T1 ,...,Tk } k X ||Ti Ei − Tj Ej )||2 Ei Ti (4) thus obtaining a latent space, G, which captures properties of each of E1...k , and potentially additional properties emerging from the combination of the spaces. On the very first iteration, prior to having any estimates of T1...k , we set G = Ei for"
K18-1021,P17-1179,0,0.141472,"Missing"
K18-1021,J13-3004,0,\N,Missing
K18-1030,W16-2206,0,0.0213051,"to all its words (Carpenter and Just, 1983; Rayner and Duffy, 1988). For example, humans are likely to omit many function words and other words that are predictable in context and focus on less predictable content words. Moreover, when they fixate on a word, the duration of that fixation depends on a number of linguistic factors (Clifton et al., 2007; Demberg and Keller, 2008). Since learning good attention functions for recurrent neural networks requires large volumes of data (Zoph et al., 2016; Britz et al., 2017), and errors in attention are known to propagate to classification decisions (Alkhouli et al., 2016), we explore the idea of using human attention, as estimated from eye-tracking corpora, as an inductive bias on such attention functions. Penalizing attention functions for departing from human attention may enable us to learn better attention functions when data is limited. Eye-trackers provide millisecond-accurate records on where humans look when they are reading, and they are becoming cheaper and more easily available by the day (San Agustin et al., 2009). In this paper, we use publicly available 2 Method We present a recurrent neural architecture that jointly learns the recurrent paramete"
K18-1030,P16-2094,1,0.583842,"ai aei = P k ak (9) Our model thus combines two distinct objectives: one at the sentence level and one at the token level. The sentence-level objective is to minimize the squared error between output activations and true sentence labels yb. Lsent = X (y (j) − yb(j) )2 j 303 (10) The token-level objective, similarly, is to minimize the squared error for the attention not aligning with our human attention metric. Ltok = XX j b(j)(t) )2 (a(j)(t) − a same text. For every token, we compute the mean duration of all fixations to this token as our measure of human attention, following previous work (Barrett et al., 2016a; Gonzalez-Garduno and Søgaard, 2018). (11) t These are finally combined to a weighted sum, using λ (between 0 and 1) to trade off loss functions at the sentence and token levels. L = Lsent + λLtok Dundee The English part of the Dundee corpus (Kennedy et al., 2003) comprises 2,368 sentences and more than 50,000 tokens. The texts were read by ten skilled, adult, native speakers. The texts are 20 newspaper articles from The Independent. The reading was self-paced and as close to natural, contextualized reading as possible for a laboratory data collection. The apparatus was a Dr Bouis Oculometer"
K18-1030,C16-1126,1,0.941516,"ai aei = P k ak (9) Our model thus combines two distinct objectives: one at the sentence level and one at the token level. The sentence-level objective is to minimize the squared error between output activations and true sentence labels yb. Lsent = X (y (j) − yb(j) )2 j 303 (10) The token-level objective, similarly, is to minimize the squared error for the attention not aligning with our human attention metric. Ltok = XX j b(j)(t) )2 (a(j)(t) − a same text. For every token, we compute the mean duration of all fixations to this token as our measure of human attention, following previous work (Barrett et al., 2016a; Gonzalez-Garduno and Søgaard, 2018). (11) t These are finally combined to a weighted sum, using λ (between 0 and 1) to trade off loss functions at the sentence and token levels. L = Lsent + λLtok Dundee The English part of the Dundee corpus (Kennedy et al., 2003) comprises 2,368 sentences and more than 50,000 tokens. The texts were read by ten skilled, adult, native speakers. The texts are 20 newspaper articles from The Independent. The reading was self-paced and as close to natural, contextualized reading as possible for a laboratory data collection. The apparatus was a Dr Bouis Oculometer"
K18-1030,N06-1038,0,0.128832,"Missing"
K18-1030,W15-1814,1,0.840513,"aard (2018) use auxiliary data to regularize attention functions in recurrent neural networks; not from psycholinguistics data, but using small amounts of task-specific, token-level annotations. While their motivation is very different from ours, technically our models are very related. In a different context, Das et al. (2017) investigated whether humans attend to the same regions as neural networks solving visual question answering problems. Lindsey (2017) also used human-inspired, unsupervised attention in a computer vision context. Gaze has also been used in the context of grammaticality (Klerke et al., 2015a,b), as well as in readability assessment (Gonzalez-Garduno and Søgaard, 2018). Other work on multi-purpose attention functions While our work is the first to use gaze data to guide attention in a recurrent architectures, there has recently been some work on sharing attention functions across tasks. Firat et al. (2016), for example, share attention functions between languages in the context of multi-way neural machine translation. Gaze has either been used as features (Barrett and Søgaard, 2015a; Barrett et al., 2016b) or as a direct supervision signal in multi-task learning scenarios (Klerke"
K18-1030,P15-1166,0,0.0306673,"we explore this idea, proposing a particular architecture and training method that, in effect, uses human attention to regularize machine attention. hi = tanh(Wh hei + bh ) (1) (2) (3) (4) The final (reduced) hidden state is sometimes used as a sentence representation s, but we instead use attention to compute s by multiplying dynamically predicted attention weights with the hidden states for each time step. The final sentence predictions y are then computed by passing s through two more hidden layers: Our training method is similar to a standard approach to training multi-task architectures (Dong et al., 2015; Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017), sometimes referred to as the alternating training approach (Luong et al., 2016): We randomly select a data point from our training data or the eye-tracking corpus with some (potentially equal) probability. If the data point is sampled from our training data, we predict a discrete category and use the computed loss to update our parameters. If the data point is sampled from the eye-tracking corpus, we still run the recurrent network to produce a category, but this time we only monitor the attention weights assigned to the input tokens. We"
K18-1030,W15-2402,1,0.776564,"aard (2018) use auxiliary data to regularize attention functions in recurrent neural networks; not from psycholinguistics data, but using small amounts of task-specific, token-level annotations. While their motivation is very different from ours, technically our models are very related. In a different context, Das et al. (2017) investigated whether humans attend to the same regions as neural networks solving visual question answering problems. Lindsey (2017) also used human-inspired, unsupervised attention in a computer vision context. Gaze has also been used in the context of grammaticality (Klerke et al., 2015a,b), as well as in readability assessment (Gonzalez-Garduno and Søgaard, 2018). Other work on multi-purpose attention functions While our work is the first to use gaze data to guide attention in a recurrent architectures, there has recently been some work on sharing attention functions across tasks. Firat et al. (2016), for example, share attention functions between languages in the context of multi-way neural machine translation. Gaze has either been used as features (Barrett and Søgaard, 2015a; Barrett et al., 2016b) or as a direct supervision signal in multi-task learning scenarios (Klerke"
K18-1030,D17-1169,1,0.853198,"Missing"
K18-1030,N16-1179,1,0.886802,"to double standards and certain rules The gaze-informed grammatical error detection 7 Discussion and related work Gaze in NLP It has previously been shown that several NLP tasks benefit from gaze information, including part-of-speech tagging (Barrett and Søgaard, 2015b; Barrett et al., 2016a), prediction of multiword expressions (Rohanian et al., 2017) and sentiment analysis (Mishra et al., 2017b). Gaze information and other measures from psycholinguistics have been used in different ways in NLP. Some authors have used discretized, single features (Pate and Goldwater, 2011, 2013; Plank, 2016; Klerke et al., 2016), whereas others have used multidimensional, continuous values (Barrett et al., 2016a; Bingel et al., 2016). We follow Gonzalez-Garduno and Søgaard (2018) in using a single, continuous feature. We did not experiment with other representations, however. Specifically, we only considered the signal from token-level, normalized mean fixation durations. Fixation duration is a feature that carries an enormous amount of information about the text and the language understanding process. Carpenter and Just (1983) show that readers are more likely to fixate on open-class words that are not predictable f"
K18-1030,N16-1101,0,0.0198253,". (2017) investigated whether humans attend to the same regions as neural networks solving visual question answering problems. Lindsey (2017) also used human-inspired, unsupervised attention in a computer vision context. Gaze has also been used in the context of grammaticality (Klerke et al., 2015a,b), as well as in readability assessment (Gonzalez-Garduno and Søgaard, 2018). Other work on multi-purpose attention functions While our work is the first to use gaze data to guide attention in a recurrent architectures, there has recently been some work on sharing attention functions across tasks. Firat et al. (2016), for example, share attention functions between languages in the context of multi-way neural machine translation. Gaze has either been used as features (Barrett and Søgaard, 2015a; Barrett et al., 2016b) or as a direct supervision signal in multi-task learning scenarios (Klerke et al., 2016; Gonzalez-Garduno and Søgaard, 2018). We are, to the best of our knowledge, the first to use gaze to inform attention functions in recurrent neural networks. Sentiment analysis While sentiment analysis is most often considered a supervised learning problem, several authors have leveraged other signals 308"
K18-1030,D16-1012,0,0.0340398,"Missing"
K18-1030,P17-1194,1,0.888294,"Missing"
K18-1030,P17-1035,0,0.287094,"double standards and certain rules. The gaze-informed sentiment classifier, on the other hand, focuses more on sorry I am not sexist which, in isolation, reads like an apologetic disclaimer. This model also gives weight to double standards and certain rules The gaze-informed grammatical error detection 7 Discussion and related work Gaze in NLP It has previously been shown that several NLP tasks benefit from gaze information, including part-of-speech tagging (Barrett and Søgaard, 2015b; Barrett et al., 2016a), prediction of multiword expressions (Rohanian et al., 2017) and sentiment analysis (Mishra et al., 2017b). Gaze information and other measures from psycholinguistics have been used in different ways in NLP. Some authors have used discretized, single features (Pate and Goldwater, 2011, 2013; Plank, 2016; Klerke et al., 2016), whereas others have used multidimensional, continuous values (Barrett et al., 2016a; Bingel et al., 2016). We follow Gonzalez-Garduno and Søgaard (2018) in using a single, continuous feature. We did not experiment with other representations, however. Specifically, we only considered the signal from token-level, normalized mean fixation durations. Fixation duration is a feat"
K18-1030,N18-1027,1,0.920247,"hierarchical model whose word representations are concatenations of the output of character-level LSTMs and word embeddings, following Plank et al. (2016), but we ignore the character-level part of our architecture in the equations below: Behind our approach lies the simple observation that we can correlate the token-level attention devoted by a recurrent neural network, even if trained on sentence-level signals, with any measure defined at the token level. In other words, we can compare the attention devoted by a recurrent neural network to various measures, including token-level annotation (Rei and Søgaard, 2018) and eye-tracking measures. The latter is particularly interesting as it is typically considered a measurement of human attention. → − −−→ hi = LST M (xi , hi−1 ) ← − ←−− hi = LST M (xi , hi+1 ) → − ← − hei = [ hi ; hi ] We go beyond this: Not only can we compare machine attention with human attention, we can also constrain or inform machine attention by human attention in various ways. In this paper, we explore this idea, proposing a particular architecture and training method that, in effect, uses human attention to regularize machine attention. hi = tanh(Wh hei + bh ) (1) (2) (3) (4) The fi"
K18-1030,P16-1112,1,0.835869,"three-way classification task with positive, negative and neutral classes. We reduce the task to binary tasks detecting negative sentences vs. non-negative and vice versa for the positive class. Therefore the dataset size is the same for POS and NEG experiments. 3.3 Grammatical error detection Our second task is grammatical error detection. We use the First Certificate in English error detection dataset (FCE) (Yannakoudakis et al., 2011). This dataset contains essays written by English learners during language examinations, where any grammatical errors have been manually annotated by experts. Rei and Yannakoudakis (2016) converted the dataset for a sequence labeling task and we use their splits for training, development and testing. Similarly to Rei and Søgaard (2018), we perform sentence-level binary classification of sentences that need some editing vs. grammatically correct sentences. We do not use the tokenlevel labels for training our model. 3.4 Hate speech detection Our third and final task is detection of abusive language; or more specifically, hate speech detection. We use the datasets of Waseem (2016) and Waseem and Hovy (2016). The former contains 6,909 tweets; the latter 14,031 tweets. They are man"
K18-1030,W17-5004,1,0.881134,"Missing"
K18-1030,rohanian-etal-2017-using,0,0.207971,"hate speech indicator. It also gives weight to double standards and certain rules. The gaze-informed sentiment classifier, on the other hand, focuses more on sorry I am not sexist which, in isolation, reads like an apologetic disclaimer. This model also gives weight to double standards and certain rules The gaze-informed grammatical error detection 7 Discussion and related work Gaze in NLP It has previously been shown that several NLP tasks benefit from gaze information, including part-of-speech tagging (Barrett and Søgaard, 2015b; Barrett et al., 2016a), prediction of multiword expressions (Rohanian et al., 2017) and sentiment analysis (Mishra et al., 2017b). Gaze information and other measures from psycholinguistics have been used in different ways in NLP. Some authors have used discretized, single features (Pate and Goldwater, 2011, 2013; Plank, 2016; Klerke et al., 2016), whereas others have used multidimensional, continuous values (Barrett et al., 2016a; Bingel et al., 2016). We follow Gonzalez-Garduno and Søgaard (2018) in using a single, continuous feature. We did not experiment with other representations, however. Specifically, we only considered the signal from token-level, normalized mean fix"
K18-1030,W11-0603,0,0.0233109,"tic disclaimer. This model also gives weight to double standards and certain rules The gaze-informed grammatical error detection 7 Discussion and related work Gaze in NLP It has previously been shown that several NLP tasks benefit from gaze information, including part-of-speech tagging (Barrett and Søgaard, 2015b; Barrett et al., 2016a), prediction of multiword expressions (Rohanian et al., 2017) and sentiment analysis (Mishra et al., 2017b). Gaze information and other measures from psycholinguistics have been used in different ways in NLP. Some authors have used discretized, single features (Pate and Goldwater, 2011, 2013; Plank, 2016; Klerke et al., 2016), whereas others have used multidimensional, continuous values (Barrett et al., 2016a; Bingel et al., 2016). We follow Gonzalez-Garduno and Søgaard (2018) in using a single, continuous feature. We did not experiment with other representations, however. Specifically, we only considered the signal from token-level, normalized mean fixation durations. Fixation duration is a feature that carries an enormous amount of information about the text and the language understanding process. Carpenter and Just (1983) show that readers are more likely to fixate on op"
K18-1030,S15-2078,0,0.0589074,"Missing"
K18-1030,Q13-1006,0,0.0309346,"Missing"
K18-1030,D14-1162,0,0.0811882,"n and using the same regularization scheme as in our human attention model. sampling from the eye-tracking corpus initially is 1 0.5, but drops linearly for each epoch ( E+1 ; see 2.1. We apply the models with the best average F1 scores over three random seeds on the validation data, to our test sets. Hyperparameters Basic hyper-parameters such as number of hidden layers, layer size, and activation functions were following the settings of Rei and Søgaard (2018). The dimensionality of our word embedding layer was set to size 300, and we use publicly available pre-trained Glove word embeddings (Pennington et al., 2014) that we finetune during training. The dimensionality of the character embedding layer was set to 100. The recurrent layers in the character-level component have dimensionality 100; the word-level recurrent layers dimensionality 300. The dimensionality of our feed-forward layer, leading to reduced combined representations hi , is 200, and the attention layer has dimensionality 100. Three hyper-parameters, however, we tune for each architecture and for each task, by measuring sentence-level F1 -scores on the development sets. These are: (a) learning rate, (b) λ in Equation (12), i.e., controlli"
K18-1030,C16-1059,0,0.0281674,"gives weight to double standards and certain rules The gaze-informed grammatical error detection 7 Discussion and related work Gaze in NLP It has previously been shown that several NLP tasks benefit from gaze information, including part-of-speech tagging (Barrett and Søgaard, 2015b; Barrett et al., 2016a), prediction of multiword expressions (Rohanian et al., 2017) and sentiment analysis (Mishra et al., 2017b). Gaze information and other measures from psycholinguistics have been used in different ways in NLP. Some authors have used discretized, single features (Pate and Goldwater, 2011, 2013; Plank, 2016; Klerke et al., 2016), whereas others have used multidimensional, continuous values (Barrett et al., 2016a; Bingel et al., 2016). We follow Gonzalez-Garduno and Søgaard (2018) in using a single, continuous feature. We did not experiment with other representations, however. Specifically, we only considered the signal from token-level, normalized mean fixation durations. Fixation duration is a feature that carries an enormous amount of information about the text and the language understanding process. Carpenter and Just (1983) show that readers are more likely to fixate on open-class words that"
K18-1030,P16-2067,1,0.755477,"eriments) overlap in any way. Our experimental protocol, in other words, does not require in-task eye-tracking recordings, but simply leverages information from existing, available corpora. Model Our architecture is a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) that encodes word representations xi into forward and backward representations, and into combined hidden states hi (of slightly lower dimensionality) at every timestep. In fact, our model is a hierarchical model whose word representations are concatenations of the output of character-level LSTMs and word embeddings, following Plank et al. (2016), but we ignore the character-level part of our architecture in the equations below: Behind our approach lies the simple observation that we can correlate the token-level attention devoted by a recurrent neural network, even if trained on sentence-level signals, with any measure defined at the token level. In other words, we can compare the attention devoted by a recurrent neural network to various measures, including token-level annotation (Rei and Søgaard, 2018) and eye-tracking measures. The latter is particularly interesting as it is typically considered a measurement of human attention. →"
K18-1030,W17-1101,0,0.0207736,"Missing"
K18-1030,D13-1170,0,0.0267271,"Missing"
K18-1030,P16-2038,1,0.742011,"a, proposing a particular architecture and training method that, in effect, uses human attention to regularize machine attention. hi = tanh(Wh hei + bh ) (1) (2) (3) (4) The final (reduced) hidden state is sometimes used as a sentence representation s, but we instead use attention to compute s by multiplying dynamically predicted attention weights with the hidden states for each time step. The final sentence predictions y are then computed by passing s through two more hidden layers: Our training method is similar to a standard approach to training multi-task architectures (Dong et al., 2015; Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017), sometimes referred to as the alternating training approach (Luong et al., 2016): We randomly select a data point from our training data or the eye-tracking corpus with some (potentially equal) probability. If the data point is sampled from our training data, we predict a discrete category and use the computed loss to update our parameters. If the data point is sampled from the eye-tracking corpus, we still run the recurrent network to produce a category, but this time we only monitor the attention weights assigned to the input tokens. We then compute the minimum sq"
K18-1030,W16-5618,0,0.100077,"Missing"
K18-1030,N16-2013,0,0.18631,"models and the model with gaze-informed attention by the attention weights of an example sentence. Though it is a single, cherry-picked example, it is representative of the general trends we observe in the data, when manually inspecting attention patterns. Table 3 presents a coarse visualization of the attention weights of six different models, namely our baseline architecture and the architecture with gaze-informed attention, trained on three different tasks: hate speech detection, negative sentiment classification, and error detection. The sentence is a positive hate speech example from the Waseem and Hovy (2016) development set. The words with more attention than the sentence average are bold-faced. First note that the baseline models only attend to one or two coherent text parts. This pattern was very consistent across all the sentences we examined. This pattern was not observed with gazeinformed attention. Our second observation is that the baseline models are more likely to attend to stop words than gaze-informed attention. This suggests that gazeinformed attention has learned to simulate human attention to some degree. We also see many differences between the jointly learned task-specific, gaze-i"
K18-1030,P11-1019,0,0.0266868,"orpus better. 3.2 set (S EMEVAL T WITTER POS |NEG), and an out-of-domain test set, SemEval-2013 SMS test set (S EMEVAL SMS POS |NEG). The SemEval2013 sentiment classification task was a three-way classification task with positive, negative and neutral classes. We reduce the task to binary tasks detecting negative sentences vs. non-negative and vice versa for the positive class. Therefore the dataset size is the same for POS and NEG experiments. 3.3 Grammatical error detection Our second task is grammatical error detection. We use the First Certificate in English error detection dataset (FCE) (Yannakoudakis et al., 2011). This dataset contains essays written by English learners during language examinations, where any grammatical errors have been manually annotated by experts. Rei and Yannakoudakis (2016) converted the dataset for a sequence labeling task and we use their splits for training, development and testing. Similarly to Rei and Søgaard (2018), we perform sentence-level binary classification of sentences that need some editing vs. grammatically correct sentences. We do not use the tokenlevel labels for training our model. 3.4 Hate speech detection Our third and final task is detection of abusive langu"
K18-1030,D16-1163,0,0.0187587,"/coastalcph/ Sequence_classification_with_ human_attention. Introduction When humans read a text, they do not attend to all its words (Carpenter and Just, 1983; Rayner and Duffy, 1988). For example, humans are likely to omit many function words and other words that are predictable in context and focus on less predictable content words. Moreover, when they fixate on a word, the duration of that fixation depends on a number of linguistic factors (Clifton et al., 2007; Demberg and Keller, 2008). Since learning good attention functions for recurrent neural networks requires large volumes of data (Zoph et al., 2016; Britz et al., 2017), and errors in attention are known to propagate to classification decisions (Alkhouli et al., 2016), we explore the idea of using human attention, as estimated from eye-tracking corpora, as an inductive bias on such attention functions. Penalizing attention functions for departing from human attention may enable us to learn better attention functions when data is limited. Eye-trackers provide millisecond-accurate records on where humans look when they are reading, and they are becoming cheaper and more easily available by the day (San Agustin et al., 2009). In this paper,"
klerke-sogaard-2012-dsim,N10-1056,0,\N,Missing
klerke-sogaard-2012-dsim,E06-1021,0,\N,Missing
klerke-sogaard-2012-dsim,C04-1051,0,\N,Missing
klerke-sogaard-2012-dsim,D11-1038,0,\N,Missing
klerke-sogaard-2012-dsim,C10-1152,0,\N,Missing
klerke-sogaard-2012-dsim,N03-1003,0,\N,Missing
klerke-sogaard-2012-dsim,P11-2117,0,\N,Missing
L16-1136,J08-4004,0,0.173758,"an blogs and chat, probably because the language of these text types is intrinsically more complex and contains more abstract concepts (for a detailed study on domain differences in the annotations see Olsen et al. 2015). Figure 2 shows how each noun supersense is represented disagreement-wise in the corpus. The rows in the disagreement plot are sorted after the size of the diagonal value. Rows with many large, spread boxes indicate supersenses with low agreement which need a closer examination or more precise guidelines. For 2 http://wordnet.princeton.edu/wordnet/man/lexnames.5WN.ht ml 3 Cf. Artstein & Poesio 2008 for discussion of agreement scores in computational linguistics. 843 instance, the supersenses n.person and n.institution seem to be hard for the annotators to distinguish from each other, whereas n.disease has proven easy to identify. a set of precise guidelines defining the sense structuring principles. In rough terms, these principles were based on the distinctions between core and subsenses as defined by Cruse (2000:110ff). Among several types of regular relations between senses, Cruse defines four types where the senses might be of the same ontological type (and therefore candidates of c"
L16-1136,brown-etal-2010-number,0,0.026183,"ent words are annotated (so-called lexical sample corpora). As discussed in Ide & Wilks (2007), Kilgarriff (2007) and others, defining appropriate sense inventories for annotation and word sense disambiguation tasks is however a very hard task. The need for coarser and more manageable sense inventories has emerged, partly driven by poor sense annotator agreement scores in the aforementioned annotations. This has resulted in a series of annotation experiments applying manually and automatically clustered senses, as seen in Agirre & Lacalle (2003), Palmer et al. (2007), Passonneau et al. (2012) Brown et al. (2010), de Melo et al. (2012), and others. The need for “light weight” semantic annotations has led researchers to focus also on very coarse word sense annotation applying so-called supersenses that are derived from the list of WordNet’s first beginners or lexicographical files. This approach is becoming a de facto standard in recent years (Ciaramita & Johnson 2003, Qiu et al. 2011, Schneider et al. 2012). In the SemDaX corpus we include both supersense annotations and lexical sample annotations with fine-grained and automatically clustered senses for a selected set of highly ambiguous nouns. All an"
L16-1136,S14-1001,1,0.848737,"Missing"
L16-1136,W15-1831,1,0.907794,"that the coarse-grained supersense scheme is quite manageable to the annotators resulting in an acceptable agreement of 0.62 applying Krippendorff’s α. However, as shown in the dispersion plot in Figure 1 the scheme leaves room for improvements and adjustments; i.e. some particular supersenses prove very hard to agree upon. Further, the considerable information loss in the coarse annotations should be addressed in future extrinsic evaluations; for instance, it can be questioned to which extent we actually capture the practically relevant ambiguities with this coarse scheme; see also Martínez Alonso et al. 2015 for a first attempt of inducing a supersense tagger from our 6 Note that the supersense scheme is not directly comparable to the fine-grained schemes since the annotation tasks differ (all-word vs. lexical sample). 845 supersense annotations. This leads us to the finer-grained dictionary-driven annotations of highly ambiguous nouns that we described in Section 4. Here we can conclude that a clustered annotation scheme based on an ontologically driven collapsing of subsenses performs substantially better than a fully fine-grained scheme (disregarding here the better chance of agreeing on few t"
L16-1136,W15-1806,1,0.857672,"Missing"
L16-1136,2016.gwc-1.30,1,0.534761,"Missing"
L16-1136,W15-2005,1,0.784725,"Missing"
L16-1136,passonneau-etal-2012-masc,0,0.047019,"Missing"
L16-1136,E14-1078,1,0.850484,"flecting formal distinctions or logical relations between senses of the word in question. Not only does doubly annotated data provide valuable feedback regarding the annotation schemes, we also think that it can help us improve our learning algorithms. Our corpus is about one fifth of the size of SemCor. However, as mentioned, a large part of the data has been doubly annotated and later adjudicated. We make available both the final adjucated version and the individual annotations in order to facilitate research that deals with the linguistic information that resides in agreement variation. In Plank et al. (2014) and Martínez Alonso et al. (2015a) we present an algorithm that learns regularizers from small seeds of doubly annotated data. In future work we will apply SemDaX for further experiments along the same lines. Finally, our project includes a pilot study on the compilation of a Danish framenet (similar to the well-known Berkeley FrameNet, cf. https://framenet.icsi.berkeley.edu/fndrupal/). This part of the project has been embarked recently by focusing on the approx. 1/3 of the sentences of our corpus where cognition and communication verbs are present (identified via the previously mentioned su"
L16-1136,P12-2050,0,0.0982508,"ations. This has resulted in a series of annotation experiments applying manually and automatically clustered senses, as seen in Agirre & Lacalle (2003), Palmer et al. (2007), Passonneau et al. (2012) Brown et al. (2010), de Melo et al. (2012), and others. The need for “light weight” semantic annotations has led researchers to focus also on very coarse word sense annotation applying so-called supersenses that are derived from the list of WordNet’s first beginners or lexicographical files. This approach is becoming a de facto standard in recent years (Ciaramita & Johnson 2003, Qiu et al. 2011, Schneider et al. 2012). In the SemDaX corpus we include both supersense annotations and lexical sample annotations with fine-grained and automatically clustered senses for a selected set of highly ambiguous nouns. All annotations in the corpus rely on the combined wordnet and dictionary resources: DanNet (cf. Pedersen 2009 et al.) and a comprehensive monolingual, corpus-based dictionary of modern Danish, Den Danske Ordbog (DDO, Hjorth et al. 2005), which share sense identifies. The aim of the corpus is twofold: i) to assess the reliability of the different sense annotation schemes in terms of different levels of gr"
L16-1136,P13-4001,0,0.0740307,"Missing"
L16-1136,J99-4008,0,0.0763419,"Missing"
L16-1136,W14-0132,0,0.160111,"Missing"
L16-1136,W03-1022,0,\N,Missing
L18-1378,L16-1136,1,0.763918,"nyms and near synonyms of skælde ud ‘to scold’. Having detected a group of closely related verbs and verbal nouns and furthermore supplied the verbs with valency patterns from DDO via the common id numbers, the editor would search for an appropriate frame in BFN and assign this to the particular group (shown in Figure 3). In this way, a relative large framenet lexicon is being compiled with relatively little effort2. In order to easily access examples which would evoke frames relating to communication and cognition, we took advantage of the coarse sense annotations available in SemDax corpus (Pedersen et al 2016) and extracted all sentences annotated with either cognition and communication events (or both)3. This extraction also enabled us to prove whether a frame lexicon based on thesaurus vocabulary was actually extensive enough. Not surprisingly, however, not all groups were equally easy to assign frames to since the relation between DT and BFN was obviously not one-to-one in all semantic areas. A classic example is the discrepancy related to antonymous word senses such as remembering and forgetting which are covered by one frame (seen as the We used an open source, browser-based framenet annotatio"
L18-1378,P15-1109,0,0.0218073,"ce. The top frames subsume a total of 6 distinct frames, making this a 6way multinomial classification problem. However, for some verbs, frames were missing because the specific sense had not been foreseen in the frame lexicon based on the DT vocabulary. The largest part of these were ad hoc (figurative) senses not to be included in the frame lexicon (nor in the dictionaries), but there were also cases which led us to expand the lexicon, e.g. cognition verbs with communication senses in corpus but not included in the thesaurus chapters on 2384 Inspired by recent semantic parsing models, e.g., Zhou and Xu (2015), we use a set of binary deep, bidirectional Long Short Term Memory (LSTM) networks to predict frame labels. Each network predicts a single label, and we evaluate each network individually by computing sentence-level F1-scores. English-Danish Ours Statement 0.31 Opinion 0.16 Telling 0.13 Text_creation 0.08 Becoming_aware 0.06 Certainty 0.08 In the following, we report implementation details and results for our experiments in the frame prediction task. We ran experiments in three setups.    Table 1: Supervised and unsupervised F1-scores for the 6 most frequent frames. First, we trained the m"
L18-1378,candito-etal-2014-developing,0,0.304605,"Missing"
L18-1378,heppin-gronostaj-2012-rocky,0,0.0192275,"ncluding verbal nouns and annotated with type 08 (acts) and semantic relations. We rely on two assumptions:  that our frame lexicon will ease annotation considerably since a very limited set of possible frames for a given word is presented to the annotator via the annotation tool, and  that BFN frames for English can be more or less directly transferred to Danish; in other words, that the same frame elements or semantic roles can be identified in a Danish textual context with a particular frame. (A similar approach is taken for most other framenets being built for a number of languages, cf. Heppin & Gronostaj 2012, 2014 for Swedish, Candito et al. 2014 for French, Ohara 2014 for Japanese). In order to test this approach, we annotated 440 sentences from the corpus with their corresponding frames and frame elements. The sentences from SemDax cover a variety of text types such as blog, chat, forum, magazine, Parliament debates (written down by professionals), and newswire, of which the latter constitutes almost half of the corpus. Figure 3: Mapping Berkeley frames onto thematically ordered verb groups from DT, in this case synonyms and near synonyms of skælde ud ‘to scold’. Having detected a group of clos"
L18-1378,D12-1127,0,0.035249,"Missing"
L18-1378,2016.gwc-1.30,1,0.861399,"Missing"
L18-1378,ohara-2014-relating,0,0.0194793,"ons. We rely on two assumptions:  that our frame lexicon will ease annotation considerably since a very limited set of possible frames for a given word is presented to the annotator via the annotation tool, and  that BFN frames for English can be more or less directly transferred to Danish; in other words, that the same frame elements or semantic roles can be identified in a Danish textual context with a particular frame. (A similar approach is taken for most other framenets being built for a number of languages, cf. Heppin & Gronostaj 2012, 2014 for Swedish, Candito et al. 2014 for French, Ohara 2014 for Japanese). In order to test this approach, we annotated 440 sentences from the corpus with their corresponding frames and frame elements. The sentences from SemDax cover a variety of text types such as blog, chat, forum, magazine, Parliament debates (written down by professionals), and newswire, of which the latter constitutes almost half of the corpus. Figure 3: Mapping Berkeley frames onto thematically ordered verb groups from DT, in this case synonyms and near synonyms of skælde ud ‘to scold’. Having detected a group of closely related verbs and verbal nouns and furthermore supplied th"
L18-1378,E17-1072,1,0.71927,"ntic areas of motion, emotion, communication and cognition. Figure 1: A wordnet (DanNet), a framenet and a semantically annotated corpus (SemDax) expanded from two dictionaries via common sense ids The paper is organized as follows. Below we sketch out how we, in order to test the strength of the lexical working method, started by compiling a pilot frame lexicon based on only two selected semantic domains in existing lexica (Section 2). Section 3 describes how we used the resulting set of frames to annotate selected languages (cf. Pedersen et al. 2009, Nimb et al. 2017, Johannsen et al. 2015, Levy et al. 2017). In order to enable this combination of methods, quite a lot of effort has been put into relating the resources to international standards (see for instance Martinez et al. 2016). 1 The Danish frame lexicon is now freely available at https://github.com/dsldk/dansk-frame-net. It will be presented in more detail at The International FrameNet Workshop 2018, Multilingual FrameNets and Constructions at LREC 2018 (Nimb, submitted for review). Most recently, effort has been put into compiling a Danish Berkeley style (Ruppenhofer et al. 2016) frame lexicon (BFN) by extracting semantic data from The 2"
N06-3008,E91-1007,0,\N,Missing
N13-1068,D12-1091,0,0.0908549,"developers can do to estimate the effect of a change to our system – not on the labeled test data that happens to be available to us, but on future, still unseen datasets provided by our end users. The usual practice in NLP is to evaluate a system on a small sample of held-out labeled data. The observed effect size on this sample is then validated by significance testing across data points, testing whether the observed difference in performance means is likely to be due to mere chance. The preferred significance test is probably the nonparametric paired bootstrap (Efron and Tibshirani, 1993; Berg-Kirkpatrick et al., 2012), but many researchers also resort to Student’s t-test for dependent means relying on the assumption that their metric scores are normally distributed. Such significance tests tell us nothing about how likely our change to our system is to lead to improvements on new datasets. The significance tests all rely on the assumption that our datapoints are sampled i.i.d. at random. The significance tests only tell us how likely it is that the observed difference in performance means would change if we sampled a bigger test sample the same way we sampled the one we have available to us right now. In s"
N13-1068,P11-3012,0,0.0227169,"n held-out datasets – that this method is significantly more predictive of success than the usual practice of using macro- or micro-averages. Finally, we present a new parametric meta-analysis based on nonstandard assumptions that seems superior to standard parametric meta-analysis. 1 Introduction NLP tools and online services such as the Stanford Parser or Google Translate are used for a wide variety of purposes and therefore also on very different kinds of data. Some use the Stanford Parser to parse literature (van Cranenburgh, 2012), while others use it for processing social media content (Brown, 2011). The parser, however, was not necessarily evaluated on literature or social media content during development. Still, users typically expect reasonable performance on any natural language input. This paper asks what we as developers can do to estimate the effect of a change to our system – not on the labeled test data that happens to be available to us, but on future, still unseen datasets provided by our end users. The usual practice in NLP is to evaluate a system on a small sample of held-out labeled data. The observed effect size on this sample is then validated by significance testing acro"
N13-1068,W06-2920,0,0.0943164,"k we repeat the experiment 20 times and report average error. We vary k to see how many observations are needed for our estimates to be reliable. The results are presented in Figure 2. We note that meta-analysis provides much better estimates than macro-averages across the board. Our parametric meta-analysis based on the assumption that error reductions are Gumbel distributed performs best with more observations. Our second experiment repeats the same procedure using available data from cross-lingual dependency parsing. We use the submitted results by participants in the CoNLL-X shared task (Buchholz and Marsi, 2006) and try to predict the error reduction of one system over another given k many observations. Given that we only have 12 submissions per system we use k ∈ {5, 7, 9} randomly extracted datasets for observations and test on another five randomly extracted datasets. While results (Figure 3) are only statistically significant with k = 7, we see that metaanalysis estimates effect size across data sets better than macro-average in all cases. 4 Conclusions We have argued that evaluation across datasets is important for developing robust NLP tools, and that meta-analysis can provide better estimates o"
N13-1068,1993.eamt-1.1,0,0.299325,". This paper asks what we as developers can do to estimate the effect of a change to our system – not on the labeled test data that happens to be available to us, but on future, still unseen datasets provided by our end users. The usual practice in NLP is to evaluate a system on a small sample of held-out labeled data. The observed effect size on this sample is then validated by significance testing across data points, testing whether the observed difference in performance means is likely to be due to mere chance. The preferred significance test is probably the nonparametric paired bootstrap (Efron and Tibshirani, 1993; Berg-Kirkpatrick et al., 2012), but many researchers also resort to Student’s t-test for dependent means relying on the assumption that their metric scores are normally distributed. Such significance tests tell us nothing about how likely our change to our system is to lead to improvements on new datasets. The significance tests all rely on the assumption that our datapoints are sampled i.i.d. at random. The significance tests only tell us how likely it is that the observed difference in performance means would change if we sampled a bigger test sample the same way we sampled the one we have"
N13-1068,W12-2508,0,0.0227019,"Missing"
N13-1070,D11-1037,0,0.010158,"the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6 , and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work There has been considerable work on down-stream evaluation of syntactic parsers in the literature, but most previous work has focused on evaluating parsing models rather than linguistic theories. No one has, to the best of our knowledge, compared the impact of choice of tree-to-dependency conversion scheme acr"
N13-1070,C10-1011,0,0.00726912,"ot rely4 The LTH conversion scheme can be obtained by running pennconverter.jar available at with the http://nlp.cs.lth.se/software/treebank converter/ ’oldLTH’ flag set. 618 ing on syntactic features, when possible, and to results in the literature, when comparable results exist. Note that negation resolution and SRL are not end applications. It is not easy to generalize across five very different tasks, but the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6 , and the sentences wer"
N13-1070,P11-2031,0,0.0123088,"sentences of the parallel news data that have been parsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with human judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 2.4 Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff-Davis corpus.12 A widely used baseline for sentence compression experiments is Knight and Marcu (2002), who introduce two models: the noisy-channel model and a decision tree-based model. Both are tree-based methods that find the most likely compressed syntactic tree and outputs the yield of this tree. McDonald et al. (2006) instead use syntactic features to d"
N13-1070,P09-1087,0,0.361999,"very different tasks, but the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6 , and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work There has been considerable work on down-stream evaluation of syntactic parsers in the literature, but most previous work has focused on evaluating parsing models rather than linguistic theories. No one has, to the best of our knowledge, compared the impact of choice of tree-to-dependency"
N13-1070,D11-1138,0,0.0118129,"uate linguistic representations directly, evaluating representations and models jointly. Bender et al. (2011) compare several parsers across linguistic representations on a carefully designed evaluation set of hard, but relatively frequent syntactic constructions. They compare dependency parsers, constituent-based parsers and deep parsers. The authors argue in favor of evaluating parsers on diverse and richly annotated data. Others have discussed various ways of evaluating across annotation guidelines or translating structures to a common format (Schwartz et al., 2011; Tsarfaty et al., 2012). Hall et al. (2011) discuss optimizing parsers for specific down-stream applications, but consider only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic 619 Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practical applications, and the heuristics used in t"
N13-1070,W12-3602,0,0.00940108,"Missing"
N13-1070,W10-2910,0,0.0245474,"(ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section describes the applications and experimental set-ups included in this study. In the five applications considered below we use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relations as additional information about words and on a par with POS, our negation resolution system uses dependency paths, conditioning decisions on both dependency arcs and labels. In perspective classification, we use dependency triples (e.g. SUBJ(John, snore)) as features, while the semantic role labeling system cond"
N13-1070,W07-2416,0,0.0543773,"performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6 , and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work There has been considerable work on down-stream evaluation of syntactic parsers in the literature, but most previous work has focused on evaluating parsing models rather than linguistic theories. No one has, to the best of our knowledge, compared the impact of choice of tree-to-dependency conversion scheme across several NLP tasks. Johansson and Nugues (2007) compare the impact of yamada and lth on semantic role labelin"
N13-1070,W08-2123,0,0.0435471,"cy features at all, the models are tested using gold cues. Table 1 shows F1 scores for scopes, events and full negations, where a true positive correctly assigns both scope tokens and events to the rightful cue. The scores are produced using the evaluation script provided by the *SEM organizers. 2.2 Semantic role labeling Semantic role labeling (SRL) is the attempt to determine semantic predicates in running text and label their arguments with semantic roles. In our experiments we have reproduced the second bestperforming system in the CoNLL 2008 shared task in syntactic and semantic parsing (Johansson and Nugues, 2008).9 The English training data for the CoNLL 2008 shared task were obtained from PropBank and NomBank. For licensing reasons, we used OntoNotes 4.0, which includes PropBank, but not NomBank. This means that our system is only trained to classify verbal predicates. We used the Clearparser conversion tool10 to convert the OntoNotes 4.0 and subsequently supplied syntactic dependency trees using our different conversion schemes. We rely on gold standard argument identification and focus solely on the performance metric semantic labeled F1. 9 http://nlp.cs.lth.se/software/semantic parsing: propbank n"
N13-1070,P09-2079,0,0.0585007,"fixed set of dependency labels (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section describes the applications and experimental set-ups included in this study. In the five applications considered below we use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relations as additional information about words and on a par with POS, our negation resolution system uses dependency paths, conditioning decisions on both dependency arcs and labels. In perspective classification, we use dependency triples (e.g. SUBJ(John, snore)) as features, while the se"
N13-1070,S12-1042,1,0.816976,"us (CD),8 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [. . . ] CD-style scopes can be discontinuous and overlapping. Events are a portion of the scope that is semantically negated, with its truth value reversed by the negation cue. The NR system used in this work (Lapponi et al., 2012), one of the best performing systems in the *SEM shared task, is a CRF model for scope resolution that relies heavily on features extracted from dependency graphs. The feature model contains token distance, direction, n-grams of word forms, lemmas, POS and combinations thereof, as well as the syntactic features presented in Figure 4. The results in our 8 http://www.clips.ua.ac.be/sem2012-st-neg/data.html 620 Syntactic Cue-dependent constituent dependency relation parent head POS grand parent head POS word form+dependency relation POS+dependency relation directed dependency distance bidirection"
N13-1070,W07-0734,0,0.0110385,"best performing system in the WMT 2011 shared task on this dataset. The four experimental systems have reordering models that are trained on the first 25,000 sentences of the parallel news data that have been parsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with human judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 2.4 Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff-Davis corpus.12 A widely used baseline for sentence compression experiments is Knight and Marcu (2002), who introduce two models: the noisy-channel model and a decision tree-based model. Both are tree-based m"
N13-1070,J93-2004,0,0.0472942,"http://nlp.cs.lth.se/software/treebank converter/ ’oldLTH’ flag set. 618 ing on syntactic features, when possible, and to results in the literature, when comparable results exist. Note that negation resolution and SRL are not end applications. It is not easy to generalize across five very different tasks, but the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6 , and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work"
N13-1070,H05-1066,0,0.0310964,"Missing"
N13-1070,E06-1038,0,0.228356,"Missing"
N13-1070,S12-1035,0,0.0116767,"a lot of information, including the word form of the head, the dependent and the argument candidates, the concatenation of the dependency labels of the predicate, and the labeled dependency relations between predicate and its head, its arguments, dependents or siblings. 2.1 Negation resolution Negation resolution (NR) is the task of finding negation cues, e.g. the word not, and determining their scope, i.e. the tokens they affect. NR has recently seen considerable interest in the NLP community (Morante and Sporleder, 2012; Velldal et al., 2012) and was the topic of the 2012 *SEM shared task (Morante and Blanco, 2012). The data set used in this work, the Conan Doyle corpus (CD),8 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [. . . ] CD-style scopes can be discontinuous and overlapping. Events are a portion of the scope that is semantically negated, with its truth value reversed"
N13-1070,J12-2001,0,0.00610037,"triples (e.g. SUBJ(John, snore)) as features, while the semantic role labeling system conditions on a lot of information, including the word form of the head, the dependent and the argument candidates, the concatenation of the dependency labels of the predicate, and the labeled dependency relations between predicate and its head, its arguments, dependents or siblings. 2.1 Negation resolution Negation resolution (NR) is the task of finding negation cues, e.g. the word not, and determining their scope, i.e. the tokens they affect. NR has recently seen considerable interest in the NLP community (Morante and Sporleder, 2012; Velldal et al., 2012) and was the topic of the 2012 *SEM shared task (Morante and Blanco, 2012). The data set used in this work, the Conan Doyle corpus (CD),8 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [. . . ] CD-style scopes can be discontinuous and overlappin"
N13-1070,P02-1040,0,0.107319,"aseline system was one of the tied best performing system in the WMT 2011 shared task on this dataset. The four experimental systems have reordering models that are trained on the first 25,000 sentences of the parallel news data that have been parsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with human judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 2.4 Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff-Davis corpus.12 A widely used baseline for sentence compression experiments is Knight and Marcu (2002), who introduce two models: the noisy-channel model and a decision tr"
N13-1070,P11-1067,0,0.0598892,"in biomedical event extraction, but do not evaluate linguistic representations directly, evaluating representations and models jointly. Bender et al. (2011) compare several parsers across linguistic representations on a carefully designed evaluation set of hard, but relatively frequent syntactic constructions. They compare dependency parsers, constituent-based parsers and deep parsers. The authors argue in favor of evaluating parsers on diverse and richly annotated data. Others have discussed various ways of evaluating across annotation guidelines or translating structures to a common format (Schwartz et al., 2011; Tsarfaty et al., 2012). Hall et al. (2011) discuss optimizing parsers for specific down-stream applications, but consider only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic 619 Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practica"
N13-1070,C12-1147,0,0.104984,"r only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic 619 Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practical applications, and the heuristics used in the experiments assume a fixed set of dependency labels (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section d"
N13-1070,E12-1006,0,0.116337,"raction, but do not evaluate linguistic representations directly, evaluating representations and models jointly. Bender et al. (2011) compare several parsers across linguistic representations on a carefully designed evaluation set of hard, but relatively frequent syntactic constructions. They compare dependency parsers, constituent-based parsers and deep parsers. The authors argue in favor of evaluating parsers on diverse and richly annotated data. Others have discussed various ways of evaluating across annotation guidelines or translating structures to a common format (Schwartz et al., 2011; Tsarfaty et al., 2012). Hall et al. (2011) discuss optimizing parsers for specific down-stream applications, but consider only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic 619 Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practical applications, and the"
N13-1070,J12-2005,0,0.0246868,"e)) as features, while the semantic role labeling system conditions on a lot of information, including the word form of the head, the dependent and the argument candidates, the concatenation of the dependency labels of the predicate, and the labeled dependency relations between predicate and its head, its arguments, dependents or siblings. 2.1 Negation resolution Negation resolution (NR) is the task of finding negation cues, e.g. the word not, and determining their scope, i.e. the tokens they affect. NR has recently seen considerable interest in the NLP community (Morante and Sporleder, 2012; Velldal et al., 2012) and was the topic of the 2012 *SEM shared task (Morante and Blanco, 2012). The data set used in this work, the Conan Doyle corpus (CD),8 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [. . . ] CD-style scopes can be discontinuous and overlapping. Events are a portion"
N13-1070,N09-1028,0,0.0260543,"al applications, and the heuristics used in the experiments assume a fixed set of dependency labels (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section describes the applications and experimental set-ups included in this study. In the five applications considered below we use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relations as additional information about words and on a par with POS, our negation resolution system uses dependency paths, conditioning decisions on both dependency arcs and labels. In perspective classifica"
N13-1070,W02-1001,0,\N,Missing
N13-1070,J03-4003,0,\N,Missing
N13-1070,C10-1088,0,\N,Missing
N13-1077,W06-1615,0,0.381107,"Missing"
N13-1077,P07-1033,0,0.335922,"Missing"
N13-1077,P08-2060,0,0.183809,"et al., 2012). Adversarial learning techniques have been developed for security-related learning tasks, e.g. where systems need to be robust to failing sensors. We also show how we can do better than straight-forward ap668 Proceedings of NAACL-HLT 2013, pages 668–672, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics plication of adversarial learning techniques by making a second assumption about our data, namely that domains are mixtures of Zipfian distributions over our features. Similar assumptions have been made before in computational linguistics, e.g. by Goldberg and Elhadad (2008). 4 2d-fit 2d-fit with missing feature 1d-fit 2 0 −2 −4 −6 2 Approach overview −8 −10 In this paper we consider the structured perceptron (Collins, 2002) – with POS tagging as our practical application. The structured perceptron is prone to feature swamping (Sutton et al., 2006), and we want to prevent that using a technique inspired by adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008). The modification presented here to the structured perceptron only affects a single line of code in a publicly available implementation (see below), but the consequences are significant."
N13-1077,J93-2004,0,0.0419717,"ir (2008), but instead we randomly sample from a Zipfian and factor its inverse into our dataset. The parameter of the Zipfians is set (to 3) on development data (the EWT-email development data). The modified learning algorithm, Z3 SP, is presented in Figure 3. 5 POS tagging POS tagging is the problem of assigning syntactic categories or POS to tokenized word forms in running text. Most approaches to POS tagging use supervised learning to learn sequence labeling models from annotated ressources. The major ressource for English is the Wall Street Journal (WSJ) sections of the English Treebank (Marcus et al., 1993). POS taggers are usually trained on Sect. 0–18 and evaluated on Sect. 22–24. In this paper we are not interested in in-domain performance on WSJ data, but rather in developing a robust POS tagger that is less sensitive to domain shifts than current state-of-theart POS taggers and use the splits from a recent parsing shared task rather than the standard POS tagging ones. 6 Experiments We train our tagger on Sections 2–21 of the WSJ sections of the English Treebank, in the Ontotes 4.0 release. This was also the training data used in the experiments in the Parsing the Web (PTW) shared task at NA"
N13-1077,P07-1096,0,0.0490992,"Missing"
N13-1077,C12-2114,1,0.892937,"g feature 1d-fit 2 0 −2 −4 −6 2 Approach overview −8 −10 In this paper we consider the structured perceptron (Collins, 2002) – with POS tagging as our practical application. The structured perceptron is prone to feature swamping (Sutton et al., 2006), and we want to prevent that using a technique inspired by adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008). The modification presented here to the structured perceptron only affects a single line of code in a publicly available implementation (see below), but the consequences are significant. Online adversarial learning (Søgaard and Johannsen, 2012), briefly, works by sampling random corruptions of our data, or random feature deletions, in the learning phase. A discriminative learner seeing corrupted data points with missing features will not update part of the model and will thus try to find a decision boundary classifying the training data correctly relying on the remaining features. This decision boundary may be very different from the decision boundary found otherwise by the discriminative learner. If we sample enough corruptions, the model learned from the corrupted data will converge on the model minimizing average loss over all co"
N13-1077,N06-1012,0,0.0228132,"a, 9–14 June 2013. 2013 Association for Computational Linguistics plication of adversarial learning techniques by making a second assumption about our data, namely that domains are mixtures of Zipfian distributions over our features. Similar assumptions have been made before in computational linguistics, e.g. by Goldberg and Elhadad (2008). 4 2d-fit 2d-fit with missing feature 1d-fit 2 0 −2 −4 −6 2 Approach overview −8 −10 In this paper we consider the structured perceptron (Collins, 2002) – with POS tagging as our practical application. The structured perceptron is prone to feature swamping (Sutton et al., 2006), and we want to prevent that using a technique inspired by adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008). The modification presented here to the structured perceptron only affects a single line of code in a publicly available implementation (see below), but the consequences are significant. Online adversarial learning (Søgaard and Johannsen, 2012), briefly, works by sampling random corruptions of our data, or random feature deletions, in the learning phase. A discriminative learner seeing corrupted data points with missing features will not update part of the model"
N13-1077,W02-1001,0,\N,Missing
N15-1135,W06-1615,0,0.437228,"Missing"
N15-1135,J92-4003,0,0.658692,"obin Williams playing an adult Peter and Dustin Hoffman as the dastardly Captain Hook. Rooofiii Oooooo, didn’t think ppl<3the movie as much as me, this movie will always b the peter pan story2me #robin #williams #hook I loved that movie... Uhm... You know, Hook. With Robin Williams, uh. peter pan williams movie Table 1: Examples from source (top row) and target domains (bottom rows) spelling variations with the standard form (youuuuuuuu → you), which reduces the vocabulary size. For languages where no such normalization dictionary is available, we use word clusterings based on Brown clusters (Brown et al., 1992) to generalize tags from unambiguous words to previously unseen words in the same class. C LUSTER 01011110 01011110 01011110 01011110 01011110 01011110 01011110 T OKEN offish alreadyyy finali aleady previously already recently TAG ∈ D ADJ ??? ??? ??? ADV ADV ADV P ROJ . TAG — ADV ADV ADV — — — Figure 1: Example of a Brown cluster with unambiguous tokens, as well as projected tags for new tokens (tokens marked “—” are unchanged in D0 ). In particular, to extend the dictionary D to D0 using clusters, we first run clustering on the unlabeled data T , using Brown clustering.2 We then assign to eac"
N15-1135,P07-1033,0,0.668034,"Missing"
N15-1135,I11-1100,0,0.0495232,"Missing"
N15-1135,P11-2008,0,0.158516,"Missing"
N15-1135,P11-1038,0,0.0629429,"generalize across spelling variations and synonyms. Additionally, we evaluate our approach on Dutch, Portuguese and Spanish Twitter and present tow novel data sets for the latter two languages. 2 Data 2.1 Wiktionary In our experiments, we use the (unigram) tag dictionaries from Wiktionary, as collected by Li et al. (2012).1 The size and quality of our tag dictionaries crucially influence how much unambiguous data we can extract, and for some languages, the number of dictionary entries is small. We can resort to normalization dictionaries to extend Wiktionary’s coverage. We do so for English (Han and Baldwin, 2011). It replaces some 1 https://code.google.com/p/ wikily-supervised-pos-tagger/ 1256 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1256–1261, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics N EWSWIRE T WITTER S POKEN Q UERIES Spielberg took the helm of this big budget live action project with Robin Williams playing an adult Peter and Dustin Hoffman as the dastardly Captain Hook. Rooofiii Oooooo, didn’t think ppl<3the movie as much as me, this movie will always b the peter pan story2me #robin #will"
N15-1135,hovy-etal-2014-pos,1,0.86932,"guese, we use the training section of the Bosque treebank.7 For Spanish, we use the training section of the Cast3LB treebank.8 In order to map between Wiktionary and the treebanks, we need a common coarse tag set. We thus map all data to the universal tag set (Petrov et al., 2012). Dev and test sets Our approach is basically parameter free. However, we did experiment with different ways of extending Wiktionary and hence used an average over three English Twitter dev sections as development set (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011), all mapped and normalized following Hovy et al. (2014). For evaluation, we use three domains: tweets, spoken data and queries. For Twitter, we performed experiments in four languages: English, Portuguese, Spanish and Dutch. The Spanish and Portuguese tweets were annotated in-house, which will be made available.9 For the other languages, we use preexisting datasets for English (Hovy et al., 2014) and Dutch (Avontuur et al., 2012). Table 2 lists the complete statistics for the different language data sets. For the other two domains, we use the manually labeled data from Switchboard section 4 as spoken data test set. For queries, we use manually lab"
N15-1135,I05-1017,0,0.0340948,"from the set of licensed ones. In the second case, we assume the unknown word to be a NOUN, since unknown words mostly tend to be proper names. When added to newswire, this data results in worse models, presumably by introducing too much noise. However, for low-resource languages or domains with longer sentences and no available newswire data, this might be a viable alternative. 6 Related Work Our approach is similar to mining high-precision items. However, previous approaches on this in NLP have mainly focused on well-defined classification tasks, such as PP attachment (Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), or discourse connective disambiguation (Marcu and Echihabi, 2002). In contrast, we mine for sequences of unambiguous tokens in a structured prediction task. While we use the same dictionaries as in Li et al. (2012) and T¨ackstr¨om et al. (2013), our approach differs in several respects. First, we use Wiktionary to mine for training data, rather than as type constraints, and second, we use Brown clusters to extend Wiktionary. We did experiment with different ways of doing this, including using various forms of word embeddings, leading to models similar to the baseline models in Socher et al."
N15-1135,2005.mtsummit-papers.11,0,0.0607748,"r example, is only about 0.012 (or 1 in 84), and the distribution of tags in the Twitter data set is heavily skewed towards nouns, while several other labels are under-represented. Twitter We collect the unlabeled data from the Twitter streaming API.3 We collected 57m tweets for English, 8.2m for Spanish, 4.1m for Portuguese, and 0.5m for Dutch. We do not perform sentence splitting on tweets, but take them as unit sequences. Spoken language We use the Switchboard corpus of transcribed telephone conversations (Godfrey et al., 1992), sections 2 and 3, as well as the English section of EuroParl (Koehn, 2005) and CHILDES (MacWhinney, 1997). We removed all meta-data and inline annotations (gestures, sounds, etc.), as well as dialogue markers. The final joint corpus contains transcriptions of 570k spoken sentences. Search queries For search queries, we use a combination of queries from Yahoo4 and AOL. We only use the search terms and ignore any additional information, such as user ID, time, and linked URLs. The resulting data set contains 10m queries. 3 2 https://github.com/percyliang/ brown-cluster 4 1257 Unlabeled data https://github.com/saffsd/langid.py http://webscope.sandbox.yahoo.com/ 2.3 Labe"
N15-1135,D12-1127,0,0.0753564,"Missing"
N15-1135,P02-1047,0,0.0803701,"n word to be a NOUN, since unknown words mostly tend to be proper names. When added to newswire, this data results in worse models, presumably by introducing too much noise. However, for low-resource languages or domains with longer sentences and no available newswire data, this might be a viable alternative. 6 Related Work Our approach is similar to mining high-precision items. However, previous approaches on this in NLP have mainly focused on well-defined classification tasks, such as PP attachment (Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), or discourse connective disambiguation (Marcu and Echihabi, 2002). In contrast, we mine for sequences of unambiguous tokens in a structured prediction task. While we use the same dictionaries as in Li et al. (2012) and T¨ackstr¨om et al. (2013), our approach differs in several respects. First, we use Wiktionary to mine for training data, rather than as type constraints, and second, we use Brown clusters to extend Wiktionary. We did experiment with different ways of doing this, including using various forms of word embeddings, leading to models similar to the baseline models in Socher et al. (2013), but the approach based on Brown clusters led to the best re"
N15-1135,P09-1113,0,0.130157,"Missing"
N15-1135,N13-1039,0,0.203014,"Missing"
N15-1135,P00-1014,0,0.0692605,"omly at training time from the set of licensed ones. In the second case, we assume the unknown word to be a NOUN, since unknown words mostly tend to be proper names. When added to newswire, this data results in worse models, presumably by introducing too much noise. However, for low-resource languages or domains with longer sentences and no available newswire data, this might be a viable alternative. 6 Related Work Our approach is similar to mining high-precision items. However, previous approaches on this in NLP have mainly focused on well-defined classification tasks, such as PP attachment (Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), or discourse connective disambiguation (Marcu and Echihabi, 2002). In contrast, we mine for sequences of unambiguous tokens in a structured prediction task. While we use the same dictionaries as in Li et al. (2012) and T¨ackstr¨om et al. (2013), our approach differs in several respects. First, we use Wiktionary to mine for training data, rather than as type constraints, and second, we use Brown clusters to extend Wiktionary. We did experiment with different ways of doing this, including using various forms of word embeddings, leading to models similar to the ba"
N15-1135,petrov-etal-2012-universal,0,0.0266651,"ndbox.yahoo.com/ 2.3 Labeled data We train our models on newswire, as well as mined unambiguous instances. For English, we use the OntoNotes release of the WSJ section of the Penn Treebank as training data for Twitter, spoken data, and queries.5 For Dutch, we use the training section of the Alpino treebank from the CoNLL task.6 For Portuguese, we use the training section of the Bosque treebank.7 For Spanish, we use the training section of the Cast3LB treebank.8 In order to map between Wiktionary and the treebanks, we need a common coarse tag set. We thus map all data to the universal tag set (Petrov et al., 2012). Dev and test sets Our approach is basically parameter free. However, we did experiment with different ways of extending Wiktionary and hence used an average over three English Twitter dev sections as development set (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011), all mapped and normalized following Hovy et al. (2014). For evaluation, we use three domains: tweets, spoken data and queries. For Twitter, we performed experiments in four languages: English, Portuguese, Spanish and Dutch. The Spanish and Portuguese tweets were annotated in-house, which will be made available.9 For"
N15-1135,C14-1168,1,0.853249,"Missing"
N15-1135,D11-1141,0,0.0257816,"ries.5 For Dutch, we use the training section of the Alpino treebank from the CoNLL task.6 For Portuguese, we use the training section of the Bosque treebank.7 For Spanish, we use the training section of the Cast3LB treebank.8 In order to map between Wiktionary and the treebanks, we need a common coarse tag set. We thus map all data to the universal tag set (Petrov et al., 2012). Dev and test sets Our approach is basically parameter free. However, we did experiment with different ways of extending Wiktionary and hence used an average over three English Twitter dev sections as development set (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011), all mapped and normalized following Hovy et al. (2014). For evaluation, we use three domains: tweets, spoken data and queries. For Twitter, we performed experiments in four languages: English, Portuguese, Spanish and Dutch. The Spanish and Portuguese tweets were annotated in-house, which will be made available.9 For the other languages, we use preexisting datasets for English (Hovy et al., 2014) and Dutch (Avontuur et al., 2012). Table 2 lists the complete statistics for the different language data sets. For the other two domains, we use the manuall"
N15-1135,Q13-1001,0,0.0669642,"Missing"
N15-1135,P99-1023,0,0.0664404,"floresta_English.html 8 http://www.iula.upf.edu/recurs01_tbk_ uk.htm 9 http://lowlands.ku.dk/results 10 https://code.google.com/p/crfpp/ 6 1258 fault parameters. As baselines we consider a) a CRF model trained only on newswire; b) available off-the-shelf systems (T OOLS); and c) a weakly supervised model (L I 10). For English, the off-theshelf tagger is the Stanford tagger (Toutanova et al., 2003), for the other languages we use TreeTagger (Schmid, 1994) with pre-trained models. The weakly supervised model trained is on the unannotated data. It is a second-order HMM model (Mari et al., 1997; Thede and Harper, 1999) (SOHMM) using logistic regression to estimate the emission probabilities. This method allows us to use feature vectors rather than just word identity, as in standard HMMs. In addition, we constrain the inference space of the tagger using type-level tag constraints derived from Wiktionary. This model, called L I 10 in Table 3, was originally proposed by Li et al. (2012). We extend the model by adding continuous word representations, induced from the unlabeled data using the skip-gram algorithm (Mikolov et al., 2013), to the feature representations. Our logistic regression model thus works over"
N15-1135,N03-1033,0,0.0285312,"10). 3 Experiments 3.1 Model We use a CRF10 model (Lafferty et al., 2001) with the same features as Owoputi et al. (2013) and de5 LDC2011T03. http://www.let.rug.nl/˜vannoord/trees/ 7 http://www.linguateca.pt/floresta/info_ floresta_English.html 8 http://www.iula.upf.edu/recurs01_tbk_ uk.htm 9 http://lowlands.ku.dk/results 10 https://code.google.com/p/crfpp/ 6 1258 fault parameters. As baselines we consider a) a CRF model trained only on newswire; b) available off-the-shelf systems (T OOLS); and c) a weakly supervised model (L I 10). For English, the off-theshelf tagger is the Stanford tagger (Toutanova et al., 2003), for the other languages we use TreeTagger (Schmid, 1994) with pre-trained models. The weakly supervised model trained is on the unannotated data. It is a second-order HMM model (Mari et al., 1997; Thede and Harper, 1999) (SOHMM) using logistic regression to estimate the emission probabilities. This method allows us to use feature vectors rather than just word identity, as in standard HMMs. In addition, we constrain the inference space of the tagger using type-level tag constraints derived from Wiktionary. This model, called L I 10 in Table 3, was originally proposed by Li et al. (2012). We e"
N15-1152,N13-1070,1,0.847311,"Related Work Plank et al. (2014a) propose IAA-weighted costsensitive learning for POS tagging. We extend their line of work to dependency parsing. A single sentence can have more than one plausible dependency annotation. Some researchers have 1360 proposed evaluation metrics that do not penalize disagreements (Schwartz et al., 2011; Tsarfaty et al., 2011), while others have argued that we should instead ensure the consistency of treebanks (Dickinson, 2010; Manning, 2011; McDonald et al., 2013). Others have claimed that because of these ambiguities, only downstream evaluations are meaningful (Elming et al., 2013). Syntactic annotation disagreement has typically been studied in the context of treebank development. Haverinen et al. (2012), for example, analyze annotator disagreement for Finnish dependency syntax, and compare it against parser performance. Skjærholt (2014) use doubly-annotated data to evaluate various agreement metrics. Our paper differs from both lines of research in that we leverage disagreements from doubly-annotated data to obtain more robust models. While we agree that evaluation metrics should probably reflect disagreements, we show that our learning algorithms can indeed benefit f"
N15-1152,C12-1059,0,0.0219341,"l1 i and hh2 , l2 i count as disagreement, iff hj < i < h k . d) H EAD P OS: disagreement on head POS. That is, hh1 , l1 i and hh2 , l2 i count as disagreement, iff POS(h1 )6=POS(h2 ). e) H EAD P OS D, i.e., H EAD P OS, plus direction. That is, hh1 , l1 i and hh2 , l2 i count as disagreement, iff POS(h1 )6=POS(h2 ) or hj < i < hk . train 13.7k/209k 3.6k/70k 4.2k/74k 3.9k/73k 3.1k/79k 9.1k/123k Cost-sensitive updates We use the cost-sensitive perceptron classifier, following Plank et al. (2014a), but extend it to transition-based dependency parsing, where the predicted values are transitions (Goldberg and Nivre, 2012). Given a gold yi and predicted label yˆi (POS tags or transitions), the loss is weighted by γ(ˆ yi , yi ): Lw (ˆ yi , yi ) = γ(ˆ yi , yi ) max(0, −yi w · xi ) Whenever a transition has been wrongly predicted, we retrieve the predicted edge and compare it to the gold dependency to calculate γ. γ(yi , yj ) is then the inverse of the confusion probability estimated from our sample of doubly-annotated data. For example, using the factorization L ABEL, if the parser predicts wi to be S UBJECT and the gold annotation is O B JECT , the confusion probability is the number of times one annotator said"
N15-1152,P05-1013,0,0.133665,"Missing"
N15-1152,E14-1078,1,0.731472,"-annotated data, and involves two steps: i) how to factorize attachment or labeling disagreements; and ii) how to inform the parser of them during learning (§3). 2 Introduction Typically, NLP annotation projects employ guidelines to maximize inter-annotator agreement. Possible inconsistencies are resolved by adjudication, and models are induced assuming there is one single ground truth. However, there exist linguistically hard cases where there is no clear answer (Zeman, 2010; Manning, 2011), and incorporating such disagreements into the training of a model has proven helpful for POS tagging (Plank et al., 2014a; Plank et al., 2014b). Inter-annotator agreement (IAA) is straightforward to calculate for POS, but not for dependency trees. There is no well-established standard for computing agreement on trees (Skjærholt, 2014). Factorizations Assume a sample of sentences annotated by annotators A1 and A2 . With such a sample we can estimate probabilities of the two annotators’ disagreeing on the annotation of a word or span, relative to some dependency tree factorization. We factorize disagreement on dependency tree annotations relative to four properties of the annotated dependency edges: the POS of th"
N15-1152,P14-2083,1,0.854013,"-annotated data, and involves two steps: i) how to factorize attachment or labeling disagreements; and ii) how to inform the parser of them during learning (§3). 2 Introduction Typically, NLP annotation projects employ guidelines to maximize inter-annotator agreement. Possible inconsistencies are resolved by adjudication, and models are induced assuming there is one single ground truth. However, there exist linguistically hard cases where there is no clear answer (Zeman, 2010; Manning, 2011), and incorporating such disagreements into the training of a model has proven helpful for POS tagging (Plank et al., 2014a; Plank et al., 2014b). Inter-annotator agreement (IAA) is straightforward to calculate for POS, but not for dependency trees. There is no well-established standard for computing agreement on trees (Skjærholt, 2014). Factorizations Assume a sample of sentences annotated by annotators A1 and A2 . With such a sample we can estimate probabilities of the two annotators’ disagreeing on the annotation of a word or span, relative to some dependency tree factorization. We factorize disagreement on dependency tree annotations relative to four properties of the annotated dependency edges: the POS of th"
N15-1152,P11-1067,0,0.259652,"Missing"
N15-1152,P14-1088,1,0.930456,"guidelines to maximize inter-annotator agreement. Possible inconsistencies are resolved by adjudication, and models are induced assuming there is one single ground truth. However, there exist linguistically hard cases where there is no clear answer (Zeman, 2010; Manning, 2011), and incorporating such disagreements into the training of a model has proven helpful for POS tagging (Plank et al., 2014a; Plank et al., 2014b). Inter-annotator agreement (IAA) is straightforward to calculate for POS, but not for dependency trees. There is no well-established standard for computing agreement on trees (Skjærholt, 2014). Factorizations Assume a sample of sentences annotated by annotators A1 and A2 . With such a sample we can estimate probabilities of the two annotators’ disagreeing on the annotation of a word or span, relative to some dependency tree factorization. We factorize disagreement on dependency tree annotations relative to four properties of the annotated dependency edges: the POS of the dependent, the POS of the head, the label of the edge and the direction (left or right) of the head with regards to the dependent. This section describes the different factorizations. We present five factorizations"
N15-1152,solberg-etal-2014-norwegian,1,0.900114,"Missing"
N15-1152,D11-1036,0,0.0431991,"Missing"
N15-1152,P10-1075,0,\N,Missing
N15-1152,arias-etal-2014-boosting,0,\N,Missing
N15-1157,P14-2131,0,0.0145018,"2014; Baroni et al., 2014), there is little doubt that continuous word representations can potentially solve some of the data sparsity problems inherent in NLP. Most research on word embeddings has focused on learning representations for the words in a single language, making syntactically or semantically similar words appear close in the embedding space. Embeddings have been applied to many tasks, from ∗ The authors contributed equally to this work. The second author is funded by the ERC Starting Grant LOWLANDS No. 313695. named entity recognition (Turian et al., 2010) to dependency parsing (Bansal et al., 2014). It has furthermore been shown that weakly supervised embedding algorithms can also lead to huge improvements for tasks like sentiment analysis (Tang et al., 2014). In this work, we also use weak or distant supervision, relying on small dictionary seeds. This paper, however, considers the problem of learning bilingual word embeddings, i.e., word embeddings such that similar words in two different languages end up close in the embedding space. Such bilingual word embeddings can potentially be used for better cross-language transfer of NLP models, as we show in this paper. Previous work on bili"
N15-1157,P14-1023,0,0.0313617,") does not require parallel data, and (c) can be adapted to specific tasks by re-defining the equivalence classes. We show how our method outperforms off-the-shelf bilingual embeddings on the task of unsupervised cross-language partof-speech (POS) tagging, as well as on the task of semi-supervised cross-language super sense (SuS) tagging. 1 Introduction Using multi-layered neural networks to learn word embeddings has become standard in NLP (Turian et al., 2010; Guo et al., 2014). While there is still some controversy whether such methods are superior to older methods (Levy and Goldberg, 2014; Baroni et al., 2014), there is little doubt that continuous word representations can potentially solve some of the data sparsity problems inherent in NLP. Most research on word embeddings has focused on learning representations for the words in a single language, making syntactically or semantically similar words appear close in the embedding space. Embeddings have been applied to many tasks, from ∗ The authors contributed equally to this work. The second author is funded by the ERC Starting Grant LOWLANDS No. 313695. named entity recognition (Turian et al., 2010) to dependency parsing (Bansal et al., 2014). It h"
N15-1157,N10-1083,0,0.00914584,"ded by Klementiev et al. (2012) (Klmtv).6 Our results are displayed in Table 1. POS-X refer to X-dimensional BARISTA embeddings trained with POS equivalence classes, and Tr-X is X-dimensional BARISTA embeddings trained using translation equivalence classes. We note that random embeddings improve over our baseline, suggesting that the random features act as regularizers. The embeddings provided by Klementiev et al. (2012) seem to lead to worse performance than random embeddings, presumably because they capture mostly semantic (topic) similarity. We also compare our results to those reported by Berg-Kirkpatrick et al. (2010) (B-K) and Das and Petrov (2011) (DP), but note that their approaches require in-sample unlabeled data, and in the latter case, also parallel bilingual data. 4 https://sites.google.com/site/rmyeid/ projects/polyglot 5 https://github.com/coastalcph/rungsted 6 http://people.mmci.uni-saarland.de/ ˜aklement/data/distrib/ 1389 While training with POS classes improves over the random baseline, training with translation equivalence classes gives even better performance. For both approaches, using more embedding features improves the performance (500 dimensions did not improve significantly over 300)."
N15-1157,W06-1670,0,0.0115954,"/data/distrib/ 1389 While training with POS classes improves over the random baseline, training with translation equivalence classes gives even better performance. For both approaches, using more embedding features improves the performance (500 dimensions did not improve significantly over 300). Our model is generally better than Berg-Kirkpatrick et al. (2010) – but worse than Das and Petrov (2011). 3.3 Cross-language super sense tagging Finally, we also tried using the BARISTAembeddings for English-Danish (with parameters still set on Spanish POS) on another task, namely super sense tagging (Ciaramita and Altun, 2006). We train a system on a mixture of 1000 randomly sampled sentences from English SemCor7 and 320 labeled Danish sentences (see below) and compare using bilingual embeddings trained with equivalence classes from English and Danish wordnets (WN-300), to embeddings trained using translation equivalence classes (Tr-300). We use 300 dimensional embeddings. We use a POS-sensitive most frequent sense baseline (MFS), as well as structured perceptron model trained only with ortographic and POS features, as well as MFS features (Johannsen et al., 2014). Our metric is a weighted average over F1 -scores f"
N15-1157,P11-1061,0,0.13231,"apper method to existing monolingual word embedding algorithms that can learn task-specific bilingual embeddings, e.g., for POS tagging, named entity recognition, or sentiment analysis. Our algorithm is simpler and performs better on the tasks where we could compare performance to existing algorithms. Also, we note that our approach, unlike existing algorithms (Klementiev et al., 2012), is as fast as learning monolingual embeddings. Our contributions In this paper we introduce a new approach for learning bilingual word embeddings and revisit the task of unsupervised crosslanguage POS tagging (Das and Petrov, 2011). Our bilingual embedding model, which we call Bilingual Adaptive Reshuffling with Individual Stochastic Alternatives (BARISTA), takes two (non-parallel) corpora and a small dictionary as input. The dictio1386 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1386–1390, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics nary is essentially a list of words in the two languages that are equivalent with respect to some task, e.g., English car and French maison (‘house’) are both nouns, and hence “equivale"
N15-1157,D14-1012,0,0.0128914,"train off-the-shelf embedding models. Our model has the advantage that it (a) is independent of the choice of embedding algorithm, (b) does not require parallel data, and (c) can be adapted to specific tasks by re-defining the equivalence classes. We show how our method outperforms off-the-shelf bilingual embeddings on the task of unsupervised cross-language partof-speech (POS) tagging, as well as on the task of semi-supervised cross-language super sense (SuS) tagging. 1 Introduction Using multi-layered neural networks to learn word embeddings has become standard in NLP (Turian et al., 2010; Guo et al., 2014). While there is still some controversy whether such methods are superior to older methods (Levy and Goldberg, 2014; Baroni et al., 2014), there is little doubt that continuous word representations can potentially solve some of the data sparsity problems inherent in NLP. Most research on word embeddings has focused on learning representations for the words in a single language, making syntactically or semantically similar words appear close in the embedding space. Embeddings have been applied to many tasks, from ∗ The authors contributed equally to this work. The second author is funded by the"
N15-1157,P14-1006,0,0.0440581,"ly available at https: //github.com/gouwsmeister/barista. 2 Our approach Standard monolingual neural language models are unsupervised models that train on raw text, learning word features that enable the model to predict the next word (the target) from a sequence of words (the context). In the process, the model learns to cluster words into soft equivalence classes (words that have similar distributions). Several authors have proposed bilingual clustering and embedding algorithms based on parallel data (T¨ackstr¨om et al., 2012; Klementiev et al., 2012; Zou et al., 2013; Kocisky et al., 2014; Hermann and Blunsom, 2014b). These authors have all evaluated their embeddings on document classification and machine translation, and not yet structured prediction tasks like POS/SuS tagging or syntactic parsing. A notable exception is Hermann and Blunsom (2014a), who do not rely on parallel data and do not use word alignments, but they still use comparable data and sentence alignments, and they only evaluate their embeddings in document classification. The assumption that large amounts of parallel data exists for a language pair of interest is sometimes too strong (Hermann and Blunsom, 2014a). 1387 On the other hand"
N15-1157,S14-1001,1,0.70042,"l embeddings on Europarl using translations obtained from Google Translate. We derived a dictionary of the top 20k most frequent words in English, translated into German. The embeddings are shown in Fig 2. The visualizations show that the models are able to extract very finegrained bilingual relationships, and some clusters still correspond to POS. 3.2 Cross-language part-of-speech tagging Next we evaluated the embeddings in the context of unsupervised cross-language POS tagging (Das and Petrov, 2011). The goal is to train a tagger – in our case, we use S EARN (Daume et al., 2009), following (Johannsen et al., 2014)– on labeled English data decorated with bilingual embeddings, and then evaluate the model on another target language. We use data from Danish, German, Spanish, Italian, Dutch, Portuguese, and Swedish. Training and test data, which are the same used in Das and Petrov (2011), were converted to use the same 12 universal parts of speech proposed by Petrov et al. (2011). All results in this section were obtained by training bilingual embedding models on the publiclyLanguage TC-Perc Random Klmtv POS-50 POS-300 Tr-50 Tr-300 DP B-K Spanish German Danish Swedish Italian Dutch Portuguese 80.6 80.4 63 7"
N15-1157,C12-1089,0,0.823511,"et al., 2014). In this work, we also use weak or distant supervision, relying on small dictionary seeds. This paper, however, considers the problem of learning bilingual word embeddings, i.e., word embeddings such that similar words in two different languages end up close in the embedding space. Such bilingual word embeddings can potentially be used for better cross-language transfer of NLP models, as we show in this paper. Previous work on bilingual word embeddings have defined similar words as translation equivalents and evaluated embeddings in the context of document classification tasks (Klementiev et al., 2012; Kocisky et al., 2014). In this paper, we present a simple wrapper method to existing monolingual word embedding algorithms that can learn task-specific bilingual embeddings, e.g., for POS tagging, named entity recognition, or sentiment analysis. Our algorithm is simpler and performs better on the tasks where we could compare performance to existing algorithms. Also, we note that our approach, unlike existing algorithms (Klementiev et al., 2012), is as fast as learning monolingual embeddings. Our contributions In this paper we introduce a new approach for learning bilingual word embeddings an"
N15-1157,P14-2037,0,0.589572,"ork, we also use weak or distant supervision, relying on small dictionary seeds. This paper, however, considers the problem of learning bilingual word embeddings, i.e., word embeddings such that similar words in two different languages end up close in the embedding space. Such bilingual word embeddings can potentially be used for better cross-language transfer of NLP models, as we show in this paper. Previous work on bilingual word embeddings have defined similar words as translation equivalents and evaluated embeddings in the context of document classification tasks (Klementiev et al., 2012; Kocisky et al., 2014). In this paper, we present a simple wrapper method to existing monolingual word embedding algorithms that can learn task-specific bilingual embeddings, e.g., for POS tagging, named entity recognition, or sentiment analysis. Our algorithm is simpler and performs better on the tasks where we could compare performance to existing algorithms. Also, we note that our approach, unlike existing algorithms (Klementiev et al., 2012), is as fast as learning monolingual embeddings. Our contributions In this paper we introduce a new approach for learning bilingual word embeddings and revisit the task of u"
N15-1157,N12-1052,0,0.186611,"Missing"
N15-1157,Q13-1001,0,0.045606,"Missing"
N15-1157,P14-1146,0,0.0168082,"research on word embeddings has focused on learning representations for the words in a single language, making syntactically or semantically similar words appear close in the embedding space. Embeddings have been applied to many tasks, from ∗ The authors contributed equally to this work. The second author is funded by the ERC Starting Grant LOWLANDS No. 313695. named entity recognition (Turian et al., 2010) to dependency parsing (Bansal et al., 2014). It has furthermore been shown that weakly supervised embedding algorithms can also lead to huge improvements for tasks like sentiment analysis (Tang et al., 2014). In this work, we also use weak or distant supervision, relying on small dictionary seeds. This paper, however, considers the problem of learning bilingual word embeddings, i.e., word embeddings such that similar words in two different languages end up close in the embedding space. Such bilingual word embeddings can potentially be used for better cross-language transfer of NLP models, as we show in this paper. Previous work on bilingual word embeddings have defined similar words as translation equivalents and evaluated embeddings in the context of document classification tasks (Klementiev et"
N15-1157,P10-1040,0,0.0809736,"pairs that we use to train off-the-shelf embedding models. Our model has the advantage that it (a) is independent of the choice of embedding algorithm, (b) does not require parallel data, and (c) can be adapted to specific tasks by re-defining the equivalence classes. We show how our method outperforms off-the-shelf bilingual embeddings on the task of unsupervised cross-language partof-speech (POS) tagging, as well as on the task of semi-supervised cross-language super sense (SuS) tagging. 1 Introduction Using multi-layered neural networks to learn word embeddings has become standard in NLP (Turian et al., 2010; Guo et al., 2014). While there is still some controversy whether such methods are superior to older methods (Levy and Goldberg, 2014; Baroni et al., 2014), there is little doubt that continuous word representations can potentially solve some of the data sparsity problems inherent in NLP. Most research on word embeddings has focused on learning representations for the words in a single language, making syntactically or semantically similar words appear close in the embedding space. Embeddings have been applied to many tasks, from ∗ The authors contributed equally to this work. The second auth"
N15-1157,D13-1141,0,0.0525445,"uS tagging. The code will be made publicly available at https: //github.com/gouwsmeister/barista. 2 Our approach Standard monolingual neural language models are unsupervised models that train on raw text, learning word features that enable the model to predict the next word (the target) from a sequence of words (the context). In the process, the model learns to cluster words into soft equivalence classes (words that have similar distributions). Several authors have proposed bilingual clustering and embedding algorithms based on parallel data (T¨ackstr¨om et al., 2012; Klementiev et al., 2012; Zou et al., 2013; Kocisky et al., 2014; Hermann and Blunsom, 2014b). These authors have all evaluated their embeddings on document classification and machine translation, and not yet structured prediction tasks like POS/SuS tagging or syntactic parsing. A notable exception is Hermann and Blunsom (2014a), who do not rely on parallel data and do not use word alignments, but they still use comparable data and sentence alignments, and they only evaluate their embeddings in document classification. The assumption that large amounts of parallel data exists for a language pair of interest is sometimes too strong (He"
N16-1130,W15-3222,0,0.0247551,"can broaden the coverage of NLP tools, and serve as an important tool for large-scale sociolinguistic analyses of language use associated with AAVE (Jørgensen et al., 2015; Stewart, 2014), which relies on the accuracy of these NLP tools. We combine several recent trends in domain adaptation, namely word embeddings, clusters, sampling, and the use of type constraints. Word representations learned from representative unlabeled data, such as word clusters or embeddings, have been proven useful for increasing the accuracy of NLP tools for low-resource languages and domains (Owoputi et al., 2013; Aldarmaki and Diab, 2015; Gouws and Søgaard, 2015). Since similar words receive similar labels, this can give the model support for words not in the training data. In this paper, we use word clusters and word embeddings in both our baseline and system models. Using unlabeled data to estimate a target distribution for importance sampling, or for semi-supervised 1 https://github.com/brendano/ark-tweet-nlp/ tree/master/data/twpos-data-v0.3 1115 Proceedings of NAACL-HLT 2016, pages 1115–1120, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics learning (Søgaard, 2013), as well as wid"
N16-1130,W02-1001,0,0.0947009,"the tag dictionaries are used in an ambiguously supervised setting, and one where they are used as type constraints at prediction time in a self-training setup. Ambiguous supervision Our algorithm is related to work in cross-lingual transfer (Wisniewski et al., 2014; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and domain adaptation (Hovy et al., 2015a; Plank et al., 2014a), where tag dictionaries are used to filter projected annotation. We use the tag dictionaries to obtain partial labeling of in-domain training data. Our baseline sequence labeling algorithm is the structured perceptron (Collins, 2002). This algorithm performs additive updates passing over labeled data, comparing predicted sequences to gold standard sequences. If the predicted sequence is identical to the gold standard, no update is performed. We use a cost-sensitive structured perceptron (Plank et al., 2014b) to learn from the partially labeled data. Each update for a sequence can be broken down into a series of transition and emission updates, passing over the sequence item-by-item from left to right. For a word like hooch labeled VERB/NOUN/ADJ/PRON/ADV, we perform an update proportional to the cost associated with the pr"
N16-1130,P11-1061,0,0.458493,"per, we use word clusters and word embeddings in both our baseline and system models. Using unlabeled data to estimate a target distribution for importance sampling, or for semi-supervised 1 https://github.com/brendano/ark-tweet-nlp/ tree/master/data/twpos-data-v0.3 1115 Proceedings of NAACL-HLT 2016, pages 1115–1120, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics learning (Søgaard, 2013), as well as wide-coverage, crowd-sourced tag dictionaries to obtain more robust predictions for out-of-domain data have been succesfully used for domain adaptation (Das and Petrov, 2011; Hovy et al., 2015a; Li et al., 2012). In this paper, we use automatically-harvested tag dictionaries for the target variety(/-ies) in two different settings: for labeling the unlabeled data using a technique elaborating on previous work (Li et al., 2012; Wisniewski et al., 2014; Hovy et al., 2015a), and for imposing type constraints at test time in a semisupervised setting (Garrette and Baldridge, 2013; Plank et al., 2014a). Our best models are obtained using partially labeled training data created using tag dictionaries. Our contributions We present a POS tagger for AAVE-like language, mini"
N16-1130,W05-0708,0,0.0482738,"cost-sensitive learning, and (iii) an annotated corpus and trained POS tagger made publicly available at https:// bitbucket.org/soegaard/aave-pos16. 2 Data For historical reasons, most of the manually annotated corpora available today are newswire corpora. In contrast, very little data is available for domains such as subtitles, lyrics and tweets — especially for language varieties such as AAVE. Learning robust models for AAVE-like language and other language varieties is often further complicated by the absence of standard writing systems (Boujelbane et al., 2013; Bernhard and Ligozat, 2013; Duh and Kirchhoff, 2005). In this paper, we use three manually annotated data sets, consisting of subtitles from the television series The Wire, hip-hop lyrics from black American artists and tweets posted within the southeastern corner of the United States. We do not use this data for training, but only for evaluation, so our experiments use unsupervised (or weakly supervised) domain adaptation. Although the language use in the three domains 1116 vary, they have several things in common: the register is very informal, and the subtitles, lyrics and tweets contain slang terms such as loc’d out, cheesing with and po’,"
N16-1130,N13-1014,0,0.0192334,"inguistics learning (Søgaard, 2013), as well as wide-coverage, crowd-sourced tag dictionaries to obtain more robust predictions for out-of-domain data have been succesfully used for domain adaptation (Das and Petrov, 2011; Hovy et al., 2015a; Li et al., 2012). In this paper, we use automatically-harvested tag dictionaries for the target variety(/-ies) in two different settings: for labeling the unlabeled data using a technique elaborating on previous work (Li et al., 2012; Wisniewski et al., 2014; Hovy et al., 2015a), and for imposing type constraints at test time in a semisupervised setting (Garrette and Baldridge, 2013; Plank et al., 2014a). Our best models are obtained using partially labeled training data created using tag dictionaries. Our contributions We present a POS tagger for AAVE-like language, mining tag dictionaries from various websites and using them to create partially labeled data. Our contributions include: (i) a POS tagger that performs significantly better than existing tools on three datasets containing AAVE markers, (ii) a new domain adaptation algorithm combining ambiguous and cost-sensitive learning, and (iii) an annotated corpus and trained POS tagger made publicly available at https:"
N16-1130,P11-2008,0,0.0784066,"Missing"
N16-1130,N15-1157,1,0.843891,"of NLP tools, and serve as an important tool for large-scale sociolinguistic analyses of language use associated with AAVE (Jørgensen et al., 2015; Stewart, 2014), which relies on the accuracy of these NLP tools. We combine several recent trends in domain adaptation, namely word embeddings, clusters, sampling, and the use of type constraints. Word representations learned from representative unlabeled data, such as word clusters or embeddings, have been proven useful for increasing the accuracy of NLP tools for low-resource languages and domains (Owoputi et al., 2013; Aldarmaki and Diab, 2015; Gouws and Søgaard, 2015). Since similar words receive similar labels, this can give the model support for words not in the training data. In this paper, we use word clusters and word embeddings in both our baseline and system models. Using unlabeled data to estimate a target distribution for importance sampling, or for semi-supervised 1 https://github.com/brendano/ark-tweet-nlp/ tree/master/data/twpos-data-v0.3 1115 Proceedings of NAACL-HLT 2016, pages 1115–1120, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics learning (Søgaard, 2013), as well as wide-coverage, crowd-sourced"
N16-1130,P11-1038,0,0.0288951,"Missing"
N16-1130,W12-1905,1,0.843772,"ord embeddings from our unlabeled corpus, we use the Gensim im2 https://github.com/coastalcph/rungsted plementation of the word2vec algorithm (Mikolov et al., 2013b; Mikolov et al., 2013a). We also learn Brown clusters from a large corpus of tweets3 (Owoputi et al., 2013), and add both as additional features to our training and test sets. The word representations capture latent similarities between words, but more importantly enable our tagging model to generalize to unseen words. Partially labeled data Model performance generally benefits from additional data and constraints during training (Hovy and Hovy, 2012; T¨ackstr¨om et al., 2013). We therefore also use the unlabeled data and tag dictionaries as additional, partially labeled training data. For this purpose, we extract a tag dictionary for AAVE-like language from various crowdsourced online lexicons. Partial constraints from tag dictionaries have previously been used to filter out incorrect label sequences from projected labels from parallel corpora (Wisniewski et al., 2014; Das and Petrov, 2011; T¨ackstr¨om et al., 2013). We use a combination of a publicly available dump of Wiktionary4 (Li et al., 2012), entries from Hepster’s glossary of mus"
N16-1130,N15-1135,1,0.359389,"ers and word embeddings in both our baseline and system models. Using unlabeled data to estimate a target distribution for importance sampling, or for semi-supervised 1 https://github.com/brendano/ark-tweet-nlp/ tree/master/data/twpos-data-v0.3 1115 Proceedings of NAACL-HLT 2016, pages 1115–1120, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics learning (Søgaard, 2013), as well as wide-coverage, crowd-sourced tag dictionaries to obtain more robust predictions for out-of-domain data have been succesfully used for domain adaptation (Das and Petrov, 2011; Hovy et al., 2015a; Li et al., 2012). In this paper, we use automatically-harvested tag dictionaries for the target variety(/-ies) in two different settings: for labeling the unlabeled data using a technique elaborating on previous work (Li et al., 2012; Wisniewski et al., 2014; Hovy et al., 2015a), and for imposing type constraints at test time in a semisupervised setting (Garrette and Baldridge, 2013; Plank et al., 2014a). Our best models are obtained using partially labeled training data created using tag dictionaries. Our contributions We present a POS tagger for AAVE-like language, mining tag dictionaries"
N16-1130,W15-4302,1,0.811291,"ources of data exhibit a very high degree of linguistic variation, some of which is due to the dialects of the speakers or authors. In this paper, we use a corpus of POS-annotated tweets recently released by CMU,1 consisting of semi-randomly sampled US tweets. We want to use this corpus to learn a POS tagger for subtitles, lyrics, and tweets, which are typically associated with African-American Vernacular English (AAVE). We believe our POS tagger can broaden the coverage of NLP tools, and serve as an important tool for large-scale sociolinguistic analyses of language use associated with AAVE (Jørgensen et al., 2015; Stewart, 2014), which relies on the accuracy of these NLP tools. We combine several recent trends in domain adaptation, namely word embeddings, clusters, sampling, and the use of type constraints. Word representations learned from representative unlabeled data, such as word clusters or embeddings, have been proven useful for increasing the accuracy of NLP tools for low-resource languages and domains (Owoputi et al., 2013; Aldarmaki and Diab, 2015; Gouws and Søgaard, 2015). Since similar words receive similar labels, this can give the model support for words not in the training data. In this"
N16-1130,D12-1127,0,0.0509388,"Missing"
N16-1130,N13-1090,0,0.0250193,"V series The Wire and The Boondocks, English hip-hop lyrics, and tweets from the southeastern states of the US. None of the unlabeled data overlaps with our evaluation datasets. We use this corpus for two purposes: to induce word clusters and embeddings, and to partially annotate a portion of it automatically, which we include in the training data of our ambiguous supervision model (see Section 3 below). 3 Robust learning Word representations To learn word embeddings from our unlabeled corpus, we use the Gensim im2 https://github.com/coastalcph/rungsted plementation of the word2vec algorithm (Mikolov et al., 2013b; Mikolov et al., 2013a). We also learn Brown clusters from a large corpus of tweets3 (Owoputi et al., 2013), and add both as additional features to our training and test sets. The word representations capture latent similarities between words, but more importantly enable our tagging model to generalize to unseen words. Partially labeled data Model performance generally benefits from additional data and constraints during training (Hovy and Hovy, 2012; T¨ackstr¨om et al., 2013). We therefore also use the unlabeled data and tag dictionaries as additional, partially labeled training data. For t"
N16-1130,N13-1039,0,0.0853359,"Missing"
N16-1130,C14-1168,1,0.916716,"2013), as well as wide-coverage, crowd-sourced tag dictionaries to obtain more robust predictions for out-of-domain data have been succesfully used for domain adaptation (Das and Petrov, 2011; Hovy et al., 2015a; Li et al., 2012). In this paper, we use automatically-harvested tag dictionaries for the target variety(/-ies) in two different settings: for labeling the unlabeled data using a technique elaborating on previous work (Li et al., 2012; Wisniewski et al., 2014; Hovy et al., 2015a), and for imposing type constraints at test time in a semisupervised setting (Garrette and Baldridge, 2013; Plank et al., 2014a). Our best models are obtained using partially labeled training data created using tag dictionaries. Our contributions We present a POS tagger for AAVE-like language, mining tag dictionaries from various websites and using them to create partially labeled data. Our contributions include: (i) a POS tagger that performs significantly better than existing tools on three datasets containing AAVE markers, (ii) a new domain adaptation algorithm combining ambiguous and cost-sensitive learning, and (iii) an annotated corpus and trained POS tagger made publicly available at https:// bitbucket.org/soe"
N16-1130,E14-1078,1,0.932416,"2013), as well as wide-coverage, crowd-sourced tag dictionaries to obtain more robust predictions for out-of-domain data have been succesfully used for domain adaptation (Das and Petrov, 2011; Hovy et al., 2015a; Li et al., 2012). In this paper, we use automatically-harvested tag dictionaries for the target variety(/-ies) in two different settings: for labeling the unlabeled data using a technique elaborating on previous work (Li et al., 2012; Wisniewski et al., 2014; Hovy et al., 2015a), and for imposing type constraints at test time in a semisupervised setting (Garrette and Baldridge, 2013; Plank et al., 2014a). Our best models are obtained using partially labeled training data created using tag dictionaries. Our contributions We present a POS tagger for AAVE-like language, mining tag dictionaries from various websites and using them to create partially labeled data. Our contributions include: (i) a POS tagger that performs significantly better than existing tools on three datasets containing AAVE markers, (ii) a new domain adaptation algorithm combining ambiguous and cost-sensitive learning, and (iii) an annotated corpus and trained POS tagger made publicly available at https:// bitbucket.org/soe"
N16-1130,E14-3004,0,0.225998,"very high degree of linguistic variation, some of which is due to the dialects of the speakers or authors. In this paper, we use a corpus of POS-annotated tweets recently released by CMU,1 consisting of semi-randomly sampled US tweets. We want to use this corpus to learn a POS tagger for subtitles, lyrics, and tweets, which are typically associated with African-American Vernacular English (AAVE). We believe our POS tagger can broaden the coverage of NLP tools, and serve as an important tool for large-scale sociolinguistic analyses of language use associated with AAVE (Jørgensen et al., 2015; Stewart, 2014), which relies on the accuracy of these NLP tools. We combine several recent trends in domain adaptation, namely word embeddings, clusters, sampling, and the use of type constraints. Word representations learned from representative unlabeled data, such as word clusters or embeddings, have been proven useful for increasing the accuracy of NLP tools for low-resource languages and domains (Owoputi et al., 2013; Aldarmaki and Diab, 2015; Gouws and Søgaard, 2015). Since similar words receive similar labels, this can give the model support for words not in the training data. In this paper, we use wo"
N16-1130,Q13-1001,0,0.0502567,"Missing"
N16-1130,D14-1187,0,0.0381948,"Missing"
N16-1179,P11-1049,0,0.0609696,"Improving sentence compression by learning to predict gaze Sigrid Klerke University of Copenhagen skl@hum.ku.dk Yoav Goldberg Bar-Ilan University yoav.goldberg@gmail.com Abstract We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches. 1 Introduction Sentence compression is a basic operation in text simplification which has the potential to improve statistical machine translation and automatic summarization (Berg-Kirkpatrick et al., 2011; Klerke et al., 2015), as well as helping poor readers in need of assistive technologies (Canning et al., 2000). This work suggests using eye-tracking recordings for improving sentence compression for text simplification systems and is motivated by two observations: (i) Sentence compression is the task of automatically making sentences easier to process by shortening them. (ii) Eye-tracking measures such as first-pass reading time and time spent on regressions, i.e., during second and later passes over the text, are known to correlate with perceived text difficulty (Rayner et al., 2012). Thes"
N16-1179,P06-2019,0,0.0709165,"mpany said in a statement. Intel would be building car batteries Table 1: Example compressions from the G OOGLE dataset. S is the source sentence, and T is the target compression. Sents Sent.len Type/token Del.rate 0.22 0.21 0.17 0.59 0.27 0.87 0.55 0.27 0.42 0.47 0.29 0.87 T RAINING Z IFF -DAVIS B ROADCAST G OOGLE 1000 880 8000 20 20 24 T EST Z IFF -DAVIS B ROADCAST G OOGLE 32 412 1000 21 19 25 Table 2: Dataset characteristics. Sentence length is for source sentences. 4.2 Compression data We use three different sentence compression datasets, Z IFF -DAVIS (Knight and Marcu, 2002), B ROADCAST (Clarke and Lapata, 2006), and the publically available subset of G OOGLE (Filippova et al., 2015). The first two consist of manually compressed newswire text in English, while the third is built heuristically from pairs of headlines and first sentences from newswire, resulting in the most aggressive compressions, as exemplified in Table 1. We present the dataset characteristics in Table 2. We use the datasets as released by the authors and do not apply any additional pre-processing. The CCG supertagging data comes from CCGbank,1 and we use sections 0-18 for training and section 19 for development. 4.3 Baselines and s"
N16-1179,C08-1018,0,0.0355492,"this reader. See Table 1 for an example of first pass duration and regression duration annotations for one reader and sentence. Figure 2: Multitask and cascaded bi-LSTMs for sentence compression. Layer L−1 contain pre-trained embeddings. Gaze prediction and CCG-tag prediction are auxiliary training tasks, and loss on all tasks are propagated back to layer L0 . 3 Sentence compression using multi-task deep bi-LSTMs Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in their model (McDonald, 2006; Clarke and Lapata, 2008). Recently, however, Filippova et al. (2015) presented an approach to sentence compression using LSTMs with word embeddings, but without syntactic features. We introduce a third way of using syntactic annotation by jointly learning a sequence model for predicting CCG supertags, in addition to our gaze and compression models. Bi-directional recurrent neural networks (biRNNs) read in sequences in both regular and reversed order, enabling conditioning predictions on both left"
N16-1179,N13-1070,1,0.786538,"openhagen soegaard@hum.ku.dk Our proposed model does not require that the gaze data and the compression data come from the same source. Indeed, in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets. While not explored here, an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users, based on their reading behavior. Several approaches to sentence compression have been proposed, from noisy channel models (Knight and Marcu, 2002) over conditional random fields (Elming et al., 2013) to tree-to-tree machine translation models (Woodsend and Lapata, 2011). More recently, Filippova et al. (2015) successfully used LSTMs for sentence compression on a large scale parallel dataset. We do not review the literature here, and only compare to Filippova et al. (2015). Our contributions • We present a novel multi-task learning approach to sentence compression using labelled data for sentence compression and a disjoint eye-tracking corpus. • Our method is fully competitive with state-ofthe-art across three corpora. • Our code is made publicly available at https://bitbucket.org/soegaard"
N16-1179,W08-1105,0,0.0444923,"ord was read at most once by this reader. See Table 1 for an example of first pass duration and regression duration annotations for one reader and sentence. Figure 2: Multitask and cascaded bi-LSTMs for sentence compression. Layer L−1 contain pre-trained embeddings. Gaze prediction and CCG-tag prediction are auxiliary training tasks, and loss on all tasks are propagated back to layer L0 . 3 Sentence compression using multi-task deep bi-LSTMs Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in their model (McDonald, 2006; Clarke and Lapata, 2008). Recently, however, Filippova et al. (2015) presented an approach to sentence compression using LSTMs with word embeddings, but without syntactic features. We introduce a third way of using syntactic annotation by jointly learning a sequence model for predicting CCG supertags, in addition to our gaze and compression models. Bi-directional recurrent neural networks (biRNNs) read in sequences in both regular and reversed order, enabling conditioning p"
N16-1179,W15-2402,1,0.738867,"by learning to predict gaze Sigrid Klerke University of Copenhagen skl@hum.ku.dk Yoav Goldberg Bar-Ilan University yoav.goldberg@gmail.com Abstract We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches. 1 Introduction Sentence compression is a basic operation in text simplification which has the potential to improve statistical machine translation and automatic summarization (Berg-Kirkpatrick et al., 2011; Klerke et al., 2015), as well as helping poor readers in need of assistive technologies (Canning et al., 2000). This work suggests using eye-tracking recordings for improving sentence compression for text simplification systems and is motivated by two observations: (i) Sentence compression is the task of automatically making sentences easier to process by shortening them. (ii) Eye-tracking measures such as first-pass reading time and time spent on regressions, i.e., during second and later passes over the text, are known to correlate with perceived text difficulty (Rayner et al., 2012). These two observations rec"
N16-1179,D15-1168,0,0.0381961,"uts for location i. We then calculate the objective function derivative for the sequence using cross-entropy (logistic loss) and use backpropagation to calculate gradients and update the weights accordingly. A deep bi-RNN or klayered bi-RNN is composed of k bi-RNNs that feed into each other such that the output of the ith RNN is the input of the i + 1th RNN. LSTMs (Hochreiter and Schmidhuber, 1997) replace the cells of RNNs with LSTM cells, in which multiplicative gate units learn to open and close access to the error signal. Bi-LSTMs have already been used for finegrained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). These and other recent applications of bi-LSTMs were constructed for solving a single task in isolation, however. We instead train deep bi-LSTMs to solve additional tasks to sentence compression, namely CCG-tagging and gaze prediction, using the additional tasks to regularize our sentence compression model. Specifically, we use bi-LSTMs with three layers. Our baseline model is simply this three-layered 1530 model trained to predict compressions (encoded as label sequences), and we consider two extensions"
N16-1179,E06-1038,0,0.0421228,"one reader and sentence. Figure 2: Multitask and cascaded bi-LSTMs for sentence compression. Layer L−1 contain pre-trained embeddings. Gaze prediction and CCG-tag prediction are auxiliary training tasks, and loss on all tasks are propagated back to layer L0 . 3 Sentence compression using multi-task deep bi-LSTMs Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in their model (McDonald, 2006; Clarke and Lapata, 2008). Recently, however, Filippova et al. (2015) presented an approach to sentence compression using LSTMs with word embeddings, but without syntactic features. We introduce a third way of using syntactic annotation by jointly learning a sequence model for predicting CCG supertags, in addition to our gaze and compression models. Bi-directional recurrent neural networks (biRNNs) read in sequences in both regular and reversed order, enabling conditioning predictions on both left and right context. In the forward pass, we run the input data through an embedding layer and com"
N16-1179,N03-1026,0,0.0415312,"ure the value 0 indicates that the word was read at most once by this reader. See Table 1 for an example of first pass duration and regression duration annotations for one reader and sentence. Figure 2: Multitask and cascaded bi-LSTMs for sentence compression. Layer L−1 contain pre-trained embeddings. Gaze prediction and CCG-tag prediction are auxiliary training tasks, and loss on all tasks are propagated back to layer L0 . 3 Sentence compression using multi-task deep bi-LSTMs Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in their model (McDonald, 2006; Clarke and Lapata, 2008). Recently, however, Filippova et al. (2015) presented an approach to sentence compression using LSTMs with word embeddings, but without syntactic features. We introduce a third way of using syntactic annotation by jointly learning a sequence model for predicting CCG supertags, in addition to our gaze and compression models. Bi-directional recurrent neural networks (biRNNs) read in sequences in both regular a"
N16-1179,D11-1038,0,0.021181,"that the gaze data and the compression data come from the same source. Indeed, in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets. While not explored here, an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users, based on their reading behavior. Several approaches to sentence compression have been proposed, from noisy channel models (Knight and Marcu, 2002) over conditional random fields (Elming et al., 2013) to tree-to-tree machine translation models (Woodsend and Lapata, 2011). More recently, Filippova et al. (2015) successfully used LSTMs for sentence compression on a large scale parallel dataset. We do not review the literature here, and only compare to Filippova et al. (2015). Our contributions • We present a novel multi-task learning approach to sentence compression using labelled data for sentence compression and a disjoint eye-tracking corpus. • Our method is fully competitive with state-ofthe-art across three corpora. • Our code is made publicly available at https://bitbucket.org/soegaard/ gaze-mtl16. 2 Gaze during reading Readers fixate longer at rare words"
N16-1179,P15-1109,0,0.0332807,"using cross-entropy (logistic loss) and use backpropagation to calculate gradients and update the weights accordingly. A deep bi-RNN or klayered bi-RNN is composed of k bi-RNNs that feed into each other such that the output of the ith RNN is the input of the i + 1th RNN. LSTMs (Hochreiter and Schmidhuber, 1997) replace the cells of RNNs with LSTM cells, in which multiplicative gate units learn to open and close access to the error signal. Bi-LSTMs have already been used for finegrained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). These and other recent applications of bi-LSTMs were constructed for solving a single task in isolation, however. We instead train deep bi-LSTMs to solve additional tasks to sentence compression, namely CCG-tagging and gaze prediction, using the additional tasks to regularize our sentence compression model. Specifically, we use bi-LSTMs with three layers. Our baseline model is simply this three-layered 1530 model trained to predict compressions (encoded as label sequences), and we consider two extensions thereof as illustrated in Figure 2. Our first extension, M ULTI - TASK -LSTM, includes t"
N16-1179,D15-1042,0,\N,Missing
N18-1027,W16-0506,0,0.0134864,"he tweet is annotated as negative: grammatically correct sentence. The task has numerous applications for writing improvement and assessment, and recent work has focused on error detection as a supervised sequence labeling task (Rei and Yannakoudakis, 2016; Kaneko et al., 2017; Rei, 2017). Error detection can also be performed on the sentence level – detecting whether the sentence needs to be edited or not. Andersen et al. (2013) described a practical tutoring system that provides sentence-level feedback to language learners. The 2016 shared task on Automated Evaluation of Scientific Writing (Daudaravicius et al., 2016) also required participants to return binary predictions on whether the input sentence needs to be corrected. We evaluate our system on the First Certificate in English (FCE, Yannakoudakis et al. (2011)) dataset, containing error-annotated short essays written by language learners. While the original corpus is focused on aligned corrections, Rei and Yannakoudakis (2016) converted the dataset to a sequence labeling format, which we make use of here. An example from the dataset, with bold font indicating tokens that have been annotated as incorrect given the context: They may have a SuperBowl in"
N18-1027,D14-1080,0,0.0715659,"ce systems. Our results indicate that attention-based methods are able to predict token-level labels more accurately, compared to gradient-based methods, sometimes even rivaling the supervised oracle network. 1 Introduction Sequence labeling is a structured prediction task where systems need to assign the correct label to every token in the input sequence. Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detection, are often formulated as variations of sequence labeling. Recent state-of-the-art models make use of bidirectional LSTM architectures (Irsoy and Cardie, 2014), character-based representations (Lample et al., 2016), and additional external features (Peters et al., 2017). Optimization of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language. In this paper, we investigate the task of performing sequence labeling without having access to any training data with token-level annotation. Instead of training the model directly to predict the label for each token, the model is optimized using 2 Network Architecture T"
N18-1027,P16-1068,1,0.8665,"ing token-level labels, based on visualization methods using gradient analysis. Research in computer vision has shown that interpretable visualizations of convolutional networks can be obtained by analyzing the gradient after a single backpropagation pass through the network (Zeiler and Fergus, 2014). Denil et al. (2014) extended this approach to natural language processing, in order to find and visualize the most important sentences in a text. Recent work has also used the gradient-based approach for visualizing the decisions of text classification models on the token level (Li et al., 2016; Alikaniotis et al., 2016). In this section we propose an adaptation that can be used for sequence labeling tasks. We first perform a forward pass through the network and calculate the predicted sentence-level score y. Next, we define a pseudo-label y ∗ = 0, regardless of the true label of the sentence. We then calculate the gradient of the word representation wi with respect to the loss function using this pseudo-label: gi = ∂L1 ∂wi (y∗ ,y) 3.2 Relative Frequency Baseline The system for producing token-level predictions based on sentence-level training data does not necessarily need to be a neural network. As the init"
N18-1027,S14-2009,0,0.135007,"(14) where γ is used to control the importance of the auxiliary objectives. 3 Alternative Methods We compare the attention-based system for inferring sequence labeling with 3 alternative methods. 3.1 Labeling Through Backpropagation We experiment with an alternative method for inducing token-level labels, based on visualization methods using gradient analysis. Research in computer vision has shown that interpretable visualizations of convolutional networks can be obtained by analyzing the gradient after a single backpropagation pass through the network (Zeiler and Fergus, 2014). Denil et al. (2014) extended this approach to natural language processing, in order to find and visualize the most important sentences in a text. Recent work has also used the gradient-based approach for visualizing the decisions of text classification models on the token level (Li et al., 2016; Alikaniotis et al., 2016). In this section we propose an adaptation that can be used for sequence labeling tasks. We first perform a forward pass through the network and calculate the predicted sentence-level score y. Next, we define a pseudo-label y ∗ = 0, regardless of the true label of the sentence. We then calculate"
N18-1027,I17-1005,0,0.0124097,"s a whole. A single tweet could contain both positive and negative phrases, regardless of its overall polarity, and was therefore separately annotated on the tweet level. In the following example from the dataset, negative phrases are indicated with a bold font and positive phrases are marked with italics, whereas the overall sentiment of the tweet is annotated as negative: grammatically correct sentence. The task has numerous applications for writing improvement and assessment, and recent work has focused on error detection as a supervised sequence labeling task (Rei and Yannakoudakis, 2016; Kaneko et al., 2017; Rei, 2017). Error detection can also be performed on the sentence level – detecting whether the sentence needs to be edited or not. Andersen et al. (2013) described a practical tutoring system that provides sentence-level feedback to language learners. The 2016 shared task on Automated Evaluation of Scientific Writing (Daudaravicius et al., 2016) also required participants to return binary predictions on whether the input sentence needs to be corrected. We evaluate our system on the First Certificate in English (FCE, Yannakoudakis et al. (2011)) dataset, containing error-annotated short essa"
N18-1027,D15-1044,0,0.0509415,"raining data with token-level annotation. Instead of training the model directly to predict the label for each token, the model is optimized using 2 Network Architecture The main system takes as input a sentence, separated into tokens, and outputs a binary prediction as the label of the sentence. We use a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) architecture for sentence classification, with dynamic attention over words for constructing the sentence representations. Related architectures have been successful for machine translation (Bahdanau et al., 2015), sentence summarization (Rush and Weston, 2015), entailment detection (Rockt¨aschel et al., 2016), and error correction (Ji et al., 2017). In this work, we modify the attention mechanism and training objective in order to make the resulting network suitable for 293 Proceedings of NAACL-HLT 2018, pages 293–302 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics also inferring binary token labels, while still performing well as a sentence classifier. Figure 1 contains a diagram of the network architecture. The tokens are first mapped to a sequence of word representations [w1 , w2 , w3 , ..., wN ], which"
N18-1027,N16-1030,0,0.306041,"ods are able to predict token-level labels more accurately, compared to gradient-based methods, sometimes even rivaling the supervised oracle network. 1 Introduction Sequence labeling is a structured prediction task where systems need to assign the correct label to every token in the input sequence. Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detection, are often formulated as variations of sequence labeling. Recent state-of-the-art models make use of bidirectional LSTM architectures (Irsoy and Cardie, 2014), character-based representations (Lample et al., 2016), and additional external features (Peters et al., 2017). Optimization of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language. In this paper, we investigate the task of performing sequence labeling without having access to any training data with token-level annotation. Instead of training the model directly to predict the label for each token, the model is optimized using 2 Network Architecture The main system takes as input a sentence, separated int"
N18-1027,N16-1082,0,0.0244825,"method for inducing token-level labels, based on visualization methods using gradient analysis. Research in computer vision has shown that interpretable visualizations of convolutional networks can be obtained by analyzing the gradient after a single backpropagation pass through the network (Zeiler and Fergus, 2014). Denil et al. (2014) extended this approach to natural language processing, in order to find and visualize the most important sentences in a text. Recent work has also used the gradient-based approach for visualizing the decisions of text classification models on the token level (Li et al., 2016; Alikaniotis et al., 2016). In this section we propose an adaptation that can be used for sequence labeling tasks. We first perform a forward pass through the network and calculate the predicted sentence-level score y. Next, we define a pseudo-label y ∗ = 0, regardless of the true label of the sentence. We then calculate the gradient of the word representation wi with respect to the loss function using this pseudo-label: gi = ∂L1 ∂wi (y∗ ,y) 3.2 Relative Frequency Baseline The system for producing token-level predictions based on sentence-level training data does not necessarily need to be a"
N18-1027,D14-1162,0,0.0796777,"Missing"
N18-1027,P17-1161,0,0.0267294,"ly, compared to gradient-based methods, sometimes even rivaling the supervised oracle network. 1 Introduction Sequence labeling is a structured prediction task where systems need to assign the correct label to every token in the input sequence. Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detection, are often formulated as variations of sequence labeling. Recent state-of-the-art models make use of bidirectional LSTM architectures (Irsoy and Cardie, 2014), character-based representations (Lample et al., 2016), and additional external features (Peters et al., 2017). Optimization of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language. In this paper, we investigate the task of performing sequence labeling without having access to any training data with token-level annotation. Instead of training the model directly to predict the label for each token, the model is optimized using 2 Network Architecture The main system takes as input a sentence, separated into tokens, and outputs a binary prediction as the label o"
N18-1027,P11-1019,0,0.0605862,"ed sequence labeling task (Rei and Yannakoudakis, 2016; Kaneko et al., 2017; Rei, 2017). Error detection can also be performed on the sentence level – detecting whether the sentence needs to be edited or not. Andersen et al. (2013) described a practical tutoring system that provides sentence-level feedback to language learners. The 2016 shared task on Automated Evaluation of Scientific Writing (Daudaravicius et al., 2016) also required participants to return binary predictions on whether the input sentence needs to be corrected. We evaluate our system on the First Certificate in English (FCE, Yannakoudakis et al. (2011)) dataset, containing error-annotated short essays written by language learners. While the original corpus is focused on aligned corrections, Rei and Yannakoudakis (2016) converted the dataset to a sequence labeling format, which we make use of here. An example from the dataset, with bold font indicating tokens that have been annotated as incorrect given the context: They may have a SuperBowl in Dallas, but Dallas ain’t winning a SuperBowl. Not with that quarterback and owner. @S4NYC @RasmussenPoll Sentiment analysis is a three-way task, as the system needs to differentiate between positive, n"
N18-1027,P17-1194,1,0.943039,"..”). An example sentence from the dataset, with bold font indicating the hedge cue and curly brackets marking the scope of uncertainty: Supervised Sequence Labeling Finally, we also report the performance of a supervised sequence labeling model on the same tasks. This serves as an indicator of an upper bound for a given dataset – how well the system is able to detect relevant tokens when directly optimized for sequence labeling and provided with token-level annotation. We construct a bidirectional LSTM tagger, following the architectures from Irsoy and Cardie (2014), Lample et al. (2016) and Rei (2017). Character-based representations are concatenated with word embeddings, passed through a bidirectional LSTM, and the hidden states from both direction are concatenated. Based on this, a probability distribution over the possible labels is predicted and the most probable label is chosen for each word. While Lample et al. (2016) used a CRF on top of the network, we exclude it here as the token-level scores coming from that network do not necessarily reflect the individual labels, since the best label sequence is chosen globally based on the combined sentence-level score. The supervised model is"
N18-1027,P16-1112,1,0.875977,"ment detection of the tweet as a whole. A single tweet could contain both positive and negative phrases, regardless of its overall polarity, and was therefore separately annotated on the tweet level. In the following example from the dataset, negative phrases are indicated with a bold font and positive phrases are marked with italics, whereas the overall sentiment of the tweet is annotated as negative: grammatically correct sentence. The task has numerous applications for writing improvement and assessment, and recent work has focused on error detection as a supervised sequence labeling task (Rei and Yannakoudakis, 2016; Kaneko et al., 2017; Rei, 2017). Error detection can also be performed on the sentence level – detecting whether the sentence needs to be edited or not. Andersen et al. (2013) described a practical tutoring system that provides sentence-level feedback to language learners. The 2016 shared task on Automated Evaluation of Scientific Writing (Daudaravicius et al., 2016) also required participants to return binary predictions on whether the input sentence needs to be corrected. We evaluate our system on the First Certificate in English (FCE, Yannakoudakis et al. (2011)) dataset, containing error"
N18-1027,S15-2078,0,0.0659446,"Missing"
N18-1027,S13-2052,0,\N,Missing
N18-1172,E17-2026,1,0.796307,"ks, possibly accounting for its poor performance on the inference task. We did not evaluate the correlation between label embeddings and task performance, but Bjerva (2017) recently suggested that mutual information of target and auxiliary task label sets is a good predictor of gains from multi-task learning. 6.2 Auxilary Tasks For each task, we show the auxiliary tasks that achieved the best performance on the development data in Table 4. In contrast to most existing work, we did not restrict ourselves to performing multitask learning with only one auxiliary task (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017). Indeed we find that most often a combination of auxiliary tasks achieves the best performance. Indomain tasks are less used than we assumed; only Target is consistently used by all Twitter main tasks. In addition, tasks with a higher number of labels, e.g. Topic-5 are used more often. Such tasks provide a more fine-grained reward signal, which may help in learning representations that generalise better. Finally, tasks with large amounts Table 4: Best-performing auxiliary tasks for different main tasks. of training data such as FNC-1 and MultiNLI are also used more often. Even if not directly"
N18-1172,W17-0225,0,0.0664442,"o provides us with a picture of what auxilary tasks are beneficial, and to what extent we can expect synergies from multitask learning. For instance, the notion of positive sentiment appears to be very similar across the topic-based and aspect-based tasks, while the conceptions of negative and neutral sentiment differ. In addition, we can see that the model has failed to learn a relationship between MultiNLI labels and those of other tasks, possibly accounting for its poor performance on the inference task. We did not evaluate the correlation between label embeddings and task performance, but Bjerva (2017) recently suggested that mutual information of target and auxiliary task label sets is a good predictor of gains from multi-task learning. 6.2 Auxilary Tasks For each task, we show the auxiliary tasks that achieved the best performance on the development data in Table 4. In contrast to most existing work, we did not restrict ourselves to performing multitask learning with only one auxiliary task (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017). Indeed we find that most often a combination of auxiliary tasks achieves the best performance. Indomain tasks are less used than we assumed; only"
N18-1172,P17-1031,1,0.833536,"ed data and auxiliary, annotated datasets. We evaluate our approach on a variety of sequence classification tasks with disparate label spaces. We outperform strong single and multi-task baselines and achieve a new stateof-the-art for topic-based sentiment analysis. 1 Introduction Multi-task learning (MTL) and semi-supervised learning are both successful paradigms for learning in scenarios with limited labelled data and have in recent years been applied to almost all areas of NLP. Applications of MTL in NLP, for example, include partial parsing (Søgaard and Goldberg, 2016), text normalisation (Bollman et al., 2017), neural machine translation (Luong et al., 2016), and keyphrase boundary classification (Augenstein and Søgaard, 2017). Contemporary work in MTL for NLP typically focuses on learning representations that are useful across tasks, often through hard parameter sharing of hidden layers of neural networks (Collobert et al., 2011; Søgaard and Goldberg, 2016). If tasks share optimal hypothesis classes at the level of these representations, MTL leads to improvements (Baxter, 2000). However, while sharing hidden layers of neural networks is an effective regulariser (Søgaard and Goldberg, 2016), we pot"
N18-1172,S16-1044,0,0.060276,"Missing"
N18-1172,D16-1070,0,0.0255161,"can thus not be directly applied to our setting with tasks using disparate label sets. Multi-task learning with neural networks Recent work in multi-task learning goes beyond hard parameter sharing (Caruana, 1993) and considers different sharing structures, e.g. only sharing at lower layers (Søgaard and Goldberg, 2016) and induces private and shared subspaces (Liu et al., 2017; Ruder et al., 2017). These approaches, however, are not able to take into account relationships between labels that may aid in learning. Another related direction is to train on disparate annotations of the same task (Chen et al., 2016; Peng et al., 2017). In contrast, the different nature of our tasks requires a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers ha"
N18-1172,W17-5307,0,0.0309675,"Missing"
N18-1172,P14-2009,0,0.0469174,"o power at home, sat in the dark listening to AC/DC in the hope it’ll make the electricity come back again” known to be about the topic “AC/DC”, which is labelled as a positive sentiment. The evaluation metrics for Topic-2 and Topic-5 are macro-averaged recall (ρP N ) and macro-averaged mean absolute error (M AE M ) respectively, which are both averaged across topics. Target-dependent sentiment analysis Targetdependent sentiment analysis (Target) seeks to classify the sentiment of a text’s author towards an entity that occurs in the text as positive, negative, or neutral. We use the data from Dong et al. (2014). An example instance is the expression “how do you like settlers of catan for the wii?” which is labelled as neutral towards the target “wii’.’ The evaluation metric is macroaveraged F1 (F1M ). Fake news detection The goal of fake news detection in the context of the Fake News Challenge2 is to estimate whether the body of a news article agrees, disagrees, discusses, or is unrelated towards a headline. We use the data from the first stage of the Fake News Challenge (FNC-1). An example for this dataset is the document “Dino Ferrari hooked the whopper wels catfish, (...), which could be the bigg"
N18-1172,D16-1084,1,0.940888,"C in the hope it’ll make the electricity come back again Topic: AC/DC Label: positive Target-dependent sentiment analysis: Text: how do you like settlers of catan for the wii? Target: wii Label: neutral Aspect-based sentiment analysis: Text: For the price, you cannot eat this well in Manhattan Aspects: restaurant prices, food quality Label: positive Stance detection Stance detection (Stance) requires a model, given a text and a target entity, which might not appear in the text, to predict whether the author of the text is in favour or against the target or whether neither inference is likely (Augenstein et al., 2016). We use the data of SemEval-2016 Task 6 Subtask B (Mohammad et al., 2016). An example from this dataset would be to predict the stance of the tweet “Be prepared - if we continue the policies of the liberal left, we will be #Greece” towards the topic “Donald Trump”, labelled as “favor”. The evaluation metric is the macro-averaged F1 score of the “favour” and “against” classes (F1F A ). Stance detection: Tweet: Be prepared - if we continue the policies of the liberal left, we will be #Greece Target: Donald Trump Label: favor Fake news detection: Document: Dino Ferrari hooked the whopper wels ca"
N18-1172,W16-6208,1,0.846241,"we potentially loose synergies between the classification functions trained to associate these representations with class labels. This paper sets out to build an architecture in which such synergies are exploited, ? The first two authors contributed equally. with an application to pairwise sequence classification tasks. Doing so, we achieve a new state of the art on topic-based sentiment analysis. For many NLP tasks, disparate label sets are weakly correlated, e.g. part-of-speech tags correlate with dependencies (Hashimoto et al., 2017), sentiment correlates with emotion (Felbo et al., 2017; Eisner et al., 2016), etc. We thus propose to induce a joint label embedding space (visualised in Figure 2) using a Label Embedding Layer that allows us to model these relationships, which we show helps with learning. In addition, for tasks where labels are closely related, we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions. To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-labels across tasks. The LTN can be used to label unlabelled and auxiliary task"
N18-1172,S16-1010,0,0.0663037,"Missing"
N18-1172,D17-1169,1,0.779568,"and Goldberg, 2016), we potentially loose synergies between the classification functions trained to associate these representations with class labels. This paper sets out to build an architecture in which such synergies are exploited, ? The first two authors contributed equally. with an application to pairwise sequence classification tasks. Doing so, we achieve a new state of the art on topic-based sentiment analysis. For many NLP tasks, disparate label sets are weakly correlated, e.g. part-of-speech tags correlate with dependencies (Hashimoto et al., 2017), sentiment correlates with emotion (Felbo et al., 2017; Eisner et al., 2016), etc. We thus propose to induce a joint label embedding space (visualised in Figure 2) using a Label Embedding Layer that allows us to model these relationships, which we show helps with learning. In addition, for tasks where labels are closely related, we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions. To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-labels across tasks. The LTN can be used to label unlabel"
N18-1172,D17-1206,0,0.0278696,"ers of neural networks is an effective regulariser (Søgaard and Goldberg, 2016), we potentially loose synergies between the classification functions trained to associate these representations with class labels. This paper sets out to build an architecture in which such synergies are exploited, ? The first two authors contributed equally. with an application to pairwise sequence classification tasks. Doing so, we achieve a new state of the art on topic-based sentiment analysis. For many NLP tasks, disparate label sets are weakly correlated, e.g. part-of-speech tags correlate with dependencies (Hashimoto et al., 2017), sentiment correlates with emotion (Felbo et al., 2017; Eisner et al., 2016), etc. We thus propose to induce a joint label embedding space (visualised in Figure 2) using a Label Embedding Layer that allows us to model these relationships, which we show helps with learning. In addition, for tasks where labels are closely related, we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions. To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-la"
N18-1172,S16-1023,0,0.0416916,"Missing"
N18-1172,P17-1186,0,0.0246557,"rectly applied to our setting with tasks using disparate label sets. Multi-task learning with neural networks Recent work in multi-task learning goes beyond hard parameter sharing (Caruana, 1993) and considers different sharing structures, e.g. only sharing at lower layers (Søgaard and Goldberg, 2016) and induces private and shared subspaces (Liu et al., 2017; Ruder et al., 2017). These approaches, however, are not able to take into account relationships between labels that may aid in learning. Another related direction is to train on disparate annotations of the same task (Chen et al., 2016; Peng et al., 2017). In contrast, the different nature of our tasks requires a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using au"
N18-1172,P16-2067,1,0.828447,"f our tasks requires a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models. Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new. Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer. More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces. There has also been work on label transformations 3.1 Multi-task learning with disparate la"
N18-1172,P15-1046,0,0.0715286,"with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models. Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new. Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer. More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces. There has also been work on label transformations 3.1 Multi-task learning with disparate label spaces Problem definition In our multi-task learning scenario, we have access to labelled datasets for T tasks T1 , . . . , TT at training time with a target task TT that we particularly care about. The training dataset for task Ti consists of Nk examples XTi = {xT1 i , . . . , xTNik } and their Ti labels YTi = {y1Ti , . . . , yN }. Our base model is k a deep neural network that performs classic hard parameter"
N18-1172,S16-1174,0,0.0165112,"e often highly specialised, taskdependent architectures. Our architectures, in contrast, have not been optimised to compare favourably against the state of the art, as our main objective is to develop a novel approach to multi-task learning leveraging synergies between label sets and knowledge of marginal distributions from unlabeled data. For example, we do not use pre-trained word embeddings (Augenstein et al., 2016; Palogiannidi et al., 2016; Vo and Zhang, 2015), class weighting to deal with label imbalance (Balikas and Amini, 2016), or domainspecific sentiment lexicons (Brun et al., 2016; Kumar et al., 2016). Nevertheless, our approach outperforms the state-of-the-art on two-way topic-based sentiment analysis (Topic-2). The poor performance compared to the stateof-the-art on FNC and MultiNLI is expected; as we alternate among the tasks during training, our model only sees a comparatively small number of examples of both corpora, which are one and two orders of magnitude larger than the other datasets. For this reason, we do not achieve good performance on these tasks as main tasks, but they are still useful as auxiliary tasks as seen in Table 4. 6 6.1 Analysis Label Embeddings Our results above s"
N18-1172,P17-1001,0,0.0348797,"009), induce a shared prior (Yu et al., 2005; Xue et al., 2007; Daumé III, 2009), or learn a grouping (Kang et al., 2011; Kumar and Daumé III, 2012). These approaches focus on homogeneous tasks and employ linear or Bayesian models. They can thus not be directly applied to our setting with tasks using disparate label sets. Multi-task learning with neural networks Recent work in multi-task learning goes beyond hard parameter sharing (Caruana, 1993) and considers different sharing structures, e.g. only sharing at lower layers (Søgaard and Goldberg, 2016) and induces private and shared subspaces (Liu et al., 2017; Ruder et al., 2017). These approaches, however, are not able to take into account relationships between labels that may aid in learning. Another related direction is to train on disparate annotations of the same task (Chen et al., 2016; Peng et al., 2017). In contrast, the different nature of our tasks requires a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closel"
N18-1172,S16-1003,0,0.11624,"Missing"
N18-1172,P17-1194,0,0.0446297,"a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models. Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new. Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer. More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces. There has also been work on label transformations 3.1 Multi-task learning with disparate label spaces P"
N18-1172,N13-1008,0,0.0450068,"t of task Ti . In practice, we apply the same weight to all tasks. We show the full set-up in Figure 1a. 3.2 Label Embedding Layer In order to learn the relationships between labels, we propose a Label Embedding Layer (LEL) that embeds the labels of all tasks in a joint space. Instead of training separate softmax output layers as above, we introduce a label compatibility function c(·, ·) that measures how similar a label with embedding l is to the hidden representation h: c(l, h) = l · h (3) where · is the dot product. This is similar to the Universal Schema Latent Feature Model introduced by Riedel et al. (2013). In contrast to 1897 12/6/2017 multi-task_learning.html 12/6/2017 1  1 = (p ,y 1 ) 2  2 = (p ,y 2 ) 3  3 = (p label_embedding_layer.html ,y 3 i  i = (p ) 1 ∈ ℝ 2 L1 p L2 ∈ ℝ 3 p i ) l i p ,y 12/6/2017 p L3 ∈ ℝ 1 1 l Li ∈ ℝ 2 3 l label_transfer_network.html i  i = (p 2 1 l l ∈ ℝ l 1 2 ,y i ) 3 l 1 1 l 2 i  pseudo = p Li ∈ ℝ 2 3 l 2 1 l l ∈ ℝ l 3 2 1 2 l T MSE(p ,y T oi−1 ∈ ℝ ) ∗ l Label Embedding Layer Label Embedding Layer oi ∈ ℝ l oi+1 ∈ ℝ [⋅, ⋅] Label Transfer Network h h ∈ ℝ h h h ∈ ℝ h ∈ ℝ z i y ∈ Y i x i ∈ X i i (a) Multi-task learning y ∈ Y i"
N18-1172,D16-1103,1,0.852079,"able 1, and summarise examples in Table 2: Topic-based sentiment analysis Topic-based sentiment analysis aims to estimate the sentiment of a tweet known to be about a given topic. We use the data from SemEval-2016 Task 4 Subtask B and C (Nakov et al., 2016) for predicting on a twopoint scale of positive and negative (Topic-2) and five-point scale ranging from highly negative to highly positive (Topic-5) respectively. An example from this dataset would be to classify the 1899 whether an aspect, i.e. a particular property of an item is associated with a positive, negative, or neutral sentiment (Ruder et al., 2016). We use the data of SemEval-2016 Task 5 Subtask 1 Slot 3 (Pontiki et al., 2016) for the laptops (ABSA-L) and restaurants (ABSA-R) domains. An example is the sentence “For the price, you cannot eat this well in Manhattan”, labelled as positive towards both the aspects “restaurant prices” and “food quality”. The evaluation metric for both domains is accuracy (Acc). Topic-based sentiment analysis: Tweet: No power at home, sat in the dark listening to AC/DC in the hope it’ll make the electricity come back again Topic: AC/DC Label: positive Target-dependent sentiment analysis: Text: how do you lik"
N18-1172,D17-1038,1,0.926539,"onding label of the target task based on auxiliary predictions. To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-labels across tasks. The LTN can be used to label unlabelled and auxiliary task data by utilising the ‘dark knowledge’ (Hinton et al., 2015) contained in auxiliary model predictions. This pseudo-labelled data is then incorporated into the model via semisupervised learning, leading to a natural combination of multi-task learning and semi-supervised learning. We additionally augment the LTN with data-specific diversity features (Ruder and Plank, 2017) that aid in learning. Contributions Our contributions are: a) We model the relationships between labels by inducing a joint label space for multi-task learning. b) We propose a Label Transfer Network that learns to transfer labels between tasks and propose to use semi-supervised learning to leverage them for training. c) We evaluate MTL approaches on a variety of classification tasks and shed new light on settings where multi-task learning works. d) We perform an extensive ablation study of our model. 1896 Proceedings of NAACL-HLT 2018, pages 1896–1906 c New Orleans, Louisiana, June 1 - 6, 20"
N18-1172,P16-2038,1,0.857175,"eddings, enabling us to jointly leverage unlabelled data and auxiliary, annotated datasets. We evaluate our approach on a variety of sequence classification tasks with disparate label spaces. We outperform strong single and multi-task baselines and achieve a new stateof-the-art for topic-based sentiment analysis. 1 Introduction Multi-task learning (MTL) and semi-supervised learning are both successful paradigms for learning in scenarios with limited labelled data and have in recent years been applied to almost all areas of NLP. Applications of MTL in NLP, for example, include partial parsing (Søgaard and Goldberg, 2016), text normalisation (Bollman et al., 2017), neural machine translation (Luong et al., 2016), and keyphrase boundary classification (Augenstein and Søgaard, 2017). Contemporary work in MTL for NLP typically focuses on learning representations that are useful across tasks, often through hard parameter sharing of hidden layers of neural networks (Collobert et al., 2011; Søgaard and Goldberg, 2016). If tasks share optimal hypothesis classes at the level of these representations, MTL leads to improvements (Baxter, 2000). However, while sharing hidden layers of neural networks is an effective regul"
N18-1172,D12-1125,0,0.0236666,"ereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models. Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new. Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer. More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces. There has also been work on label transformations 3.1 Multi-task learning with disparate label spaces Problem definition In our multi-task learning scenario, we have access to labelled datasets for T tasks T1 , . . . , TT at training time with a target task TT that we particularly care about. The training dat"
N18-1172,W17-5301,0,\N,Missing
N18-1184,P16-2094,1,0.675472,"is data is used to analyze our grammatical competencies. Psycholinguists are typically interested in falsifying a specific hypothesis about our grammatical competencies and therefore collect data with this hypothesis in mind. In NLP, we typically require big, representative corpora. NLP usually has inLea Frermann carried out this work while at the University of Edinburgh. duced the models from expensive corpus annotations by professional linguists, but recently, a few researchers have shown that data traces from human processing can be used directly to improve NLP models (Klerke et al., 2016; Barrett et al., 2016; Plank, 2016). In this paper, we investigate whether unsupervised POS induction and unsupervised syntactic chunking can be improved using human text processing traces. We also explore what traces are beneficial, and how they are best combined. Our work supplements psycholinguistic research by evaluating human data on larger scale than usual, but more robust unsupervised POS induction also contributes to NLP for low-resource languages for which professional annotators are hard to find, and where instead, data from native speakers can be used to augment unsupervised learning. We explore three d"
N18-1184,K15-1038,1,0.8676,"reader (Rayner, 1998). Words from closed word classes are usually fixated less often and for shorter time than words from open word classes (Rayner and Duffy, 1988). Psycholinguistics, however, is generally not interested in covering all linguistic categories, and psycholinguists typically do not study corpora, but focus instead on small suites of controlled examples in order to explore human cognition. This is in contrast with NLP. Some studies have, however, tried to bridge between psycholinguistics and NLP. Demberg and Keller (2008) found that eye movements reflected syntactic complexity . Barrett and Søgaard (2015a) and Barrett and Søgaard (2015b) have tried to–respectively–predict a full set of syntactic classes and syntactic functions across domains in supervised setups. Barrett et al. (2016), which is the work most similar to ours, used eye-tracking features from the Dundee Corpus (Kennedy et al., 2003), which has been augmented with POS tags by Barrett et al. (2015). They tried for POS induction both on tokenlevel and type-level features. They found that eyetracking features significantly improved tagging accuracy and that type-level eye-tracking features helped more than token-level. We use the sa"
N18-1184,E14-1049,0,0.344721,"ind, and where instead, data from native speakers can be used to augment unsupervised learning. We explore three different modalities of data reflecting human processing plus standard, pretrained distributional word embeddings for comparison, but also because some modalities might fare better when combined with distributional vectors. Data reflecting human processing come from reading (two different eye-tracking corpora), speaking (prosody), and typing (keystroke logging). We test three different methods of combining the different word representations: a) canonical correlation analysis (CCA) (Faruqui and Dyer, 2014b) and b) singular value decomposision and inverted softmax feature projection (SVD+IS) (Smith et al., 2017) and c) simple concatenation of feature vectors. Contributions We present experiments in unsupervised POS and syntactic chunk induction using multi-modal word representations, obtained from records of reading, speaking, and writing. Individually, all modalities are known to contain syntactic processing signals, but to the best of our 2028 Proceedings of NAACL-HLT 2018, pages 2028–2038 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics knowledge, we"
N18-1184,W15-2401,1,0.828068,"reader (Rayner, 1998). Words from closed word classes are usually fixated less often and for shorter time than words from open word classes (Rayner and Duffy, 1988). Psycholinguistics, however, is generally not interested in covering all linguistic categories, and psycholinguists typically do not study corpora, but focus instead on small suites of controlled examples in order to explore human cognition. This is in contrast with NLP. Some studies have, however, tried to bridge between psycholinguistics and NLP. Demberg and Keller (2008) found that eye movements reflected syntactic complexity . Barrett and Søgaard (2015a) and Barrett and Søgaard (2015b) have tried to–respectively–predict a full set of syntactic classes and syntactic functions across domains in supervised setups. Barrett et al. (2016), which is the work most similar to ours, used eye-tracking features from the Dundee Corpus (Kennedy et al., 2003), which has been augmented with POS tags by Barrett et al. (2015). They tried for POS induction both on tokenlevel and type-level features. They found that eyetracking features significantly improved tagging accuracy and that type-level eye-tracking features helped more than token-level. We use the sa"
N18-1184,N10-1083,0,0.0188203,"VD+IS GECO Key Pros Concat Eig GECO Key chunks NW development as well as Penn Treebank development sets. For syntactic chunk induction, we use the bracketing data from Penn Treebank with the standard splits for syntactic chunking. We tune hyperparameters for chunking on the development set and select best models based on the development result. Model We used a modification of the implementation of a type-constrained, second-order hidden Markov model with maximum entropy emissions from Li et al. (2012) (SHMM-ME). It is a second-order version of the first order maximum entropy HMM presented in (Berg-Kirkpatrick et al., 2010) with the important addition that it is constrained by a crowd-sourced tag dictionary (Wiktionary). This means that for all words in the Wiktionary, the model is only allowed to predict one of the tags listed for it in Wiktionary The same model was used in Barrett et al. (2016) to improve unsupervised POS inducing using gaze data from the Dundee Corpus, and in Bingel et al. (2016) to augment an unsupervised POS tagger with features from fMRI recordings. The number of EM iterations used for inducing our taggers was tuned using eigenvector embeddings on the development data, considering values 1"
N18-1184,N13-1014,0,0.0517705,"Missing"
N18-1184,P16-1071,1,0.814525,"tion of a type-constrained, second-order hidden Markov model with maximum entropy emissions from Li et al. (2012) (SHMM-ME). It is a second-order version of the first order maximum entropy HMM presented in (Berg-Kirkpatrick et al., 2010) with the important addition that it is constrained by a crowd-sourced tag dictionary (Wiktionary). This means that for all words in the Wiktionary, the model is only allowed to predict one of the tags listed for it in Wiktionary The same model was used in Barrett et al. (2016) to improve unsupervised POS inducing using gaze data from the Dundee Corpus, and in Bingel et al. (2016) to augment an unsupervised POS tagger with features from fMRI recordings. The number of EM iterations used for inducing our taggers was tuned using eigenvector embeddings on the development data, considering values 1..50. PoS performance peaked at iterations 30 and 31. We use 30 in all our POS experiments. For syntactic chunking, we use 48 iterations, which led to the best performance on the PTB development data using only eigenword embeddings. 5.3 TA Best combined models Table 2: Heuristics for expanding our POS dictionary to 5.2 Feature set Wiktionary The Wiktionary constrains the predicted"
N18-1184,W15-0914,0,0.0137534,"M¨akisalo (2010) found that for English-Finnish translation and monolingual Finnish text production, predicate phrases are often preceded by short pauses, whereas adpositional phrases are more likely to be preceded by long pauses. Pauses preceding noun phrases grow with the length of the phrase. They suggest that the difference is explained by the processing of the predicate begins before the production of the clause starts, whereas noun phrases and adpositional phrases are processed during writing. Pre-word pauses from keystroke logs have been explored with respect to multi-word expressions (Goodkind and Rosenberg, 2015) and have also been used to aid shallow parsing (Plank, 2016) in a multi-task bi-LSTM setup. Prosodic features provide knowledge about how words are pronounced (tone, duration, voice etc.). Acoustic cues have already been used to improve unsupervised chunking (Pate and Goldwater, 2011) and parsing (Pate and Goldwater, 2013). Pate and Goldwater (2011) cluster the acoustic signal and use cluster label as a discrete feature whereas Pate and Goldwater (2013) use a quantized word duration feature. Plank (2016) and Goodkind and Rosenberg (2015) also used a single keystroke feature (keystroke pre-wor"
N18-1184,N16-1179,1,0.747528,"psycholinguistics this data is used to analyze our grammatical competencies. Psycholinguists are typically interested in falsifying a specific hypothesis about our grammatical competencies and therefore collect data with this hypothesis in mind. In NLP, we typically require big, representative corpora. NLP usually has inLea Frermann carried out this work while at the University of Edinburgh. duced the models from expensive corpus annotations by professional linguists, but recently, a few researchers have shown that data traces from human processing can be used directly to improve NLP models (Klerke et al., 2016; Barrett et al., 2016; Plank, 2016). In this paper, we investigate whether unsupervised POS induction and unsupervised syntactic chunking can be improved using human text processing traces. We also explore what traces are beneficial, and how they are best combined. Our work supplements psycholinguistic research by evaluating human data on larger scale than usual, but more robust unsupervised POS induction also contributes to NLP for low-resource languages for which professional annotators are hard to find, and where instead, data from native speakers can be used to augment unsupervised learni"
N18-1184,D12-1127,0,0.0555409,"Missing"
N18-1184,J93-2004,0,0.063601,"r training, development and test. We set all hyperparameters on the newswire (NW) domain, optimizing performance on the development set. Size of the development set is 154,146 tokens. We run individual experiments on each of the seven domains, with these hyper-parameters, reporting performance on the relevant test set. The domains are broadcast conversation (BC), broadcast news (BN), magazines (MZ), newswire (NW), the Bible (PT), telephone conversations (TC), and weblogs (WB). We also train and test unsupervised POS induction on the CoNLL 2007 (Nivre et al., 2007) splits of the Penn Treebank (Marcus et al., 1993) using the hyper-parameter settings from Ontonotes. We mapped all POS labels to Google’s coarse-grained, universal POS tagset (Petrov et al., 2012). For model selection, we select based both on best results on Ontonotes 2032 Rules DET VERB NOUN|PRONOUN|NUM . ADJ ADV PRT CONJ ADP → → → → → → → → → NP VP NP O NP|ADJP NP|VP|ADVP|AD NP|PRT O|NP PP|VP|SBAR No embeddings Eigenwords 60.32 59.26 CCA Dun GECO Pros SVD+IS GECO Key Pros Concat Eig GECO Key chunks NW development as well as Penn Treebank development sets. For syntactic chunk induction, we use the bracketing data from Penn Treebank with the"
N18-1184,W11-0603,0,0.0201418,"hrase. They suggest that the difference is explained by the processing of the predicate begins before the production of the clause starts, whereas noun phrases and adpositional phrases are processed during writing. Pre-word pauses from keystroke logs have been explored with respect to multi-word expressions (Goodkind and Rosenberg, 2015) and have also been used to aid shallow parsing (Plank, 2016) in a multi-task bi-LSTM setup. Prosodic features provide knowledge about how words are pronounced (tone, duration, voice etc.). Acoustic cues have already been used to improve unsupervised chunking (Pate and Goldwater, 2011) and parsing (Pate and Goldwater, 2013). Pate and Goldwater (2011) cluster the acoustic signal and use cluster label as a discrete feature whereas Pate and Goldwater (2013) use a quantized word duration feature. Plank (2016) and Goodkind and Rosenberg (2015) also used a single keystroke feature (keystroke pre-word pause) and the former study also discretized the feature. Our work, in contrast, uses acoustic and keystroke features as multidimensional, continuous word representations. 3 Modalities In our experiments, we begin with five sets of word representations: prosody, keystroke, gaze as re"
N18-1184,Q13-1006,0,0.019121,"is explained by the processing of the predicate begins before the production of the clause starts, whereas noun phrases and adpositional phrases are processed during writing. Pre-word pauses from keystroke logs have been explored with respect to multi-word expressions (Goodkind and Rosenberg, 2015) and have also been used to aid shallow parsing (Plank, 2016) in a multi-task bi-LSTM setup. Prosodic features provide knowledge about how words are pronounced (tone, duration, voice etc.). Acoustic cues have already been used to improve unsupervised chunking (Pate and Goldwater, 2011) and parsing (Pate and Goldwater, 2013). Pate and Goldwater (2011) cluster the acoustic signal and use cluster label as a discrete feature whereas Pate and Goldwater (2013) use a quantized word duration feature. Plank (2016) and Goodkind and Rosenberg (2015) also used a single keystroke feature (keystroke pre-word pause) and the former study also discretized the feature. Our work, in contrast, uses acoustic and keystroke features as multidimensional, continuous word representations. 3 Modalities In our experiments, we begin with five sets of word representations: prosody, keystroke, gaze as recorded in the GECO corpus, gaze as reco"
N18-1184,petrov-etal-2012-universal,0,0.105386,"Missing"
N18-1184,C16-1059,0,0.510384,"lyze our grammatical competencies. Psycholinguists are typically interested in falsifying a specific hypothesis about our grammatical competencies and therefore collect data with this hypothesis in mind. In NLP, we typically require big, representative corpora. NLP usually has inLea Frermann carried out this work while at the University of Edinburgh. duced the models from expensive corpus annotations by professional linguists, but recently, a few researchers have shown that data traces from human processing can be used directly to improve NLP models (Klerke et al., 2016; Barrett et al., 2016; Plank, 2016). In this paper, we investigate whether unsupervised POS induction and unsupervised syntactic chunking can be improved using human text processing traces. We also explore what traces are beneficial, and how they are best combined. Our work supplements psycholinguistic research by evaluating human data on larger scale than usual, but more robust unsupervised POS induction also contributes to NLP for low-resource languages for which professional annotators are hard to find, and where instead, data from native speakers can be used to augment unsupervised learning. We explore three different modal"
N18-2038,P15-2139,0,0.0330238,"training is to share all hidden parameters between different networks trained in parallel on different, but related datasets. The only requirement to the datasets is that they are defined in the same input space, and that there is a shared optimal hypothesis class for the shared parameters (Baxter, 2000), i.e., that there is a representation that is optimal for all the related tasks in question. Obvious extensions to this approach include sharing only parameters in specific layers (Søgaard and Goldberg, 2016; Misra et al., 2016), subspaces (Bousmalis et al., 2016), or doing only soft sharing (Duong et al., 2015), i.e., penalizing the `p distance between the models. In addition to a single-task recurrent neural network baseline, we use the approach in Søgaard and Goldberg (2016) where only initial layers are shared, as our baseline. Our approach to sluice resolution is largely inspired by the network archi4 Experiments Corpora We evaluate our models on two datasets, the newswire corpus introduced in Anand 237 in context. Our data is from the Wall Street Journal section of the English Penn Treebank, using the splits in the CONLL 2007 shared task (Nivre et al., 2007). Chunk-ing is a partial parsing task"
N18-2038,D16-1131,1,0.758725,"resolution in English dialogue. Introduction Sluices, also known as wh-fronted ellipses, are questions where the specification of what is asked for (beyond the wh-word), is elided (and thus needs to be retrieved from context). Below we distinguish two types of sluices: (i) embedded sluices, and (ii) root sluices. Embedded sluices occur in both single-authored texts and dialogue, while root sluices are particularly frequent in dialogue. 2 Related Work Anand and McCloskey (2015) introduced the problem of sluice resolution and presented the newswire corpus which we use in our experiments below. Anand and Hardt (2016) presented the first, and to the best of our knowledge only previous, sluice resolution system. They learn a linear combination of fifteen features across five feature groups, through a simple hill climbing procedure. Each (1) If [this is not practical], explain why. (2) A: [Jennifer is looking for you/me]. B: Why? Example 1 is an embedded sluice. In it, why is the remnant of the embedded question, which we understand to mean ’why this is not practical’. 1 In this work, we set aside cases where the discourse context does not provide an explicit antecedent. 236 Proceedings of NAACL-HLT 2018, pa"
N18-2038,W15-1621,0,0.0288375,"ages. In addition to the implementation of our architecture, which we make publicly available, we also make a new benchmark available for sluice resolution in English dialogue. Introduction Sluices, also known as wh-fronted ellipses, are questions where the specification of what is asked for (beyond the wh-word), is elided (and thus needs to be retrieved from context). Below we distinguish two types of sluices: (i) embedded sluices, and (ii) root sluices. Embedded sluices occur in both single-authored texts and dialogue, while root sluices are particularly frequent in dialogue. 2 Related Work Anand and McCloskey (2015) introduced the problem of sluice resolution and presented the newswire corpus which we use in our experiments below. Anand and Hardt (2016) presented the first, and to the best of our knowledge only previous, sluice resolution system. They learn a linear combination of fifteen features across five feature groups, through a simple hill climbing procedure. Each (1) If [this is not practical], explain why. (2) A: [Jennifer is looking for you/me]. B: Why? Example 1 is an embedded sluice. In it, why is the remnant of the embedded question, which we understand to mean ’why this is not practical’. 1"
N18-2038,D14-1162,0,0.0838091,"of the auxiliary tasks in terms of abstractness. The architecture learns part of speech (POS) tagging at the initial layer; then syntactic chunking, then combinatory categorial grammar (CCG) supertags, before learning sluice resolution at the outer layer. See Figure 1 for a diagram of our architecture. We train our architecture by sampling from all our tasks with equal probability. The instance loss is computed at the appropriate level of the network, and backpropagation will only affect the previous levels. All our neural networks use 50 dimensional pre-trained GloVe embeddings, trained by (Pennington et al., 2014) on Wikipedia and Gigaword 5. The word embeddings are not updated during training. Similarly, all our networks were trained for 30 epochs. They all use ZoneOut (Krueger et al., 2016) regularization with Z-state 0 and Z-cell 0.2 (except the single-task baseline, which used Z-cell 0.0), batches of 10 examples and are optimized using the Adam optimizer (Kingma and Ba, 2014) with initial learning rate 0.001 (except the single-task baseline, which used a learning rate of 0.01). All LSTMs contain 64 hidden units. All additional hyper-parameters were tuned manually. Our work builds on recent progress"
N18-2038,L18-1249,1,0.798486,"i.e., the semantic overlap between the candidate and the sluice, and v) correlate, i.e., semantic properties of the candidate, which may be predictive of sluice type (temporal, reason, degree, etc.). The linear model ranks all candidates and resolves a sluice by choosing the highest ranking candidate. Anand and Hardt (2016) use a slightly different metric than we do, because they rank syntactic subtrees that are potential antecedents, rather than labeling individual words in sequence. See §4. This paper is, to the best of our knowledge, the first to consider sluice resolution in dialogue, but Baird et al. (2018) consider sluice type classification in dialogue data. 3 Our approach Our approach is an extension of previous work on multi-task learning, largely inspired by Hashimoto et al. (2016). We construct a neural architecture based on recurrent neural networks (Hochreiter and Schmidhuber, 1997), which differ only from the architectures discussed above in using label embeddings that are also passed on to subsequent layers, skip connections from the embedding layer, and regularization. The stacking on label embeddings from auxiliary tasks makes our approach similar to stacked learning (Wolpert, 1992)"
N18-2038,P16-2038,1,0.939048,"rueger et al., 2016) regularization with Z-state 0 and Z-cell 0.2 (except the single-task baseline, which used Z-cell 0.0), batches of 10 examples and are optimized using the Adam optimizer (Kingma and Ba, 2014) with initial learning rate 0.001 (except the single-task baseline, which used a learning rate of 0.01). All LSTMs contain 64 hidden units. All additional hyper-parameters were tuned manually. Our work builds on recent progress in multitask training of neural networks. Multi-task training of neural networks goes back to Caruana (1993), but was popularized by Collobert et al. (2011) and Søgaard and Goldberg (2016). The most common approach to multi-task training is to share all hidden parameters between different networks trained in parallel on different, but related datasets. The only requirement to the datasets is that they are defined in the same input space, and that there is a shared optimal hypothesis class for the shared parameters (Baxter, 2000), i.e., that there is a representation that is optimal for all the related tasks in question. Obvious extensions to this approach include sharing only parameters in specific layers (Søgaard and Goldberg, 2016; Misra et al., 2016), subspaces (Bousmalis et"
N18-2038,W00-0726,0,0.188717,"Missing"
N19-1142,D16-1084,1,0.929529,"Missing"
N19-1142,N18-1172,1,0.852195,"y describe both setups in the following. An overview over tasks and data used in the different models is shown in Table 3. Multi-Task Learning To exploit synergies between additional datasets/annotations, we explore a simple multi-task learning with hard parameter sharing strategy, pioneered by Caruana (1993), introduced in the context of NLP by Collobert et al. (2011), and to RNNs by Søgaard and Goldberg (2016), which has been shown to be useful for a variety of NLP tasks, e.g. sequence labelling (Rei, 2017; Ruder et al., 2019; Augenstein and Søgaard, 2017), pairwise sequence classification (Augenstein et al., 2018) or machine translation (Dong et al., 2015). Here, parameters are shared between hidden layers. Intuitively, it works by training several networks in parallel, tying a subset of the hidden parameters so that updates in one network affect the parameters of the others. By sharing parameters, the networks regularize each other, and the network for one task can benefit from repre1403 1 5 6 7 13 Figure 1: Overview over the multi-task model (left) and the adversarial model (right). The baseline LSTM model corresponds to the same architecture with only one task. Figure 2: Improvement in F-score over"
N19-1142,P17-2054,1,0.857426,"t domain, multi-task learning and adversarial learning. We briefly describe both setups in the following. An overview over tasks and data used in the different models is shown in Table 3. Multi-Task Learning To exploit synergies between additional datasets/annotations, we explore a simple multi-task learning with hard parameter sharing strategy, pioneered by Caruana (1993), introduced in the context of NLP by Collobert et al. (2011), and to RNNs by Søgaard and Goldberg (2016), which has been shown to be useful for a variety of NLP tasks, e.g. sequence labelling (Rei, 2017; Ruder et al., 2019; Augenstein and Søgaard, 2017), pairwise sequence classification (Augenstein et al., 2018) or machine translation (Dong et al., 2015). Here, parameters are shared between hidden layers. Intuitively, it works by training several networks in parallel, tying a subset of the hidden parameters so that updates in one network affect the parameters of the others. By sharing parameters, the networks regularize each other, and the network for one task can benefit from repre1403 1 5 6 7 13 Figure 1: Overview over the multi-task model (left) and the adversarial model (right). The baseline LSTM model corresponds to the same architectur"
N19-1142,N16-1138,0,0.0231077,"evoking in political news articles with hand-crafted features. Field et al. (2018) analyse how Russian news articles frame the U.S. using a keyword-based cross-lingual projection setup. Tsur et al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011), detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al., 2016; Augenstein et al., 2016; Ferreira and Vlachos, 2016). 2 Online Discussion Annotations We create a new resource of issue-frame annotated online fora discussions, by annotating a subset of the Argument Extraction Corpus (Swanson et al., 2015) with a subset of the frames in the Policy Frames Codebook. The Argument Extraction 1 Code and annotations are available at https:// github.com/coastalcph/issue_framing. Corpus is a collection of argumentative dialogues across topics and platforms.2 The corpus contains posts on the following topics: gay marriage, gun control, death penalty and evolution. A subset of the corpus was annotated with argument qual"
N19-1142,D18-1393,0,0.0563443,"ructure into a recursive neural network. Naderi and Hirst (2017) use the same resource, but make predictions at the sentence level, using topic models and recurrent neural networks. Johnson et al. (2017) predict frames in social media data at the micro-post level, using probabilistic soft logic based on lists of keywords, as well as temporal similarity and network structure. All the work mentioned above uses the generic frames of Boydstun et al. (2014)’s Policy Frames Codebook. Baumer et al. (2015) predict words perceived as frame-evoking in political news articles with hand-crafted features. Field et al. (2018) analyse how Russian news articles frame the U.S. using a keyword-based cross-lingual projection setup. Tsur et al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011), detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al., 2016; Augenstein et al., 2016; Ferreira and Vlachos, 2016). 2 Online Discussion Annotations We create a new resou"
N19-1142,N15-1171,0,0.0255579,"-based features in a logistic regression model. Ji and Smith (2017) improve on previous work integrating discourse structure into a recursive neural network. Naderi and Hirst (2017) use the same resource, but make predictions at the sentence level, using topic models and recurrent neural networks. Johnson et al. (2017) predict frames in social media data at the micro-post level, using probabilistic soft logic based on lists of keywords, as well as temporal similarity and network structure. All the work mentioned above uses the generic frames of Boydstun et al. (2014)’s Policy Frames Codebook. Baumer et al. (2015) predict words perceived as frame-evoking in political news articles with hand-crafted features. Field et al. (2018) analyse how Russian news articles frame the U.S. using a keyword-based cross-lingual projection setup. Tsur et al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011), detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al"
N19-1142,E17-2026,1,0.902594,"Missing"
N19-1142,P17-1092,0,0.0373499,"distribution in the online discussion test set. The frame labels correspond to the classes Economic (1), Political (13), Legality, Jurisprudence and Constitutionality (5), Policy prescription and evaluation (6) and Crime and Punishment (7). multi-task and adversarial learning, leading to improved results in the target domain.1 Related Work Previous work on automatic frame classification focused on news articles and social media. Card et al. (2016) predict frames in news articles at the document level, using clusters of latent dimensions and word-based features in a logistic regression model. Ji and Smith (2017) improve on previous work integrating discourse structure into a recursive neural network. Naderi and Hirst (2017) use the same resource, but make predictions at the sentence level, using topic models and recurrent neural networks. Johnson et al. (2017) predict frames in social media data at the micro-post level, using probabilistic soft logic based on lists of keywords, as well as temporal similarity and network structure. All the work mentioned above uses the generic frames of Boydstun et al. (2014)’s Policy Frames Codebook. Baumer et al. (2015) predict words perceived as frame-evoking in po"
N19-1142,P15-2072,0,0.369883,"inion (Dardis et al., 2008; Iyengar, 1991). As an illustration, contrast the statement Illegal workers depress wages with This country is abusing and terrorizing undocumented immigrant workers. The first statement puts focus on the economic consequences of immigration, whereas the second one evokes a morality frame by pointing out the inhumane conditions under which immigrants may have to work. Being exposed to primarily one of those perspectives might affect the publics attitude towards immigration. Computational methods for frame classification have previously been studied in news articles (Card et al., 2015) and social media posts (Johnson et al., 2017). In this work, we introduce a new benchmark dataset, based on a subset of the 15 generic frames in the Policy Frames Codebook by Boydstun et al. (2014). We focus on frame classification in online discussion fora, which have beLegality Frame, Topic: Same sex marriage Congress must fight to ensure LGBT people have the full protection of the law everywhere in America. #EqualityAct Table 1: Example instances from the datasets described in §2 and 3. come crucial platforms for public dialogue on social and political issues. Table 1 shows example annotat"
N19-1142,D16-1148,0,0.0133088,"01–1407 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Frames # instances 1 78 13 96 5 234 6 166 7 186 Table 2: Class distribution in the online discussion test set. The frame labels correspond to the classes Economic (1), Political (13), Legality, Jurisprudence and Constitutionality (5), Policy prescription and evaluation (6) and Crime and Punishment (7). multi-task and adversarial learning, leading to improved results in the target domain.1 Related Work Previous work on automatic frame classification focused on news articles and social media. Card et al. (2016) predict frames in news articles at the document level, using clusters of latent dimensions and word-based features in a logistic regression model. Ji and Smith (2017) improve on previous work integrating discourse structure into a recursive neural network. Naderi and Hirst (2017) use the same resource, but make predictions at the sentence level, using topic models and recurrent neural networks. Johnson et al. (2017) predict frames in social media data at the micro-post level, using probabilistic soft logic based on lists of keywords, as well as temporal similarity and network structure. All t"
N19-1142,P15-1166,0,0.035307,"iew over tasks and data used in the different models is shown in Table 3. Multi-Task Learning To exploit synergies between additional datasets/annotations, we explore a simple multi-task learning with hard parameter sharing strategy, pioneered by Caruana (1993), introduced in the context of NLP by Collobert et al. (2011), and to RNNs by Søgaard and Goldberg (2016), which has been shown to be useful for a variety of NLP tasks, e.g. sequence labelling (Rei, 2017; Ruder et al., 2019; Augenstein and Søgaard, 2017), pairwise sequence classification (Augenstein et al., 2018) or machine translation (Dong et al., 2015). Here, parameters are shared between hidden layers. Intuitively, it works by training several networks in parallel, tying a subset of the hidden parameters so that updates in one network affect the parameters of the others. By sharing parameters, the networks regularize each other, and the network for one task can benefit from repre1403 1 5 6 7 13 Figure 1: Overview over the multi-task model (left) and the adversarial model (right). The baseline LSTM model corresponds to the same architecture with only one task. Figure 2: Improvement in F-score over the random baseline by class. The absolute"
N19-1142,P17-1069,0,0.187939,"Missing"
N19-1142,I11-1129,0,0.0641425,"Missing"
N19-1142,S16-1003,0,0.0875545,"Missing"
N19-1142,naderi-hirst-2017-classifying,0,0.154555,"tical (13), Legality, Jurisprudence and Constitutionality (5), Policy prescription and evaluation (6) and Crime and Punishment (7). multi-task and adversarial learning, leading to improved results in the target domain.1 Related Work Previous work on automatic frame classification focused on news articles and social media. Card et al. (2016) predict frames in news articles at the document level, using clusters of latent dimensions and word-based features in a logistic regression model. Ji and Smith (2017) improve on previous work integrating discourse structure into a recursive neural network. Naderi and Hirst (2017) use the same resource, but make predictions at the sentence level, using topic models and recurrent neural networks. Johnson et al. (2017) predict frames in social media data at the micro-post level, using probabilistic soft logic based on lists of keywords, as well as temporal similarity and network structure. All the work mentioned above uses the generic frames of Boydstun et al. (2014)’s Policy Frames Codebook. Baumer et al. (2015) predict words perceived as frame-evoking in political news articles with hand-crafted features. Field et al. (2018) analyse how Russian news articles frame the"
N19-1142,D14-1162,0,0.0838758,"ific features. During training, the model alternates between 1) pre5 Experiments We compare the multi-task learning and the adversarial setup with two baseline models: (a) a Random Forest classifier using tf-idf weighted bagof-words-representations, and (b) the LSTM baseline model. For the multi-task model, we use both the Twitter dataset and the argument quality dataset as auxiliary tasks. For all models, we report results on the test set using the optimal hyperparameters that we found averaged over 3 runs on the validation set. For the neural models, we use 100-dimensional GloVe embeddings (Pennington et al., 2014), pre-trained on Wikipedia and Gigaword.6 Details about hyper-parameter tuning and optimal settings can be found in Appendix B. Results The results in Table 5 show that both the multi-task and the adversarial model improve over 5 In the forward pass, this layer multiplies its input with the identity matrix. 6 https://nlp.stanford.edu/projects/ glove/ 1404 Nr. Gold Adv MTL LSTM (1) 5 5 5 7 Sentence But, star gazer, we had guns then when the Constitution was written and enshrined in the BOR and now incorporated into th 14th Civil Rights Amendment. (2) 6 6 5 1 Gun control is about preventing such"
N19-1142,P13-1162,0,0.0259738,"t al. (2014)’s Policy Frames Codebook. Baumer et al. (2015) predict words perceived as frame-evoking in political news articles with hand-crafted features. Field et al. (2018) analyse how Russian news articles frame the U.S. using a keyword-based cross-lingual projection setup. Tsur et al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011), detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al., 2016; Augenstein et al., 2016; Ferreira and Vlachos, 2016). 2 Online Discussion Annotations We create a new resource of issue-frame annotated online fora discussions, by annotating a subset of the Argument Extraction Corpus (Swanson et al., 2015) with a subset of the frames in the Policy Frames Codebook. The Argument Extraction 1 Code and annotations are available at https:// github.com/coastalcph/issue_framing. Corpus is a collection of argumentative dialogues across topics and platforms.2 The corpus contains posts on the following topics: gay marriage,"
N19-1142,P17-1194,0,0.0148855,"nformation from source to target domain, multi-task learning and adversarial learning. We briefly describe both setups in the following. An overview over tasks and data used in the different models is shown in Table 3. Multi-Task Learning To exploit synergies between additional datasets/annotations, we explore a simple multi-task learning with hard parameter sharing strategy, pioneered by Caruana (1993), introduced in the context of NLP by Collobert et al. (2011), and to RNNs by Søgaard and Goldberg (2016), which has been shown to be useful for a variety of NLP tasks, e.g. sequence labelling (Rei, 2017; Ruder et al., 2019; Augenstein and Søgaard, 2017), pairwise sequence classification (Augenstein et al., 2018) or machine translation (Dong et al., 2015). Here, parameters are shared between hidden layers. Intuitively, it works by training several networks in parallel, tying a subset of the hidden parameters so that updates in one network affect the parameters of the others. By sharing parameters, the networks regularize each other, and the network for one task can benefit from repre1403 1 5 6 7 13 Figure 1: Overview over the multi-task model (left) and the adversarial model (right). The base"
N19-1142,P16-2038,1,0.826517,"nd Schmidhuber) trained on only the news articles data. We then apply two strategies to facilitate the transfer of information from source to target domain, multi-task learning and adversarial learning. We briefly describe both setups in the following. An overview over tasks and data used in the different models is shown in Table 3. Multi-Task Learning To exploit synergies between additional datasets/annotations, we explore a simple multi-task learning with hard parameter sharing strategy, pioneered by Caruana (1993), introduced in the context of NLP by Collobert et al. (2011), and to RNNs by Søgaard and Goldberg (2016), which has been shown to be useful for a variety of NLP tasks, e.g. sequence labelling (Rei, 2017; Ruder et al., 2019; Augenstein and Søgaard, 2017), pairwise sequence classification (Augenstein et al., 2018) or machine translation (Dong et al., 2015). Here, parameters are shared between hidden layers. Intuitively, it works by training several networks in parallel, tying a subset of the hidden parameters so that updates in one network affect the parameters of the others. By sharing parameters, the networks regularize each other, and the network for one task can benefit from repre1403 1 5 6 7"
N19-1142,W15-4631,0,0.0851032,"al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011), detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al., 2016; Augenstein et al., 2016; Ferreira and Vlachos, 2016). 2 Online Discussion Annotations We create a new resource of issue-frame annotated online fora discussions, by annotating a subset of the Argument Extraction Corpus (Swanson et al., 2015) with a subset of the frames in the Policy Frames Codebook. The Argument Extraction 1 Code and annotations are available at https:// github.com/coastalcph/issue_framing. Corpus is a collection of argumentative dialogues across topics and platforms.2 The corpus contains posts on the following topics: gay marriage, gun control, death penalty and evolution. A subset of the corpus was annotated with argument quality scores by Swanson et al. (2015), which we exploit in our multi-task setup (see §3). We collect new issue frame annotations for each argument in the argument-quality annotated data.3 We"
N19-1142,P15-1157,0,0.0885542,"e level, using topic models and recurrent neural networks. Johnson et al. (2017) predict frames in social media data at the micro-post level, using probabilistic soft logic based on lists of keywords, as well as temporal similarity and network structure. All the work mentioned above uses the generic frames of Boydstun et al. (2014)’s Policy Frames Codebook. Baumer et al. (2015) predict words perceived as frame-evoking in political news articles with hand-crafted features. Field et al. (2018) analyse how Russian news articles frame the U.S. using a keyword-based cross-lingual projection setup. Tsur et al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011), detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al., 2016; Augenstein et al., 2016; Ferreira and Vlachos, 2016). 2 Online Discussion Annotations We create a new resource of issue-frame annotated online fora discussions, by annotating a subset of the Argument Extraction Corpus (Swanson et"
N19-1142,walker-etal-2012-corpus,0,0.0383126,"have labeled training data for this domain, we exploit additional corpora and additional annotations, which are described in the next subsection. Statistics of the filtered datasets as well as preprocessing details are given in Appendix A. Media Frames Corpus The Media Frames Corpus (Card et al., 2015) contains US newspaper articles on three topics: Immigration, smoking and same-sex marriage. The articles are annotated with the 15 framing dimensions defined in the Policy Frames Codebook.4 The annotations are on 2 The corpus is a combination of dialogues from http: //www.createdebate.com/, and Walker et al. (2012)’s Internet Argument Corpus, which contains dialogues from 4forums.com. 3 Topic cluster Evolution was dropped, because it contained too few examples matching our frame categories. 4 We discard all instances that do not correspond to the frame categories in the online discussions data. 1402 Model Baseline Task Main task Target task Domain Labelset # classes # sequences News articles Online disc. (test) Frames Frames 5 5 10,480 692 Multitask +Aux task +Aux task Tweets Online disc. Frames Argument quality 5 2 1,636 3,785 Adversarial +Adv task Online disc. + News articles Domain 2 4,731 + 10,480 O"
N19-1251,W13-1704,1,0.827158,"uding GED and the subtask of identifying SVA errors, have, in recent years, been handled with Recurrent Neural Networks (RNNs) trained on large amounts of data (Rei and Yannakoudakis, 2016, 2017). However, most publicly available datasets for GED are relatively small, making it difficult to learn a general grammar representation and potentially leading to over-fitting. Previous work has also shown that neural language models with a similar architecture have difficulty learning subject–verb agreement patterns in the presence of agreement attractors (Linzen et al., 2016). Rule-based approaches (Andersen et al., 2013) are still considered a strong alternative to end-toend neural networks, with many industry solutions still relying on rules defined over syntactic trees. The rule-based approach has the advantage of not requiring manual annotation, while also allowing easy access to adding and removing individual rules. On the other hand, language is continuously evolving, and there are exceptions to most grammar rules we know. Additionally, rule-based matching typically relies on syntactic pre-processing, which is error-prone, leading to compounding errors that hurt the downstream GED performance. Our contri"
N19-1251,P06-1032,0,0.0728054,"er the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we th"
N19-1251,W18-0529,0,0.0173854,"focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we therefore compare our work to current state-of-the-art approaches to detecting errors and do not report the performance of correction systems. 3 Subject–verb agreement detection Following recent work on GED (Rei and Yannakoudakis, 2016), we define SVA error detection as a sequence labeling task, where each token is simply labeled as correct or incorrect. For a given SVA error, only the verb is labeled as incorrect. Error types other than SVA are ignored, i.e., we do not corr"
N19-1251,P17-1074,0,0.0690779,"Missing"
N19-1251,Y09-1008,0,0.0357055,") who argue that bidirectional (bi-) LSTMs, in particular, are superior to other RNNs when evaluated on standard ESL benchmarks for GED and give state-of-the-art results. Rei and Yannakoudakis (2017) show even better performance using a multi-task learning architecture for training bi-LSTMs that additionally predicts linguistic properties of words, such as their part of speech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified, after parsing the simplified input sentence, a PoS tagger is used to check agreement. This is similar in spirit to the rule-based baseline system used in our experiments below. Wang et al. (2015) use a similar approach, distinguishing between four different sentence types and using slightly different rules for each type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun e"
N19-1251,D14-1082,0,0.0353693,"Missing"
N19-1251,W13-1703,0,0.0312982,"5.3 Training data ESL writings. We use the following ESL datasets as training data: • Lang8 is a parallel corpus of sentences with errors and their corrected versions created by scraping the Lang-8 website5 , which is an open platform where language learners can write texts and native speakers of that language can provide feedback via error correction (Mizumoto et al., 2011). It contains 1, 047, 393 sentences. • NUCLE comprises around 1, 400 essays written by students from the National University of Singapore. It is annotated for error tags and corrections by professional English instructors (Dahlmeier et al., 2013). It contains 57, 151 sentences. • FCE train set. We use the publicly available FCE training set, containing 25, 748 sentences. A subset of 5, 000 sentences was separated and used for development experiments. 4 Artificial errors. We generate artificial subject– verb agreement errors from large amounts of data. Specifically, we use the British National Corpus (BNC, BNC-Consortium et al., 2007), a collection of British English sentences that includes samples from different media such as newspapers, journals, letters or essays. Subject–verb agreement in English merely consists of inflecting 3rd p"
N19-1251,W16-0506,0,0.0147561,"there. 2 2420 • FCE. The Cambridge Learner Corpus of First Certificate in English (FCE) exam scripts consists of texts produced by ESL learners taking the FCE exam, which assesses English at the upper-intermediate proficiency level (Yannakoudakis et al., 2011). We use the publicly available test set. • AESW. The dataset from the Automated Evaluation of Scientific Writing Shared Task 3 https://github.com/chrisjbryant/errant 2016 (AESW) is a collection of text extracts from published journal articles (mostly in physics and mathematics) along with their (sentence-aligned) corrected counterparts (Daudaravicius et al., 2016). We test on the combined trained, development and test set.4 • JFLEG. The JHU Fluency-Extended GUG corpus (JFLEG) represents a cross-section of ungrammatical data, consisting of sentences written by ESL learners with different proficiency levels and L1s (Napoles et al., 2017). We evaluate our models on the public test set. • CoNLL14. The test dataset from the CoNLL 2014 shared task consists of (mostly argumentative) essays written by advanced undergraduate students from the National University of Singapore, and are annotated for grammatical errors by two native speakers of English (Ng et al.,"
N19-1251,E14-3013,0,0.0221815,"g et al. (2015) use a similar approach, distinguishing between four different sentence types and using slightly different rules for each type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical error"
N19-1251,N18-1108,0,0.022276,"s, and on average achieves a new state-of-the-art on detecting SVA errors. 2 Related work Neural approaches. Recent neural approaches to GED include Rei and Yannakoudakis (2016) who argue that bidirectional (bi-) LSTMs, in particular, are superior to other RNNs when evaluated on standard ESL benchmarks for GED and give state-of-the-art results. Rei and Yannakoudakis (2017) show even better performance using a multi-task learning architecture for training bi-LSTMs that additionally predicts linguistic properties of words, such as their part of speech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified, after parsing the simplified input sentence, a PoS tagger is used to check agreement. This is similar in spirit to the rule-based baseline system used in our experiments below. Wang et al. (2015) use a similar approach, distinguishing between four"
N19-1251,D18-1541,0,0.0142505,"imilar approach, distinguishing between four different sentence types and using slightly different rules for each type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al"
N19-1251,L16-1498,0,0.0153074,"ers or essays. Subject–verb agreement in English merely consists of inflecting 3rd person singular verbs in the present tense (and be in the past), which makes any text in English fairly easy to corrupt with SVA errors. We assume that the BNC data is written in correct British English. Using predicted PoS tags provided by the Stanford Log-linear PoS Tagger, we identify verbs in present tense, as well as was and were for the past tense, and flip them to their respective opposite version using the list of inflected English words (annotated with morphological features) from the Unimorph project (Kirov et al., 2016). The final artificial training set includes the sentences with injected errors (265, 742 sentences), their original counterpart, and sentences where SVA errors could not be injected due to not containing candidate verbs that could be flipped (241, 295 sentences). Sentences containing special placeholders for mathematical equations, dates, etc. are filtered out. 5 http://lang-8.com/ 6 Experiments The models. We compare our neural model trained on both artificially generated errors and ESL data (LSTMESL+art ) to three baselines: a neural model trained only on ESL data (LSTMESL ) (i.e., reflecti"
N19-1251,P18-1132,0,0.019321,"es a new state-of-the-art on detecting SVA errors. 2 Related work Neural approaches. Recent neural approaches to GED include Rei and Yannakoudakis (2016) who argue that bidirectional (bi-) LSTMs, in particular, are superior to other RNNs when evaluated on standard ESL benchmarks for GED and give state-of-the-art results. Rei and Yannakoudakis (2017) show even better performance using a multi-task learning architecture for training bi-LSTMs that additionally predicts linguistic properties of words, such as their part of speech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified, after parsing the simplified input sentence, a PoS tagger is used to check agreement. This is similar in spirit to the rule-based baseline system used in our experiments below. Wang et al. (2015) use a similar approach, distinguishing between four different sentence typ"
N19-1251,N16-1030,0,0.0592706,"rd Neural Network Dependency Parser (Chen and Manning, 2014) respectively. 4.2 Neural system We use the state-of-the-art neural sequence labeling architecture for error detection (Rei and Yannakoudakis, 2016). The model receives a sequence of tokens (w1 , ..., wT ) as input and outputs a sequence of labels (l1 , ..., lT ), i.e., one for each token, indicating whether a token is grammatically correct (in agreement) or not, in the given context. All tokens are first mapped to distributed word representations, pre-trained using word2vec (Mikolov et al., 2013) on the Google News corpus. Following Lample et al. (2016), character-based representations are also built for every word using a bi-LSTM (Hochreiter and Schmidhuber, 1997) and then concatenated onto the word embedding. The combined embeddings are then given as input to a word-level bi-LSTM, creating representations that are conditioned on the context from both sides of the target word. These representations are then passed through an additional feedforward layer, in order to combine the extracted features and map them to a more suitable space. A softmax output layer returns the probability distribution over the two possible labels (correct or incorr"
N19-1251,Q16-1037,0,0.479233,"ches. Sequence labeling problems in NLP, including GED and the subtask of identifying SVA errors, have, in recent years, been handled with Recurrent Neural Networks (RNNs) trained on large amounts of data (Rei and Yannakoudakis, 2016, 2017). However, most publicly available datasets for GED are relatively small, making it difficult to learn a general grammar representation and potentially leading to over-fitting. Previous work has also shown that neural language models with a similar architecture have difficulty learning subject–verb agreement patterns in the presence of agreement attractors (Linzen et al., 2016). Rule-based approaches (Andersen et al., 2013) are still considered a strong alternative to end-toend neural networks, with many industry solutions still relying on rules defined over syntactic trees. The rule-based approach has the advantage of not requiring manual annotation, while also allowing easy access to adding and removing individual rules. On the other hand, language is continuously evolving, and there are exceptions to most grammar rules we know. Additionally, rule-based matching typically relies on syntactic pre-processing, which is error-prone, leading to compounding errors that"
N19-1251,J93-2004,0,0.0653251,"cts and verbs are far apart, i.e., when the agreement relation is defined over a long-distance dependency. In order to see how our systems are affected by the distance between the subject and verb, we split the test sets based on different subject–verb distances. Note, however, that our benchmarks are not annotated with PoS tags and dependency relations. If we binned our test data based on predicted dependencies, the inductive bias of our syntactic parser and the errors it made would bias our evaluation. Instead, we perform our analyses on section 22 and 23 of the Penn Treebank (PTB) dataset (Marcus et al., 1993). The PTB however is not annotated with grammatical errors. We therefore corrupt the sentences by injecting SVA errors, in the same 2423 way we corrupted the BNC (§5.3) to create additional training data. For each sentence in the PTB, we identify a subject–verb pair, and group the sentences by the subject–verb distance. We then run our models on two versions of each sentence: an unaltered version and a corrupted one, where we have generated an SVA error by corrupting the verb, using the method described earlier (§5.3). This way we can compute the performance of our models as F0.5 scores over t"
N19-1251,I11-1017,0,0.023707,"CoNLL 2014 shared task consists of (mostly argumentative) essays written by advanced undergraduate students from the National University of Singapore, and are annotated for grammatical errors by two native speakers of English (Ng et al., 2014). 5.3 Training data ESL writings. We use the following ESL datasets as training data: • Lang8 is a parallel corpus of sentences with errors and their corrected versions created by scraping the Lang-8 website5 , which is an open platform where language learners can write texts and native speakers of that language can provide feedback via error correction (Mizumoto et al., 2011). It contains 1, 047, 393 sentences. • NUCLE comprises around 1, 400 essays written by students from the National University of Singapore. It is annotated for error tags and corrections by professional English instructors (Dahlmeier et al., 2013). It contains 57, 151 sentences. • FCE train set. We use the publicly available FCE training set, containing 25, 748 sentences. A subset of 5, 000 sentences was separated and used for development experiments. 4 Artificial errors. We generate artificial subject– verb agreement errors from large amounts of data. Specifically, we use the British National"
N19-1251,C10-2103,0,0.0340272,"00, while character representations have size 100. The word-level LSTM hidden layers have size 300 for each direction, and the character-level LSTM hidden layers have size 100 for each direction. Evaluation. Existing approaches are typically optimised for high precision at the cost of recall, as a system’s utility depends strongly on the ratio of true to false positives, which has been found to be more important in terms of learning effect. A high number of false positives would mean that the system often flags correct language as incorrect, and may therefore end up doing more harm than good (Nagata and Nakatani, 2010). Because of this, F0.5 is preferred to F1 in the GED domain as it puts more weight on precision than recall. For each experiment, we report the token-level precision (P), the recall (R), and the F0.5 scores. 7 Results The main results are summarized in Table 1. Looking at the performance of the LSTMESL+art system, we see that on 3 out of 4 benchmarks, our neural model trained on artificially generated errors outperforms the LSTMESL system with respect to F0.5 . On average, over the four benchmarks, its F0.5 score is 2.43 points higher than the best performing baseline. Both neural models obta"
N19-1251,E17-2037,0,0.0178762,"vailable test set. • AESW. The dataset from the Automated Evaluation of Scientific Writing Shared Task 3 https://github.com/chrisjbryant/errant 2016 (AESW) is a collection of text extracts from published journal articles (mostly in physics and mathematics) along with their (sentence-aligned) corrected counterparts (Daudaravicius et al., 2016). We test on the combined trained, development and test set.4 • JFLEG. The JHU Fluency-Extended GUG corpus (JFLEG) represents a cross-section of ungrammatical data, consisting of sentences written by ESL learners with different proficiency levels and L1s (Napoles et al., 2017). We evaluate our models on the public test set. • CoNLL14. The test dataset from the CoNLL 2014 shared task consists of (mostly argumentative) essays written by advanced undergraduate students from the National University of Singapore, and are annotated for grammatical errors by two native speakers of English (Ng et al., 2014). 5.3 Training data ESL writings. We use the following ESL datasets as training data: • Lang8 is a parallel corpus of sentences with errors and their corrected versions created by scraping the Lang-8 website5 , which is an open platform where language learners can write"
N19-1251,P17-1194,1,0.856235,"(Hochreiter and Schmidhuber, 1997) and then concatenated onto the word embedding. The combined embeddings are then given as input to a word-level bi-LSTM, creating representations that are conditioned on the context from both sides of the target word. These representations are then passed through an additional feedforward layer, in order to combine the extracted features and map them to a more suitable space. A softmax output layer returns the probability distribution over the two possible labels (correct or incorrect) for each word. We also include the language modeling objective proposed by Rei (2017), which encourages the model to learn better representations via multi-tasking and predicting surrounding words in the sentence. Dropout (Srivastava et al., 2014) with probability 0.5 is applied to word representations and to the output from the word-level bi-LSTM. The model is optimised using categorical cross-entropy with AdaDelta (Zeiler, 2012). 5 Data 5.1 Data preprocessing As the public datasets either have their own taxonomy or they are not annotated with error types at all, we apply the error type extraction tool of Bryant, Felice, and Briscoe (2017) to automatically get error types map"
N19-1251,W17-5032,1,0.856999,"tterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we therefore compare our work to current state-of-the-art approaches to detecting errors and do not report the performance of correcti"
N19-1251,P16-1112,1,0.831037,"ine translation systems, guiding automatically generated output towards grammatically correct sequences. The problem of detecting subject–verb agreement (SVA) errors is an important subtask of GED. In this work, we focus on detecting subject– verb agreement errors in the English as a Second Language (ESL) domain. Most SVA errors occur at the third-person present tense when determining Approaches. Sequence labeling problems in NLP, including GED and the subtask of identifying SVA errors, have, in recent years, been handled with Recurrent Neural Networks (RNNs) trained on large amounts of data (Rei and Yannakoudakis, 2016, 2017). However, most publicly available datasets for GED are relatively small, making it difficult to learn a general grammar representation and potentially leading to over-fitting. Previous work has also shown that neural language models with a similar architecture have difficulty learning subject–verb agreement patterns in the presence of agreement attractors (Linzen et al., 2016). Rule-based approaches (Andersen et al., 2013) are still considered a strong alternative to end-toend neural networks, with many industry solutions still relying on rules defined over syntactic trees. The rule-ba"
N19-1251,W17-5004,1,0.852217,"ural sequence labeling models. We demonstrate that a system trained on a combination of available labeled data and large volumes of silver standard data outperforms both neural and rule-based baselines by a margin on three out of four standard benchmarks, and on average achieves a new state-of-the-art on detecting SVA errors. 2 Related work Neural approaches. Recent neural approaches to GED include Rei and Yannakoudakis (2016) who argue that bidirectional (bi-) LSTMs, in particular, are superior to other RNNs when evaluated on standard ESL benchmarks for GED and give state-of-the-art results. Rei and Yannakoudakis (2017) show even better performance using a multi-task learning architecture for training bi-LSTMs that additionally predicts linguistic properties of words, such as their part of speech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified,"
N19-1251,P11-1093,0,0.0378914,"ch type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task o"
N19-1251,N03-1033,0,0.0125469,"the system. However, our rulebased system is not limited to the detection of simple cases of SVA errors. It relies on PoS tags and dependency relations to identify all types of SVA errors. Specifically, our rule-based system operates as follows: (i) it identifies the candidate verbs based on PoS tags;1 (ii) for a given verb, it uses the dependency relations to find its subject;2 (iii) the PoS tag of the verb and its subject are used to check whether they agree in number and person. We use predicted Penn Treebank PoS tags and dependency relations provided by the Stanford Loglinear PoS Tagger (Toutanova et al., 2003) and the Stanford Neural Network Dependency Parser (Chen and Manning, 2014) respectively. 4.2 Neural system We use the state-of-the-art neural sequence labeling architecture for error detection (Rei and Yannakoudakis, 2016). The model receives a sequence of tokens (w1 , ..., wT ) as input and outputs a sequence of labels (l1 , ..., lT ), i.e., one for each token, indicating whether a token is grammatically correct (in agreement) or not, in the given context. All tokens are first mapped to distributed word representations, pre-trained using word2vec (Mikolov et al., 2013) on the Google News cor"
N19-1251,Y15-2040,0,0.0259707,"ech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified, after parsing the simplified input sentence, a PoS tagger is used to check agreement. This is similar in spirit to the rule-based baseline system used in our experiments below. Wang et al. (2015) use a similar approach, distinguishing between four different sentence types and using slightly different rules for each type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan"
N19-1251,N18-1057,0,0.0158796,"cause of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we therefore compare our work to current state-of-the-art approaches to detecting errors and do not report the performance of correction systems. 3 Subject–verb agreement d"
N19-1251,P11-1019,1,0.706811,"s as our target class. 5.2 Test data We compare the rule-based and neural approaches for the task of SVA error detection on four benchmarks in the ESL domain. 1 Present tense verbs + “was” and “were”. The subject can be direct – attached with a nsubj relation – or indirect, such as when the syntactic subject is a relative pronoun, e.g., who, or an expletive, e.g., there. 2 2420 • FCE. The Cambridge Learner Corpus of First Certificate in English (FCE) exam scripts consists of texts produced by ESL learners taking the FCE exam, which assesses English at the upper-intermediate proficiency level (Yannakoudakis et al., 2011). We use the publicly available test set. • AESW. The dataset from the Automated Evaluation of Scientific Writing Shared Task 3 https://github.com/chrisjbryant/errant 2016 (AESW) is a collection of text extracts from published journal articles (mostly in physics and mathematics) along with their (sentence-aligned) corrected counterparts (Daudaravicius et al., 2016). We test on the combined trained, development and test set.4 • JFLEG. The JHU Fluency-Extended GUG corpus (JFLEG) represents a cross-section of ungrammatical data, consisting of sentences written by ESL learners with different profi"
N19-1251,D17-1297,1,0.826452,"asewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we therefore compare our work to current state-of-the-art approaches to detecting errors and do not report the performance of correction systems. 3 Subject–verb agreement detection Following recent work on GED (Rei and Yannakoudakis, 2016), we define SVA error detection as a sequence labeling task, where each token is simply labeled as correct or incorrect. For a given SVA error, only the verb is labeled as incorrect. Error types other than SVA are ign"
N19-1341,W14-6110,0,0.0403625,"Missing"
N19-1341,E17-2026,1,0.860598,"e 2. 2.3 NP PRP 2.2 Multi-task Learning Multi-task learning is used to solve multiple tasks using a single model architecture, with taskspecific classifier functions from the outer-most representations (Caruana, 1997; Collobert and Weston, 2008). The benefits are intuitive: sharing a common representation for different tasks acts as a generalization mechanism and allows to address them in a parallel fashion. The hard-sharing strategy is the most basic MTL architecture, where the internal representation is fully shared across all tasks. The approach has proven robust for a number of NLP tasks (Bingel and Søgaard, 2017) and comes with certain guarantees if a common, op3373 (1,S,NP) FF LSTM LSTM LSTM LSTM I (1,VP,) FF LSTM LSTM LSTM LSTM find (3,NP,) (-1,NP,) (1,PP,) (-2,S,) FF FF FF FF LSTM LSTM LSTM LSTM your LSTM LSTM LSTM LSTM LSTM LSTM lack LSTM LSTM of (-2,S,ADJP) LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM FF FF LSTM LSTM faith disturbing . Input to the network: word embedding postag embedding character embeddings (with char-LSTM) Figure 2: The baseline architecture used in this work. The input to the network is a concatenation of word embeddings, PoS-tag embeddings and a second word embedding le"
N19-1341,A00-2018,0,0.71512,"ven greater improvements across the board, including a new state of the art on Basque, Hebrew, Polish and Swedish.1 1 Introduction Constituent parsing is a core task in natural language processing (NLP), with a wide set of applications. Most competitive parsers are slow, however, to the extent that it is prohibitive of downstream applications in large-scale environments (Kummerfeld et al., 2012). Previous efforts to obtain speed-ups have focused on creating more efficient versions of traditional shift-reduce (Sagae and Lavie, 2006; Zhang and Clark, 2009) or chart-based parsers (Collins, 1997; Charniak, 2000). Zhu et al. (2013), for example, presented 1 After this paper was submitted, Kitaev and Klein (2018b) have improved our results using their previous self-attentive constituent parser (Kitaev and Klein, 2018a) and BERT representations (Devlin et al., 2018) as input to their system. We will acknowledge these results in the Experiments section. Contribution We first explore different factors that prevent sequence tagging constituent parsers from obtaining better results. These include: high error rates when long constituents need to be closed, label sparsity, and error propagation arising from g"
N19-1341,D14-1082,0,0.0733292,"learn a parametrized policy, by which an agent selects actions based on the gradient of a scalar performance measure with respect to the policy. Compared to other reinforcement learning methods, PG is well-suited to NLP problems due to its appealing convergence properties and effectiveness in highdimensional spaces (Sutton and Barto, 2018). Previous work on constituent parsing has employed PG methods to mitigate the effect of exposure bias, finding that they function as a modelagnostic substitute for dynamic oracles (Fried and Klein, 2018). Similarly, Le and Fokkens (2017) apply PG methods to Chen and Manning (2014)’s transition-based dependency parser to reduce error propagation. In this work, we also employ PG to fine-tune models trained using supervised learning. However, our setting (sequence tagging) has a considerably larger action space than a transition parser. To deal with that, we will adopt a number of variance reduction and regularization techniques to make reinforcement learning stable. 3 Methods We describe the methods introduced in this work, motivated by current limitations of existing sequence tagging models, which are first reviewed. The source code can be found as a part of https: //gi"
N19-1341,P16-1017,0,0.118967,"Missing"
N19-1341,E17-2053,0,0.123513,"Missing"
N19-1341,P97-1003,0,0.724099,"s, we observe even greater improvements across the board, including a new state of the art on Basque, Hebrew, Polish and Swedish.1 1 Introduction Constituent parsing is a core task in natural language processing (NLP), with a wide set of applications. Most competitive parsers are slow, however, to the extent that it is prohibitive of downstream applications in large-scale environments (Kummerfeld et al., 2012). Previous efforts to obtain speed-ups have focused on creating more efficient versions of traditional shift-reduce (Sagae and Lavie, 2006; Zhang and Clark, 2009) or chart-based parsers (Collins, 1997; Charniak, 2000). Zhu et al. (2013), for example, presented 1 After this paper was submitted, Kitaev and Klein (2018b) have improved our results using their previous self-attentive constituent parser (Kitaev and Klein, 2018a) and BERT representations (Devlin et al., 2018) as input to their system. We will acknowledge these results in the Experiments section. Contribution We first explore different factors that prevent sequence tagging constituent parsers from obtaining better results. These include: high error rates when long constituents need to be closed, label sparsity, and error propagati"
N19-1341,P15-1166,0,0.036777,"STM I (1,VP,) FF LSTM LSTM LSTM LSTM find (3,NP,) (-1,NP,) (1,PP,) (-2,S,) FF FF FF FF LSTM LSTM LSTM LSTM your LSTM LSTM LSTM LSTM LSTM LSTM lack LSTM LSTM of (-2,S,ADJP) LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM FF FF LSTM LSTM faith disturbing . Input to the network: word embedding postag embedding character embeddings (with char-LSTM) Figure 2: The baseline architecture used in this work. The input to the network is a concatenation of word embeddings, PoS-tag embeddings and a second word embedding learned by a character-based LSTM layer. timal representation exists (Baxter, 2000). Dong et al. (2015) use it for their multilingual machine translation system, where the encoder is a shared gated recurrent neural network (Cho et al., 2014) and the decoder is language-specific. Plank et al. (2016) also use a hard-sharing setup to improve the performance of BILSTM-based PoS taggers. To do so, they rely on auxiliary tasks, i.e, tasks that are not of interest themselves, but that are co-learned in a MTL setup with the goal of improving the network’s performance on the main task(s). We will introduce auxiliary tasks for sequence tagging constituent parsing later on in this work. A MTL architecture"
N19-1341,P15-2139,0,0.0220696,"ral network (Cho et al., 2014) and the decoder is language-specific. Plank et al. (2016) also use a hard-sharing setup to improve the performance of BILSTM-based PoS taggers. To do so, they rely on auxiliary tasks, i.e, tasks that are not of interest themselves, but that are co-learned in a MTL setup with the goal of improving the network’s performance on the main task(s). We will introduce auxiliary tasks for sequence tagging constituent parsing later on in this work. A MTL architecture can also rely on partial sharing when the different tasks do not fully share the internal representations (Duong et al., 2015; Rei, 2017; Ruder et al., 2019) and recent work has also shown that hierarchical sharing (e.g. lowlevel task outputs used as input for higher-level ones) could be beneficial (Søgaard and Goldberg, 2016; Sanh et al., 2018). 2.4 Policy Gradient Fine-tuning Policy gradient (PG) methods are a class of reinforcement learning algorithms that directly learn a parametrized policy, by which an agent selects actions based on the gradient of a scalar performance measure with respect to the policy. Compared to other reinforcement learning methods, PG is well-suited to NLP problems due to its appealing co"
N19-1341,N16-1024,0,0.531327,"ne of work, some authors have proposed new parsing paradigms that aim to both reduce the complexity of existing parsers and improve their speed. Vinyals et al. (2015) proposed a machine translation-inspired sequence-tosequence approach to constituent parsing, where the input is the raw sentence, and the ‘translation’ is a parenthesized version of its tree. G´omezRodr´ıguez and Vilares (2018) reduced constituent parsing to sequence tagging, where only n tagging actions need to be made, and obtained one of the fastest parsers to date. However, the performance is well below the state of the art (Dyer et al., 2016; Stern et al., 2017; Kitaev and Klein, 2018a). Sequence tagging models for constituent parsing are faster, but less accurate than other types of parsers. In this work, we address the following weaknesses of such constituent parsers: (a) high error rates around closing brackets of long constituents, (b) large label sets, leading to sparsity, and (c) error propagation arising from greedy decoding. To effectively close brackets, we train a model that learns to switch between tagging schemes. To reduce sparsity, we decompose the label set and use multi-task learning to jointly learn to predict su"
N19-1341,P15-1147,0,0.0926233,"Missing"
N19-1341,P18-2075,0,0.053778,"(PG) methods are a class of reinforcement learning algorithms that directly learn a parametrized policy, by which an agent selects actions based on the gradient of a scalar performance measure with respect to the policy. Compared to other reinforcement learning methods, PG is well-suited to NLP problems due to its appealing convergence properties and effectiveness in highdimensional spaces (Sutton and Barto, 2018). Previous work on constituent parsing has employed PG methods to mitigate the effect of exposure bias, finding that they function as a modelagnostic substitute for dynamic oracles (Fried and Klein, 2018). Similarly, Le and Fokkens (2017) apply PG methods to Chen and Manning (2014)’s transition-based dependency parser to reduce error propagation. In this work, we also employ PG to fine-tune models trained using supervised learning. However, our setting (sequence tagging) has a considerably larger action space than a transition parser. To deal with that, we will adopt a number of variance reduction and regularization techniques to make reinforcement learning stable. 3 Methods We describe the methods introduced in this work, motivated by current limitations of existing sequence tagging models, w"
N19-1341,D18-1162,1,0.89635,"Missing"
N19-1341,P14-1022,0,0.062069,"Missing"
N19-1341,P18-1249,0,0.0794906,"new parsing paradigms that aim to both reduce the complexity of existing parsers and improve their speed. Vinyals et al. (2015) proposed a machine translation-inspired sequence-tosequence approach to constituent parsing, where the input is the raw sentence, and the ‘translation’ is a parenthesized version of its tree. G´omezRodr´ıguez and Vilares (2018) reduced constituent parsing to sequence tagging, where only n tagging actions need to be made, and obtained one of the fastest parsers to date. However, the performance is well below the state of the art (Dyer et al., 2016; Stern et al., 2017; Kitaev and Klein, 2018a). Sequence tagging models for constituent parsing are faster, but less accurate than other types of parsers. In this work, we address the following weaknesses of such constituent parsers: (a) high error rates around closing brackets of long constituents, (b) large label sets, leading to sparsity, and (c) error propagation arising from greedy decoding. To effectively close brackets, we train a model that learns to switch between tagging schemes. To reduce sparsity, we decompose the label set and use multi-task learning to jointly learn to predict sublabels. Finally, we mitigate issues from gr"
N19-1341,D12-1096,0,0.0247734,"Combining these techniques, we clearly surpass the performance of sequence tagging constituent parsers on the English and Chinese Penn Treebanks, and reduce their parsing time even further. On the SPMRL datasets, we observe even greater improvements across the board, including a new state of the art on Basque, Hebrew, Polish and Swedish.1 1 Introduction Constituent parsing is a core task in natural language processing (NLP), with a wide set of applications. Most competitive parsers are slow, however, to the extent that it is prohibitive of downstream applications in large-scale environments (Kummerfeld et al., 2012). Previous efforts to obtain speed-ups have focused on creating more efficient versions of traditional shift-reduce (Sagae and Lavie, 2006; Zhang and Clark, 2009) or chart-based parsers (Collins, 1997; Charniak, 2000). Zhu et al. (2013), for example, presented 1 After this paper was submitted, Kitaev and Klein (2018b) have improved our results using their previous self-attentive constituent parser (Kitaev and Klein, 2018a) and BERT representations (Devlin et al., 2018) as input to their system. We will acknowledge these results in the Experiments section. Contribution We first explore differen"
N19-1341,E17-1064,0,0.0204333,"learning algorithms that directly learn a parametrized policy, by which an agent selects actions based on the gradient of a scalar performance measure with respect to the policy. Compared to other reinforcement learning methods, PG is well-suited to NLP problems due to its appealing convergence properties and effectiveness in highdimensional spaces (Sutton and Barto, 2018). Previous work on constituent parsing has employed PG methods to mitigate the effect of exposure bias, finding that they function as a modelagnostic substitute for dynamic oracles (Fried and Klein, 2018). Similarly, Le and Fokkens (2017) apply PG methods to Chen and Manning (2014)’s transition-based dependency parser to reduce error propagation. In this work, we also employ PG to fine-tune models trained using supervised learning. However, our setting (sequence tagging) has a considerably larger action space than a transition parser. To deal with that, we will adopt a number of variance reduction and regularization techniques to make reinforcement learning stable. 3 Methods We describe the methods introduced in this work, motivated by current limitations of existing sequence tagging models, which are first reviewed. The sourc"
N19-1341,Q17-1029,0,0.0727914,"asets for sequence tagging constituent parsers. Metrics We report bracketing F-scores, using the EVALB and the EVAL - SPMRL scripts. We measure the speed in terms of sentences per second. Setup We use NCRFpp (Yang and Zhang, 2018), for direct comparison against G´omez-Rodr´ıguez and Vilares (2018). We adopt bracketing F-score instead of label accuracy for model selection and report this performance as our second baseline. After 100 epochs, we select the model that fared best on the development set. We use GloVe embeddings (Pennington et al., 2014) for our English models and zzgiga embeddings (Liu and Zhang, 2017) for the Chinese models, for a more homogeneous comparison against other parsers (Dyer et al., 2016; Liu and Zhang, 2017; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018). ELMo (Peters et al., 2018) or BERT (Devlin et al., 2018) could be used to improve the precision, but in this paper we focus on keeping a good speed-accuracy tradeoff. For SPMRL, no pretrained embeddings are used, following Kitaev and Klein (2018a). As a side note, if we wanted to improve the performance on these languages we could rely on the CoNLL 2018 shared task pretrained word embeddings (Zeman et al., 2018) or even the"
N19-1341,J93-2004,0,0.0664859,"Missing"
N19-1341,H05-1066,0,0.341051,"Missing"
N19-1341,D14-1162,0,0.0824362,"best of our knowledge, we provide the first evaluation on the SPMRL datasets for sequence tagging constituent parsers. Metrics We report bracketing F-scores, using the EVALB and the EVAL - SPMRL scripts. We measure the speed in terms of sentences per second. Setup We use NCRFpp (Yang and Zhang, 2018), for direct comparison against G´omez-Rodr´ıguez and Vilares (2018). We adopt bracketing F-score instead of label accuracy for model selection and report this performance as our second baseline. After 100 epochs, we select the model that fared best on the development set. We use GloVe embeddings (Pennington et al., 2014) for our English models and zzgiga embeddings (Liu and Zhang, 2017) for the Chinese models, for a more homogeneous comparison against other parsers (Dyer et al., 2016; Liu and Zhang, 2017; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018). ELMo (Peters et al., 2018) or BERT (Devlin et al., 2018) could be used to improve the precision, but in this paper we focus on keeping a good speed-accuracy tradeoff. For SPMRL, no pretrained embeddings are used, following Kitaev and Klein (2018a). As a side note, if we wanted to improve the performance on these languages we could rely on the CoNLL 2018 share"
N19-1341,N18-1202,0,0.0277448,"se NCRFpp (Yang and Zhang, 2018), for direct comparison against G´omez-Rodr´ıguez and Vilares (2018). We adopt bracketing F-score instead of label accuracy for model selection and report this performance as our second baseline. After 100 epochs, we select the model that fared best on the development set. We use GloVe embeddings (Pennington et al., 2014) for our English models and zzgiga embeddings (Liu and Zhang, 2017) for the Chinese models, for a more homogeneous comparison against other parsers (Dyer et al., 2016; Liu and Zhang, 2017; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018). ELMo (Peters et al., 2018) or BERT (Devlin et al., 2018) could be used to improve the precision, but in this paper we focus on keeping a good speed-accuracy tradeoff. For SPMRL, no pretrained embeddings are used, following Kitaev and Klein (2018a). As a side note, if we wanted to improve the performance on these languages we could rely on the CoNLL 2018 shared task pretrained word embeddings (Zeman et al., 2018) or even the multilingual BERT model6 . Our models are run on a single CPU7 (and optionally on a consumer-grade GPU for further comparison) using a batch size of 128 for testing. Additional hyperparameters can b"
N19-1341,N07-1051,0,0.242489,"Missing"
N19-1341,P16-2067,1,0.850221,"M LSTM LSTM LSTM LSTM LSTM FF FF LSTM LSTM faith disturbing . Input to the network: word embedding postag embedding character embeddings (with char-LSTM) Figure 2: The baseline architecture used in this work. The input to the network is a concatenation of word embeddings, PoS-tag embeddings and a second word embedding learned by a character-based LSTM layer. timal representation exists (Baxter, 2000). Dong et al. (2015) use it for their multilingual machine translation system, where the encoder is a shared gated recurrent neural network (Cho et al., 2014) and the decoder is language-specific. Plank et al. (2016) also use a hard-sharing setup to improve the performance of BILSTM-based PoS taggers. To do so, they rely on auxiliary tasks, i.e, tasks that are not of interest themselves, but that are co-learned in a MTL setup with the goal of improving the network’s performance on the main task(s). We will introduce auxiliary tasks for sequence tagging constituent parsing later on in this work. A MTL architecture can also rely on partial sharing when the different tasks do not fully share the internal representations (Duong et al., 2015; Rei, 2017; Ruder et al., 2019) and recent work has also shown that h"
N19-1341,P17-2018,0,0.0493147,"Missing"
N19-1341,P17-1194,0,0.106685,"al., 2014) and the decoder is language-specific. Plank et al. (2016) also use a hard-sharing setup to improve the performance of BILSTM-based PoS taggers. To do so, they rely on auxiliary tasks, i.e, tasks that are not of interest themselves, but that are co-learned in a MTL setup with the goal of improving the network’s performance on the main task(s). We will introduce auxiliary tasks for sequence tagging constituent parsing later on in this work. A MTL architecture can also rely on partial sharing when the different tasks do not fully share the internal representations (Duong et al., 2015; Rei, 2017; Ruder et al., 2019) and recent work has also shown that hierarchical sharing (e.g. lowlevel task outputs used as input for higher-level ones) could be beneficial (Søgaard and Goldberg, 2016; Sanh et al., 2018). 2.4 Policy Gradient Fine-tuning Policy gradient (PG) methods are a class of reinforcement learning algorithms that directly learn a parametrized policy, by which an agent selects actions based on the gradient of a scalar performance measure with respect to the policy. Compared to other reinforcement learning methods, PG is well-suited to NLP problems due to its appealing convergence p"
N19-1341,D17-1035,0,0.0192439,"RP NN IN NN find your lack of faith disturbing . JJ . (1,S,NP) (1,VP,) (3,NP,) (-1,NP,) (1,PP,) (-2,S,) (-2,S,ADJP) - Figure 1: A constituent tree linearized as by G´omezRodr´ıguez and Vilares (2018). 2 They (1) generate a dummy label for the last word and (2) pad sentences with a beginning- and end-of-sentence tokens. Sequence Tagging Sequence tagging is a structured prediction task that generates an output label for every input token. Long short-term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997) are a popular architecture for such tasks, often giving stateof-the-art performance (Reimers and Gurevych, 2017; Yang and Zhang, 2018). Tagging with LSTMs In LSTMs, the prediction for the ith element is conditioned on the output of the previous steps. Let LSTMθ (x1:n ) be a parametrized function of the network, where the input is a sequence of vectors x1:n , its output is a sequence of hidden vectors h1:n . To obtain better contextualized hidden vectors, it is possible to instead use bidirectional LSTMS (Schuster and Paliwal, 1997). First, a LSTMlθ processes the tokens from left-to-right and then an independent LSTM rθ processes them from right-to-left. The ith final hidden vector is represented as the"
N19-1341,P06-2089,0,0.0623123,"anks, and reduce their parsing time even further. On the SPMRL datasets, we observe even greater improvements across the board, including a new state of the art on Basque, Hebrew, Polish and Swedish.1 1 Introduction Constituent parsing is a core task in natural language processing (NLP), with a wide set of applications. Most competitive parsers are slow, however, to the extent that it is prohibitive of downstream applications in large-scale environments (Kummerfeld et al., 2012). Previous efforts to obtain speed-ups have focused on creating more efficient versions of traditional shift-reduce (Sagae and Lavie, 2006; Zhang and Clark, 2009) or chart-based parsers (Collins, 1997; Charniak, 2000). Zhu et al. (2013), for example, presented 1 After this paper was submitted, Kitaev and Klein (2018b) have improved our results using their previous self-attentive constituent parser (Kitaev and Klein, 2018a) and BERT representations (Devlin et al., 2018) as input to their system. We will acknowledge these results in the Experiments section. Contribution We first explore different factors that prevent sequence tagging constituent parsers from obtaining better results. These include: high error rates when long const"
N19-1341,W14-6111,0,0.122443,"Missing"
N19-1341,P18-1108,0,0.278934,"he hard-sharing model. In this context, in addition to a generalization mechanism, the shared representation could be also acting as way to keep the model aware of the potential interdependencies that might exist between subtasks. 3.4 1. Predict partial labels nt+k that are k steps from the current time step t. This way we can jointly optimize at each time step a prediction for the pairs (wt , wt+1 ), . . . , (wt+k , wt+k+1 ). In particular, we will experiment both with previous and upcoming nk ’s, setting |k|=1. Decomposition of the label space 2. Predict the syntactic distances presented by Shen et al. (2018), which reflect the order a sentence must be split to obtain its constituent tree using a top-down parsing algorithm (Stern et al., 2017). The algorithm was initially defined for binary trees, but its adaptation to n-ary trees is immediate: leaf nodes have a split priority of zero and the ancestors’ priority is computed as the maximum priority of their children plus one. In this work, we use this algorithm in a sequence tagging setup: the label assigned to each token corresponds to the syntactic distance of the lowest common ancestor with the next token. This is illustrated in Figure 5. Shen e"
N19-1341,D18-1291,0,0.0788061,"Missing"
N19-1341,P16-2038,1,0.840598,"on auxiliary tasks, i.e, tasks that are not of interest themselves, but that are co-learned in a MTL setup with the goal of improving the network’s performance on the main task(s). We will introduce auxiliary tasks for sequence tagging constituent parsing later on in this work. A MTL architecture can also rely on partial sharing when the different tasks do not fully share the internal representations (Duong et al., 2015; Rei, 2017; Ruder et al., 2019) and recent work has also shown that hierarchical sharing (e.g. lowlevel task outputs used as input for higher-level ones) could be beneficial (Søgaard and Goldberg, 2016; Sanh et al., 2018). 2.4 Policy Gradient Fine-tuning Policy gradient (PG) methods are a class of reinforcement learning algorithms that directly learn a parametrized policy, by which an agent selects actions based on the gradient of a scalar performance measure with respect to the policy. Compared to other reinforcement learning methods, PG is well-suited to NLP problems due to its appealing convergence properties and effectiveness in highdimensional spaces (Sutton and Barto, 2018). Previous work on constituent parsing has employed PG methods to mitigate the effect of exposure bias, finding t"
N19-1341,P17-1076,0,0.425887,"thors have proposed new parsing paradigms that aim to both reduce the complexity of existing parsers and improve their speed. Vinyals et al. (2015) proposed a machine translation-inspired sequence-tosequence approach to constituent parsing, where the input is the raw sentence, and the ‘translation’ is a parenthesized version of its tree. G´omezRodr´ıguez and Vilares (2018) reduced constituent parsing to sequence tagging, where only n tagging actions need to be made, and obtained one of the fastest parsers to date. However, the performance is well below the state of the art (Dyer et al., 2016; Stern et al., 2017; Kitaev and Klein, 2018a). Sequence tagging models for constituent parsing are faster, but less accurate than other types of parsers. In this work, we address the following weaknesses of such constituent parsers: (a) high error rates around closing brackets of long constituents, (b) large label sets, leading to sparsity, and (c) error propagation arising from greedy decoding. To effectively close brackets, we train a model that learns to switch between tagging schemes. To reduce sparsity, we decompose the label set and use multi-task learning to jointly learn to predict sublabels. Finally, we"
N19-1341,P18-4013,0,0.107484,"f faith disturbing . JJ . (1,S,NP) (1,VP,) (3,NP,) (-1,NP,) (1,PP,) (-2,S,) (-2,S,ADJP) - Figure 1: A constituent tree linearized as by G´omezRodr´ıguez and Vilares (2018). 2 They (1) generate a dummy label for the last word and (2) pad sentences with a beginning- and end-of-sentence tokens. Sequence Tagging Sequence tagging is a structured prediction task that generates an output label for every input token. Long short-term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997) are a popular architecture for such tasks, often giving stateof-the-art performance (Reimers and Gurevych, 2017; Yang and Zhang, 2018). Tagging with LSTMs In LSTMs, the prediction for the ith element is conditioned on the output of the previous steps. Let LSTMθ (x1:n ) be a parametrized function of the network, where the input is a sequence of vectors x1:n , its output is a sequence of hidden vectors h1:n . To obtain better contextualized hidden vectors, it is possible to instead use bidirectional LSTMS (Schuster and Paliwal, 1997). First, a LSTMlθ processes the tokens from left-to-right and then an independent LSTM rθ processes them from right-to-left. The ith final hidden vector is represented as the concatenation of both"
N19-1341,K18-2001,0,0.0230028,"embeddings (Liu and Zhang, 2017) for the Chinese models, for a more homogeneous comparison against other parsers (Dyer et al., 2016; Liu and Zhang, 2017; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018). ELMo (Peters et al., 2018) or BERT (Devlin et al., 2018) could be used to improve the precision, but in this paper we focus on keeping a good speed-accuracy tradeoff. For SPMRL, no pretrained embeddings are used, following Kitaev and Klein (2018a). As a side note, if we wanted to improve the performance on these languages we could rely on the CoNLL 2018 shared task pretrained word embeddings (Zeman et al., 2018) or even the multilingual BERT model6 . Our models are run on a single CPU7 (and optionally on a consumer-grade GPU for further comparison) using a batch size of 128 for testing. Additional hyperparameters can be found in Appendix A. 4.1 Results Table 1 contrasts the performance of our models against the baseline on the PTB development set. 5 Except for Arabic, for which we do not have the license. https://github.com/google-research/ bert/blob/master/multilingual.md 7 Intel Core i7-7700 CPU 4.2 GHz 3377 6 Model G´omez and Vilares (2018) Our baseline + DE + MTL aux(nt+1 ) aux(nt−1 ) aux(distanc"
N19-1341,W09-3825,0,0.0733709,"parsing time even further. On the SPMRL datasets, we observe even greater improvements across the board, including a new state of the art on Basque, Hebrew, Polish and Swedish.1 1 Introduction Constituent parsing is a core task in natural language processing (NLP), with a wide set of applications. Most competitive parsers are slow, however, to the extent that it is prohibitive of downstream applications in large-scale environments (Kummerfeld et al., 2012). Previous efforts to obtain speed-ups have focused on creating more efficient versions of traditional shift-reduce (Sagae and Lavie, 2006; Zhang and Clark, 2009) or chart-based parsers (Collins, 1997; Charniak, 2000). Zhu et al. (2013), for example, presented 1 After this paper was submitted, Kitaev and Klein (2018b) have improved our results using their previous self-attentive constituent parser (Kitaev and Klein, 2018a) and BERT representations (Devlin et al., 2018) as input to their system. We will acknowledge these results in the Experiments section. Contribution We first explore different factors that prevent sequence tagging constituent parsers from obtaining better results. These include: high error rates when long constituents need to be close"
N19-1341,P13-1043,0,0.606574,"vements across the board, including a new state of the art on Basque, Hebrew, Polish and Swedish.1 1 Introduction Constituent parsing is a core task in natural language processing (NLP), with a wide set of applications. Most competitive parsers are slow, however, to the extent that it is prohibitive of downstream applications in large-scale environments (Kummerfeld et al., 2012). Previous efforts to obtain speed-ups have focused on creating more efficient versions of traditional shift-reduce (Sagae and Lavie, 2006; Zhang and Clark, 2009) or chart-based parsers (Collins, 1997; Charniak, 2000). Zhu et al. (2013), for example, presented 1 After this paper was submitted, Kitaev and Klein (2018b) have improved our results using their previous self-attentive constituent parser (Kitaev and Klein, 2018a) and BERT representations (Devlin et al., 2018) as input to their system. We will acknowledge these results in the Experiments section. Contribution We first explore different factors that prevent sequence tagging constituent parsers from obtaining better results. These include: high error rates when long constituents need to be closed, label sparsity, and error propagation arising from greedy inference. We"
P10-2038,J94-2001,0,0.15989,"Missing"
P10-2038,E09-1087,0,0.0879258,"Missing"
P10-2038,P06-3002,0,0.0496418,"Missing"
P10-2038,P08-1076,0,0.0576269,"Missing"
P10-2038,gimenez-marquez-2004-svmtool,0,0.0626344,"Missing"
P10-2038,N09-2054,0,0.0220906,"Missing"
P10-2038,J93-2004,0,\N,Missing
P10-2038,W03-0407,0,\N,Missing
P11-2009,P06-3002,0,0.0307415,"details. 3 Part-of-speech tagging Our part-of-speech tagging data set is the standard data set from Wall Street Journal included in PennIII (Marcus et al., 1993). We use the standard splits and construct our data set in the following way, following Søgaard (2010): Each word in the data wi is associated with a feature vector xi = hx1i , x2i i where x1i is the prediction on wi of a supervised partof-speech tagger, in our case SVMTool1 (Gimenez and Marquez, 2004) trained on Sect. 0–18, and x2i is a prediction on wi from an unsupervised part-ofspeech tagger (a cluster label), in our case Unsupos (Biemann, 2006) trained on the British National Corpus.2 We train a semi-supervised condensed nearest neighbor classifier on Sect. 19 of the development data and unlabeled data from the Brown corpus and apply it to Sect. 22–24. The labeled data 1 2 http://www.lsi.upc.es/∼nlp/SVMTool/ http://wortschatz.uni-leipzig.de/∼cbiemann/software/ points are thus of the form (one data point or word per line): JJ NNS IN DT JJ NNS IN DT 17* 1 428 425 where the first column is the class labels or the gold tags, the second column the predicted tags and the third column is the ”tags” provided by the unsupervised tagger. Word"
P11-2009,W96-0102,0,0.055146,"el on the mixture of the original labeled data and the newly labeled data. The nearest neighbor algorithm (Cover and Hart, 1967) is a memory-based or so-called lazy learning algorithm. It is one of the most extensively used nonparametric classification algorithms, simple to implement yet powerful, owing to its theoretical properties guaranteeing that for all distributions, its probability of error is bound by twice the Bayes probability of error (Cover and Hart, 1967). Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al., 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (K¨ubler and Zhekova, 2009). Memorybased learning algorithms are said to be lazy because no model is learned from the labeled data points. The labeled data points are the model. Consequently, classification time is proportional to the number of labeled data points. This is of course impractical. Many algorithms have been proposed to make memory-based learning more efficient. The intuition behind many of them is that the set of labeled data points can be reduced or condensed, since many labeled data points are more or less redunda"
P11-2009,gimenez-marquez-2004-svmtool,0,0.419132,"Missing"
P11-2009,R09-1037,0,0.045828,"Missing"
P11-2009,J93-2004,0,0.0500006,"Missing"
P11-2009,W03-3017,0,0.0383025,"a and the newly labeled data. The nearest neighbor algorithm (Cover and Hart, 1967) is a memory-based or so-called lazy learning algorithm. It is one of the most extensively used nonparametric classification algorithms, simple to implement yet powerful, owing to its theoretical properties guaranteeing that for all distributions, its probability of error is bound by twice the Bayes probability of error (Cover and Hart, 1967). Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al., 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (K¨ubler and Zhekova, 2009). Memorybased learning algorithms are said to be lazy because no model is learned from the labeled data points. The labeled data points are the model. Consequently, classification time is proportional to the number of labeled data points. This is of course impractical. Many algorithms have been proposed to make memory-based learning more efficient. The intuition behind many of them is that the set of labeled data points can be reduced or condensed, since many labeled data points are more or less redundant. The algorithms try to extract"
P11-2009,P10-2038,1,0.806413,"Missing"
P11-2009,E09-1087,0,0.234749,"Missing"
P11-2009,D09-1058,0,0.0504824,"Missing"
P11-2120,P09-1042,0,0.2167,"Missing"
P11-2120,P10-2036,0,0.0337938,"Missing"
P11-2120,P04-1061,0,0.294912,"Missing"
P11-2120,P05-1012,0,0.259684,"Missing"
P11-2120,D10-1120,0,0.116731,"Missing"
P11-2120,D09-1086,0,0.204315,"Missing"
P11-2120,W09-1104,0,0.1678,"Missing"
P11-2120,spreyer-etal-2010-training,0,0.0484815,"Missing"
P11-2120,I08-3008,0,0.337049,"and not yet good enough to solve real-world problems. In this paper, we will be interested in an alternative strategy, namely cross-language adaptation of dependency parsers. The idea is, briefly put, to learn how to parse Arabic, for example, from, say, a Danish treebank, comparing unlabeled data from both languages. This is similar to, but more difficult than most domain adaptation or transfer learning scenarios, where differences between source and target distributions are smaller. Most previous work in cross-language adaptation has used parallel corpora to project dependency Related work Zeman and Resnik (2008) simply mapped part-ofspeech tags of source and target language treebanks into a common tagset, delexicalized them (removed all words), trained a parser on the source language treebank and applied it to the target language. The intuition is that, at least for relatively similar languages, features based on part-of-speech tags are enough to do reasonably well, and languages are relatively similar at this level of abstraction. Of course annotations differ, but nouns are likely to be dependents of verbs, prepositions are likely to be dependents of nouns, and so on. Specifically, Zeman and Resnik"
P13-2113,W10-2921,0,0.0236957,"oach taken is similar in spirit to confidenceweighted learning (Dredze et al., 2008). The intuition behind confidence-weighted learning is to more agressively update rare features or features that we are less confident about. In learning with antagonistic adversaries the adversaries delete predictive features; that is, features that we are confident about. When these features are deleted, we do not update the corresponding weights. In relative terms, we therefore update rare features more aggressively than common ones. Note also that by doing so we regularize toward weights with low variance (Bergsma et al., 2010). 3 https://github.com/gracaninja/lxmls-toolkit LDC Catalog No.: LDC2012T13. 5 LDC Catalog No.: LDC2011T03. 4 642 EWT-answers EWT-newsgroups EWT-reviews EWT-weblogs PTB-biomedical PTB-chemistry CHO-broadcast CHO-magazines CHO-weblogs CDT-law CDT-literature CDT-magazines Wilcoxon p macro-av. err.red SP 86.04 87.70 85.96 87.59 95.05 90.32 78.38 78.50 79.64 93.96 93.93 94.95 Our 86.06 87.92 86.10 87.89 95.26 90.60 78.42 78.57 79.76 95.64 94.19 95.06 <0.01 4.0 L∞ 85.90 87.78 85.80 87.60 95.46 90.56 78.27 76.80 79.24 93.91 94.15 94.71 LRA 86.06 87.66 86.00 87.54 94.43 90.58 78.28 78.29 79.37 94.25"
P13-2113,W06-1615,0,0.163002,"Missing"
P13-2113,P07-1033,0,0.1861,"Missing"
P13-2113,N06-1012,0,0.641283,"evelop NLP tools and on-line services. We will do crossdomain experiments using several target domains in order to compute significance across domains, enabling us to say something about likely performance on new domains. Several authors have noted how POS tagging performance is sensitive to cross-domain shifts (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007), and while most authors have assumed known target distributions and pool unlabeled target data in order to automatically correct cross-domain bias (Jiang and Zhai, 2007; Foster et al., 2010), methods such as feature bagging (Sutton et al., 2006), learning with random adversaries (Globerson and Roweis, 2006) and L∞ -regularization (Dekel and Shamir, 2008) have been proposed to improve performance on unknown target distributions. These methods explicitly or implicitly try to minimize average or worst-case expected error across a set of possible test distributions in various ways. These algorithms are related because of the intimate relationship between adversarial corruption and regularization (Ghaoui and Lebret, 1997; Xu et al., Supervised NLP tools and on-line services are often used on data that is very different from the manually a"
P13-2113,D10-1044,0,0.0604725,"Missing"
P13-2113,I11-1100,0,0.0842127,"Missing"
P13-2113,P07-1034,0,0.0828177,"Missing"
P13-2113,D11-1139,0,0.0157885,"by the current model parameters w, but random adversaries randomly select transformations from a predefined set of possible transformations, e.g. deletions of at most k features (Globerson and Roweis, 2006). Feature bagging. In feature bagging (Sutton et al., 2006), the data is represented by different bags of features or different views, and the models learned using different feature bags are combined by averaging. We can reformulate feature bagging as an 1 Note that the batch version of feature bagging is an instance of group L1 regularization (Jacob et al., 2009; Schmidt and Murphy, 2010; Martins et al., 2011). Often group regularization is about finding sparser models rather than robust models. Sparse models can be obtained by grouping correlated features; non-sparse models can be obtained by using independent, exhaustive views. 641 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 4 X = {hyi , xi i}N i=1 , δ deletion rate w0 = 0, v = 0, i = 0 for k ∈ K do for n ∈ N do ξ 1 ← random.sample(P (1) = 1 − δ) ξ 2 ← ||w ||< µ||w ||+ σ||w|| ξ ← (ξ 1 + ξ 2 )(0,1) if sign(w · xn ◦ ξ) 6= yn then wi+1 ← update(wi ) i←i+1 end if v ← v + wi end for end for return w = v/(N × K) We consider part-of-speech (POS)"
P13-2113,W02-1001,0,\N,Missing
P13-2113,petrov-etal-2012-universal,0,\N,Missing
P13-2113,D07-1096,0,\N,Missing
P13-3021,W12-2202,0,0.0202381,"reasonable sub-sentence from the words present in the original. The corpus-example below illustrates how a simplified sentence can be embedded as scattered parts of a non-simplified sentence. The words in bold are the common parts which make up almost the entire human generated simplification and constitutes a suitable simplification on its own. Introduction As a field of research in NLP, text simplification (TS) has gained increasing attention recently, primarily for English text, but also for Brazilian Portuguese (Specia, 2010; Aluísio et al., 2008), Dutch (Daelemans et al., 2004), Spanish (Drndarevic and Saggion, 2012), Danish (Klerke and Søgaard, 2012), French (Seretan, 2012) and Swedish (Rybing and Smith, 2009; Decker, 2003). Our experiments use Danish text which is similar to English in that it has a deep orthography making it hard to map between letters and sounds. Danish has a relatively free word order and sparse morfology. TS can help readers with below average reading skills access information and may supply relevant training material, which is crucial for developing reading skills. However, manual TS is as expensive as translation, which is a key limiting factor on the availability of easy-to-read"
P13-3021,W12-2910,0,0.0120965,"es are presented in the last section. 3 Related work Approaches for automatic TS traditionally focus on lexical substitution (De Belder and Moens, 2012; Specia et al., 2012; Yatskar et al., 2010), on identifying re-write rules at sentence level either manually (Chandrasekar et al., 1996; Carroll et al., 1999; Canning et al., 2000; Siddharthan, 2010; Siddharthan, 2011; Seretan, 2012) or automatically from parallel corpora (Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Zhu et al., 2010) and possibly learning cues for when to apply such changes (Petersen and Ostendorf, 2007; Medero, 2011; Bott et al., 2012). Chandrasekar et al. (1996) propose a structural approach, which uses syntactic cues to recover relative clauses and appositives. Sentence level syntactic re-writing has since seen a variety of manually constructed general sentence splitting rules, 143 4.2 text. Such metrics are used as a tool for teachers and publishers, but existing standard metrics (like Flesch-Kincaid (Flesch, 1948) and LIX (Bjornsson, 1983)) were designed and optimized for easy manual application to human written text, requiring thehuman reader to assess that the text is congruent and coherent. More recent methods promis"
P13-3021,W12-2205,0,0.0252312,"xical substitution from frequency counts and eliminate anaphora by resolving and replacing the referring expressions with the entity referred to. Their system further include compound sentence splitting and rewriting of passive sentences to active ones (Canning et al., 2000). Research into lexical simplification remains an active topic. De Belder and Moens (2012; Specia et al. (2012) are both recent publications of new resources for evaluating lexical simplification in English consisting of lists of synonyms ranked by human judges. Another type of resource is graded word-lists as described in Brooke et al. (2012). Annotator agreement and comparisons so far shows that it is easy to overfit to reflect individual annotator and domain differences that are not of relevance to generalized systems. In a minimally supervised setup, our TS approach can be modified to include lexical simplifications as part of the random generation process. This would require a broad coverage list of words and simpler synonyms, which could for instance be extracted from a parallel corpus like the DSim corpus. For the majority of research in automatic TS the question of what constitutes cognitive load is not discussed. An except"
P13-3021,klerke-sogaard-2012-dsim,1,0.915041,"present in the original. The corpus-example below illustrates how a simplified sentence can be embedded as scattered parts of a non-simplified sentence. The words in bold are the common parts which make up almost the entire human generated simplification and constitutes a suitable simplification on its own. Introduction As a field of research in NLP, text simplification (TS) has gained increasing attention recently, primarily for English text, but also for Brazilian Portuguese (Specia, 2010; Aluísio et al., 2008), Dutch (Daelemans et al., 2004), Spanish (Drndarevic and Saggion, 2012), Danish (Klerke and Søgaard, 2012), French (Seretan, 2012) and Swedish (Rybing and Smith, 2009; Decker, 2003). Our experiments use Danish text which is similar to English in that it has a deep orthography making it hard to map between letters and sounds. Danish has a relatively free word order and sparse morfology. TS can help readers with below average reading skills access information and may supply relevant training material, which is crucial for developing reading skills. However, manual TS is as expensive as translation, which is a key limiting factor on the availability of easy-to-read material. One of the persistent cha"
P13-3021,E99-1042,0,0.658916,"-9 2013. 2013 Association for Computational Linguistics designed to operate both on dependencies and phrase structure trees, and typically including lexical cues (Siddharthan, 2011; Heilman and Smith, 2010; Canning et al., 2000). Similar rules have been created from direct inspection of simplification corpora (Decker, 2003; Seretan, 2012) and discovered automatically from large scale aligned corpora (Woodsend and Lapata, 2011; Zhu et al., 2010). In our experiment we apply few basic sentence splitting rules as a pre-processing technique before using an over-generating random deletion approach. Carroll et al. (1999) perform lexical substitution from frequency counts and eliminate anaphora by resolving and replacing the referring expressions with the entity referred to. Their system further include compound sentence splitting and rewriting of passive sentences to active ones (Canning et al., 2000). Research into lexical simplification remains an active topic. De Belder and Moens (2012; Specia et al. (2012) are both recent publications of new resources for evaluating lexical simplification in English consisting of lists of synonyms ranked by human judges. Another type of resource is graded word-lists as de"
P13-3021,C96-2183,0,0.232452,"n be observed in professionally simplified text. (Medero, 2011; Klerke, 2012) The next section positions this research in the context of related work. Section 4 presents the experimental setup including generation and evaluation. In Section 5, the results are presented and discussed and, finally, concluding remarks and future perspectives are presented in the last section. 3 Related work Approaches for automatic TS traditionally focus on lexical substitution (De Belder and Moens, 2012; Specia et al., 2012; Yatskar et al., 2010), on identifying re-write rules at sentence level either manually (Chandrasekar et al., 1996; Carroll et al., 1999; Canning et al., 2000; Siddharthan, 2010; Siddharthan, 2011; Seretan, 2012) or automatically from parallel corpora (Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Zhu et al., 2010) and possibly learning cues for when to apply such changes (Petersen and Ostendorf, 2007; Medero, 2011; Bott et al., 2012). Chandrasekar et al. (1996) propose a structural approach, which uses syntactic cues to recover relative clauses and appositives. Sentence level syntactic re-writing has since seen a variety of manually constructed general sentence splitting rules, 143 4.2 text. Such"
P13-3021,P07-2045,0,0.00251746,"SMT system, a large resource of aligned parallel text and a language model of the target language are needed. We combined the 25 million words Danish Korpus 20001 with the entire 1.75 million words unaligned DSim corpus (Klerke and Søgaard, 2012) to build the language model2 . Including both corpora gives better coverage and assigns lower average ppl and a simlar difference in average ppl between the two sides of a held out part of the DSim corpus compared to using only the simplified part of DSim for the language model. Following Coster and Kauchak (2011), we used the phrase-based SMT Moses (Koehn et al., 2007), with GIZA++ wordalignment (Och and Ney, 2000) and phrase tables learned from the sentence aligned portion of the DSim corpus. Structural Heuristics To preserve nodes from later deletion we applied heuristics using simple structural cues from the dependency structures. We favored nodes headed by a subject relation, subj, and object relations, *obj, and negating modifiers (the Danish word ikke) under the assumption that these were most likely to be important for preserving semantics and generating wellformed candidates under the sampling procedure described below. The heuristics were applied b"
P13-3021,P11-2117,0,0.260408,"p including generation and evaluation. In Section 5, the results are presented and discussed and, finally, concluding remarks and future perspectives are presented in the last section. 3 Related work Approaches for automatic TS traditionally focus on lexical substitution (De Belder and Moens, 2012; Specia et al., 2012; Yatskar et al., 2010), on identifying re-write rules at sentence level either manually (Chandrasekar et al., 1996; Carroll et al., 1999; Canning et al., 2000; Siddharthan, 2010; Siddharthan, 2011; Seretan, 2012) or automatically from parallel corpora (Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Zhu et al., 2010) and possibly learning cues for when to apply such changes (Petersen and Ostendorf, 2007; Medero, 2011; Bott et al., 2012). Chandrasekar et al. (1996) propose a structural approach, which uses syntactic cues to recover relative clauses and appositives. Sentence level syntactic re-writing has since seen a variety of manually constructed general sentence splitting rules, 143 4.2 text. Such metrics are used as a tool for teachers and publishers, but existing standard metrics (like Flesch-Kincaid (Flesch, 1948) and LIX (Bjornsson, 1983)) were designed and optimized for easy manu"
P13-3021,daelemans-etal-2004-automatic,0,0.06877,"Missing"
P13-3021,C00-2163,0,0.0764025,"text and a language model of the target language are needed. We combined the 25 million words Danish Korpus 20001 with the entire 1.75 million words unaligned DSim corpus (Klerke and Søgaard, 2012) to build the language model2 . Including both corpora gives better coverage and assigns lower average ppl and a simlar difference in average ppl between the two sides of a held out part of the DSim corpus compared to using only the simplified part of DSim for the language model. Following Coster and Kauchak (2011), we used the phrase-based SMT Moses (Koehn et al., 2007), with GIZA++ wordalignment (Och and Ney, 2000) and phrase tables learned from the sentence aligned portion of the DSim corpus. Structural Heuristics To preserve nodes from later deletion we applied heuristics using simple structural cues from the dependency structures. We favored nodes headed by a subject relation, subj, and object relations, *obj, and negating modifiers (the Danish word ikke) under the assumption that these were most likely to be important for preserving semantics and generating wellformed candidates under the sampling procedure described below. The heuristics were applied both to trees, acting by preserving entire subtr"
P13-3021,W12-2019,0,0.0458858,"manually constructed general sentence splitting rules, 143 4.2 text. Such metrics are used as a tool for teachers and publishers, but existing standard metrics (like Flesch-Kincaid (Flesch, 1948) and LIX (Bjornsson, 1983)) were designed and optimized for easy manual application to human written text, requiring thehuman reader to assess that the text is congruent and coherent. More recent methods promise to be applicable to unassessed text. Language modeling in particular has shown to be a robust and informative component of systems for assessing text readability (Schwarm and Ostendorf, 2005; Vajjala and Meurers, 2012) as it is better suited to evaluate grammaticality than standard metrics. We use language modeling alongside traditional metrics for selecting good simplification candidates. 4 4.1 Experimental setup Three system variants were set up to generate simplified output from the original news wire of the development and test partitions of the DSim corpus. The texts were dependency-parsed using Bohnet’s parser (Bohnet, 2010) trained on the Danish Treebank3 (Kromann, 2003) with default settings4 . 1. Split only performed simple sentence splitting. 2. Sample over-generated candidates by sampling the heu"
P13-3021,D11-1038,0,0.395642,"in Japan To generate candidate sub-sentences we use a random deletion procedure in combination with 142 Proceedings of the ACL Student Research Workshop, pages 142–149, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics designed to operate both on dependencies and phrase structure trees, and typically including lexical cues (Siddharthan, 2011; Heilman and Smith, 2010; Canning et al., 2000). Similar rules have been created from direct inspection of simplification corpora (Decker, 2003; Seretan, 2012) and discovered automatically from large scale aligned corpora (Woodsend and Lapata, 2011; Zhu et al., 2010). In our experiment we apply few basic sentence splitting rules as a pre-processing technique before using an over-generating random deletion approach. Carroll et al. (1999) perform lexical substitution from frequency counts and eliminate anaphora by resolving and replacing the referring expressions with the entity referred to. Their system further include compound sentence splitting and rewriting of passive sentences to active ones (Canning et al., 2000). Research into lexical simplification remains an active topic. De Belder and Moens (2012; Specia et al. (2012) are both r"
P13-3021,D08-1020,0,0.0548082,"Missing"
P13-3021,N10-1056,0,0.0289481,"dramatically reduce sentence length. Both simple deletions and extraction of clauses can be observed in professionally simplified text. (Medero, 2011; Klerke, 2012) The next section positions this research in the context of related work. Section 4 presents the experimental setup including generation and evaluation. In Section 5, the results are presented and discussed and, finally, concluding remarks and future perspectives are presented in the last section. 3 Related work Approaches for automatic TS traditionally focus on lexical substitution (De Belder and Moens, 2012; Specia et al., 2012; Yatskar et al., 2010), on identifying re-write rules at sentence level either manually (Chandrasekar et al., 1996; Carroll et al., 1999; Canning et al., 2000; Siddharthan, 2010; Siddharthan, 2011; Seretan, 2012) or automatically from parallel corpora (Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Zhu et al., 2010) and possibly learning cues for when to apply such changes (Petersen and Ostendorf, 2007; Medero, 2011; Bott et al., 2012). Chandrasekar et al. (1996) propose a structural approach, which uses syntactic cues to recover relative clauses and appositives. Sentence level syntactic re-writing has since"
P13-3021,P05-1065,0,0.0391196,"g has since seen a variety of manually constructed general sentence splitting rules, 143 4.2 text. Such metrics are used as a tool for teachers and publishers, but existing standard metrics (like Flesch-Kincaid (Flesch, 1948) and LIX (Bjornsson, 1983)) were designed and optimized for easy manual application to human written text, requiring thehuman reader to assess that the text is congruent and coherent. More recent methods promise to be applicable to unassessed text. Language modeling in particular has shown to be a robust and informative component of systems for assessing text readability (Schwarm and Ostendorf, 2005; Vajjala and Meurers, 2012) as it is better suited to evaluate grammaticality than standard metrics. We use language modeling alongside traditional metrics for selecting good simplification candidates. 4 4.1 Experimental setup Three system variants were set up to generate simplified output from the original news wire of the development and test partitions of the DSim corpus. The texts were dependency-parsed using Bohnet’s parser (Bohnet, 2010) trained on the Danish Treebank3 (Kromann, 2003) with default settings4 . 1. Split only performed simple sentence splitting. 2. Sample over-generated ca"
P13-3021,C10-1152,0,0.393314,"ate sub-sentences we use a random deletion procedure in combination with 142 Proceedings of the ACL Student Research Workshop, pages 142–149, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics designed to operate both on dependencies and phrase structure trees, and typically including lexical cues (Siddharthan, 2011; Heilman and Smith, 2010; Canning et al., 2000). Similar rules have been created from direct inspection of simplification corpora (Decker, 2003; Seretan, 2012) and discovered automatically from large scale aligned corpora (Woodsend and Lapata, 2011; Zhu et al., 2010). In our experiment we apply few basic sentence splitting rules as a pre-processing technique before using an over-generating random deletion approach. Carroll et al. (1999) perform lexical substitution from frequency counts and eliminate anaphora by resolving and replacing the referring expressions with the entity referred to. Their system further include compound sentence splitting and rewriting of passive sentences to active ones (Canning et al., 2000). Research into lexical simplification remains an active topic. De Belder and Moens (2012; Specia et al. (2012) are both recent publications"
P13-3021,seretan-2012-acquisition,0,0.196875,"-example below illustrates how a simplified sentence can be embedded as scattered parts of a non-simplified sentence. The words in bold are the common parts which make up almost the entire human generated simplification and constitutes a suitable simplification on its own. Introduction As a field of research in NLP, text simplification (TS) has gained increasing attention recently, primarily for English text, but also for Brazilian Portuguese (Specia, 2010; Aluísio et al., 2008), Dutch (Daelemans et al., 2004), Spanish (Drndarevic and Saggion, 2012), Danish (Klerke and Søgaard, 2012), French (Seretan, 2012) and Swedish (Rybing and Smith, 2009; Decker, 2003). Our experiments use Danish text which is similar to English in that it has a deep orthography making it hard to map between letters and sounds. Danish has a relatively free word order and sparse morfology. TS can help readers with below average reading skills access information and may supply relevant training material, which is crucial for developing reading skills. However, manual TS is as expensive as translation, which is a key limiting factor on the availability of easy-to-read material. One of the persistent chalenges of TS is that dif"
P13-3021,W12-2203,0,0.023667,"ator agreement and comparisons so far shows that it is easy to overfit to reflect individual annotator and domain differences that are not of relevance to generalized systems. In a minimally supervised setup, our TS approach can be modified to include lexical simplifications as part of the random generation process. This would require a broad coverage list of words and simpler synonyms, which could for instance be extracted from a parallel corpus like the DSim corpus. For the majority of research in automatic TS the question of what constitutes cognitive load is not discussed. An exception is Siddharthan and Katsos (2012), who seek to isolate the psycholinguistically motivated notions of sentence comprehension from sentence acceptability by actually measuring the effect of TS on cognition on a small scale. Readability research is a line of research that is more directly concerned with the nature of cognitive load in reading building on insights from psycholinguistics. One goal is to develop techniques and metrics for assessing the readability of unseen general dependency-based heuristics for conserving main sentence constituents, and then introduce a loss-function for choosing between candidates. Since we avoi"
P13-3021,W10-4213,0,0.0218089,"2012) The next section positions this research in the context of related work. Section 4 presents the experimental setup including generation and evaluation. In Section 5, the results are presented and discussed and, finally, concluding remarks and future perspectives are presented in the last section. 3 Related work Approaches for automatic TS traditionally focus on lexical substitution (De Belder and Moens, 2012; Specia et al., 2012; Yatskar et al., 2010), on identifying re-write rules at sentence level either manually (Chandrasekar et al., 1996; Carroll et al., 1999; Canning et al., 2000; Siddharthan, 2010; Siddharthan, 2011; Seretan, 2012) or automatically from parallel corpora (Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Zhu et al., 2010) and possibly learning cues for when to apply such changes (Petersen and Ostendorf, 2007; Medero, 2011; Bott et al., 2012). Chandrasekar et al. (1996) propose a structural approach, which uses syntactic cues to recover relative clauses and appositives. Sentence level syntactic re-writing has since seen a variety of manually constructed general sentence splitting rules, 143 4.2 text. Such metrics are used as a tool for teachers and publishers, but exi"
P13-3021,W11-2802,0,0.0992476,"as hit by earthquakes in Japan Simplified: Der er målt en stor mængde radioaktivt materiale i havet nær atom-kraftværket Fukushima i Japan . A large amount of radioactivity has been measured in the sea near the nuclear power plant Fukushima in Japan To generate candidate sub-sentences we use a random deletion procedure in combination with 142 Proceedings of the ACL Student Research Workshop, pages 142–149, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics designed to operate both on dependencies and phrase structure trees, and typically including lexical cues (Siddharthan, 2011; Heilman and Smith, 2010; Canning et al., 2000). Similar rules have been created from direct inspection of simplification corpora (Decker, 2003; Seretan, 2012) and discovered automatically from large scale aligned corpora (Woodsend and Lapata, 2011; Zhu et al., 2010). In our experiment we apply few basic sentence splitting rules as a pre-processing technique before using an over-generating random deletion approach. Carroll et al. (1999) perform lexical substitution from frequency counts and eliminate anaphora by resolving and replacing the referring expressions with the entity referred to. Th"
P13-3021,S12-1046,0,\N,Missing
P14-2062,W10-0701,0,0.0321628,", we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks. 1 Introduction Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive. Snow et al. (2008) showed, however, that crowdsourced annotations can produce similar results to annotations made by experts. Crowdsourcing services such as Amazon’s Mechanical Turk has since been successfully used for various annotation tasks in NLP (Jha et al., 2010; Callison-Burch and Dredze, 2010). However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010). A large part of NLP problems, however, are structured prediction tasks. Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations. Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder. 1 One of the reviewers alerted us to an unpublished masters the"
P14-2062,R13-1026,0,0.0281052,"Missing"
P14-2062,N13-1037,0,0.0131419,", and the data contains mostly non-entities, so the complexity is manageable. The question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed.1 In this paper, we investigate how well lay annotators can produce POS labels for Twitter data. In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side). Our choice of annotating Twitter data is not coincidental: with the shortlived nature of Twitter messages, models quickly lose predictive power (Eisenstein, 2013), and retraining models on new samples of more representative data becomes necessary. Expensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter. We use a minimum of instructions and require few qualifications. Obviously, lay annotation is generally less reliable than professional annotation. It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations. In this paper we compare two aggregation schemes, namely majority voting (MV) and MACE (Hovy et al., 2013). We also sho"
P14-2062,W10-0713,0,0.224342,"Missing"
P14-2062,I11-1100,0,0.0604865,"Missing"
P14-2062,P11-2008,0,0.210133,"Missing"
P14-2062,D08-1027,0,0.435201,"Missing"
P14-2062,W10-0702,0,0.140457,"s experts. Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks. 1 Introduction Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive. Snow et al. (2008) showed, however, that crowdsourced annotations can produce similar results to annotations made by experts. Crowdsourcing services such as Amazon’s Mechanical Turk has since been successfully used for various annotation tasks in NLP (Jha et al., 2010; Callison-Burch and Dredze, 2010). However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010). A large part of NLP problems, however, are structured prediction tasks. Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations. Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder. 1 One of the reviewers alerte"
P14-2062,Q13-1001,0,0.0363069,"Missing"
P14-2062,D07-1031,0,0.113169,"Missing"
P14-2062,D12-1127,0,0.0605467,"Missing"
P14-2062,J94-2001,0,0.583625,"Missing"
P14-2062,N13-1039,0,0.0526092,"Missing"
P14-2062,petrov-etal-2012-universal,0,0.0651366,"and annotation effort, we can produce structured annotations of near-expert quality. We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexicons (Li et al., 2012). Finally, we show that models learned from these annotations are competitive with models learned from expert annotations on various downstream tasks. 2 In order to use the annotations to train models that can be applied across various data sets, i.e., making out-of-sample evaluation possible (see Section 5), we follow Hovy et al. (2014) in using the universal tag set (Petrov et al., 2012) with 12 labels. Our Approach We crowdsource the training section of the data from Gimpel et al. (2011)2 with POS tags. We use Crowdflower,3 to collect five annotations for each word, and then find the most likely label for each word among the possible annotations. See Figure 1 for an example. If the correct label is not among the annotations, we are unable to recover the correct answer. This was the case for 1497 instances in our data (cf. the token “:” in the example). We thus report on oracle score, i.e., the best label sequence that could possibly be found, which is correct except for the"
P14-2062,P09-1057,0,0.0433099,"Missing"
P14-2062,D11-1141,0,0.0374043,"Missing"
P14-2062,N03-1028,0,0.197607,"Missing"
P14-2062,hovy-etal-2014-pos,1,\N,Missing
P14-2062,N13-1132,1,\N,Missing
P14-2083,W07-1508,0,0.074711,"Missing"
P14-2083,P11-2008,0,0.0296121,"Missing"
P14-2083,N13-1132,1,0.642283,"Missing"
P14-2083,P14-2062,1,0.89815,"Missing"
P14-2083,jurgens-2014-analysis,0,0.0843971,"Missing"
P14-2083,J93-2004,0,0.0463277,"Missing"
P14-2083,petrov-etal-2012-universal,0,0.0307925,"Missing"
P14-2083,E14-1078,1,0.860375,"Missing"
P14-2083,C12-1149,0,0.0661935,"e hard cases. Moreover, we compare professional annotation to that of lay people. We instructed annotators to use the 12 universal POS tags of Petrov et al. (2012). We did so in order to make comparison between existing data sets possible. Moreover, this allows us to focus on really hard cases, as any debatable case in the coarse-grained tag set is necessarily also part of the finer-grained tag set.2 For each domain, we collected exactly 500 doubly-annotated sentences/tweets. Besides these English data sets, we also obtained doubly-annotated POS data from the French Social Media Bank project (Seddah et al., 2012).3 All data sets, except the French one, are publicly available at http://lowlands.ku. dk/. We present disagreements as Hinton diagrams in Figure 1a–c. Note that the spoken language data does not include punctuation. The correlations between the disagreements are highly significant, with Spearman coefficients ranging from 0.644 Our analyses show that a) experts disagree on the known hard cases when freely annotating text, and b) that these disagreements are the same across text types. More surprisingly, though, we also find that, as discussed next, c) roughly the same disagreements are also ob"
P14-2083,E03-1068,0,0.0104121,"probably because they rely more on orthographic cues than on distributional evidence. The disagreements are still strongly correlated with the ones observed with expert annotators, but at a slightly lower coefficient (with a Spearman’s ρ of 0.493 and Kendall’s τ of 0.366 for WSJ). Figure 3: Disagreement on French social media of the variation. In this section, we investigate what happens if we weed out obvious errors by detecting annotation inconsistencies across a corpus. The disagreements that remain are the truly hard cases. We use a modified version of the a priori algorithm introduced in Dickinson and Meurers (2003) to identify annotation inconsistencies. It works by collecting “variation n-grams”, i.e. the longest sequence of words (n-gram) in a corpus that has been observed with a token being tagged differently in another occurence of the same n-gram in the same corpus. The algorithm starts off by looking for unigrams and expands them until no longer n-grams are found. For each variation n-gram that we found in WSJ-00, i.e, a word in various contexts and the possible tags associated with it, we present annotators with the cross product of contexts and tags. Essentially, we ask for a binary decision: Is"
P15-1165,P14-2131,0,0.0530581,"a mean vector to out-of-vocabulary words. System For our system, we simply augment the delexicalized POS tagger with the I NVERTED distributional representation of the current word. The best parameter setting on Spanish development data was σ = 0.01, δ = 160. 3.3 http://code.google.com/p/uni-dep-tb/ https://code.google.com/p/ wikily-supervised-pos-tagger/ 8 For our embeddings baselines, we augment the feature space by adding embedding vectors for head h and dependent d. We experimented with different versions of combining embedding vectors, from firing separate h and d per-dimension features (Bansal et al., 2014) to combining their information. We found that combining the embeddings of h and d is effective and consistently use the absolute difference between the embedding vectors, since that worked better than addition and multiplication on development data. Delexicalized transfer (D ELEX) uses three (3) iterations over the data in both the single-source and the multi-source set-up, a parameter set on the Spanish development data. The remaining parameters were obtained by averaging over performance with different embeddings on the Spanish development data, obtaining: σ = 0.005, δ = 20, i = 3, and abso"
P15-1165,P14-1023,0,0.0109756,"proaches focus on different kinds of similarity, some more syntactic, some more semantic. The representations are typically either clusters of distributionally similar words, e.g., Brown et al. (1992), or vector representations. In this paper, we focus on vector representations. In vector-based approaches, similar representations are vectors close in some multi-dimensional space. 2.1 Count-based and prediction-based representations There are, briefly put, two approaches to inducing vector-based distributional word representations from large corpora: count-based and predictionbased approaches (Baroni et al., 2014). Countbased approaches represent words by their cooccurrences. Dimensionality reduction is typically performed on a raw or weighted co-occurrence matrix using methods such as singular value decomposition (SVD), a method for maximizing the variance in a dataset in few dimensions. In our inverted indexing, we use raw co-occurrence data. Prediction-based methods use discriminative learning techniques to learn how to predict words from their context, or vice versa. They rely on a neural network architecture, and once the network converges, they use word representations from a middle layer as thei"
P15-1165,C10-1011,1,0.695318,"ing. For compatibility with Xiao and Guo (2014), we also present results on CoNLL 2006 and 2007 treebanks for languages for which we had baseline and system word representations (de, es, sv). Our parameter settings for these experiments were the same as those tuned on the Spanish development data from the Google Universal Treebanks v. 1.0. Baselines The most obvious baseline in our experiments is delexicalized transfer (D ELEX) (McDonald et al., 2011; Søgaard, 2011). This baseline system simply learns models without lexical features. We use a modified version of the first-order Mate 7 parser (Bohnet, 2010) that also takes continuousvalued embeddings as input an disregards features that include lexical items. 3.4 Word alignment Data We use the manually word-aligned EnglishSpanish Europarl data from Graca et al. (2008). The dataset contains 100 sentences. The annotators annotated whether word alignments were certain or possible, and we present results with all word alignments and with only the certain ones. See Graca et al. (2008) for details. Baselines For word alignment, we simply align every aligned word in the gold data, for which we have a word embedding, to its (Euclidean) nearest neighbor"
P15-1165,J92-4003,0,0.127536,"al and sparse models. Also, simple bagof-words models fail to capture the relatedness of words. In many tasks, synonymous words should be treated alike, but their bag-of-words representations are as different as those of dog and therefore. Distributional word representations are supposed to capture distributional similarities between words. Intuitively, we want similar words to have similar representations. Known approaches focus on different kinds of similarity, some more syntactic, some more semantic. The representations are typically either clusters of distributionally similar words, e.g., Brown et al. (1992), or vector representations. In this paper, we focus on vector representations. In vector-based approaches, similar representations are vectors close in some multi-dimensional space. 2.1 Count-based and prediction-based representations There are, briefly put, two approaches to inducing vector-based distributional word representations from large corpora: count-based and predictionbased approaches (Baroni et al., 2014). Countbased approaches represent words by their cooccurrences. Dimensionality reduction is typically performed on a raw or weighted co-occurrence matrix using methods such as sing"
P15-1165,W02-1001,0,0.0575931,"s well as tag dictionaries (Li et al., 2012) needed for the POS tagging experiments. Baselines One baseline method is a typeconstrained structured perceptron with only ortographic features, which are expected to transfer across languages. The type constraints come from Wiktionary, a crowd-sourced tag dictionary.8 Type constraints from Wiktionary were first used by Li et al. (2012), but note that their set-up is unsupervised learning. T¨ackstr¨om et al. (2013) also used type constraints in a supervised set-up. Our learning algorithm is the structured perceptron algorithm originally proposed by Collins (2002). In our POS tagging experiments, we always do 10 passes over the data. We also present two other baselines, where we augment the feature representation with different embeddings for the target word, K LEMENTIEV and C HANDAR. With all the embeddings in POS tagging, we assign a mean vector to out-of-vocabulary words. System For our system, we simply augment the delexicalized POS tagger with the I NVERTED distributional representation of the current word. The best parameter setting on Spanish development data was σ = 0.01, δ = 160. 3.3 http://code.google.com/p/uni-dep-tb/ https://code.google.com"
P15-1165,N15-1157,1,0.757356,"peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply using several source languages. This paper introduces a simple method for obtaining truly inter-lingual word representations in order to train models with lexical features on several source languages at the same time. Briefly put,"
P15-1165,graca-etal-2008-building,0,0.0195628,"tings for these experiments were the same as those tuned on the Spanish development data from the Google Universal Treebanks v. 1.0. Baselines The most obvious baseline in our experiments is delexicalized transfer (D ELEX) (McDonald et al., 2011; Søgaard, 2011). This baseline system simply learns models without lexical features. We use a modified version of the first-order Mate 7 parser (Bohnet, 2010) that also takes continuousvalued embeddings as input an disregards features that include lexical items. 3.4 Word alignment Data We use the manually word-aligned EnglishSpanish Europarl data from Graca et al. (2008). The dataset contains 100 sentences. The annotators annotated whether word alignments were certain or possible, and we present results with all word alignments and with only the certain ones. See Graca et al. (2008) for details. Baselines For word alignment, we simply align every aligned word in the gold data, for which we have a word embedding, to its (Euclidean) nearest neighbor in the target sentence. We evaluate this strategy by its precision (P@1). System We compare I NVERTED with K LEMEN TIEV and C HANDAR . To ensure a fair comparison, we use the subset of words covered by all three emb"
P15-1165,C12-1089,0,0.772355,"th rely on three level architectures with input, output and a middle layer for intermediate target word representations. The major difference is that skip-gram uses the target word as input and the context as output, whereas the CBOW model does it the other way around. Learning goes by back-propagation, and random target words are used as negative examples. Levy and Goldberg (2014) show that prediction-based representations obtained with the skip-gram model can be related to count-based ones obtained with PMI. They argue that which is best, varies across tasks. 2.1.2 Bilingual representations Klementiev et al. (2012) learn distinct embedding models for the source and target languages, but while learning to minimize the sum of the two models’ losses, they jointly learn a regularizing interaction matrix, enforcing word pairs aligned in parallel text to have similar representations. Note that Klementiev et al. (2012) rely on word-aligned parallel text, and thereby on a large-coverage soft mapping of source words to target words. Other approaches rely on small coverage dictionaries with hard 1:1 mappings between words. Klementiev et al. (2012) do not use skip-gram or CBOW, but the language model presented in"
P15-1165,P14-2050,0,0.00763126,"ed Table 1: Three nearest neighbors in the English training data of six words occurring in the Spanish test data, in the embeddings used in our experiments. Only 2/6 words were in the German data. skip-gram model and CBOW. The two models both rely on three level architectures with input, output and a middle layer for intermediate target word representations. The major difference is that skip-gram uses the target word as input and the context as output, whereas the CBOW model does it the other way around. Learning goes by back-propagation, and random target words are used as negative examples. Levy and Goldberg (2014) show that prediction-based representations obtained with the skip-gram model can be related to count-based ones obtained with PMI. They argue that which is best, varies across tasks. 2.1.2 Bilingual representations Klementiev et al. (2012) learn distinct embedding models for the source and target languages, but while learning to minimize the sum of the two models’ losses, they jointly learn a regularizing interaction matrix, enforcing word pairs aligned in parallel text to have similar representations. Note that Klementiev et al. (2012) rely on word-aligned parallel text, and thereby on a lar"
P15-1165,D12-1127,0,0.0369956,"Missing"
P15-1165,D11-1006,0,0.435697,"the availability of English resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao"
P15-1165,D09-1139,0,0.0521645,"Missing"
P15-1165,P10-1114,0,0.0276803,"on and word alignment, we fix the number of dimensions to 40. For both our baselines and systems, we also tune a scaling factor σ ∈ {1.0, 0.1, 0.01, 0.001} for POS tagging and dependency parsing, using the scaling method from Turian et al. (2010), also used in Gouws and Søgaard (2015). We do not scale our embeddings for document classification or word alignment. 3 Experiments The data set characteristics are found in Table 2.3. 3.1 Document classification Data Our first document classification task is topic classification on the cross-lingual multi-domain sentiment analysis dataset A MAZON in Prettenhofer and Stein (2010).4 We represent each document by the average of the representations of those words that we find both in the documents and in our embeddings. Rather than classifying reviews by sentiment, we classify by topic, trying to discriminate between book reviews, music reviews and DVD reviews, as a three-way classification problem, training on English and testing on German. Unlike in the other tasks below, we always 4 use unscaled word representations, since these are our only features. All word representations have 40 dimensions. The other document classification task is a fourway classification proble"
P15-1165,P11-1061,0,0.039505,"the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply using several source lan"
P15-1165,P11-2120,1,0.534532,"lish resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014)."
P15-1165,I05-1075,0,0.0207395,"reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply u"
P15-1165,Q13-1001,0,0.0450202,"Missing"
P15-1165,P10-1040,0,0.0800173,"NLL 07 – D EPENDENCY PARSING en es de sv 18.6 – – – 447k – – – en es – – – – – 206 357 389 – 5.7k 5.7k 5.7k – 0.841 0.616 n/a E UROPARL – W ORD A LIGNMENT 100 100 – – 0.370 0.533 Table 2: Characteristics of the data sets. Embeddings coverage (token-level) for K LEMENTIEV, C HAN DAR and I NVERTED on the test sets. We use the common vocabulary on W ORD A LIGNMENT . sification and word alignment, we fix the number of dimensions to 40. For both our baselines and systems, we also tune a scaling factor σ ∈ {1.0, 0.1, 0.01, 0.001} for POS tagging and dependency parsing, using the scaling method from Turian et al. (2010), also used in Gouws and Søgaard (2015). We do not scale our embeddings for document classification or word alignment. 3 Experiments The data set characteristics are found in Table 2.3. 3.1 Document classification Data Our first document classification task is topic classification on the cross-lingual multi-domain sentiment analysis dataset A MAZON in Prettenhofer and Stein (2010).4 We represent each document by the average of the representations of those words that we find both in the documents and in our embeddings. Rather than classifying reviews by sentiment, we classify by topic, trying t"
P15-1165,W14-1613,0,0.53136,"2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, b"
P15-1165,I08-3008,0,0.017072,"asons for this; namely, the availability of English resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackst"
P15-2044,I05-1075,0,0.892023,"the mappings in Petrov et al. (2011) for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian). The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/. Introduction Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages. While it has been observed repeatedly that using multiple source languages improves performance (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Fossum and Abney, 2005; McDonald et al., 2011), most available techniques work best for closely related languages. In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources. Most low-resource languages 2 Experiments Our approach is a combination of simple techniques. Part of the process is depicted in Figure 1, and the algorithm is presented in Algorithm 1. Assume we have n languages for which we assume the availability of m verses of the Bible. We run IBM-21 on all n(n − 1) pairs of languages. Assume also manual"
P15-2044,N13-1014,0,0.0620146,"Missing"
P15-2044,D12-1127,0,0.146746,"Missing"
P15-2044,D11-1006,0,0.108197,"t al. (2011) for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian). The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/. Introduction Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages. While it has been observed repeatedly that using multiple source languages improves performance (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Fossum and Abney, 2005; McDonald et al., 2011), most available techniques work best for closely related languages. In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources. Most low-resource languages 2 Experiments Our approach is a combination of simple techniques. Part of the process is depicted in Figure 1, and the algorithm is presented in Algorithm 1. Assume we have n languages for which we assume the availability of m verses of the Bible. We run IBM-21 on all n(n − 1) pairs of languages. Assume also manually POS-annotated trainin"
P15-2044,W03-0414,0,0.043386,"tical to, overlaps, is a subset, or is a superset of the Wiktionary tags. German, and Spanish to Czech and French. The resulting annotated target language corpora enable them to train POS taggers for these languages. Yarowsky and Ngai (2001) showed similar results using just the Hansards corpus on English to French and Chinese. Our work is inspired by these approaches, yet broader in scope on both the source and target side. Das and Petrov (2011) use word-aligned text to automatically create type-level tag dictionaries. Earlier work on building tag dictionaries from word-aligned text includes Probst (2003). Their tag dictionaries contain target language trigrams to be able to disambiguate ambiguous target language words. To handle the noise in the automatically obtained dictionaries, they use label propagation on a similarity graph to smooth and expand the label distributions. Our approach is similar to theirs in using projections to obtain type-level tag dictionaries, but we keep the token supervision and type supervision apart and end up with a model more similar to that of T¨ackstr¨om et al. (2013), who combine word-aligned text with crowdsourced type-level tag dictionaries. T¨ackstr¨om et a"
P15-2044,W14-5302,0,0.0715114,"ollow T¨ackstr¨om et al. (2013) in using our automatically created, not crowdsourced, tag dictionaries to prune tags during search, but we use word alignments to obtain token-level annotations that we use as annotated training data, similar to Fossum Related work The Bible has been used as a resource for machine translation and multi-lingual information retrieval before, e.g., (Chew et al., 2006). It has also been used in cross-lingual POS tagging (Yarowsky et al., 2001; Fossum and Abney, 2005), NP-chunking (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and cross-lingual dependency parsing (Sukhareva and Chiarcos, 2014) before. Yarowsky et al. (2001) and Fossum and Abney (2005) use word-aligned parallel translations of the Bible to project the predictions of POS taggers for several language pairs, including English, 271 and Abney (2005), Yarowsky et al. (2001), and Yarowsky and Ngai (2001). Duong et al. (2013) use word-alignment probabilities to select training data for their cross-lingual POS models. They consider a simple single-source training set-up. We also tried ranking projected training data by confidence, using an ensemble of projections from 17–99 source languages and majority voting to obtain prob"
P15-2044,Q13-1001,0,0.0845969,"Missing"
P15-2044,N01-1026,0,0.568625,"cly available and extend the mappings in Petrov et al. (2011) for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian). The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/. Introduction Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages. While it has been observed repeatedly that using multiple source languages improves performance (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Fossum and Abney, 2005; McDonald et al., 2011), most available techniques work best for closely related languages. In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources. Most low-resource languages 2 Experiments Our approach is a combination of simple techniques. Part of the process is depicted in Figure 1, and the algorithm is presented in Algorithm 1. Assume we have n languages for which we assume the availability of m verses of the Bible. We run IBM-21 on all n(n − 1) pairs of langu"
P15-2044,N10-1083,0,0.0532965,"Missing"
P15-2044,H01-1035,0,0.956542,"for 100 languages publicly available and extend the mappings in Petrov et al. (2011) for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian). The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/. Introduction Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages. While it has been observed repeatedly that using multiple source languages improves performance (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Fossum and Abney, 2005; McDonald et al., 2011), most available techniques work best for closely related languages. In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources. Most low-resource languages 2 Experiments Our approach is a combination of simple techniques. Part of the process is depicted in Figure 1, and the algorithm is presented in Algorithm 1. Assume we have n languages for which we assume the availability of m verses of the Bible. We run IBM-21 on al"
P15-2044,A00-1031,0,0.37703,"than we assume, as well as a representative sample of unlabeled data. Such data is simply not available for many of the languages considered here. The weakly supervised system in Li et al. (2012) (L I) also relies on crowd-sourced type-level tag dictionaries, not available for most of the languages of concern to us. We present their reported results. Finally, we train the two base POS taggers (G AR and T N T) on the manually annotated data available for 17 of our languages, to be able to compare against state-of-the-art performance of supervised POS taggers. ModelsWe train T N T POS taggers (Brants, 2000) using only token-level projections. We also train semi-supervised POS taggers using the approach in Garrette and Baldridge (2013) (G AR), using both projections and dictionaries, as well as the unlabelled Bible translations.3 We use the English data as development data. We train T N T and G AR 4 3 github.com/dhgarrette/ low-resource-pos-tagging-2014/ 5 270 github.com/percyliang/brown-cluster code.google.com/p/wikily-supervised-pos-tagger/ Results Our results on the 25 test languages are consistently better than the unsupervised baselines, with the exceptions of Marathi and Persian, and by a v"
P15-2044,W06-1009,0,0.0336178,"2013) constrain Viterbi search via type-level tag dictionaries, pruning all tags not licensed by the dictionary. For the remaining tags, they use high-confidence word alignments to further prune the Viterbi search. We follow T¨ackstr¨om et al. (2013) in using our automatically created, not crowdsourced, tag dictionaries to prune tags during search, but we use word alignments to obtain token-level annotations that we use as annotated training data, similar to Fossum Related work The Bible has been used as a resource for machine translation and multi-lingual information retrieval before, e.g., (Chew et al., 2006). It has also been used in cross-lingual POS tagging (Yarowsky et al., 2001; Fossum and Abney, 2005), NP-chunking (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and cross-lingual dependency parsing (Sukhareva and Chiarcos, 2014) before. Yarowsky et al. (2001) and Fossum and Abney (2005) use word-aligned parallel translations of the Bible to project the predictions of POS taggers for several language pairs, including English, 271 and Abney (2005), Yarowsky et al. (2001), and Yarowsky and Ngai (2001). Duong et al. (2013) use word-alignment probabilities to select training data for their cross-"
P15-2044,W02-1001,0,0.0695296,"un IBM-21 on all n(n − 1) pairs of languages. Assume also manually POS-annotated training data 1 268 github.com/clab/fast_align Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 268–272, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics l1 EN … HR DE word type in the target language as a type-level tag dictionary. We combine the tag dictionary and the token-level projections to train discriminative, type-constrained POS taggers (Collins, 2002; T¨ackstr¨om et al., 2013). Below we refer to these POS taggers as using k sources (k-S RC). These n many POS taggers can now also be used to obtain predictions for all word tokens in our tensor object. This corresponds to doing the second loop over lines 8–17 in Algorithm 1. For each of our n languages, we thus complete the tensor by projecting tags into word tokens from the n − 1 remaining source languages. For the k supervised languages, we project the tags produced by the supervised POS taggers rather than the tags obtained by projection. We can then train our final POS taggers for all n"
P15-2044,P11-1061,0,0.356124,"the NLTK corpora, the HamleDT resources, and the Universal Dependencies project. We provide a complete overview of the resources at https://bitbucket.org/lowlands/ using k or n − 1 source languages, leading to four taggers in total. Baselines Our baselines are two standard unsupervised POS induction algorithms: Brown clustering using the implementation by Percy Liang4 and second-order unsupervised HMMs using logistic regression for emission probabilities (BergKirkpatrick et al., 2010; Li et al., 2012), with and without our Bible tag dictionaries.5 Upper bounds The weakly supervised system in Das and Petrov (2011) (DAS) relies on larger volumes of more representative and perfectly tokenized parallel data than we assume, as well as a representative sample of unlabeled data. Such data is simply not available for many of the languages considered here. The weakly supervised system in Li et al. (2012) (L I) also relies on crowd-sourced type-level tag dictionaries, not available for most of the languages of concern to us. We present their reported results. Finally, we train the two base POS taggers (G AR and T N T) on the manually annotated data available for 17 of our languages, to be able to compare agains"
P15-2044,P13-2112,0,0.144625,"Missing"
P15-2079,Y00-1004,0,0.804011,"length and compute the average score for each region under each tagger. We single out the two regions in England and Germany with the highest, respectively lowest, average log-likelihoods from both taggers. We do this to be able to control for dialectal variation. In each region, we randomly sample 200 reviews written by women under 35, 200 reviews written by men under 35, 200 reviews written by women over 45, and 200 reviews written by men over 45. This selection enables us to study and control for gender, region, and age. While sociolinguistics agrees on language change between age groups (Barke, 2000; Schler et al., 2006; Barbieri, 2008; Rickford and Price, 2013), it is not clear where to draw the line. The age groups selected here are thus solely based on the availability of even-sized groups that are separated by 10 years. Data Wall Street Journal and Frankfurter Rundschau The Wall Street Journal is a New York City-based newspaper, in print since 1889, with about two million readers. It employs 2,000 journalists in 85 news bureaus across 51 countries. Wall Street Journal is often considered business-friendly, but conservative. In 2007, Rupert Murdoch bought the newspaper. The English Pe"
P15-2079,P07-1033,0,0.0212688,"Missing"
P15-2079,N15-1135,1,0.916495,"st majority are news pieces.1 Frankfurter Rundschau is a German language newspaper based in Frankfurt am Main. Its first issue dates back to 1945, shortly after the end of the second world war. It has about 120,000 readers. It is often considered a left-wing liberal newspaper. According to a study conducted by the newspaper itself,2 its readers are found in “comfortable” higher jobs, well-educated, and on average in their mid-forties. While the paper is available internationally, most of its users come from the RhineMain region. 2.2 POS annotations The Trustpilot Corpus The Trustpilot Corpus (Hovy et al., 2015a) consists of user reviews scraped from the multilingual website trustpilot.com. The reviewer base has been shown to be representative of the populations in the countries for which large reviewer bases exist, at least wrt. age, gender, and geographical spread (Hovy et al., 2015a). The language is more informal than newswire, but less creative than social media posts. This is similar to the language in the reviews section of the English Web Treebank.3 For the experiments below, we annotated parts of the British and German sections 3 3.1 Experiments Training data and models As training data for"
P15-2079,P15-1073,1,0.52386,"e (Holmes, 2013; Nguyen et al., 2014). In this paper, we focus on the most widely used manually annotated resources for English and German, namely the English Penn Treebank and the TIGER Treebank for German. The English treebank consists of manually annotated Wall Street Journal articles from 1989. The TIGER Treebank consists of manually annotated Frankfurter Rundschau articles from the early 1990s. Both newspapers have regionally and demographically biased reader bases, e.g., with more old than young readers. We discuss the biases in §2. In the light of recent research (Volkova et al., 2013; Hovy, 2015; Jørgensen et al., 2015), we explore the hypothesis that these biases transfer to NLP tools induced from these resources. As a result, these models perform better on texts written by certain people, namely those whose language is closer to the training data. Language dynamics being what they are, we expect English and German POS taggers to perform better on texts written by older people. To evaluate this hypothesis, we collected English and German user reviews from a user review site used by representative samples of the English and German populations. We annotated reviews written by users wh"
P15-2079,K15-1011,1,0.155606,"DV–VERB, while the TIGER treebank contains more NOUN–DET, NOUN–ADP, and NOUN–NOUN. Again, the younger group is more dissimilar to the CoNLL data, but less so than for English, with CONJ–PRON, NOUN–VERB, VERB– VERB, and PRON–DET, while the older group shows more ADV–ADJ, ADP–NOUN, NOUN–ADV, and ADJ–NOUN. In all of these cases, vocabulary does not factor into the differences, since we are at the POS level. The results indicate that there exist fundamental grammatical differences between the age groups, which go well beyond mere lexical differences. These findings are in line with the results in Johannsen et al. (2015), who showed that entire (delexicalized) dependency structures correlate with age and gender, often across several languages. 4.1 Tagging Error Analysis Analyzing the tagging errors of our model can give us an insight into the constructions that differ most between groups. In German, most of the errors in the younger group occur with adverbs, determiners, and verbs. Adverbs are often confused with adjectives, because adverbs and adjectives are used as modifiers in similar ways. The taggers also frequently confused adverbs with nouns, especially sentenceinitially, presumably largely because the"
P15-2079,W15-4302,1,0.242909,"013; Nguyen et al., 2014). In this paper, we focus on the most widely used manually annotated resources for English and German, namely the English Penn Treebank and the TIGER Treebank for German. The English treebank consists of manually annotated Wall Street Journal articles from 1989. The TIGER Treebank consists of manually annotated Frankfurter Rundschau articles from the early 1990s. Both newspapers have regionally and demographically biased reader bases, e.g., with more old than young readers. We discuss the biases in §2. In the light of recent research (Volkova et al., 2013; Hovy, 2015; Jørgensen et al., 2015), we explore the hypothesis that these biases transfer to NLP tools induced from these resources. As a result, these models perform better on texts written by certain people, namely those whose language is closer to the training data. Language dynamics being what they are, we expect English and German POS taggers to perform better on texts written by older people. To evaluate this hypothesis, we collected English and German user reviews from a user review site used by representative samples of the English and German populations. We annotated reviews written by users whose age, gender, and loca"
P15-2079,N13-1039,0,0.0276639,"Missing"
P15-2079,D13-1187,0,0.148722,"vers of language change (Holmes, 2013; Nguyen et al., 2014). In this paper, we focus on the most widely used manually annotated resources for English and German, namely the English Penn Treebank and the TIGER Treebank for German. The English treebank consists of manually annotated Wall Street Journal articles from 1989. The TIGER Treebank consists of manually annotated Frankfurter Rundschau articles from the early 1990s. Both newspapers have regionally and demographically biased reader bases, e.g., with more old than young readers. We discuss the biases in §2. In the light of recent research (Volkova et al., 2013; Hovy, 2015; Jørgensen et al., 2015), we explore the hypothesis that these biases transfer to NLP tools induced from these resources. As a result, these models perform better on texts written by certain people, namely those whose language is closer to the training data. Language dynamics being what they are, we expect English and German POS taggers to perform better on texts written by older people. To evaluate this hypothesis, we collected English and German user reviews from a user review site used by representative samples of the English and German populations. We annotated reviews written"
P15-2079,D07-1112,0,\N,Missing
P15-2138,P11-1049,0,0.00539576,"mmarization with syntactic and semantic concepts. Specifically, we replace bigram concepts with new ones based on syntactic dependencies, semantic frames, as well as named entities. We show that using such concepts can lead to significant improvements in text summarization performance outside of the newswire domain. We evaluate coverage maximization incorporating syntactic and semantic concepts across three different domains: newswire, legal judgments, and Wikipedia articles. Introduction State-of-the-art approaches to extractive summarization are based on the notion of coverage maximization (Berg-Kirkpatrick et al., 2011). The assumption is that a good summary is a selection of sentences from the document that contains as many of the important concepts as possible. The importance of concepts is implemented by assigning weights wi to each concept i with binary variable ci , yielding the following coverage maximization objective, subject to the appropriate constraints: N X wi ci (1) 2 Concept coverage maximization for extractive summarization In extractive summarization, the unsupervised version of the task is sometimes set up as that of finding a subset of sentences in a document, within some relatively small b"
P15-2138,W12-2601,0,0.00465366,"er concepts, we do not use any threshold. (1−α) 9 wi bigrami +α i 3.3 N2 X wj new conceptj j Results We evaluate output summaries using ROUGE-1, ROUGE-2, and ROUGE-SU4 (Lin, 2004), with no stemming and retaining all stopwords. These measures have been shown to correlate best with human judgments in general, but among the automatic measures, ROUGE-1 and ROUGE-2 also correlate best with the Pyramid (Nenkova and Passonneau, 2004; Nenkova et al., 2007) and Responsiveness manual metrics (Louis and Nenkova, 2009). Moreover, ROUGE-1 has been shown to best reflect human-automatic summary comparisons (Owczarzak et al., 2012). For single concept systems, the results are shown in Table 1, and concept combination system results are given in Table 2. We first note that our runs of the current distribution of icsisumm yield significantly worse ROUGE-2 results than reported in (Gillick and Favre, 2009) (see Table 1, BIGRAMS): 0.081 compared to 0.110 respectively. 3. First concept weighting: in multi-document summarization, there is the possibility for repeated sentences. Concepts from firstencountered sentences may be weighted higher: these concept counts from firstencountered sentences are doubled for ‘B’ documents an"
P15-2138,W00-1438,0,0.0870805,"Missing"
P15-2138,W04-1017,0,0.0229141,"113) 0.114 (0.105-0.123) 0.116 (0.107-0.125) 0.118 (0.109-0.128) 0.102 (0.094-0.112) 0.152 (0.134-0.163) 0.178 (0.169-0.186) 0.179 (0.169-0.188) 0.18 (0.17-0.189) 0.172 (0.164-0.182) Table 1: Single concept results on ECHR, TAC 08, and WIKIPEDIA. Multidocument measure first proposed by Goldstein et al. (2000) for evaluating the importance of sentences in query-based extractive summarization, yielding improvements for their Japanese newswire dataset. Related work 5 Most researchers have used bigrams as concepts in coverage maximization-based approaches to unsupervised extractive summarization. Filatova and Hatzivassiloglou (2004), however,use relations between named entities as concepts in extractive summarization. They use slightly different extraction algorithms, but their work is similar in spirit to ours. Nishikawa et al. (2010), also, use opinions – tuples of targets, aspects, and polarity – as concepts in opinion summarization. In early work on summarization, Silber and McCoy (2000) used WordNet synsets as concepts. Kitajima and Kobayashi (2011) replace words by syntactic dependencies in the Maximal Marginal Relevance Conclusions This paper challenges the assumption that bigrams make better concepts for unsuperv"
P15-2138,D12-1022,0,0.00484275,"r sentence selection. (Gillick and Favre, 2009) Coverage maximization with bigram concepts is a state-of-the-art approach to unsupervised extractive summarization. It has been argued that such concepts are adequate and, in contrast to more linguistic concepts such as named entities or syntactic dependencies, more robust, since they do not rely on automatic processing. In this paper, we show that while this seems to be the case for a commonly used newswire dataset, use of syntactic and semantic concepts leads to significant improvements in performance in other domains. 1 Several authors, e.g., Woodsend and Lapata (2012), and Li et al. (2013), have followed Gillick and Favre (2009) in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these. In this paper, we revisit this assumption and evaluate the maximum coverage objective for extractive text summarization with syntactic and semantic concepts. Specifically, we replace bigram concepts with new ones based on syntactic dependencies, semantic frames, as well as named entities. We show that using such concepts can lead to significant improvements in tex"
P15-2138,W09-1802,0,0.206464,"ation with bigram concepts is a state-of-the-art approach to unsupervised extractive summarization. It has been argued that such concepts are adequate and, in contrast to more linguistic concepts such as named entities or syntactic dependencies, more robust, since they do not rely on automatic processing. In this paper, we show that while this seems to be the case for a commonly used newswire dataset, use of syntactic and semantic concepts leads to significant improvements in performance in other domains. 1 Several authors, e.g., Woodsend and Lapata (2012), and Li et al. (2013), have followed Gillick and Favre (2009) in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these. In this paper, we revisit this assumption and evaluate the maximum coverage objective for extractive text summarization with syntactic and semantic concepts. Specifically, we replace bigram concepts with new ones based on syntactic dependencies, semantic frames, as well as named entities. We show that using such concepts can lead to significant improvements in text summarization performance outside of the newswire domain. We"
P15-2138,W00-0405,0,0.0228395,"Missing"
P15-2138,P11-3006,0,0.0500714,"Missing"
P15-2138,P13-1099,0,0.00616413,"nd Favre, 2009) Coverage maximization with bigram concepts is a state-of-the-art approach to unsupervised extractive summarization. It has been argued that such concepts are adequate and, in contrast to more linguistic concepts such as named entities or syntactic dependencies, more robust, since they do not rely on automatic processing. In this paper, we show that while this seems to be the case for a commonly used newswire dataset, use of syntactic and semantic concepts leads to significant improvements in performance in other domains. 1 Several authors, e.g., Woodsend and Lapata (2012), and Li et al. (2013), have followed Gillick and Favre (2009) in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these. In this paper, we revisit this assumption and evaluate the maximum coverage objective for extractive text summarization with syntactic and semantic concepts. Specifically, we replace bigram concepts with new ones based on syntactic dependencies, semantic frames, as well as named entities. We show that using such concepts can lead to significant improvements in text summarization perfor"
P15-2138,C10-2105,0,\N,Missing
P15-2138,N04-1019,0,\N,Missing
P15-2138,D09-1032,0,\N,Missing
P15-2138,W04-1013,0,\N,Missing
P16-1071,D14-1162,0,0.0828385,"ated by both annotators, who reached an inter-annotator agreement of 0.926 in accuracy and 0.928 in weighted F1 . For the final development and test data, disagreements were resolved by the annotators. 79 78 Dev set accuracy 77 76 75 74 73 72 5.2 71 70 1 2 3 4 5 Subject ID 6 7 Our first baseline is a second-order HMM with type constraints from Wiktionary; this in all respects the model proposed by Liu et al. (2012), except trained on our small Harry Potter corpus. In a second baseline model, we also incorporate 300dimensional GloVe word embeddings trained on Wikipedia and the Gigaword corpus (Pennington et al., 2014). We also test a version of the baseline without the basic features to get an estimate of the contribution of this aspect of the setup. 8 Figure 4: Accuracy on the development set for the different subjects when trained and tested on fMRI data from only this one subject. Dashed line is the development set baseline. Only in one out of eight cases does adding fMRI features lead to worse performance. EM-HMM Parameters We use the same setting as Li et al. (2012) for the number of EM iterations, fixing this parameter to 30 for all experiments. 5 5.3 Token-level fMRI We run a series of experiments w"
P16-1071,P99-1023,0,0.0079628,"sian window of |V |points, with a standard deviation of 1. In factoring the Gaussian weight vector into the equation, we lend less weight to the fMRI recordings at the outset and at the end of the time window specified through s (start) and e (end). 4 (3) where w is a weight vector and f (xi , zi ) is a feature function that will, in our case, consider the fMRI vectors vt that we computed in section 3.2.1 and a number of basic features that we adopt from the original model (Li et al., 2012). See Section 5 for details. In addition, we use a second-order HMM, first introduced for PoS tagging in Thede and Harper (1999), in which transitional probabilities are also considered for second-degree subsequent states (cf. figure 3). Here, the joint probability becomes |V | vt = i=2 (2) Following Berg-Kirkpatrick et al. (2010), the model calculates the probability distribution θ that parameterizes the emission probabilities as the output of a maximum entropy model, which enables unsupervised learning with a rich set of features. We thus let Figure 3: Second-order HMM incorporating transitional probabilities from first and second-degree preceding states. (4) i=3 In order to optimize the HMM (including the weight vec"
P16-1071,N16-1179,1,0.874477,"Missing"
P16-1071,D12-1127,0,0.104377,"Missing"
P16-1071,P12-1055,0,0.0228311,"original implementation by Li et al. (2012). The model is 750 annotated for universal part-of-speech tags (Petrov et al., 2011) by two linguistically trained annotators. The development set was annotated by both annotators, who reached an inter-annotator agreement of 0.926 in accuracy and 0.928 in weighted F1 . For the final development and test data, disagreements were resolved by the annotators. 79 78 Dev set accuracy 77 76 75 74 73 72 5.2 71 70 1 2 3 4 5 Subject ID 6 7 Our first baseline is a second-order HMM with type constraints from Wiktionary; this in all respects the model proposed by Liu et al. (2012), except trained on our small Harry Potter corpus. In a second baseline model, we also incorporate 300dimensional GloVe word embeddings trained on Wikipedia and the Gigaword corpus (Pennington et al., 2014). We also test a version of the baseline without the basic features to get an estimate of the contribution of this aspect of the setup. 8 Figure 4: Accuracy on the development set for the different subjects when trained and tested on fMRI data from only this one subject. Dashed line is the development set baseline. Only in one out of eight cases does adding fMRI features lead to worse perfor"
P16-1071,N10-1083,0,\N,Missing
P16-1071,petrov-etal-2012-universal,0,\N,Missing
P16-2038,P08-1076,0,0.0231001,"Missing"
P16-2038,P15-2041,0,0.0102758,"Missing"
P16-2038,P82-1020,0,0.7894,"Missing"
P16-2038,D14-1080,0,0.16738,"gging, coupled with the additional task of POS-tagging. We show that it is consistently better to have POS supervision at the innermost rather than the outermost layer. We argue that this is because “lowlevel” tasks are better kept at the lower layers, enabling the higher-level tasks to make use of the shared representation of the lower-level tasks. Finally, we also show how this architecture can be used for domain adaptation. 1 Introduction We experiment with a multi-task learning (MTL) architecture based on deep bi-directional recurrent neural networks (bi-RNNs) (Schuster and Paliwal, 1997; Irsoy and Cardie, 2014). MTL can be seen as a way of regularizing model induction by sharing representations (hidden layers) with other inductions (Caruana, 1993). We use deep bi-RNNs with task supervision from multiple tasks, sharing one or more bi-RNNs layers among the tasks. Our main contribution is the novel insight that (what has historically been thought of as) low-level tasks are better modeled in the low layers of such an architecture. This is in contrast to previous work on deep MTL (Collobert et al., 2011; Luong et al., 2015) , in which supervision for all tasks happen at the same (outermost) layer. Multip"
P16-2038,W00-0726,0,\N,Missing
P16-2038,D07-1096,0,\N,Missing
P16-2055,N15-1156,0,0.0400424,"Missing"
P16-2055,E99-1042,0,0.122912,"graphs that jointly predicts possible compressions and paraphrases. Our model reaches readability scores comparable to word-based compression approaches across a range of metrics and human judgements while maintaining more of the important information. 1 Introduction Sentence-level text simplification is the problem of automatically modifying sentences so that they become easier to read, while maintaining most of the relevant information in them. This can benefit applications as pre-processing for machine translation (Bernth, 1998) and assisting technologies for readers with reduced literacy (Carroll et al., 1999; Watanabe et al., 2009; Rello et al., 2013). Sentence-level text simplification ignores sentence splitting and reordering, and typically focuses on compression (deletion of words) and paraphrasing or lexical substitution (Cohn and Lapata, 2008). We include paraphrasing and lexical substitution here, while previous work in sentence simplification has often focused exclusively on deletion. Approaches that address compression and paraphrasing (or more tasks) integrally include (Zhu et al., 2010; Narayan and Gardent, 2014; Mandya et al., 2014). Simplification beyond deletion is motivated by Pitle"
P16-2055,C14-1188,0,0.396484,"sting technologies for readers with reduced literacy (Carroll et al., 1999; Watanabe et al., 2009; Rello et al., 2013). Sentence-level text simplification ignores sentence splitting and reordering, and typically focuses on compression (deletion of words) and paraphrasing or lexical substitution (Cohn and Lapata, 2008). We include paraphrasing and lexical substitution here, while previous work in sentence simplification has often focused exclusively on deletion. Approaches that address compression and paraphrasing (or more tasks) integrally include (Zhu et al., 2010; Narayan and Gardent, 2014; Mandya et al., 2014). Simplification beyond deletion is motivated by Pitler’s (2010) observation that abstractive sentence summaries written by humans often “include paraphrases or synonyms (‘said’ versus ‘stated’) and use alternative syntactic constructions (‘gave John the book’ versus ‘gave the book to John’).” Such lexical or syntactic alternations may conContributions We present a sentence simplification model which is, to the best of our knowledge, the first model that uses structured prediction over dependency trees and models compression and paraphrasing jointly. Our model uses Viterbi decoding rather than"
P16-2055,P14-5010,0,0.00424041,"te that elements in subtrees dominated by a deleted node are automatically deleted (analogously for paraphrases). For each dependency tree G = (V, A) in a training set of T sentences, we derive an input sequence of K-dimensional feature vectors x = x1 , . . . , xn and an output sequence of y = y1 , . . . , yn . Our tree-to-string simplification model is a second-order linear-chain conditional random field (CRF) (2) Queen elizabeth ii used wrong name for Republic (3) The Queen of England used the wrong name for the Republic of Ireland. The data is pre-processed with the Stanford CoreNLP tools (Manning et al., 2014), retrieving lemmas, parts-of-speech, named entities and dependency trees. We reserve the first 200 sentences from the data set for evaluation, the next 200 for tuning parameters (including the used PPDB versions, see next paragraph), and use the remaining 9,600 sentences for training our model. Deletion and paraphrase targets As our approach operates on dependency trees, aiming to prune or paraphrase subtrees from the dependency tree of a sentence, we identify deleted or paraphrased subtrees, marking their heads with a corresponding label. A subtree receives a Delete label if none of the word"
P16-2055,C08-1018,0,0.406234,"ormation. 1 Introduction Sentence-level text simplification is the problem of automatically modifying sentences so that they become easier to read, while maintaining most of the relevant information in them. This can benefit applications as pre-processing for machine translation (Bernth, 1998) and assisting technologies for readers with reduced literacy (Carroll et al., 1999; Watanabe et al., 2009; Rello et al., 2013). Sentence-level text simplification ignores sentence splitting and reordering, and typically focuses on compression (deletion of words) and paraphrasing or lexical substitution (Cohn and Lapata, 2008). We include paraphrasing and lexical substitution here, while previous work in sentence simplification has often focused exclusively on deletion. Approaches that address compression and paraphrasing (or more tasks) integrally include (Zhu et al., 2010; Narayan and Gardent, 2014; Mandya et al., 2014). Simplification beyond deletion is motivated by Pitler’s (2010) observation that abstractive sentence summaries written by humans often “include paraphrases or synonyms (‘said’ versus ‘stated’) and use alternative syntactic constructions (‘gave John the book’ versus ‘gave the book to John’).” Such"
P16-2055,E06-1038,0,0.456014,"aluation setup, as well as word-level performance, are listed in Table 1 and compared to RT. Note that for deletion and paraphrasing, our model consistently has higher precision than recall, thus generating more confident simplifications and less ungrammatical output. Evaluation Baselines In the following experiments, we compare our approach to state-of-the-art approaches to sentence compression and joint compression/paraphrasing. For the first of these two categories, we consider the LSTM system described in Filippova et al. (2015) as well as the results reported therein for the MIRA system (McDonald, 2006). As a joint approach, we consider Reluctant Trimmer (RT), a simplification system that employs synchronous dependency grammars (Mandya et al., 2014). Since the LSTM system requires great amounts of training data, which were not available to us, we cannot reproduce its outAutomated Readability Scores Table 2 reports the compression ratio (CR, percentage of retained words) as well as automated readability scores that our model achieves on the test set and compares it to the output of the RT baseline. Our system manages to compress the original texts by more than one third, but the gold simplifi"
P16-2055,D13-1032,0,0.0482463,"Missing"
P16-2055,N13-1070,1,0.850641,"(lower). 5 Related Work Several approaches to sentence compression have been presented in the last decade. Knight and Marcu (2002) and Turner and Charniak (2005) apply noisy channel models, using language models to control for grammaticality. McDonald (2006) introduces a different approach, discriminatively training a scoring function, informed by syntactic features, to score all possible subtrees of a sentence. His work was inspired by Riezler et al. (2003) scoring substrings generated from LFG parses. A third approach to sentence compression is sequence labeling, which has been explored by Elming et al. (2013) using linear-chain CRFs with syntactic features, and more recently by Filippova et al. (2015) and Klerke et al. (2016) using recurrent neural networks with LSTM cells. Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in their model (McDonald, 2006; Clarke and Lapata, 2008). Recently, however, Filippova et al. (2015) presented an approach to sentence compression using 1948) an"
P16-2055,W11-1610,0,0.0464204,"Missing"
P16-2055,W08-1105,0,0.849574,"y readers (Rello et al., 2013). Our joint approach to deletion and paraphrasing works against the limitation that abstractive simplifications “are not capable of being generated by [...] most sentence compression algorithms” (Pitler, 2010). Furthermore, a central concern in text simplification is to ensure the grammaticality of the output, especially with low-proficiency readers as the target audience. Our approach to this problem is to remove or paraphrase entire syntactic units in the original sentence, thus avoiding to remove phrase heads without removing their arguments or modifiers. Like Filippova and Strube (2008), we rely on dependency structures rather than constituent structures, which promises more robust syntactic analysis and allows us to operate on discontinuous syntactic units. We present a new, structured approach to text simplification using conditional random fields over top-down traversals of dependency graphs that jointly predicts possible compressions and paraphrases. Our model reaches readability scores comparable to word-based compression approaches across a range of metrics and human judgements while maintaining more of the important information. 1 Introduction Sentence-level text simp"
P16-2055,P14-1041,0,0.0862688,"ion (Bernth, 1998) and assisting technologies for readers with reduced literacy (Carroll et al., 1999; Watanabe et al., 2009; Rello et al., 2013). Sentence-level text simplification ignores sentence splitting and reordering, and typically focuses on compression (deletion of words) and paraphrasing or lexical substitution (Cohn and Lapata, 2008). We include paraphrasing and lexical substitution here, while previous work in sentence simplification has often focused exclusively on deletion. Approaches that address compression and paraphrasing (or more tasks) integrally include (Zhu et al., 2010; Narayan and Gardent, 2014; Mandya et al., 2014). Simplification beyond deletion is motivated by Pitler’s (2010) observation that abstractive sentence summaries written by humans often “include paraphrases or synonyms (‘said’ versus ‘stated’) and use alternative syntactic constructions (‘gave John the book’ versus ‘gave the book to John’).” Such lexical or syntactic alternations may conContributions We present a sentence simplification model which is, to the best of our knowledge, the first model that uses structured prediction over dependency trees and models compression and paraphrasing jointly. Our model uses Viterb"
P16-2055,D15-1042,0,0.11608,"Missing"
P16-2055,N13-1092,0,0.0469816,"Missing"
P16-2055,W03-1602,0,0.172892,"Missing"
P16-2055,N03-1026,0,0.0607997,"adability and informativeness ratings for the first 200 sentences in the Google data (upper) and for the 11 sample sentences listed in Filippova et al. (2015) (lower). 5 Related Work Several approaches to sentence compression have been presented in the last decade. Knight and Marcu (2002) and Turner and Charniak (2005) apply noisy channel models, using language models to control for grammaticality. McDonald (2006) introduces a different approach, discriminatively training a scoring function, informed by syntactic features, to score all possible subtrees of a sentence. His work was inspired by Riezler et al. (2003) scoring substrings generated from LFG parses. A third approach to sentence compression is sequence labeling, which has been explored by Elming et al. (2013) using linear-chain CRFs with syntactic features, and more recently by Filippova et al. (2015) and Klerke et al. (2016) using recurrent neural networks with LSTM cells. Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in t"
P16-2055,P05-1036,0,0.0203246,"y a high Flesh Reading Ease score and a low Dale-Chall score. * indicates differences compared to the original sentences that are significant at p &lt; 10−3 . System MIRA LSTM TL RT (11) LSTM (11) TL (11) Readability 4.31 4.51 4.14 Informativeness 3.55 3.78 4.01 3.09 4.23 4.21 4.12 3.42 4.15 Table 3: Mean readability and informativeness ratings for the first 200 sentences in the Google data (upper) and for the 11 sample sentences listed in Filippova et al. (2015) (lower). 5 Related Work Several approaches to sentence compression have been presented in the last decade. Knight and Marcu (2002) and Turner and Charniak (2005) apply noisy channel models, using language models to control for grammaticality. McDonald (2006) introduces a different approach, discriminatively training a scoring function, informed by syntactic features, to score all possible subtrees of a sentence. His work was inspired by Riezler et al. (2003) scoring substrings generated from LFG parses. A third approach to sentence compression is sequence labeling, which has been explored by Elming et al. (2013) using linear-chain CRFs with syntactic features, and more recently by Filippova et al. (2015) and Klerke et al. (2016) using recurrent neural"
P16-2055,D11-1038,0,0.305749,"Missing"
P16-2055,C10-1152,0,0.149166,"r machine translation (Bernth, 1998) and assisting technologies for readers with reduced literacy (Carroll et al., 1999; Watanabe et al., 2009; Rello et al., 2013). Sentence-level text simplification ignores sentence splitting and reordering, and typically focuses on compression (deletion of words) and paraphrasing or lexical substitution (Cohn and Lapata, 2008). We include paraphrasing and lexical substitution here, while previous work in sentence simplification has often focused exclusively on deletion. Approaches that address compression and paraphrasing (or more tasks) integrally include (Zhu et al., 2010; Narayan and Gardent, 2014; Mandya et al., 2014). Simplification beyond deletion is motivated by Pitler’s (2010) observation that abstractive sentence summaries written by humans often “include paraphrases or synonyms (‘said’ versus ‘stated’) and use alternative syntactic constructions (‘gave John the book’ versus ‘gave the book to John’).” Such lexical or syntactic alternations may conContributions We present a sentence simplification model which is, to the best of our knowledge, the first model that uses structured prediction over dependency trees and models compression and paraphrasing joi"
P16-2067,Q16-1023,1,0.732171,"ry loss, being predictive of word frequency, helps to differentiate the representations of rare and common words. We indeed observe performance gains on rare and out-of-vocabulary words. These performance gains transfer into general improvements for morphologically rich languages. Introduction Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel biLSTM model with auxiliary loss (L OGFREQ)"
P16-2067,D15-1041,0,0.302413,"model is that the auxiliary loss, being predictive of word frequency, helps to differentiate the representations of rare and common words. We indeed observe performance gains on rare and out-of-vocabulary words. These performance gains transfer into general improvements for morphologically rich languages. Introduction Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel biLSTM mod"
P16-2067,D15-1025,0,0.0859971,"Missing"
P16-2067,A00-1031,0,0.175325,"ank that has the canonical language name (e.g., Finnish instead of Finnish-FTB). We consider all languages that have at least 60k tokens and are distributed with word forms, resulting in 22 languages. We also report accuracies on WSJ (45 POS) using the standard splits (Collins, 2002; Manning, 2011). The overview of languages is provided in Table 1. 3.2 FINE Semitic Slavic Slavic Germanic Germanic Germanic Romance Table 1: Grouping of languages. Taggers We want to compare POS taggers under varying conditions. We hence use three different types of taggers: our implementation of a bi-LSTM; T NT (Brants, 2000)—a second order HMM with suffix trie handling for OOVs. We use T NT as it was among the best performing taggers evaluated in Horsmann et al. (2015).3 We complement the NN-based and HMM-based tagger with a CRF tagger, using a freely available implementation (Plank et al., 2014) based on crfsuite. 3.1 COARSE non-IE Indoeuropean Indoeuropean Indoeuropean Indoeuropean Indoeuropean Indoeuropean Language isolate Indoeuropean non-IE Indoeuropean Results Our results are given in Table 2. First of all, notice that T N T performs remarkably well across the 22 languages, closely followed by CRF. The biLS"
P16-2067,D15-1085,0,0.0532663,"close to taggers using carefully designed feature templates. Ling et al. (2015) extend this line and compare a novel bi-LSTM model, learning word representations through character embeddings. They evaluate their model on a language modeling and POS tagging setup, and show that bi-LSTMs outperform the CNN approach of Santos and Zadrozny (2014). Similarly, Labeau et al. (2015) evaluate character embeddings for German. Bi-LSTMs for POS tagging are also reported in Wang et al. (2015), however, they only explore word embeddings, orthographic information and evaluate on WSJ only. A related study is Cheng et al. (2015) who propose a multi-task RNN for named entity recognition by jointly predicting the next token and current token’s name label. Our model is simpler, it uses a very coarse set of labels rather then integrating an entire language modeling task which is computationally more expensive. An interesting recent study is Gillick et al. (2016), they build a single byte-to-span model for multiple languages based on a sequence-to-sequence RNN (Sutskever et al., 2014) achieving impressive results. We would like to extend this work in their direction. Figure 3: Amount of training data (number of sentences)"
P16-2067,D15-1176,0,0.324201,"s the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that biLSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed. 1 We consider using bi-LSTMs for POS tagging. Previous work on using deep learning-based methods for POS tagging has focused either on a single language (Collobert et al., 2011; Wang et al., 2015) or a small set of languages (Ling et al., 2015; Santos and Zadrozny, 2014). Instead we evaluate our models across 22 languages. In addition, we compare performance with representations at different levels of granularity (words, characters, and bytes). These levels of representation were previously introduced in different efforts (Chrupała, 2013; Zhang et al., 2015; Ling et al., 2015; Santos and Zadrozny, 2014; Gillick et al., 2016; Kim et al., 2015), but a comparative evaluation was missing. Moreover, deep networks are often said to require large volumes of training data. We investigate to what extent bi-LSTMs are more sensitive to the am"
P16-2067,D15-1168,0,0.0430138,"e the representations of rare and common words. We indeed observe performance gains on rare and out-of-vocabulary words. These performance gains transfer into general improvements for morphologically rich languages. Introduction Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel biLSTM model with auxiliary loss (L OGFREQ). 412 Proceedings of the 54th Annual Meeting of the"
P16-2067,C14-1168,1,0.903295,"ollins, 2002; Manning, 2011). The overview of languages is provided in Table 1. 3.2 FINE Semitic Slavic Slavic Germanic Germanic Germanic Romance Table 1: Grouping of languages. Taggers We want to compare POS taggers under varying conditions. We hence use three different types of taggers: our implementation of a bi-LSTM; T NT (Brants, 2000)—a second order HMM with suffix trie handling for OOVs. We use T NT as it was among the best performing taggers evaluated in Horsmann et al. (2015).3 We complement the NN-based and HMM-based tagger with a CRF tagger, using a freely available implementation (Plank et al., 2014) based on crfsuite. 3.1 COARSE non-IE Indoeuropean Indoeuropean Indoeuropean Indoeuropean Indoeuropean Indoeuropean Language isolate Indoeuropean non-IE Indoeuropean Results Our results are given in Table 2. First of all, notice that T N T performs remarkably well across the 22 languages, closely followed by CRF. The biLSTM tagger (w) ~ without lower-level bi-LSTM for subtokens falls short, outperforms the traditional taggers only on 3 languages. The bi-LSTM Rare words In order to evaluate the effect of modeling sub-token information, we examine accuracy rates at different frequency rates. Fig"
P16-2067,P15-1109,0,0.0284485,"are and out-of-vocabulary words. These performance gains transfer into general improvements for morphologically rich languages. Introduction Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel biLSTM model with auxiliary loss (L OGFREQ). 412 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 412–418, c Berlin, Germany, August 7-12,"
P16-2067,W13-3520,0,\N,Missing
P16-2067,N16-1155,0,\N,Missing
P16-2091,Q16-1022,1,0.867526,"Missing"
P16-2091,A00-1031,0,0.265162,"st alignments across typologically dis¨ tant language pairs (Ostling, 2015). We modify Parallel texts We exploit two sources of parallel text: the Edinburgh Multilingual Bible corpus (EBC) (Christodouloupoulos and Steedman, 2014), and our own collection of online texts published by the Wathctower Society (WTC).4 While 4 ∈ {0, 1} Flow φi,k,j,l Data sources 3 ∈ {0, 1} 5 https://github.com/bplank/ multilingualtokenizer http://hdl.handle.net/11234/1-1548 https://www.jw.org/ 563 the aligner to output alignment probabilities. All the source-side texts are POS-tagged and dependency parsed using TnT (Brants, 2000) and TurboParser (Martins et al., 2013). We use our own fork of the arc-factored TurboParser to output the edge weight matrices.6 4 Approach Experiments 4.1 ILP 51.62 (18) 53.58 (20) DCA 48.39 (8) 48.40 (0) D ELEX 42.44 (1) 47.35 (3) Gold POS EBC WTC 65.43 (25) 66.51 (23) 59.94 (2) 55.73 (0) 64.13 (–) 66.68 (–) Table 1: Macro-averaged UAS scores summarizing our evaluation. EBC: Edinburgh Bible corpus, WTC: Watchtower corpus. Numbers of languages with top performance per system are reported in brackets. All parsers use their respective EBC or WTC taggers.8 Setup In our experiments, as in the pr"
P16-2091,C14-1175,0,0.438713,"aggers, yielding the top score for 16/23 languages from the overlap. Notably, on several non-IndoEuropean languages, we observe significant improvements. For example, on Indonesian, DCA improves over D ELEX by 12 points UAS, while ILP adds 6 more points on top. We observe a similar pattern for Arabic and Estonian. We note that D ELEX tops ILP and DCA on only 1 EBC and 3 WTC languages, and by a narrow margin. ILP The ILP-based joint projection algorithm we presented in Section 2. DCA Our implementation of the de facto standard annotation projection algorithm of Hwa et al. (2005), as refined by Tiedemann (2014). In contrast to our ILP approach, it uses heuristics to ensure dependency tree constraints on a sourcetarget sentence pair basis. We gather all the pairwise projections into a target sentence graph and then perform maximum spanning tree decoding following Sagae and Lavie (2006). D ELEX The multi-source direct delexicalized transfer baseline of McDonald et al. (2011). Each source is represented by an approximately equal number of sentences. 4.2 Predicted POS EBC WTC Analysis A projected parse is allowed to be a composite of edges from many source languages. To find out to what degree this actu"
P16-2091,mayer-cysouw-2014-creating,0,0.0280542,"gual taggers and parsers, we use the Universal Dependencies (UD) version 1.2 treebanks with the corresponding test sets.3 Preprocessing We use simple sentence splitting and tokenization models to segment the parallel corpora.5 To sentence- and word-align the individual language pairs, we use a Gibbs samplingbased IBM1 alignment model called efmaral ¨ (Ostling, 2015). IBM1 has been shown to lead to more robust alignments across typologically dis¨ tant language pairs (Ostling, 2015). We modify Parallel texts We exploit two sources of parallel text: the Edinburgh Multilingual Bible corpus (EBC) (Christodouloupoulos and Steedman, 2014), and our own collection of online texts published by the Wathctower Society (WTC).4 While 4 ∈ {0, 1} Flow φi,k,j,l Data sources 3 ∈ {0, 1} 5 https://github.com/bplank/ multilingualtokenizer http://hdl.handle.net/11234/1-1548 https://www.jw.org/ 563 the aligner to output alignment probabilities. All the source-side texts are POS-tagged and dependency parsed using TnT (Brants, 2000) and TurboParser (Martins et al., 2013). We use our own fork of the arc-factored TurboParser to output the edge weight matrices.6 4 Approach Experiments 4.1 ILP 51.62 (18) 53.58 (20) DCA 48.39 (8) 48.40 (0) D ELEX 42"
P16-2091,H01-1035,0,0.863149,"n Integer Linear Programming (ILP) algorithm that simultaneously projects annotation for multiple tasks from multiple source languages, relying on parallel corpora available for hundreds of languages. When training POS taggers and dependency parsers on jointly projected POS tags and syntactic dependencies using our algorithm, we obtain better performance than a standard approach on 20/23 languages using one parallel corpus; and 18/27 languages using another. 1 Introduction Cross-language annotation projection for unsupervised POS tagging and syntactic parsing was introduced fifteen years ago (Yarowsky et al., 2001; Hwa et al., 2005), and the best unsupervised dependency parsers today rely on annotation projection (Rasooli and Collins, 2015). Despite the maturity of the field, there is an inherent language bias in previous work on crosslanguage annotation projection. Cross-language annotation projection experiments require training data in m source languages, a parallel corpus of translations from the m source languages into the target language of interest, as well as evaluation data for the target language.1 Since the canonical resource for parallel text is the Europarl Corpus (Koehn, 2005), which cove"
P16-2091,P11-1061,0,0.129341,"ss-lingual taggers and parsers for 18/27 and 20/23 languages, depending on the parallel corpora used. We made no unrealistic assumptions as to the availability of parallel texts and preprocessing tools for the target languages. Our code and data is freely available.9 Related work In recent years, we note an increased interest for work in cross-lingual processing, and particularly in POS tagging and dependency parsing of lowresource languages. Yarowsky et al. (2001) proposed the idea of inducing NLP tools via parallel corpora. Their contribution started a line of work in annotation projection. Das and Petrov (2011) used graph-based label propagation to yield competitive POS taggers, while Hwa et al. (2005) introduced the projection of dependency trees. Tiedemann (2014) further improved this approach to single-source projection in the context of synthesizing dependency treebanks (Tiedemann and Agi´c, 2016). The current state of the art in cross-lingual dependency parsing also involves exploiting large parallel corpora (Ma and Xia, 2014; Rasooli and Acknowledgements This research is partially funded by the ERC Starting Grant LOWLANDS (#313695). 9 https://bitbucket.org/lowlands/ release 565 References Kenj"
P16-2091,I08-3008,0,0.295176,". Note that D ELEX is trained on gold POS and therefore has an advantage in this 6 https://github.com/andersjo/ TurboParser 8 We do not include D ELEX in the comparison for the gold POS scenario only. In this particular scenario, D ELEX is also trained on gold POS, and thus biased: the cross-lingual taggers do not have gold POS available for training, and the same holds for D ELEX and projected POS. Manually annotated data We annotate a small number of sentences in English from EBC and 564 Collins, 2015). Transferring models by training parsers without lexical features was first introduced by Zeman and Resnik (2008). McDonald et al. (2011) and Søgaard (2011) coupled delexicalization with contributions from multiple sources, while McDonald et al. (2013) were the first to leverage uniform representations of POS and syntactic dependencies in cross-lingual parsing. Even more recently, Agi´c et al. (2015) exposed a bias towards closely related Indo-European languages shared by most previous work on annotation projection, while introducing a bias-free projection algorithm for learning 100 POS taggers from multiple sources. Their line of work is non-trivially extended to multilingual dependency parsing by Agi´c"
P16-2091,2005.mtsummit-papers.11,0,0.0204285,"o (Yarowsky et al., 2001; Hwa et al., 2005), and the best unsupervised dependency parsers today rely on annotation projection (Rasooli and Collins, 2015). Despite the maturity of the field, there is an inherent language bias in previous work on crosslanguage annotation projection. Cross-language annotation projection experiments require training data in m source languages, a parallel corpus of translations from the m source languages into the target language of interest, as well as evaluation data for the target language.1 Since the canonical resource for parallel text is the Europarl Corpus (Koehn, 2005), which covers languages spoken in the European parliament, annotation projection is Contributions We present a novel ILP-based algorithm for jointly projecting POS labels and dependency annotations across word-aligned parallel corpora. The performance of our algorithm compares favorably to that of a state-of-the-art projection algorithm, as well as to multi-source delexicalized transfer. Our experiments include between 23 and 27 languages using two parallel corpora that are available for hundreds of languages, namely a collection of Bibles and Watchtower periodicals. Finally, we make both the"
P16-2091,P14-1126,0,0.244444,"Missing"
P16-2091,P13-2109,0,0.101891,"Missing"
P16-2091,D11-1006,0,0.623582,"rmance of our algorithm compares favorably to that of a state-of-the-art projection algorithm, as well as to multi-source delexicalized transfer. Our experiments include between 23 and 27 languages using two parallel corpora that are available for hundreds of languages, namely a collection of Bibles and Watchtower periodicals. Finally, we make both the parallel corpora and the code publicly available.2 2 Projection algorithm The projection algorithm is divided into two distinct steps. First, we project potential syntactic 1 All previous work that we are aware of—with the possible exception of McDonald et al. (2011); but see Sections 2 and 5—uses only a single source (m = 1), but in our experiments, we use multiple source languages. 2 https://bitbucket.org/lowlands/ release 561 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 561–566, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics the POS labels are unknown, which is to say that every target token is ambiguous between |Σ |POS tags. We represent this ambiguity in the graph by creating a vertex for each possible combination of target word and POS. Concretely, if a source se"
P16-2091,P15-2034,0,0.0686959,"languages, we focus on the subsets that overlap with the UD languages to facilitate evaluation. For EBC, that amounts to 27 languages, and 23 for WTC. Treebanks To train the source-side taggers and dependency parsers, and to evaluate the crosslingual taggers and parsers, we use the Universal Dependencies (UD) version 1.2 treebanks with the corresponding test sets.3 Preprocessing We use simple sentence splitting and tokenization models to segment the parallel corpora.5 To sentence- and word-align the individual language pairs, we use a Gibbs samplingbased IBM1 alignment model called efmaral ¨ (Ostling, 2015). IBM1 has been shown to lead to more robust alignments across typologically dis¨ tant language pairs (Ostling, 2015). We modify Parallel texts We exploit two sources of parallel text: the Edinburgh Multilingual Bible corpus (EBC) (Christodouloupoulos and Steedman, 2014), and our own collection of online texts published by the Wathctower Society (WTC).4 While 4 ∈ {0, 1} Flow φi,k,j,l Data sources 3 ∈ {0, 1} 5 https://github.com/bplank/ multilingualtokenizer http://hdl.handle.net/11234/1-1548 https://www.jw.org/ 563 the aligner to output alignment probabilities. All the source-side texts are PO"
P16-2091,D15-1039,0,0.139868,"languages, relying on parallel corpora available for hundreds of languages. When training POS taggers and dependency parsers on jointly projected POS tags and syntactic dependencies using our algorithm, we obtain better performance than a standard approach on 20/23 languages using one parallel corpus; and 18/27 languages using another. 1 Introduction Cross-language annotation projection for unsupervised POS tagging and syntactic parsing was introduced fifteen years ago (Yarowsky et al., 2001; Hwa et al., 2005), and the best unsupervised dependency parsers today rely on annotation projection (Rasooli and Collins, 2015). Despite the maturity of the field, there is an inherent language bias in previous work on crosslanguage annotation projection. Cross-language annotation projection experiments require training data in m source languages, a parallel corpus of translations from the m source languages into the target language of interest, as well as evaluation data for the target language.1 Since the canonical resource for parallel text is the Europarl Corpus (Koehn, 2005), which covers languages spoken in the European parliament, annotation projection is Contributions We present a novel ILP-based algorithm for"
P16-2091,N06-2033,0,\N,Missing
P16-2091,P15-2044,1,\N,Missing
P16-2094,K15-1038,1,0.354253,"the near future, including eyetracking by smartphone or webcam (Skovsgaard et al., 2013; Xu et al., 2015). Gaze patterns during reading are strongly influenced by the parts of speech of the words being read. Psycholinguistic experiments show that readers are less likely to fixate on closed-class words that are predictable from context. Readers also fixate longer on rare words, on words that are semantically ambiguous, and on words that are morphologically complex (Rayner, 1998). These findings indicate that eye-tracking data should be useful for classifying words by part of speech, and indeed Barrett and Søgaard (2015) show that word-type-level aggregate statistics collected from eye-tracking corpora can be used as features for supervised PoS tagging, leading to substantial gains in accuracy across domains. This leads us to hypothesize that gaze data should also improve weakly supervised PoS tagging. In this paper, we test this hypothesis by experimenting with a PoS tagging model that uses raw text, dictionary information, and eye-tracking For many of the world’s languages, there are no or very few linguistically annotated resources. On the other hand, raw text, and often also dictionaries, can be harvested"
P16-2094,N10-1083,0,0.0534428,"Missing"
P16-2094,D10-1056,0,0.0110893,"of eye-tracking data and significantly outperforms a baseline that does not have access to eye-tracking data. 1 Introduction According to Ethnologue, there are around 7,000 languages in the world.1 For most of these languages, no or very little linguistically annotated resources are available. This is why over the past decade or so, NLP researchers have focused on developing unsupervised algorithms that learn from raw text, which for many languages is widely available on the web. An example is part-ofspeech (PoS) tagging, in which unsupervised approaches have been increasingly successful (see Christodoulopoulos et al. (2010) for an overview). The performance of unsupervised PoS taggers can be improved further if dictionary information is available, making it possible to constrain the PoS 1 http://www.ethnologue.com/world 579 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 579–584, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics 82 full baseline zi-2 xi-2 zi-1 xi-1 zi Dev. tagging accuracy 81 80 79 78 77 76 75 xi 74 Figure 1: Second-order HMM. In addition to the transitional probabilities of the antecedent state zi−1 in first-order"
P16-2094,W11-2123,0,0.0390121,"rden path sentences. 5. C ONTEXT features of the surrounding tokens. This group contains features relating to the fixations of the words in near proximity of the token. The eye can only recognize words a few characters to the left, and seven to eight characters to the right of the fixation (Rayner, 1998). Therefore it is useful to know the fixation pattern around the token. 6. N O G AZE BNC includes word length and word frequency obtained from the British National Corpus, as well as forward and backward transitional probabilities. These were computed using the KenLM language modeling toolkit (Heafield, 2011) with Kneser-Ney smoothing for unseen bigrams. 7. N O G AZE D UN includes the same features as N OGAZE BNC, but computed on the Dundee Corpus. They were extracted using CMUCambridge language modeling toolkit.4 Setup The Dundee Corpus does not include a standard train-development-test split, so we di4 http://www.speech.cs.cmu.edu/SLM/toolkit.html 582 All groups −N O G AZE BNC −N O G AZE D UN −BASIC −E ARLY −L ATE −R EG F ROM −C ONTEXT (Baseline) Accuracy 81.00 80.80 80.28 80.20 79.78 79.53 79.24 79.77 ∆ −0.20 −0.52* −0.08 −0.42* −0.25 −0.29* +0.53* Table 4: Results of an ablation study over fea"
P16-2094,D12-1127,0,0.025679,"Missing"
P17-2037,W01-1605,0,0.597384,".) [and replaster walls.] c. [Under Superfund, those] [who owned, generated or transported hazardous waste] [are liable for its cleanup, (. . .)] Finally, in a multi-lingual and multi-domain setting, note that all the corpora do not follow the same rules: for example, the relation ATTRIBU TION is only annotated in the English RST-DT and the corpora for Brazilian Portuguese, consequently, complements of attribution verbs are not segmented in the other corpora. 2 For German (Sidarenka et al., 2015) propose a segmenter in clauses (that may be EDU or not). 3 238 All the examples given come from (Carlson et al., 2001). 5 Binary task As in previous studies, we view segmentation as a binary task at the word level: a word is either an EDU boundary (label B, beginning an EDU) or not (label I, inside an EDU). This design choice is motivated by the fact that, in RST corpora, the EDUs cover the documents entirely, and that EDUs mostly are adjacent spans of text. An exception is when embedded EDUs break up another EDU, as in Example (3). The units 1 and 3 form in fact one EDU. We follow previous work on treating this as three segments, but note that this may not be the optimal solution. Our models perform sequence"
P17-2037,D14-1220,0,0.0584913,"Missing"
P17-2037,W11-0401,0,0.503259,"00 385 330 266 176 174 28, 260 21, 789 12, 594 3, 325 5, 754 2, 979 16, 827 9, 074 4, 385 1, 816 3, 090 1, 805 328, 362 210, 584 136, 346 57, 768 56, 197 33, 591 En-Gum-DT Nl-DT 54 80 3, 151 2, 345 2, 400 1, 692 44, 577 25, 095 Table 1: Number of documents, EDUs, sentences and words (according to UDPipe, see Section 7). Table 2: Results (F1 ), comparing cross-lingual and cross-domain results with UDPipe. on instruction manuals; and the GUM corpus6 (En-Gum-DT) containing interviews, news, travel guides and how-tos. For cross-lingual experiments, we use annotated corpora for Spanish (Es-DT) (da Cunha et al., 2011),7 German (De-DT) (Stede, 2004; Stede and Neumann, 2014), Dutch (Nl-DT) (Vliet et al., 2011; Redeker et al., 2012) and, for Brazilian Portuguese, we merged four corpora (PtDT) (Cardoso et al., 2011; Collovini et al., 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al., 2015). Three other RST corpora exist, but we were not able to obtain cross-lingual word embeddings for Basque (Iruskieta et al., 2013) and Chinese (Wu et al., 2016), and could not obtain the data for Tamil (Subalalitha and Parthasarathi, 2012). the languages and domains represented by enough data"
P17-2037,P14-1048,0,0.103368,"Missing"
P17-2037,P07-1062,0,0.741917,"Missing"
P17-2037,P82-1020,0,0.807647,"Missing"
P17-2037,P14-1002,0,0.157037,"Missing"
P17-2037,J15-3002,0,0.319229,"Missing"
P17-2037,P13-1048,0,0.0609576,"Missing"
P17-2037,N16-1179,1,0.803572,"k stacked layers. At the upper level, we compute the softmax predictions for each word based on a linear transformation. We use a logistic loss. We also investigate joint training of multiple languages and domains for discourse segmentation. We thus try to leverage languages and domains regularities by sharing the architecture and parameters through multi-task training, where an auxiliary task is a source language (resp. domain) different from the target language (resp. domain) of interest. Specifically, we train models based on hard parameters sharing (Caruana, 1993; Collobert et al., 2011; Klerke et al., 2016; Plank et al., 2016):4 each task is associated with a specific output layer, whereas the inner layers – the stacked LSTMs – are shared across the tasks. At training time, we randomly sample data points from one task and do forward predictions. During backpropagation, we modify the weights of the shared layers and the task-specific outer layer. The model is optimized for one target task (corresponding to the development data used). Except for the outer layer, the target task model is thus regularized by the induction of auxiliary models. (3) [But maintaining the key components (. . .)]1 [– a s"
P17-2037,E17-1072,1,0.858208,"Missing"
P17-2037,W12-1623,0,0.221992,"Missing"
P17-2037,P16-2067,1,0.906434,"the upper level, we compute the softmax predictions for each word based on a linear transformation. We use a logistic loss. We also investigate joint training of multiple languages and domains for discourse segmentation. We thus try to leverage languages and domains regularities by sharing the architecture and parameters through multi-task training, where an auxiliary task is a source language (resp. domain) different from the target language (resp. domain) of interest. Specifically, we train models based on hard parameters sharing (Caruana, 1993; Collobert et al., 2011; Klerke et al., 2016; Plank et al., 2016):4 each task is associated with a specific output layer, whereas the inner layers – the stacked LSTMs – are shared across the tasks. At training time, we randomly sample data points from one task and do forward predictions. During backpropagation, we modify the weights of the shared layers and the task-specific outer layer. The model is optimized for one target task (corresponding to the development data used). Except for the outer layer, the target task model is thus regularized by the induction of auxiliary models. (3) [But maintaining the key components (. . .)]1 [– a stable exchange rate a"
P17-2037,redeker-etal-2012-multi,0,0.40916,"Missing"
P17-2037,N03-1030,0,0.839387,"Missing"
P17-2037,H05-1033,0,0.453774,"Missing"
P17-2037,W04-0213,0,0.158461,"12, 594 3, 325 5, 754 2, 979 16, 827 9, 074 4, 385 1, 816 3, 090 1, 805 328, 362 210, 584 136, 346 57, 768 56, 197 33, 591 En-Gum-DT Nl-DT 54 80 3, 151 2, 345 2, 400 1, 692 44, 577 25, 095 Table 1: Number of documents, EDUs, sentences and words (according to UDPipe, see Section 7). Table 2: Results (F1 ), comparing cross-lingual and cross-domain results with UDPipe. on instruction manuals; and the GUM corpus6 (En-Gum-DT) containing interviews, news, travel guides and how-tos. For cross-lingual experiments, we use annotated corpora for Spanish (Es-DT) (da Cunha et al., 2011),7 German (De-DT) (Stede, 2004; Stede and Neumann, 2014), Dutch (Nl-DT) (Vliet et al., 2011; Redeker et al., 2012) and, for Brazilian Portuguese, we merged four corpora (PtDT) (Cardoso et al., 2011; Collovini et al., 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al., 2015). Three other RST corpora exist, but we were not able to obtain cross-lingual word embeddings for Basque (Iruskieta et al., 2013) and Chinese (Wu et al., 2016), and could not obtain the data for Tamil (Subalalitha and Parthasarathi, 2012). the languages and domains represented by enough data (upper part of Table 1). The “"
P17-2037,stede-neumann-2014-potsdam,0,0.251301,"25 5, 754 2, 979 16, 827 9, 074 4, 385 1, 816 3, 090 1, 805 328, 362 210, 584 136, 346 57, 768 56, 197 33, 591 En-Gum-DT Nl-DT 54 80 3, 151 2, 345 2, 400 1, 692 44, 577 25, 095 Table 1: Number of documents, EDUs, sentences and words (according to UDPipe, see Section 7). Table 2: Results (F1 ), comparing cross-lingual and cross-domain results with UDPipe. on instruction manuals; and the GUM corpus6 (En-Gum-DT) containing interviews, news, travel guides and how-tos. For cross-lingual experiments, we use annotated corpora for Spanish (Es-DT) (da Cunha et al., 2011),7 German (De-DT) (Stede, 2004; Stede and Neumann, 2014), Dutch (Nl-DT) (Vliet et al., 2011; Redeker et al., 2012) and, for Brazilian Portuguese, we merged four corpora (PtDT) (Cardoso et al., 2011; Collovini et al., 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al., 2015). Three other RST corpora exist, but we were not able to obtain cross-lingual word embeddings for Basque (Iruskieta et al., 2013) and Chinese (Wu et al., 2016), and could not obtain the data for Tamil (Subalalitha and Parthasarathi, 2012). the languages and domains represented by enough data (upper part of Table 1). The “cross” models are trained"
P17-2037,L16-1680,0,0.0323505,"Missing"
P17-2037,W12-5607,0,0.0279523,"s-lingual experiments, we use annotated corpora for Spanish (Es-DT) (da Cunha et al., 2011),7 German (De-DT) (Stede, 2004; Stede and Neumann, 2014), Dutch (Nl-DT) (Vliet et al., 2011; Redeker et al., 2012) and, for Brazilian Portuguese, we merged four corpora (PtDT) (Cardoso et al., 2011; Collovini et al., 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al., 2015). Three other RST corpora exist, but we were not able to obtain cross-lingual word embeddings for Basque (Iruskieta et al., 2013) and Chinese (Wu et al., 2016), and could not obtain the data for Tamil (Subalalitha and Parthasarathi, 2012). the languages and domains represented by enough data (upper part of Table 1). The “cross” models are trained using multi-task learning. 7 Parameters The hyper-parameters are tuned on the development set: number of iterations i ∈ {10, 20, 30}, Gaussian noise σ ∈ {0.1, 0.2}, and number of dimensions d ∈ {50, 500}. We fix the number n of stacked hidden layers to 2 and the size of the hidden layers h to 100 after experimenting on the En-DT.9 Our final models use σ = 0.2 and d = 500. Representation We use tokens and POS tags as input data.10 The aim is to build a representation considering the cu"
P17-2037,N09-1064,0,0.140666,"Missing"
P17-2054,J14-1002,0,0.0173998,"urer, 2007). Here, we follow the probably most common approach to multi-task learning, known as hard parameter sharing. This was introduced in Caruana (1993) in the context of deep neural networks, in which hidden layers can be shared among tasks. We assume T different training set, D1 , · · · , DT , Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following Søgaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task at a time. Note that the datasets for the auxiliary tasks are not annotated with keyphrase boundary identification or classification labels. Datasets We evaluate on the SemEval 2017 Task 10 dataset (Augenstein et al., 2017) and the the ACL RD-TEC 2.0 dataset (Qase"
P17-2054,P05-1045,0,0.0412449,"Missing"
P17-2054,P14-1119,0,0.0792035,"harder. Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the probability of each output label conditioned on the label at the previous time step. Lample et al. (2016), currently state-of-the-art for NER, stack CRFs on top of recurrent neural networks. We leave exploring such models in combination with multi-task learning for future work. Keyphrase detection methods specific to the scientific domain often use keyphrase gazetteers as features or exploit citation graphs (Hasan and Ng, 2014). However, previous methods relied on corpora annotated for type-level identification, not for mention-level identification (Kim et al., 2010; Sterckx et al., 2016). While most applications Multi-Task Learning Hard sharing of all hidden layers was introduced in Caruana (1993), and popularised in NLP by Collobert et al. (2011a). Several variants have been introduced, including hard sharing of selected layers (Søgaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al., 2015). Søgaard and Goldberg (2016) show that hard parameter sharing is an effective regulariser, also on"
P17-2054,S14-1001,1,0.774038,"Missing"
P17-2054,S17-2091,1,0.835359,"Missing"
P17-2054,D15-1168,0,0.0322281,"to the scientific domain often use keyphrase gazetteers as features or exploit citation graphs (Hasan and Ng, 2014). However, previous methods relied on corpora annotated for type-level identification, not for mention-level identification (Kim et al., 2010; Sterckx et al., 2016). While most applications Multi-Task Learning Hard sharing of all hidden layers was introduced in Caruana (1993), and popularised in NLP by Collobert et al. (2011a). Several variants have been introduced, including hard sharing of selected layers (Søgaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al., 2015). Søgaard and Goldberg (2016) show that hard parameter sharing is an effective regulariser, also on heterogeneous tasks such as the ones considered here. Hard parameter sharing has been studied for several tasks, including CCG super tagging (Søgaard and Goldberg, 2016), text normalisation (Bollman and Søgaard, 2016), neural machine translation (Dong et al., 2015; Luong et al., 2016), and super-sense tagging (Mart´ınez Alonso and Plank, 2017). Sharing of information can further be achieved by extending LSTMs with an external memory shared 344 rely on extracting keyphrases (as types), this has t"
P17-2054,D16-1012,0,0.0275467,"+ Multi-word BiLSTM + Super-sense 83.36 84.11 83.94 84.86 84.67 79.46 79.39 79.12 76.92 78.29 59.26 60.64 60.18 59.81 61.35 57.24 57.24 56.73 54.21 56.73 81.37 81.68 81.46 80.69 81.36 57.84 58.89 58.40 56.87 58.95 Table 3: Results for keyphrase boundary classification on the ACL RD-TEC corpus automatic discovery of text in parallel translation’, ‘honeycomb network of graphite bricks’). Being able to recognise long keyphrases correctly is part of the reason our multi-task models outperform the baselines, especially on the SemEval dataset, which contains many such long keyphrases. across tasks (Liu et al., 2016). A further instance of multi-task learning is to optimise a supervised training objective jointly with an unsupervised training objective, as shown in Yu et al. (2016) for natural language generation and autoencoding, and in Rei (2017) for different sequence labelling tasks and language modelling. 6 Related Work Boundary Classification KBC is very similar to named entity recognition (NER), though arguably harder. Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the pro"
P17-2054,C16-1013,1,0.853895,"Missing"
P17-2054,E17-1005,0,0.115293,"Missing"
P17-2054,L16-1294,0,0.104901,"014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task at a time. Note that the datasets for the auxiliary tasks are not annotated with keyphrase boundary identification or classification labels. Datasets We evaluate on the SemEval 2017 Task 10 dataset (Augenstein et al., 2017) and the the ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016). The SemEval 2017 dataset is annotated with three keyphrase types, the ACL RD-TEC dataset with seven. For the former, we test on the development portion of the dataset, as the test set is not released yet. We randomly split ACL RD-TEC into a training and test set, reserv342 SemEval 2017 Task 10 Labels ACL RD-TEC Material, Process, Task Technology and Method, Tool and Library, Language Resource, Language Resource Product, Measures and Measurements, Models, Other Topics Computer Science, Physics, Natural Language Processing Material Science Number all keyphrases 5730 2939 Proportion singleton k"
P17-2054,P17-1194,0,0.0589963,"e boundary classification on the ACL RD-TEC corpus automatic discovery of text in parallel translation’, ‘honeycomb network of graphite bricks’). Being able to recognise long keyphrases correctly is part of the reason our multi-task models outperform the baselines, especially on the SemEval dataset, which contains many such long keyphrases. across tasks (Liu et al., 2016). A further instance of multi-task learning is to optimise a supervised training objective jointly with an unsupervised training objective, as shown in Yu et al. (2016) for natural language generation and autoencoding, and in Rei (2017) for different sequence labelling tasks and language modelling. 6 Related Work Boundary Classification KBC is very similar to named entity recognition (NER), though arguably harder. Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the probability of each output label conditioned on the label at the previous time step. Lample et al. (2016), currently state-of-the-art for NER, stack CRFs on top of recurrent neural networks. We leave exploring such models in combination wi"
P17-2054,N15-1177,0,0.0282534,"ontext of deep neural networks, in which hidden layers can be shared among tasks. We assume T different training set, D1 , · · · , DT , Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following Søgaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task at a time. Note that the datasets for the auxiliary tasks are not annotated with keyphrase boundary identification or classification labels. Datasets We evaluate on the SemEval 2017 Task 10 dataset (Augenstein et al., 2017) and the the ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016). The SemEval 2017 dataset is annotated with three keyphrase types, the ACL RD-TEC dataset with seven. For the former, we test on the development po"
P17-2054,P16-2038,1,0.934379,"st as a regulariser as studies show reductions in Rademacher complexity in multi-task architectures over single-task architectures (Baxter, 2000; Maurer, 2007). Here, we follow the probably most common approach to multi-task learning, known as hard parameter sharing. This was introduced in Caruana (1993) in the context of deep neural networks, in which hidden layers can be shared among tasks. We assume T different training set, D1 , · · · , DT , Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following Søgaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task at a time. Note that the datasets for the auxiliary tasks are not annotated with keyphrase boundary identification or classifi"
P17-2054,P10-1130,0,0.0132169,"lti-task learning, known as hard parameter sharing. This was introduced in Caruana (1993) in the context of deep neural networks, in which hidden layers can be shared among tasks. We assume T different training set, D1 , · · · , DT , Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following Søgaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task at a time. Note that the datasets for the auxiliary tasks are not annotated with keyphrase boundary identification or classification labels. Datasets We evaluate on the SemEval 2017 Task 10 dataset (Augenstein et al., 2017) and the the ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016). The SemEval 2017 dataset is annotated with thr"
P17-2054,D16-1198,0,0.102576,"ds, thereby modelling the probability of each output label conditioned on the label at the previous time step. Lample et al. (2016), currently state-of-the-art for NER, stack CRFs on top of recurrent neural networks. We leave exploring such models in combination with multi-task learning for future work. Keyphrase detection methods specific to the scientific domain often use keyphrase gazetteers as features or exploit citation graphs (Hasan and Ng, 2014). However, previous methods relied on corpora annotated for type-level identification, not for mention-level identification (Kim et al., 2010; Sterckx et al., 2016). While most applications Multi-Task Learning Hard sharing of all hidden layers was introduced in Caruana (1993), and popularised in NLP by Collobert et al. (2011a). Several variants have been introduced, including hard sharing of selected layers (Søgaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al., 2015). Søgaard and Goldberg (2016) show that hard parameter sharing is an effective regulariser, also on heterogeneous tasks such as the ones considered here. Hard parameter sharing has been studied for several tasks, including CCG super tagging (Søgaard and Goldberg,"
P17-2054,D16-1138,0,0.023168,"46 80.69 81.36 57.84 58.89 58.40 56.87 58.95 Table 3: Results for keyphrase boundary classification on the ACL RD-TEC corpus automatic discovery of text in parallel translation’, ‘honeycomb network of graphite bricks’). Being able to recognise long keyphrases correctly is part of the reason our multi-task models outperform the baselines, especially on the SemEval dataset, which contains many such long keyphrases. across tasks (Liu et al., 2016). A further instance of multi-task learning is to optimise a supervised training objective jointly with an unsupervised training objective, as shown in Yu et al. (2016) for natural language generation and autoencoding, and in Rei (2017) for different sequence labelling tasks and language modelling. 6 Related Work Boundary Classification KBC is very similar to named entity recognition (NER), though arguably harder. Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the probability of each output label conditioned on the label at the previous time step. Lample et al. (2016), currently state-of-the-art for NER, stack CRFs on top of recurre"
P18-1072,P17-1042,0,0.677938,"form of explicit alignment. Our results indicate this assumption is not true in general, and that approaches based on this assumption have important limitations. Introduction Cross-lingual word representations enable us to reason about word meaning in multilingual contexts and facilitate cross-lingual transfer (Ruder et al., 2018). Early cross-lingual word embedding models relied on large amounts of parallel data (Klementiev et al., 2012; Mikolov et al., 2013a), but more recent approaches have tried to minimize the amount of supervision necessary (Vuli´c and Korhonen, 2016; Levy et al., 2017; Artetxe et al., 2017). Some researchers have even presented unsupervised methods that do not rely on any form Contributions We focus on the recent stateof-the-art unsupervised model of Conneau et al. (2018).1 Our contributions are: (a) In §2, we show that the monolingual word embeddings used in Conneau et al. (2018) are not approximately isomorphic, using the VF2 algorithm (Cordella et al., 2001) and we therefore introduce a metric for quantifying the similarity of word embeddings, based on Laplacian eigenvalues. (b) In §3, we identify circumstances under which the unsupervised bilingual 1 Our motivation for this"
P18-1072,D18-1549,0,0.0413667,"ion for this is that dimensionality reduction will alter the geometric shape and remove characteristics of the embedding graphs that are important for alignment; but on the other hand, lower dimensionality introduces regularization, which will make the graphs more similar. Finally, in §4.6, we investigate the impact of different types of query test words on performance, including how performance varies across part-of-speech word classes and on shared vocabulary items. Learning scenarios Unsupervised neural machine translation relies on BDI using cross-lingual embeddings (Lample et al., 2018a; Artetxe et al., 2018), which in turn relies on the assumption that word embedding graphs are approximately isomorphic. The work of Conneau et al. (2018), which we focus on here, also makes several implicit assumptions that may or may not be necessary to achieve such isomorphism, and which may or may not scale to low-resource languages. The algorithms are not intended to be limited to learning scenarios where these assumptions hold, but since they do in the reported experiments, it is important to see to what extent these assumptions are necessary for the algorithms to produce useful embeddings or dictionaries. We"
P18-1072,W16-1614,0,0.386638,"l Dictionary Induction Anders Søgaard♥ Sebastian Ruder♠♣ Ivan Vuli´c3 ♥ University of Copenhagen, Copenhagen, Denmark ♠ Insight Research Centre, National University of Ireland, Galway, Ireland ♣ Aylien Ltd., Dublin, Ireland 3 Language Technology Lab, University of Cambridge, UK soegaard@di.ku.dk,sebastian@ruder.io,iv250@cam.ac.uk Abstract of cross-lingual supervision at all (Barone, 2016; Conneau et al., 2018; Zhang et al., 2017). Unsupervised cross-lingual word embeddings hold promise to induce bilingual lexicons and machine translation models in the absence of dictionaries and translations (Barone, 2016; Zhang et al., 2017; Lample et al., 2018a), and would therefore be a major step toward machine translation to, from, or even between low-resource languages. Unsupervised approaches to learning crosslingual word embeddings are based on the assumption that monolingual word embedding graphs are approximately isomorphic, that is, after removing a small set of vertices (words) (Mikolov et al., 2013b; Barone, 2016; Zhang et al., 2017; Conneau et al., 2018). In the words of Barone (2016): Unsupervised machine translation—i.e., not assuming any cross-lingual supervision signal, whether a dictionary,"
P18-1072,P07-1056,0,0.0348626,"ES when the two monolingual embedding spaces are induced by two different algorithms (see the results of the entire Spanish cbow column).9 In sum, this means that the unsupervised approach is unlikely to work on pre-trained word embeddings unless they are induced on same6 One exception here is French, which they include in their paper, but French arguably has a relatively simple morphology. 7 In order to get comparable term distributions, we translate the source language to the target language using the bilingual dictionaries provided by Conneau et al. (2018). 8 We also computed A-distances (Blitzer et al., 2007) and confirmed that trends were similar. 9 We also checked if this result might be due to a lowerquality monolingual ES space. However, monolingual word similarity scores on available datasets in Spanish show performance comparable to that of Spanish skip-gram vectors: e.g., Spearman’s ρ correlation is ≈ 0.7 on the ES evaluation set from SemEval-2017 Task 2 (Camacho-Collados et al., 2017). 783 EP 0.75 Wiki EMEA Wiki EMEA 0.60 0.55 0.65 0.60 0.55 0.50 EN:EP (a) en-es: domain similarity EMEA 0.65 0.60 0.55 0.50 EN:Wiki EN:EMEA Training Corpus (English) Wiki 0.70 Jensen-Shannon Similarity 0.65 EP"
P18-1072,Q17-1010,0,0.798117,"here, also makes several implicit assumptions that may or may not be necessary to achieve such isomorphism, and which may or may not scale to low-resource languages. The algorithms are not intended to be limited to learning scenarios where these assumptions hold, but since they do in the reported experiments, it is important to see to what extent these assumptions are necessary for the algorithms to produce useful embeddings or dictionaries. We focus on the work of Conneau et al. (2018), who present a fully unsupervised approach to aligning monolingual word embeddings, induced using fastText (Bojanowski et al., 2017). We describe the learning algorithm in §3.2. Conneau et al. (2018) consider a specific set of learning scenarios: 3.2 Summary of Conneau et al. (2018) We now introduce the method of Conneau et al. (2018).4 The approach builds on existing work on learning a mapping between monolingual word embeddings (Mikolov et al., 2013b; Xing et al., 2015) and consists of the following steps: 1) Monolingual word embeddings: An off-the-shelf word embedding algorithm (Bojanowski et al., 2017) is used to learn source and target language spaces X (a) The authors work with the following languages: English-{Frenc"
P18-1072,C12-1089,0,0.251885,"isomorphic between languages, and that this isomorphism can be learned from the statistics of the realizations of these processes, the monolingual corpora, in principle without any form of explicit alignment. Our results indicate this assumption is not true in general, and that approaches based on this assumption have important limitations. Introduction Cross-lingual word representations enable us to reason about word meaning in multilingual contexts and facilitate cross-lingual transfer (Ruder et al., 2018). Early cross-lingual word embedding models relied on large amounts of parallel data (Klementiev et al., 2012; Mikolov et al., 2013a), but more recent approaches have tried to minimize the amount of supervision necessary (Vuli´c and Korhonen, 2016; Levy et al., 2017; Artetxe et al., 2017). Some researchers have even presented unsupervised methods that do not rely on any form Contributions We focus on the recent stateof-the-art unsupervised model of Conneau et al. (2018).1 Our contributions are: (a) In §2, we show that the monolingual word embeddings used in Conneau et al. (2018) are not approximately isomorphic, using the VF2 algorithm (Cordella et al., 2001) and we therefore introduce a metric for q"
P18-1072,2005.mtsummit-papers.11,0,0.0562212,"Missing"
P18-1072,S17-2002,0,0.0503011,"morphology. 7 In order to get comparable term distributions, we translate the source language to the target language using the bilingual dictionaries provided by Conneau et al. (2018). 8 We also computed A-distances (Blitzer et al., 2007) and confirmed that trends were similar. 9 We also checked if this result might be due to a lowerquality monolingual ES space. However, monolingual word similarity scores on available datasets in Spanish show performance comparable to that of Spanish skip-gram vectors: e.g., Spearman’s ρ correlation is ≈ 0.7 on the ES evaluation set from SemEval-2017 Task 2 (Camacho-Collados et al., 2017). 783 EP 0.75 Wiki EMEA Wiki EMEA 0.60 0.55 0.65 0.60 0.55 0.50 EN:EP (a) en-es: domain similarity EMEA 0.65 0.60 0.55 0.50 EN:Wiki EN:EMEA Training Corpus (English) Wiki 0.70 Jensen-Shannon Similarity 0.65 EP 0.75 0.70 Jensen-Shannon Similarity 0.70 Jensen-Shannon Similarity EP 0.75 EN:EP 0.50 EN:Wiki EN:EMEA Training Corpus (English) (b) en-fi: domain similarity EN:EP EN:Wiki EN:EMEA Training Corpus (English) (c) en-hu: domain similarity 64.09 50 49.24 46.52 30 25.17 BLI: P@1 BLI: P@1 40 25.48 20 60 60 50 50 40 40 BLI: P@1 60 30 28.63 20 30 26.99 20 15.56 14.79 0 EN:EP 9.63 10 6.63 4.84 0 EN"
P18-1072,J82-2005,0,0.817708,"Missing"
P18-1072,E17-1072,1,0.939289,"nciple without any form of explicit alignment. Our results indicate this assumption is not true in general, and that approaches based on this assumption have important limitations. Introduction Cross-lingual word representations enable us to reason about word meaning in multilingual contexts and facilitate cross-lingual transfer (Ruder et al., 2018). Early cross-lingual word embedding models relied on large amounts of parallel data (Klementiev et al., 2012; Mikolov et al., 2013a), but more recent approaches have tried to minimize the amount of supervision necessary (Vuli´c and Korhonen, 2016; Levy et al., 2017; Artetxe et al., 2017). Some researchers have even presented unsupervised methods that do not rely on any form Contributions We focus on the recent stateof-the-art unsupervised model of Conneau et al. (2018).1 Our contributions are: (a) In §2, we show that the monolingual word embeddings used in Conneau et al. (2018) are not approximately isomorphic, using the VF2 algorithm (Cordella et al., 2001) and we therefore introduce a metric for quantifying the similarity of word embeddings, based on Laplacian eigenvalues. (b) In §3, we identify circumstances under which the unsupervised bilingual 1 O"
P18-1072,D16-1235,1,0.901982,"Missing"
P18-1072,K15-1026,0,0.0281285,"models are evaluated on a held-out set of query words. Here, we analyze the performance of the unsupervised approach across different parts-ofspeech, frequency bins, and with respect to query words that have orthographically identical counterparts in the target language with the same or a different meaning. Part-of-speech We show the impact of the partof-speech of the query words in Table 4; again on a representative subset of our languages. The results indicate that performance on verbs is lowest across the board. This is consistent with research on distributional semantics and verb meaning (Schwartz et al., 2015; Gerz et al., 2016). Frequency We also investigate the impact of the frequency of query words. We calculate the word frequency of English words based on Google’s Trillion Word Corpus: query words are divided in groups based on their rank – i.e., the first group contains the top 100 most frequent words, the second one contains the 101th-1000th most frequent words, etc. – and plot performance (P@1) relative to rank in Figure 3. For EN - FI, P@1 was 0 across all frequency ranks. The plot shows sensitivity to frequency for HU, but less so for ES. Homographs Since we use identical word forms (homo"
P18-1072,P08-1088,0,0.122331,"arity is strong (ρ ∼ 0.89). 5 Related work Cross-lingual word embeddings Cross-lingual word embedding models typically, unlike Conneau et al. (2018), require aligned words, sentences, or documents (Levy et al., 2017). Most approaches based on word alignments learn an explicit mapping between the two embedding spaces (Mikolov et al., 2013b; Xing et al., 2015). Recent approaches try to minimize the amount of supervision needed (Vuli´c and Korhonen, 2016; Artetxe et al., 2017; Smith et al., 2017). See Upadhyay et al. (2016) and Ruder et al. (2018) for surveys. Unsupervised cross-lingual learning Haghighi et al. (2008) were first to explore unsupervised BDI, using features such as context counts and orthographic substrings, and canonical correlation analysis. Recent approaches use adversarial learning (Goodfellow et al., 2014) and employ a discriminator, trained to distinguish between the translated source and the target language space, and a generator learning a translation matrix (Barone, 2016). Zhang et al. (2017), in addition, use different forms of regularization for convergence, while Conneau et al. (2018) uses additional steps to refine the induced embedding space. 6 Conclusion We investigated when u"
P18-1072,P16-1157,0,0.0607205,"value in the half-open interval [0, ∞). The correlation between BDI performance and graph similarity is strong (ρ ∼ 0.89). 5 Related work Cross-lingual word embeddings Cross-lingual word embedding models typically, unlike Conneau et al. (2018), require aligned words, sentences, or documents (Levy et al., 2017). Most approaches based on word alignments learn an explicit mapping between the two embedding spaces (Mikolov et al., 2013b; Xing et al., 2015). Recent approaches try to minimize the amount of supervision needed (Vuli´c and Korhonen, 2016; Artetxe et al., 2017; Smith et al., 2017). See Upadhyay et al. (2016) and Ruder et al. (2018) for surveys. Unsupervised cross-lingual learning Haghighi et al. (2008) were first to explore unsupervised BDI, using features such as context counts and orthographic substrings, and canonical correlation analysis. Recent approaches use adversarial learning (Goodfellow et al., 2014) and employ a discriminator, trained to distinguish between the translated source and the target language space, and a generator learning a translation matrix (Barone, 2016). Zhang et al. (2017), in addition, use different forms of regularization for convergence, while Conneau et al. (2018)"
P18-1072,P16-1024,1,0.899531,"Missing"
P18-1072,D13-1168,1,0.909528,"Missing"
P18-1072,N15-1104,0,0.328041,"Missing"
P18-1072,P17-1179,0,0.413596,"nduction Anders Søgaard♥ Sebastian Ruder♠♣ Ivan Vuli´c3 ♥ University of Copenhagen, Copenhagen, Denmark ♠ Insight Research Centre, National University of Ireland, Galway, Ireland ♣ Aylien Ltd., Dublin, Ireland 3 Language Technology Lab, University of Cambridge, UK soegaard@di.ku.dk,sebastian@ruder.io,iv250@cam.ac.uk Abstract of cross-lingual supervision at all (Barone, 2016; Conneau et al., 2018; Zhang et al., 2017). Unsupervised cross-lingual word embeddings hold promise to induce bilingual lexicons and machine translation models in the absence of dictionaries and translations (Barone, 2016; Zhang et al., 2017; Lample et al., 2018a), and would therefore be a major step toward machine translation to, from, or even between low-resource languages. Unsupervised approaches to learning crosslingual word embeddings are based on the assumption that monolingual word embedding graphs are approximately isomorphic, that is, after removing a small set of vertices (words) (Mikolov et al., 2013b; Barone, 2016; Zhang et al., 2017; Conneau et al., 2018). In the words of Barone (2016): Unsupervised machine translation—i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or com"
P18-1072,W13-3520,0,\N,Missing
P19-1157,N19-1389,1,0.830496,"from reinforcement learning, i.e., the distribution of correct normalizations by reinforcement learning that our baseline architecture classified wrongly, correlate significantly with the length of the input across all four languages. 2. MLE L ENGTH : The correlations are even stronger with the length of the output of 5 Conclusions 2 Note that for the MLE baseline, we performed our own hyperparameter tuning, which results in a different configuration than used in previous work (e.g., Bollmann et al., 2017; Tang et al., 2018). We observe that our baseline is weaker than the models reported in Bollmann (2019), but even so, the MLE+PG approach yields state-of-the-art performance on the Slovene dataset. 3 Correlations are Pearson’s r. Samples are big enough to motivate a parametric test, but we obtain similar coefficients and significance levels with Spearman’s ρ. Our experiments show that across several languages, policy gradient fine-tuning outperforms maximum likelihood training of sequence-tosequence models for historical text normalization. Since historical text normalization is a characterlevel transduction task, it is feasible to experiment with reinforcement learning, and we believe 1617 our"
P19-1157,P17-1031,1,0.847236,"Rewards Simon Flachs, Marcel Bollmann, Anders Søgaard Department of Computer Science University of Copenhagen Denmark {flachs,marcel,soegaard}@di.ku.dk Abstract learning to optimize directly for the evaluation metric (Ranzato et al., 2016; Shen et al., 2016). Reinforcement learning enables direct optimization of exact matches or other non-decomposable metrics, computing updates based on delayed rewards rather than token-level error signals. This paper contrasts maximum likelihood training and training with delayed rewards, in the context of sequence-to-sequence historical text normalization (Bollmann et al., 2017). Training neural sequence-to-sequence models with simple token-level log-likelihood is now a standard approach to historical text normalization, albeit often outperformed by phrasebased models. Policy gradient training enables direct optimization for exact matches, and while the small datasets in historical text normalization are prohibitive of from-scratch reinforcement learning, we show that policy gradient fine-tuning leads to significant improvements across the board. Policy gradient training, in particular, leads to more accurate normalizations for long or unseen words. 1 Introduction Hi"
P19-1157,C16-1013,1,0.75711,"cross the board. Policy gradient training, in particular, leads to more accurate normalizations for long or unseen words. 1 Introduction Historical text normalization is a common approach to making historical documents accessible and searchable. It is a challenging problem, since most historical texts were written without fixed spelling conventions, and spelling is therefore at times idiosyncratic (Piotrowski, 2012). Traditional approaches to historical text normalization relied on hand-written rules, but recently, several authors have proposed neural models for historical text normalization (Bollmann and Søgaard, 2016; Bollmann, 2018; Tang et al., 2018). Such models are trained using characterlevel maximum-likelihood training, which is inconsistent with the objective of historical text normalization; namely, transduction into modern, searchable word forms. The discrepancy between character-level loss and our word-level objective means that model decision costs are biased. Our objective, however, is reflected by the standard evaluation metric, which is computed as the fraction of benchmark words that are translated correctly. In order to mitigate the discrepancy between the optimization method and the task"
P19-1157,P16-1159,0,0.0673811,"Missing"
P19-1157,C18-1112,0,0.172616,"n particular, leads to more accurate normalizations for long or unseen words. 1 Introduction Historical text normalization is a common approach to making historical documents accessible and searchable. It is a challenging problem, since most historical texts were written without fixed spelling conventions, and spelling is therefore at times idiosyncratic (Piotrowski, 2012). Traditional approaches to historical text normalization relied on hand-written rules, but recently, several authors have proposed neural models for historical text normalization (Bollmann and Søgaard, 2016; Bollmann, 2018; Tang et al., 2018). Such models are trained using characterlevel maximum-likelihood training, which is inconsistent with the objective of historical text normalization; namely, transduction into modern, searchable word forms. The discrepancy between character-level loss and our word-level objective means that model decision costs are biased. Our objective, however, is reflected by the standard evaluation metric, which is computed as the fraction of benchmark words that are translated correctly. In order to mitigate the discrepancy between the optimization method and the task objective, work has been carried out"
P19-1232,S15-2162,0,0.424899,"Missing"
P19-1232,D15-1041,0,0.0128658,"-based approaches; (ii) we show that multitask learning of state representations for this parsing algorithm is superior to single-task training; (iii) we improve this model by task-specific policy gradient fine-tuning; (iv) we achieve a new state of the art result across three linguistic formalisms; finally, (v) we show that policy gradient fine-tuning learns an easy-first strategy, which reduces error propagation. 2 Related Work There are generally two kinds of dependency parsing algorithms, namely transition-based parsing algorithms (McDonald and Nivre, 2007; Kiperwasser and Goldberg, 2016; Ballesteros et al., 2015) and graph-based ones (McDonald and Pereira, 2006; Zhang and Clark, 2008; Galley and Manning, 2009; Zhang et al., 2017). In graphbased parsing, a model is trained to score all possible dependency arcs between words, and decoding algorithms are subsequently applied to find the most likely dependency graph. The Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm are often used for finding the most likely dependency trees, whereas the AD3 algorithm (Martins et al., 2011) is used for finding SDP graphs that form DAGs in Peng et al. (2017) and Peng et al. (2018). During training, the"
P19-1232,P18-2077,0,0.147416,"-task learning of the shared BiLSTM (IPS+ML) leads to small improvements across the board, which is consistent with the results of Peng et al. (2017). The model trained with reinforcement learning (IPS+RL) performs better than the model trained by supervised learning (IPS). These differences are significant (p < 10−3 ). Most importantly, the combination of multi-task learning and policy gradient-based reinforcement learning (IPS+ML+RL) achieves the best results among all IPS models and the previous state of the art models, by some margin. We also obtain similar results for the out-of-domain 7 Dozat and Manning (2018) report macro-averaged scores instead, as mentioned in their ACL 2018 talk, and their results are therefore not comparable to ours. For details, see the video of their talk on ACL2018 that is available on Vimeo. 2426 Dist. a) Supervised Transitions 1st 2nd 3rd 4th Dist. b) Reinforcement Arc Length Figure 4: Arc length distributions: (a) Supervised learning (IPS+ML). (b) Reinforcement learning (IPS+ML+RL). The four lines correspond to the first to fourth transitions in the derivations. SL 4 1 RL 3 2 2 3 a) The U.S. Commerce Department reported a $ 10.77 billion deficit in August compared with ."
P19-1232,S15-2154,0,0.424492,"Missing"
P19-1232,N16-1024,0,0.0233331,"herefore likely to happen when the parser predicts wrong transitions leading to unseen states (McDonald and Nivre, 2007; Goldberg and Nivre, 2013). There have been several attempts to train transition-based parsers with reinforcement learning: Zhang and Chan (2009) applied SARSA (Baird III, 1999) to an Arc-Standard model, using SARSA updates to fine-tune a model that was pre-trained using a feed-forward neural network. Fried and Klein (2018), more recently, presented experiments with applying policy gradient training to several constituency parsers, including the RNNG transition-based parser (Dyer et al., 2016). In their experiments, however, the models trained with policy gradient did not always perform better than the models trained with supervised learning. We hypothesize this is due to credit assignment being difficult in transition-based parsing. Iterative refinement approaches have been proposed in the context of sentence generation (Lee et al., 2018). Our proposed model explores multiple transition paths at once and avoids making risky decisions in the initial transitions, in part inspired by such iterative refinement techniques. We also pre-train our model with supervised learning to avoid s"
P19-1232,C96-1058,0,0.32435,"learns an easy-first strategy, which reduces error propagation. 2 Related Work There are generally two kinds of dependency parsing algorithms, namely transition-based parsing algorithms (McDonald and Nivre, 2007; Kiperwasser and Goldberg, 2016; Ballesteros et al., 2015) and graph-based ones (McDonald and Pereira, 2006; Zhang and Clark, 2008; Galley and Manning, 2009; Zhang et al., 2017). In graphbased parsing, a model is trained to score all possible dependency arcs between words, and decoding algorithms are subsequently applied to find the most likely dependency graph. The Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm are often used for finding the most likely dependency trees, whereas the AD3 algorithm (Martins et al., 2011) is used for finding SDP graphs that form DAGs in Peng et al. (2017) and Peng et al. (2018). During training, the loss is computed after decoding, leading the models to reflect a structured loss. The advantage of graphbased algorithms is that there is no real error propagation to the extent the decoding algorithms are global inference algorithm, but this also means that reinforcement learning is not obviously applicable to graph-based parsing. In trans"
P19-1232,P09-1087,0,0.0120886,"lgorithm is superior to single-task training; (iii) we improve this model by task-specific policy gradient fine-tuning; (iv) we achieve a new state of the art result across three linguistic formalisms; finally, (v) we show that policy gradient fine-tuning learns an easy-first strategy, which reduces error propagation. 2 Related Work There are generally two kinds of dependency parsing algorithms, namely transition-based parsing algorithms (McDonald and Nivre, 2007; Kiperwasser and Goldberg, 2016; Ballesteros et al., 2015) and graph-based ones (McDonald and Pereira, 2006; Zhang and Clark, 2008; Galley and Manning, 2009; Zhang et al., 2017). In graphbased parsing, a model is trained to score all possible dependency arcs between words, and decoding algorithms are subsequently applied to find the most likely dependency graph. The Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm are often used for finding the most likely dependency trees, whereas the AD3 algorithm (Martins et al., 2011) is used for finding SDP graphs that form DAGs in Peng et al. (2017) and Peng et al. (2018). During training, the loss is computed after decoding, leading the models to reflect a structured loss. The advantage of"
P19-1232,N10-1115,0,0.0671712,"state to the final parsing state, depending on the orders of creating the arcs. This is known as the nondeterministic oracle problem (Goldberg and Nivre, 2013). In IPS parsing, some arcs are easy to predict; others are very hard to predict. Long-distance arcs are generally difficult to predict, but they are very important for down-stream applications, including reordering for machine translation (Xu et al., 2009). Since long-distance arcs are harder to predict, and transition-based parsers are prone to error propagation, several easy-first strategies have been introduced, both in supervised (Goldberg and Elhadad, 2010; Ma et al., 2013) and unsupervised dependency parsing (Spitkovsky et al., 2011), to prefer some paths over others in the face of the non-deterministic oracle problem. Easy-first principles have also proven effective with sequence taggers (Tsuruoka and Tsujii, 2005; Martins and Kreutzer, 2017). In this paper, we take an arguably more principled approach, learning a strategy for choosing transition paths over others using reinforcement learning. We observe, however, that the learned strategies exhibit a clear easy-first preference. were extremely rare in our experiments, and can be avoided by s"
P19-1232,D07-1013,0,0.612809,"a directed acyclic graph that expresses various relations among words. However, the fact that SDP structures are directed acyclic graphs means that we cannot apply standard dependency parsing algorithms to SDP. Standard dependency parsing algorithms are often said to come in two flavors: transition-based parsers score transitions between states, and gradually build up dependency graphs on the side. Graph-based parsers, in contrast, score all candidate edges directly and apply tree decoding algorithms for the resulting score table. The two types of parsing algorithms have different advantages (McDonald and Nivre, 2007), with transitionbased parsers often having more problems with error propagation and, as a result, with long-distance dependencies. This paper presents a compromise between transition-based and graph-based parsing, called iterative predicate selection (IPS) – inspired by head selection algorithms for dependency parsing (Zhang et al., 2017) – and show that error propagation, for this algorithm, can be reduced by a combination of multi-task and reinforcement learning. Multi-task learning is motivated by the fact that there are several linguistic formalisms for SDP. Fig. 1 shows the three formali"
P19-1232,Q13-1033,0,0.484418,"ror propagation to the extent the decoding algorithms are global inference algorithm, but this also means that reinforcement learning is not obviously applicable to graph-based parsing. In transition-based parsing, the model is typically taught to follow a gold transition path to obtain a perfect dependency graph during training. This training paradigm has the limitation that the model only ever gets to see states that are on gold transition paths, and error propagation is therefore likely to happen when the parser predicts wrong transitions leading to unseen states (McDonald and Nivre, 2007; Goldberg and Nivre, 2013). There have been several attempts to train transition-based parsers with reinforcement learning: Zhang and Chan (2009) applied SARSA (Baird III, 1999) to an Arc-Standard model, using SARSA updates to fine-tune a model that was pre-trained using a feed-forward neural network. Fried and Klein (2018), more recently, presented experiments with applying policy gradient training to several constituency parsers, including the RNNG transition-based parser (Dyer et al., 2016). In their experiments, however, the models trained with policy gradient did not always perform better than the models trained w"
P19-1232,E06-1011,0,0.136071,"earning of state representations for this parsing algorithm is superior to single-task training; (iii) we improve this model by task-specific policy gradient fine-tuning; (iv) we achieve a new state of the art result across three linguistic formalisms; finally, (v) we show that policy gradient fine-tuning learns an easy-first strategy, which reduces error propagation. 2 Related Work There are generally two kinds of dependency parsing algorithms, namely transition-based parsing algorithms (McDonald and Nivre, 2007; Kiperwasser and Goldberg, 2016; Ballesteros et al., 2015) and graph-based ones (McDonald and Pereira, 2006; Zhang and Clark, 2008; Galley and Manning, 2009; Zhang et al., 2017). In graphbased parsing, a model is trained to score all possible dependency arcs between words, and decoding algorithms are subsequently applied to find the most likely dependency graph. The Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm are often used for finding the most likely dependency trees, whereas the AD3 algorithm (Martins et al., 2011) is used for finding SDP graphs that form DAGs in Peng et al. (2017) and Peng et al. (2018). During training, the loss is computed after decoding, leading the mode"
P19-1232,hajic-etal-2012-announcing,0,0.113012,"Missing"
P19-1232,D17-1206,0,0.0364044,"78.5 78.6 78.9 88.0 86.9 - IPS IPS +ML IPS +RL IPS +ML +RL 91.1 91.2 91.6‡ 92.0‡ 92.4 92.5 92.8‡ 92.8‡ 78.6 78.8 79.2‡ 79.3‡ 88.2 88.3 88.7‡ 88.8‡ Table 3: Labeled parsing performance on in-domain test data. Avg. is the micro-averaged score of three formalisms. ‡ of the +RL models represents that the scores are statistically significant at p < 10−3 with their nonRL counterparts. makes training quite unstable. Therefore we fix the BiLSTM parameters during policy gradient. In our multi-task learning set-up, we apply multi-task learning of the shared stacked BiLSTMs (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) in supervised learning. We use task-specific MLPs for the three different linguistic formalisms: DM, PAS and PSD. We train the shared BiLSTM using multi-task learning beforehand, and then we finetune the task-specific MLPs with policy gradient. We summarize the rest of our hyper-parameters in Table 2. 4 DM PAS PSD Avg. Peng+ 17 Freda3 Peng+ 18 85.3 86.7 89.0 - 76.4 77.1 84.4 - IPS +ML IPS +ML +RL 86.0 87.2‡ 88.2 88.8‡ 77.2 77.7‡ 84.6 85.3‡ Table 4: Labeled parsing performance on out-ofdomain test data. Avg. is the micro-averaged score of three formalisms. ‡ of the +RL models represents that t"
P19-1232,P17-1104,0,0.0567319,"g difficult in transition-based parsing. Iterative refinement approaches have been proposed in the context of sentence generation (Lee et al., 2018). Our proposed model explores multiple transition paths at once and avoids making risky decisions in the initial transitions, in part inspired by such iterative refinement techniques. We also pre-train our model with supervised learning to avoid sampling from irrelevant states at the early stages of policy gradient training. Several models have been presented for DAG parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and G¨ulsen, 2015; Hershcovich et al., 2017). Wang et al. (2018) proposed a similar transition-based parsing model for SDP; they modified the possible transitions of the ArcEager algorithm (Nivre and Scholz, 2004b) to create multi-headed graphs. We are, to the best of our knowledge, first to explore reinforcement learning for DAG parsing. 3 3.1 Model Iterative Predicate Selection We propose a new semantic dependency parsing algorithm based on the head-selection algorithm for syntactic dependency parsing (Zhang et al., 2421 Initial state τ =0 The man went back and spoke to the desk clerk. transitions t0w , · · · , t0w 1 τ =1 The man went"
P19-1232,Q16-1023,0,0.0173093,"bines transition-based and graph-based approaches; (ii) we show that multitask learning of state representations for this parsing algorithm is superior to single-task training; (iii) we improve this model by task-specific policy gradient fine-tuning; (iv) we achieve a new state of the art result across three linguistic formalisms; finally, (v) we show that policy gradient fine-tuning learns an easy-first strategy, which reduces error propagation. 2 Related Work There are generally two kinds of dependency parsing algorithms, namely transition-based parsing algorithms (McDonald and Nivre, 2007; Kiperwasser and Goldberg, 2016; Ballesteros et al., 2015) and graph-based ones (McDonald and Pereira, 2006; Zhang and Clark, 2008; Galley and Manning, 2009; Zhang et al., 2017). In graphbased parsing, a model is trained to score all possible dependency arcs between words, and decoding algorithms are subsequently applied to find the most likely dependency graph. The Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm are often used for finding the most likely dependency trees, whereas the AD3 algorithm (Martins et al., 2011) is used for finding SDP graphs that form DAGs in Peng et al. (2017) and Peng et al. (2"
P19-1232,D18-1149,0,0.024031,"ne a model that was pre-trained using a feed-forward neural network. Fried and Klein (2018), more recently, presented experiments with applying policy gradient training to several constituency parsers, including the RNNG transition-based parser (Dyer et al., 2016). In their experiments, however, the models trained with policy gradient did not always perform better than the models trained with supervised learning. We hypothesize this is due to credit assignment being difficult in transition-based parsing. Iterative refinement approaches have been proposed in the context of sentence generation (Lee et al., 2018). Our proposed model explores multiple transition paths at once and avoids making risky decisions in the initial transitions, in part inspired by such iterative refinement techniques. We also pre-train our model with supervised learning to avoid sampling from irrelevant states at the early stages of policy gradient training. Several models have been presented for DAG parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and G¨ulsen, 2015; Hershcovich et al., 2017). Wang et al. (2018) proposed a similar transition-based parsing model for SDP; they modified the possible transitions of t"
P19-1232,P13-2020,0,0.0196948,"state, depending on the orders of creating the arcs. This is known as the nondeterministic oracle problem (Goldberg and Nivre, 2013). In IPS parsing, some arcs are easy to predict; others are very hard to predict. Long-distance arcs are generally difficult to predict, but they are very important for down-stream applications, including reordering for machine translation (Xu et al., 2009). Since long-distance arcs are harder to predict, and transition-based parsers are prone to error propagation, several easy-first strategies have been introduced, both in supervised (Goldberg and Elhadad, 2010; Ma et al., 2013) and unsupervised dependency parsing (Spitkovsky et al., 2011), to prefer some paths over others in the face of the non-deterministic oracle problem. Easy-first principles have also proven effective with sequence taggers (Tsuruoka and Tsujii, 2005; Martins and Kreutzer, 2017). In this paper, we take an arguably more principled approach, learning a strategy for choosing transition paths over others using reinforcement learning. We observe, however, that the learned strategies exhibit a clear easy-first preference. were extremely rare in our experiments, and can be avoided by simple heuristics d"
P19-1232,D11-1022,0,0.151691,"rithms, namely transition-based parsing algorithms (McDonald and Nivre, 2007; Kiperwasser and Goldberg, 2016; Ballesteros et al., 2015) and graph-based ones (McDonald and Pereira, 2006; Zhang and Clark, 2008; Galley and Manning, 2009; Zhang et al., 2017). In graphbased parsing, a model is trained to score all possible dependency arcs between words, and decoding algorithms are subsequently applied to find the most likely dependency graph. The Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm are often used for finding the most likely dependency trees, whereas the AD3 algorithm (Martins et al., 2011) is used for finding SDP graphs that form DAGs in Peng et al. (2017) and Peng et al. (2018). During training, the loss is computed after decoding, leading the models to reflect a structured loss. The advantage of graphbased algorithms is that there is no real error propagation to the extent the decoding algorithms are global inference algorithm, but this also means that reinforcement learning is not obviously applicable to graph-based parsing. In transition-based parsing, the model is typically taught to follow a gold transition path to obtain a perfect dependency graph during training. This t"
P19-1232,D17-1036,0,0.0195215,"ct, but they are very important for down-stream applications, including reordering for machine translation (Xu et al., 2009). Since long-distance arcs are harder to predict, and transition-based parsers are prone to error propagation, several easy-first strategies have been introduced, both in supervised (Goldberg and Elhadad, 2010; Ma et al., 2013) and unsupervised dependency parsing (Spitkovsky et al., 2011), to prefer some paths over others in the face of the non-deterministic oracle problem. Easy-first principles have also proven effective with sequence taggers (Tsuruoka and Tsujii, 2005; Martins and Kreutzer, 2017). In this paper, we take an arguably more principled approach, learning a strategy for choosing transition paths over others using reinforcement learning. We observe, however, that the learned strategies exhibit a clear easy-first preference. were extremely rare in our experiments, and can be avoided by simple heuristics during decoding. We discuss this issue in the Supplementary Material, §A.1. 2422 a) Encoder and MLP ... sij NULL U.S. ROOT U.S. The ... MLP hj gij fij NULL ROOT softmax . . . sij+2 The U.S. contends that ... sij+3 U.S. contends U.S. that . . . MLP hi ... MLP hi The U.S. conten"
P19-1232,C04-1010,0,0.0425038,"plores multiple transition paths at once and avoids making risky decisions in the initial transitions, in part inspired by such iterative refinement techniques. We also pre-train our model with supervised learning to avoid sampling from irrelevant states at the early stages of policy gradient training. Several models have been presented for DAG parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and G¨ulsen, 2015; Hershcovich et al., 2017). Wang et al. (2018) proposed a similar transition-based parsing model for SDP; they modified the possible transitions of the ArcEager algorithm (Nivre and Scholz, 2004b) to create multi-headed graphs. We are, to the best of our knowledge, first to explore reinforcement learning for DAG parsing. 3 3.1 Model Iterative Predicate Selection We propose a new semantic dependency parsing algorithm based on the head-selection algorithm for syntactic dependency parsing (Zhang et al., 2421 Initial state τ =0 The man went back and spoke to the desk clerk. transitions t0w , · · · , t0w 1 τ =1 The man went back and spoke to the desk clerk. transitions t1w , · · · , t1w The man went back and spoke to the desk clerk. The man went back and spoke to the desk clerk. transitio"
P19-1232,S15-2153,0,0.755879,"Missing"
P19-1232,S14-2008,0,0.177042,", we observe that policy gradient training learns an easy-first strategy. 1 ROOT b) PAS DET_ARG1 VERB_ARG1 ROOT PREP_ARG2 COORD_ARG1 VERB_ARG1 ADJ_ARG1 DET_ARG1 COORD_ARG2 PREP_ARG1 NOUN_ARG1 The man went back and spoke to the desk clerk. c) PSD ROOT CONJ.MEMBER ACT DIR3 ROOT CONJ.MEMBER ADDR RSTR The man went back and spoke to the desk clerk. Figure 1: Semantic dependency parsing arcs of DM, PAS and PSD formalisms. Introduction Dependency parsers assign syntactic structures to sentences in the form of trees. Semantic dependency parsing (SDP), first introduced in the SemEval 2014 shared task (Oepen et al., 2014), in contrast, is the task of assigning semantic structures in the form of directed acyclic graphs to sentences. SDP graphs consist of binary semantic relations, connecting semantic predicates and their arguments. A notable feature of SDP is that words can be the semantic arguments of multiple predicates. For example, in the English sentence: “The man went back and spoke to the desk clerk” – the word “man” is the subject of the two predicates “went back” and “spoke”. SDP formalisms typically express this by two directed arcs, from the two predicates to the argument. This yields a directed acyc"
P19-1232,P17-1186,0,0.215735,"Missing"
P19-1232,P18-1173,0,0.0719606,"Missing"
P19-1232,N18-1135,0,0.203261,"Goldberg, 2016; Ballesteros et al., 2015) and graph-based ones (McDonald and Pereira, 2006; Zhang and Clark, 2008; Galley and Manning, 2009; Zhang et al., 2017). In graphbased parsing, a model is trained to score all possible dependency arcs between words, and decoding algorithms are subsequently applied to find the most likely dependency graph. The Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm are often used for finding the most likely dependency trees, whereas the AD3 algorithm (Martins et al., 2011) is used for finding SDP graphs that form DAGs in Peng et al. (2017) and Peng et al. (2018). During training, the loss is computed after decoding, leading the models to reflect a structured loss. The advantage of graphbased algorithms is that there is no real error propagation to the extent the decoding algorithms are global inference algorithm, but this also means that reinforcement learning is not obviously applicable to graph-based parsing. In transition-based parsing, the model is typically taught to follow a gold transition path to obtain a perfect dependency graph during training. This training paradigm has the limitation that the model only ever gets to see states that are on"
P19-1232,D14-1162,0,0.0844628,"at different time steps. The reward riτ of the i-th word is determined as shown in Table 1. The model gets a positive reward for creating a new correct arc to the i-th word, or if the model for the first time chooses a NULL transition after all arcs to the i-th word are correctly created. The model gets a negative reward when the model creates wrong arcs. When our model chooses NULL transitions for the i-th word before all gold arcs are created, the reward riτ becomes 0. 3.4 Implementation Details This section includes details of our implementation.6 We use 100-dimensional, pre-trained Glove (Pennington et al., 2014) word vectors. Words or lemmas in the training corpora that do not appear in pre-trained embeddings are associated with randomly initialized vector representations. Embeddings of POS tags and other special symbol are also randomly initialized. We apply Adam as our optimizer. Preliminary experiments show that mini-batching led to a degradation in performance. When we apply policy gradient, we pre-train our model using supervised learning. We then use policy gradient for task-specific fine-tuning of our model. We find that updating parameters of BiLSTM and word embeddings during policy gradient"
P19-1232,D08-1059,0,0.0400106,"ions for this parsing algorithm is superior to single-task training; (iii) we improve this model by task-specific policy gradient fine-tuning; (iv) we achieve a new state of the art result across three linguistic formalisms; finally, (v) we show that policy gradient fine-tuning learns an easy-first strategy, which reduces error propagation. 2 Related Work There are generally two kinds of dependency parsing algorithms, namely transition-based parsing algorithms (McDonald and Nivre, 2007; Kiperwasser and Goldberg, 2016; Ballesteros et al., 2015) and graph-based ones (McDonald and Pereira, 2006; Zhang and Clark, 2008; Galley and Manning, 2009; Zhang et al., 2017). In graphbased parsing, a model is trained to score all possible dependency arcs between words, and decoding algorithms are subsequently applied to find the most likely dependency graph. The Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm are often used for finding the most likely dependency trees, whereas the AD3 algorithm (Martins et al., 2011) is used for finding SDP graphs that form DAGs in Peng et al. (2017) and Peng et al. (2018). During training, the loss is computed after decoding, leading the models to reflect a structu"
P19-1232,C08-1095,0,0.0933981,"supervised learning. We hypothesize this is due to credit assignment being difficult in transition-based parsing. Iterative refinement approaches have been proposed in the context of sentence generation (Lee et al., 2018). Our proposed model explores multiple transition paths at once and avoids making risky decisions in the initial transitions, in part inspired by such iterative refinement techniques. We also pre-train our model with supervised learning to avoid sampling from irrelevant states at the early stages of policy gradient training. Several models have been presented for DAG parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and G¨ulsen, 2015; Hershcovich et al., 2017). Wang et al. (2018) proposed a similar transition-based parsing model for SDP; they modified the possible transitions of the ArcEager algorithm (Nivre and Scholz, 2004b) to create multi-headed graphs. We are, to the best of our knowledge, first to explore reinforcement learning for DAG parsing. 3 3.1 Model Iterative Predicate Selection We propose a new semantic dependency parsing algorithm based on the head-selection algorithm for syntactic dependency parsing (Zhang et al., 2421 Initial state τ =0 The man went back an"
P19-1232,P16-2038,1,0.799395,"8 90.4 90.3 91.6 92.7 91.7 - 78.5 78.6 78.9 88.0 86.9 - IPS IPS +ML IPS +RL IPS +ML +RL 91.1 91.2 91.6‡ 92.0‡ 92.4 92.5 92.8‡ 92.8‡ 78.6 78.8 79.2‡ 79.3‡ 88.2 88.3 88.7‡ 88.8‡ Table 3: Labeled parsing performance on in-domain test data. Avg. is the micro-averaged score of three formalisms. ‡ of the +RL models represents that the scores are statistically significant at p < 10−3 with their nonRL counterparts. makes training quite unstable. Therefore we fix the BiLSTM parameters during policy gradient. In our multi-task learning set-up, we apply multi-task learning of the shared stacked BiLSTMs (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) in supervised learning. We use task-specific MLPs for the three different linguistic formalisms: DM, PAS and PSD. We train the shared BiLSTM using multi-task learning beforehand, and then we finetune the task-specific MLPs with policy gradient. We summarize the rest of our hyper-parameters in Table 2. 4 DM PAS PSD Avg. Peng+ 17 Freda3 Peng+ 18 85.3 86.7 89.0 - 76.4 77.1 84.4 - IPS +ML IPS +ML +RL 86.0 87.2‡ 88.2 88.8‡ 77.2 77.7‡ 84.6 85.3‡ Table 4: Labeled parsing performance on out-ofdomain test data. Avg. is the micro-averaged score of three formalisms. ‡ of the +RL"
P19-1232,D11-1118,0,0.0687999,"Missing"
P19-1232,P15-3004,0,0.0395593,"Missing"
P19-1232,H05-1059,0,0.0828573,"enerally difficult to predict, but they are very important for down-stream applications, including reordering for machine translation (Xu et al., 2009). Since long-distance arcs are harder to predict, and transition-based parsers are prone to error propagation, several easy-first strategies have been introduced, both in supervised (Goldberg and Elhadad, 2010; Ma et al., 2013) and unsupervised dependency parsing (Spitkovsky et al., 2011), to prefer some paths over others in the face of the non-deterministic oracle problem. Easy-first principles have also proven effective with sequence taggers (Tsuruoka and Tsujii, 2005; Martins and Kreutzer, 2017). In this paper, we take an arguably more principled approach, learning a strategy for choosing transition paths over others using reinforcement learning. We observe, however, that the learned strategies exhibit a clear easy-first preference. were extremely rare in our experiments, and can be avoided by simple heuristics during decoding. We discuss this issue in the Supplementary Material, §A.1. 2422 a) Encoder and MLP ... sij NULL U.S. ROOT U.S. The ... MLP hj gij fij NULL ROOT softmax . . . sij+2 The U.S. contends that ... sij+3 U.S. contends U.S. that . . . MLP"
P19-1232,N09-1028,0,0.0117074,"o to 1. Fig. 2 shows the transitions of the IPS algorithm during the DM parsing of the sentence “The man went back and spoke to the desk clerk.” In this case, there are several paths from the initial state to the final parsing state, depending on the orders of creating the arcs. This is known as the nondeterministic oracle problem (Goldberg and Nivre, 2013). In IPS parsing, some arcs are easy to predict; others are very hard to predict. Long-distance arcs are generally difficult to predict, but they are very important for down-stream applications, including reordering for machine translation (Xu et al., 2009). Since long-distance arcs are harder to predict, and transition-based parsers are prone to error propagation, several easy-first strategies have been introduced, both in supervised (Goldberg and Elhadad, 2010; Ma et al., 2013) and unsupervised dependency parsing (Spitkovsky et al., 2011), to prefer some paths over others in the face of the non-deterministic oracle problem. Easy-first principles have also proven effective with sequence taggers (Tsuruoka and Tsujii, 2005; Martins and Kreutzer, 2017). In this paper, we take an arguably more principled approach, learning a strategy for choosing t"
P19-1232,W09-3838,0,0.035092,"t learning is not obviously applicable to graph-based parsing. In transition-based parsing, the model is typically taught to follow a gold transition path to obtain a perfect dependency graph during training. This training paradigm has the limitation that the model only ever gets to see states that are on gold transition paths, and error propagation is therefore likely to happen when the parser predicts wrong transitions leading to unseen states (McDonald and Nivre, 2007; Goldberg and Nivre, 2013). There have been several attempts to train transition-based parsers with reinforcement learning: Zhang and Chan (2009) applied SARSA (Baird III, 1999) to an Arc-Standard model, using SARSA updates to fine-tune a model that was pre-trained using a feed-forward neural network. Fried and Klein (2018), more recently, presented experiments with applying policy gradient training to several constituency parsers, including the RNNG transition-based parser (Dyer et al., 2016). In their experiments, however, the models trained with policy gradient did not always perform better than the models trained with supervised learning. We hypothesize this is due to credit assignment being difficult in transition-based parsing. I"
P19-1232,E17-1063,0,0.286587,"and gradually build up dependency graphs on the side. Graph-based parsers, in contrast, score all candidate edges directly and apply tree decoding algorithms for the resulting score table. The two types of parsing algorithms have different advantages (McDonald and Nivre, 2007), with transitionbased parsers often having more problems with error propagation and, as a result, with long-distance dependencies. This paper presents a compromise between transition-based and graph-based parsing, called iterative predicate selection (IPS) – inspired by head selection algorithms for dependency parsing (Zhang et al., 2017) – and show that error propagation, for this algorithm, can be reduced by a combination of multi-task and reinforcement learning. Multi-task learning is motivated by the fact that there are several linguistic formalisms for SDP. Fig. 1 shows the three formalisms used in the shared task. The DELPH-IN MRS (DM) formalism derives from DeepBank (Flickinger et al., 2012) and minimal recursion semantics (Copestake et al., 2005). Predicate-Argument Structure (PAS) is a formalism based on the Enju HPSG parser (Miyao et al., 2004) and is generally considered slightly more syntactic of nature than the 24"
P19-1232,S14-2012,0,\N,Missing
P19-1232,P18-2075,0,\N,Missing
P19-4007,Q16-1031,0,0.0204922,"offer an elegant and language-pair independent way to represent content across different languages. They enable us to reason about word meaning in multilingual contexts and serve as an integral source of knowledge for multilingual applications such as machine translation (Artetxe et al., 2018d; Qi et al., 2018; Lample et al., 2018b) or multilingual search and question answering (Vuli´c and Moens, 2015). In addition, they are a key facilitator of cross-lingual transfer and joint multilingual training, offering support to NLP applications in a large spectrum of languages (Søgaard et al., 2015; Ammar et al., 2016a). While NLP is increasingly more embedded into a variety of products related to, e.g., translation, conversational or search tasks, resources such as annotated training data are still lacking or insufficient to induce satisfying models for many resource-poor languages. There are often no trained linguistic annotators for these languages, and markets may be too small or premature to invest in such training. This is a major challenge, but cross-lingual modelling and transfer can help by exploiting observable correlations between major languages and low-resource languages. Recent work has alrea"
P19-4007,D18-1024,0,0.0563881,"Missing"
P19-4007,Q17-1010,0,0.178231,"Missing"
P19-4007,D16-1136,0,0.0662203,"Missing"
P19-4007,P17-2037,1,0.876661,"Missing"
P19-4007,D12-1001,0,0.0762697,"Missing"
P19-4007,S17-2002,0,0.078474,"Missing"
P19-4007,N18-2029,1,0.901703,"Missing"
P19-4007,D18-1023,0,0.0295173,"Missing"
P19-4007,P18-1004,1,0.898441,"Missing"
P19-4007,P19-1070,1,0.876625,"Missing"
P19-4007,Q19-1007,0,0.0295517,"Missing"
P19-4007,D18-1330,0,0.069032,"Missing"
P19-4007,K18-1021,1,0.897312,"Missing"
P19-4007,N19-1188,1,0.889169,"Missing"
P19-4007,E17-1102,1,0.891004,"Missing"
P19-4007,D17-1269,0,0.060333,"Missing"
P19-4007,D18-1043,0,0.0398969,"Missing"
P19-4007,D11-1006,0,0.103244,"Missing"
P19-4007,D15-1127,0,0.0637398,"Missing"
P19-4007,N19-1386,0,0.0455551,"Missing"
P19-4007,P18-2035,0,0.0666436,"Missing"
P19-4007,Q17-1022,1,0.905704,"Missing"
P19-4007,D18-1063,0,0.0557082,"Missing"
P19-4007,P15-1165,1,0.881513,"Missing"
P19-4007,D18-1047,0,0.013888,"chosen hyper-parameters, etc. In this part, we will analyze the current problems with robustness and stability of weaklysupervised and unsupervised alignment methods in relation to all these factors, and introduce latest solutions to alleviate these problems. We will provide advice on how to approach weakly-supervised and unsupervised training based on a series of empirical observations available in recent literature (Søgaard et al., 2018; Hartmann et al., 2018). We will also discuss the (im)possibility of learning nonlinear mappings using either non-linear generators or locally linear maps (Nakashole, 2018). We will conclude by providing publicly available software packages and implementations, as well as available training datasets and evaluation protocols and systems. We will also list current state-of-the-art results on standard evaluation datasets, and sketch future research paths. • A general framework for mapping-based approaches. 3 • Importance of seed bilingual lexicons. • Learning alignment with weak supervision: small seed lexicons, shared words, numerals. Part III: Adversarial Seed Induction (30 minutes) • Fully unsupervised models using adversarial training; MUSE and related approach"
P19-4007,P18-1072,1,0.89956,"Missing"
P19-4007,P18-2036,0,0.0408295,"Missing"
P19-4007,D17-1264,0,0.0588289,"Missing"
P19-4007,N16-1072,0,0.071611,"Missing"
P19-4007,N10-1135,0,0.0513539,"Missing"
P19-4007,P16-1157,0,0.0233929,"still lacking or insufficient to induce satisfying models for many resource-poor languages. There are often no trained linguistic annotators for these languages, and markets may be too small or premature to invest in such training. This is a major challenge, but cross-lingual modelling and transfer can help by exploiting observable correlations between major languages and low-resource languages. Recent work has already verified the usefulness of cross-lingual word representations in a wide variety of downstream tasks, and has provided extensive model classifications in several survey papers (Upadhyay et al., 2016; Ruder et al., 2018b). They cluster supervised cross-lingual word representation models according to the bilingual supervision required to induce such shared cross-lingual semantic spaces, covering models based on word alignments and readily available bilingual dictionaries (Mikolov et al., 2013; Smith et al., 2017), sentence-aligned parallel data (Gouws et al., 2015), document-aligned data (Søgaard et al., 2015; Vuli´c 1 Learning unsupervised cross-lingual models has indeed taken the field by storm: there are 10+ papers on this very topic published in EMNLP 2018 proceedings alone, with even"
P19-4007,D18-1026,1,0.906688,"Missing"
P19-4007,N18-2084,0,0.0307978,"ource-poor settings where bilingual supervision cannot be guaranteed; 2) critical examinations of different training conditions and requirements under which unsupervised algorithms can and cannot work effectively; 3) more robust methods for distant language pairs that Cross-lingual word representations offer an elegant and language-pair independent way to represent content across different languages. They enable us to reason about word meaning in multilingual contexts and serve as an integral source of knowledge for multilingual applications such as machine translation (Artetxe et al., 2018d; Qi et al., 2018; Lample et al., 2018b) or multilingual search and question answering (Vuli´c and Moens, 2015). In addition, they are a key facilitator of cross-lingual transfer and joint multilingual training, offering support to NLP applications in a large spectrum of languages (Søgaard et al., 2015; Ammar et al., 2016a). While NLP is increasingly more embedded into a variety of products related to, e.g., translation, conversational or search tasks, resources such as annotated training data are still lacking or insufficient to induce satisfying models for many resource-poor languages. There are often no tra"
P19-4007,D18-1270,0,0.0630194,"Missing"
P19-4007,P18-1084,1,0.89476,"Missing"
P19-4007,N18-1056,0,0.0443504,"Missing"
P19-4007,D18-1042,1,0.916875,"ficient to induce satisfying models for many resource-poor languages. There are often no trained linguistic annotators for these languages, and markets may be too small or premature to invest in such training. This is a major challenge, but cross-lingual modelling and transfer can help by exploiting observable correlations between major languages and low-resource languages. Recent work has already verified the usefulness of cross-lingual word representations in a wide variety of downstream tasks, and has provided extensive model classifications in several survey papers (Upadhyay et al., 2016; Ruder et al., 2018b). They cluster supervised cross-lingual word representation models according to the bilingual supervision required to induce such shared cross-lingual semantic spaces, covering models based on word alignments and readily available bilingual dictionaries (Mikolov et al., 2013; Smith et al., 2017), sentence-aligned parallel data (Gouws et al., 2015), document-aligned data (Søgaard et al., 2015; Vuli´c 1 Learning unsupervised cross-lingual models has indeed taken the field by storm: there are 10+ papers on this very topic published in EMNLP 2018 proceedings alone, with even more papers availabl"
P19-4007,P16-1024,1,0.90887,"Missing"
P19-4007,D13-1168,1,0.869883,"Missing"
P19-4007,N19-1162,0,0.0610116,"Missing"
P19-4007,D17-1270,1,0.903119,"Missing"
P19-4007,D18-1268,0,0.0445512,"Missing"
P19-4007,P18-1005,0,0.0613929,"Missing"
P19-4007,C16-1300,0,0.060658,"Missing"
P19-4007,P17-1179,0,0.0632695,"Missing"
P19-4007,D17-1207,0,0.0552572,"Missing"
P19-4007,P19-1307,0,0.0367463,"Missing"
P19-4007,N16-1156,0,0.0701558,"Missing"
P19-4007,D18-1022,0,0.0481647,"Missing"
Q16-1022,A00-1031,0,0.555773,"observe a major advantage in using reverse-mode alignment for POS projection (4-5 accuracy points absolute).10 In addition, we use the IBM1 aligner efmaral11 by ¨ Ostling (2015). The intuition behind using IBM1 is that IBM2 introduces a bias toward more closely related languages, and we confirm this intuition through our experiments. We modify both aligners so that they output the alignment probability for each aligned token pair. Tagging and parsing The source-sides of the two multi-parallel corpora, EBC and WTC, are POStagged by taggers trained on the respective source languages, using TnT (Brants, 2000). We parse the corpora using TurboParser (Martins et al., 2013). The parser is used in simple arc-factored mode with pruning.12 We alter it to output per-sentence arc 7 http://universaldependencies.org/ format.html 8 https://github.com/coastalcph/ ud-conversion-tools. 9 Parameters used: utf, bisent, cautious, realign. 10 Parameters used: d, o, v, r. 11 Also reverse mode, with default settings, see https:// github.com/robertostling/efmaral. 12 Parameters used: basic. weight matrices.13 4 Experiments Outline For each sentence in a target language corpus, we retrieve the aligned sentences in the"
Q16-1022,P11-1061,0,0.159967,"ning data. In our dependency graph projection, we normalize the weights per sentence. For future development, we note that corpus-level normalization might achieve the same balancing effect while still preserving possibly important language-specific signals regarding structural disambiguations. EBC and WTC constitute a (hopefully small) subset of the publicly available multilingual parallel corpora. The outdated EBC texts can be replaced by newer ones, and the EBC itself replaced or aug310 Related work POS tagging While projection annotation of POS labels goes back to Yarowsky’s seminal work, Das and Petrov (2011) recently renewed interest in this problem. Das and Petrov (2011) go beyond our approach to POS annotation by combining annotation projection and unsupervised learning techniques, but they restrict themselves to Indo-European languages and a coarser tagset. Li et al. (2012) introduce an approach that leverages potentially noisy, but sizeable POS tag dictionaries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c e"
Q16-1022,P13-1057,0,0.0330031,"newer ones, and the EBC itself replaced or aug310 Related work POS tagging While projection annotation of POS labels goes back to Yarowsky’s seminal work, Das and Petrov (2011) recently renewed interest in this problem. Das and Petrov (2011) go beyond our approach to POS annotation by combining annotation projection and unsupervised learning techniques, but they restrict themselves to Indo-European languages and a coarser tagset. Li et al. (2012) introduce an approach that leverages potentially noisy, but sizeable POS tag dictionaries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up"
Q16-1022,P16-2091,1,0.868444,"Missing"
Q16-1022,D12-1127,0,0.0917574,"Missing"
Q16-1022,C14-1075,0,0.0462189,"dea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19 http://opus.lingfil.uu.se/ soft projection of reliable dependencies by Li et al. (2014), and the work of Ma and Xia (2014), who make use of the source-side distributions through a training objective function. Tiedemann and Agi´c (2016) provide a more detailed overview of model transfer and annotation projection, while introducing a competitive machine translation-based approach to synthesizing dependency treebanks. In their work, we note the IBM4 word alignments favor more closely related languages, and that building machine translation systems requires parallel data in quantities that far surpass EBC and WTC combined. The best results reported to date were presented by Rasooli"
Q16-1022,P14-1126,0,0.169202,"r Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19 http://opus.lingfil.uu.se/ soft projection of reliable dependencies by Li et al. (2014), and the work of Ma and Xia (2014), who make use of the source-side distributions through a training objective function. Tiedemann and Agi´c (2016) provide a more detailed overview of model transfer and annotation projection, while introducing a competitive machine translation-based approach to synthesizing dependency treebanks. In their work, we note the IBM4 word alignments favor more closely related languages, and that building machine translation systems requires parallel data in quantities that far surpass EBC and WTC combined. The best results reported to date were presented by Rasooli and Collins (2015). They use the in"
Q16-1022,P13-2109,0,0.0643836,"Missing"
Q16-1022,P05-1012,0,0.0672856,"Missing"
Q16-1022,D11-1006,0,0.675369,"es in the arc projection, but we use unit votes in POS voting. The opposite yields the best IBM2 scores: binarizing the alignment scores in dependency projection, while weight-voting the POS tags. We also evaluated a number of different normalization techniques in projection, only to arrive at standardization and softmax as by far the best choices. Baselines and upper bounds We compare our systems to three competitive baselines, as well as three informed upper bounds or oracles. First, we list our baselines. D ELEX -MS: This is the multi-source direct delexicalized parser transfer baseline of McDonald et al. (2011).15 DCA-P ROJ: This is the direct correspondence assumption (DCA)-based approach to projection, i.e., the de facto standard for projecting dependencies. First introduced by Hwa et al. (2005), it was recently elucidated by Tiedemann (2014), whose implementation we follow here. In contrast to our approach, 15 Referred to as multi-dir in the original paper. DCA projects trees on a source-target sentence pair basis, relying on heuristics and spurious nodes or edges to maintain the tree structure. In the setup, we basically plug DCA into our projection-voting pipeline instead of our own method. R E"
Q16-1022,P13-2017,0,0.0731737,"Missing"
Q16-1022,P15-2034,0,0.0166339,"there are more candidates, we select one through POS ranking.8 Alignment We sentence- and word-align all language pairs in both our multi-parallel corpora. We use hunalign (Varga et al., 2005) to perform conservative sentence alignment.9 The selected sentence pairs then enter word alignment. Here, we use two different aligners. The first one is IBM2 fastalign by Dyer et al. (2013), where we adopt the setup of Agi´c et al. (2015) who observe a major advantage in using reverse-mode alignment for POS projection (4-5 accuracy points absolute).10 In addition, we use the IBM1 aligner efmaral11 by ¨ Ostling (2015). The intuition behind using IBM1 is that IBM2 introduces a bias toward more closely related languages, and we confirm this intuition through our experiments. We modify both aligners so that they output the alignment probability for each aligned token pair. Tagging and parsing The source-sides of the two multi-parallel corpora, EBC and WTC, are POStagged by taggers trained on the respective source languages, using TnT (Brants, 2000). We parse the corpora using TurboParser (Martins et al., 2013). The parser is used in simple arc-factored mode with pruning.12 We alter it to output per-sentence a"
Q16-1022,petrov-etal-2012-universal,0,0.0990105,"S tagging Below, we present results with POS taggers based on annotation projection with both IBM1 and IBM2; cf. Table 3. We train TnT with default settings on the projected annotations. Note that we use the resulting POS taggers in our dependency parsing experiments in order not to have our parsers assume the existence of POS-annotated corpora. For a more extensive assessment, we refer to the work by Agi´c et al. (2015) who report baseline and upper bounds. In contrast to their work, we consider two different alignment models and use the UD POS tagset (17 tags), in contrast to the 12 tags of Petrov et al. (2012). This makes our POS tagging problem slightly more challenging, but our parsing models potentially benefit from the extended tagset.14 Dependency parsing We use arc-factored TurboParser for all parsing models, applying the same setup as in preprocessing. There are three sets of models: our systems, baselines, and upper bounds. 13 Our fork of TurboParser is available from https:// github.com/andersjo/TurboParser. 14 For example, the AUX vs. VERB distinction from UD POS does not exist the tagset of Petrov et al. (2012), and neither does NOUN vs. PROPN (proper noun). 307 Our systems are trained o"
Q16-1022,D15-1039,0,0.358424,"(73m), Hausa (50m), and Kurdish (30m). Cross-lingual transfer learning—or simply cross-lingual learning—refers to work on using annotated resources in other (source) languages to induce models for such low-resource (target) languages. Even simple cross-lingual learning techniques outperform unsupervised grammar induction by a large margin. Most work in cross-lingual learning, however, makes assumptions about the availability of linguistic resources that do not hold for the majority of low-resource languages. The best cross-lingual dependency parsing results reported to date were presented by Rasooli and Collins (2015). They use the intersection of languages covered in the Google dependency treebanks project and those contained in the Europarl corpus. Consequently, they only consider closely related Indo-European languages for which high-quality tokenization can be obtained with simple heuristics. In other words, we argue that recent approaches to cross-lingual POS tagging and dependency parsing are biased toward Indo-European languages, in particular the Germanic and Romance families. The bias is not hard to explain: treebanks, as well as large volumes of parallel data, are readily available for many Germa"
Q16-1022,P15-2040,0,0.143668,"Missing"
Q16-1022,N06-2033,0,0.0734653,"entation we follow here. In contrast to our approach, 15 Referred to as multi-dir in the original paper. DCA projects trees on a source-target sentence pair basis, relying on heuristics and spurious nodes or edges to maintain the tree structure. In the setup, we basically plug DCA into our projection-voting pipeline instead of our own method. R EPARSE: For this baseline, we parse a target sentence using multiple single-source delexicalized parsers. Then, we collect the output trees in a graph, unit-voting the individual edge weights, and finally using DMST to compute the best dependency tree (Sagae and Lavie, 2006). Now, we explain the three upper bounds: D ELEX -SB: This result is using the best singlesource delexicalized system for a given target language following McDonald et al. (2013). We parse a target with multiple single-source delexicalized parsers, and select the best-performing one. S ELF -T RAIN: For this result we parse the targetlanguage EBC and WTC data, train parsers on the output predictions, and evaluate the resulting parsers on the evaluation data. Note this result is available only for the source languages. Also, note that while we refer to this as self-training, we do not concatenat"
Q16-1022,spreyer-etal-2010-training,0,0.0207293,"naries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up the idea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19"
Q16-1022,P11-2120,1,0.889189,"rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up the idea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19 http://opus.lingfil.uu.se/ soft projection"
Q16-1022,W14-1614,1,0.84878,"Missing"
Q16-1022,tiedemann-2012-parallel,0,0.0365742,"Missing"
Q16-1022,C14-1175,0,0.211283,"ncy such that (ut , vt ) becomes a dependency edge in the target sentence, making the a dependent of word. Obviously, dependency annotation projection is more challenging than projecting POS, as there is a structural constraint: the projected edges must form a dependency tree on the target side. Hwa et al. (2005) were the first to consider this problem, applying heuristics to ensure well-formed trees on the target side. The heuristics were not perfect, as they have been shown to result in excessive non-projectivity and the introduction of spurious relations and tokens (Tiedemann et al., 2014; Tiedemann, 2014). These design choices all lead to di1 https://bitbucket.org/lowlands/release Figure 1: An outline of dependency annotation projection, voting, and decoding in our method, using two sources i (German) and j (Croatian) and a target t (English). Part 1 represents the multi-parallel corpus preprocessing, while parts 2 and 3 relate to our projection method. The graphs are represented as adjacency matrices with column indices encoding dependency heads. We highlight how the weight of target edge (ut = was, vt = beginning) is computed from the two contributing sources. minished parsing quality. We in"
Q16-1022,H01-1035,0,0.430375,"ts weight matrices from multiple sources, rather than dependency trees or individual dependencies from a single source. (iii) We show that our approach performs significantly better than commonly used heuristics for annotation projection, as well as than delexicalized transfer baselines. Moreover, in comparison to these systems, our approach performs particularly well on truly low-resource non-Indo-European languages. 302 All code and data are made freely available for general use.1 2 Weighted annotation projection Motivation Our approach is based on the general idea of annotation projection (Yarowsky et al., 2001) using parallel sentences. The goal is to augment an unannotated target sentence with syntactic annotations projected from one or more source sentences through word alignments. The principle is illustrated in Figure 1, where the source languages are German and Croatian, and the target is English. The simplest case is projecting POS labels, which are observed in the source sentences but unknown in the target language. In order to induce the grammatical category of the target word beginning, we project POS from the aligned words Anfang and poˇcetku, both of which are correctly annotated as N OUN"
Q16-1022,I08-3008,0,0.081872,"ges and a coarser tagset. Li et al. (2012) introduce an approach that leverages potentially noisy, but sizeable POS tag dictionaries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up the idea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency"
Q16-1022,P15-2044,1,\N,Missing
Q16-1022,N13-1073,0,\N,Missing
R19-1013,P07-2045,0,0.0131077,"equent words such as et in French and its English translation and. We start by computing the frequency vectors − −→ v→ in and vout , containing the frequency for every word wi in the input and MT output sentence, respectively: → − v = hf (w1 ), . . . , f (wn )i Sentence Pairs https://en.wikipedia.org/wiki/Wikipedia:Database 105 Preprocessing The purpose of our experiments is to learn how to efficiently translate low-resource languages. For that purpose, we do not use any advanced preprocessing for any of our translation tasks except tokenization where we use the script from the Moses toolkit (Koehn et al., 2007). We also set the maximum sentence length to 70 tokens and the vocabulary size to 50k. 5.2 BLEU Luong et al. (2015) Luong et al. (2017) (dropout=0.2) Baseline (dropout=0.2) + Length + Punct + Frequency + Combined 23.30 25.10 26.43 26.77 26.71 26.12 27.13 Training Details Table 3: Baseline vs. our proposed models on the English–Vietnamese translation task, using the same dataset as Luong et al. (2015b). The results in bold represent statistically significant results compared to the baseline according to MultEval (Clark et al., 2011). We use the attention-based model described in Luong et al. (2"
R19-1013,2012.eamt-1.60,0,0.0130075,"Germanic, and Austro-Asian. We further vary the size of the training data to test how our regularization methods affect the quality of the MT output in different setups. Table 2 contains the size of the training, development and test set for every language pair. Note that the training sets vary considerably in size, from 17k sentence pairs for Slovene to almost 233k for French. The data is from the International Workshop on Spoken Language Translation (IWSLT), except for Russian, Slovene and Vietnamese which are from IWSLT 2015, the data for the remaining translation tasks is from IWSLT 2017 (Cettolo et al., 2012). (4) Essentially, this regularizer penalizes translations if their word frequency distributions diverge too strongly from those of the source sentence. (4) I N It was a big lady who wore a fur around her neck R EF C’était une dame forte qui portait une fourrure autour du cou 2 Development O UT C’était une femme forte portant une fourrure autour du cou To calculate the word frequencies f (w) for each language, we use the Wikipedia database2 as an external resource. Table 1 contains the size of the datasets (in number of words) used to estimate these. We note that there is considerably more dat"
R19-1013,W14-4012,0,0.167281,"Missing"
R19-1013,W17-3204,0,0.0222016,"eural machine translation has achieved stateof-the-art performance for various language pairs (Luong et al., 2015a; Sennrich et al., 2015; Luong and Manning, 2016; Neubig, 2015; Vaswani et al., 2017), especially when trained on large volumes of parallel data, i.e., millions of parallel sentences (also called bi-sentences), humanly translated or validated. Such amounts of training data, however, are difficult to obtain for low-resource languages such as Slovene or Vietnamese, and in their absence, neural machine translation is known to come with diminishing returns, suffering from overfitting (Koehn and Knowles, 2017). In order to avoid overfitting, NMT models are often trained with L1 or L2 regularization, as well as other forms of regularization such as momentum training or dropout (Srivastava et al., 2014; Wang et al., 2015; Miceli Barone et al., 2017). However, these regularization methods are very general and do not carry any language specific information. On the other hand, it has been shown that transfer learning approaches using out of domain data, 3.2 Regularized NMT To apply our new regularizers, we add each regularizer to the loss function during the training of the NMT model (Luong et al., 2015"
R19-1013,P11-2031,0,0.445887,"t tokenization where we use the script from the Moses toolkit (Koehn et al., 2007). We also set the maximum sentence length to 70 tokens and the vocabulary size to 50k. 5.2 BLEU Luong et al. (2015) Luong et al. (2017) (dropout=0.2) Baseline (dropout=0.2) + Length + Punct + Frequency + Combined 23.30 25.10 26.43 26.77 26.71 26.12 27.13 Training Details Table 3: Baseline vs. our proposed models on the English–Vietnamese translation task, using the same dataset as Luong et al. (2015b). The results in bold represent statistically significant results compared to the baseline according to MultEval (Clark et al., 2011). We use the attention-based model described in Luong et al. (2015b). Our model is composed of two LSTM layers each of which has 512-dimensional units and embeddings; we also use a mini-batch size of 128. Adding an attention mechanism in neural machine translation helps to encode relevant parts of the source sentence when learning the model. We propose to add additional regularizers on top of the attention-based model at each translation step. We have noticed that the convergence highly depends on the language pairs involved. While our baseline model is identical to the NMT model described by"
R19-1013,P16-1100,0,0.0440376,"Missing"
R19-1013,P15-1166,0,0.0657432,"Missing"
R19-1013,D15-1166,0,0.743566,"Missing"
R19-1013,N16-1101,0,0.0210209,"ource Neural Machine Translation Meriem Beloucif1 , Ana Valeria Gonzalez2 , Marcel Bollmann2 , and Anders Søgaard2 1 2 Language Technology Group, Universität Hamburg, Hamburg, Germany Dpt. of Computer Science, University of Copenhagen, Copenhagen, Denmark Abstract where large volumes of training data are not available, neural machine translation has come with diminishing returns (Koehn and Knowles, 2017). The general-purpose regularizers do not provide enough inductive bias to enable generalization, it seems. This is an area of active research, and other work has explored multi-task learning (Firat et al., 2016; Dong et al., 2015), zero-shot learning (Johnson et al., 2016), and unsupervised machine translation (Gehring et al., 2017) to resolve the data bottleneck. In this paper, we consider a fully complementary, but much simpler alternative: naive, linguistically motivated regularizers that penalize the output sentences of translation models departing heavily from simple characteristics of the input sentences. The proposed regularizers are based on three surface properties of sentences: their length (measured as number of tokens), their amount of punctuation (measured as number of punctuation signs"
R19-1013,W18-1811,0,0.0204395,"he input sentence, which is obviously incorrect. Our punctuation regularizer further penalizes examples like this one. (1) 4.3 Here, l0 and l1 represent the input sentence and the MT output sentence lengths, respectively, as measured by the number of words (not to be confused with L1 and L2 regularization methods). Note that this regularizer is different from the word penalty feature in phrase-based machine translation (Zens and Ney, 2004), which only penalizes the target sentence length. The relative difference between the input and the MT output sentence lengths is also used as a feature in Marie and Fujita (2018). 4.2 Our last regularizer is based on the distribution of word frequencies between the source and the target sentences. Generally speaking, if the source sentence contains an uncommon word, we assume that its translation in the target language is also uncommon. The intuition behind this regularizer is that if the source sentence contains one uncommon word and three common words, then its accurate translation should contain similar word frequencies. The example below is extracted from the English–French translation task: Punctuation-Based Regularizer (3) I N But now there is a bold new solutio"
R19-1013,D17-1156,0,0.0825041,"requent word (for dog) and at least one relatively infrequent word (for Chinook). This assumption is the main motivation for our work. Contributions Our contribution is three-fold: (a) We propose three relatively naïve, yet linguistically motivated, regularization methods for machine translation with low-resource languages. 102 Proceedings of Recent Advances in Natural Language Processing, pages 102–111, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_013 such as the European Parliament data1 , to regularize the learning helps improve the translation quality (Miceli Barone et al., 2017). This approach produces good results, but it is not applicable in low-resource settings because it requires large amounts of data in the language of interest. To the best of our knowledge, our work is the first to introduce naive, linguistically motivated regularization methods such as sentence length, punctuation and word frequency. Two of the regularizers are derived directly from the input, without relying on any additional linguistic resources. This makes them adequate for low-resource settings, where the availability of linguistic resources can generally not assumed. Our third regularize"
R19-1013,Q17-1024,0,0.0620467,"Missing"
R19-1013,P02-1040,0,0.104891,"nvergence. In Table 4, we report the number of epochs it took to converge by translation task when translating to/from English. We note that except for Czech and Slovene, which converged the quickest, most of the translation tasks took between 15k and 20k steps to converge. 6 System Translation Task #Steps Lang→English Czech French German Russian Slovene Vietnamese 12K 20K 20K 22K 10K 15K English→Lang Czech French German Russian Slovene Vietnamese 12K 22K 20K 18K 11K 15K Table 4: Number of steps it took until the models stopped improving for all the translation tasks. gram based metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), as well as the error-rate based metric TER (Snover et al., 2006). The evaluation metric BLEU (Papineni et al., 2002) is based on n-gram matching between the input and the output, whereas the error-rate based metric TER (Snover et al., 2006) measures how many edits are needed so that the machine translation resembles the man-made reference. Evaluation In order to show that the naive regularizers which we propose in this paper significantly boost the translation quality, we test the machine translation output using the toolkit MultEval defined in Clark et"
R19-1013,W14-4009,0,0.0586452,"Missing"
R19-1013,2006.amta-papers.25,0,0.0520761,"ting to/from English. We note that except for Czech and Slovene, which converged the quickest, most of the translation tasks took between 15k and 20k steps to converge. 6 System Translation Task #Steps Lang→English Czech French German Russian Slovene Vietnamese 12K 20K 20K 22K 10K 15K English→Lang Czech French German Russian Slovene Vietnamese 12K 22K 20K 18K 11K 15K Table 4: Number of steps it took until the models stopped improving for all the translation tasks. gram based metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), as well as the error-rate based metric TER (Snover et al., 2006). The evaluation metric BLEU (Papineni et al., 2002) is based on n-gram matching between the input and the output, whereas the error-rate based metric TER (Snover et al., 2006) measures how many edits are needed so that the machine translation resembles the man-made reference. Evaluation In order to show that the naive regularizers which we propose in this paper significantly boost the translation quality, we test the machine translation output using the toolkit MultEval defined in Clark et al. (2011). In this paper, we report the results using three commonly used metrics: the n6.1 Results Tab"
R19-1013,D15-1186,0,0.0256574,"ined on large volumes of parallel data, i.e., millions of parallel sentences (also called bi-sentences), humanly translated or validated. Such amounts of training data, however, are difficult to obtain for low-resource languages such as Slovene or Vietnamese, and in their absence, neural machine translation is known to come with diminishing returns, suffering from overfitting (Koehn and Knowles, 2017). In order to avoid overfitting, NMT models are often trained with L1 or L2 regularization, as well as other forms of regularization such as momentum training or dropout (Srivastava et al., 2014; Wang et al., 2015; Miceli Barone et al., 2017). However, these regularization methods are very general and do not carry any language specific information. On the other hand, it has been shown that transfer learning approaches using out of domain data, 3.2 Regularized NMT To apply our new regularizers, we add each regularizer to the loss function during the training of the NMT model (Luong et al., 2015a; Luong and Manning, 2016; Luong et al., 2017). Since we aim to minimize the cross-entropy loss, this means that we favor training instances which have a low penalty from the regularizers (e.g., a small length di"
R19-1013,N04-1033,0,0.0426495,"he desired English reference. However, during an early training step, the NMT model translates the input to a sequence containing six times the number of punctuation marks in the input sentence, which is obviously incorrect. Our punctuation regularizer further penalizes examples like this one. (1) 4.3 Here, l0 and l1 represent the input sentence and the MT output sentence lengths, respectively, as measured by the number of words (not to be confused with L1 and L2 regularization methods). Note that this regularizer is different from the word penalty feature in phrase-based machine translation (Zens and Ney, 2004), which only penalizes the target sentence length. The relative difference between the input and the MT output sentence lengths is also used as a feature in Marie and Fujita (2018). 4.2 Our last regularizer is based on the distribution of word frequencies between the source and the target sentences. Generally speaking, if the source sentence contains an uncommon word, we assume that its translation in the target language is also uncommon. The intuition behind this regularizer is that if the source sentence contains one uncommon word and three common words, then its accurate translation should"
S12-1054,S12-1046,0,0.0797668,"Missing"
S14-1001,P05-3014,0,0.0381676,"se (MFS) of a word has been proven to be a strong baseline. Following this, our MFS baseline simply predicts the supersense of the most frequent WordNet sense for a tuple of a word and a part of speech. We use the part of speech predicted by the L APOS tagger (Tsuruoka et al., 2011). Any word not in WordNet is labeled as noun.person, which is the most frequent sense overall in the training data. After tagging, we run a script to correct the BI tag prefixes, as described above for the annotation ask. We also compare to the performance of existing SST systems. In particular we use SenseLearner (Mihalcea and Csomai, 2005) as a baseline, which produces estimates of the WordNet sense for each word. For these predictions, we retrieve the corresponding supersense. Finally, we use a publicly available reimplementation of Ciaramita and Altun (2006) by Michael Heilman, which reaches comparable performance on goldtagged S EM C OR.3 3.3 4 Results The results are presented in Table 2. We distinguish between three settings with various degrees of supervision: weakly supervised, which uses no domain annotated information, but solely relies on embeddings trained on unlabeled Twitter data; unsupervised domain adaptation (DA"
S14-1001,C12-1028,0,0.0175268,"rds. 8 Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on S ENS E VAL -2 and S ENS E VAL -3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum, existing work on supersense tagging and coarse-grained word sense disambiguation for English has to the best of our knowledge all focused on newswire and literature. Moreover, they all rely on gold standard POS information, making previous performance estimates rather optimistic. still predicted the correct verb.cognition as supersense. 6 Related Work There has been relatively little previous work on supersense tagging, and to the best of our knowledge, all of it has been limited to English newswire and literature (S EM C OR and S ENS E VAL). The task of supersense tagging was first"
S14-1001,W06-1670,0,0.762458,"aptation (here, from newswire to Twitter). In We present two Twitter datasets annotated with coarse-grained word senses (supersenses), as well as a series of experiments with three learning scenarios for supersense tagging: weakly supervised learning, as well as unsupervised and supervised domain adaptation. We show that (a) off-the-shelf tools perform poorly on Twitter, (b) models augmented with embeddings learned from Twitter data perform much better, and (c) errors can be reduced using type-constrained inference with distant supervision from WordNet. 1 Introduction Supersense tagging (SST, Ciaramita and Altun, 2006) is the task of assigning high-level ontological classes to open-class words (here, nouns and verbs). It is thus a coarse-grained word sense disambiguation task. The labels are based on the lexicographer file names for Princeton WordNet (Fellbaum, 1998). They include 15 senses for verbs and 26 for nouns (see Table 1). While WordNet also provides catch-all supersenses for adjectives and adverbs, these are grammatically, not semantically motivated, and do not provide any higherlevel abstraction (recently, however, Tsvetkov et al. (2014) proposed a semantic taxonomy for adjectives). They will not"
S14-1001,H94-1046,0,0.25003,"simply express the opinions of the author on some subject matter. Supersense tagging is relevant for Twitter, because it can aid e.g. QA and open RE. If someone posts a message saying that some LaTeX module now supports “drawing trees”, it is important to know whether the post is about drawing natural objects such as oaks or pines, or about drawing tree-shaped data representations. This paper is, to the best of our knowledge, the first work to address the problem of SST for Twitter. While there exist corpora of newswire and literary texts that are annotated with supersenses, e.g., S EM C OR (Miller et al., 1994), no data is available for microblogs or related domains. This paper introduces two new data sets. Furthermore, most, if not all, of previous work on SST has relied on gold standard part-of-speech (POS) tags as input. However, in a domain such as Twitter, which has proven to be challenging for POS tagging (Foster et al., 2011; Ritter et al., 2011), results obtained under the assumption of available perfect POS information are almost meaningless for any real-life application. In this paper, we instead use predicted POS tags and investigate experimental settings in which one or more of the follo"
S14-1001,W02-1001,0,0.0447847,"ocial v.stative v.weather Table 1: The 41 noun and verb supersenses in WordNet Finally, we annotated data sets for Twitter, making supervised domain adaptation (SU) experiments possible. For supervised domain adaptation, we use the annotated training data sets from both the newswire and the Twitter domain, as well as WordNet. 2 More or less supervised models This sections covers the varying degree of supervision of our systems as well as the usage of type constraints as distant supervision. For both unsupervised domain adaptation and supervised domain adaptation, we use structured perceptron (Collins, 2002), i.e., a discriminative HMM model, and search-based structured prediction (S EARN) (Daume et al., 2009). We augment both the EM-trained HMM2, discriminative HMMs and S EARN with type constraints and continuous word representations. We also experimented with conditional random fields (Lafferty et al., 2001), but obtained worse or similar results than with the other models. 2.1 Distant supervision Distant supervision in these experiments was implemented by only allowing a system to predict a certain supersense for a given word if that supersense had either been observed in the training data, or"
S14-1001,R13-1026,0,0.0254562,"aramita and Altun (2006).1 S EARN performed slightly better than structured perceptron, so we use it as our inhouse baseline in the experiments below. In this section, we briefly explain the two approaches. 3 Experiments We experiment with weakly supervised learning, unsupervised domain adaptation, as well as supervised domain adaptation, i.e., where our models are induced from hand-annotated newswire and Twitter data. Note that in all our experiments, 1 https://github.com/coastalcph/ rungsted 2 3 http://hunch.net/˜vw/ use for training, development and evaluation, using the splits proposed in Derczynski et al. (2013), and (b) supersense annotations for a sample of 200 tweets, which we use for additional, out-of-sample evaluation. We call these data sets R ITTER{T RAIN ,D EV,E VAL} and I N -H OUSE -E VAL, respectively. The I N -H OUSE -E VAL dataset was downloaded in 2013 and is a sample of tweets that contain links to external homepages but are otherwise unbiased. It was previously used (with partof-speech annotation) in (Plank et al., 2014). Both data sets are made publicly available with this paper. Supersenses are annotated with in spans defined by the BIO (Begin-Inside-Other) notation. To obtain the T"
S14-1001,E14-1078,1,0.832545,"l our experiments, 1 https://github.com/coastalcph/ rungsted 2 3 http://hunch.net/˜vw/ use for training, development and evaluation, using the splits proposed in Derczynski et al. (2013), and (b) supersense annotations for a sample of 200 tweets, which we use for additional, out-of-sample evaluation. We call these data sets R ITTER{T RAIN ,D EV,E VAL} and I N -H OUSE -E VAL, respectively. The I N -H OUSE -E VAL dataset was downloaded in 2013 and is a sample of tweets that contain links to external homepages but are otherwise unbiased. It was previously used (with partof-speech annotation) in (Plank et al., 2014). Both data sets are made publicly available with this paper. Supersenses are annotated with in spans defined by the BIO (Begin-Inside-Other) notation. To obtain the Twitter data sets, we carried out an annotation task. We first pre-annotated all data sets with WordNet’s most frequent senses. If the word was not in WordNet and a noun, we assigned it the sense n.person. All other words were labeled O. Chains of nouns were altered to give every element the sense of the head noun, and the BI tags adjusted, i.e.: we use predicted POS tags as input to the system, in order to produce a realistic est"
S14-1001,I11-1100,0,0.0237767,"Missing"
S14-1001,D11-1141,0,0.1048,"ng tree-shaped data representations. This paper is, to the best of our knowledge, the first work to address the problem of SST for Twitter. While there exist corpora of newswire and literary texts that are annotated with supersenses, e.g., S EM C OR (Miller et al., 1994), no data is available for microblogs or related domains. This paper introduces two new data sets. Furthermore, most, if not all, of previous work on SST has relied on gold standard part-of-speech (POS) tags as input. However, in a domain such as Twitter, which has proven to be challenging for POS tagging (Foster et al., 2011; Ritter et al., 2011), results obtained under the assumption of available perfect POS information are almost meaningless for any real-life application. In this paper, we instead use predicted POS tags and investigate experimental settings in which one or more of the following resources are available to us: • a large corpus of unlabeled Twitter data; • Princeton WordNet (Fellbaum, 1998); • S EM C OR (Miller et al., 1994); and • a small corpus of Twitter data annotated with supersenses. We approach SST of Twitter using various degrees of supervision for both learning and domain adaptation (here, from newswire to Twi"
S14-1001,E14-4042,0,0.0248191,"tiv e .pers on noun .grou p noun .artif noun a ct .com muni catio n noun .even t noun .loca tion noun .time noun Figure 2: Inter-annotator confusion matrix on T WITTER -E VAL. 0.4 0.3 0.2 0.1 0 Figure 3: Sense distribution of OOV words. 8 Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on S ENS E VAL -2 and S ENS E VAL -3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum, existing work on supersense tagging and coarse-grained word sense disambiguation for English has to the best of our knowledge all focused on newswire and literature. Moreover, they all rely on gold standard POS information, making previous performance estimates rather optimistic. still predicted the correct verb.cognition as supersense. 6 Related Work Th"
S14-1001,P12-2050,0,0.379949,"rovide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from S ENSE E VAL -2 and S ENSE E VAL -3. They use a classification approach rather than structured prediction. 7 Conclusion In this paper, we present two Twitter data sets wit"
S14-1001,P05-1005,0,0.0587046,"tion sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from S ENSE E VAL -2 and S ENSE E VAL -3. They use a classification approach rather than structured prediction. 7 Conclusion In this paper, we present two Twitter data sets with manually annotated supersenses, as well as a series of experiments with these data sets. The data is publicly available for download. In this article we have provided, t"
S14-1001,P99-1023,0,0.187848,"Missing"
S14-1001,S10-1049,0,0.0509089,"nd Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNe"
S14-1001,D12-1127,0,0.0563414,"Missing"
S14-1001,W11-0328,0,0.0190122,"er k on development data for using the k-most frequent senses inWordNet as type constraints. Our supervised models are trained on S EM C OR +R ITTER -T RAIN or simply R ITTER -T RAIN, depending on what gave us the best performance on the held-out data. Baselines For most word sense disambiguation studies, predicting the most frequent sense (MFS) of a word has been proven to be a strong baseline. Following this, our MFS baseline simply predicts the supersense of the most frequent WordNet sense for a tuple of a word and a part of speech. We use the part of speech predicted by the L APOS tagger (Tsuruoka et al., 2011). Any word not in WordNet is labeled as noun.person, which is the most frequent sense overall in the training data. After tagging, we run a script to correct the BI tag prefixes, as described above for the annotation ask. We also compare to the performance of existing SST systems. In particular we use SenseLearner (Mihalcea and Csomai, 2005) as a baseline, which produces estimates of the WordNet sense for each word. For these predictions, we retrieve the corresponding supersense. Finally, we use a publicly available reimplementation of Ciaramita and Altun (2006) by Michael Heilman, which reach"
S14-1001,W13-0906,0,0.0402627,"d Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) a"
S14-1001,tsvetkov-etal-2014-augmenting-english,1,0.22216,"n from WordNet. 1 Introduction Supersense tagging (SST, Ciaramita and Altun, 2006) is the task of assigning high-level ontological classes to open-class words (here, nouns and verbs). It is thus a coarse-grained word sense disambiguation task. The labels are based on the lexicographer file names for Princeton WordNet (Fellbaum, 1998). They include 15 senses for verbs and 26 for nouns (see Table 1). While WordNet also provides catch-all supersenses for adjectives and adverbs, these are grammatically, not semantically motivated, and do not provide any higherlevel abstraction (recently, however, Tsvetkov et al. (2014) proposed a semantic taxonomy for adjectives). They will not be considered in this paper. Coarse-grained categories such as supersenses are useful for downstream tasks such as questionanswering (QA) and open relation extraction (RE). SST is different from NER in that it has a larger set of labels and in the absence of strong orthographic cues (capitalization, quotation marks, etc.). Moreover, supersenses can be applied to any of the lexical parts of speech and not only proper names. Also, while high-coverage gazetteers can be found for named entity recognition, the lexical resources available"
S14-1001,S07-1051,0,0.166033,"and Paaß (Reichartz and Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers"
S14-1001,J10-1004,0,0.0158174,"ystems (DA) S EARN (Baseline) S EARN S EARN HMM HMM + + + + + - Supervised domain adaptation systems (SU) S EARN (Baseline) S EARN S EARN HMM HMM + + + + + + + + + + Table 2: Weighted F1 average over 41 supersenses. 7 noun .act noun .food noun .attri bute noun .relat ion verb. cogn ition verb. creat ion verb. emot ion verb. moti on verb. perce ption verb. stativ e .pers on noun .grou p noun .artif noun a ct .com muni catio n noun .even t noun .loca tion noun .time noun Figure 2: Inter-annotator confusion matrix on T WITTER -E VAL. 0.4 0.3 0.2 0.1 0 Figure 3: Sense distribution of OOV words. 8 Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on S ENS E VAL -2 and S ENS E VAL -3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum,"
S14-2034,W09-1206,0,0.114151,"Missing"
S14-2034,C10-1011,0,0.164674,"Missing"
S14-2034,J93-2004,0,0.0469115,"Missing"
S14-2034,D07-1111,0,0.0867606,"Missing"
S14-2034,C08-1095,0,0.112956,"Missing"
S19-2026,W16-6208,0,0.0621973,"Missing"
S19-2026,D17-1169,1,0.834888,"ection We normalize elongated words and use a spell correction tool which replaces misspelled words with the most probable candidate based on 2 corpora (Wikipedia and Twitter) (Baziotis et al., 2017a). Tokenization We employ a tokenizer which emphasizes expressions and words typically used in social media. These include: 1) censored words, 2) words with emphasis, 3) elongated words, 4) splitting emoticons etc. All words are also lowercased when tokenized. Emoji Descriptions As the dialogues contain a wide variety of emojis, which can contain a great deal of information about a users emotions (Felbo et al., 2017), we replace the emojis found in the utterances with their textual description. We used the emoji descriptions utilized for training Emoji2Vec (Eisner et al., 2016) which can be found in the Unicode emoji standard 2 . For most of the preprocessing steps described above, we relied on the Ekphrasis3 text processing tool (Baziotis et al., 2017b). 4 Model Description This section describes our conversational sentiment classification model as was used in the EmoContext shared task. Our architecture is illustrated in figure 1. Embedding Layer We initiate the embedding layer with an embedding matrix"
S19-2026,S17-2126,0,0.127075,"he dialogues in the dataset, properly preprocessing these is an essential part of the classification process. The preprocessing pipeline consists of multiple steps that we describe in more depth below. Text Normalization We use a custom normalization function which takes commonly used contractions in social media and maps them to a normalized version by unpacking them i.e. idk → i don’t know and plz → please. Spell Correction We normalize elongated words and use a spell correction tool which replaces misspelled words with the most probable candidate based on 2 corpora (Wikipedia and Twitter) (Baziotis et al., 2017a). Tokenization We employ a tokenizer which emphasizes expressions and words typically used in social media. These include: 1) censored words, 2) words with emphasis, 3) elongated words, 4) splitting emoticons etc. All words are also lowercased when tokenized. Emoji Descriptions As the dialogues contain a wide variety of emojis, which can contain a great deal of information about a users emotions (Felbo et al., 2017), we replace the emojis found in the utterances with their textual description. We used the emoji descriptions utilized for training Emoji2Vec (Eisner et al., 2016) which can be f"
S19-2026,N18-1193,0,0.0323045,"tlets. Our proposed system achieves a micro F1 score of 0.7340 on the test set and shows significant gains in performance compared to a system using dialogue level encoding. 1 Introduction Recognizing emotion is crucial to human-human communication and has for a long time been a goal in human-machine interaction. Although there has been growing interest in emotion detection across many fields (Liscombe et al., 2005; Agrafioti et al., 2012; Craggs and Wood, 2004), much of the work has focused on developing empathetic systems using multimodal approaches i.e. speech and gestures as well as text (Hazarika et al., 2018). Approaching emotion detection as a multimodal problem certainly makes sense, as face-face human communication involves many modalities, however, this fails to consider all the communication that is increasingly happening solely via chat, or written means. Detecting emotion in textual dialogue without the other modalities, such as work done by Gupta et al., can allow us to improve a number of applications dealing with social media ∗ Authors contributed equally interactions, opinion mining, and customer interactions, unfortunately, this is a great challenge that has remained largely unexplored"
S19-2026,S19-2005,0,0.0141172,"without the other modalities, such as work done by Gupta et al., can allow us to improve a number of applications dealing with social media ∗ Authors contributed equally interactions, opinion mining, and customer interactions, unfortunately, this is a great challenge that has remained largely unexplored. SemEval 2019 Task 3 attempts to encourage research in this direction. Given a user utterance and the previous two turns of context, the task consists in classifying the user utterance according to one of four emotion classes: happy, sad, angry or other. For a full description of the task see (Chatterjee et al., 2019). In this paper, we describe our turn-level attention model used to tackle this task, specifically using the attention mechanism presented in (Yang et al., 2016). Our model encodes turns in a conversation separately using an Attentive Bidirectional LSTM encoder. In the model presented in the shared task, the turn encoders do not share parameters, achieving a micro F1 score of 0.7340. The code for all experiments presented here is available. 1 2 Related Work Due to its many potential applications across many fields, detection of speaker emotional state in spoken dialogue systems has been studie"
S19-2026,N16-1174,0,0.351452,"ly interactions, opinion mining, and customer interactions, unfortunately, this is a great challenge that has remained largely unexplored. SemEval 2019 Task 3 attempts to encourage research in this direction. Given a user utterance and the previous two turns of context, the task consists in classifying the user utterance according to one of four emotion classes: happy, sad, angry or other. For a full description of the task see (Chatterjee et al., 2019). In this paper, we describe our turn-level attention model used to tackle this task, specifically using the attention mechanism presented in (Yang et al., 2016). Our model encodes turns in a conversation separately using an Attentive Bidirectional LSTM encoder. In the model presented in the shared task, the turn encoders do not share parameters, achieving a micro F1 score of 0.7340. The code for all experiments presented here is available. 1 2 Related Work Due to its many potential applications across many fields, detection of speaker emotional state in spoken dialogue systems has been studied extensively. Early studies showed that the use of prosody as well as speaking style features leads to increases in accuracy for emotion prediction (Ang et al.,"
W05-1727,J82-3001,0,0.321111,"atural language grammar, so to speak; the lexicon and the grammar itself. In other words, the lexicon is designed such that a unique phonological string is associated with a unique lexical entry. Similarly, the grammar only produces one, possibly underspecified, analysis per unique string. The structure of the paper is simple. In the next section, the computational and methodological advantages of functionality are demonstrated. It is shown that the recognition problem of fully functional (and off-line parsable) constraint-based grammars is in ptime, compared to the NP completeness results of Berwick (1982) and Trautwein (1994) for offline parsable, but non-functional constraintbased grammars without lexical rules. The reader may suspect that by imposing this functionality constraint on the lexicon, complexity is merely moved from the lexicon into the rules. This can be avoided in constraint-based grammar formalisms with typed feature structures. The relevant mechanisms are illustrated by a S. Werner (ed.), Proceedings of the 15th NODALIDA conference, Joensuu 2005, Ling@JoY 1, 2006, pp. 193–202 ISBN 952-458-771-8, ISSN 1796-1114. http://ling.joensuu.fi/lingjoy/ © by the authors Proceedings of th"
W05-1727,P91-1033,0,0.0881205,"tes into L ⊆ C × W , truth preservation, i.e. reentrancies, and preservation of satisfiability. In other words, “the ability to have multiple derivation trees and lexical categorizations for one and the same terminal item plays a crucial role in the reduction proof” (Berwick, 1982). The standard proof does not apply to a cfg− f constraint-based grammar. It is thus natural to ask if there is a tractable (ptime) parse algorithm for constraint-based grammars with restricted backbones, i.e. cfg− f? Since weak subsumption of typed feature structures (without disjunction or negation) is polynomial (Dörre, 1991), the recognition problem of cfg− f constraint-based grammars with simple typed feature logic, say Tf , is also in ptime. 194 Proceedings of the 15th NODALIDA conference, Joensuu 2005 Theorem 3.3. The recognition problem of Tf is in ptime. Proof. This follows immediately from the polynomial nature of satisfiability in standard feature logic where unification is defined wrt. weak subsumption. Tf provides a tractable, but very restrictive grammar formalism. Since it is a trivial task to write a tractable grammar formalism, the next sections are devoted to demonstrating the adequacy of Tf . First"
W07-2426,P06-1137,0,0.0406497,"Missing"
W07-2426,W05-1727,1,0.668962,"ordered Languages Remark 4.7. For unordered type 2 grammars, and possibly for totally unordered ones too, it is an alternative to say that all totally unordered productions in a chart have a yield of at most k. This gives a bound on chart size: 0≤j 0≤i X (|N |× (n − i) × i&lt;k X (|N | k−j) × (k − i) × (n − j)) j&lt;(k−i) + k&lt;i X (|N |3 × (n − i)) i&lt;n This fragment no longer generates the MIX language. Such a constraint is obviously not enough for u-AVG, since s-AVG is NP-complete. A third possibility is to restrict the arity of productions. 5 Conclusions and related work In last year’s conference, Søgaard and Haugereid (2006) presented, in a rather informal fashion, a restrictive typed attribute-value structure grammar formalism Tf for free word order phenomena, equipped with a polynomial parsing algorithm. In Tf , horizontal k = 1. The purpose of their paper was mostly philosophical, i.e. in favor of underspecification rather than ambiguity, but many details were left unclear. In a sense, this paper provides a formal basis for some of the claims made in that paper. In particular, types are easily added to s-AVG and u-AVG, and more flexible attribute-value structures can be employed (as long as they are at most po"
W09-2303,W07-1512,0,0.0206485,"gner aligned only sammen and add, but not lægger and add. This would mean that the alignments or translations of add would most likely be associated with the following probabilities: .66 .33 (add, sammen) (add, addere) which again means that our system is likely to arrive at the wrong alignment or translation in (4). Nevertheless these alignments are rewarded in AER. TUER, on the other hand, reflects the intuition that unless you get the entire translation unit it’s better to get nothing at all. The hand-aligned parallel corpora in our experiments come from the Copenhagen Dependency Treebank (Buch-Kromann, 2007), for five different language pairs, the German-English parallel corpus used in Pad´o and Lapata (2006), and the six parallel corpora of the first 100 sentences of Europarl (Koehn, 2005) for different language pairs documented in Graca et al. (2008). Consequently, our experiments include a total of 12 parallel corpora. The biggest parallel corpus consists of 4,729 sentence pairs; the smallest of 61 sentence pairs. The average size is 541 sentence pairs. The six parallel corpora documented in Graca et al. (2008) use sure and possible alignments; in our experiments, as already mentioned, the two"
W09-2303,E06-1019,0,0.0156313,"for syntax-based machine translation, but some prior knowledge is assumed. Sect. 5 brings the three sections together and presents lower bounds on the coverage of the systems discussed in Sect. 4, obtained by inspection of the results in Sect. 2 and 3. Sect. 6 compares our results to related work, in particular Zens and Ney (2003). 2 Inside-out alignments Wu (1997) identified so-called inside-out alignments, two alignment configurations that cannot be induced by binary synchronous context-free grammars; these alignment configurations, while infrequent in language pairs such as English–French (Cherry and Lin, 2006; Wellington et al., 2006), have been argued to be frequent in other language pairs, incl. English–Chinese (Wellington et al., 2006) and English–Spanish (Lepage and Denoual, 2005). While our main focus is on configurations that involve discontinuous translation units, the frequencies of inside-out alignments in our parallel corpora are also reported. Recall that inside-out alignments are of the form (or upside-down): a b c d a b c d or e f g h e f g h Our findings are summarized in Figure 1. Note that there is some variation across the corpora. The fact that there are no inside-out alignments"
W09-2303,W04-3302,0,0.109693,"Missing"
W09-2303,J07-2003,0,0.159621,"d by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora; and while he was a Postdoctoral Researcher at the ISV Computational Linguistics Group, Copenhagen Business School, supported by the Danish Research Foundation in the project Efficient syntax- and semantics-based machine translation. † The second author is supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora. production rules are typically learned from alignment structures (Wu, 1997; Zhang and Gildea, 2004; Chiang, 2007) or from alignment structures and derivation trees for the source string (Yamada and Knight, 2001; Zhang and Gildea, 2004). They are also used for inducing alignments (Wu, 1997; Zhang and Gildea, 2004). It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006). Such alignments are supposed to reflect the structure of translations, typically contain fewer"
W09-2303,P03-2041,0,0.107286,"hang et al. (2006) and Chiang (2007) are not expressive enough to do that. The synchronous grammars used in these systems are, formally, synchronous context-free grammars of rank two (2-SCFGs), or equivalently (normal form) inversion transduction grammars (ITGs).1 The notion of rank is defined as the maximum number of constituents aligned by a production rule, i.e. the maximum number of distinct indeces. Our results will be extended to slight extensions of 2SCFGs, incl. the extension of ITGs proposed by Zens and Ney (2003) (xITGs), synchronous tree substitution grammars of rank two (2-STSGs) (Eisner, 2003; Shieber, 2007), i.e. where tree pairs include at most two linked pairs of nonterminals, and synchronous tree-adjoining grammars of rank two 1 2-SCFGs allow distinct LHS nonterminals, while ITGs do not; but for any 2-SCFG an equivalent ITG can be constructed by creating a cross-product of nonterminals from two sides. 19 Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 19–27, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics (2-STAGs) (Shieber and Schabes, 1990; Harbusch and Poller, 1996; Nesson et al., 2008). The over"
W09-2303,J07-3002,0,0.0657338,"r is a set of terminals that are recognized or generated simultaneously. Consequently, synchronous grammars can only induce complete alignment structures (by transitivity of simultaneity).2 Syntax-based approaches to machine translations are commonly evaluated in terms of their alignment error rate (AER) on one or more parallel corpora (Och and Ney, 2000; Zhang and Gildea, 2004). The AER, in the case where all alignments are sure alignments, is AER = 1 − 2|SA ∩ GA | |SA |+ |GA | where GA are the gold standard alignments, and SA the alignments produced by the system. AER has been criticized by Fraser and Marcu (2007). They show that AER does not penalize unequal precision and recall when a distinction between sure and possible alignments is 2 One of the hand-aligned parallel corpora used in our experiments, the one also used in Pad´o and Lapata (2006), includes incomplete alignment structures. 20 made. Since no such distinction is assumed below, the classical definition is used. We introduce also the notion of translation unit error rate (TUER), which is defined as TUER = 1 − 2|SU ∩ GU | |SU |+ |GU | where GU are the translation units in the gold standard, and SU the translation units produced by the syst"
W09-2303,P04-1064,0,0.153862,"verage of the systems are derived from our results. Our notion of an alignment structure is standard. Words can be aligned to multiple words. Unaligned nodes are permitted. Maximally connected subgraphs are called translation units. There is one more choice to make in the context of many-to-many alignments, namely whether the alignment relation is such that if wi |wk′ and wi |wl′ , resp., are aligned, and wj |wk′ are aligned too, then wj |wl′ are also aligned. If so, the alignment structure is divided into complete translation units. Such alignment structures are therefore called complete; in Goutte et al. (2004), alignment structures with this property are said to be closed under transitivity. An alignment structure is simply written as a sequence of alignments, e.g. hwi |wk′ , wi |wl′ , wj |wk′ , wj |wk′ i, or, alternatively, as sequences of (possibly discontinuous) translation units, e.g. hwi wj |wk′ wl′ i. A translation unit induced by a synchronous grammar is a set of terminals that are recognized or generated simultaneously. Consequently, synchronous grammars can only induce complete alignment structures (by transitivity of simultaneity).2 Syntax-based approaches to machine translations are comm"
W09-2303,graca-etal-2008-building,0,0.131827,"tem is likely to arrive at the wrong alignment or translation in (4). Nevertheless these alignments are rewarded in AER. TUER, on the other hand, reflects the intuition that unless you get the entire translation unit it’s better to get nothing at all. The hand-aligned parallel corpora in our experiments come from the Copenhagen Dependency Treebank (Buch-Kromann, 2007), for five different language pairs, the German-English parallel corpus used in Pad´o and Lapata (2006), and the six parallel corpora of the first 100 sentences of Europarl (Koehn, 2005) for different language pairs documented in Graca et al. (2008). Consequently, our experiments include a total of 12 parallel corpora. The biggest parallel corpus consists of 4,729 sentence pairs; the smallest of 61 sentence pairs. The average size is 541 sentence pairs. The six parallel corpora documented in Graca et al. (2008) use sure and possible alignments; in our experiments, as already mentioned, the two types of alignments are treated alike.3 3 The annotations of the parallel corpora differ in format and consistency. In fact the empirical lower bounds obtained below are lower bounds in two senses: (i) they are lower bounds on TUERs because TUERs m"
W09-2303,2005.mtsummit-papers.11,0,0.00807339,"dd, sammen) (add, addere) which again means that our system is likely to arrive at the wrong alignment or translation in (4). Nevertheless these alignments are rewarded in AER. TUER, on the other hand, reflects the intuition that unless you get the entire translation unit it’s better to get nothing at all. The hand-aligned parallel corpora in our experiments come from the Copenhagen Dependency Treebank (Buch-Kromann, 2007), for five different language pairs, the German-English parallel corpus used in Pad´o and Lapata (2006), and the six parallel corpora of the first 100 sentences of Europarl (Koehn, 2005) for different language pairs documented in Graca et al. (2008). Consequently, our experiments include a total of 12 parallel corpora. The biggest parallel corpus consists of 4,729 sentence pairs; the smallest of 61 sentence pairs. The average size is 541 sentence pairs. The six parallel corpora documented in Graca et al. (2008) use sure and possible alignments; in our experiments, as already mentioned, the two types of alignments are treated alike.3 3 The annotations of the parallel corpora differ in format and consistency. In fact the empirical lower bounds obtained below are lower bounds in"
W09-2303,P08-1069,0,0.0849397,"nk two (2-STSGs) (Eisner, 2003; Shieber, 2007), i.e. where tree pairs include at most two linked pairs of nonterminals, and synchronous tree-adjoining grammars of rank two 1 2-SCFGs allow distinct LHS nonterminals, while ITGs do not; but for any 2-SCFG an equivalent ITG can be constructed by creating a cross-product of nonterminals from two sides. 19 Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 19–27, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics (2-STAGs) (Shieber and Schabes, 1990; Harbusch and Poller, 1996; Nesson et al., 2008). The overall frequency of alignment structures that cannot be induced by these approaches is examined across a wide collection of hand-aligned parallel corpora. Empirical lower bounds on the coverage of the systems are derived from our results. Our notion of an alignment structure is standard. Words can be aligned to multiple words. Unaligned nodes are permitted. Maximally connected subgraphs are called translation units. There is one more choice to make in the context of many-to-many alignments, namely whether the alignment relation is such that if wi |wk′ and wi |wl′ , resp., are aligned, a"
W09-2303,C00-2163,0,0.452993,"ank two (2-SCFGs) (Satta and Peserico, 2005), used in syntaxbased machine translation systems such as Wu (1997), Zhang et al. (2006) and Chiang (2007), in terms of what alignments they induce, has been discussed in Wu (1997) and Wellington et al. (2006), but with a one-sided focus on so-called “inside-out alignments”. Other alignment configurations that cannot be induced by 2-SCFGs are identified in this paper, and their frequencies across a wide collection of hand-aligned parallel corpora are examined. Empirical lower bounds on two measures of alignment error rate, i.e. the one introduced in Och and Ney (2000) and one where only complete translation units are considered, are derived for 2-SCFGs and related formalisms. 1 Introduction Syntax-based approaches to machine translation typically use synchronous grammars to recognize or produce translation equivalents. The synchronous ∗ This work was done while the first author was a Senior Researcher at the Dpt. of Linguistics, University of Potsdam, supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora; and while he was a Postdoctoral Researcher at the ISV Computational Linguistics Gr"
W09-2303,P06-1146,0,0.327535,"Missing"
W09-2303,C90-3045,0,0.424016,"(xITGs), synchronous tree substitution grammars of rank two (2-STSGs) (Eisner, 2003; Shieber, 2007), i.e. where tree pairs include at most two linked pairs of nonterminals, and synchronous tree-adjoining grammars of rank two 1 2-SCFGs allow distinct LHS nonterminals, while ITGs do not; but for any 2-SCFG an equivalent ITG can be constructed by creating a cross-product of nonterminals from two sides. 19 Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 19–27, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics (2-STAGs) (Shieber and Schabes, 1990; Harbusch and Poller, 1996; Nesson et al., 2008). The overall frequency of alignment structures that cannot be induced by these approaches is examined across a wide collection of hand-aligned parallel corpora. Empirical lower bounds on the coverage of the systems are derived from our results. Our notion of an alignment structure is standard. Words can be aligned to multiple words. Unaligned nodes are permitted. Maximally connected subgraphs are called translation units. There is one more choice to make in the context of many-to-many alignments, namely whether the alignment relation is such th"
W09-2303,2008.eamt-1.23,1,0.712883,"computer science, there is a trade-off between expressivity and complexity. The results presented here, namely that classes of alignment structures excluded by syntax-based translation systems, occur frequently in hand-aligned parallel corpora, could be taken to indicate that more expressive formalisms are needed. This at least seems to be the case to the extent alignment error rates are reasonable measures of the adequacy of syntax-based machine translation systems. On the other hand parsing complexities in 26 Two other challenges for this type of approach are: (i) The use of intersection in Søgaard (2008b) to induce insideout alignments and cross-serial DTUs seems to miss important generalizations; see Chiang (2004) for a similar point in the context of parsing. (ii) If the class of alignment structures is restricted in any natural way, i.e. to 1 : 1 alignments, the problem whether there exists a possible alignment given two sentences and a grammar becomes NP-hard (Søgaard, 2009). NB: The undecidability of computing tight estimators was pointed out to us by Mark-Jan Nederhof (p.c.), but Alexander Clark (p.c.) and others have suggested that pseudo-tight estimators can be used in practice. Refe"
W09-2303,C08-2026,1,0.857018,"computer science, there is a trade-off between expressivity and complexity. The results presented here, namely that classes of alignment structures excluded by syntax-based translation systems, occur frequently in hand-aligned parallel corpora, could be taken to indicate that more expressive formalisms are needed. This at least seems to be the case to the extent alignment error rates are reasonable measures of the adequacy of syntax-based machine translation systems. On the other hand parsing complexities in 26 Two other challenges for this type of approach are: (i) The use of intersection in Søgaard (2008b) to induce insideout alignments and cross-serial DTUs seems to miss important generalizations; see Chiang (2004) for a similar point in the context of parsing. (ii) If the class of alignment structures is restricted in any natural way, i.e. to 1 : 1 alignments, the problem whether there exists a possible alignment given two sentences and a grammar becomes NP-hard (Søgaard, 2009). NB: The undecidability of computing tight estimators was pointed out to us by Mark-Jan Nederhof (p.c.), but Alexander Clark (p.c.) and others have suggested that pseudo-tight estimators can be used in practice. Refe"
W09-2303,P06-1123,0,0.701618,"rpora. production rules are typically learned from alignment structures (Wu, 1997; Zhang and Gildea, 2004; Chiang, 2007) or from alignment structures and derivation trees for the source string (Yamada and Knight, 2001; Zhang and Gildea, 2004). They are also used for inducing alignments (Wu, 1997; Zhang and Gildea, 2004). It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006). Such alignments are supposed to reflect the structure of translations, typically contain fewer errors and are used to evaluate automatically induced alignments. In this paper it is shown that the synchronous grammars used in Wu (1997), Zhang et al. (2006) and Chiang (2007) are not expressive enough to do that. The synchronous grammars used in these systems are, formally, synchronous context-free grammars of rank two (2-SCFGs), or equivalently (normal form) inversion transduction grammars (ITGs).1 The notion of rank is defined as the maximum number of constituents aligned by a production rule"
W09-2303,J97-3002,0,0.921945,"s, University of Potsdam, supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora; and while he was a Postdoctoral Researcher at the ISV Computational Linguistics Group, Copenhagen Business School, supported by the Danish Research Foundation in the project Efficient syntax- and semantics-based machine translation. † The second author is supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora. production rules are typically learned from alignment structures (Wu, 1997; Zhang and Gildea, 2004; Chiang, 2007) or from alignment structures and derivation trees for the source string (Yamada and Knight, 2001; Zhang and Gildea, 2004). They are also used for inducing alignments (Wu, 1997; Zhang and Gildea, 2004). It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006). Such alignments are supposed to reflect the structure o"
W09-2303,P01-1067,0,0.0949335,"earning from parallel corpora; and while he was a Postdoctoral Researcher at the ISV Computational Linguistics Group, Copenhagen Business School, supported by the Danish Research Foundation in the project Efficient syntax- and semantics-based machine translation. † The second author is supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora. production rules are typically learned from alignment structures (Wu, 1997; Zhang and Gildea, 2004; Chiang, 2007) or from alignment structures and derivation trees for the source string (Yamada and Knight, 2001; Zhang and Gildea, 2004). They are also used for inducing alignments (Wu, 1997; Zhang and Gildea, 2004). It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006). Such alignments are supposed to reflect the structure of translations, typically contain fewer errors and are used to evaluate automatically induced alignments. In this paper it is shown that"
W09-2303,P03-1019,0,0.74412,"ed alignments. In this paper it is shown that the synchronous grammars used in Wu (1997), Zhang et al. (2006) and Chiang (2007) are not expressive enough to do that. The synchronous grammars used in these systems are, formally, synchronous context-free grammars of rank two (2-SCFGs), or equivalently (normal form) inversion transduction grammars (ITGs).1 The notion of rank is defined as the maximum number of constituents aligned by a production rule, i.e. the maximum number of distinct indeces. Our results will be extended to slight extensions of 2SCFGs, incl. the extension of ITGs proposed by Zens and Ney (2003) (xITGs), synchronous tree substitution grammars of rank two (2-STSGs) (Eisner, 2003; Shieber, 2007), i.e. where tree pairs include at most two linked pairs of nonterminals, and synchronous tree-adjoining grammars of rank two 1 2-SCFGs allow distinct LHS nonterminals, while ITGs do not; but for any 2-SCFG an equivalent ITG can be constructed by creating a cross-product of nonterminals from two sides. 19 Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 19–27, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics (2-STAGs) ("
W09-2303,C04-1060,0,0.15659,"ity of Potsdam, supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora; and while he was a Postdoctoral Researcher at the ISV Computational Linguistics Group, Copenhagen Business School, supported by the Danish Research Foundation in the project Efficient syntax- and semantics-based machine translation. † The second author is supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora. production rules are typically learned from alignment structures (Wu, 1997; Zhang and Gildea, 2004; Chiang, 2007) or from alignment structures and derivation trees for the source string (Yamada and Knight, 2001; Zhang and Gildea, 2004). They are also used for inducing alignments (Wu, 1997; Zhang and Gildea, 2004). It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006). Such alignments are supposed to reflect the structure of translations, typicall"
W09-2303,N06-1033,0,0.094295,"for inducing alignments (Wu, 1997; Zhang and Gildea, 2004). It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006). Such alignments are supposed to reflect the structure of translations, typically contain fewer errors and are used to evaluate automatically induced alignments. In this paper it is shown that the synchronous grammars used in Wu (1997), Zhang et al. (2006) and Chiang (2007) are not expressive enough to do that. The synchronous grammars used in these systems are, formally, synchronous context-free grammars of rank two (2-SCFGs), or equivalently (normal form) inversion transduction grammars (ITGs).1 The notion of rank is defined as the maximum number of constituents aligned by a production rule, i.e. the maximum number of distinct indeces. Our results will be extended to slight extensions of 2SCFGs, incl. the extension of ITGs proposed by Zens and Ney (2003) (xITGs), synchronous tree substitution grammars of rank two (2-STSGs) (Eisner, 2003; Shie"
W09-2303,H05-1101,0,\N,Missing
W09-2303,W07-0412,0,\N,Missing
W09-2303,W90-0102,0,\N,Missing
W09-2308,J99-4005,0,0.0594348,"ut alignments discussed in Wu (1997). Another class that ITGs do not induce is that of alignments with discontinuous translation units (Søgaard, 2008). Søgaard (2008), on the other hand, shows that the alignments induced by (2,2)-BRCGs are closed under union, i.e. (2,2)-BRCGs induce all possible alignments. The universal recognition problems of both ITGs and (2,2)-BRCGs can be solved in time O(n6 |G|). This may come as a surprise, as ITGs restrict the alignment search space considerably, while (2,2)BRCGs do not. In the context of the NP-hardness of decoding in statistical machine translation (Knight, 1999; Udupa and Maji, 2006), it is natural to ask why the universal recognition problem of (2,2)BRCGs isn’t NP-hard? How can (2,2)-BRCGs induce all possible alignments and still avoid NPhardness? This paper bridges the gap between these results and shows that when alignments are restricted to be 1 : 1, island-free or sure-possible sorted (see below), or all combinations thereof, the alignment problem of (2,2)-BRCGs is NP-hard. (2,2)-BRCGs in a sense avoid NP-hardness by giving up control over global properties of alignments, e.g. any pair of words may be aligned multiple times in a derivation. 60"
W09-2308,J99-1003,0,0.0442733,") (10) (11) (12) (13) (14) (15) 1:1 X X X X X X X X IF(s) X X X X X X X X IF(t) X SP X X X X X X X X X X X X X X X ITGs O(n6 |G|) O(n6 |G|) O(n6 |G|) O(n6 |G|) O(n6 |G|) O(n6 |G|) O(n6 |G|) O(n6 |G|) O(n6 |G|) O(n6 |G|) O(n6 |G|) O(n6 |G|) O(n6 |G|) O(n6 |G|) O(n6 |G|) (2,2)-BRCGs NP-complete NP-complete NP-complete NP-complete NP-complete NP-complete NP-complete NP-complete NP-complete NP-complete NP-complete NP-complete NP-complete NP-complete NP-complete Figure 1: The complexity of restricted alignment problems for ITGs and (2,2)-BRCGs. • 1 : 1-alignments have been argued to be adequate by Melamed (1999) and elsewhere, and it may therefore be useful to know if a grammar extracted from a parallel corpus produces 1 : 1alignments for a finite set of sentence pairs. • Island-free alignments are interesting to the extent that unaligned nodes increase the chance of translation errors. An island threshold may for instance be used to rule out risky translations. • The notion of sure-possible sorted alignments is more unusual, but can, for instance, be used to check if the use of possible alignments is consistently triggered by words that are hard to align. The results for all cross-classifications of"
W09-2308,C08-2026,1,0.913733,"alignment problem for synchronous grammars in its unrestricted form, i.e. whether for a grammar and a string pair the grammar induces an alignment of the two strings, reduces to the universal recognition problem, but restrictions may be imposed on the alignment sought, e.g. alignments may be 1 : 1, island-free or sure-possible sorted. The complexities of 15 restricted alignment problems in two very different synchronous grammar formalisms of syntax-based machine translation, inversion transduction grammars (ITGs) (Wu, 1997) and a restricted form of range concatenation grammars ((2,2)-BRCGs) (Søgaard, 2008), are investigated. The universal recognition problems, and therefore also the unrestricted alignment problems, of both formalisms can be solved in time O(n6 |G|). The complexities of the restricted alignment problems differ significantly, however. 1 Introduction The synchronous grammar formalisms used in syntax-based machine translation typically induce alignments by aligning all words that are recognized simultaneously (Wu, 1997; Zhang and Gildea, ∗ This work was done while the first author was a Senior Researcher at the Dpt. of Linguistics, University of Potsdam, supported by the German Res"
W09-2308,E06-1004,0,0.017917,"discussed in Wu (1997). Another class that ITGs do not induce is that of alignments with discontinuous translation units (Søgaard, 2008). Søgaard (2008), on the other hand, shows that the alignments induced by (2,2)-BRCGs are closed under union, i.e. (2,2)-BRCGs induce all possible alignments. The universal recognition problems of both ITGs and (2,2)-BRCGs can be solved in time O(n6 |G|). This may come as a surprise, as ITGs restrict the alignment search space considerably, while (2,2)BRCGs do not. In the context of the NP-hardness of decoding in statistical machine translation (Knight, 1999; Udupa and Maji, 2006), it is natural to ask why the universal recognition problem of (2,2)BRCGs isn’t NP-hard? How can (2,2)-BRCGs induce all possible alignments and still avoid NPhardness? This paper bridges the gap between these results and shows that when alignments are restricted to be 1 : 1, island-free or sure-possible sorted (see below), or all combinations thereof, the alignment problem of (2,2)-BRCGs is NP-hard. (2,2)-BRCGs in a sense avoid NP-hardness by giving up control over global properties of alignments, e.g. any pair of words may be aligned multiple times in a derivation. 60 Proceedings of SSST-3,"
W09-2308,J97-3002,0,0.921188,"or Language Technology University of Copenhagen soegaard@hum.ku.dk Abstract The alignment problem for synchronous grammars in its unrestricted form, i.e. whether for a grammar and a string pair the grammar induces an alignment of the two strings, reduces to the universal recognition problem, but restrictions may be imposed on the alignment sought, e.g. alignments may be 1 : 1, island-free or sure-possible sorted. The complexities of 15 restricted alignment problems in two very different synchronous grammar formalisms of syntax-based machine translation, inversion transduction grammars (ITGs) (Wu, 1997) and a restricted form of range concatenation grammars ((2,2)-BRCGs) (Søgaard, 2008), are investigated. The universal recognition problems, and therefore also the unrestricted alignment problems, of both formalisms can be solved in time O(n6 |G|). The complexities of the restricted alignment problems differ significantly, however. 1 Introduction The synchronous grammar formalisms used in syntax-based machine translation typically induce alignments by aligning all words that are recognized simultaneously (Wu, 1997; Zhang and Gildea, ∗ This work was done while the first author was a Senior Resea"
W09-2308,C04-1060,0,0.513134,"Missing"
W09-3805,P06-1146,0,0.221643,"Missing"
W09-3805,H05-1101,0,0.172609,"s at the level of translation units, it is natural to interpret them conjunctively. Otherwise we would in reality count failures at the level of alignments. (iv) We use (c). The conjunctive interpretation of translation units was also adopted by Fox (2002) and is motivated by the importance of translation units and discontinuous ones in particular to machine translation in general (Simard and colleagues, 2005; Ayan and Dorr, 2006; Macken, 2007; Shieber, 2007). In brief, 2 Related work Aho and Ullman (1972) showed that 4-ary synchronous context-free grammars (SCFGs) could not be binarized, and Satta and Peserico (2005) showed that the hiearchy of SCFGs beyond ternary ones does not collapse; they also showed that the complexity of the universal recognition problem for SCFGs is NP-complete. ITGs on the other hand has a O(|G|n6 ) solvable universal recognition problem, which coincides with the unrestricted alignment problem (Søgaard, 2009). The result extends to decoding in conjunction with a bigram language model (Huang et al., 2005). Wu (1997) introduced ITGs and normal form ITGs. ITGs are a notational variant of the subclass of SCFGs such that all indexed nonterminals in the source side of the RHS occur in"
W09-3805,W02-1039,0,0.0554008,"dy uses the data also used by Søgaard and Kuhn (2009), which to the best of our knowledge uses the largest collection of handaligned parallel corpora used in any of these studies. (ii) Failures are counted at the level of translation units as argued for in the above, but supplemented by parse failure rates for completeness. (iii) Since we count failures at the level of translation units, it is natural to interpret them conjunctively. Otherwise we would in reality count failures at the level of alignments. (iv) We use (c). The conjunctive interpretation of translation units was also adopted by Fox (2002) and is motivated by the importance of translation units and discontinuous ones in particular to machine translation in general (Simard and colleagues, 2005; Ayan and Dorr, 2006; Macken, 2007; Shieber, 2007). In brief, 2 Related work Aho and Ullman (1972) showed that 4-ary synchronous context-free grammars (SCFGs) could not be binarized, and Satta and Peserico (2005) showed that the hiearchy of SCFGs beyond ternary ones does not collapse; they also showed that the complexity of the universal recognition problem for SCFGs is NP-complete. ITGs on the other hand has a O(|G|n6 ) solvable universal"
W09-3805,J07-3002,0,0.0386341,"B nothing/pas] and B → [change/modifie], for example, induces a DTU with a gap in the French side for the pair of substrings hchange nothing, ne modifie pasi. Multigap DTUs with up to three gaps are frequent (Søgaard and Kuhn, 2009) and have shown to be important for translation quality (Simard and colleagues, 2005). While normal form ITGs do not induce multigap DTUs, ITGs induce a particular subclass of multigap DTUs, namely those that are constructed by linear or inverse interpolation. 4 Discussion The usefulness of alignment error rate (AER) (Och and Ney, 2000) has been questioned lately (Fraser and Marcu, 2007); most importantly, AER does not always seem to correlate with translation quality. TUER is likely to correlate better with translation quality, since it by definition correlates with CPER (Ayan and Dorr, 2006). No large-scale experiment has been done yet to estimate the strength of this correlation. Our study also relies on the assumption that simulatenously recognized words are aligned in bilingual parsing. The relationship between parsing and alignment can of course be complicated in ways that will alter the alignment capacity of ITG and its normal form; on some definitions the two formalis"
W09-3805,H05-1095,0,0.565888,"el corpora used in any of these studies. (ii) Failures are counted at the level of translation units as argued for in the above, but supplemented by parse failure rates for completeness. (iii) Since we count failures at the level of translation units, it is natural to interpret them conjunctively. Otherwise we would in reality count failures at the level of alignments. (iv) We use (c). The conjunctive interpretation of translation units was also adopted by Fox (2002) and is motivated by the importance of translation units and discontinuous ones in particular to machine translation in general (Simard and colleagues, 2005; Ayan and Dorr, 2006; Macken, 2007; Shieber, 2007). In brief, 2 Related work Aho and Ullman (1972) showed that 4-ary synchronous context-free grammars (SCFGs) could not be binarized, and Satta and Peserico (2005) showed that the hiearchy of SCFGs beyond ternary ones does not collapse; they also showed that the complexity of the universal recognition problem for SCFGs is NP-complete. ITGs on the other hand has a O(|G|n6 ) solvable universal recognition problem, which coincides with the unrestricted alignment problem (Søgaard, 2009). The result extends to decoding in conjunction with a bigram l"
W09-3805,P04-1064,0,0.100379,"y an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004; Wellington et al., 2006; Søgaard and Kuhn, 2009). As noted by Søgaard (2009), this definition of alignment has the advantageous consequence that candidate alignments can be singled out by mere inspection of the grammar rules. It also has the consequence that alignments are transitive (Goutte et al., 2004), since simultaneity is transitive. While previous work (Søgaard and Kuhn, 2009) has estimated empirical lower bounds for normal form ITGs at the level of translation units (TUER), or cepts (Goutte et al., 2004), defined as maximally connected subgraphs in alignments, nobody has done this for the full class of ITGs. What is important to understand is that while normal form ITGs can induce the same class of translations as the full class of ITGs, they do not induce the same class of alignments. They do not, for exEmpirical lower bounds studies in which the frequency of alignment configurations"
W09-3805,W09-2303,1,0.687645,"or the full class of inversion transduction grammars Dekai Wu Anders Søgaard Human Language Technology Center Center for Language Technology Hong Kong Univ. of Science and Technology University of Copenhagen dekai@cs.ust.hk soegaard@hum.ku.dk Abstract While it is easy to estimate the consequences of restrictions to n-grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax-based machine translation formalisms. Consequently, much work has been devoted to this task (Wu, 1997; Zens and Ney, 2003; Wellington et al., 2006; Macken, 2007; Søgaard and Kuhn, 2009). The task of estimating the consequences of the structural constraints imposed by a particular syntax-based formalism consists in finding what is often called “empirical lower bounds” on the coverage of the formalism (Wellington et al., 2006; Søgaard and Kuhn, 2009). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful"
W09-3805,graca-etal-2008-building,0,0.0692754,"cal item in many cases. 34 3.1 different sides, i.e. D has a gap in the source side that contains at least one token in E, and E has a gap in the target side that contains at least one token in D. Here’s an example of a bonbon configuration from Simard et al. (2005): Data The characteristics of the hand-aligned gold standard parallel corpora used are presented in Figure 1. The Danish-Spanish text is part of the Copenhagen Dependency Treebank (Parole), English-German is from Pado and Lapata (2006) (Europarl), and the six combinations of English, French, Portuguese and Spanish are documented in Graca et al. (2008) (Europarl). 3.2 Pierre ne mange pas Pierre does not eat Multigap DTUs with mixed transfer are, as already mentioned multigap DTUs with crossing alignments from material in two distinct gaps. Alignment configurations 3.3 Results The full class of ITGs induces many alignment configurations that normal form ITGs do not induce, incl. discontinuous translation units (DTUs), i.e. translation units with at least one gap, doublesided DTUs, i.e. DTUs with both a gap in the source side and a gap in the target side, and multigap DTUs with arbitrarily many gaps (as long as the contents in the gap are eit"
W09-3805,W09-2308,1,0.909515,"unds” on the coverage of the formalism (Wellington et al., 2006; Søgaard and Kuhn, 2009). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004; Wellington et al., 2006; Søgaard and Kuhn, 2009). As noted by Søgaard (2009), this definition of alignment has the advantageous consequence that candidate alignments can be singled out by mere inspection of the grammar rules. It also has the consequence that alignments are transitive (Goutte et al., 2004), since simultaneity is transitive. While previous work (Søgaard and Kuhn, 2009) has estimated empirical lower bounds for normal form ITGs at the level of translation units (TUER), or cepts (Goutte et al., 2004), defined as maximally connected subgraphs in alignments, nobody has done this for the full class of ITGs. What is important to understand is that while normal"
W09-3805,W05-1507,0,0.0604146,", 2006; Macken, 2007; Shieber, 2007). In brief, 2 Related work Aho and Ullman (1972) showed that 4-ary synchronous context-free grammars (SCFGs) could not be binarized, and Satta and Peserico (2005) showed that the hiearchy of SCFGs beyond ternary ones does not collapse; they also showed that the complexity of the universal recognition problem for SCFGs is NP-complete. ITGs on the other hand has a O(|G|n6 ) solvable universal recognition problem, which coincides with the unrestricted alignment problem (Søgaard, 2009). The result extends to decoding in conjunction with a bigram language model (Huang et al., 2005). Wu (1997) introduced ITGs and normal form ITGs. ITGs are a notational variant of the subclass of SCFGs such that all indexed nonterminals in the source side of the RHS occur in the same order or exactly in the inverse order in the target side of the RHS. It turns out that this subclass of SCFGs defines the same set of translations that can be defined by binary SCFGs. The different forms of production rules are listed below with the more restricted normal form production rules in the right column, with φ ∈ (N ∪ {e/f |e ∈ T ∗ , f ∈ T ∗ })∗ (N nonterminals and T terminals, as usual). The RHS op"
W09-3805,P06-1123,0,0.827122,"bounds on translation unit error rate for the full class of inversion transduction grammars Dekai Wu Anders Søgaard Human Language Technology Center Center for Language Technology Hong Kong Univ. of Science and Technology University of Copenhagen dekai@cs.ust.hk soegaard@hum.ku.dk Abstract While it is easy to estimate the consequences of restrictions to n-grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax-based machine translation formalisms. Consequently, much work has been devoted to this task (Wu, 1997; Zens and Ney, 2003; Wellington et al., 2006; Macken, 2007; Søgaard and Kuhn, 2009). The task of estimating the consequences of the structural constraints imposed by a particular syntax-based formalism consists in finding what is often called “empirical lower bounds” on the coverage of the formalism (Wellington et al., 2006; Søgaard and Kuhn, 2009). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work th"
W09-3805,J97-3002,1,0.658865,"a particular syntax-based formalism consists in finding what is often called “empirical lower bounds” on the coverage of the formalism (Wellington et al., 2006; Søgaard and Kuhn, 2009). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004; Wellington et al., 2006; Søgaard and Kuhn, 2009). As noted by Søgaard (2009), this definition of alignment has the advantageous consequence that candidate alignments can be singled out by mere inspection of the grammar rules. It also has the consequence that alignments are transitive (Goutte et al., 2004), since simultaneity is transitive. While previous work (Søgaard and Kuhn, 2009) has estimated empirical lower bounds for normal form ITGs at the level of translation units (TUER), or cepts (Goutte et al., 2004), defined as maximally connected subgraphs in alignments,"
W09-3805,P03-1019,0,0.0581106,"the target side of the RHS. It turns out that this subclass of SCFGs defines the same set of translations that can be defined by binary SCFGs. The different forms of production rules are listed below with the more restricted normal form production rules in the right column, with φ ∈ (N ∪ {e/f |e ∈ T ∗ , f ∈ T ∗ })∗ (N nonterminals and T terminals, as usual). The RHS operator [ ] preserves source language constituent order in the target language, while h i reverses it.1 A A → → [φ] hφi A A A → → → [BC] hBCi e/f Several studies have adressed the alignment capacity of ITGs and normal form ITGs. Zens and Ney (2003) induce lower bounds on PRFs for normal form ITGs. Wellington et al. (2006) induce lower bounds on PRFs for ITGs. Søgaard and Kuhn (2009) induce lower bounds on TUER for normal form ITGs and more expressive formalisms for syntax-based machine translation. No one has, however, to the best our knowledge induced lower bounds on TUER for ITGs. TUER = 1 − 2|SU ∩ GU | |SU |+ |GU | where GU are the translation units in the gold standard, and SU the translation units produced by the system. This evaluation measure is related to consistent phrase error rate (CPER) introduced in Ayan and Dorr (2006), ex"
W09-3805,C00-2163,0,0.068155,"e combination of the production rules A → [ǫ/ne B nothing/pas] and B → [change/modifie], for example, induces a DTU with a gap in the French side for the pair of substrings hchange nothing, ne modifie pasi. Multigap DTUs with up to three gaps are frequent (Søgaard and Kuhn, 2009) and have shown to be important for translation quality (Simard and colleagues, 2005). While normal form ITGs do not induce multigap DTUs, ITGs induce a particular subclass of multigap DTUs, namely those that are constructed by linear or inverse interpolation. 4 Discussion The usefulness of alignment error rate (AER) (Och and Ney, 2000) has been questioned lately (Fraser and Marcu, 2007); most importantly, AER does not always seem to correlate with translation quality. TUER is likely to correlate better with translation quality, since it by definition correlates with CPER (Ayan and Dorr, 2006). No large-scale experiment has been done yet to estimate the strength of this correlation. Our study also relies on the assumption that simulatenously recognized words are aligned in bilingual parsing. The relationship between parsing and alignment can of course be complicated in ways that will alter the alignment capacity of ITG and i"
W09-3805,C04-1060,0,0.0191821,"ar syntax-based formalism consists in finding what is often called “empirical lower bounds” on the coverage of the formalism (Wellington et al., 2006; Søgaard and Kuhn, 2009). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004; Wellington et al., 2006; Søgaard and Kuhn, 2009). As noted by Søgaard (2009), this definition of alignment has the advantageous consequence that candidate alignments can be singled out by mere inspection of the grammar rules. It also has the consequence that alignments are transitive (Goutte et al., 2004), since simultaneity is transitive. While previous work (Søgaard and Kuhn, 2009) has estimated empirical lower bounds for normal form ITGs at the level of translation units (TUER), or cepts (Goutte et al., 2004), defined as maximally connected subgraphs in alignments, nobody has done this fo"
W09-3805,W07-0412,0,\N,Missing
W09-3805,P06-1002,0,\N,Missing
W09-3831,W06-2920,0,0.0414787,"ich considers both words and tags), scores are now comparable to more mature dependency parsers: ULA excl. punctuation for Arabic is 70.74 for Vine+ROOT+LEFT+RIGHT which is better than six of the systems who participated in the CONLL-X Shared Task and who had access to all data in the treebank, i.e. tokens, lemmas, POS 2 Data Our languages are chosen from different language families. Arabic is a Semitic language, Czech is Slavic, Dutch is Germanic, Italian is Romance, Japanese is Japonic-Ryukyuan, and Turkish is Uralic. All treebanks, except Italian, were also used in the CONLL-X Shared Task (Buchholz and Marsi, 2006). The Italian treebank is the law section of the TUT Treebank used in the Evalita 2007 Dependency Parsing Challenge (Bosco et al., 2000). 3 Experiments The Python/C++ implementation of the maximum entropy-based part-of-speech (POS) tagger first described in Ratnaparkhi (1998) that comes with 207 Arabic ROOT LEFT RIGHT Czech ROOT LEFT RIGHT Dutch ROOT LEFT RIGHT Italian ROOT LEFT RIGHT Japanese ROOT LEFT RIGHT Turkish ROOT LEFT RIGHT Gold 443 3035 313 Gold 737 1485 1288 Gold 522 1734 1300 Gold 100 1601 192 Gold 939 1398 2838 Gold 694 750 3433 Predicted 394 3180 196 Predicted 649 1384 1177 Predi"
W09-3831,2003.mtsummit-papers.6,0,0.125342,"Missing"
W09-3831,W06-2929,0,0.469474,"84.64 84.17 84.78 85.45 Turkish 68.94 68.53 68.45 68.37 69.87 69.79 69.74 Figure 2: Labeled attachment scores (LASs) for MaltParser limited to POS tags, our baseline vine parser (Vine) and our extensions of Vine. Best scores bold-faced. 208 which may be used to hardwire dependency relations into candidate weight matrices. POS taggers may also be used to identify other dependency relations or more fine-grained features that can improve the accuracy of dependency parsers. tags, features and dependency relations; not just the POS tags as in our case. In particular, our result is 2.28 better than Dreyer et al. (2006) who also use soft and hard constraints on dependency lengths. They extend the parsing algorithm in Eisner and Smith (2005) to labeled k-best parsing and use a reranker to find the best parse according to predefined global features. ULA excl. punctuation for Turkish is 67.06 which is better than six of the shared task participants, incl. Dreyer et al. (2006) (60.45). The improvements come at an extremely low cost. The POS tagger simply stores its decisions in a very small table, typically 5–10 cells per sentence, that is queried in no time in parsing. Parsing a standard small test suite takes"
W09-3831,W05-1504,0,0.315097,"Missing"
W09-3831,P09-1087,0,0.118938,"Missing"
W09-3831,P08-2060,0,0.023726,"w in terms of training (hours, sometimes days), and relatively big models are queried in parsing. MSTParser and MaltParser can be optimized for speed in various ways,1 but the many applications of dependency parsers today may turn model size into a serious problem. MSTParser typically takes about a minute to parse a small standard test suite, say 2–300 sentences; the stand-alone version of MaltParser may take 5–8 minutes. Such parsing times are problematic in, say, a machine translation system where for each sentence pair multiple 1 Recent work has optimized MaltParser considerably for speed. Goldberg and Elhadad (2008) speed up the MaltParser by a factor of 30 by simplifying the decision function for the classifiers. Parsing is still considerably slower than with our vine parser, i.e. a test suite is parsed in about 15–20 seconds, whereas our vine parser parses a test suite in less than two seconds. 206 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 206–209, c Paris, October 2009. 2009 Association for Computational Linguistics the maximum entropy library in Zhang (2004) was used to identify arcs to the root node and to tokens immediately left or right of the dependent"
W09-3831,H05-1066,0,0.203303,"Missing"
W09-3831,bosco-etal-2000-building,0,\N,Missing
W09-3831,D07-1096,0,\N,Missing
W09-4626,E91-1005,0,0.553111,"rchy” in a non-standard way. It restricts expressivity in some ways (by read-first determinism and λ-acylicity), but adds expressivity in other ways (by introducing a bag). In the conclusion, once we have established the necessary results, our proposal is also compared to Bertsch and Nederhof (1999). Bertsch and Nederhof (1999) define another linear time extension of deterministic pushdown automata, but their extension remains context-free. Finally, our use of bags is related to the use of sets of elementary trees in certain linguistically motivated extensions of tree-adjoining grammar, incl. Becker et al. (1991) and Lichte (2007). A very brief summary of tree-adjoining grammar: Tree substitution grammar is a variation over context-free grammar. Instead of production rules of the form → S NP VP tree fragments of the following form are introduced: S @ @ NP VP In derivation, trees with root labels A are plugged into trees with leaf nodes labeled by A. If a tree is obtained with root label S (the start symbol) and all leaf nodes are labeled by terminal symbols, the tree is a parse of its yield. Treeadjoining grammar extends this context-free formalism by an operation on trees called adjunction, e.g.: S +"
W09-4626,C02-1028,0,0.0146534,"posal, the intuition is the same: The constructive transitions Left-Arc and Right-Arc are tried out first, and only if no constructive transitions are applicable can the λ-transitions be applied. The underlying if-then-else structure means that the procedure remains deterministic. The three algorithms introduced in Nivre (2003) all terminate in linear time. Other related classes of automata include extended pushdown automata (Vijay-Shanker, 1987), weakly equivalent to tree-adjoining grammars, which use nested stacks to provide an additional control layer, and thread automata (Villemonte de La Clergerie, 2002), weakly equivalent to simple range concatenation grammar. These classes are not discussed here, but it should be noted that they were constructed to capture the expressivity of linguistic theories, while the class of automata introduced here “cross-cuts the Chomsky hierarchy” in a non-standard way. It restricts expressivity in some ways (by read-first determinism and λ-acylicity), but adds expressivity in other ways (by introducing a bag). In the conclusion, once we have established the necessary results, our proposal is also compared to Bertsch and Nederhof (1999). Bertsch and Nederhof (1999"
W09-4626,W00-2030,0,0.0262127,"nstruction involved in scrambling is of the following form, ignoring the internal syntax of the verb cluster: dass permute(NP1 . . . NPn−1 NP’) V1 . . . Vn where NPi is the object complement of Vi for 1 ≤ i < n. The NP’ is the subject of the finite verb Vn . This construction is recognized by the SBPA S4 = h{q0 , q1 , q2 , q3 }, {NP 1 , . . . , NP n−1 , NP ′ , V1 , . . . Vn }, {NP 1 , . . . , NP n−1 , NP ′ , V1 , . . . Vn }, δ, q0 , {q3 }i with the following transitions δ:7 6 Its set-local variant (Weir, 1988), which may not suffice for analyses of scrambling (Rambow et al., 1992), though see Xia and Bleam (2000) for discussion, is weakly equivalent to simple range concatenation grammar whose universal recognition problem can be solved in deterministic time O(|G|n6k ), where k, intuitively, is the number of (possibly scrambled) complements a verb may take. The complexity is to be precise O(|G|n2k(l+1) ) where l is the maximum number of RHS nonterminals/predicates. See Boullier (1998) for an example of a parsing algorithm, applicable via the conversion described in Weir (1988). Set-local MCTAG is more succinct than simple range concatenation grammar, however, and its universal recognition problem can b"
W09-4626,W03-3017,0,0.270508,"on the read-first strategy. The third provides a bit of background on our use of bags. Aho and Corasick (1975) design a class of automata for bibliographic search in which transitions are replaced by a function g : Q × Σ → Q that maps pairs of states and input symbols into states or the failure message fail. There are no empty transitions, i.e. λ 6∈ Σ; instead a failure function f : Q → Q is consulted whenever g returns fail. It is not difficult to see that this is equivalent to a read-first strategy. The read-first strategy is also related to work on deterministic shift-reduce parsers, e.g. Nivre (2003) for projective dependency grammars. A projective dependency grammar annotates a finite string w1 . . . wn with directed edges E, i.e. governor-dependent relations, such that the string positions, decorated by words, and the edges form an acyclic connected graph G = h{w1 . . . wn }, Ei in which each node has at most one governor and the edges are wellnested. Call such a graph a projective dependency graph. The deterministic shift-reduce parser introduced in Nivre (2003) begins with a 3-tuple hnil, λ, ∅i, in which the first element is the empty stack and the third element is the empty graph, an"
W09-4626,J93-4002,0,\N,Missing
W09-4627,E91-1005,0,0.52923,"such strucures in TIGER and NeGra (Maier and Søgaard, 2008). The HPSG treebanks (Redwoods and BulTreeBank) also contain context-sensitive derivation structures (and beyond). The obvious question to ask now is: Are there less complex logics that can be used to correct and query contextsensitive treebanks? This paper introduces a polyadic modal logic called decharge logic. Its model checking problem can be solved in low polynomial time; a model checking algorithm is spelled out. It is shown that decharge logic captures context-sensitive nonlocal multicomponent tree-adjoining grammars (MCTAGs) (Becker et al., 1991) in the following sense: For each non-local MCTAG G, there exists a decharge logic D such that ω ∈ L(G) iff ∃M.M |=D ω, i.e. if a string is recognized by the grammar G it is satisfiable in the corresponding logic. D is thus a model-theoretic characterization of G. Nonlocal MCTAG is context-sensitive, but not mildly context-sensitive (Rambow and Satta, 1992), and its fixed and universal recognition problems are NP-complete. Head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994) is strictly more expressive, i.e. it is possible to reconstruct nonlocal MCTAGs in the HPSG formalism (Sø"
W09-4627,W07-1512,0,0.0265478,"nks (with some qualifications). The model checking procedures can also be used in heuristics-based parsing. 1 Introduction First order logics and monadic second order logics have been used to query standard treebanks of context-free derivation structures (Kepser, 2004). The model checking problems for both logics are known to be PSPACE-complete (Blackburn et al., 2001), however. Moreover, treebanks are now being constructed that replace context-free derivation structures with context-sensitive ones, incl. The Prague Dependency Treebank (Hajiˇcová et al., 2001), The Danish Dependency Treebank (Buch-Kromann, 2007), The LinGO Redwoods Treebank (English) (Oepen et al., 2002), and BulTreeBank (Simov et al., 2004). Maier and Søgaard (2008) show that even German standard treebanks such as TIGER and NeGra contain mildly context-sensitive derivation structures. The dependency treebanks also use mildly ∗ The model checking procedure described in this paper uses constructs from a model checking procedure introduced in joint work with Martin Lange. Thanks also to Laura Kallmeyer, Timm Lichte and Wolfgang Maier for introducing me to various extensions of tree-adjoining grammar, incl. nonlocal MCTAG. Kristiina Jok"
W09-4627,P07-1021,0,0.0148127,"008) show that even German standard treebanks such as TIGER and NeGra contain mildly context-sensitive derivation structures. The dependency treebanks also use mildly ∗ The model checking procedure described in this paper uses constructs from a model checking procedure introduced in joint work with Martin Lange. Thanks also to Laura Kallmeyer, Timm Lichte and Wolfgang Maier for introducing me to various extensions of tree-adjoining grammar, incl. nonlocal MCTAG. Kristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 190–197 context-sensitive derivation structures (Kuhlmann and Möhle, 2007); the frequency of non-contextfree structures in these treebanks is estimated in Nivre (2006) and is similar to the frequency of such strucures in TIGER and NeGra (Maier and Søgaard, 2008). The HPSG treebanks (Redwoods and BulTreeBank) also contain context-sensitive derivation structures (and beyond). The obvious question to ask now is: Are there less complex logics that can be used to correct and query contextsensitive treebanks? This paper introduces a polyadic modal logic called decharge logic. Its model checking problem can be solved in low polynomial time; a model checking algorithm is sp"
W09-4627,T75-2001,0,0.456641,"generalized to a few restricted variants in Champollion (2007), while Søgaard et al. (2007) present a (weaker) proof of the NPhardness of the universal recognition problem that is generalized to all variants of MCTAG. It follows from the linear upper bound on the size of derivation structures that the universal recognition problem can also be solved in nondeterministic linear space, which also implies that nonlocal multicomponent tree-adjoining languages can be recognized by linear bounded automata. Since any language that can be represented by a linear bounded automaton is context-sensitive (Landweber, 1963), it holds that nonlocal MCTAG is context-sensitive. It also follows from the result obtained in this paper, namely that model checking can be done in low polynomial time, that the universal recognition problem is in NP and thereby NP-complete. It is possible to nondeterministically guess a derivation structure linear in the length of the input string and verify it in low polynomial time. On the other hand it is easy to prove that nonlocal MCTAG is not mildly context-sensitive; see also Rambow and Satta (1992). Consider the grammar with the auxiliary tree set:  S S S   , , b S* c S*   a S"
W09-4627,E89-1024,0,0.0839956,"ed, but this is no longer the case. It is, unlike nonlocal MCTAG, supposed to be model-theoretic. Consequently, logical formalizations already exist. Conventionally, an HPSG grammar is defined as a tuple hhTypes, ⊑i, Principlesi, where hTypes, ⊑i is the inheritance hierarchy, a finite bounded complete partial order, and Principles is a set of linguistic principles. The linguistic principles correspond intuitively to generative rules, but are constraints over a set of legitimate derivation structures. The inheritance hierarchy is formally simple and can be reconstructed in propositional logic (Moens et al., 1989). Consequently, the tricky part is the linguistic principles. The main challenges are set saturation, covered in extended decharge logic by the decharge operator, and union of sets. Note that set union cannot be expressed by the decharge operator. Example 4.1. An example of a linguistic principle in HPSG that uses set union is the Nonlocal Feature Principle (Pollard and Sag, 1994): For each nonlocal feature, the INHER ITED value on the mother is the union of the INHERITED values on the daughters minus the TO - BIND value on the head daughter. In Pollard and Sag (1994), there are three nonlocal"
W09-4627,E06-1010,0,0.0291545,"ivation structures. The dependency treebanks also use mildly ∗ The model checking procedure described in this paper uses constructs from a model checking procedure introduced in joint work with Martin Lange. Thanks also to Laura Kallmeyer, Timm Lichte and Wolfgang Maier for introducing me to various extensions of tree-adjoining grammar, incl. nonlocal MCTAG. Kristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 190–197 context-sensitive derivation structures (Kuhlmann and Möhle, 2007); the frequency of non-contextfree structures in these treebanks is estimated in Nivre (2006) and is similar to the frequency of such strucures in TIGER and NeGra (Maier and Søgaard, 2008). The HPSG treebanks (Redwoods and BulTreeBank) also contain context-sensitive derivation structures (and beyond). The obvious question to ask now is: Are there less complex logics that can be used to correct and query contextsensitive treebanks? This paper introduces a polyadic modal logic called decharge logic. Its model checking problem can be solved in low polynomial time; a model checking algorithm is spelled out. It is shown that decharge logic captures context-sensitive nonlocal multicomponent"
W11-1109,E09-1005,0,0.0264362,"supervised dependency parsing is that the nodes near the root in a dependency structure are in some sense the most important ones. Semantically, the nodes near the root typically express the main predicate and its arguments. Iterative graph-based ranking (Page and Brin, 1998) was first used to rank webpages according to their centrality, but the technique has found wide application in natural language processing. Variations of the algorithm presented in Page and Brin (1998) have been used in keyword extraction and extractive summarization (Mihalcea and Tarau, 2004), word sense disambiguation (Agirre and Soroa, 2009), and abstractive summarization (Ganesan et al., 2010). In this paper, we use it as the first step in a two-step unsupervised dependency parsing procedure. The parser assigns a dependency structure to a sequence of words in two stages. It first decorates the n nodes of what will become our dependency structure with word forms and distributional clusters, constructs a directed acyclic graph from the nodes in O(n2 ), and ranks the nodes using iterative graph-based ranking. Subsequently, it constructs a tree from the ranked list of words using a simple O(n log n) parsing algorithm. This section d"
W11-1109,D10-1118,0,0.014664,"et al. (2010) try to penalize models with a large number of distinct dependency types by using sparse posteriors. They evaluate their system on 11 treebanks from the CoNLL 2006 Shared Task and the Penn-III treebank and achieve state-of-the-art performance. An exception to using linguistically-biased priors is Spitkovsky et al. (2009) who use predictions on sentences of length n to initialize search on sentences of length n + 1. In other words, their method requires no manual tuning and bootstraps itself on increasingly longer sentences. A very different, but interesting, approach is taken in Brody (2010) who use methods from unsupervised word alignment for unsupervised dependency parsing. In particular, he sees dependency parsing as directional alignment from a sentence (possible dependents) to itself (possible heads) with the modification that words cannot align to themselves; following Klein and Manning (2004) and the subsequent papers mentioned above, Brody (2010) considers sequences of POS tags rather than raw text. Results are below state-of-the-art, but in some cases 62 better than the DMV model. 2 Ranking dependency tree nodes The main intuition behind our approach to unsupervised depe"
W11-1109,W06-2920,0,0.198637,"ss by explicitly disregarding the root node once we have attached the node with highest rank to it (line 6–7). Our parsing algorithm does not guarantee projectivity, since the iterative graph-based ranking of nodes can permute the nodes in any order. 4 Experiments We use exactly the same experimental set-up as Gillenwater et al. (2010). The edge model was developed on development data from the English Penn-III treebank (Marcus et al., 1993), and we evaluate on Sect. 23 of the English treebanks and the test sections of the remaining 11 treebanks, which were all used in the CoNLL-X Shared Task (Buchholz and Marsi, 2006). Gillenwater et al. (2010) for some reason did not evaluate on the Arabic and Chinese treebanks also used in the shared task. We also follow Gillenwater et al. (2010) in only evaluating our parser on sentences of at most 10 non-punctuation words and in reporting unlabeled attachment scores excluding punctuation. 65 4.1 Strictly unsupervised dependency parsing We first evaluate the strictly unsupervised parsing model that has no access to POS information. Since we are not aware of other work in strictly unsupervised multi-lingual dependency parsing, so we compare against the best structural ba"
W11-1109,P09-1041,0,0.0285561,"Missing"
W11-1109,W05-1504,0,0.0228744,"of the edge assignments discussed below may seem rather heuristic. The edge template was developed on development data from the English Penn-III treebank (Marcus et al., 1993). Our edge selection was incremental considering first an extended set of candidate edges with arbitrary parameters and then considering each edge type at a time. If the edge type was helpful, we optimized any possible parameters (say context windows) and went on to the next edge type: otherwise we disregarded it.3 Following data set et al. (2010), we apply the best setting for English to all other languages. Vine edges. Eisner and Smith (2005) motivate a vine parsing approach to supervised dependency parsing arguing that language users have a strong preference for short dependencies. Reflecting preference for short dependencies, we first add links between all words and their neighbors and neighbors’ neighbors. This also guarantees that the final graph is connected. Keywords and closed class words. We use a keyword extraction algorithm without stop word lists to extract non-content words and the most important content words, typically nouns. The algorithm is a crude simplification of TextRank (Mihalcea and Tarau, 2004) that does not"
W11-1109,P10-2036,0,0.145792,"and since our models are much smaller, parsing will be very fast. The parser assigns a dependency structure to a sequence of words in two stages. It first decorates the n nodes of what will become our dependency structure with word forms and distributional clusters, constructs a directed acyclic graph from the nodes in O(n2 ), and ranks the nodes using iterative graph-based ranking (Page and Brin, 1998). Subsequently, it constructs a tree from the ranked list of words using a simple O(n log n) parsing algorithm. Our parser is evaluated on the selection of 12 dependency treebanks also used in Gillenwater et al. (2010). We consider two cases: parsing raw text and parsing text with information about POS. Strictly unsupervised dependency parsing is of course a more difficult problem than unsupervised dependency parsing of manually annotated POS sequences. Nevertheless our strictly unsupervised parser, which only sees word forms, performs significantly better than structural baselines, and it outperforms the standard POS-informed DMV-EM model (Klein and Manning, 2004) on 3/12 languages. The full parser, which sees manually annotated text, is competitive to state-of-the-art models such as EDMV PR AS 140 (Gillen"
W11-1109,P04-1061,0,0.798365,"ked list of words using a simple O(n log n) parsing algorithm. Our parser is evaluated on the selection of 12 dependency treebanks also used in Gillenwater et al. (2010). We consider two cases: parsing raw text and parsing text with information about POS. Strictly unsupervised dependency parsing is of course a more difficult problem than unsupervised dependency parsing of manually annotated POS sequences. Nevertheless our strictly unsupervised parser, which only sees word forms, performs significantly better than structural baselines, and it outperforms the standard POS-informed DMV-EM model (Klein and Manning, 2004) on 3/12 languages. The full parser, which sees manually annotated text, is competitive to state-of-the-art models such as EDMV PR AS 140 (Gillenwater et al., 2010).1 1.1 Preliminaries The observed variables in unsupervised dependency parsing are a corpus of sentences s = s1 , . . . , sn where each word wj in si is associated with a POS tag pj . The hidden variables are dependency structures t = t1 , . . . , tn where si labels the vertices of ti . Each vertex has a single incoming edge, possibly except one called the root of the tree. In this work and in most other work in dependency parsing,"
W11-1109,J93-2004,0,0.0453731,"The text graph is now constructed by adding different kinds of directed edges between nodes. The edges are not weighted, but multiple edges between nodes will make transitions between these nodes in 2 http://www.cs.berkeley.edu/∼pliang/software/browncluster-1.2.zip iterative graph-based ranking more likely. The different kinds of edges play the same role in our model as the rule templates in the DMV model, and they are motivated below. Some of the edge assignments discussed below may seem rather heuristic. The edge template was developed on development data from the English Penn-III treebank (Marcus et al., 1993). Our edge selection was incremental considering first an extended set of candidate edges with arbitrary parameters and then considering each edge type at a time. If the edge type was helpful, we optimized any possible parameters (say context windows) and went on to the next edge type: otherwise we disregarded it.3 Following data set et al. (2010), we apply the best setting for English to all other languages. Vine edges. Eisner and Smith (2005) motivate a vine parsing approach to supervised dependency parsing arguing that language users have a strong preference for short dependencies. Reflecti"
W11-1109,H05-1066,0,0.0653863,"Missing"
W11-1109,D10-1120,0,0.162463,"hidden variables are dependency structures t = t1 , . . . , tn where si labels the vertices of ti . Each vertex has a single incoming edge, possibly except one called the root of the tree. In this work and in most other work in dependency parsing, we introduce an artificial root node so that all vertices decorated by word forms have an incoming edge. A dependency structure such as the one in Figure 1 is thus a tree decorated with labels and augmented with a linear order on the nodes. Each edge (i, j) is referred to as a dependency between a head word wi and a dependent word wj and sometimes 1 Naseem et al. (2010) obtain slightly better results, but only evaluate on six languages. They made their code public, though: http://groups.csail.mit.edu/rbg/code/dependency/ 61 written wi → wj . Let w0 be the artificial root of the dependency structure. We use →+ to denote the transitive closure on the set of edges. Both nodes and edges are typically labeled. Since a dependency structure is a tree, it satisfies the following three constraints: A dependency structure over a sentence s : w1 , . . . , wn is connected, i.e.: ∀wi ∈ s.w0 →+ wi A dependency structure is also acyclic, i.e.: ¬∃wi ∈ s.wi →+ wi Finally, a"
W11-1109,P09-1040,0,0.0269089,"nated by wi , i.e. wi →+ wk . Intuitively, a projective dependency structure contains no crossing edges. Projectivity is not a necessary property of dependency structures. Some dependency structures are projective, others are not. Most if not all previous work in unsupervised dependency parsing has focused on projective dependency parsing, building on work in context-free parsing, but our parser is guaranteed to produce well-formed nonprojective dependency trees. Non-projective parsing algorithms for supervised dependency parsing have, for example, been presented in McDonald et al. (2005) and Nivre (2009). 1.2 Related work Dependency Model with Valence (DMV) by Klein and Manning (2004) was the first unsupervised dependency parser to achieve an accuracy for manually POS-tagged English above a right-branching baseline. DMV is a generative model in which the sentence root is generated and then each head recursively generates its left and right dependents. For each si ∈ s, ti is assumed to have been built the following way: The arguments of a head h in direction d are generated one after another with the probability that no more arguments of h should be generated in direction d conditioned on h, d"
W11-1109,P07-1049,0,0.060676,"ilable, and a scenario in which parsing relies only on word forms and distributional clusters. Our approach is competitive to state-of-the-art in both scenarios. 1 Introduction The standard method in unsupervised dependency parsing is to optimize the overall probability of the corpus by assigning trees to its sentences that capture general patterns in the distribution of part-ofspeech (POS). This happens in several iterations over the corpus. This method requires clever initialization, which can be seen as a kind of minimal supervision. State-of-the-art unsupervised dependency parsers, except Seginer (2007), also rely on manually annotated text or text processed by supervised POS taggers. Since there is an intimate relationship between POS tagging and dependency parsing, the POS tags can also be seen as a seed or as partial annotation. Inducing a model from the corpus is typically a very slow process. Unsupervised dependency parsers do not achieve the same quality as supervised or semi-supervised parsers, but in some situations precision may be less important compared to the cost of producing manually annotated data. Moreover, unsupervised dependency parsing is attractive from a theoretical poin"
W11-1109,P05-1044,0,0.0248669,"e root is generated and then each head recursively generates its left and right dependents. For each si ∈ s, ti is assumed to have been built the following way: The arguments of a head h in direction d are generated one after another with the probability that no more arguments of h should be generated in direction d conditioned on h, d and whether this would be the first argument of h in direction d. The POS tag of the argument of h is generated given h and d. Klein and Manning (2004) use expectation maximization (EM) to estimate probabilities with manually tuned linguistically-biased priors. Smith and Eisner (2005) use contrastive estimation instead of EM, while Smith and Eisner (2006) use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training. Cohen et al. (2008) use Bayesian priors (Dirichlet and Logistic Normal) with DMV. All of the above approaches to unsupervised dependency parsing build on the linguistically-biased priors introduced by Klein and Manning (2004). In a similar way Gillenwater et al. (2010) try to penalize models with a large number of distinct dependency types by using sparse posteriors. They evaluate their system on"
W11-1109,P06-1072,0,0.0143529,"d right dependents. For each si ∈ s, ti is assumed to have been built the following way: The arguments of a head h in direction d are generated one after another with the probability that no more arguments of h should be generated in direction d conditioned on h, d and whether this would be the first argument of h in direction d. The POS tag of the argument of h is generated given h and d. Klein and Manning (2004) use expectation maximization (EM) to estimate probabilities with manually tuned linguistically-biased priors. Smith and Eisner (2005) use contrastive estimation instead of EM, while Smith and Eisner (2006) use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training. Cohen et al. (2008) use Bayesian priors (Dirichlet and Logistic Normal) with DMV. All of the above approaches to unsupervised dependency parsing build on the linguistically-biased priors introduced by Klein and Manning (2004). In a similar way Gillenwater et al. (2010) try to penalize models with a large number of distinct dependency types by using sparse posteriors. They evaluate their system on 11 treebanks from the CoNLL 2006 Shared Task and the Penn-III treebank"
W11-1109,D09-1086,0,0.0455825,"Missing"
W11-1109,W10-2902,0,0.0510963,"Missing"
W11-1109,spreyer-etal-2010-training,0,0.0273382,"Missing"
W11-1109,W04-3252,0,\N,Missing
W11-1109,C10-1039,0,\N,Missing
W11-1305,D10-1029,0,0.0282494,"or ”hot dog” we thus generate ”hot terrier” and ”warm dog”, but not ”warm terrier”. Specifically, PARAPHR≥100 means 2 http://www.cs.berkeley.edu/∼pliang/software/ http://wordnet.princeton.edu/ 4 c 1996, 2008 by University of GermaNet Copyright T¨ubingen. 3 feat that at least one of the alternative compounds has a document count of more than 100 in the corpus. PARAPHRav is the average count for all paraphrases, PARAPHRsum is the sum of these counts, and PARAPHRrel is the average count for all paraphrases over the count of the word pair in question. 2.4 H YPH The H YPH features were inspired by Bergsma et al. (2010). It was only used for English. Specifically, we used the relative frequency of hyphenated forms as features. For adjective-noun pairs we counted the number of hyphenated occurrences, e.g. ”front-page”, and divided that number by the number of non-hyphenated occurrences, e.g. ”front page”. For subject-verb and object-verb pairs, we add -ing to the verb, e.g. ”information-collecting”, and divided the number of such forms with nonhyphenated equivalents, e.g. ”information collecting”. 2.5 T RANS -L EN The intuition behind our bilingual features is that non-compositional words typically translate"
W11-1305,P10-4006,0,0.0934899,"s endocentric, but an endocentric compound need not be highly compositional. For example, the distribution of ”olive oil”, which is endocentric and highly compositional, is very similar to the distribution of ”oil”, the head word. On the other hand, ”golden age” which is ranked as highly non-compositional in the training data, is certainly endocentric. The distribution of ”golden age” is not very different from that of ”age”. We used COALS (Rohde et al., 2009) to calculate word distributions. The COALS algorithm builds a word-to-word semantic space from a corpus. We used the implementation by Jurgens and Stevens (2010), generating the semantic space from the Wacky corpora for English and German with duplicate sentences removed and low-frequency words substituted by dummy symbols. The word pairs have been fed to COALS as compounds that have to be treated as single tokens, and the semantic space has been generated and reduced using singular value decompositon. The vectors for w1 , w2 and ”w1 w2 ” are calculated, and we compute the cosine distance between the semantic space vectors for the word pair and its parts, and between the parts themselves, namely for ”w1 w2 ” and w1 , for ”w1 w2 ” and w2 , and for w1 a"
W11-1305,2005.mtsummit-papers.11,0,0.0643952,"arts. The compositionality relation is not defined more precisely, however, and this may in part explain why compositionality prediction seems frustratingly hard. 2 Introduction The challenge in the DiSCo 2011 shared task is to estimate and predict the semantic compositionality of word pairs. Specifically, the data set consists of adjective-noun, subject-verb and object-verb pairs in English and German. The organizers also provided the Wacky corpora for English and German with lowercased lemmas.1 In addition, we also experimented with wordnets and using Europarl corpora for the two languages (Koehn, 2005), but none of the features based on these resources were used in the final submission. Semantic compositionality is an ambiguous term in the linguistics litterature. It may refer to the position that the meaning of sentences is built from 1 http://wacky.sslmit.unibo.it/ Features Many of our features were evaluated with different amounts of slop. The slop parameter permits nonexact matches without resorting to language-specific shallow patterns. The words in the compounds are allowed to move around in the sentence one position at a time. The value of the parameter is the maximum number of steps"
W11-2155,P06-3002,0,0.075889,"Missing"
W11-2155,J92-4003,0,0.27917,"Missing"
W11-2155,W11-2123,0,0.0120337,"aracteristics4 : Lang cs de en es fr Corpus LCC Wortschatz Medline 2004 LCC LCC # Sents 4M 40 M 34 M 4.5 M 3M # Tags 539 396 480 415 359 4 Results For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al., 2010]5 , with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus. 3 For the unsupervised word clusters, 5-gram language models were used as well, built from tagged versions of the same corpora. All language models were binarised and loaded using KenLM [Heaﬁeld, 2011]. Minimum error rate training (MERT) was used to optimise parameters on both baseline and factored models against the 2008 news test set, as suggested on the shared task website6 . All phrase tables were ﬁltered and binarised for the development and testing corpora during tuning and testing, respectively. Seeing that the preparation of the raw corpora, word clustering models, factored corpora, language models, as well as training, optimization and evaluation of the various models was a rather involved, yet repetitive process, we took a stab at making a GNU Makeﬁle-based approach for automated"
W11-2155,P10-1040,0,0.0209744,"apparent patterns should be taken with a pinch of salt. Although the qualities suggested can be expected to relate to distributional properties that the clusters reﬂect, exceptional members are perhaps to be expected. In the present work, we went with the pre-trained models for jUnsupos3 , which have the following characteristics4 : Lang cs de en es fr Corpus LCC Wortschatz Medline 2004 LCC LCC # Sents 4M 40 M 34 M 4.5 M 3M # Tags 539 396 480 415 359 4 Results For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al., 2010]5 , with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus. 3 For the unsupervised word clusters, 5-gram language models were used as well, built from tagged versions of the same corpora. All language models were binarised and loaded using KenLM [Heaﬁeld, 2011]. Minimum error rate training (MERT) was used to optimise parameters on both baseline and factored models against the 2008 news test set, as suggested on the shared task website6 . All phrase tables were ﬁltered and binarised for the development and testing corpora during tuning and te"
W11-2906,P07-1056,0,0.116095,"Missing"
W11-2906,P11-2120,1,0.737265,"earning and structural correspondence learning were used by participants in the CoNLL 2007 Shared Task, none of the participants used instance-weighting techniques. In this paper, we follow suggestions in the related literature on learning under sample selection bias to transform the density ratio estimation problem in co-variate shift into a problem of predicting whether an instance is from the source domain or from the target domain (Zadrozny, 2004; Bickel and Scheffer, 2007). We show how to do this in the context of graph-based and transition-based dependency parsing. Related work includes Søgaard (2011) who uses perplexity per word to select the source data most similar to the target data, so a form of instance weighting with weights 0 and 1, but applies the technique to cross-language adaptation of dependency parsers; but also Plank and van Noord (2011) who in a similar fashion use topic similarity measures to select articles rather than sentences. 2.2 Graph-Based Dependency Parsing Graph-based dependency parsing is a heterogeneous family of approaches to the dependency parsing algorithms, each of which couples a learning algorithm and a parsing algorithm. Some of these algorithms assume de"
W11-2906,P09-1076,0,0.0217847,"Missing"
W11-2906,W06-2920,0,0.0672191,"the correct and predicted structures is less aggressively enforced when learning from distant data points. This is achieved by weighting the loss of incorrect classifications by the probability that the sentence was sampled from the target domain. 2.3 3 Data We evaluate our instance-weighted parsers on two domain adaptation data sets from English and Danish annotated corpora, one of which (Danish) has not previously been used in the literature. The Danish corpus is a balanced corpus, annotated building the Danish Dependency Treebank (DD (Buch-Kromann, 2003) and used in the CoNLLX Shared Task (Buchholz and Marsi, 2006). The DDT comes with metadata revealing the original source of each sentence. This metadata was used to split the DDT into four domains: law (77 sent.), literature (lit; 984 sent.), magazines (magz; 190 sent.) and newspapers (news; 5052 sent.). The second dataset was also used for the CoNLL 2007 Shared Task on domain adaptation Transition-Based Dependency Parsing Transition-based parsing reduces the problem of finding the most likely dependency tree for a sentence to a series of classification problems by seeing parsing as transitions between configurations. Parsing is incremental and left-to-"
W11-2906,D07-1082,0,0.0267577,"cally related to differences in textual domains, computational 43 Proceedings of the 12th International Conference on Parsing Technologies, pages 43–47, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. narios where there is a bias in P (y|x). Others have proposed using priors to encode knowledge about one domain in a model induced from data in another domain, or they have promoted frequent target domain classes if they were less frequent in the source domain. Such approaches assume a bias in P (y) and have become popular in word sense disambiguation (Zhu and Hovy, 2007), for example, where a particular reading of bank may be much more frequent in some domains rather than others. Classes can be promoted using instance weighting, but instance weighting can also be used to change the marginal distribution of data. The first case is typically referred to as solving class imbalance, while the second case is called covariate shift (Shimodaira, 2000). We will, assuming a bias in P (x), consider the covariate shift scenario. A fourth line of research in domain adaptation applies semi-supervised or transductive learning algorithms to domain adaptation problems, using"
W11-2906,P07-1033,0,0.257529,"Missing"
W11-2906,C96-1058,0,0.011466,"o select the source data most similar to the target data, so a form of instance weighting with weights 0 and 1, but applies the technique to cross-language adaptation of dependency parsers; but also Plank and van Noord (2011) who in a similar fashion use topic similarity measures to select articles rather than sentences. 2.2 Graph-Based Dependency Parsing Graph-based dependency parsing is a heterogeneous family of approaches to the dependency parsing algorithms, each of which couples a learning algorithm and a parsing algorithm. Some of these algorithms assume dependency trees are projective (Eisner, 1996), while others allow for non-projective dependency trees (McDonald et al., 2005). One approach to graph-based parsing of nonprojective dependency trees is applying minimum spanning tree algorithms to matrices of weighted head-dependent candidate pairs. The learning alOur instance-weighted parsers are evaluated primarily on a new data set, namely a partitioning of the Danish treebank (Buch-Kromann, 2003) into four different textual domains. We do experiments with all pair-wise combinations of the four domain-specific treebanks. Our results are 44 law-lit law-magz law-news lit-laws lit-magz lit-"
W11-2906,D10-1044,0,0.0849694,"Missing"
W11-2906,2005.mtsummit-papers.11,0,0.00881926,"independently and identically (iid) drawn from the same distribution. If the distributions differ, we face what is referred to as sample selection bias in the statistical literature. Sample selection bias is typically ignored in machine learning, but it occurs often in practice. In natural language processing, the problem shows up in almost any real-world application. Machine translation systems are trained on large amounts of parallel text, but typically this text comes from a small set of sources or institutions, e.g. the Europarl corpora of transcribed debates from the European Parliament (Koehn, 2005). Machine translation systems are used to translate many different kinds of texts, however. In machine translation, which can be seen as a structured learning problem of predicting target sentence y given a source sentence x, we typically see a bias in P (y) and P (x), but not in P (y|x). Statistical parsers for English are typically trained on annotated text from the Wall Street Journal corpus of newspaper articles (Marcus et al., 1993), but are used to process many different kinds of text. Since the problem of sample selection bias in natural language processing is typically related to diffe"
W11-2906,J93-2004,0,0.0369388,"rallel text, but typically this text comes from a small set of sources or institutions, e.g. the Europarl corpora of transcribed debates from the European Parliament (Koehn, 2005). Machine translation systems are used to translate many different kinds of texts, however. In machine translation, which can be seen as a structured learning problem of predicting target sentence y given a source sentence x, we typically see a bias in P (y) and P (x), but not in P (y|x). Statistical parsers for English are typically trained on annotated text from the Wall Street Journal corpus of newspaper articles (Marcus et al., 1993), but are used to process many different kinds of text. Since the problem of sample selection bias in natural language processing is typically related to differences in textual domains, computational 43 Proceedings of the 12th International Conference on Parsing Technologies, pages 43–47, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. narios where there is a bias in P (y|x). Others have proposed using priors to encode knowledge about one domain in a model induced from data in another domain, or they have promoted frequent target domain classes if th"
W11-2906,N10-1004,0,0.061531,"Missing"
W11-2906,H05-1066,0,0.0590847,"Missing"
W11-2906,P11-1157,0,0.0214889,"Missing"
W11-2906,D07-1111,0,0.0259669,"Missing"
W11-2906,D07-1112,0,\N,Missing
W11-2906,I08-2097,0,\N,Missing
W11-2906,D07-1096,0,\N,Missing
W11-4628,W99-0606,0,0.118457,"Missing"
W11-4628,J07-4002,0,0.0278763,"Missing"
W11-4628,J04-4004,0,0.0275091,"2008). This setting led to the best results on the development data. We trained two different parsers on the converted PP attachment dataset, namely MaltParser (Nivre et al., 2007) and MSTParser (McDonald et al., 2005). We report results for the two parsers with default parameters and a result for MaltParser with a feature model partially optimized on development data; all results are listed in Figure 6. The optimized feature model is presented in Figure 7.5 The third column reports significance compared to the results in Toutanova et al. (2004), using χ2 test. The result of the Bikel parser (Bikel, 2004) is taken directly from Atterer and Sch¨utze (2007). It follows that state-of-the-art dependency parsers are not significantly worse than state-of-the-art PP attachment classifiers. This questions the usefulness of PP attachment classifiers in statistical parsing. 6 Conclusions We have contributed to the PP attachment literature in two ways. First we have presented two new algorithms that equal state-of-the-art in their performance on the standard Wall Street Journal dataset. One is based on Bayesian networks, the other is based on conditional random fields. Second we have strengthened the cla"
W11-4628,A00-2018,0,0.236504,"Missing"
W11-4628,W95-0103,0,0.243935,"Missing"
W11-4628,I05-1017,0,0.0352914,"Missing"
W11-4628,P08-1068,0,0.0534434,"Missing"
W11-4628,H05-1066,0,0.118456,"Missing"
W11-4628,H94-1048,0,0.335495,"of the main challenges in parsing has for a long time been assumed to be the resolution of ambiguity. One frequently studied type of ambiguity is prepositional phrase (PP) attachment. Given a quadruple of a verb, a direct object, a preposition and a prepositional complement (the head of the NP2 embedded in the PP), PP attachment – or PP re-attachment – is the task of determining whether the PP should attach to the verb (V) or the direct object (N). PP attachment is thus construed as a binary classification problem, typically with labels N and V. The standard features for PP-attachment used in Ratnaparkhi et al. (1994) and subsequent studies are listed in Figure 1. The seven features are the ones in rows 2–8. A distributional cluster is a set of words that have similar distributions according to a hierarchical clustering algorithm, typically based on probabilities in a bigram language model. The granularity of clusters varies, but we will use a 1000 clusters in our experiments below. Example. To see the complexity of this learning problem, consider the following four examples: (1) (Andy Warhol) painted paintings with 3D-glasses. (2) (Andy Warhol) painted [portraits with 3D-glasses]. (3) (Andy Warhol) painte"
W11-4628,P98-2177,0,0.131382,"Missing"
W11-4628,P10-1040,0,0.0594909,"Missing"
W12-1910,P10-2036,0,0.237264,"nglish Penn-III treebank (Marcus et al., 1993) and first presented in Søgaard (2012): Results in unsupervised dependency parsing are typically compared to branching baselines and the DMV-EM parser of Klein and Manning (2004). State-of-the-art results are now well beyond these baselines. This paper describes two simple, heuristic baselines that are much harder to beat: a simple, heuristic algorithm recently presented in Søgaard (2012) and a heuristic application of the universal rules presented in Naseem et al. (2010). Our first baseline (RANK) outperforms existing baselines, including PR-DVM (Gillenwater et al., 2010), while relying only on raw text, but all submitted systems in the Pascal Grammar Induction Challenge score better. Our second baseline (RULES), however, outperforms several submitted systems. • Short edges. To favor short dependencies, we add links between all words and their neighbors. This makes probability mass flow from central words to their neighboring words. 1 RANK: a simple heuristic baseline Our first baseline RANK is a simple heuristic baseline that does not rely on part of speech. It only assumes raw text. The intuition behind it is that a dependency structure encodes something rel"
W12-1910,D10-1120,0,0.415897,"ansitions between them more likely. The edge template was validated on development data from the English Penn-III treebank (Marcus et al., 1993) and first presented in Søgaard (2012): Results in unsupervised dependency parsing are typically compared to branching baselines and the DMV-EM parser of Klein and Manning (2004). State-of-the-art results are now well beyond these baselines. This paper describes two simple, heuristic baselines that are much harder to beat: a simple, heuristic algorithm recently presented in Søgaard (2012) and a heuristic application of the universal rules presented in Naseem et al. (2010). Our first baseline (RANK) outperforms existing baselines, including PR-DVM (Gillenwater et al., 2010), while relying only on raw text, but all submitted systems in the Pascal Grammar Induction Challenge score better. Our second baseline (RULES), however, outperforms several submitted systems. • Short edges. To favor short dependencies, we add links between all words and their neighbors. This makes probability mass flow from central words to their neighboring words. 1 RANK: a simple heuristic baseline Our first baseline RANK is a simple heuristic baseline that does not rely on part of speech."
W12-1910,P11-1067,0,0.0826371,"Missing"
W12-1910,W04-3252,0,\N,Missing
W12-1910,J93-2004,0,\N,Missing
W12-2507,P07-1056,0,0.0497878,"and lemmatization 54 Workshop on Computational Linguistics for Literature, pages 54–58, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics both lead to impoverished results. We also observe that stop words are extremely useful for quotation mining. Polarity classification is the task of determining whether an opinionated text about a particular topic, say a user review of a product, is positive or negative. Polarity classification is different from quotation mining in that there is a small set of strong predictors of polarity (pivot features) (Wang et al., 2005; Blitzer et al., 2007), e.g. the polarity words listed in subjectivity lexica, including opinionated adjectives such as good or awful. The meaning of polarity words is context-sensitive, however, so context is extremely important when modeling polarity. Some quotes are expressions of opinion, and there has been some previous research on polarity classification in direct quotations (not famous quotes). Balahur et al. (2009) present work on polarity classification of newspaper quotations, for example. They use an SVM classifier on a bag-of-words representation of direct quotes in the news, but using only words taken"
W12-2507,R11-1060,0,0.0510418,"Missing"
W12-2507,U06-1005,0,0.019546,"ts from international politics and letters to the editor. Several resources exist for evaluating topic classifiers such as Reuters 20 Newsgroups. Common baselines are Naive Bayes, logistic regression, or SVM classifiers trained on bag-of-words representations of n-grams with stop words removed. While newspaper articles typically consist of tens or hundreds of sentences, famous quotes typically consist of one or two sentences, and it is interesting to compare quotation mining to work on applying topic classification techniques to short texts or sentences (Cohen et al., 2003; Wang et al., 2005; Khoo et al., 2006). Cohen et al. (2003) and Khoo et al. (2006) classify sentences in email wrt. their role in discourse. Khoo et al. (2006) argue that extending a bag-of-words representation with frequency counts is meaningless in small text and restrict themselves to binary representations. They show empirically that excluding stop words and lemmatization 54 Workshop on Computational Linguistics for Literature, pages 54–58, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics both lead to impoverished results. We also observe that stop words are extremely useful for quotation minin"
W12-2507,W04-3240,0,\N,Missing
W13-3733,C10-1011,0,0.017102,"sts that parser bias cancels out many of the differences between conversion schemes. Section 4 discusses the learnability and derivational perplexity of tree-to-dependency conversion schemes. Section 5 presents a series of experiments, evaluating the downstream performance of conversion schemes in negation scope resolution, sentence compression, statistical machine translation, semantic role labeling and author perspective classification. Section 6 concludes with a discussion of the analyses presented in the previous sections. In our experiments we will use the publicly available MATE parser (Bohnet, 2010). Obviously the downstream performance of a conversion scheme depends on the parsing model chosen and how syntactic features are incorporated in the downstream task, but we do not vary parser or syntactic feature representations in our experiments. 2 Tree-to-dependency conversion schemes such as the Prague School’s Functional Generative Description, Meaning-Text Theory, or Hudson’s Word Grammar. In practice most parsers constrain dependency structures to be tree-like structures such that each word has a single syntactic head, limiting diversity between annotation a bit; but while many dependen"
W13-3733,P11-2031,0,0.0227704,"sentences of the parallel news data that have been parsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with human judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 5.4 Sentence compression Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff4 Davis corpus.5 A widely used baseline for sentence compression experiments is the two models introduced in Knight and Marcu (2002): the noisy-channel model and the decision tree-based model. Both are tree-based methods that find the most likely compressed syntactic tree and outputs the yield of this tree. McDonald et al. (2006) instead u"
W13-3733,N13-1070,1,0.719752,"conversion schemes raises the question of what scheme is better? So does the existence of different parsers relying on different linguistic formalisms, but whose output can be mapped to dependencies (Tsarfaty et al., 2012). But is the question of which is better really a meaningful question? Better at what? Schwartz et al. (2012) propose to evaluate conversion schemes in terms of learnability. We argue that while learnability is relevant to assess the robustness of dependency schemes, the most important parameter when choosing conversion schemes in practice is down-stream performance. We cite Elming et al. (2013), who show that down-stream performance is very sensitive to choice of conversion scheme. We also suggest that derivational perplexity (Søgaard and Haulrich, 2010) is a less biased measure of robustness than learnability – at least the way it is measured in Schwartz et al. (2012). The paper presents (a) an empirical analysis of distance between conversion schemes, (b) an 298 Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 298–307, c 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic Prague, August 27–30, 2013. Clear cases H"
W13-3733,P09-1087,0,0.113552,"nally, annotation interacts with parser bias. Some choices of head status may be easy to learn for transition-based dependency parsers, but comparatively harder for graph-based ones. See McDonald and Nivre (2007) for an analysis of the biases of transition-based and graph-based dependency parsers. Since derivational perplexity is parser-independent we are not sensitive to regularization or parser bias, only to the choice of canonical derivation scheme. 5 Downstream performance Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). Below we introduce five NLP applications where dependency parsing has been succesfully applied: negation resolution, semantic role labeling, statistical machine translation, sentence compression and perspective classification. We will then report on 302 evaluations of the downstream effects of the four conversion schemes in these five applications, first published in Elming et al. (2013). In the five applications we use syntactic features in slightly different wa"
W13-3733,W12-3602,0,0.0179256,"he difficult cases, however, morpho-syntactic evidence is in conflict with the semantic evidence. While auxiliary verbs have the same distribution as finite verbs in head position and share morpho-syntactic properties with them, and govern the infinite main verbs, main verbs seem semantically superior, expressing the main predicate. There may be distributional evidence that complementizers head verbs syntactically, but the verbs seem more important from a semantic point of view. Some authors have distinguished between the notion of functional (distributional) and substantive dependency heads (Ivanova et al., 2012). Tree-to-dependency conversion schemes used to convert constituent-based treebanks into dependency-based ones also take different stands on the difficult cases. In this paper we consider four different conversion schemes: the Yamada-Matsumoto conversion scheme (Yamada) (Yamada and Matsumoto, 2003), the CoNLL (2007) format, the Stanford conversion scheme used in the English Web Treebank (Petrov and McDonald, 2012), and the LTH conversion scheme (Johansson and Nugues, 2007). The Yamada scheme can be replicated by running penn2malt.jar available at http://w3.msi.vxu.se/∼nivre/ research/Penn2Malt"
W13-3733,W10-2910,0,0.012539,"y parsers, but comparatively harder for graph-based ones. See McDonald and Nivre (2007) for an analysis of the biases of transition-based and graph-based dependency parsers. Since derivational perplexity is parser-independent we are not sensitive to regularization or parser bias, only to the choice of canonical derivation scheme. 5 Downstream performance Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). Below we introduce five NLP applications where dependency parsing has been succesfully applied: negation resolution, semantic role labeling, statistical machine translation, sentence compression and perspective classification. We will then report on 302 evaluations of the downstream effects of the four conversion schemes in these five applications, first published in Elming et al. (2013). In the five applications we use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relations as additional information a"
W13-3733,W07-2416,0,0.385719,"umber of cases. Do auxiliary verbs head main verbs? Do prepositions head their nominal com∗ Section 5 is joint work with Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi and Hector Martinez, published at NAACL 2013. plements? And how should we analyze punctuation? Many dependency treebanks are created by automatic conversion from pre-existing constituency treebanks. Since there exist linguistic phenomena whose analyses linguist do not agree on, it comes as no surprise that different conversion schemes have been proposed over the years (Collins, 1999; Yamada and Matsumoto, 2003; Johansson and Nugues, 2007). The output of these schemes differ considerably in their choices concerning head status and dependency relation inventories (Schwartz et al., 2012; Johansson, 2013). The number of languages for which we have several dependency treebanks, is limited (Johansson, 2013), but the availability of different tree-todependency conversion schemes raises the question of what scheme is better? So does the existence of different parsers relying on different linguistic formalisms, but whose output can be mapped to dependencies (Tsarfaty et al., 2012). But is the question of which is better really a meanin"
W13-3733,W08-2123,0,0.0138894,"eatures at all, the models are tested using gold cues. Table 4 shows F1 scores for scopes, events and full negations, where a true positive correctly assigns both scope tokens and events to the rightful cue. The scores are produced using the evaluation script provided by the *SEM organizers. 5.2 Semantic role labeling Semantic role labeling (SRL) is the attempt to determine semantic predicates in running text and la303 bel their arguments with semantic roles. In our experiments we have reproduced the second bestperforming system in the CoNLL 2008 shared task in syntactic and semantic parsing (Johansson and Nugues, 2008).2 The English training data for the CoNLL 2008 shared task were obtained from PropBank and NomBank. For licensing reasons, we used OntoNotes 4.0, which includes PropBank, but not NomBank. This means that our system is only trained to classify verbal predicates. We used the Clearparser conversion tool3 to convert the OntoNotes 4.0 and subsequently supplied syntactic dependency trees using our different conversion schemes. We rely on gold standard argument identification and focus solely on the performance metric semantic labeled F1. 5.3 Statistical machine translation The effect of the differe"
W13-3733,N13-1013,0,0.0187071,"Lapponi and Hector Martinez, published at NAACL 2013. plements? And how should we analyze punctuation? Many dependency treebanks are created by automatic conversion from pre-existing constituency treebanks. Since there exist linguistic phenomena whose analyses linguist do not agree on, it comes as no surprise that different conversion schemes have been proposed over the years (Collins, 1999; Yamada and Matsumoto, 2003; Johansson and Nugues, 2007). The output of these schemes differ considerably in their choices concerning head status and dependency relation inventories (Schwartz et al., 2012; Johansson, 2013). The number of languages for which we have several dependency treebanks, is limited (Johansson, 2013), but the availability of different tree-todependency conversion schemes raises the question of what scheme is better? So does the existence of different parsers relying on different linguistic formalisms, but whose output can be mapped to dependencies (Tsarfaty et al., 2012). But is the question of which is better really a meaningful question? Better at what? Schwartz et al. (2012) propose to evaluate conversion schemes in terms of learnability. We argue that while learnability is relevant to"
W13-3733,P09-2079,0,0.0339182,"for transition-based dependency parsers, but comparatively harder for graph-based ones. See McDonald and Nivre (2007) for an analysis of the biases of transition-based and graph-based dependency parsers. Since derivational perplexity is parser-independent we are not sensitive to regularization or parser bias, only to the choice of canonical derivation scheme. 5 Downstream performance Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). Below we introduce five NLP applications where dependency parsing has been succesfully applied: negation resolution, semantic role labeling, statistical machine translation, sentence compression and perspective classification. We will then report on 302 evaluations of the downstream effects of the four conversion schemes in these five applications, first published in Elming et al. (2013). In the five applications we use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relat"
W13-3733,S12-1042,0,0.0172917,"us (CD),1 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [. . . ] CD-style scopes can be discontinuous and overlapping. Events are a portion of the scope that is semantically negated, with its truth value reversed by the negation cue. The NR system used in this work (Lapponi et al., 2012), one of the best performing systems in the *SEM shared task, is a CRF model for scope resolution that relies heavily on features extracted from dependency graphs. The feature model contains token distance, direction, n-grams of word 1 http://www.clips.ua.ac.be/sem2012-st-neg/data.html R EFERENCE : S OURCE : Yamada: CoNLL: Stanford: LTH: BASELINE : Zum Gl¨uck kam ich beim Strassenbahnfahren an die richtige Stelle . Luckily , on the way to the tram , I found the right place . Gl¨ucklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort . Gl¨ucklicherweise hat auf dem"
W13-3733,W07-0734,0,0.0122462,"best performing system in the WMT 2011 shared task on this dataset. The four experimental systems have reordering models that are trained on the first 25,000 sentences of the parallel news data that have been parsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with human judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 5.4 Sentence compression Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff4 Davis corpus.5 A widely used baseline for sentence compression experiments is the two models introduced in Knight and Marcu (2002): the noisy-channel model and the decision tree-based mo"
W13-3733,J93-2004,0,0.0421646,"Main verb Complementizer Verb Coordinator Conjuncts Preposition Nominal Punctuation Figure 1: Clear and difficult cases in dependency annotation. analysis of the theoretical learnability of conversion schemes, (c) a complexity analysis of conversion schemes in terms of derivational perplexity, and (d) empirical evaluations of the downstream usefulness of conversion schemes. Section 2 introduces a few common conversion schemes and their linguistic differences. In our empirical analyses we will focus on standard conversions of the Wall Street Journal section of the Penn-III treebank of English (Marcus et al., 1993). Section 3 introduces three distance metrics defined over pairs of output dependency structures. The results presented in this section suggests that parser bias cancels out many of the differences between conversion schemes. Section 4 discusses the learnability and derivational perplexity of tree-to-dependency conversion schemes. Section 5 presents a series of experiments, evaluating the downstream performance of conversion schemes in negation scope resolution, sentence compression, statistical machine translation, semantic role labeling and author perspective classification. Section 6 conclu"
W13-3733,E06-1038,0,0.0728598,"Missing"
W13-3733,S12-1035,0,0.0172497,"a lot of information, including the word form of the head, the dependent and the argument candidates, the concatenation of the dependency labels of the predicate, and the labeled dependency relations between predicate and its head, its arguments, dependents or siblings. 5.1 Negation resolution Negation resolution (NR) is the task of finding negation cues, e.g. the word not, and determining their scope, i.e. the tokens they affect. NR has recently seen considerable interest in the NLP community (Morante and Sporleder, 2012; Velldal et al., 2012) and was the topic of the 2012 *SEM shared task (Morante and Blanco, 2012). The data set used in this work, the Conan Doyle corpus (CD),1 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [. . . ] CD-style scopes can be discontinuous and overlapping. Events are a portion of the scope that is semantically negated, with its truth value reversed"
W13-3733,J12-2001,0,0.0176755,"triples (e.g. SUBJ(John, snore)) as features, while the semantic role labeling system conditions on a lot of information, including the word form of the head, the dependent and the argument candidates, the concatenation of the dependency labels of the predicate, and the labeled dependency relations between predicate and its head, its arguments, dependents or siblings. 5.1 Negation resolution Negation resolution (NR) is the task of finding negation cues, e.g. the word not, and determining their scope, i.e. the tokens they affect. NR has recently seen considerable interest in the NLP community (Morante and Sporleder, 2012; Velldal et al., 2012) and was the topic of the 2012 *SEM shared task (Morante and Blanco, 2012). The data set used in this work, the Conan Doyle corpus (CD),1 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [. . . ] CD-style scopes can be discontinuous and overlappin"
W13-3733,P02-1040,0,0.088127,"aseline system was one of the tied best performing system in the WMT 2011 shared task on this dataset. The four experimental systems have reordering models that are trained on the first 25,000 sentences of the parallel news data that have been parsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with human judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 5.4 Sentence compression Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff4 Davis corpus.5 A widely used baseline for sentence compression experiments is the two models introduced in Knight and Marcu (2002): the noisy-channel"
W13-3733,C12-1147,0,0.281456,"igrid Klerke, Emanuele Lapponi and Hector Martinez, published at NAACL 2013. plements? And how should we analyze punctuation? Many dependency treebanks are created by automatic conversion from pre-existing constituency treebanks. Since there exist linguistic phenomena whose analyses linguist do not agree on, it comes as no surprise that different conversion schemes have been proposed over the years (Collins, 1999; Yamada and Matsumoto, 2003; Johansson and Nugues, 2007). The output of these schemes differ considerably in their choices concerning head status and dependency relation inventories (Schwartz et al., 2012; Johansson, 2013). The number of languages for which we have several dependency treebanks, is limited (Johansson, 2013), but the availability of different tree-todependency conversion schemes raises the question of what scheme is better? So does the existence of different parsers relying on different linguistic formalisms, but whose output can be mapped to dependencies (Tsarfaty et al., 2012). But is the question of which is better really a meaningful question? Better at what? Schwartz et al. (2012) propose to evaluate conversion schemes in terms of learnability. We argue that while learnabil"
W13-3733,E12-1006,0,0.0254505,"years (Collins, 1999; Yamada and Matsumoto, 2003; Johansson and Nugues, 2007). The output of these schemes differ considerably in their choices concerning head status and dependency relation inventories (Schwartz et al., 2012; Johansson, 2013). The number of languages for which we have several dependency treebanks, is limited (Johansson, 2013), but the availability of different tree-todependency conversion schemes raises the question of what scheme is better? So does the existence of different parsers relying on different linguistic formalisms, but whose output can be mapped to dependencies (Tsarfaty et al., 2012). But is the question of which is better really a meaningful question? Better at what? Schwartz et al. (2012) propose to evaluate conversion schemes in terms of learnability. We argue that while learnability is relevant to assess the robustness of dependency schemes, the most important parameter when choosing conversion schemes in practice is down-stream performance. We cite Elming et al. (2013), who show that down-stream performance is very sensitive to choice of conversion scheme. We also suggest that derivational perplexity (Søgaard and Haulrich, 2010) is a less biased measure of robustness"
W13-3733,J12-2005,0,0.0132863,"e)) as features, while the semantic role labeling system conditions on a lot of information, including the word form of the head, the dependent and the argument candidates, the concatenation of the dependency labels of the predicate, and the labeled dependency relations between predicate and its head, its arguments, dependents or siblings. 5.1 Negation resolution Negation resolution (NR) is the task of finding negation cues, e.g. the word not, and determining their scope, i.e. the tokens they affect. NR has recently seen considerable interest in the NLP community (Morante and Sporleder, 2012; Velldal et al., 2012) and was the topic of the 2012 *SEM shared task (Morante and Blanco, 2012). The data set used in this work, the Conan Doyle corpus (CD),1 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [. . . ] CD-style scopes can be discontinuous and overlapping. Events are a portion"
W13-3733,N09-1028,0,0.124701,"s with parser bias. Some choices of head status may be easy to learn for transition-based dependency parsers, but comparatively harder for graph-based ones. See McDonald and Nivre (2007) for an analysis of the biases of transition-based and graph-based dependency parsers. Since derivational perplexity is parser-independent we are not sensitive to regularization or parser bias, only to the choice of canonical derivation scheme. 5 Downstream performance Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). Below we introduce five NLP applications where dependency parsing has been succesfully applied: negation resolution, semantic role labeling, statistical machine translation, sentence compression and perspective classification. We will then report on 302 evaluations of the downstream effects of the four conversion schemes in these five applications, first published in Elming et al. (2013). In the five applications we use syntactic features in slightly different ways. While our sta"
W13-3733,W03-3023,0,0.621451,"pread disagreement about a number of cases. Do auxiliary verbs head main verbs? Do prepositions head their nominal com∗ Section 5 is joint work with Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi and Hector Martinez, published at NAACL 2013. plements? And how should we analyze punctuation? Many dependency treebanks are created by automatic conversion from pre-existing constituency treebanks. Since there exist linguistic phenomena whose analyses linguist do not agree on, it comes as no surprise that different conversion schemes have been proposed over the years (Collins, 1999; Yamada and Matsumoto, 2003; Johansson and Nugues, 2007). The output of these schemes differ considerably in their choices concerning head status and dependency relation inventories (Schwartz et al., 2012; Johansson, 2013). The number of languages for which we have several dependency treebanks, is limited (Johansson, 2013), but the availability of different tree-todependency conversion schemes raises the question of what scheme is better? So does the existence of different parsers relying on different linguistic formalisms, but whose output can be mapped to dependencies (Tsarfaty et al., 2012). But is the question of wh"
W13-3733,W02-1001,0,\N,Missing
W13-3733,J03-4003,0,\N,Missing
W13-3733,D07-1096,0,\N,Missing
W14-1601,N13-1070,1,0.862135,"in the absence of bias and with perfect metrics – gives us the level of confidence we expect as a research community, i.e., P P V = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may perform equally well as measured by some metric, but contribute very differently to downstream tasks. Elming et al. (2013) show how parsers trained on different annotation schemes lead to very different downstream results. This suggests that being wrong with respect to a gold standard, e.g., choosing NP analysis over a “correct” DP analysis, may in some cases lead to better downstream performance. See the discussion in Manning (2011) for POS tagging. One simple approach to this problem is to report results across available metrics. If System A improves over System B wrt. most metrics, we obtain significance against the odds. POS taggers and dependency parsers should also be evaluated by their impact on downstream"
W14-1601,W03-0425,0,0.0235909,"Missing"
W14-1601,W05-0909,0,0.0206194,"01 <0.001 0.2020 0.3965 0.0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric"
W14-1601,I11-1100,0,0.0289698,"Missing"
W14-1601,D12-1091,0,0.0568125,"ata sets, such tests seem to be the right choice (Demsar, 2006; Søgaard, 2013). The draw-back of rank-based tests is their relatively weak statistical power. When we reduce scores to ranks, we throw away information, and rank-based tests are therefore relatively conservative, potentially leading to high type 2 error rate ( , i.e., the number of false negatives over trials). An alternative, however, are randomization-based tests such as the bootstrap test (Efron and Tibshirani, 1993) and approximate randomization (Noreen, 1989), which are the de facto standards in NLP. In this paper, we follow Berg-Kirkpatrick et al. (2012) in focusing on the bootstrap test. The bootstrap test is non-parametric and stronger than rank-based testing, i.e., introduces fewer type 2 errors. For small samples, however, it does so at the expense of a 1 In many fields, including NLP, it has become good practice to report actual p-values, but we still need to understand how significance levels relate to the probability that research findings are false, to interpret such values. The fact that we propose a new cut-off level for the ideal case with perfect metrics and no bias does not mean that we do not recommend reporting actual p-values."
W14-1601,W06-1615,0,0.0543626,"Missing"
W14-1601,P07-1034,0,0.0536678,"Missing"
W14-1601,C00-1011,0,0.168794,"Missing"
W14-1601,W04-1013,0,0.0270518,"0.3569 <0.001 <0.001 0.2020 0.3965 0.0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-"
W14-1601,W06-2920,0,0.0650816,"Missing"
W14-1601,C08-1015,0,0.0300507,"Missing"
W14-1601,W03-0423,0,0.0336765,"odels from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely F LORIAN (Florian et al., 2003) and C HIEU -N G (Chieu and Ng, 2003).3 • • • • • • • Table 1: Evaluation data. 3 Experiments Throughout the rest of the paper, we use four running examples: a synthetic toy example and three standard experimental NLP tasks, namely POS tagging, dependency parsing and NER. The toy example is supposed to illustrate the logic behind our reasoning and is not specific to NLP. It shows how likely we are to obtain a low p-value for the difference in means when sampling from exactly the same (Gaussian) distributions. For the NLP setups (2-4), we use off-the-shelf models or available runs, as described next. 3.2 Standard comparisons POS t"
W14-1601,P11-2031,0,0.00842907,"0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting resu"
W14-1601,P02-1040,0,0.113383,"Twitter TA (b) 0.3445 0.3569 <0.001 <0.001 0.2020 0.3965 0.0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers"
W14-1601,P07-1033,0,0.0204031,"Missing"
W14-1601,P11-1157,1,0.595908,"Missing"
W14-1601,Y09-1013,0,0.0190285,"acy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting results, this increases the a priori chance of establishing significance. In POS tagging, most papers report significant improvements over tagging accuracy, but some report significant improvements over tagging accuracy of unknown words, e.g., Denis and Sagot (2009) and Umansky-Pesin et al. (2010). This corresponds to the situation in psychology where researchers cherry-pick between several dependent variables (Simmons et al., 2011), which also increases the chance of finding a significant correlation. UAS <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 Table 3: Parsing p-values (M ALT-L IN VS . S TANFORD -RNN) across LAS and UAS (p < 0.05 gray-shaded). than S TANFORD, but in one case it is the other way around. If we do a Wilcoxon test over the results on the 10 data sets, following the methodology in Demsar (2006) and Søgaard (2013), the difference, which is"
W14-1601,D11-1043,0,0.0493232,"Missing"
W14-1601,W05-0908,0,0.132601,"Missing"
W14-1601,D09-1085,0,0.0396616,"Missing"
W14-1601,P11-1067,0,0.0160959,"Figure 7: PPV for different ↵ (horizontal line is PPV for p = 0.05, vertical line is ↵ for PPV=0.95). could propose a p-value cut-off at p < 0.0025. This is the cut-off that – in the absence of bias and with perfect metrics – gives us the level of confidence we expect as a research community, i.e., P P V = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may perform equally well as measured by some metric, but contribute very differently to downstream tasks. Elming et al. (2013) show how parsers trained on different annotation schemes lead to very different downstream results. This suggests that being wrong with respect to a gold standard, e.g., choosing NP analysis over a “correct” DP analysis, may in some cases lead to better downstream performance. See the discussion in Manning (2011) for POS tagging. One simple approach to this problem is to report results across available metrics. If System"
W14-1601,P13-1045,0,0.00363571,"ASKS Dep. NER al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the L A POS tagger (Tsuruoka et al., 2011) and the S TAN FORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely F LORIAN (Florian et al., 2003) and C HIEU -N G (Chieu and Ng, 2003).3 • • • • • • • Table 1: Evaluation data. 3 Experiments Throughout the rest of the paper, we use four running examples: a synthetic toy example and three standard experimental NLP tasks, namely POS tagging, dependency"
W14-1601,N13-1068,1,0.72957,"nalysis from previous experiments, and research findings often consist in system comparisons showing that System A is better than System B. Effect size, i.e., one system’s improvements over another, can be seen as a random variable. If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 1 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1–10, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics Covariates. Sometimes we may bin our results by variables that are actually predictive of the outcome (covariates) (Simmons et al., 2011). In some subfields of NLP, such as machine translation or (unsupervised) syntactic parsing, for example, it is common to report resul"
W14-1601,N03-1033,0,0.00466178,"ndency parsing. D OMAIN #W ORDS C O NLL 2007 Bio Chem POS 4k 5k • • S WITCHBOARD 4 Spoken 162k • E NGLISH W EB T REEBANK Answers Emails Newsgrs Reviews Weblogs WSJ 29k 28k 21k 28k 20k 40k • • • • • • F OSTER Twitter 3k • C O NLL 2003 News 50k Figure 1: Accuracies of L APOS VS . S TANFORD across 10 data sets. TASKS Dep. NER al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the L A POS tagger (Tsuruoka et al., 2011) and the S TAN FORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003"
W14-1601,E12-1006,0,0.0571022,"or a choice between tagging accuracy and sentence-level accuracy, we see a significant improvement in 4/10 cases, i.e., for 4/10 data sets the effect is significance wrt. at least one metric. If we allow for a free choice between all three metrics (TA, UA, and SA), we observe significance in 9/10 cases. This way the existence of multiple metrics almost guarantees significant differences. Note that there are only two data sets (Answers and Spoken), where all metric differences appear significant. Dependency parsing. While there are multiple metrics in dependency parsing (Schwartz et al., 2011; Tsarfaty et al., 2012), we focus on the two standard metrics: labeled (LAS) and unlabeled attachment score (UAS) (Buchholz and Marsi, 2006). If we just consider the results in Table 3, i.e., only the comparison of M ALT-L IN VS . S TANFORD -RNN, we observe significant improvements in all cases, if we allow for a free choice between metrics. Bod (2000) provides a good example of a parsing paper evaluating models using different metrics on different test sets. Chen et al. (2008), similarly, only report UAS. NER. While macro-f1 is fairly standard in NER, we do have several available multiple metrics, including the unl"
W14-1601,W11-0328,0,0.0154577,"across 3 runs for POS and NER and 10 runs for dependency parsing. D OMAIN #W ORDS C O NLL 2007 Bio Chem POS 4k 5k • • S WITCHBOARD 4 Spoken 162k • E NGLISH W EB T REEBANK Answers Emails Newsgrs Reviews Weblogs WSJ 29k 28k 21k 28k 20k 40k • • • • • • F OSTER Twitter 3k • C O NLL 2003 News 50k Figure 1: Accuracies of L APOS VS . S TANFORD across 10 data sets. TASKS Dep. NER al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the L A POS tagger (Tsuruoka et al., 2011) and the S TAN FORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly availab"
W14-1601,C10-2146,0,0.0272174,"Missing"
W14-1601,C00-2137,0,0.652215,"as error analysis from previous experiments, and research findings often consist in system comparisons showing that System A is better than System B. Effect size, i.e., one system’s improvements over another, can be seen as a random variable. If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 1 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1–10, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics Covariates. Sometimes we may bin our results by variables that are actually predictive of the outcome (covariates) (Simmons et al., 2011). In some subfields of NLP, such as machine translation or (unsupervised) syntactic parsing, for example, it is common"
W14-1601,E14-4014,0,\N,Missing
W14-1601,W03-0419,0,\N,Missing
W14-1601,D07-1096,0,\N,Missing
W15-1617,ide-etal-2008-masc,0,0.0739475,"Missing"
W15-1617,D14-1108,0,0.0276301,"istribution of the Wall Street Journal dependency treebank (Bies et al., 2012; Petrov and McDonald, 2012). 2. Answers: The Yahoo! Answers test section from the English Web Treebank (Bies et al., 2012; Petrov and McDonald, 2012). 3. Spoken: The Switchboard corpus section of the MASC corpus (Ide et al., 2008). 4. Fiction: The literature subset of the test section of the Brown test set from CoNLL 2008 (Surdeanu et al., 2008), which encompasses the fiction, mystery, science-fiction, romance and humor categories of the Brown corpus. 5. Twitter: The test section of the Tweebank dependency treebank (Kong et al., 2014). WSJ is the perceived-of-as-canonical dataset. Answers and Twitter are datasets of social media texts from two different social media. We include Switchboard as an example of spoken language (transcriptions of telephone conversations), and Fiction to incorporate carefully edited (i.e., not user-generated) text that is lexically and syntactically different to newswire. From each corpus, we randomly selected 50 sentences and doubly-annotated them. D OMAIN A0 A1 MV WSJ Twitter Answers Spoken Fiction 99 88 92 100 96 76 72 79 86 76 72 56 63 81 78 Table 2: Frequency counts for arguments in the anno"
W15-1617,P05-1012,0,0.0577417,"ce (average per-edge confidence). between sentence length and sentence-wise agreement for all 250 annotated sentences, however, found the correlation to be low (0.1364). Consequently, it seems unlikely that sentence length had a major effect on our annotations. We may speculate that annotation disagreements can be due to rare linguistic phenomena and linguistic outliers. In Table 4 we show the correlation per domain between sentence-wise agreement and dependency parsing confidence. We have obtained this confidence from the edge-wise confidence scores provided by an instance of the MST parser (McDonald et al., 2005) trained on WSJ. The parsing confidence for a sentence is obtained from the average of the edges that have received a label (A0, MV, A1) by the annotators, averaged between the two annotators. The correlation for newswire is high, but not the highest, because despite high parsing confidence, annotation agreement is rather low. On the other end, the lowest correlation between parser confidence and agreement is for Answers, which has the highest inter-annotator agreement. These results, in our view, indicate that what makes annotating social media text hard (at times) is not what makes annotatin"
W15-1617,W08-2121,0,0.122021,"Missing"
W15-1806,W06-1670,0,0.0494196,", and it has mostly been limited to English newswire and literature (namely running on SemCor and SensEval data).9 Nevertheless, the interest in applying word-sense disambiguation techniques to reduced, coarser sense inventories has been a topic since the development of the first wordnets (Peters et al., 1998). Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on S EM C OR via 5-fold cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNe"
W15-1806,W97-0802,0,0.0681552,"embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns. Tsvetkov et al. (2014) propose an extension for adjectives, along the lines of the adjective sense of the German wordnet(Hamp and Feldweg, 1997). To the best of our knowledge, the current work is the first SST approach to Danish, which also extends to less canonical, characteristically webbased text types like chats or fora. 7 Conclusions We have presented a resource for SST that includes an extension of the English supersense inventory that can be used for any language, plus three additional tags that give account for characteristics of the syntax-semantics interface of a satellite-framing language like Danish. We have conducted an annotation task on 1,500 sentences, reaching 0.63 κ score after refining the annotation guidelines. Aft"
W15-1806,S14-1001,1,0.877501,"Missing"
W15-1806,P05-1005,0,0.0094743,"7.8 39.3 34.3 39.8 37.0 64.7 55.0 28.8 46.2 33.3 32.6 34.2 SAT. COLL SAT. PARTICLE SAT. REFLPRON 37.9 59.4 69.6 7.7 47.9 76.4 Table 11: Performance for extended noun and verb supersenses, and satellites. 6 Related work There has been relatively little previous work on supersense tagging, and it has mostly been limited to English newswire and literature (namely running on SemCor and SensEval data).9 Nevertheless, the interest in applying word-sense disambiguation techniques to reduced, coarser sense inventories has been a topic since the development of the first wordnets (Peters et al., 1998). Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on S EM C OR via 5-fold cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as p"
W15-1806,H94-1046,0,0.0286344,"in that the labels are comprised within spans of one or more tokens. NER, however, only recognizes a handful of entity types 1 The data is available at clarin.dk under Danish Supersense Corpus and does not extend beyond nouns, while supersenses may be defined for all part of speech and permit more granular semantic distinctions. While coarse-grained semantic types find use in a range of applications, such as information retrieval, question answering (QA), and relation extraction, one of the main intended uses of the annotated corpus is building a semantic concordancer in the style of SemCor (Miller et al., 1994). We base our annotation effort on the set of supersenses derived from Princeton Wordnet, which makes our annotations interoperable across many languages through the already existing linkings to Princeton Wordnet. However, we found several cases where the Princeton supersenses made overly broad distinctions that caused large groups of lexemes to be grouped together (e.g. buildings and vehicles falling under the ARTIFACT class). The original sense inventory comprises a total of 41 senses, spread over 26 noun senses, and 15 verb senses, plus a single “catch-all” sense for adjectives, which is gr"
W15-1806,P12-2050,0,0.21141,"redictions. The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on S EM C OR via 5-fold cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns. Tsvetkov et al. (2014) propose an extension for adjectives, along the lines of the adjective sense of the German wordnet(Hamp and Feldweg, 1997). To the best of our knowledge, the current work is the first SST approach to Danish, which also extends to less canonical, characteristically webbased text types like chats or fora. 7 Conclusions We have presented a resource for SST that includes an extension of the English supersense inventory that can b"
W15-1806,N03-1033,0,0.0302794,"kov, 2012), which consists of newspapers, magazines, oral debates, blogs, and social media.3 Table 4 lists the amount of training data (1,500 sentences in total) currently annotated for each domain. We describe each domain in terms of its average sentence length (SL) and proportion of tokens per type, namely the average amount of repetitions for a certain type. The final release will be made up of 600 sentences from all of the domains in Table 4, plus the test section of the Danish Dependency Treebank (Buch-Kromann et al., 2003). All the data has been POS-tagged using the Stanford POS-tagger (Toutanova et al., 2003) trained on the Danish PAROLE corpus.4 Note that we strictly use predicted POS instead of goldstandard to provide a more realistic setup for the evaluation of our system in Section 5. 3.2 Annotation guidelines Sense inventory The guidelines for the supersense annotation comprise the list of supersenses provided with an explanation and examples for each supersense. 3 http://cst.ku.dk/Workshop311012/sprogtekno2012.pdf 4 http://korpus.dsl.dk/e-resurser/paroledoc en.pdf Application rules The second part of the guidelines consists of a set of more specific rules for each part of speech. The rules f"
W15-1806,S10-1049,0,0.102102,"n addition, discuss how to retrieve fine-grained senses from those predictions. The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on S EM C OR via 5-fold cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns. Tsvetkov et al. (2014) propose an extension for adjectives, along the lines of the adjective sense of the German wordnet(Hamp and Feldweg, 1997). To the best of our knowledge, the current work is the first SST approach to Danish, which also extends to less canonical, characteristically webbased text types like chats or fora. 7 Conclusions We have presented a resource for SST th"
W15-1806,W13-0906,0,0.0474527,"w to retrieve fine-grained senses from those predictions. The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on S EM C OR via 5-fold cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns. Tsvetkov et al. (2014) propose an extension for adjectives, along the lines of the adjective sense of the German wordnet(Hamp and Feldweg, 1997). To the best of our knowledge, the current work is the first SST approach to Danish, which also extends to less canonical, characteristically webbased text types like chats or fora. 7 Conclusions We have presented a resource for SST that includes an extensio"
W15-1806,tsvetkov-etal-2014-augmenting-english,0,0.0802991,"d cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns. Tsvetkov et al. (2014) propose an extension for adjectives, along the lines of the adjective sense of the German wordnet(Hamp and Feldweg, 1997). To the best of our knowledge, the current work is the first SST approach to Danish, which also extends to less canonical, characteristically webbased text types like chats or fora. 7 Conclusions We have presented a resource for SST that includes an extension of the English supersense inventory that can be used for any language, plus three additional tags that give account for characteristics of the syntax-semantics interface of a satellite-framing language like Danish. We"
W15-1806,S07-1051,0,0.246823,"s (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on S EM C OR via 5-fold cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns. Tsvetkov et al. (2014) propose an extension for adjectives, along the lines of the adjective sense of the German wordnet(Hamp and Feldweg, 1997). To the best of our knowledge, the current work is the first SST approach to Danish, which also extends to less canonical, characteristically webbased text types like chats or fora. 7 Conclusions We have presented"
W15-1806,P13-4001,0,0.134357,"Missing"
W15-1806,W15-2005,1,0.785087,"etaProceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 22 Domain Blog Chat Forum Magazine Newswire Parliament SL tokens types Sentences 16.44 14.61 20.51 19.45 17.43 31.21 2.95 3.70 3.85 2.95 3.28 5.00 100 200 200 200 600 200 Table 4: Supersense tagging data sets. tion is annotated in the corpus in the following way: han satte(VERB . COMMUNICATION) ham p˚a(COLL) plads(COLL). 3 Annotation process This section the describes the annotation task for supersenses, including detailes on corpus, guidelines and resulting agreement scores. For further information, cf. Olsen et al. (2015). 3.1 Corpus We have chosen to annotate from the Danish CLARIN Reference Corpus (Asmussen and Halskov, 2012), which consists of newspapers, magazines, oral debates, blogs, and social media.3 Table 4 lists the amount of training data (1,500 sentences in total) currently annotated for each domain. We describe each domain in terms of its average sentence length (SL) and proportion of tokens per type, namely the average amount of repetitions for a certain type. The final release will be made up of 600 sentences from all of the domains in Table 4, plus the test section of the Danish Dependency Tree"
W15-1814,W14-1214,0,0.013336,"and complexity. These analyses enable us to evaluate whether sentence-level simplification operations can be meaningfully and directly assessed using eye tracking, which would be of relevance to both manual and automated simplification efforts. Introduction 1.1 Intuitively, the readability of a text should reflect the effort that a reader must put into recognizing the meaning encoded in the text. As a concept, readability thus integrates both content and form. Sentence-level readability assessment is desirable from a computational point of view because Automatic Simplification by Compression Amancio et al. (2014) found that more than one fourth of the transformations observed in sentence pairs from Wikipedia and Single English Wikipedia were compressions. To obtain automatically simplified sentences we therefore train a sentence-compression model. Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 97 With inspiration from McDonald (2006), we train a sentence compression system on a corpus of parallel sentences of manually expert-simplified and original newswire text where all simplifications are compressions. The system is described in detail in section 2. Sentence"
W15-1814,W12-4903,0,0.0670919,"Missing"
W15-1814,P11-2117,0,0.0280778,"S-tags and parsing features are more reliable in the beginning of the sentence. This could be tested in the future by applying the model to text from a domain with different information structure. 5.2 Implications for System Development We found that the very simple compression model presented in this paper was performing extensive simplifications, which is important in light of the fact that humans consider it harder to produce more aggressive simplifications. We trained our model on a relatively small, specialized compression corpus. The Simple English Wikipedia simplification corpus (SEW) (Coster and Kauchak, 2011), which has been used in a range of statistical text simplification systems (Coster and Kauchak, 2011; Zhu et al., 2010; Woodsend and Lapata, 2011), is far bigger, but also noisier. We found fewer than 50 sentence pairs fitting our compression criteria when exploring the possibility of generating a similar training set for English from the SEW. However, in future work, other, smaller simplification corpora could be adapted to the task, providing insight into the robustness of using compression for simplification. 5.3 Implications for Evaluation Methodology In many natural language generation a"
W15-1814,W14-1213,0,0.0424761,"Missing"
W15-1814,C10-2032,0,0.0120436,"token deletions per sentence. 2.2 Compression Model and Features The compression model is a conditional random field (CRF) model trained to make a sequence of categorical decisions, in each determining whether the current word should be left out of the compression output while taking into account the previous decision. We used CRF++ (Lafferty et al., 2001) trained with default parameter settings. Below, we describe the features we implemented. The features focus on surface form, PoStags, dependencies and word frequency information. Our initial choice of features is based on the comparisons in Feng et al. (2010) and Falkenjack and J¨onsson (2014), who both find that parsing 1 The corpus was PoS-tagged and parsed using the Bohnet parser (Bohnet, 2010) trained on the Danish Dependency Treebank (Kromann, 2003) with Universal PoS-tags (Petrov et al., 2011). 2 Note that this dataset did not contribute to training, tuning or choosing the model. Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 98 Figure 1: We extract observed compressions from the simplification corpus and train an automatic compression model. For the eye tracking and subjective evaluation we run the mo"
W15-1814,klerke-sogaard-2012-dsim,1,0.860803,"slation (Stymne et al., 2013). Below, we present the automatic simplification setup, including the parallel data, features and model selection and details on how we select the data for the eye-tracking experiment. The following section details the eye movement recording and subjective evaluation setup. Section 4 presents our results followed by a discussion and our conclusions. 2 Automatic Simplification Setup 2.1 Training and Evaluation Corpus For the sentence compression training and evaluation data we extracted a subset of ordinary and simplified newswire texts from the Danish DSim corpus (Klerke and Søgaard, 2012). In Figure 1 we give a schematic overview of how the data for our experiments was obtained. For model development and selection we extracted all pairs of original and simplified sentences under the following criteria: 1. No sentence pair differs by more than 150 characters excluding punctuation. 2. The simplified sentence must be a strict subset of the original and contain a minimum of four tokens. 3. The original sentence must have at least one additional token compared to the simplified sentence and this difference must be nonpunctuation and of minimum three characters’ length. This results"
W15-1814,C10-1152,0,0.0263809,"he model to text from a domain with different information structure. 5.2 Implications for System Development We found that the very simple compression model presented in this paper was performing extensive simplifications, which is important in light of the fact that humans consider it harder to produce more aggressive simplifications. We trained our model on a relatively small, specialized compression corpus. The Simple English Wikipedia simplification corpus (SEW) (Coster and Kauchak, 2011), which has been used in a range of statistical text simplification systems (Coster and Kauchak, 2011; Zhu et al., 2010; Woodsend and Lapata, 2011), is far bigger, but also noisier. We found fewer than 50 sentence pairs fitting our compression criteria when exploring the possibility of generating a similar training set for English from the SEW. However, in future work, other, smaller simplification corpora could be adapted to the task, providing insight into the robustness of using compression for simplification. 5.3 Implications for Evaluation Methodology In many natural language generation and manipulation setups, it is important that the system is able Proceedings of the 20th Nordic Conference of Computatio"
W15-1814,E06-1038,0,0.0448541,"eaning encoded in the text. As a concept, readability thus integrates both content and form. Sentence-level readability assessment is desirable from a computational point of view because Automatic Simplification by Compression Amancio et al. (2014) found that more than one fourth of the transformations observed in sentence pairs from Wikipedia and Single English Wikipedia were compressions. To obtain automatically simplified sentences we therefore train a sentence-compression model. Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 97 With inspiration from McDonald (2006), we train a sentence compression system on a corpus of parallel sentences of manually expert-simplified and original newswire text where all simplifications are compressions. The system is described in detail in section 2. Sentence compression works by simply dropping parts of a sentence and outputting the shorter sentence with less information content and simpler syntax. This approach allows us to control a number of variables, and in particular, it guarantees that each expert simplification and each system output are true subsets of the original input, providing three highly comparable vers"
W15-1814,petrov-etal-2012-universal,0,0.020957,"Missing"
W15-1814,P05-1065,0,0.0471776,". Bjornsson (1983) and Flesch (1948)) and are by their own definitions unsuitable for automatic application (cf. Benjamin (2012) for an evaluation of readability-formula usability). The standard approach to assessing text readability in natural language processing (NLP) is to ask readers to judge the quality of the output in terms of comprehensibility, grammaticality and meaning preservation (cf. Siddharthan and Katsos (2012)). An alternative is to use existing text collections categorized by readability level for learning models of distinct categories of readability e.g. age or grade levels (Schwarm and Ostendorf, 2005; Vajjala and Meurers, 2014). In this paper we seek to establish whether readers share an intuitive conceptualization of the readability of single sentences, and to what extent this conceptualization is reflected in their reading behavior. We research this by comparing subjective sentence-level readability judgments to recordings of readers’ eye movements and by testing to what extent these measures co-vary across sentences of varying length and complexity. These analyses enable us to evaluate whether sentence-level simplification operations can be meaningfully and directly assessed using eye"
W15-1814,W12-2203,0,0.0299398,"h information into account with each decision. This computer-centric approach is in contrast to traditional human-centric readability metrics which are explicitly constructed for use at text level (cf. Bjornsson (1983) and Flesch (1948)) and are by their own definitions unsuitable for automatic application (cf. Benjamin (2012) for an evaluation of readability-formula usability). The standard approach to assessing text readability in natural language processing (NLP) is to ask readers to judge the quality of the output in terms of comprehensibility, grammaticality and meaning preservation (cf. Siddharthan and Katsos (2012)). An alternative is to use existing text collections categorized by readability level for learning models of distinct categories of readability e.g. age or grade levels (Schwarm and Ostendorf, 2005; Vajjala and Meurers, 2014). In this paper we seek to establish whether readers share an intuitive conceptualization of the readability of single sentences, and to what extent this conceptualization is reflected in their reading behavior. We research this by comparing subjective sentence-level readability judgments to recordings of readers’ eye movements and by testing to what extent these measures"
W15-1814,W13-5634,0,0.0308642,"ntence with less information content and simpler syntax. This approach allows us to control a number of variables, and in particular, it guarantees that each expert simplification and each system output are true subsets of the original input, providing three highly comparable versions of each sentence. Further the system serves as a proof of concept that a relatively small amount of taskspecific data can be sufficient for this task. Sentence compression is, in addition, an important step in several downstream NLP tasks, including summarization (Knight and Marcu, 2000) and machine translation (Stymne et al., 2013). Below, we present the automatic simplification setup, including the parallel data, features and model selection and details on how we select the data for the eye-tracking experiment. The following section details the eye movement recording and subjective evaluation setup. Section 4 presents our results followed by a discussion and our conclusions. 2 Automatic Simplification Setup 2.1 Training and Evaluation Corpus For the sentence compression training and evaluation data we extracted a subset of ordinary and simplified newswire texts from the Danish DSim corpus (Klerke and Søgaard, 2012). In"
W15-1814,W14-1203,0,0.0303104,"(1948)) and are by their own definitions unsuitable for automatic application (cf. Benjamin (2012) for an evaluation of readability-formula usability). The standard approach to assessing text readability in natural language processing (NLP) is to ask readers to judge the quality of the output in terms of comprehensibility, grammaticality and meaning preservation (cf. Siddharthan and Katsos (2012)). An alternative is to use existing text collections categorized by readability level for learning models of distinct categories of readability e.g. age or grade levels (Schwarm and Ostendorf, 2005; Vajjala and Meurers, 2014). In this paper we seek to establish whether readers share an intuitive conceptualization of the readability of single sentences, and to what extent this conceptualization is reflected in their reading behavior. We research this by comparing subjective sentence-level readability judgments to recordings of readers’ eye movements and by testing to what extent these measures co-vary across sentences of varying length and complexity. These analyses enable us to evaluate whether sentence-level simplification operations can be meaningfully and directly assessed using eye tracking, which would be of"
W15-1831,P07-1007,0,0.0315542,"2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013). 7 Conclusions The systems that use the M AX selection strategy have a strong bias for the longest possible sentence, resulting from the low probability values obtained when calculating the prediction confidence of very long sequences. With few exceptions (e.g. an 11-word sentence on the 5th iteration for AM1 ), the systems exhaust the maximum-length sentences, and proceed to choose the longest available, and so forth. We do not take our individual annotator’s bias into consideration, but we believe that such bias plays a minor role in the differences of performance be"
W15-1831,D14-1097,0,0.0143127,"e take a small seed of data points, train a sequential labeling, and iterate over an unlabeled pool of data, selecting the data points our labeler is least confident about. In the AL literature, the selected data points are often those close to a decision boundary or those most likely to decrease overall uncertainty. This obviously leads to biased sampling, which can sometimes be avoided using different techniques, e.g., by exploiting cluster structure in the data. Generally, active learning for sequential labeling has received less attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2"
W15-1831,D14-1104,1,0.838297,"ss attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013). 7 Conclusions The systems that use the M AX selection strategy have a strong bias for the longest possible sentence, resulting from the low probability values obtained when calculating the prediction confidence of very long sequences. With few exceptions (e.g. an 11-word sentence on the 5th iteration for AM1 ), the systems exhaust the maximum-length sentences, and proceed to choose the longest available, and so forth. We do not take our individual annotator’s bias into consideration, but we believe that such bia"
W15-1831,W10-0104,0,0.0158409,"or those most likely to decrease overall uncertainty. This obviously leads to biased sampling, which can sometimes be avoided using different techniques, e.g., by exploiting cluster structure in the data. Generally, active learning for sequential labeling has received less attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013). 7 Conclusions The systems that use the M AX selection strategy have a strong bias for the longest possible sentence, resulting from the low probability values obtained when calculating the prediction confidence of very long sequences. With few"
W15-1831,D08-1112,0,0.0559162,"here are very standard. We take a small seed of data points, train a sequential labeling, and iterate over an unlabeled pool of data, selecting the data points our labeler is least confident about. In the AL literature, the selected data points are often those close to a decision boundary or those most likely to decrease overall uncertainty. This obviously leads to biased sampling, which can sometimes be avoided using different techniques, e.g., by exploiting cluster structure in the data. Generally, active learning for sequential labeling has received less attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan"
W15-1831,W13-3501,0,0.0135807,"Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013). 7 Conclusions The systems that use the M AX selection strategy have a strong bias for the longest possible sentence, resulting from the low probability values obtained when calculating the prediction confidence of very long sequences. With few exceptions (e.g. an 11-word sentence on the 5th iteration for AM1 ), the systems exhaust the maximum-length sentences, and proceed to choose the longest available, and so forth. We do not take our individual annotator’s bias into consideration, but we believe that such bias plays a minor role in the differences of performance between M AX and S AMPLE."
W15-2401,K15-1038,1,0.683331,"average fixation duration per word frequency (Rayner, 1998) and these could be as good as gaze features, while being easier to obtain. We use frequencies from the unlabelled portions of the English Web Treebank and word length as baseline in all types of experiments and find that gaze features to be better predictors for the noun experiment as well as for improving parsers. Introduction Readers fixate more and longer on open syntactic categories (verbs, nouns, adjectives) than on closed class items like prepositions and conjunctions (Rayner and Duffy, 1988; Nilsson and Nivre, 2009). Recently, Barrett and Søgaard (2015) presented evidence that gaze features can be used to discriminate between most pairs of parts of speech (POS). Their study uses all the coarse-grained POS labels proposed by Petrov et al. (2011). This paper investigates to what extent gaze data can also be used to predict grammatical functions such as subjects and objects. We first show that a simple logistic regression classifier trained on a very small seed of data using gaze features discriminates between some pairs of grammatical functions. We show that the same kind of classifier distinguishes well between the four main grammatical funct"
W15-2401,P10-1040,0,0.0644952,"Missing"
W15-2401,P11-2033,0,0.0599702,"Missing"
W15-2401,I11-1100,0,0.0192798,"Missing"
W15-2401,P12-2047,0,0.0123678,"d it harder to process object relative clauses than subject relative clauses. This paper is related to such work, but our interest is a broader model of syntactic influences on reading patterns. Related work In addition to Barrett and Søgaard (2015), our work relates to Matthies and Søgaard (2013), who study the robustness of a fixation prediction model across readers, not domains, but our work also relates in spirit to research on using weak supervision in NLP, e.g., work on using HTML markup to improve dependency parsers (Spitkovsky, 2013) or using click-through data to improve POS taggers (Ganchev et al., 2012). There have been few studies correlating reading behavior and general dependency syntax in the literature. Demberg and Keller (2008), having parsed the Dundee corpus using M INI PAR, show that dependency integration cost, roughly the distance between a word and its head, is pre5 Conclusions We have shown that gaze features can be used to discriminate between a subset of grammatical functions, even across domains, using only a small dataset and explored which features are more useful. Furthermore, we have shown that gaze features can be used to improve a state-of-the-art dependency parsing mod"
W15-2401,D13-1075,1,0.883038,"fferent dependency integration costs, while DOBJ and POBJ have about the same. Their study thus seems to support our finding that gaze features can be used to discriminate between the grammatical functions of nouns. Most other work of this kind focus on specific phenomena, e.g., Traxler et al. (2002), who show that subjects find it harder to process object relative clauses than subject relative clauses. This paper is related to such work, but our interest is a broader model of syntactic influences on reading patterns. Related work In addition to Barrett and Søgaard (2015), our work relates to Matthies and Søgaard (2013), who study the robustness of a fixation prediction model across readers, not domains, but our work also relates in spirit to research on using weak supervision in NLP, e.g., work on using HTML markup to improve dependency parsers (Spitkovsky, 2013) or using click-through data to improve POS taggers (Ganchev et al., 2012). There have been few studies correlating reading behavior and general dependency syntax in the literature. Demberg and Keller (2008), having parsed the Dundee corpus using M INI PAR, show that dependency integration cost, roughly the distance between a word and its head, is p"
W15-2401,W09-1113,0,0.0933149,"gure 1: A dependency structure with average fixation duration per word frequency (Rayner, 1998) and these could be as good as gaze features, while being easier to obtain. We use frequencies from the unlabelled portions of the English Web Treebank and word length as baseline in all types of experiments and find that gaze features to be better predictors for the noun experiment as well as for improving parsers. Introduction Readers fixate more and longer on open syntactic categories (verbs, nouns, adjectives) than on closed class items like prepositions and conjunctions (Rayner and Duffy, 1988; Nilsson and Nivre, 2009). Recently, Barrett and Søgaard (2015) presented evidence that gaze features can be used to discriminate between most pairs of parts of speech (POS). Their study uses all the coarse-grained POS labels proposed by Petrov et al. (2011). This paper investigates to what extent gaze data can also be used to predict grammatical functions such as subjects and objects. We first show that a simple logistic regression classifier trained on a very small seed of data using gaze features discriminates between some pairs of grammatical functions. We show that the same kind of classifier distinguishes well b"
W15-2401,W02-1001,0,\N,Missing
W15-2401,petrov-etal-2012-universal,0,\N,Missing
W15-2402,2014.eamt-1.40,1,0.891519,"Missing"
W15-2402,2012.amta-commercial.4,0,0.117328,"puzzles. Our most important finding is that BLEU does not correlate with E FFICIENCY, while two of our reading-derived metrics do. In other words, the normalised reading time and fixation counts are better measures of task performance, and thereby of translation quality, than the state-of-the-art metric, BLEU in this context. This is an important finding since reading-derived metrics are potentially also more useful as they do not depend on the availability of professional translators. 5 This shortcoming was alleviated in a recent reanalysis of previous experiments (Doherty and O’Brien, 2014; Doherty et al., 2012) which compares the usability of raw machine translation output in different languages and the original, wellformed English input. In order to test usability, a plausible task has to be set up. In this study the authors used an instructional text on how to complete a sequence of steps using a software service, previously unknown to the participants. MT output was obtained for four different languages and three to four native speakers worked with each output. Participants’ subjective assessment of the usability of the instructions, their performance in terms of efficiency and the cognitive load"
W15-2402,1999.mtsummit-1.85,0,0.108975,"e propose various metrics derived from natural reading behavior as proxies of task-performance. Reading has several advantages over other human judgments: It is fast, is relatively unbiased, and, most importantly, something that most of us do effortlessly all the time. Hence, with the development of robust eye tracking methods for home computers and mobile devices, this can potentially provide us with largescale, on-line evaluation of MT output. Introduction What’s a good translation? One way of thinking about this question is in terms of what the translations can be used for. In the words of Doyon et al. (1999), “a poor translation may suffice to determine the general topic of a text, but may not permit accurate identification of participants or the specific event.” Text-based tasks can thus be ordered according to their tolerance of translation errors, as determined by actual task outcomes, and task outcome can in turn be used to measure the quality of translation (Doyon et al., 1999). Machine translation (MT) evaluation metrics must be both adequate and practical. Human task performance, say participants’ ability to extract information from translations, is perhaps the most adequate measure of tra"
W15-2402,P02-1040,0,0.104197,"allowing for more efficient task solving. The experiments are designed to test the following hypothesized partial ordering of the text versions (summarized in Table 1): text simplification (S(·)) eases reading processing relative to second language reading processing (L2) while professional human translations into L1 eases processing more (H1). In addition, machine translated text (M(·)) is expected to ease the processing load, but less so than machine translation of sim1 The data will be made available from https:// github.com/coastalcph 7 H1: H2: L1 L1 ≺ S(·) ≺ ≺ M(S(·)) ≺ M(·) ≺ but BLEU (Papineni et al., 2002) remains the de facto state-of-the-art evaluation metric. Our findings, related to evaluation, are, as already mentioned, that (a) human judgments surprisingly do not correlate with task performance, and that (b) the reading-derived metrics T IME and F IXATIONS correlate strongly with task performance, while BLEU does not. This, in our view, questions the validity of human judgments and the BLEU metric and shows that reading-derived MT metrics may provide a better feedback in system development and adaptation. L2 L2 Table 1: Expected relative difficulty of processing. L1 and L2 are human edite"
W15-2402,2011.eamt-1.12,0,0.031818,"ut of different MT systems with expected quality differences. However, they showed that both of these two eyetracking measures were increased for the parts of the text containing errors in comparison with Related work Eye tracking has been used for MT evaluation in both post-editing and instruction tasks (Castilho et al., 2014; Doherty and O’Brien, 2014). Doherty et al. (2010) also used eye-tracking measures for evaluating MT output and found 12 error-free passages. In addition, they found gaze time to vary with specific error types in machine translated text. From an application perspective, Specia (2011) suggested the time-to-edit measure as an objective and accessible measure of translation quality. In their study it outperformed subjective quality assessments as annotations for a model for translation candidate ranking. Their tool was aimed at optimizing the productivity in post-editing tasks. Eye tracking can be seen as a similarly objective metric for fluency estimation (Stymne et al., 2012). The fact that eye tracking does not rely on translators makes annotation even more accessible. Both Doherty and O’Brien (2014) and Castilho et al. (2014) found subjective comprehensibility, satisfact"
W15-2402,stymne-etal-2012-eye,0,0.231995,"of translation quality (or usefulness), we see that the best indicator of translation quality that only takes the initial reading into account are F IXA TIONS and T IME . This indicates that F IXATIONS and T IME may be better MT benchmarking metrics than BLEU. 6 Castilho et al. (2014) employed a similar design to compare the usability of lightly post-edited MT output to raw MT output and found that also light post-editing was accompanied by fewer fixations and lower total fixation time (proportional to total task time) as well as fewer attentional shifts and increased efficiency. In contrast, Stymne et al. (2012) found no significant differences in total fixation counts and overall gaze time (proportional to total task time), when directly comparing output of different MT systems with expected quality differences. However, they showed that both of these two eyetracking measures were increased for the parts of the text containing errors in comparison with Related work Eye tracking has been used for MT evaluation in both post-editing and instruction tasks (Castilho et al., 2014; Doherty and O’Brien, 2014). Doherty et al. (2010) also used eye-tracking measures for evaluating MT output and found 12 error-"
W15-2402,W08-0309,0,\N,Missing
W15-4302,K15-1011,1,0.403535,"rk City dialect by six, etc. Data is costly to collect, and, as a consequence, scarce. Written language was traditionally used for formal purposes, and therefore differed in style from colloquial, spoken language. However, with the rise of social media platforms and the vast production of user generated content, differences between written and spoken language diminish. A number of recent papers have explored social media with respect to sociolinguistic and dialectological questions (Rao et al., 2010; Eisenstein, 2013; Volkova et al., 2013; Doyle, 2014; Hovy et al., 2015; Volkova et al., 2015; Johannsen et al., 2015; Hovy and Søgaard, 2015; Eisenstein, to appear). Emails, chats and social media posts serve purposes similar to those of spoken language, and consequently, features of spoken language, such as interjections, ellipses, and phonological variation, have found their way into this type of written language. Our work differs from most previous approaches by investigating several phonological spelling correlates of a specific language variety. The 284 million active users on Twitter post more than half a billion tweets every day, and some fraction of these tweets are geo-located. Eisenstein (2013) an"
W15-4302,E14-1011,0,0.385375,"soegaard@hum.ku.dk Abstract four subjects, the New York City dialect by six, etc. Data is costly to collect, and, as a consequence, scarce. Written language was traditionally used for formal purposes, and therefore differed in style from colloquial, spoken language. However, with the rise of social media platforms and the vast production of user generated content, differences between written and spoken language diminish. A number of recent papers have explored social media with respect to sociolinguistic and dialectological questions (Rao et al., 2010; Eisenstein, 2013; Volkova et al., 2013; Doyle, 2014; Hovy et al., 2015; Volkova et al., 2015; Johannsen et al., 2015; Hovy and Søgaard, 2015; Eisenstein, to appear). Emails, chats and social media posts serve purposes similar to those of spoken language, and consequently, features of spoken language, such as interjections, ellipses, and phonological variation, have found their way into this type of written language. Our work differs from most previous approaches by investigating several phonological spelling correlates of a specific language variety. The 284 million active users on Twitter post more than half a billion tweets every day, and so"
W15-4302,P11-1137,0,0.031987,"phenomena. With ”bafroom”, it seems that about 1 in 20 occurrences on Twitter are metauses. Meta-uses may also serve social functions. AAVE features are used as cultural markers by Latinos in North Carolina (Carter, 2013), for example. Some of the research hypotheses considered (H3 and H5) relate to demographic variables such as income and educational levels. While we do not have socio-economic information about the individual Twitter user, we can use the geo-located tweets to study the correlation between socio-economic variables and linguistic features at the level of cities or ZIP codes.1 Eisenstein et al. (2011) note that this level of abstraction introduces some noise. Since Twitter users do not form representative samples of the population, the mean income for a city or ZIP code is not necessarily the mean income for the Twitter users in that area. We refer to this problem as the (3) U SER P OPU LATION B IAS. Another serious methodological problem known as (4) G ALTON ’ S P ROBLEM (Naroll, 1961; Roberts and Winters, 2013), is the observation that cross-cultural associations are H8: Backing in /str/ (to /skr/) is unique to AAVE (Rickford, 1999; Thomas, 2007; Labov, 2006). Hypotheses 1–8 are investig"
W15-4302,W13-1102,0,0.640113,"hagen Njalsgade 140 DK-2300 Copenhagen S soegaard@hum.ku.dk Abstract four subjects, the New York City dialect by six, etc. Data is costly to collect, and, as a consequence, scarce. Written language was traditionally used for formal purposes, and therefore differed in style from colloquial, spoken language. However, with the rise of social media platforms and the vast production of user generated content, differences between written and spoken language diminish. A number of recent papers have explored social media with respect to sociolinguistic and dialectological questions (Rao et al., 2010; Eisenstein, 2013; Volkova et al., 2013; Doyle, 2014; Hovy et al., 2015; Volkova et al., 2015; Johannsen et al., 2015; Hovy and Søgaard, 2015; Eisenstein, to appear). Emails, chats and social media posts serve purposes similar to those of spoken language, and consequently, features of spoken language, such as interjections, ellipses, and phonological variation, have found their way into this type of written language. Our work differs from most previous approaches by investigating several phonological spelling correlates of a specific language variety. The 284 million active users on Twitter post more than half"
W15-4302,P15-2079,1,0.238458,"etc. Data is costly to collect, and, as a consequence, scarce. Written language was traditionally used for formal purposes, and therefore differed in style from colloquial, spoken language. However, with the rise of social media platforms and the vast production of user generated content, differences between written and spoken language diminish. A number of recent papers have explored social media with respect to sociolinguistic and dialectological questions (Rao et al., 2010; Eisenstein, 2013; Volkova et al., 2013; Doyle, 2014; Hovy et al., 2015; Volkova et al., 2015; Johannsen et al., 2015; Hovy and Søgaard, 2015; Eisenstein, to appear). Emails, chats and social media posts serve purposes similar to those of spoken language, and consequently, features of spoken language, such as interjections, ellipses, and phonological variation, have found their way into this type of written language. Our work differs from most previous approaches by investigating several phonological spelling correlates of a specific language variety. The 284 million active users on Twitter post more than half a billion tweets every day, and some fraction of these tweets are geo-located. Eisenstein (2013) and Doyle (2014) studied t"
W15-4302,D13-1187,0,0.124542,"0 DK-2300 Copenhagen S soegaard@hum.ku.dk Abstract four subjects, the New York City dialect by six, etc. Data is costly to collect, and, as a consequence, scarce. Written language was traditionally used for formal purposes, and therefore differed in style from colloquial, spoken language. However, with the rise of social media platforms and the vast production of user generated content, differences between written and spoken language diminish. A number of recent papers have explored social media with respect to sociolinguistic and dialectological questions (Rao et al., 2010; Eisenstein, 2013; Volkova et al., 2013; Doyle, 2014; Hovy et al., 2015; Volkova et al., 2015; Johannsen et al., 2015; Hovy and Søgaard, 2015; Eisenstein, to appear). Emails, chats and social media posts serve purposes similar to those of spoken language, and consequently, features of spoken language, such as interjections, ellipses, and phonological variation, have found their way into this type of written language. Our work differs from most previous approaches by investigating several phonological spelling correlates of a specific language variety. The 284 million active users on Twitter post more than half a billion tweets ever"
W15-4324,P11-1087,0,0.0221534,"to learn transliteration of SNOMED CT terms in Basque. Spelling variations and transliteration seem to form a continuum, from nondialectal spelling variations such as Facebook/fbook, over dialectal variations such as Baltimore/Baltimaw (observed on Twitter), to cross-language variations such as M¨unchen/Munich. POS tagging with Brown clusters Brown et al. (1992) introduced the Brown clustering algorithm, which induces a hiearchy of clusters optimizing the likelihood of a hidden Markov model. Each word is assigned to at most one cluster. The algorithm can be used as an unsupervised POS tagger (Blunsom and Cohn, 2011), but Brown clusters have also been used as features in discriminative sequence modeling (Turian et al., 2010). Ritter et al. (2011) and Owoputi et al. (2013) use Brown clusters induced from a large Twitter corpus to improve a POS tagger trained on a small corpus on hand-annotated tweets (Gimpel et al., 2011). Several recent papers on domain adaptation of POS taggers use discriminative taggers trained with Brown clusters as features as their baseline, e.g., Plank et al. (2014). 3 FSA word representations Our approach is to learn FSAs from Brown clusters and use statistics over the learned FSAs"
W15-4324,J92-4003,0,0.582571,"med entity transliteration, a problem which is very related to ours. They learned transliteration patterns using techniques from phrasebased SMT, but formalized the transliteration grammars by composing FSAs. Similarly, de Vinaspre et al. (2013) use FSAs to learn transliteration of SNOMED CT terms in Basque. Spelling variations and transliteration seem to form a continuum, from nondialectal spelling variations such as Facebook/fbook, over dialectal variations such as Baltimore/Baltimaw (observed on Twitter), to cross-language variations such as M¨unchen/Munich. POS tagging with Brown clusters Brown et al. (1992) introduced the Brown clustering algorithm, which induces a hiearchy of clusters optimizing the likelihood of a hidden Markov model. Each word is assigned to at most one cluster. The algorithm can be used as an unsupervised POS tagger (Blunsom and Cohn, 2011), but Brown clusters have also been used as features in discriminative sequence modeling (Turian et al., 2010). Ritter et al. (2011) and Owoputi et al. (2013) use Brown clusters induced from a large Twitter corpus to improve a POS tagger trained on a small corpus on hand-annotated tweets (Gimpel et al., 2011). Several recent papers on doma"
W15-4324,W13-1815,0,0.0299432,"Missing"
W15-4324,R13-1026,0,0.0261057,"on dev). Effect significant over the entire test data (p &lt; 0.01 using Wilcoxon’s test) 164 4 Experiments We train a linear CRF model on newswire, using a publicly available implementation (CRFsuite),1 and adapt the feature representation to optimize performance on Twitter data. Data As our training data we use the OntoNotes 4.0 training split of the Wall Street Journal section of the Penn Treebank. As our held-out data, we use the development sections of Foster et al. (2011) and Gimpel et al. (2011). Our test datasets come from Foster et al. (2011), Ritter et al. (2011) (using the splits from Derczynski et al. (2013)), and Hovy et al. (2014). In other words, one out of three test sets comes from the same sample as one of our development sets, but two come from new ones. This prevents false findings due to over-fitting. All datasets were mapped to the universal tagset presented in Petrov et al. (2011), following Hovy et al. (2014). Learning CRFsuite uses L-BFGS and L2regularization by default. Features Our baseline feature representation uses a combination of unigram, bigram and Brown cluster features, i.e., the CRFsuite default feature model augmented with Brown clusters. The Brown clusters were induced f"
W15-4324,I11-1100,0,0.0733825,"Missing"
W15-4324,W10-2408,0,0.0131827,"and morphology can be analyzed as special cases of regular expressions, and many linguistic descriptions at this level can be compiled into finite state automata (FSAs) (Kaplan and Kay, 1994; Karttunen et al., 1997). Learning minimal FSAs from samples is generally NP-hard (Gold, 1978), and most FSAs used to model phono-/morphotactic constraints have been manually constructed. However, learning a minimal FSA for a fixed set of members of a Brown clusters, is obviously a much easier problem. We extend the FSAs to capture spelling variations better using a simple propagation principle (see §3). Noeman and Madkour (2010) use FSAs for named entity transliteration, a problem which is very related to ours. They learned transliteration patterns using techniques from phrasebased SMT, but formalized the transliteration grammars by composing FSAs. Similarly, de Vinaspre et al. (2013) use FSAs to learn transliteration of SNOMED CT terms in Basque. Spelling variations and transliteration seem to form a continuum, from nondialectal spelling variations such as Facebook/fbook, over dialectal variations such as Baltimore/Baltimaw (observed on Twitter), to cross-language variations such as M¨unchen/Munich. POS tagging with"
W15-4324,N13-1039,0,0.0584231,"Missing"
W15-4324,E14-1078,1,0.851256,"rkov model. Each word is assigned to at most one cluster. The algorithm can be used as an unsupervised POS tagger (Blunsom and Cohn, 2011), but Brown clusters have also been used as features in discriminative sequence modeling (Turian et al., 2010). Ritter et al. (2011) and Owoputi et al. (2013) use Brown clusters induced from a large Twitter corpus to improve a POS tagger trained on a small corpus on hand-annotated tweets (Gimpel et al., 2011). Several recent papers on domain adaptation of POS taggers use discriminative taggers trained with Brown clusters as features as their baseline, e.g., Plank et al. (2014). 3 FSA word representations Our approach is to learn FSAs from Brown clusters and use statistics over the learned FSAs to propagate non-determinisms, increasing the coverage of our word representations in domains such as Twitter. We explain our word representation algorithm by the following example: Say we have learned the Brown cluster {cos, coz, coss, cozzz} consisting of short forms of because. We can then construct the minimal FSA that accepts only these four strings. However, in this case, we can construct an even smaller FSA if we allow cyclic transitions going out from the first accept"
W15-4324,D11-1141,0,0.305463,"tal spelling variations such as Facebook/fbook, over dialectal variations such as Baltimore/Baltimaw (observed on Twitter), to cross-language variations such as M¨unchen/Munich. POS tagging with Brown clusters Brown et al. (1992) introduced the Brown clustering algorithm, which induces a hiearchy of clusters optimizing the likelihood of a hidden Markov model. Each word is assigned to at most one cluster. The algorithm can be used as an unsupervised POS tagger (Blunsom and Cohn, 2011), but Brown clusters have also been used as features in discriminative sequence modeling (Turian et al., 2010). Ritter et al. (2011) and Owoputi et al. (2013) use Brown clusters induced from a large Twitter corpus to improve a POS tagger trained on a small corpus on hand-annotated tweets (Gimpel et al., 2011). Several recent papers on domain adaptation of POS taggers use discriminative taggers trained with Brown clusters as features as their baseline, e.g., Plank et al. (2014). 3 FSA word representations Our approach is to learn FSAs from Brown clusters and use statistics over the learned FSAs to propagate non-determinisms, increasing the coverage of our word representations in domains such as Twitter. We explain our word"
W15-4324,P10-1040,0,0.361485,"(OOV) effects are probably the most common sources of errors in natural language processing. OOV effects arise when supervised models are trained on manually annotated corpora (labeled data) and applied to new text containing words not in the ∗ The work was done while Julie was a MSc student at University of Copenhagen. Anders Søgaard Center for Language Technology University of Copenhagen soegaard@hum.ku.dk labeled data. The most popular technique to combat OOV effects in the last decade has arguably been Brown clustering (Brown et al., 1992). Other alternatives exist, like word embeddings (Turian et al., 2010), but more than twice as many ACL papers talk about word clusters than about word embeddings. The main problem with Brown clusters – as well as word embeddings – is that they are intended for transductive use, i.e., Brown clusters are used to induce distributional classes (representations) for observed words. In other words, while they may minimize OOV effects by bridging between words observed in small labeled corpora and words observed in huge unlabeled corpora, they still do not give us representations for words that we encounter for the first time in our test data. That is, words neither i"
W15-4324,J94-3001,0,0.0369794,"nite state automata (FSAs) from Brown clusters, collect evidence for productive morpho-phonological alternations (reentrant branchings; see §3), and using these to augment the FSAs. We apply the FSA-based word representations to unsupervised domain adaptation of POS taggers to Twitter data and show how this leads to significant improvements over a strong baseline system. 2 Related work FSAs Many of the rules used in phonology and morphology can be analyzed as special cases of regular expressions, and many linguistic descriptions at this level can be compiled into finite state automata (FSAs) (Kaplan and Kay, 1994; Karttunen et al., 1997). Learning minimal FSAs from samples is generally NP-hard (Gold, 1978), and most FSAs used to model phono-/morphotactic constraints have been manually constructed. However, learning a minimal FSA for a fixed set of members of a Brown clusters, is obviously a much easier problem. We extend the FSAs to capture spelling variations better using a simple propagation principle (see §3). Noeman and Madkour (2010) use FSAs for named entity transliteration, a problem which is very related to ours. They learned transliteration patterns using techniques from phrasebased SMT, but"
W16-2521,P16-1071,1,0.798827,"Missing"
W16-2521,W16-2502,0,0.0444495,"(Søgaard et al., 2015). While this proposal adopts a specific encoding of termdocument matrices, similar encodings can be built for any definition of a Sprachspiel. Any such metric can thus be ”hacked” or build into the model, directly. Note, again, that whether such a metric is adequate (measuring meaning) remains an open question. Cognitive lexical semantics How well does our embeddings align with our mental representations of words? Obviously, we do not have direct access to our mental representations, and most researchers have relied on word associations norms 2 See Faruqui et al. (2016; Batchkarov et al. (2016; Chiu et al. (2016) for critiques of using word association norms. The problem with word association norms is inadequacy (and statistical power): They conflate several types of similarity, e.g., synonymy and antonymy, and they are culture-specific. 117 Note that we have reasons to think such metrics are not entirely inadequate, since we know humans understand words when they read them. fMRI data, for example, may contain a lot of noise and other types of information, but semantic word processing is bound to the contribute to the signal, one way or the other. well as evaluation, in the same wa"
W16-2521,W16-2501,0,0.0261058,"While this proposal adopts a specific encoding of termdocument matrices, similar encodings can be built for any definition of a Sprachspiel. Any such metric can thus be ”hacked” or build into the model, directly. Note, again, that whether such a metric is adequate (measuring meaning) remains an open question. Cognitive lexical semantics How well does our embeddings align with our mental representations of words? Obviously, we do not have direct access to our mental representations, and most researchers have relied on word associations norms 2 See Faruqui et al. (2016; Batchkarov et al. (2016; Chiu et al. (2016) for critiques of using word association norms. The problem with word association norms is inadequacy (and statistical power): They conflate several types of similarity, e.g., synonymy and antonymy, and they are culture-specific. 117 Note that we have reasons to think such metrics are not entirely inadequate, since we know humans understand words when they read them. fMRI data, for example, may contain a lot of noise and other types of information, but semantic word processing is bound to the contribute to the signal, one way or the other. well as evaluation, in the same way as we do evaluatio"
W16-2521,jurgens-2014-analysis,0,0.0259456,"ed by Tsvetkov et al. (2015). However, again, Faruqui and Dyer (2015) recently proposed this as a learning strategy, encoding words by their occurrence in Wordnet. Using mental lexica as gold standard annotation thus suffers from the same problem as defining the meaning of words by their co-occurrencies or distributions over situations or documents; the derived metrics can be hacked. Also, there’s a number of problems with using Wordnets and the like for evaluating word embeddings. The most obvious ones are low coverage and low interannotator agreement in such resources. Moreover, as shown by Juergens (2014), some inter-annotator disagreements are not random (errors), but reflect different, linguistically motivated choices. There are different ways to structure word meanings that lead to different semantic networks. Different lexicographic theories suggest different ways to do this. This means that our resources are theoretically biased. After all, while psycholinguistic priming effects and word association norms suggest that semantically similar words are retrieved faster than orthographically similar words, there is to the best of my knowledge no bullet-proof evidence that our brain does not or"
W16-2521,Q15-1016,0,0.0281897,"o-occurrence theory In C O - OCCURRENCE T HEORY, the meaning of a word is defined by its co-occurrences with other words – e.g., the meaning of big is given by its co-occurrence with words such as house and small, i.e., its value in a co-occurrence matrix. Word embeddings should therefore predict lexical co-occurrences, which can be evaluated in terms of perplexity or word error rate. This was how embeddings were evaluated in the early papers, e.g., (Mikolov et al., 2010). But note that constructing co-occurrence matrices is also an integral part of standard approaches to inducing embeddings (Levy et al., 2015). In Any metrics here? From (one or more of) these theories we want to derive a valid evaluation metric. In my view, a valid metric satisfies two principles: (i) that it measures what we want to measure (adequacy), and (ii) that it cannot easily be 1 See the discussion in the last paragraph. 116 Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP, pages 116–121, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics instead.2 In matrix terms, C OGNITIVE L EXICAL S EMANTICS defines the meaning of a word as a vector over vertices in an o"
W16-2521,P15-1165,1,0.823382,"Missing"
W16-2521,D15-1243,0,0.0216286,"istics instead.2 In matrix terms, C OGNITIVE L EXICAL S EMANTICS defines the meaning of a word as a vector over vertices in an ontology or a mental lexicon. The hypothesis is that our mental lexicon is organized as a undirected, colored, weighted network, and the meaning of words are defined by the edges connecting them to other words. The meaning of big, for example, is in a synonym relation with large, an antonym of small, etc. Such networks are typically informed by word association norms and corpus linguistic evidence. Using Wordnets for evaluating word embeddings was recently proposed by Tsvetkov et al. (2015). However, again, Faruqui and Dyer (2015) recently proposed this as a learning strategy, encoding words by their occurrence in Wordnet. Using mental lexica as gold standard annotation thus suffers from the same problem as defining the meaning of words by their co-occurrencies or distributions over situations or documents; the derived metrics can be hacked. Also, there’s a number of problems with using Wordnets and the like for evaluating word embeddings. The most obvious ones are low coverage and low interannotator agreement in such resources. Moreover, as shown by Juergens (2014), some inter-"
W16-2521,K15-1038,1,0.795463,"Missing"
W16-2521,W16-2506,0,\N,Missing
W17-4409,D15-1256,0,0.251951,"Missing"
W17-4409,P15-2104,0,0.448506,"styles to write words in a nonstandard way have implicit location information. 6 Geolocation We also evaluated the above hypotheses in the context of a geolocation experiment using the geographically diverse more dataset, WORLD (Han et al., 2012c). The WORLD dataset covers 3,709 cities worldwide and consists of tweets from 1.4M words. 65 users, where 10,000 users are held out as development set and 10,000 as test set. The task is to predict the primary location of a new user based on that person’s tweet history. We use logistic regression as classifier to predict the users location, following Rahimi et al. (2015). The results (median distance error, city accuracy and country accuracy) are shown in Table 3. Similar to our findings in our analysis above, we see that OOV words are better features than dictionary words. Also geonames features, alone, have high performance, even better than dictionary words. The rest of the examined groups are not performing as good, individually. The combination of all words (shown as All) results in the best performance. 7 the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 1277–1287. Bo Han, Paul Cook"
W17-4409,W16-3930,0,0.163719,"al., 2010; Ahmed et al., 2013; Hong et al., 2012), or feature selection/weighting to find words indicative of location (Priedhorsky et al., 2014; Han et al., 2012a, 2014; Wing and Baldridge, 2014). Han et al. (2012b) showed that information gain ratio is a useful metric for measuring how location-indicative words are. They used a sample of 26 million tweets in their study, obtained through the public Twitter API. Salehi et al. (2017) evaluate nine name entity types. Using various metrics, they find that GEO-LOC, FACILITY and SPORT-TEAM are more informative for geolocation than other NE types. Chi et al. (2016) specifically study the contributions of city and country names, hashtags, and user mentions, to geolocation. Their results suggested that a combination of city and country names, as well as hashtags, are good location predictors. They used a sample of 9 million tweets in their study, obtained through the public Twitter API. Pavalanathan and Eisenstein (2015) investigate the potential demographic biases of using nonstandard words and entities. They show that younger users are more likely to use geographically specific non-standard words, while old men tend to use more location revealing entiti"
W17-4409,D12-1137,0,0.344435,"of words over US counties in this corpus, limiting ourselves to the most frequent 100,000 words. This is important to ensure support, but also makes geolocation harder, since rare words are generally more predictive of location. On the other hand, the fact that we rely on relatively frequent words makes our analysis more widely applicable. The corpus has 4.5B tokens of the 100,000 most frequent words. So, on average we have 45,000 occurrences of each word. The minimum frequency is 612 tokens; the most frequent word occurs 138M times. The median is 1,742 occurrences. (Wing and Baldridge, 2011; Roller et al., 2012), regional topic distributions (Eisenstein et al., 2010; Ahmed et al., 2013; Hong et al., 2012), or feature selection/weighting to find words indicative of location (Priedhorsky et al., 2014; Han et al., 2012a, 2014; Wing and Baldridge, 2014). Han et al. (2012b) showed that information gain ratio is a useful metric for measuring how location-indicative words are. They used a sample of 26 million tweets in their study, obtained through the public Twitter API. Salehi et al. (2017) evaluate nine name entity types. Using various metrics, they find that GEO-LOC, FACILITY and SPORT-TEAM are more inf"
W17-4409,D10-1124,0,0.460433,"Missing"
W17-4409,D14-1039,0,0.469127,"ther hand, the fact that we rely on relatively frequent words makes our analysis more widely applicable. The corpus has 4.5B tokens of the 100,000 most frequent words. So, on average we have 45,000 occurrences of each word. The minimum frequency is 612 tokens; the most frequent word occurs 138M times. The median is 1,742 occurrences. (Wing and Baldridge, 2011; Roller et al., 2012), regional topic distributions (Eisenstein et al., 2010; Ahmed et al., 2013; Hong et al., 2012), or feature selection/weighting to find words indicative of location (Priedhorsky et al., 2014; Han et al., 2012a, 2014; Wing and Baldridge, 2014). Han et al. (2012b) showed that information gain ratio is a useful metric for measuring how location-indicative words are. They used a sample of 26 million tweets in their study, obtained through the public Twitter API. Salehi et al. (2017) evaluate nine name entity types. Using various metrics, they find that GEO-LOC, FACILITY and SPORT-TEAM are more informative for geolocation than other NE types. Chi et al. (2016) specifically study the contributions of city and country names, hashtags, and user mentions, to geolocation. Their results suggested that a combination of city and country names,"
W17-4409,P11-1096,0,0.202092,"look at the distribution of words over US counties in this corpus, limiting ourselves to the most frequent 100,000 words. This is important to ensure support, but also makes geolocation harder, since rare words are generally more predictive of location. On the other hand, the fact that we rely on relatively frequent words makes our analysis more widely applicable. The corpus has 4.5B tokens of the 100,000 most frequent words. So, on average we have 45,000 occurrences of each word. The minimum frequency is 612 tokens; the most frequent word occurs 138M times. The median is 1,742 occurrences. (Wing and Baldridge, 2011; Roller et al., 2012), regional topic distributions (Eisenstein et al., 2010; Ahmed et al., 2013; Hong et al., 2012), or feature selection/weighting to find words indicative of location (Priedhorsky et al., 2014; Han et al., 2012a, 2014; Wing and Baldridge, 2014). Han et al. (2012b) showed that information gain ratio is a useful metric for measuring how location-indicative words are. They used a sample of 26 million tweets in their study, obtained through the public Twitter API. Salehi et al. (2017) evaluate nine name entity types. Using various metrics, they find that GEO-LOC, FACILITY and S"
W17-4409,D12-1039,0,\N,Missing
W17-4409,C12-1064,0,\N,Missing
W17-4409,W17-4415,1,\N,Missing
W17-4415,D10-1124,0,0.802123,"Missing"
W17-4415,C12-1064,0,0.796157,"easure the geographic distribution of each NE type, and measure their entropy. In the second experiment, we conduct feature selection via randomized logistic regression, and, in the third experiment, we establish a baseline by using majority classes for all types. Geographic diversity We first measure the geographic distribution of each type. We extract all NEs in the WORLD training set and use the Tweet corpus to measure entropy and mean pairwise distance (in kilometers) between tweets that contain the same NEs. We compute unpredictability as entropy: Resources Data We use the WORLD dataset (Han et al., 2012), which covers 3,709 cities worldwide and consists of tweets from 1.4M users. Han et al. (2012) hold out 10,000 users as development and 10,000 as test set. For each user with at least 10 geotagged tweets, the user’s location is set to be the city in which the majority of their tweets are from. We also use Han et al. (2012)’s method to extract the nearest city to a given latitudelongitude coordinate. H(x) = − n X P(xi ) log P(xi ) i=1 1 We map the latitude and longitude coordinates to cities/regions based on Han et al. (2012). 117 GEO-LOC FACILITY SPORT-TEAM MOVIE TV-SHOW PERSON BAND PRODUCT C"
W17-4415,D15-1256,0,0.115835,"Missing"
W17-4415,P15-2104,0,0.726545,"33.6 53.6 17.7 82.2 29.9 83.3 33.7 83.6 34.0 – 10.3 – – 31.0 24.1 Distance Median↓ Mean↓ 515 1727 2186 5317 612 1885 520 1769 495 1735 509 646 1669 1953 Table 4: Accuracy and distance results for various methods. – indicates no report in respective paper NE. To divide the world into regions with roughly the same number of users, we use a k-d tree approach proposed by Roller et al. (2012). As a result, we will cover larger regions when the population density is low and vice versa. Each region is then considered as a label to train the classifiers. The approach of using k-d tree is also used in Rahimi et al. (2015); Han et al. (2012) and Wing and Baldridge (2014). See Table 3 for an example of the following methods. All use logistic regression as classifier, following Rahimi et al. (2015). repetition is used to put more emphasis/weight based on frequency.3 In order to measure the effectiveness of the three top NE types discovered in Section 4, we experiment with (1) considering all NE types (shown as Our MethodallN Es in Table 4), and (2) the three most useful types (shown as Our Methodtop3 ). Evaluation metrics We use the same evaluation metrics as previous studies: accuracy depending on location granu"
W17-4415,D11-1141,0,0.168968,"Missing"
W17-4415,D12-1137,0,0.675438,"ent detection, recommender systems, sentiment analysis, and knowledge base population. Since tweets contain at most 140 characters, geolocation of individual tweets is rarely feasible. Instead, most studies focus on predicting the primary location of a user by concatenating their entire tweet history. While this provides more context, it is still a noisy source with features of varying informativeness. 2 Related Work Most previous studies use textual features as input. Some use KL divergence between the distribution of a users words and the words used in each region (Wing and Baldridge, 2011; Roller et al., 2012), regional topic distributions (Eisenstein et al., 2010; Ahmed et al., 2013; Hong et al., 2012), or feature selection/weighting to find words indicative of location (Priedhorsky et al., 2014; Han et al., 2012, 2014; Wing and Baldridge, 2014). All these studies require relatively large training sets to fit the models, and can be heavily biased by 116 Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 116–121 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics Type PERSON GEO-LOC FACILITY COMPANY MOVIE BAND PRODUCT TV-SHOW SPORT-TEAM All Exam"
W17-4415,W17-4409,1,0.339601,"ce location names can refer to multiple locations (Brunsting et al., 2016). Chi et al. (2016) explicitly study the contributions of city and country names, hashtags, and user mentionings, to geolocation. Their results suggested that a combination of city and country names, as well as hashtags, are good location predictors. Pavalanathan and Eisenstein (2015) suggest that non-standard words are more locationspecific, and also, more likely to occur in geotagged tweets. In contrast to this paper, none of the previous works study how much various NE types reveal about the user location. Similarly, Salehi and Søgaard (2017) evaluate common hypotheses about language and location. However, they do not explicitly study named entities. 3 4 NE types and Geolocation In Table 1, we have seen the general distribution of NE types, with PERSON, GEO-LOC and FACILITY as top three. In this section, we focus on the predictiveness of NEs (as features) for geolocation. Later, in Section 5, we will propose a method to improve geolocation by putting more emphasis on the top NEs and their hidden location information. We conduct three experiments to quantify predictiveness of NEs. In the first, we measure the geographic distributio"
W17-4415,W16-3930,0,0.0544298,"or events during the time of collection, such as an election or a disaster. In contrast to our work, most do not consider multi-word NEs. Only few text-based studies consider NEs, and if so, focus on location names using gazetteers like GeoNames, limiting the methods to the completeness of these gazetteers. Since they usually also use other text-based models, it is hard to determine how much location names contribute. These approaches depend on a namedisambiguation phase, using Wikipedia, DBPedia, or OpenStreetMap, since location names can refer to multiple locations (Brunsting et al., 2016). Chi et al. (2016) explicitly study the contributions of city and country names, hashtags, and user mentionings, to geolocation. Their results suggested that a combination of city and country names, as well as hashtags, are good location predictors. Pavalanathan and Eisenstein (2015) suggest that non-standard words are more locationspecific, and also, more likely to occur in geotagged tweets. In contrast to this paper, none of the previous works study how much various NE types reveal about the user location. Similarly, Salehi and Søgaard (2017) evaluate common hypotheses about language and location. However, th"
W17-4415,D14-1039,0,0.532042,"he impact of NEs and their hidden location information for geolocation. To extract the hidden location information of each NE, we collect the locations of all tweets in our tweet corpus that contain that 118 Example baseline Only NE Baseline without NE Our Method Me, my friend and the Eiffel tower Me my friend and the Eiffel tower [Eiffel tower] ME my friend and the Me my friend and the Eiffel tower Paris Paris Paris Paris [Las Vegas] [Las Vegas] Table 3: Examples and features of methods in Section 5 Method Baseline Only NE Baseline without NE Our MethodallN Es Our Methodtop3 Previous studies Wing and Baldridge (2014) Han et al. (2012) city↑ 17.6 9.3 14.8 17.5 17.8 Accuracy country↑ @161↑ 83.6 33.6 53.6 17.7 82.2 29.9 83.3 33.7 83.6 34.0 – 10.3 – – 31.0 24.1 Distance Median↓ Mean↓ 515 1727 2186 5317 612 1885 520 1769 495 1735 509 646 1669 1953 Table 4: Accuracy and distance results for various methods. – indicates no report in respective paper NE. To divide the world into regions with roughly the same number of users, we use a k-d tree approach proposed by Roller et al. (2012). As a result, we will cover larger regions when the population density is low and vice versa. Each region is then considered as a l"
W17-4415,P11-1096,0,0.256809,"as technologies such as event detection, recommender systems, sentiment analysis, and knowledge base population. Since tweets contain at most 140 characters, geolocation of individual tweets is rarely feasible. Instead, most studies focus on predicting the primary location of a user by concatenating their entire tweet history. While this provides more context, it is still a noisy source with features of varying informativeness. 2 Related Work Most previous studies use textual features as input. Some use KL divergence between the distribution of a users words and the words used in each region (Wing and Baldridge, 2011; Roller et al., 2012), regional topic distributions (Eisenstein et al., 2010; Ahmed et al., 2013; Hong et al., 2012), or feature selection/weighting to find words indicative of location (Priedhorsky et al., 2014; Han et al., 2012, 2014; Wing and Baldridge, 2014). All these studies require relatively large training sets to fit the models, and can be heavily biased by 116 Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 116–121 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics Type PERSON GEO-LOC FACILITY COMPANY MOVIE BAND PRODUCT TV-SH"
W17-4905,D09-1036,0,0.0578306,"Missing"
W17-4905,P11-1100,0,0.0412422,"Missing"
W17-4905,P12-2034,0,0.0502998,"Missing"
W17-4905,P09-2078,0,0.0714622,"Missing"
W17-4905,P11-1032,0,0.0973982,"Missing"
W17-4905,P09-2004,0,0.0666574,"Missing"
W17-4905,C08-2022,0,0.0766974,"Missing"
W17-4905,prasad-etal-2008-penn,0,0.0302061,"Missing"
W17-4905,N15-1081,0,0.0326961,"Missing"
W17-4905,L16-1680,0,0.0237757,"Missing"
W17-5050,J91-3004,0,0.511189,"e probability of the derivation of a certain tree. The total surprisal of Wn is then defined as: Feature representation In this study, features known to affect the complexity of text, such as syntactic, lexical and total surprisal (Hale, 2001; Demberg and Keller, 2008), were used. Most of these features were extracted using a probabilistic top-down parser introduced by Roark (2001). After removing duplicate sentences and sentences with typos, the final corpus used was of about 80,000 sentence pairs. The features extracted are shown in table 1. The prefix probability of word wn is explained by Jelinek and Lafferty (1991) as the probability that wn occurs as a prefix of some string generated by a grammar. It is the sum of the probabilities of SG (Wn ) = − log P PG (W [1, n]) P PG (W [1, n − 1]) Syntactic surprisal and lexical surprisal are calculated to account for high surprisal scores (Roark et al., 2009). As Roark et al. (2009) mentions, a word may surprise because it is unconventional, or because it occurs in an unusual context. In order to separate the lexical and syntactic 439 components of surprisal, the incremental parser calculates partial derivations immediately before word Wn is integrated into the"
W17-5050,P16-2094,1,0.693906,"ant insight about the reader and the text being read. For example, long regressive eye movements, which typically involve regressing more than 10 letter spaces (Rayner, 1998), may indicate that the reader is facing some difficulty in understanding the text (Frazier and Rayner, 1982; Rayner, 2012). In addition, regressions have been shown to occur during the disambiguation of a sentence (Frazier and Rayner, 1982). This relationship between text and eye movements, has led to an influx of studies investigating the use of eye tracking data to improve and test computational models of language i.e. Barrett et al. (2016); Demberg and Keller (2008); Klerke et al. (2015). In this study we aim to incorporate eye movement data for the task of autoThe work most related to ours is by Singh et al. (2016), who used eye tracking measures taken from the Dundee corpus in order to predict word by word reading times for each sentence. Subsequently, they used these word by word reading times as features for predicting readability. The two tasks were performed separately, and their feature representations were different from the ones presented here. In contrast, we present a model that predicts gaze and sentence-level reada"
W17-5050,W15-2402,1,0.917613,"stract matic readability assessment. Automatic readability assessment is the task of automatically labeling a text with a certain difficulty level. An accurate and robust system has many potential applications, for example it can help educators obtain appropriate reading materials for students with normal learning capacities, as well as students with disabilities and language learners. It can also be used to assess the performance of machine translation, text simplification and language generation systems. Eye-tracking data has previously been used to evaluate readability models (Green, 2014; Klerke et al., 2015), however, our main contribution is to explore the way that eye tracking data can help improve models for readability assessment through multi-task learning (Caruana, 1997) and parser metrics based on the surprisal theory of syntactic complexity (Hale, 2001, 2016). Multi task learning allows the model to learn various tasks in parallel and improve performance by sharing parameters in the hidden layers. We show that text readability prediction improves significantly from hard parameter sharing with models predicting first pass duration, total fixation duration and regression duration. Specifica"
W17-5050,P11-2117,0,0.105176,"m the Dundee corpus in order to predict word by word reading times for each sentence. Subsequently, they used these word by word reading times as features for predicting readability. The two tasks were performed separately, and their feature representations were different from the ones presented here. In contrast, we present a model that predicts gaze and sentence-level readability simultaneously. We use gaze data from the Dundee corpus (Kennedy et al., 2003) and two different datasets for the readability prediction task: aligned Wikipedia sentences used for the task of text simplification by Coster and Kauchak (2011) and the OneStopEnglish dataset used by Vajjala and Meurers (2014). 438 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 438–443 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Features 1. Prefix probability -word1 2. Total surprisal - word1 3. Syntactic Surprisal -word1 4. Lexical Surprisal - word1 5. Ambiguity - word1 6. Prefix probability -word2 7. Total surprisal - word2 8. Syntactic Surprisal - word2 9. Lexical Surprisal - word2 10. Ambiguity - word2 11. Total surprisal – sent mean 12. Syntactic"
W17-5050,C10-2032,0,0.0196095,"the region is first entered until it is left. Regression path duration includes regressions made out of a region prior to moving forward in the text and total fixation duration is the sum of all fixations in the region including, regressions to that region. As mentioned in Rayner et al. (2006), these measures typically concern research questions focusing on sentence or discourse processing. − 1]) P PG (W [1, n − 1]) and lexical surprisal (LexSG (Wn )) as: Logistic/linear regression and MLPs Logistic Regression (LR) models have been widely used in document level readability classification i.e. Feng et al. (2010) and (Xia et al., 2016). LR models are linear models and can be thought of as singlelayer perceptrons with softmax or sigmoid activation functions. The objective is typically to minimize a cross-entropy loss function. The same architecture can be used for linear regression, however, when trained to minimize mean squared error. Here, we compare LR with a 3-layered Multi Layer Perceptron (MLP). For our MLP architecture, we use sigmoid activation at the input and output layers and use ReLu activation in the hidden layer. The hidden layer contains 100 neurons. All models presented here use the Ada"
W17-5050,J01-2004,0,0.0354659,"o probabilistic context free grammar G and whose last step used a production with terminal Wn . We can then express the prefix probability of W P[1, n] with respect to G as P PG (W [1, n]) = D∈D(G,W [1,n]) ρ(D), where ρ(D) is the probability of the derivation of a certain tree. The total surprisal of Wn is then defined as: Feature representation In this study, features known to affect the complexity of text, such as syntactic, lexical and total surprisal (Hale, 2001; Demberg and Keller, 2008), were used. Most of these features were extracted using a probabilistic top-down parser introduced by Roark (2001). After removing duplicate sentences and sentences with typos, the final corpus used was of about 80,000 sentence pairs. The features extracted are shown in table 1. The prefix probability of word wn is explained by Jelinek and Lafferty (1991) as the probability that wn occurs as a prefix of some string generated by a grammar. It is the sum of the probabilities of SG (Wn ) = − log P PG (W [1, n]) P PG (W [1, n − 1]) Syntactic surprisal and lexical surprisal are calculated to account for high surprisal scores (Roark et al., 2009). As Roark et al. (2009) mentions, a word may surprise because it"
W17-5050,D09-1034,0,0.0240606,"tures were extracted using a probabilistic top-down parser introduced by Roark (2001). After removing duplicate sentences and sentences with typos, the final corpus used was of about 80,000 sentence pairs. The features extracted are shown in table 1. The prefix probability of word wn is explained by Jelinek and Lafferty (1991) as the probability that wn occurs as a prefix of some string generated by a grammar. It is the sum of the probabilities of SG (Wn ) = − log P PG (W [1, n]) P PG (W [1, n − 1]) Syntactic surprisal and lexical surprisal are calculated to account for high surprisal scores (Roark et al., 2009). As Roark et al. (2009) mentions, a word may surprise because it is unconventional, or because it occurs in an unusual context. In order to separate the lexical and syntactic 439 components of surprisal, the incremental parser calculates partial derivations immediately before word Wn is integrated into the syntactic structure. Syntactic surprisal (SynSG (Wn )) is defined as: P − log D∈D(G,W [1,n]) ρ(D[1, |D| First pass duration refers to the sum of all fixations on a region once the region is first entered until it is left. Regression path duration includes regressions made out of a region pr"
W17-5050,W14-1205,0,0.0171389,"d@di.ku.dk Abstract matic readability assessment. Automatic readability assessment is the task of automatically labeling a text with a certain difficulty level. An accurate and robust system has many potential applications, for example it can help educators obtain appropriate reading materials for students with normal learning capacities, as well as students with disabilities and language learners. It can also be used to assess the performance of machine translation, text simplification and language generation systems. Eye-tracking data has previously been used to evaluate readability models (Green, 2014; Klerke et al., 2015), however, our main contribution is to explore the way that eye tracking data can help improve models for readability assessment through multi-task learning (Caruana, 1997) and parser metrics based on the surprisal theory of syntactic complexity (Hale, 2001, 2016). Multi task learning allows the model to learn various tasks in parallel and improve performance by sharing parameters in the hidden layers. We show that text readability prediction improves significantly from hard parameter sharing with models predicting first pass duration, total fixation duration and regressi"
W17-5050,W16-4123,0,0.333604,"dicate that the reader is facing some difficulty in understanding the text (Frazier and Rayner, 1982; Rayner, 2012). In addition, regressions have been shown to occur during the disambiguation of a sentence (Frazier and Rayner, 1982). This relationship between text and eye movements, has led to an influx of studies investigating the use of eye tracking data to improve and test computational models of language i.e. Barrett et al. (2016); Demberg and Keller (2008); Klerke et al. (2015). In this study we aim to incorporate eye movement data for the task of autoThe work most related to ours is by Singh et al. (2016), who used eye tracking measures taken from the Dundee corpus in order to predict word by word reading times for each sentence. Subsequently, they used these word by word reading times as features for predicting readability. The two tasks were performed separately, and their feature representations were different from the ones presented here. In contrast, we present a model that predicts gaze and sentence-level readability simultaneously. We use gaze data from the Dundee corpus (Kennedy et al., 2003) and two different datasets for the readability prediction task: aligned Wikipedia sentences us"
W17-5050,N01-1021,0,0.252141,"ding materials for students with normal learning capacities, as well as students with disabilities and language learners. It can also be used to assess the performance of machine translation, text simplification and language generation systems. Eye-tracking data has previously been used to evaluate readability models (Green, 2014; Klerke et al., 2015), however, our main contribution is to explore the way that eye tracking data can help improve models for readability assessment through multi-task learning (Caruana, 1997) and parser metrics based on the surprisal theory of syntactic complexity (Hale, 2001, 2016). Multi task learning allows the model to learn various tasks in parallel and improve performance by sharing parameters in the hidden layers. We show that text readability prediction improves significantly from hard parameter sharing with models predicting first pass duration, total fixation duration and regression duration. Specifically, we induce multi-task Multilayer Perceptrons and Logistic Regression models over sentence representations that capture various aggregate statistics, from two different text readability corpora for English, as well as the Dundee eye-tracking corpus. Our"
W17-5050,W12-2019,0,0.0235034,"English sentences from Wikipedia (Coster and Kauchak, 2011). Similar datasets have been used in the past, e.g., in Ambati et al. (2016) and Hwang et al. (2015). The easy-to-read sentences were taken from Simple Wikipedia and paired with sentences from the standard Wikipedia using cosine similarity. In addition, we also evaluate our models on the OneStopEnglish corpus (Vajjala and Meurers, 2014), specifically the elementary-intermediate and elementary-advanced sentence pairs. This dataset has been used for readability assessment (Vajjala and Meurers, 2014) using the WeeBit model presented by (Vajjala and Meurers, 2012), so we compare our results with theirs. Table 1: data. Features extracted for the readability all trees from the first word to the current word. Surprisal is then the difference between the log of the prefix probability of wn and wn−1 . If we describe D(G, W [1, n]) as the set of all possible leftmost derivations D with respect to probabilistic context free grammar G and whose last step used a production with terminal Wn . We can then express the prefix probability of W P[1, n] with respect to G as P PG (W [1, n]) = D∈D(G,W [1,n]) ρ(D), where ρ(D) is the probability of the derivation of a cer"
W17-5050,N15-1022,0,0.0418808,"een logistic and linear regression. We evaluate our models on Simple Wikipedia and the OneStopEnglish corpus. Finally, we present learning curves that show that the improvements are robust across different sample sizes. 2 Experiments Data Our target task is sentence-level readability prediction, i.e. a binary classification problem of sentences into easy-to-read and hard-to-read. Our main corpus is a sentence-aligned corpus of 137,000 simple versus normal English sentences from Wikipedia (Coster and Kauchak, 2011). Similar datasets have been used in the past, e.g., in Ambati et al. (2016) and Hwang et al. (2015). The easy-to-read sentences were taken from Simple Wikipedia and paired with sentences from the standard Wikipedia using cosine similarity. In addition, we also evaluate our models on the OneStopEnglish corpus (Vajjala and Meurers, 2014), specifically the elementary-intermediate and elementary-advanced sentence pairs. This dataset has been used for readability assessment (Vajjala and Meurers, 2014) using the WeeBit model presented by (Vajjala and Meurers, 2012), so we compare our results with theirs. Table 1: data. Features extracted for the readability all trees from the first word to the cu"
W17-5050,W16-0502,0,0.0200108,"red until it is left. Regression path duration includes regressions made out of a region prior to moving forward in the text and total fixation duration is the sum of all fixations in the region including, regressions to that region. As mentioned in Rayner et al. (2006), these measures typically concern research questions focusing on sentence or discourse processing. − 1]) P PG (W [1, n − 1]) and lexical surprisal (LexSG (Wn )) as: Logistic/linear regression and MLPs Logistic Regression (LR) models have been widely used in document level readability classification i.e. Feng et al. (2010) and (Xia et al., 2016). LR models are linear models and can be thought of as singlelayer perceptrons with softmax or sigmoid activation functions. The objective is typically to minimize a cross-entropy loss function. The same architecture can be used for linear regression, however, when trained to minimize mean squared error. Here, we compare LR with a 3-layered Multi Layer Perceptron (MLP). For our MLP architecture, we use sigmoid activation at the input and output layers and use ReLu activation in the hidden layer. The hidden layer contains 100 neurons. All models presented here use the Adam optimizer, and a drop"
W17-6310,E17-2026,1,0.875821,"Missing"
W17-6310,W06-1670,0,0.0372686,"the above assumption quite a bit. We do not need the optimal hypothesis classes to overlap. Hidden layer sharing can work even with the optimal hypothesis classes of P and R distinct, if there is a joint representation such that P and R both become linearly separable. Whether this is the case, is an empirical question. bitbucket.org/soegaard/hyperlink-iwpt17 English data In our English in-domain experiments, we use three datasets for our target tasks, namely the Penn Treebank for syntactic chunking (Marcus et al., 1993), the SemCor corpus for semantic supersense tagging (Miller et al., 1994; Ciaramita and Altun, 2006), and the CCGBank2 for CCG super-tagging. See Figure 1 for an example of all three layers of annotation. The training section of the chunking dataset consists of 8936 sentences. SemCor contains 15465 sentences, and the CCGBank contains 39604 sentences. For our auxiliary task, for replicability (and as a tribute 2 Experiments Model Our model merges two deep recurrent neural networks through hard parameter sharing. We use three-layered, bi-directional long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997), in a way similar to Søgaard and Goldberg (2016). We op1 2 68 http://ro"
W17-6310,N16-1179,1,0.837209,"improvements on out-of-domain English data, as well as in experiments with syntactic chunking with hyperlinks for Quechua. Related work Hard parameter sharing of hidden layers has become a popular approach to multi-task learning. It was originally introduced in Caruana (1993), but first applied in NLP in Collobert et al. (2011), and it was shown, empirically, to be an effective regularizer across two different NLP tasks in Søgaard and Goldberg (2016). Using more readily available data resources that are not annotated by linguists, but still carry linguistic signals, was previously explored by Klerke et al. (2016) and Plank (2016). Baxter (2000) shows, in the context of linear models, that if two problems, P and R, share optimal hypothesis classes, then the induction of a model from a sample of P can efficiently regularize the induction of a model from a sample of R. This is too strong an assumption for our purposes, obviously, since even our label sets are different, but we also have more wiggle-room than heavily mean-constrained linear models, for example. In fact, hidden layer sharing relaxes the above assumption quite a bit. We do not need the optimal hypothesis classes to overlap. Hidden layer sha"
W17-6310,J93-2004,0,0.0586178,"han heavily mean-constrained linear models, for example. In fact, hidden layer sharing relaxes the above assumption quite a bit. We do not need the optimal hypothesis classes to overlap. Hidden layer sharing can work even with the optimal hypothesis classes of P and R distinct, if there is a joint representation such that P and R both become linearly separable. Whether this is the case, is an empirical question. bitbucket.org/soegaard/hyperlink-iwpt17 English data In our English in-domain experiments, we use three datasets for our target tasks, namely the Penn Treebank for syntactic chunking (Marcus et al., 1993), the SemCor corpus for semantic supersense tagging (Miller et al., 1994; Ciaramita and Altun, 2006), and the CCGBank2 for CCG super-tagging. See Figure 1 for an example of all three layers of annotation. The training section of the chunking dataset consists of 8936 sentences. SemCor contains 15465 sentences, and the CCGBank contains 39604 sentences. For our auxiliary task, for replicability (and as a tribute 2 Experiments Model Our model merges two deep recurrent neural networks through hard parameter sharing. We use three-layered, bi-directional long short-term memory networks (LSTMs) (Hochr"
W17-6310,H94-1046,0,0.298255,"layer sharing relaxes the above assumption quite a bit. We do not need the optimal hypothesis classes to overlap. Hidden layer sharing can work even with the optimal hypothesis classes of P and R distinct, if there is a joint representation such that P and R both become linearly separable. Whether this is the case, is an empirical question. bitbucket.org/soegaard/hyperlink-iwpt17 English data In our English in-domain experiments, we use three datasets for our target tasks, namely the Penn Treebank for syntactic chunking (Marcus et al., 1993), the SemCor corpus for semantic supersense tagging (Miller et al., 1994; Ciaramita and Altun, 2006), and the CCGBank2 for CCG super-tagging. See Figure 1 for an example of all three layers of annotation. The training section of the chunking dataset consists of 8936 sentences. SemCor contains 15465 sentences, and the CCGBank contains 39604 sentences. For our auxiliary task, for replicability (and as a tribute 2 Experiments Model Our model merges two deep recurrent neural networks through hard parameter sharing. We use three-layered, bi-directional long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997), in a way similar to Søgaard and Goldberg ("
W17-6310,C16-1059,0,0.123203,"main English data, as well as in experiments with syntactic chunking with hyperlinks for Quechua. Related work Hard parameter sharing of hidden layers has become a popular approach to multi-task learning. It was originally introduced in Caruana (1993), but first applied in NLP in Collobert et al. (2011), and it was shown, empirically, to be an effective regularizer across two different NLP tasks in Søgaard and Goldberg (2016). Using more readily available data resources that are not annotated by linguists, but still carry linguistic signals, was previously explored by Klerke et al. (2016) and Plank (2016). Baxter (2000) shows, in the context of linear models, that if two problems, P and R, share optimal hypothesis classes, then the induction of a model from a sample of P can efficiently regularize the induction of a model from a sample of R. This is too strong an assumption for our purposes, obviously, since even our label sets are different, but we also have more wiggle-room than heavily mean-constrained linear models, for example. In fact, hidden layer sharing relaxes the above assumption quite a bit. We do not need the optimal hypothesis classes to overlap. Hidden layer sharing can work eve"
W17-6310,P16-2038,1,0.931037,"g with hyperlink prediction as an auxiliary task improves performance across three tasks: syntactic chunking, semantic supersense tagging, and CCG supertagging. We also see improvements on out-of-domain English data, as well as in experiments with syntactic chunking with hyperlinks for Quechua. Related work Hard parameter sharing of hidden layers has become a popular approach to multi-task learning. It was originally introduced in Caruana (1993), but first applied in NLP in Collobert et al. (2011), and it was shown, empirically, to be an effective regularizer across two different NLP tasks in Søgaard and Goldberg (2016). Using more readily available data resources that are not annotated by linguists, but still carry linguistic signals, was previously explored by Klerke et al. (2016) and Plank (2016). Baxter (2000) shows, in the context of linear models, that if two problems, P and R, share optimal hypothesis classes, then the induction of a model from a sample of P can efficiently regularize the induction of a model from a sample of R. This is too strong an assumption for our purposes, obviously, since even our label sets are different, but we also have more wiggle-room than heavily mean-constrained linear m"
W17-6310,P10-1130,0,0.0933404,"- <href>intifada< href>, <href>jihad<href>, <href>jihad<guerilla war<href>, <href>insurrection<href>, <href>rebellion<href>, and <href>civil war<href> -- prompts several reflections. This sentence is a random sentence taken from the Internet. The mark-up is hyperlinks, referring the reader to related websites. The hyperlinks mark passsages of the text highlighting the topics of the linked websites. The marked passages are intifada, jihad, guerilla war, insurrection, rebellion and civil war. Note that these are not just words, but also phrases. In this example, they are all noun phrases. Spitkovsky et al. (2010) also looked at hyperlinks and observed that the vast majority of marked passages were syntactic constituents such as noun and verb phrases. He then went on to show that this data is potentially useful for unsupervised induction of dependency parsers. We build directly on this work, but go on to show that hyperlinks are not just useful for unsupervised induction of NLP models. It is also possible to improve state-of-the-art supervised NLP models, by jointly learning to predict hyperlinks from raw HTML files. Specifically, we show that 1 Introduction Syntactic analysis can be used to improve kn"
W17-7620,D10-1044,0,0.0956778,"Missing"
W17-7620,E14-1078,1,0.84935,"in, if a treebank contains rich meta-data, such as Ontonotes, we can easily set up such splits. However, a list of standard splits would ensure comparability across the work of different research groups. 4 Learning from Disagreements One thing that makes parsing harder than necessary, is our insisting on perfect agreement with the human gold-standard annotation. There are often multiple possible analyses of a sentence, even when wholeheartedly adopting a particular linguistic theory, but parsers are only rewarded for picking the analysis accepted by the treebank annotators after adjudication (Plank et al., 2014b).9 In unsupervised dependency parsing, some researchers have proposed alternative, less conservative metrics that would not penalize linguistically acceptable deviation from the gold standard (Schwartz et al., 2011; Tsarfaty et al., 2012). An alternative, however, is to use multiple reference annotations for each sentence in the test data. This is the approach taken in machine translation, for example. I am confident that performance across multiple reference annotations (multiply annotated test data) is more predictive for downstream performance than performance on adjudicated annotations."
W17-7620,P14-2083,1,0.854032,"in, if a treebank contains rich meta-data, such as Ontonotes, we can easily set up such splits. However, a list of standard splits would ensure comparability across the work of different research groups. 4 Learning from Disagreements One thing that makes parsing harder than necessary, is our insisting on perfect agreement with the human gold-standard annotation. There are often multiple possible analyses of a sentence, even when wholeheartedly adopting a particular linguistic theory, but parsers are only rewarded for picking the analysis accepted by the treebank annotators after adjudication (Plank et al., 2014b).9 In unsupervised dependency parsing, some researchers have proposed alternative, less conservative metrics that would not penalize linguistically acceptable deviation from the gold standard (Schwartz et al., 2011; Tsarfaty et al., 2012). An alternative, however, is to use multiple reference annotations for each sentence in the test data. This is the approach taken in machine translation, for example. I am confident that performance across multiple reference annotations (multiply annotated test data) is more predictive for downstream performance than performance on adjudicated annotations."
W17-7620,E17-1021,1,0.907932,"clic graphs for that matter). Some linguists have argued that some sentences are best described by cyclic structures, for example (Pollard and Sag, 1994). Such analyses never make it into treebanks,10 and I think the main motivation is the idea that modern parsers require well-formed input trees. Many parsers are designed to work only on trees, whether dependency trees or constituent trees, but recently several architectures have been introduced that do not hardwire this constraint into their models. Examples include sequence-to-sequence parsers (Luong et al., 2016) and so-called tensorLSTMs (Schlichtkrull and Soegaard, 2017). Sequence-to-sequence parsers encode input sentences using recurrent neural networks, recurrently appliying the transition parameters of the encoder. In their simplest version, they then generate a sequence of output symbols, one symbol at a time. After encoding the input sentence, the initial state is the vector sentence representation. From this vector, the parsers predict the most likely output symbol. The next state, frmo which the next output symbol is predicted, is obtained by applying the transition parameters of the decoder to the current state. The encoder, responsible for learning a"
W17-7620,P11-1067,0,0.030897,"g from Disagreements One thing that makes parsing harder than necessary, is our insisting on perfect agreement with the human gold-standard annotation. There are often multiple possible analyses of a sentence, even when wholeheartedly adopting a particular linguistic theory, but parsers are only rewarded for picking the analysis accepted by the treebank annotators after adjudication (Plank et al., 2014b).9 In unsupervised dependency parsing, some researchers have proposed alternative, less conservative metrics that would not penalize linguistically acceptable deviation from the gold standard (Schwartz et al., 2011; Tsarfaty et al., 2012). An alternative, however, is to use multiple reference annotations for each sentence in the test data. This is the approach taken in machine translation, for example. I am confident that performance across multiple reference annotations (multiply annotated test data) is more predictive for downstream performance than performance on adjudicated annotations. Moreover, we already know that dependency parsers benefit from observing disagreements between annotators at training time (Plank et al., 2014a); learning from such disagreements using cost-sensitive agreements can l"
W17-7620,E12-1006,0,0.023129,"e thing that makes parsing harder than necessary, is our insisting on perfect agreement with the human gold-standard annotation. There are often multiple possible analyses of a sentence, even when wholeheartedly adopting a particular linguistic theory, but parsers are only rewarded for picking the analysis accepted by the treebank annotators after adjudication (Plank et al., 2014b).9 In unsupervised dependency parsing, some researchers have proposed alternative, less conservative metrics that would not penalize linguistically acceptable deviation from the gold standard (Schwartz et al., 2011; Tsarfaty et al., 2012). An alternative, however, is to use multiple reference annotations for each sentence in the test data. This is the approach taken in machine translation, for example. I am confident that performance across multiple reference annotations (multiply annotated test data) is more predictive for downstream performance than performance on adjudicated annotations. Moreover, we already know that dependency parsers benefit from observing disagreements between annotators at training time (Plank et al., 2014a); learning from such disagreements using cost-sensitive agreements can lead to better performanc"
W18-3021,E17-1072,1,0.834546,"The work presented in our paper differs from these approaches, in that we do not use image data to improve semantic representations, but use images as a resource to learn crosslingual representations. Even though lexicon induction from text resources might be more promising in terms of performance, we think that lexicon induction from visual data is worth exploring as it might give insights in the way that language is grounded in visual context. Introduction Typically, cross-lingual word representations are learned from word alignments, sentence alignments, from aligned, comparable documents (Levy et al., 2017), or from monolingual corpora using seed dictionaries (Ammar et al., 2016).1 However, for many languages such resources are not available. Bergsma and Van Durme (2011) introduced an alternative idea, namely to learn bilingual representations from image data collected via web image search. The idea behind their approach is to represent words in a visual space and find valid translations between words based on similarities between their visual representations. Representations of words in the visual space are built by rep1 Recent work by Lample et al. (2018) introduces unsupervised bilingual lexi"
W18-3021,D16-1235,0,0.0462487,"Missing"
W18-3021,D14-1032,0,0.0219679,"ons for other POS from images is possible.2 In order to evaluate whether this work scales to verbs and adjectives, we compile wordlists containing these POS in several languages. We collect image sets for each image word and represent all words in a visual space. Then, we rank translations computing similarities between image sets and evaluate performance on this task. Another field of research that exploits image data for NLP applications is the induction of multi-modal embeddings, i.e. semantic representations that are learned from textual and visual information jointly (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2017; Kiela et al., 2016; Vuli´c et al., 2016). The work presented in our paper differs from these approaches, in that we do not use image data to improve semantic representations, but use images as a resource to learn crosslingual representations. Even though lexicon induction from text resources might be more promising in terms of performance, we think that lexicon induction from visual data is worth exploring as it might give insights in the way that language is grounded in visual context. Introduction Typically, cross-lingu"
W18-3021,P18-1072,1,0.8901,"Missing"
W18-3021,J15-4004,0,0.0808929,"Missing"
W18-3021,P16-2031,0,0.0343301,"Missing"
W18-3021,D14-1005,0,0.0942495,"ages is possible.2 In order to evaluate whether this work scales to verbs and adjectives, we compile wordlists containing these POS in several languages. We collect image sets for each image word and represent all words in a visual space. Then, we rank translations computing similarities between image sets and evaluate performance on this task. Another field of research that exploits image data for NLP applications is the induction of multi-modal embeddings, i.e. semantic representations that are learned from textual and visual information jointly (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2017; Kiela et al., 2016; Vuli´c et al., 2016). The work presented in our paper differs from these approaches, in that we do not use image data to improve semantic representations, but use images as a resource to learn crosslingual representations. Even though lexicon induction from text resources might be more promising in terms of performance, we think that lexicon induction from visual data is worth exploring as it might give insights in the way that language is grounded in visual context. Introduction Typically, cross-lingual word representations"
W18-3021,P14-2135,0,0.19011,"lingual representations for other POS from images is possible.2 In order to evaluate whether this work scales to verbs and adjectives, we compile wordlists containing these POS in several languages. We collect image sets for each image word and represent all words in a visual space. Then, we rank translations computing similarities between image sets and evaluate performance on this task. Another field of research that exploits image data for NLP applications is the induction of multi-modal embeddings, i.e. semantic representations that are learned from textual and visual information jointly (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2017; Kiela et al., 2016; Vuli´c et al., 2016). The work presented in our paper differs from these approaches, in that we do not use image data to improve semantic representations, but use images as a resource to learn crosslingual representations. Even though lexicon induction from text resources might be more promising in terms of performance, we think that lexicon induction from visual data is worth exploring as it might give insights in the way that language is grounded in visual context. Introducti"
W18-3021,D16-1043,0,0.322603,"Missing"
W18-3021,D15-1015,0,0.276072,"the world’s languages. Previous work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words. However, that work focused (almost exclusively) on the translation of nouns only. Here, we investigate whether the meaning of other parts-of-speech (POS), in particular adjectives and verbs, can be learned in the same way. Our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns. 1 Kiela et al. (2015) improve performance for the same task using a feature representation extracted from convolutional networks. However, both works only consider nouns, leaving open the question of whether learning cross-lingual representations for other POS from images is possible.2 In order to evaluate whether this work scales to verbs and adjectives, we compile wordlists containing these POS in several languages. We collect image sets for each image word and represent all words in a visual space. Then, we rank translations computing similarities between image sets and evaluate performance on this task. Anothe"
W18-3401,D15-1041,0,0.028899,"with a character-based sequence-to-sequence model. Furthermore, we experiment with different choices of external resources and corresponding auxiliary tasks and show that autoencoding can be as efficient as an auxiliary task for low-resource POS tagging as lemmatization. Finally, we evaluate our models on 34 typologically diverse languages. 2 Figure 1: Our multi-task architecture, consisting of a shared character LSTM (down), as well as a sequence labeling (up) and a sequence-tosequence (right) part. subword-level LSTM is bi-directional and operates on the character level (Ling et al., 2015; Ballesteros et al., 2015). Its input is the character sequence of each input word, represented by the embedding sequence c1 , c2 , . . . , cm . The final character-based representation of each word is the concatenation of the two last LSTM hidden states: POS Tagging with Subword-level Supervision Hierarchical POS tagging LSTMs that receive both word-level and subword-level input, such as Plank et al. (2016), are known to perform well on unseen words. This is due to their ability to associate subword-level patterns with POS tags. However, hierarchical LSTMs are also very expressive, and thus prone to overfitting. We be"
W18-3401,E17-2026,1,0.947527,".6776(.00) .6226( - ) .6141( - ) .6188( - ) .6529( - ) Table 1: Averaged accuracies and standard deviations over 5 training runs on UD 2.0 test sets, with 478 tokens of POS-annotated data and varying amounts of data for the auxiliary task (low, medium and high). Best result for each language in bold. Autoencoding and lemmatization are on par across the board, and with 100 training sentences (low), random autoencoding is also competitive. 6.2 Why does Random String Autoencoding Help? languages look similar). They show exactly the patterns found to be predictive of multi-task learning gains by Bingel and Søgaard (2017), who offer the explanation that when the auxiliary loss does not plateau before the target task, it can help the model out of local minima during training. In the low setting, i.e., when using only 100 auxiliary task examples, autoencoding, especially of random strings, works better than or equally well as lemmatization for highly agglutinative languages such as Basque, Finnish, Hungarian, and Turkish. Further, while random string autoencoding is in general less efficient than autoencoding or lemmatization, it performs on par with these auxiliary tasks in the set-up with least auxiliary task"
W18-3401,W17-0225,1,0.850592,"or results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-reso"
W18-3401,C16-1333,1,0.850199,"owever, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevan"
W18-3401,D11-1005,0,0.0784096,"Missing"
W18-3401,K17-2001,0,0.389323,"- ) .6141( - ) .6188( - ) .6529( - ) Table 1: Averaged accuracies and standard deviations over 5 training runs on UD 2.0 test sets, with 478 tokens of POS-annotated data and varying amounts of data for the auxiliary task (low, medium and high). Best result for each language in bold. Autoencoding and lemmatization are on par across the board, and with 100 training sentences (low), random autoencoding is also competitive. 6.2 Why does Random String Autoencoding Help? languages look similar). They show exactly the patterns found to be predictive of multi-task learning gains by Bingel and Søgaard (2017), who offer the explanation that when the auxiliary loss does not plateau before the target task, it can help the model out of local minima during training. In the low setting, i.e., when using only 100 auxiliary task examples, autoencoding, especially of random strings, works better than or equally well as lemmatization for highly agglutinative languages such as Basque, Finnish, Hungarian, and Turkish. Further, while random string autoencoding is in general less efficient than autoencoding or lemmatization, it performs on par with these auxiliary tasks in the set-up with least auxiliary task"
W18-3401,P15-2044,1,0.928316,"Missing"
W18-3401,N16-1077,0,0.0302103,"Lemmatization is a task from the area of inflectional morphology. In particular, it is a special case of morphological inflection. Its goal is to map a given inflected word form to its lemma, e.g., sue˜no 7→ so˜nar. (6) Word autoencoding. For the word autoencoding task, we use the inflected forms from the SIGMORPHON 2017 shared task dataset for each respective setting. Due to identical forms for different slot in the morphological paradigm of some lemmas, we might have duplicate examples in those datasets. Sequence-to-sequence models have shown strong performances on morphological inflection (Aharoni et al., 2016; Kann and Sch¨utze, 2016; Makarov et al., 2017). Therefore, when morphological dictionaries are available, we can easily combine a neural model for lemmatization with a POS tagger, using our architecture. Our intuition for this auxiliary task is that it should be possible to include morphological information into our character-based word representations. Formally, the task can be described as follows. Let AL be a discrete alphabet for language L and let TL be a set of morphological tags for L. The morphological paradigm π of a lemma w in L is a set of pairs n o π(w) = fk [w], tk (7) Random s"
W18-3401,P15-1166,0,0.0831235,"Missing"
W18-3401,P07-1094,0,0.133713,"Missing"
W18-3401,P17-2054,1,0.900977,"Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-resource settings and, thus, want to mention some important work here. Cross-lingual approaches have been used for a large variety of tasks, e.g., automatic speech recognition (Huang et al., 2013), entity recognition (Wang and Manning, 2014), language modeling (Ts"
W18-3401,N18-1172,1,0.839691,"nt training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-resource settings and, thus, want to me"
W18-3401,P16-2067,1,0.917337,"ting of a shared character LSTM (down), as well as a sequence labeling (up) and a sequence-tosequence (right) part. subword-level LSTM is bi-directional and operates on the character level (Ling et al., 2015; Ballesteros et al., 2015). Its input is the character sequence of each input word, represented by the embedding sequence c1 , c2 , . . . , cm . The final character-based representation of each word is the concatenation of the two last LSTM hidden states: POS Tagging with Subword-level Supervision Hierarchical POS tagging LSTMs that receive both word-level and subword-level input, such as Plank et al. (2016), are known to perform well on unseen words. This is due to their ability to associate subword-level patterns with POS tags. However, hierarchical LSTMs are also very expressive, and thus prone to overfitting. We believe that using subword-level auxiliary tasks to regularize the character-level encoding in hierarchical LSTMs is a flexible and efficient way to get the best of both worlds: such a model is still able to make predictions about unknown words, but the subword-level auxiliary task should prevent it from overfitting. 2.1 vc,i = conc(LSTMc,f (c1:m ), (1) LSTMc,b (cm:1 )) Second, a cont"
W18-3401,P17-1194,0,0.0202508,". Furthermore, their model is also a multi-task model, being trained jointly on predicting the POS and the log-frequency of a word. Their architecture obtained state-of-the-art results for POS tagging in several languages. Hence, in the low-resource setting considered here, we build upon the architecture developed by Plank et al. (2016), and extend it to a multi-task architecture involving sequenceto-sequence learning. Note though that in contrast to our setup, their tasks are both sequence-labeling tasks and using the same input for both tasks. The same holds true for the multi-task model by Rei (2017), which is used to investigate how an additional language modeling objective could improve performance for sequence labeling without any need for additional training data. He reported Minimum Distance low medium high 0.031 0.027 0.074 0.032 0.031 0.092 0.033 0.032 0.104 0.018 Table 4: Minimum character embedding distances, averaged over all languages. In small sample regimes, pushing individual characters further apart is a potential advantage, since character collisions can be hurtful at inference time. We note how this is analogous to feature swamping of covariate features, as described in S"
W18-3401,P17-1182,1,0.881428,"Missing"
W18-3401,P16-2090,1,0.82551,"Missing"
W18-3401,P11-2120,1,0.816951,"Missing"
W18-3401,P16-2038,1,0.707545,"o obtain better performance on a sequence classification task. However, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lin"
W18-3401,Q16-1023,0,0.0435006,"individual languages. Here, we notice that autoencoders often outperform lemmatization for agglutinative languages. An explanation for this might be that agglutinative morphology is harder to learn, and the chance of overfitting on a small sample is therefore higher. Hyperparameters For all networks, we use 300-dimensional character embeddings, 64-dimensional word embeddings and 100-dimensional LSTM hidden states. Encoder and decoder LSTMs have 1 hidden layer each. For training, we use ADAM (Kingma and Ba, 2014), as well as word dropout and character dropout, each with a coefficient of 0.25 (Kiperwasser and Goldberg, 2016). Gaussian noise is added to the concatenation of the last states of the character LSTMs for POS tagging. All models are trained using early stopping, with a minimum number of 75 (single-task and low), 30 (medium) or 20 (high) epochs and a maximum number of 300 epochs, which is never reached. We stop training if we obtain no improvement for 10 consecutive epochs. The best model on the development set is used for testing. 5 Results The test results for all languages and settings are presented in Table 1. Our first observation is that using 100 words of auxiliary task data seems to be sufficient"
W18-3401,N06-1012,0,0.0581584,"), which is used to investigate how an additional language modeling objective could improve performance for sequence labeling without any need for additional training data. He reported Minimum Distance low medium high 0.031 0.027 0.074 0.032 0.031 0.092 0.033 0.032 0.104 0.018 Table 4: Minimum character embedding distances, averaged over all languages. In small sample regimes, pushing individual characters further apart is a potential advantage, since character collisions can be hurtful at inference time. We note how this is analogous to feature swamping of covariate features, as described in Sutton et al. (2006). Sutton et al. (2006) use a group lasso regularizer to prevent feature swamping. In the same way, we could also detect distributionally similar characters and use a group lasso regularizer to prevent covariate characters to swamp each other. However, this effect can potentially also hurt performance if done in an uninformed way. We intuit that this makes it also impossible for the model to learn useful similarities between characters (random string autoencod7 4.5 4.5 POS, main task POS+AE-Random, main task POS+AE-Random, aux. task POS+Lemmatization, main task POS+Lemmatization, aux. task POS+"
W18-3401,Q13-1001,0,0.0766196,"Missing"
W18-3401,D12-1127,0,0.148688,"Missing"
W18-3401,N16-1161,0,0.0651137,"Missing"
W18-3401,E17-1005,1,0.841116,"ce classification task. However, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we cons"
W18-3401,Q14-1005,0,0.0607788,"Missing"
W18-3401,D13-1032,0,0.0818875,"Missing"
W18-3401,P12-1066,0,0.0687448,"Missing"
W18-3401,H01-1035,0,0.151424,"Missing"
W18-3401,N16-1004,0,0.0425229,"Missing"
W18-3403,P17-1031,1,0.71589,"ann, Anders Søgaard, Joachim Bingel Dept. of Computer Science University of Copenhagen Denmark {marcel,soegaard,bingel}@di.ku.dk Abstract ducing normalization models, and the models we induce may be very specific to these datasets and not scale to writings from other historic periods— or even just writings from another monastery or by another author. Bollmann and Søgaard (2016) and Bollmann et al. (2017) recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks. Specifically, Bollmann et al. (2017) showed that multitask learning with German grapheme-to-phoneme translation as an auxiliary task improves a stateof-the-art sequence-to-sequence model for historical text normalization of medieval German manuscripts. Historical text normalization suffers from small datasets that exhibit high variance, and previous work has shown that multitask learning can be used to leverage data from related problems in order to obtain more robust models. Previous work has been limited to datasets from a specific language and a specific historical period, and it is not clear whether results generalize. It th"
W18-3403,C16-1013,1,0.580068,"Missing"
W18-3403,P15-1166,0,0.0417603,"ask models; rows are targets, columns auxiliary data. Left: full data; right: sparse data. Blue scores are improvements, reds increases in error. 5 Related work and conclusion References Joachim Bingel and Anders Søgaard. 2017. Identifying beneficial task relations for multi-task learning in deep neural networks. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 164–169. Association for Computational Linguistics. There has been considerable work on multitask sequence-to-sequence models for other tasks (Dong et al., 2015; Luong et al., 2015; Elliott and Kádár, 2017). There is a wide range of design questions and sharing strategies that we ignore here, focusing instead on under what circumstances the approach advocated in (Bollmann et al., 2017) works. Our main observation—that the size of the target dataset is most predictive of multi-task learning gains—runs counter previous findings for other NLP tasks (Martínez Alonso and Plank, 2017; Bingel and Søgaard, 2017). Martínez Alonso and Plank (2017) find that the label entropy of the auxiliary dataset is more predictive; Bingel and Søgaard (2017) find that the r"
W18-3403,I17-1014,0,0.0262746,"uxiliary data. Left: full data; right: sparse data. Blue scores are improvements, reds increases in error. 5 Related work and conclusion References Joachim Bingel and Anders Søgaard. 2017. Identifying beneficial task relations for multi-task learning in deep neural networks. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 164–169. Association for Computational Linguistics. There has been considerable work on multitask sequence-to-sequence models for other tasks (Dong et al., 2015; Luong et al., 2015; Elliott and Kádár, 2017). There is a wide range of design questions and sharing strategies that we ignore here, focusing instead on under what circumstances the approach advocated in (Bollmann et al., 2017) works. Our main observation—that the size of the target dataset is most predictive of multi-task learning gains—runs counter previous findings for other NLP tasks (Martínez Alonso and Plank, 2017; Bingel and Søgaard, 2017). Martínez Alonso and Plank (2017) find that the label entropy of the auxiliary dataset is more predictive; Bingel and Søgaard (2017) find that the relative differences in the steepness of the tw"
W18-3403,P11-1038,0,0.0247274,"e hypothesis about multi-task learning is that its usefulness correlates with either synergistic or complementary properties of the datasets. In other words, it is conceivable that the performance on one dataset improves most with an MTL setup when it is paired with another dataset that is either (i) very similar, or (ii) provides an additional signal that is useful for, but not covered in, the first dataset. The results in Figure 1 show that English with non-historical auxiliary data We also conduct a follow-up experiment on the (sparse) English dataset using a Twitter normalization dataset (Han and Baldwin, 2011) and a grammatical error corpus (Yannakoudakis et al., 2011) as auxiliary data. The results are presented in Table 4. Surprisingly, the Twitter dataset is actually more helpful than the best historical dataset; but of course, it is also in-language, unlike the historical datasets. 22 + 1 .8 D E A D E R -6 .4 + 5 .4 + 6 .5 + 4 .8 + 4 .5 + 4 .7 + 8 .3 + 5 .6 + 6 .5 D E A -1 8 .5 -1 4 .4 -1 0 .9 -1 4 .0 -1 4 .1 -1 4 .4 -1 7 .1 -1 4 .2 D E R -1 2 .9 + 0 .7 -5 .1 + 1 .3 -0 .5 + 0 .0 + 1 .4 + 6 .4 E N -1 8 .5 -2 3 .6 + 1 3 .2 + 1 7 .8 + 1 .7 + 5 .3 + 3 .1 + 5 .2 E S -1 0 .2 -1 3 .8 -1 8 .2 + 2 .2 +"
W18-3403,E17-1005,0,0.0421538,"rning helps at all seems to depend mostly on the dataset being evaluated. With few exceptions, for most datasets, the error rate either always improves or always worsens, independently of the auxiliary task. Considering the dataset statistics in Table 1, it appears that the size of the training corpus is the most important factor for these results. The four corpora that consistently benefit from MTL— German (RIDGES), Icelandic, Slovene (Bohorič), and Swedish—also have the smallest training sets, with about 50,000 tokens or less. For other tasks, different patterns have been observed (Martínez Alonso and Plank, 2017; Bingel and Søgaard, 2017); see Sec. 5. Table 3: Normalization accuracy (in percent) using the full or sparse training sets, both for the single-task setup and the best-performing multitask (MTL) setup. 4 Accuracy Results We evaluate our models using normalization accuracy, i.e., the percentage of correctly normalized word forms. Table 3 compares the accuracy scores of our single-task baseline models and for multi-task learning, in both the full and the sparse data scenario. For multi-task learning, we report the test set performance of the best target-auxiliary task pair combination, as eval"
W18-3403,P11-1019,0,0.0138796,"ulness correlates with either synergistic or complementary properties of the datasets. In other words, it is conceivable that the performance on one dataset improves most with an MTL setup when it is paired with another dataset that is either (i) very similar, or (ii) provides an additional signal that is useful for, but not covered in, the first dataset. The results in Figure 1 show that English with non-historical auxiliary data We also conduct a follow-up experiment on the (sparse) English dataset using a Twitter normalization dataset (Han and Baldwin, 2011) and a grammatical error corpus (Yannakoudakis et al., 2011) as auxiliary data. The results are presented in Table 4. Surprisingly, the Twitter dataset is actually more helpful than the best historical dataset; but of course, it is also in-language, unlike the historical datasets. 22 + 1 .8 D E A D E R -6 .4 + 5 .4 + 6 .5 + 4 .8 + 4 .5 + 4 .7 + 8 .3 + 5 .6 + 6 .5 D E A -1 8 .5 -1 4 .4 -1 0 .9 -1 4 .0 -1 4 .1 -1 4 .4 -1 7 .1 -1 4 .2 D E R -1 2 .9 + 0 .7 -5 .1 + 1 .3 -0 .5 + 0 .0 + 1 .4 + 6 .4 E N -1 8 .5 -2 3 .6 + 1 3 .2 + 1 7 .8 + 1 .7 + 5 .3 + 3 .1 + 5 .2 E S -1 0 .2 -1 3 .8 -1 8 .2 + 2 .2 + 0 .4 + 5 .3 + 7 .6 + 1 5 .6 H U -1 2 .1 -1 2 .5 -4 .8 -8 .0"
W18-5401,N16-1179,1,0.91746,"ay to obtain more robust NLP for more domains and languages. Multi-task learning has seen a revival in recent years, amplified by the success of deep learning techniques. Multi-task learning algorithms have been proven to lead to better performance for similar tasks, e.g., Baxter and others (2000), such as models of individual patients in health care, but recently multi-task learning has been applied to more loosely related sets of tasks in artificial intelligence. Examples include machine translation and syntactic parsing (Kaiser et al., 2017) or fixation prediction and sentence compression (Klerke, Goldberg, and Søgaard, 2016). Reported results ∗ This work was done, when the third author was affiliated with Dpt. of Computer Science, University of Copenhagen. 1 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 1–8 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics tion is to represent documents by what is known as bags of words, i.e., vector representations where each dimension encodes the presence or relative frequency of a particular n-gram (sequence of words). In this work, we use TF-IDF scores and only encode the pres"
W18-5401,P16-2067,1,0.814777,"dients of the loss curve at 10, 25, 50 and 75 percent of a training of 150 epochs, for each single-task, as well as the relative differences in the learning curve gradients, Both our single and multi-task learning architectures consist of a multi-layered perceptron with two hidden layers. In the case of multi-task learning, those layers are shared across all tasks. This setting is known as hard parameter sharing. Hard parameter sharing was first introduced by (Caruana, 1993) and used with success for different tasks, for example in (Collobert et al., 2011; Klerke, Goldberg, and Søgaard, 2016; Plank, Søgaard, and Goldberg, 2016). Hard parameter sharing greatly reduces the risk of overfitting. In fact, Baxter and others (2000) showed that the risk of overfitting the shared parameters is an order n where n is the number of tasks smaller than overfitting the task-specific parameters, i.e. the output layers. The input is thus a 10,000-dimensional TF-IDF vector representation of the texts. A training step consists of sampling a random batch of 32 instances, i.e. texts (for both main and auxiliary task in the case of multi-task learning) and minimizing the binary cross-entropy loss using an Adam optimizer (Kingma and Ba,"
W18-5401,N16-3020,0,0.0350264,"requency of a particular n-gram (sequence of words). In this work, we use TF-IDF scores and only encode the presence of unigrams (words). Each document is thus a |V |-dimensional array of floats, where |V |is the size of our vocabulary. The dataset that we use, is 20 Newsgroups.1 It has been used in several comparisons of classification algorithms (Dredze, Crammer, and Pereira, 2008; Crammer and Chechik, 2012), and some of the best results have been achieved with random forests and multi-layered perceptrons (deep learning models). The dataset, however, is also known to allow for over-fitting (Ribeiro, Singh, and Guestrin, 2016). Such overfitting can be remedied by multi-task learning. In this paper, we focus on multi-task learning with multi-layered perceptrons. rent neural networks, is thus motivated by a) an interest in whether previous findings generalize to document classification algorithms – in our case, multi-layered perceptrons, b) a practical consideration that any recommendations coming out of a study of document classification would be helpful to a wider audience. As already said, our focus on topic-level classification is motivated by the observation that this is an extremely common problem, and key to"
W18-5401,C12-2114,1,0.825135,"ve of multi-task learning gains. rec sport talk vehicles ... B=autos D=motorcycles A=guns politic religion C=mideast ... Figure 3: Problem 2 (Unrelated topics):A and B are the main tasks, C and D the auxiliary ones. Classification tasks We obtained 516 different pairs of mainauxiliary tasks for U NRELATED T OPICS. Note that the instances (i.e. pairs of mainauxiliary tasks) of R ELATED T OPICS are included in the set of instances of U NRELATED T OPICS. We have many more instances for U NRELATED Based on the 20 Newsgroups’ structure, we define pairs of tasks in ways similar to previous studies (Søgaard and Johannsen, 2012). We do this in two different ways, leading to Problem 1 and 2, defined below. 3 T OPICS than for R ELATED T OPICS, which means that we have many more training points when trying to predict the performance of multi-task learning. 2017), we apply the same hyper-parameter values to multi-task learning: number of hidden layers (2) and layer size (100). See §4.1 for number of epochs (100). 3.3 3.5 Representation of the data We want to investigate whether we can predict gains from multi-task learning given features of the data sets and single-task learning characteristics, as well as understand how"
W18-5401,E17-1005,0,0.0590851,"e ranks for R ELATED T OPICS Data all all all all aux main test train ratio aux ratio ratio aux main main main aux aux aux main main aux aux ratio ratio main ratio aux aux main Coefficient -0.93 -0.88 0.81 0.64 0.63 0.58 -0.49 -0.47 0.34 -0.31 0.26 0.24 -0.21 -0.17 0.17 0.13 -0.11 0.10 -0.08 -0.07 0.07 0.07 -0.05 0.04 0.04 0.03 -0.03 -0.02 -0.02 0.01 (b) Coefficients for U NRELATED T OPICS Table 3: Average inverse ranks and average logistic regression coefficients of various predictors of gains from multi-task learning haps a bit surprising in the light of recent results for sequence tagging (Alonso and Plank, 2017; Bingel and Søgaard, 2017). These recent results suggested that JSD is not predictive of multi-task learning performance at all. Of course, JSD over unigram occurrences is more closely related to the model bias arising when training document classification models on loosely related tasks, than to the model bias in sequence models. After all, transition probabilities are typically at least as important as emission probabilities in statistical sequence tagging models. multi-task learning gains. The intuition offered there is that multi-task learning is more likely to work when the target task q"
W18-5401,E17-2026,1,0.876757,"ulti-task learning work for loosely related document classification tasks? Anders Søgaard Emma Kerinec ´Ecole Normale Sup´erieure Dpt. of Computer Science University of Copenhagen de Lyon soegaard@di.ku.dk Lyon, France emma.kerinec@ens-lyon.fr Abstract have been promising, but in the case of loosely related tasks, often also with different label spaces, we have no guarantees that multi-task learning will work. Recent studies have tried to study empirically when multi-task learning leads to improvements (Alonso and Plank, 2017; Bingel and Søgaard, 2017). These preliminary studies have argued – Bingel and Søgaard (2017) most clearly – that multi-task learning is particularly effective when the target task otherwise plateaus faster than the auxiliary task. This study compliments these studies, considering new tasks and architectures, and our findings are largely supportive of this conclusion. In text classification, however, performance also depends crucially on the divergence between the marginal distributions of words in the target and auxiliary task. Document classification comes in many different flavors, including spam detection, sentiment analysis, customer support ticket routing, and diagnosis support"
W18-5404,W08-1301,0,0.0438607,"Missing"
W18-5404,P15-1111,0,0.128509,"re uniquely labeled by a single dependency label. Perturbation maps Since dots consistently attach to the root token of the sentence, and commas attach to their left neighbour or to the root token, we can remove and inject additional punctuation in a sentence without affecting the rest of its syntactic structure and without violating the wellformedness of dependency trees. Note, however, that injecting a root-dominated dot or comma may lead to crossing edges, i.e., turn a projective dependency tree into a non-projective one. This may lead to cascading errors for projective dependency parsers (Ng and Curran, 2015). In our experiments, arc-eager M ALT PARSER and S TAN FORD are the only projective parsers. We therefore propose two perturbation maps (Jo and Bengio, 2017): (a) simply removing punctuation, and (b) a simple injection scheme with two parameters χ and δ. Let a dependency structure be an ordered tree with n nodes decorated with words w1 , . . . , wn . At any node 1 ≤ i ≤ n, we (a) inject a comma at position i with probability χ and move nodes i ≤ j ≤ n to positions j + 1, increasing the size of the graph by 1; and (b) inject a dot at position i+1 with probability δ and move nodes i < j ≤ n to p"
W18-5404,D14-1082,0,0.0264032,"Missing"
W18-5404,W16-0518,0,0.0259517,"ovember 1, 2018. 2018 Association for Computational Linguistics words, and many Asian languages, e.g., Thai and Lao, still do not. A period is typically used to mark the end of a grammatical sentence, and commas are often used to separate clauses. Therefore, punctuation also correlates strongly with properties of syntactic structures and is therefore very predictive of dependency structures. Variation in punctuation is often observed in informal texts, but variation may also be the result of errors. Punctuation errors are by far the most frequent error type in scientific writing, for example (Remse et al., 2016). Modern parsers should be robust to such variation, just like humans are (Baldwin and Coady, 1978). nsubj amod punct John , punct dobj punct 27 , likes jazz . Figure 1: Punctuation in Stanford dependencies We show that (a) projective parsers are, unsurprisingly, more sensitive to punctuation injection than non-projective ones, since punctuation injection may introduce crossing edges, and (b) neural parsers are more sensitive than vintage parsers. The latter is our main contribution, but we also show that training a neural parser without punctuation outperforms all parsers trained in a regular"
W18-5404,W11-0303,0,0.0115619,"PARSER is much more robust than the other parsers. That said, it still does much worse than the UUPARSER trained without punctuation. Evaluation on informal text with non-standard punctuation We also evaluate the models on sentences with non-standard punctuation in the development sections in the Google Web Treebank with informal text (from Yahoo Answers and user reviews). Specifically, we evaluate the models on sentences with more than one dot. Again, we show that the neural dependency parser trained without punctuation is superior to the other parsers. 5 Related work Punctuation in parsing Spitkovsky et al. (2011) introduced the idea of splitting sentences at punctuation and imposing parsing restrictions over the fragments and observed significant improvements in the context of unsupervised parsing. Ng and Curran (2015) aim to prevent cascading errors by enforcing correct punctuation arcs. They restrict themselves to projective dependency parsing; erroneous punctuation arcs do not lead to cascading errors in non-projective dependency parsing. Ma et al. (2014), motivated by the same observation, treat punctuation marks as properties of their neighboring words rather than as individual tokens, showing im"
W18-5404,W17-5401,0,0.0438876,"Missing"
W18-5404,P15-1147,0,0.0266262,"Missing"
W18-5404,D17-1215,0,0.0640031,"Missing"
W18-5404,Q16-1023,0,0.0623643,"Missing"
W18-5404,K17-3022,1,0.885964,"Missing"
W18-5404,W17-6314,1,0.894343,"Missing"
W18-5404,P14-2128,0,\N,Missing
W18-5409,D16-1131,1,0.5,"ieved from context. Consider the following example from Rønning et al. (2018): (1) Anders Søgaard University of Copenhagen soegaard@di.ku.dk 1. We select linguistically-defined subsets of the data, and examine the output of different systems on these subsets; and If [this is not practical], explain why. 2. we examine activations of the networks, focusing in particular on the activations associated with the wh-word that identifies a sluice, to assess how well the network notices, remembers, and classifies them. Here, the antecedent is the complete sentential constituent, this is not practical. Anand and Hardt (2016) present a sluice resolution system, in which candidate antecedents are required to be sentential constituents. Furthermore, each candidate is represented by features manually defined over syntactic dependency structures. Anand and Hardt report an accuracy of antecedent selection of 0.72, and a token-level F1 score 0.72, applied to a dataset based on news content (Anand and McCloskey, 2015). Rønning et al. (2018) show that neural network architectures with multi-task learning are able to achieve comparable results to Anand and Hardt, without relying on structured syntactic annotation or handcr"
W18-5409,D08-1069,0,0.0446704,"ression, (CCG) super-tagging and (POS) tagging. Plain denotes a RNN-layer with no associated task. RHS: This system also cascades the auxiliary tasks. However, it uses a different set of auxiliary tasks than KGS, computes label embeddings that are also passed on to subsequent layers, and has skip connections from the embedding layer to all layers in the network. RHS uses the auxiliary tasks described below: antecedent among a set of candidates. The candidate set consist of all sentential constituents within a predefined context window of the sluice. AH parameterizes a log-linear score akin to Denis and Baldridge (2008) using hill-climbing. Anand and Hardt (2016) evaluates AH by the accuracy of chosen constituents, but also report token-level F1 scores. We focus on token-F1 score in this paper to compare with the neural networks as these do not operate with predefined constituents. 2.2 CCG super-tagging: another form of partial parsing, using a more fine-grained tagset. Chunking: same task as described for KSG. POS tagging: determining the syntactic category (part of speech) of a word in context. RHS cycles through all data for each of the auxiliary tasks during a epoch, only layers up to and including the l"
W18-5409,N16-1179,1,0.830416,"obtained by applying the model in Pennington et al. (2014) on Wikipedia and Gigaword 5. The sluice expression is not specifically marked in the text. Instead, a copy of the sluice expression is prefixed to the sequence. The networks assign either a begin, inside or outside (BIO) label to each token and the task is to align these with the span of the antecedent. The three networks are: System AH Random RHS KSG BI BI: This is a single-task, two-layered longshort-term memory (LSTM) network, with a projection layer and a softmax layer. KSG: This is a cascading, three-layered LSTM, as described by Klerke et al. (2016). The KSG system is trained with the following auxiliary tasks: Score 0.67 0.45 0.70 0.64 0.54 Table 1: Token-F1 Score on complete test set. 3 Chunking: a partial parsing task, in which we need to identify the boundaries of the phrases in a sentence; and Data Subsets Below we introduce various linguistic dimensions of the ellipsis resolution data, which we can use to 67 a representation that was too dependent on adjacency information; in our case, it seems like the neural architectures have successfully learned a representation that makes them less dependent on adjacency than the baseline BI s"
W18-5409,Q16-1037,0,0.0271655,"tecedent is adjacent. AH scores marginally higher on adjacent candidates than RHS, but has a significantly higher difference between adjacent and non-adjacent compared to the three neural systems. Since AH has explicit constituency information, it makes sense that it would have a high token-level F1 score, when the antecedent is adjacent. For the MTL systems, RHS and KSG, adjacency also makes a big difference in F1 scores, albeit less than for AH. The smallest performance drop is seen with BI, the single-task system. This is somewhat comparable to the case of subject-verb agreement studied in Linzen et al. (2016), where it was found that an LSTM could learn structural information necessary to identify the subject, but that performance decreased when the subject was not the nearest noun phrase to the verb. In their case, the neural model had learned LR 85.0 85.6 75.2 62.3 R 84.9 85.5 74.1 62.3 L 85.8 80.7 71.1 58.7 Table 3: F1 score for punctuation as boundary token for antecedent The results, which are given in Table 3, partially support our hypothesis. Focusing on LR (where punctuation marks both edges of the adjacent antecedent), we observe that RHS and KSG see improvements of .9 and .7 respectively"
W18-5409,D14-1162,0,0.084106,"level F1 score for each system on the dataset used in Rønning et al. (2018). It also includes the baseline performance of choosing a random constituent within the two sentence window of the sluice site. This is the same window size AH uses to determine its candidate set. Neural Network Architectures We examine three neural network architectures, all defined in Rønning et al. (2018), and depicted in Figure 1. In all three systems, the input is a sequence of tokens without syntactic annotations. All our neural networks use 50 dimensional fixed GloVe embeddings, obtained by applying the model in Pennington et al. (2014) on Wikipedia and Gigaword 5. The sluice expression is not specifically marked in the text. Instead, a copy of the sluice expression is prefixed to the sequence. The networks assign either a begin, inside or outside (BIO) label to each token and the task is to align these with the span of the antecedent. The three networks are: System AH Random RHS KSG BI BI: This is a single-task, two-layered longshort-term memory (LSTM) network, with a projection layer and a softmax layer. KSG: This is a cascading, three-layered LSTM, as described by Klerke et al. (2016). The KSG system is trained with the f"
W18-5409,N18-2038,1,0.845211,"Missing"
W18-6210,P07-1056,0,0.156335,"Missing"
W18-6210,cook-stevenson-2010-automatically,0,0.184415,"looking for the price. Certainly did not look like a wallet. Very disappointed in the quality.” Figure 1: Examples of reviews Introduction feature selection perform significantly better than models that simply rely on the most predictive features across the training data without estimating temporal shifts. Sentiment analysis models often rely on data that is several years old. Such data, e.g., product reviews, continuously undergo shifts, leading to changes in term frequency, for example. We also observe the emergence of novel expressions, as well as the amelioration and pejoration of words (Cook and Stevenson, 2010). Such changes are typically studied over decades or centuries; however, we hypothesize that change is continuous, and small changes can be detected over shorter time spans (years), and that their cumulation can influence the quality of sentiment analysis models. In this paper, we analyze temporal polarity changes of individual features using product reviews data. Additionally, we show that predictive feature selection, trying to counteract shifts in polarity, significantly improves model accuracy over time. Training data Model accuracy Runs 1 2 3 Mean 2001-2004 2008-2011 0.858 0.877 0.855 0.8"
W19-4420,N18-2046,0,0.104812,"its to estimate the channel model. Additionally, we use two pretrained language models: 1) Google’s BERT model, which we fine-tune for specific error types and 2) OpenAI’s GPT-2 model, utilizing that it can operate with previous sentences as context. Furthermore, we search for the optimal combinations of corrections using beam search. 1 Unsupervised GEC In order to combat these problems, in recent years several approaches to GEC have used the concept of language modeling, which allows for training GEC systems without supervised data, and have so far given promising results. Bryant and Briscoe (2018) uses a 5-gram language model while Makarenkov et al. (2019) uses a bidirectional LSTM-based language model. Kaili et al. (2018) fine-tunes LSTM-based language models for specific error types. Using a language modeling approach means that we can create models that are trained unsupervised by only being based on high quality native text corpora. This means that our systems will only require a small amount of labeled data for tuning purposes. We can therefore build GEC systems for any language given enough native text. Introduction Grammatical Error Correction Grammatical Error Correction (GEC)"
