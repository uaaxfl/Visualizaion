2017.lilt-15.1,Lexical Factorization and Syntactic Behavior,2017,-1,-1,2,0,993,james pustejovsky,"Linguistic Issues in Language Technology, Volume 15, 2017",0,"In this paper, we examine the correlation between lexical semantics and the syntactic realization of the different components of a word{'}s meaning in natural language. More specifically, we will explore the effect that lexical factorization in verb semantics has on the suppression or expression of semantic features within the sentence. Factorization was a common analytic tool employed in early generative linguistic approaches to lexical decomposition, and continues to play a role in contemporary semantics, in various guises and modified forms. Building on the unpublished analysis of verbs of seeing in Joshi (1972), we argue here that the significance of lexical factorization is twofold: first, current models of verb meaning owe much of their insight to factor-based theories of meaning; secondly, the factorization properties of a lexical item appear to influence, both directly and indirectly, the possible syntactic expressibility of arguments and adjuncts in sentence composition. We argue that this information can be used to compute what we call the factor expression likelihood (FEL) associated with a verb in a sentence. This is the likelihood that the overt syntactic expression of a factor will cooccur with the verb. This has consequences for the compositional mechanisms responsible for computing the meaning of the sentence, as well as significance in the creation of computational models attempting to capture linguistic behavior over large corpora."
2017.lilt-15.2,Factorization of Verbs: An Analysis of Verbs of Seeing,2017,0,0,1,1,33217,aravind joshi,"Linguistic Issues in Language Technology, Volume 15, 2017",0,"In 1972, I spent a year at the Institute for Advanced Study, Princeton, as a Guggenheim Fellow. I was invited to join a group of visitors, all interested in various aspects of language. George Miller was heading this group. The other visitors were Phil Johnson Laird, now at Princeton University and Pim Levelt, now at Max Plank Institute, Nijmegen, The Netherlands. It was truly a wonderful year. I was allowed to continue my research in Computational Linguistics and branch out in other directions of my choosing. I continued my work in Mathematical Linguistics, xe2x80x98testingxe2x80x99 it by explaining it to George, Phil, and Pim. These interactions allowed me to formulate a long term program in Mathematical Linguistics (ML). At the same time I was quite attracted by the work on Verbs of Motion that George and Phil had initiated. I thought it might be interesting to look at the Verbs of Seeing from this perspective. George encouraged me very strongly, always reminding me to follow an independent direction. The enclosed manuscript was finished in June 1972, a couple of months before I returned to Penn, where they roped me into the job of the Department Chair for Computer and Information Science at the University of Pennsylvania, a newly created Department. I was xe2x80x98stuckxe2x80x99 in this job for almost 12 years. Over the years, I have talked about this paper and the general topic of lexical decomposition but I never got around to write a long paper on this topic."
W16-1704,A Discourse-Annotated Corpus of Conjoined {VP}s,2016,13,12,4,0,9578,bonnie webber,Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with {ACL} 2016 ({LAW}-X 2016),0,"English grammars indicate a variety of relations holding between conjoined VPs. VPs conjoined by and evince such senses as Result, Temporal Sequence and Concession. Although all these senses are ones associated with discourse relations, conjoined VPs have not been fully included in discourse annotation. Because of the value of discourse-annotated corpora for developing approaches to automated sense recognition, we have added their annotation to the Penn Discourse TreeBank. This paper describes how tokens were identified; how the process of span and sense annotation was modified and extended in order to keep the annotation of intra-sentential multi-clausal structures consistent with the rest of the corpus; and what the resulting corpus looks like, in terms of token frequency and common sense patterns."
C16-2026,Annotating Discourse Relations with the {PDTB} Annotator,2016,6,2,4,1,24887,alan lee,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"The PDTB Annotator is a tool for annotating and adjudicating discourse relations based on the annotation framework of the Penn Discourse TreeBank (PDTB). This demo describes the benefits of using the PDTB Annotator, gives an overview of the PDTB Framework and discusses the tool{'}s features, setup requirements and how it can also be used for adjudication."
W15-2707,Bridging Sentential and Discourse-level Semantics through Clausal Adjuncts,2015,16,2,5,1,4772,rashmi prasad,"Proceedings of the First Workshop on Linking Computational Models of Lexical, Sentential and Discourse-level Semantics",0,"It is in PropBankxe2x80x99s ARGM annotation of clausal adjuncts that sentential semantics meets discourse relation annotation in the Penn Discourse TreeBank. This paper discusses complementarities between the two annotation systems: How PropBank ARGM annotation can be used to seed annotation of additional discourse relations in the PDTB, and how PDTB annotation can be used to refine or enrich PropBank ARGM annotation."
J15-4009,{O}bituaries: Jane {J}. Robinson,2015,-1,-1,3,0,30392,barbara grosz,Computational Linguistics,0,None
W14-5101,Keynote Lecture 1: Complexity of Dependency Representations for Natural Languages,2014,0,0,1,1,33217,aravind joshi,Proceedings of the 11th International Conference on Natural Language Processing,0,None
J14-4007,"Reflections on the {P}enn {D}iscourse {T}ree{B}ank, Comparable Corpora, and Complementary Annotation",2014,77,38,3,1,4772,rashmi prasad,Computational Linguistics,0,"The Penn Discourse Treebank (PDTB) was released to the public in 2008. It remains the largest manually annotated corpus of discourse relations to date. Its focus on discourse relations that are either lexically-grounded in explicit discourse connectives or associated with sentential adjacency has not only facilitated its use in language technology and psycholinguistics but also has spawned the annotation of comparable corpora in other languages and genres.n n Given this situation, this paper has four aims: (1) to provide a comprehensive introduction to the PDTB for those who are unfamiliar with it; (2) to correct some wrong (or perhaps inadvertent) assumptions about the PDTB and its annotation that may have weakened previous results or the performance of decision procedures induced from the data; (3) to explain variations seen in the annotation of comparable resources in other languages and genres, which should allow developers of future comparable resources to recognize whether the variations are relevant to them; and (4) to enumerate and explain relationships between PDTB annotation and complementary annotation of other linguistic phenomena. The paper draws on work done by ourselves and others since the corpus was released."
W13-3702,"Invited talk: Dependency Representations, Grammars, Folded Structures, among Other Things!",2013,0,0,1,1,33217,aravind joshi,Proceedings of the Second International Conference on Dependency Linguistics ({D}ep{L}ing 2013),0,None
W12-4601,"Delayed Tree-Locality, Set-locality, and Clitic Climbing",2012,12,0,3,1,42141,joan chenmain,Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms ({TAG}+11),0,"Since Bleam's (2000) initial claim that capturing clitic climbing patterns in Romance requires the descriptive power of set-local MCTAG (Weir, 1988), alternative approaches to relaxing tree-locality restrictions have been developed, including delayed tree-local MCTAG (Chiang and Scheffler, 2008), which, unlike set-local MCTAG, is weakly equivalent to standard TAG. This paper compares 2-delayed treelocal MCTAG with set-local MCTAG in terms of how well the two formalisms can account for the clitic climbing data. We confirm that 2-delay tree-local MCTAG has the formal expressivity to cover the data by proposing an explicit grammar to do so. However, we also find that the constraint on set locality is particularly well-suited for capturing these clitic climbing patterns. I.e., though globally less restrictive, set-local MCTAG appears to be restrictive in just the right way in this specific case."
W12-3205,"Discourse Structure and Computation: Past, Present and Future",2012,105,18,2,0,9578,bonnie webber,Proceedings of the {ACL}-2012 Special Workshop on Rediscovering 50 Years of Discoveries,0,"The discourse properties of text have long been recognized as critical to language technology, and over the past 40 years, our understanding of and ability to exploit the discourse properties of text has grown in many ways. This essay briefly recounts these developments, the technology they employ, the applications they support, and the new challenges that each subsequent development has raised. We conclude with the challenges faced by our current understanding of discourse, and the applications that meeting these challenges will promote."
kolachina-etal-2012-evaluation,Evaluation of Discourse Relation Annotation in the {H}indi Discourse Relation Bank,2012,15,10,4,0.634921,24267,sudheer kolachina,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We describe our experiments on evaluating recently proposed modifications to the discourse relation annotation scheme of the Penn Discourse Treebank (PDTB), in the context of annotating discourse relations in Hindi Discourse Relation Bank (HDRB). While the proposed modifications were driven by the desire to introduce greater conceptual clarity in the PDTB scheme and to facilitate better annotation quality, our findings indicate that overall, some of the changes render the annotation task much more difficult for the annotators, as also reflected in lower inter-annotator agreement for the relevant sub-tasks. Our study emphasizes the importance of best practices in annotation task design and guidelines, given that a major goal of an annotation effort should be to achieve maximally high agreement between annotators. Based on our study, we suggest modifications to the current version of the HDRB, to be incorporated in our future annotation work."
W11-0806,Tree-Rewriting Models of Multi-Word Expressions,2011,7,7,2,0,7110,william schuler,Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World,0,"Multi-word expressions (MWEs) account for a large portion of the language used in day-to-day interactions. A formal system that is flexible enough to model these large and often syntactically-rich non-compositional chunks as single units in naturally occurring text could considerably simplify large-scale semantic annotation projects, in which it would be undesirable to have to develop internal compositional analyses of common technical expressions that have specific idiosyncratic meanings. This paper will first define a notion of functor-argument decomposition on phrase structure trees analogous to graph coloring, in which the tree is cast as a graph, and the elementary structures of a grammar formalism are colors. The paper then presents a formal argument that tree-rewriting systems, a class of grammar formalism that includes Tree Adjoining Grammars, are able to produce a proper superset of the functor-argument decompositions that string-rewriting systems can produce."
D11-1111,Computing Logical Form on Regulatory Texts,2011,27,0,2,1,42438,nikhil dinesh,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"The computation of logical form has been proposed as an intermediate step in the translation of sentences to logic. Logical form encodes the resolution of scope ambiguities. In this paper, we describe experiments on a modest-sized corpus of regulation annotated with a novel variant of logical form, called abstract syntax trees (ASTs). The main step in computing ASTs is to order scope-taking operators. A learning model for ranking is adapted fortius ordering. We design features by studying the problem of comparing the scope of one operator to another. The scope comparisons are used to compute ASTs, with an F-score of 90.6% on the set of ordering decisons."
W10-4407,Unavoidable Ill-nestedness in Natural Language and the Adequacy of Tree Local-{MCTAG} Induced Dependency Structures,2010,0,9,2,1,42141,joan chenmain,Proceedings of the 10th International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+10),0,None
W10-4310,Using entity features to classify implicit discourse relations,2010,11,46,2,0,20595,annie louis,Proceedings of the {SIGDIAL} 2010 Conference,0,"We report results on predicting the sense of implicit discourse relations between adjacent sentences in text. Our investigation concentrates on the association between discourse relations and properties of the referring expressions that appear in the related sentences. The properties of interest include coreference information, grammatical role, information status and syntactic form of referring expressions. Predicting the sense of implicit discourse relations based on these features is considerably better than a random baseline and several of the most discriminative features conform with linguistic intuitions. However, these features do not perform as well as lexical features traditionally used for sense prediction."
W10-4327,Discourse indicators for content selection in summarization,2010,27,90,2,0,20595,annie louis,Proceedings of the {SIGDIAL} 2010 Conference,0,"We present analyses aimed at eliciting which specific aspects of discourse provide the strongest indication for text importance. In the context of content selection for single document summarization of news, we examine the benefits of both the graph structure of text provided by discourse relations and the semantic sense of these relations. We find that structure information is the most robust indicator of importance. Semantic sense only provides constraints on content selection but is not indicative of important content by itself. However, sense features complement structure information and lead to improved performance. Further, both types of discourse information prove complementary to non-discourse features. While our results establish the usefulness of discourse features, we also find that lexical overlap provides a simple and cheap alternative to discourse for computing text structure with comparable performance for the task of content selection."
W10-3809,A Discriminative Approach for Dependency Based Statistical Machine Translation,2010,19,4,3,1,18971,sriram venkatapathy,Proceedings of the 4th Workshop on Syntax and Structure in Statistical Translation,0,"In this paper, we propose a dependency based statistical system that uses discriminative techniques to train its parameters. We conducted experiments on an EnglishHindi parallel corpora. The use of syntax (dependency tree) allows us to address the large word-reorderings between English and Hindi. And, discriminative training allows us to use rich feature sets, including linguistic features that are useful in the machine translation task. We present results of the experimental implementation of the system in this paper."
W10-3714,Multiword Expressions as Discourse Relation Markers ({DRM}s),2010,0,2,1,1,33217,aravind joshi,Proceedings of the 2010 Workshop on Multiword Expressions: from Theory to Applications,0,"Usually, by Multi-Word Expressions (MWEs) we mean expressions whose structure and meaning cannot be derived from their component words as they occur independently. In this talk I will discuss a different kind of multi-word expressions that behave as discourse relation markers (DRMs), yet do not seem to belong to well-defined syntactic classes. The apparent open-endedness of these expressions is a challenge for their automatic identification.1"
tonelli-etal-2010-annotation,Annotation of Discourse Relations for Conversational Spoken Dialogs,2010,14,33,4,0,38,sara tonelli,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we make a qualitative and quantitative analysis of discourse relations within the LUNA conversational spoken dialog corpus. In particular, we first describe the Penn Discourse Treebank (PDTB) and then we detail the adaptation of its annotation scheme to the LUNA corpus of Italian task-oriented dialogs in the domain of software/hardware assistance. We discuss similarities and differences between our approach and the PDTB paradigm and point out the peculiarities of spontaneous dialogs w.r.t. written text, which motivated some changes in the annotation strategy. In particular, we introduced the annotation of relations between non-contiguous arguments and we modified the sense hierarchy in order to take into account the important role of pragmatics in dialogs. In the final part of the paper, we present a comparison between the sense and connective frequency in a representative subset of the LUNA corpus and in the PDTB. Such analysis confirmed the differences between the two corpora and corroborates our choice to introduce dialog-specific adaptations."
prasad-etal-2010-exploiting,Exploiting Scope for Shallow Discourse Parsing,2010,17,30,2,1,4772,rashmi prasad,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present an approach to automatically identifying the arguments of discourse connectives based on data from the Penn Discourse Treebank. Of the two arguments of connectives, called Arg1 and Arg2, we focus on Arg1, which has proven more challenging to identify. Our approach employs a sentence-based representation of arguments, and distinguishes ''``intra-sentential connectives'''', which take both their arguments in the same sentence, from ''``inter-sentential connectives'''', whose arguments are found in different sentences. The latter are further distinguished by paragraph position into ''``ParaInit'''' connectives, which appear in a paragraph-initial sentence, and ''``ParaNonInit'''' connectives, which appear elsewhere. The paper focusses on predicting Arg1 of Inter-sentential ParaNonInit connectives, presenting a set of scope-based filters that reduce the search space for Arg1 from all the previous sentences in the paragraph to a subset of them. For cases where these filters do not uniquely identify Arg1, coreference-based heuristics are employed. Our analysis shows an absolute 3{\%} performance improvement over the high baseline of 83.3{\%} for identifying Arg1 of Inter-sentential ParaNonInit connectives."
C10-2118,Realization of Discourse Relations by Other Means: Alternative Lexicalizations,2010,21,52,2,1,4772,rashmi prasad,Coling 2010: Posters,0,"Studies of discourse relations have not, in the past, attempted to characterize what serves as evidence for them, beyond lists of frozen expressions, or markers, drawn from a few well-defined syntactic classes. In this paper, we describe how the lexicalized discourse relation annotations of the Penn Discourse Treebank (PDTB) led to the discovery of a wide range of additional expressions, annotated as AltLex (alternative lexicalizations) in the PDTB 2.0. Further analysis of AltLex annotation suggests that the set of markers is open-ended, and drawn from a wider variety of syntactic types than currently assumed. As a first attempt towards automatically identifying discourse relation markers, we propose the use of syntactic paraphrase methods."
W09-3029,The {H}indi Discourse Relation Bank,2009,7,32,5,0,42439,umangi oza,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"We describe the Hindi Discourse Relation Bank project, aimed at developing a large corpus annotated with discourse relations. We adopt the lexically grounded approach of the Penn Discourse Treebank, and describe our classification of Hindi discourse connectives, our modifications to the sense classification of discourse relations, and some cross-linguistic comparisons based on some initial annotations carried out so far."
W08-2302,"Flexible Composition, Multiple Adjoining and Word Order Variation",2008,15,3,2,1,42141,joan chenmain,Proceedings of the Ninth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+9),0,None
W08-0614,A Pilot Annotation to Investigate Discourse Connectivity in Biomedical Text,2008,5,3,6,0,4041,hong yu,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"The goal of the Penn Discourse Treebank (PDTB) project is to develop a large-scale corpus, annotated with coherence relations marked by discourse connectives. Currently, the primary application of the PDTB annotation has been to news articles. In this study, we tested whether the PDTB guidelines can be adapted to a different genre. We annotated discourse connectives and their arguments in one 4,937-token full-text biomedical article. Two linguist annotators showed an agreement of 85% after simple conventions were added. For the remaining 15% cases, we found that biomedical domain-specific knowledge is needed to capture the linguistic cues that can be used to resolve inter-annotator disagreement. We found that the two annotators were able to reach an agreement after discussion. Thus our experiments suggest that the PDTB annotation can be adapted to new domains by minimally adjusting the guidelines and by adding some further domain-specific linguistic cues."
prasad-etal-2008-penn,The {P}enn {D}iscourse {T}ree{B}ank 2.0.,2008,23,680,6,1,4772,rashmi prasad,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We present the second version of the Penn Discourse Treebank, PDTB-2.0, describing its lexically-grounded annotations of discourse relations and their two abstract object arguments over the 1 million word Wall Street Journal corpus. We describe all aspects of the annotation, including (a) the argument structure of discourse relations, (b) the sense annotation of the relations, and (c) the attribution of discourse relations and each of their arguments. We list the differences between PDTB-1.0 and PDTB-2.0. We present representative statistics for several aspects of the annotation in the corpus."
I08-7010,Towards an Annotated Corpus of Discourse Relations in {H}indi,2008,9,16,4,1,4772,rashmi prasad,Proceedings of the 6th Workshop on {A}sian Language Resources,0,"We describe our initial efforts towards developing a large-scale corpus of Hindi texts annotated with discourse relations. Adopting the lexically grounded approach of the Penn Discourse Treebank (PDTB), we present a preliminary analysis of discourse connectives in a small corpus. We describe how discourse connectives are represented in the sentence-level dependency annotation in Hindi, and discuss how the discourse annotation can enrich this level for research and applications. The ultimate goal of our work is to build a Hindi Discourse Relation Bank along the lines of the PDTB. Our work will also contribute to the cross-linguistic understanding of discourse connectives."
D08-1052,{LTAG} Dependency Parsing with Bidirectional Incremental Construction,2008,24,24,2,1,6930,libin shen,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we first introduce a new architecture for parsing, bidirectional incremental parsing. We propose a novel algorithm for incremental construction, which can be applied to many structure learning problems in NLP. We apply this algorithm to LTAG dependency parsing, and achieve significant improvement on accuracy over the previous best result on the same data set."
C08-2022,Easily Identifiable Discourse Relations,2008,18,82,6,0,5826,emily pitler,Coling 2008: Companion volume: Posters,0,"We present a corpus study of local discourse relations based on the Penn Discourse Tree Bank, a large manually annotated corpus of explicitly or implicitly realized relations. We show that while there is a large degree of ambiguity in temporal explicit discourse connectives, overall connectives are mostly unambiguous and allow high-accuracy prediction of discourse relation type. We achieve 93.09% accuracy in classifying the explicit relations and 74.74% accuracy overall. In addition, we show that some pairs of relations occur together in text more often than expected by chance. This finding suggests that global sequence classification of the relations in text can lead to better results, especially for implicit relations."
W07-1201,"Multi-Component {T}ree {A}djoining {G}rammars, Dependency Graph Models, and Linguistic Analyses",2007,16,3,2,1,42141,joan chenmain,{ACL} 2007 Workshop on Deep Linguistic Processing,0,"Recent work identifies two properties that appear particularly relevant to the characterization of graph-based dependency models of syntactic structure: the absence of interleaving substructures (well-nestedness) and a bound on a type of discontinuity (gap-degree xe2x89xa4 1) successfully describe more than 99% of the structures in two dependency treebanks (Kuhlmann and Nivre 2006). Bodirsky et al. (2005) establish that every dependency structure with these two properties can be recast as a lexicalized Tree Adjoining Grammar (LTAG) derivation and vice versa. However, multi-component extensions of TAG (MC-TAG), argued to be necessary on linguistic grounds, induce dependency structures that do not conform to these two properties (Kuhlmann and Mohl 2006). In this paper, we observe that several types of MC-TAG as used for linguistic analysis are more restrictive than the formal system is in principle. In particular, tree-local MC-TAG, tree-local MC-TAG with flexible composition (Kallmeyer and Joshi 2003), and special cases of set-local TAG as used to describe certain linguistic phenomena satisfy the well-nested and gap degree xe2x89xa4 1 criteria. We also observe that gap degree can distinguish between prohibited and allowed wh-extractions in English, and report some preliminary work comparing the predictions of the graph approach and the MC-TAG approach to scrambling."
W07-0407,Discriminative word alignment by learning the alignment structure and syntactic divergence between a language pair,2007,10,7,2,1,18971,sriram venkatapathy,"Proceedings of {SSST}, {NAACL}-{HLT} 2007 / {AMTA} Workshop on Syntax and Structure in Statistical Translation",0,"Discriminative approaches for word alignment have gained popularity in recent years because of the flexibility that they offer for using a large variety of features and combining information from various sources. But, the models proposed in the past have not been able to make much use of features that capture the likelihood of an alignment structure (the set of alignment links) and the syntactic divergence between sentences in the parallel text. This is primarily because of the limitation of their search techniques. In this paper, we propose a generic discriminative re-ranking approach for word alignment which allows us to make use of structural features effectively. These features are particularly useful for language pairs with high structural divergence (like English-Hindi, English-Japanese). We have shown that by using the structural features, we have obtained a decrease of 2.3% in the absolute value of alignment error rate (AER). When we add the cooccurence probabilities obtained from IBM model-4 to our features, we achieved the best AER (50.50) for the English-Hindi parallel corpus."
P07-1096,Guided Learning for Bidirectional Sequence Classification,2007,15,136,3,1,6930,libin shen,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm. We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features."
D07-1039,Detecting Compositionality of Verb-Object Combinations using Selectional Preferences,2007,29,35,3,0,9803,diana mccarthy,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"In this paper we explore the use of selectional preferences for detecting noncompositional verb-object combinations. To characterise the arguments in a given grammatical relationship we experiment with three models of selectional preference. Two use WordNet and one uses the entries from a distributional thesaurus as classes for representation. In previous work on selectional preference acquisition, the classes used for representation are selected according to the coverage of argument tokens rather than being selected according to the coverage of argument types. In our distributional thesaurus models and one of the methods using WordNet we select classes for representing the preferences by virtue of the number of argument types that they cover, and then only tokens under these classes which are representative of the argument head data are used to estimate the probability distribution for the selectional preference model. We demonstrate a highly signicant correlation between measures which use these xe2x80x98typebasedxe2x80x99 selectional preferences and compositionality judgements from a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individual features used in previous research on the same dataset."
W06-3902,Extracting formal specifications from natural language regulatory documents,2006,-1,-1,2,1,42438,nikhil dinesh,Proceedings of the Fifth International Workshop on Inference in Computational Semantics ({IC}o{S}-5),0,None
W06-3601,A Syntax-Directed Translator with Extended Domain of Locality,2006,32,63,3,0,8438,liang huang,Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing,0,"A syntax-directed translator first parses the source-language input into a parse-tree, and then recursively converts the tree into a string in the target-language. We model this conversion by an extended tree-to-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring. Initial experimental results on English-to-Chinese translation are presented."
W06-1635,Protein folding and chart parsing,2006,25,1,2,0,8351,julia hockenmaier,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"How can proteins fold so quickly into their unique native structures? We show here that there is a natural analogy between parsing and the protein folding problem, and demonstrate that CKY can find the native structures of a simplified lattice model of proteins with high accuracy."
W06-1503,The Metagrammar Goes Multilingual: A Cross-Linguistic Look at the V2-Phenomenon,2006,8,9,5,0,49777,alexandra kinyon,Proceedings of the Eighth International Workshop on Tree Adjoining Grammar and Related Formalisms,0,"We present an initial investigation into the use of a metagrammar for explicitly sharing abstract grammatical specifications among languages. We define a single class hierarchy for a metagrammar which allows us to automatically generate grammars for different languages from a single compact metagrammar hierarchy. We use as our linguistic example the verb-second phenomenon, which shows considerable variation while retaining a basic property, namely the fact that the verb can appear in one of two positions in the clause."
W06-1204,Using Information about Multi-word Expressions for the Word-Alignment Task,2006,13,22,2,1,18971,sriram venkatapathy,Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,0,"It is well known that multi-word expressions are problematic in natural language processing. In previous literature, it has been suggested that information about their degree of compositionality can be helpful in various applications but it has not been proven empirically. In this paper, we propose a framework in which information about the multi-word expressions can be used in the word-alignment task. We have shown that even simple features like point-wise mutual information are useful for word-alignment task in English-Hindi parallel corpora. The alignment error rate which we achieve (AER = 0.5040) is significantly better (about 10% decrease in AER) than the alignment error rates of the state-of-art models (Och and Ney, 2003) (Best AER = 0.5518) on the English-Hindi dataset."
W06-0305,Annotating Attribution in the {P}enn {D}iscourse {T}ree{B}ank,2006,22,22,4,1,4772,rashmi prasad,Proceedings of the Workshop on Sentiment and Subjectivity in Text,0,"An emerging task in text understanding and generation is to categorize information as fact or opinion and to further attribute it to the appropriate source. Corpus annotation schemes aim to encode such distinctions for NLP applications concerned with such tasks, such as information extraction, question answering, summarization, and generation. We describe an annotation scheme for marking the attribution of abstract objects such as propositions, facts and eventualities associated with discourse relations and their arguments annotated in the Penn Discourse TreeBank. The scheme aims to capture the source and degrees of factuality of the abstract objects. Key aspects of the scheme are annotation of the text spans signalling the attribution, and annotation of features recording the source, type, scopal polarity, and determinacy of attribution."
2006.amta-papers.8,Statistical Syntax-Directed Translation with Extended Domain of Locality,2006,30,183,3,0,8438,liang huang,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"In syntax-directed translation, the source-language input is first parsed into a parse-tree, which is then recursively converted into a string in the target-language. We model this conversion by an extended tree-to-string transducer that has multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear frame-work in order to incorporate other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring. Preliminary experiments on English-to-Chinese translation show a significant improvement in terms of translation quality compared to a state-of-the- art phrase-based system."
W05-0305,Attribution and the (Non-)Alignment of Syntactic and Discourse Arguments of Connectives,2005,15,56,5,1,42438,nikhil dinesh,Proceedings of the Workshop on Frontiers in Corpus Annotations {II}: Pie in the Sky,0,"The annotations of the Penn Discourse Treebank (PDTB) include (1) discourse connectives and their arguments, and (2) attribution of each argument of each connective and of the relation it denotes. Because the PDTB covers the same text as the Penn TreeBank WSJ corpus, syntactic and discourse annotation can be compared. This has revealed significant differences between syntactic structure and discourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and finally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation."
I05-1049,Relative Compositionality of Multi-word Expressions: A Study of Verb-Noun ({V}-N) Collocations,2005,22,9,2,1,18971,sriram venkatapathy,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Recognition of Multi-word Expressions (MWEs) and their relative compositionality are crucial to Natural Language Processing. Various statistical techniques have been proposed to recognize MWEs. In this paper, we integrate all the existing statistical features and investigate a range of classifiers for their suitability for recognizing the non-compositional Verb-Noun (V-N) collocations. In the task of ranking the V-N collocations based on their relative compositionality, we show that the correlation between the ranks computed by the classifier and human ranking is significantly better than the correlation between ranking of individual features and human ranking. We also show that the properties xe2x80x98Distributed frequency of object' (as defined in [27] ) and xe2x80x98Nearest Mutual Information' (as adapted from [18]) contribute greatly to the recognition of the non-compositional MWEs of the V-N type and to the ranking of the V-N collocations based on their relative compositionality."
H05-1102,Incremental {LTAG} Parsing,2005,37,33,2,1,6930,libin shen,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We present a very efficient statistical incremental parser for LTAG-spinal, a variant of LTAG. The parser supports the full adjoining operation, dynamic predicate coordination, and non-projective dependencies, with a formalism of provably stronger generative capacity as compared to CFG. Using gold standard POS tags as input, on section 23 of the PTB, the parser achieves an f-score of 89.3% for syntactic dependency defined on LTAG derivation trees, which are deeper than the dependencies extracted from PTB alone with head rules (for example, in Magerman's style)."
H05-1113,Measuring the Relative Compositionality of Verb-Noun ({V}-N) Collocations by Integrating Features,2005,22,55,2,1,18971,sriram venkatapathy,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Measuring the relative compositionality of Multi-word Expressions (MWEs) is crucial to Natural Language Processing. Various collocation based measures have been proposed to compute the relative compositionality of MWEs. In this paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of MWEs of V-N type. We show that the correlation of these features with the human ranking is much superior to the correlation of the traditional features with the human ranking. We then integrate the proposed features and the traditional features using a SVM based ranking function to rank the collocations of V-N type based on their relative compositionality. We then show that the correlation between the ranks computed by the SVM based ranking function and human ranking is significantly better than the correlation between ranking of individual features and human ranking."
W04-2703,Annotating Discourse Connectives and Their Arguments,2004,13,74,2,1,26240,eleni miltsakaki,Proceedings of the Workshop Frontiers in Corpus Annotation at {HLT}-{NAACL} 2004,0,"This paper describes a new, large scale discourse-level annotation project xe2x80x93 the Penn Discourse TreeBank (PDTB). We present an approach to annotating a level of discourse structure that is based on identifying discourse connectives and their arguments. The PDTB is being built directly on top of the Penn TreeBank and Propbank, thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms. We provide a detailed preliminary analysis of inter-annotator agreement xe2x80x93 both the level of agreement and the types of inter-annotator variation."
W04-2704,{P}roposition {B}ank {II}: Delving Deeper,2004,19,13,4,0,43023,olga babkomalaya,Proceedings of the Workshop Frontiers in Corpus Annotation at {HLT}-{NAACL} 2004,0,"The PropBank project is creating a corpus of text annotated with information about basic semantic propositions. PropBank I (Kingsbury & Palmer, 2002) added a layer of predicateargument information, or semantic roles, to the syntactic structures of the English Penn Treebank. This paper presents an overview of the second phase of PropBank Annotation, PropBank II, which is being applied to English and Chinese, and includes (Neodavidsonian) eventuality variables, nominal references, sense tagging, and connections to the Penn Discourse Treebank (PDTB), a project for annotating discourse connectives and their arguments."
W04-0212,Annotation and Data Mining of the {P}enn {D}iscourse {T}ree{B}ank,2004,22,33,3,1,4772,rashmi prasad,Proceedings of the Workshop on Discourse Annotation,0,"The Penn Discourse TreeBank (PDTB) is a new resource built on top of the Penn Wall Street Journal corpus, in which discourse connectives are annotated along with their arguments. Its use of standoff annotation allows integration with a stand-off version of the Penn TreeBank (syntactic structure) and PropBank (verbs and their arguments), which adds value for both linguistic discovery and discourse modeling. Here we describe the PDTB and some experiments in linguistic discovery based on the PDTB alone, as well as on the linked PTB and PDTB corpora."
miltsakaki-etal-2004-penn,The {P}enn {D}iscourse {T}reebank,2004,7,161,3,1,26240,eleni miltsakaki,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper describes a new discourse-level annotation project xe2x80x93 the Penn Discourse Treebank (PDTB) xe2x80x93 that aims to produce a large-scale corpus in which discourse connectives are annotated, along with their arguments, thus exposing a clearly defined level of discourse structure. The PDTB is being built directly on top of the Penn Treebank and Propbank, thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms. We present a preliminary analysis of inter-annotator agreement xe2x80x93 both the level of agreement and the types of inter-annotator variation."
W03-2608,Anaphoric arguments of discourse connectives: Semantic properties of antecedents versus non-antecedents,2003,6,19,4,1,26240,eleni miltsakaki,Proceedings of the 2003 {EACL} Workshop on The Computational Treatment of Anaphora,0,"We have argued extensively in prior work that discourse connectives can be analyzed as encoding predicate-argument relations whose arguments derived from the interpretation of discourse units. All adverbial connectives we have analyzed to date have expressed binary relations. But they are special in taking one of their two arguments structurally, and the other, anaphorically. As such, interpreting adverbial discourse connectives can be understood as a problem of anaphora resolution. In this paper we study the S-modifying adverbial connective xe2x80x9cinsteadxe2x80x9d and what, in the context, does and does not serve as antecedent for its anaphoric argument. This work extends earlier work investigating syntactic patterns of anaphoric arguments across a range of adverbial discourse connectives and the reliability with which these arguments can be annotated. The current work establishes, for 100 successive corpus instances of xe2x80x9cinsteadxe2x80x9d, lexico-syntactic features of the antecedents of their anaphoric arguments that can be automatically annotated and therefore used to distinguish actual antecedents from potential competitors in the context."
W03-1012,Using {LTAG} Based Features in Parse Reranking,2003,13,53,3,1,6930,libin shen,Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,0,"We propose the use of Lexicalized Tree Adjoining Grammar (LTAG) as a source of features that are useful for reranking the output of a statistical parser. In this paper, we extend the notion of a tree kernel over arbitrary sub-trees of the parse to the derivation trees and derived trees provided by the LTAG formalism, and in addition, we extend the original definition of the tree kernel, making it more lexicalized and more compact. We use LTAG based features for the parse reranking task and obtain labeled recall and precision of 89.7%/90.0% on WSJ section 23 of Penn Treebank for sentences of length xe2x89xa4 100 words. Our results show that the use of LTAG based tree kernel gives rise to a 17% relative difference in f-score improvement over the use of a linear kernel without LTAG based features."
W03-0402,An {SVM}-based voting algorithm with application to parse reranking,2003,26,52,2,1,6930,libin shen,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"This paper introduces a novel Support Vector Machines (SVMs) based voting algorithm for reranking, which provides a way to solve the sequential models indirectly. We have presented a risk formulation under the PAC framework for this voting algorithm. We have applied this algorithm to the parse reranking problem, and achieved labeled recall and precision of 89.4%/89.8% on WSJ section 23 of Penn Treebank."
P03-1064,A {SN}o{W} Based Supertagger with Application to {NP} Chunking,2003,24,14,2,1,6930,libin shen,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"Supertagging is the tagging process of assigning the correct elementary tree of LTAG, or the correct supertag, to each word of an input sentence. In this paper we propose to use supertags to expose syntactic dependencies which are unavailable with POS tags. We first propose a novel method of applying Sparse Network of Winnow (SNoW) to sequential models. Then we use it to construct a supertagger that uses long distance syntactical dependencies, and the supertagger achieves an accuracy of 92.41%. We apply the supertagger to NP chunking. The use of supertags in NP chunking gives rise to almost 1% absolute increase (from 92.03% to 92.95%) in F-score under Transformation Based Learning(TBL) frame. The surpertagger described here provides an effective and efficient way to exploit syntactic information."
J03-4002,Anaphora and Discourse Structure,2003,71,190,3,0,9578,bonnie webber,Computational Linguistics,0,"We argue in this article that many common adverbial phrases generally taken to signal a discourse relation between syntactically connected units within discourse structure instead work anaphorically to contribute relational meaning, with only indirect dependence on discourse structure. This allows a simpler discourse structure to provide scaffolding for compositional semantics and reveals multiple ways in which the relational meaning conveyed by adverbial connectives can interact with that associated with discourse structure. We conclude by sketching out a lexicalized grammar for discourse that facilitates discourse interpretation as a product of compositional rules, anaphor resolution, and inference."
W01-1803,The {XTAG} Project at {P}enn,2001,0,3,1,1,33217,aravind joshi,Proceedings of the Seventh International Workshop on Parsing Technologies,0,None
W00-2015,Relationship between strong and weak generative power of formal systems,2000,11,12,1,1,33217,aravind joshi,Proceedings of the Fifth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+5),0,None
W00-1605,Some Experiments on Indicators of Parsing Complexity for Lexicalized Grammars,2000,9,27,3,0.37037,8364,anoop sarkar,Proceedings of the {COLING}-2000 Workshop on Efficiency In Large-Scale Parsing Systems,0,"In this paper, we identify syntactic lexical ambiguity and sentence complexity as factors that contribute to parsing complexity in fully lexicalized grammar formalisms such as Lexicalized Tree Adjoining Grammars. We also report on experiments that explore the effects of these factors on parsing complexity. We discuss how these constraints can be exploited in improving efficiency of parsers for such grammar formalisms."
W00-1307,A Uniform Method of Grammar Extraction and Its Applications,2000,26,44,3,0,16067,fei xia,2000 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,"Grammars are core elements of many NLP applications. In this paper, we present a system that automatically extracts lexicalized grammars from annotated corpora. The data produced by this system have been used in several tasks, such as training NLP tools (such as Supertaggers) and estimating the coverage of hand-crafted grammars. We report experimental results on two of those tasks and compare our approaches with related work."
W00-1208,"Comparing Lexicalized Treebank Grammars Extracted from {C}hinese, {K}orean, and {E}nglish Corpora",2000,9,9,4,0,16067,fei xia,Second {C}hinese Language Processing Workshop,0,"In this paper, we present a method for comparing Lexicalized Tree Adjoining Grammars extracted from annotated corpora for three languages: English, Chinese and Korean. This method makes it possible to do a quantitative comparison between the syntactic structures of each language, thereby providing a way of testing the Universal Grammar Hypothesis, the foundation of modern linguistic theories."
P99-1006,Discourse Relations: A Structural and Presuppositional Account Using Lexicalised {TAG},1999,19,58,4,0,9578,bonnie webber,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"We show that discourse structure need not bear the full burden of conveying discourse relations by showing that many of them can be explained nonstructurally in terms of the grounding of anaphoric presuppositions (Van der Sandt, 1992). This simplifies discourse structure, while still allowing the realisation of a full range of discourse relations. This is achieved using the same semantic machinery used in deriving clause-level semantics."
J99-2004,{S}upertagging: An Approach to Almost Parsing,1999,58,331,2,0,4704,srinivas bangalore,Computational Linguistics,0,"In this paper, we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques. Our thesis is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context. The supertags are designed such that only those elements on which the lexical item imposes constraints appear within a given supertag. Further, each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear. This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser. But this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses. We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework. The supertags in LTAG combine both phrase structure information and dependency information in a single representation. Supertag disambiguation results in a representation that is effectively a parse (an almost parse), and the parser need only combine the individual supertags. This method of parsing can also be used to parse sentence fragments such as in spoken utterances where the disambiguated supertag sequence may not combine into a single structure."
W98-0315,Anchoring a {L}exicalized {T}ree-{A}djoining {G}rammar for Discourse,1998,10,58,2,0,9578,bonnie webber,Discourse Relations and Discourse Markers,0,"We here explore a ``fully'' lexicalized Tree-Adjoining Grammar for discourse that takes the basic elements of a (monologic) discourse to be not simply clauses, but larger structures that are anchored on variously realized discourse cues. This link with intra-sentential grammar suggests an account for different patterns of discourse cues, while the different structures and operations suggest three separate sources for elements of discourse meaning: (1) a compositional semantics tied to the basic trees and operations; (2) a presuppositional semantics carried by cue phrases that freely adjoin to trees; and (3) general inference, that draws additional, defeasible conclusions that flesh out what is conveyed compositionally."
W98-0119,Partial proof trees and structural modalities,1998,2,1,1,1,33217,aravind joshi,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,None
C96-2103,Coordination in {T}ree {A}djoining {G}rammars: Formalization and Implementation,1996,6,39,2,0.37037,8364,anoop sarkar,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,In this paper we show that an account for coordination can be constructed using the derivation structures in a lexicalized Tree Adjoining Grammar (LTAG). We present a notion of derivation in LTAGs that preserves the notion of fixed constituency in the LTAG lexicon while providing the flexibility needed for coordination phenomena. We also discuss the construction of a practical parser for LTAGs that can handle coordination including cases of non-constituent coordination.
P95-1036,Some Novel Applications of Explanation-Based Learning to Parsing {L}exicalized {T}ree-{A}djoining {G}rammars,1995,15,19,2,0,31242,srinivas,33rd Annual Meeting of the Association for Computational Linguistics,1,"In this paper we present some novel applications of Explanation-Based Learning (EBL) technique to parsing Lexicalized Tree-Adjoining grammars. The novel aspects are (a) immediate generalization of parses in the training set, (b) generalization over recursive structures and (c) representation of generalized parses as Finite State Transducers. A highly impoverished parser called a stapler has also been introduced. We present experimental results using EBL for different corpora and architectures to show the effectiveness of our approach."
J95-2003,{C}entering: A Framework for Modeling the Local Coherence of Discourse,1995,35,1408,2,0.428571,30392,barbara grosz,Computational Linguistics,0,"This paper concerns relationships among focus of attention, choice of referring expression, and perceived coherence of utterances within a discourse segment. It presents a framework and initial theory of centering intended to model the local component of attentional state. The paper examines interactions between local coherence and choice of referring expressions; it argues that differences in coherence correspond in part to the inference demands made by different types of referring expressions, given a particular attentional state. It demonstrates that the attentional state properties modeled by centering can account for these differences."
W94-0107,Complexity of Description of Primitives: Relevance to Local Statistical Computations,1994,-1,-1,1,1,33217,aravind joshi,The Balancing Act: Combining Symbolic and Statistical Approaches to Language,0,None
P94-1005,From Strings to Trees to Strings to Trees ... (Abstract),1994,0,0,1,1,33217,aravind joshi,32nd Annual Meeting of the Association for Computational Linguistics,1,None
C94-1024,Disambiguation of Super Parts of Speech (or Supertags): Almost Parsing,1994,6,130,1,1,33217,aravind joshi,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In a lexicalized grammar formalism such as Lexicalized Tree-Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (supertag) that localizes syntactic and semantic dependencies. Thus a parser for a lexicalized grammar must search a large set of supertags to choose the right ones to combine for the parse of the sentence. We present techniques for disambiguating supertags using local information such as lexical preference and local lexical dependencies. The similarity between LTAG and Dependency grammars is exploited in the dependency model of supertag disambiguation. The performance results for various models of supertag disambiguation such as unigram, trigram and dependency-based models are presented."
H93-1113,Natural Language Research,1993,-1,-1,1,1,33217,aravind joshi,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,None
H92-1019,Session 4: Statistical Language Modeling,1992,-1,-1,1,1,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,None
H92-1123,Natural Language Research,1992,-1,-1,1,1,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,None
A92-1030,{XTAG} - A Graphical Workbench for Developing {T}ree-{A}djoining {G}rammars,1992,23,27,3,0,5615,patrick paroubek,Third Conference on Applied Natural Language Processing,0,"We describe a workbench (XTAG) for the development of tree-adjoining grammars and their parsers, and discuss some issues that arise in the design of the graphical interface.Contrary to string rewriting grammars generating trees, the elementary objects manipulated by a tree-adjoining grammar are extended trees (i.e. trees of depth one or more) which capture syntactic information of lexical items. The unique characteristics of tree-adjoining grammars, its elementary objects found in the lexicon (extended trees) and the derivational history of derived trees (also a tree), require a specially crafted interface in which the perspective has shifted from a string-based to a tree-based system. XTAG provides such a graphical interface in which the elementary objects are trees (or tree sets) and not symbols (or strings).The kernel of XTAG is a predictive left to right parser for unification-based tree-adjoining grammar [Schabes, 1991]. XTAG includes a graphical editor for trees, a graphical tree printer, utilities for manipulating and displaying feature structures for unification-based tree-adjoining grammar, facilities for keeping track of the derivational history of TAG trees combined with adjoining and substitution, a parser for unification based tree-adjoining grammars, utilities for defining grammars and lexicons for tree-adjoining grammars, a morphological recognizer for English (75 000 stems deriving 280 000 inflected forms) and a tree-adjoining grammar for English that covers a large range of linguistic phenomena.Considerations of portability, efficiency, homogeneity and ease of maintenance, lead us to the use of Common Lisp without its object language addition and to the use of the X Window interface to Common Lisp (CLX) for the implementation of XTAG.XTAG without the large morphological and syntactic lexicons is public domain software. The large morphological and syntactic lexicons can be obtained through an agreement with ACL's Data Collection Initiative.XTAG runs under Common Lisp and X Window (CLX)."
H91-1035,Fixed and Flexible Phrase Structure: Coordination in {T}ree {A}djoining {G}rammars,1991,5,8,1,1,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"Phrase-structure grammars assign a unique phrase structure (constituency) to an unambiguous sentence. Thus, for example, John likes apples will be bracketed as follows (ignoring the phrase labels and ignoring some brackets not essential for our present purpose):(1) (John (likes apples))"
H91-1103,Natural Language Research,1991,0,0,1,1,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,None
E91-1005,Long-Distance Scrambling and {T}ree {A}djoining {G}rammars,1991,10,74,2,0,46850,tilman becker,Fifth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,None
1991.iwpt-1.1,Proceedings of the Second International Workshop on Parsing Technologies ({IWPT} {'}91),1991,-1,-1,5,0,56874,masaru tomita,Proceedings of the Second International Workshop on Parsing Technologies,0,"February 13-25, 1991"
W90-0214,A {TAG} analysis of the Third construction in {G}erman,1990,-1,-1,3,0,39158,anthony kroch,Proceedings of the First International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+1),0,None
W90-0217,Coordination in {TAG} in the manner of {CCG} (Combinatory Category Grammars): Fixed vs. Flexible Phrase Structure,1990,0,0,1,1,33217,aravind joshi,Proceedings of the First International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+1),0,None
H90-1010,Two Recent Developments in {T}ree {A}djoining {G}rammars: Semantics and Efficient Processing,1990,14,0,2,0.882353,55905,yves schabes,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"During the past year there have been two very significant developments in the area of Tree Adjoining Grammars (TAGs).The first development is a variant of TAGs, called synchronous TAGs, which allows TAG to be used beyond the confines of syntax by characterizing correspondences between languages. The formalism's intended usage is to relate expressions of natural languages to their associated semantics represented by a logical form language in TAG, or to their translates in another natural language. The formalism is incremental and inherently nondirectional. We will show by detailed examples the working of synchronous TAGs and some of its applications, for example in generation and in machine translation.The second development is the design of LR-style parsers for TAGs. LR parsing strategies evolved out of the original work of Knuth. Even though they are not powerful enough for NLP, they have found use in natural language processing (NLP) by solving by pseudo-parallelism conflicts between multiple choices. This gives rise to a class of powerful yet efficient parsers for natural language. In order to extend the LR techniques to TAGs it is necessary to find bottom-up automaton that is exactly equivalent to TAGs. This is precisely what has been achieved by the discovery of the Bottom-up Embedded Push Down Automaton (BEPDA). Using BEPDA, deterministic left to right parsers for the Tree Adjoining Languages have been developed."
H90-1099,Natural Language Research,1990,0,0,1,1,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"The main objective is basic research and system development leading to (1) characterization of information carried by (a) syntax, semantics, and discourse structure, (b) their relation to information carried by intonation, and (c) development of methods for using this information for generation and understanding; (2) development of architectures for integration of utterance planning with lexical, syntactic and intonational choice; (3) development of incremental strategies for using syntactic, semantic, and pragmatic knowledge in understanding and generating language."
C90-3001,Using Lexicalized Tags for Machine Translation,1990,13,73,3,0.666667,23421,anne abeille,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"Lexicalized Tree Adjoining Grammar (LTAG) is an attractive formalism for linguistic description mainly because of its extended domain of locality and its factoring recursion out from the domain of local dependencies (Joshi, 1985, Kroch and Joshi, 1985, Abeille, 1988). LTAG's extended domain of locality enables one to localize syntactic dependencies (such as filler-gap), as well as semantic dependencies (such as predicate-arguments). The aim of this paper is to show that these properties combined with the lexicalized property of LTAG are especially attractive for machine translation.The transfer between two languages, such as French and English, can be done by putting directly into correspondence large elementary units without going through some interlingual representation and without major changes to the source and target grammars. The underlying formalism for the transfer is synchronous Tree Adjoining Grammars (Shieber and Schabes [1990]). Transfer rules are stated as correspondences between nodes of trees of large domain of locality which are associated with words. We can thus define lexical transfer rules that avoid the defects of a mere word-to-word approach but still benefit from the simplicity and elegance of a lexical approach.We rely on the French and English LTAG grammars (Abeille [1988], Abeille [1990 (b)], Abeille et al. [1990], Abeille and Schabes [1989, 1990]) that have been designed over the past two years jointly at University of Pennsylvania and University of Paris 7-Jussieu."
W89-0235,The Relevance of Lexicalization to Parsing,1989,0,9,2,1,55905,yves schabes,Proceedings of the First International Workshop on Parsing Technologies,0,"In this paper, we investigate the processing of the so-called {`}lexicalized{'} grammar. In {`}lexicalized{'} grammars (Schabes, Abeille and Joshi, 1988), each elementary structure is systema tically associated with a lexical {`}head{'}. These structures specify extended domains of locality (as compared to CFGs) over which constraints can be stated. The {`}grammar{'} consists of a lexicon where each lexical item is associated with a finite number of structures for which that item is the {`}head{'} . There are no separate grammar rules. There are, of course, {`}rules{'} which tell us how these structures are combined. A general two-pass parsing strategy for {`}lexicalized{'} grammars follows naturally. In the first stage, the parser selects a set of elementary structures associated with the lexical items in the input sentence, and in the second stage the sentence is parsed with respect to this set. We evaluate this strategy with respect to two characteristics. First, the amount of filtering on the entire grammar is evaluated: once the first pass is performed, the parser uses only a subset of the grammar. Second, we evaluate the use of non-local information: the structures selected during the first pass encode the morphological value (and therefore the position in the string) of their {`}head{'}; this enables the parser to use non-local in form ation to guide its search. We take Lexicalized Tree Adjoining Grammars as an in stance of lexicalized grammar. We illustrate the organization of the grammar. Then we show how a general Earley-type TAG parser (Schabes and Joshi, 1988) can take advantage of lexicalization. Empirical data show that the filtering of the grammar and the non-local in formation provided by the two-pass strategy improve the performance of the parser. We explain how constraints over the elementary structures expressed by unification equations can be parsed by a simple extension of the Earley-type TAG parser. Lexicalization guarantees termination of the algorithm without special devices such as restrictors."
P89-1027,Treatment of Long Distance Dependencies in {LFG} and {TAG}: Functional Uncertainty in {LFG} Is a Corollary in {TAG},1989,9,7,1,1,33217,aravind joshi,27th Annual Meeting of the Association for Computational Linguistics,1,"In this paper the functional uncertainty machinery in LFG is compared with the treatment of long distance dependencies in TAG. It is shown that the functional uncertainty machinery is redundant in TAG, i.e., what functional uncertainty accomplishes for LFG follows from the TAG formalism itself and some aspects of the linguistic theory instantiated in TAG. It is also shown that the analyses provided by the functional uncertainty machinery can be obtained without requiring power beyond mildly context-sensitive grammars. Some linguistic and computational aspects of these results have been briefly discussed also."
H89-2053,An Evaluation of Lexicalization in Parsing,1989,21,0,1,1,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"In this paper, we evaluate a two-pass parsing strategy proposed for the so-called 'lexicalized' grammar. In 'lexicalized' grammars (Schabes, Abeille and Joshi, 1988), each elementary structure is systematically associated with a lexical item called anchor. These structures specify extended domains of locality (as compared to CFGs) over which constraints can be stated. The 'grammar' consists of a lexicon where each lexical item is associated with a finite number of structures for which that item is the anchor. There are no separate grammar rules. There are, of course, 'rules' which tell us how these structures are combined.A general two-pass parsing strategy for 'lexicalized' grammars follows naturally. In the first stage, the parser selects a set of elementary structures associated with the lexical items in the input sentence, and in the second stage the sentence is parsed with respect to this set. We evaluate this strategy with respect to two characteristics. First, the amount of filtering on the entire grammar is evaluated: once the first pass is performed, the parser uses only a subset of the grammar. Second, we evaluate the use of non-local information: the structures selected during the first pass encode the morphological value (and therefore the position in the string) of their anchor; this enables the parser to use non-local information to guide its search.We take Lexicalized Tree Adjoining Grammars as an instance of lexicalized grammar. We illustrate the organization of the grammar. Then we show how a general Earley-type TAG parser (Schabes and Joshi, 1988) can take advantage of lexicalization. Empirical data show that the filtering of the grammar and the non-local information provided by the two-pass strategy improve the performance of the parser."
H89-1035,Natural Language Research,1989,-1,-1,1,1,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,None
H89-1036,"Lexicalized {TAG}s, Parsing and Lexicons",1989,12,2,4,0.714286,23421,anne abeille,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"In our approach, each elementary structure is systematically associated with a lexical head. These structures specify extended domains of locality (as compared to a context-free grammar) over which constraints can be stated. These constraints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure. The 'grammar' consists of a lexicon where each lexical item is associated with a finite number of structures for which that item is the head. There are no separate grammar rules. There are, of course, 'rules' which tell us how these structures are composed. A grammar of this form will be said to be 'lexicalized'. A 'lexicalized' grammar naturally follows from the extended domain of locality of TAGs.A general parsing strategy for 'lexicalized' grammars is discussed. In the first stage, the parser selects a set of elementary structures associated with the lexical items in the input sentence, and in the second stage the sentence is parsed with respect to this set. An Earley-type parser for TAGs has been has been developed. It can be adapted to take advantage of the two steps parsing strategy. The system parses unification formalisms that have a CFG skeleton and that have a TAG skeleton.Along with the development of an Earley-type parser for TAGs, lexicons for English are under development. A lexicons for French is also being developed. Subsets of these lexicons are being incrementally interfaced to the parser.We finally show how idioms are represented in lexicalized TAGs. We assign them regular syntactic structures while representing them semantically as one entry. We finally show how they can be parsed by a parsing strategy as mentioned above."
P88-1032,An {E}arley-Type Parsing Algorithm for {T}ree {A}djoining {G}rammars,1988,9,61,2,1,55905,yves schabes,26th Annual Meeting of the Association for Computational Linguistics,1,"We will describe an Earley-type parser for Tree Adjoining Grammars (TAGs). Although a CKY-type parser for TAGs has been developed earlier (Vijay-Shanker and Joshi, 1985), this is the first practical parser for TAGs because as is well known for CFGs, the average behavior of Earley-type parsers is superior to that of CKY-type parsers. The core of the algorithm is described. Then we discuss modifications of the parsing algorithm that can parse extensions of TAGs such as constraints on adjunction, substitution, and feature structures for TAGs. We show how with the use of substitution in TAGs the system is able to parse directly CFGs and TAGs. The system parses unification formalisms that have a CFG skeleton and also those with a TAG skeleton. Thus it also allows us to embed the essential aspects of PATR-II."
P88-1034,{C}ombinatory {C}ategorial {G}rammars: Generative Power and Relationship to Linear Context-Free Rewriting Systems,1988,14,39,2,1,2464,david weir,26th Annual Meeting of the Association for Computational Linguistics,1,"Recent results have established that there is a family of languages that is exactly the class of languages generated by three independently developed grammar formalisms: Tree Adjoining Grammars, Head Grammars, and Linear Indexed Grammars. In this paper we show that Combinatory Categorial Grammars also generates the same class of languages. We discuss the structural descriptions produced by Combinatory Categorial Grammars and compare them to those of grammar formalisms in the class of Linear Context-Free Rewriting Systems. We also discuss certain extensions of Combinatory Categorial Grammars and their effect on the weak generative capacity."
C88-2121,Parsing Strategies with {`}Lexicalized{'} Grammars: Application to {T}ree {A}djoining {G}rammars,1988,22,202,3,1,55905,yves schabes,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper we present a general parsing strategy that arose from the development of an Earley-type parsing algorithm for TAGs (Schabes and Joshi 1988) and from recent linguistic work in TAGs (Abeille 1988).In our approach elementary structures are associated with their lexical heads. These structures specify extended domains of locality (as compared to a context-free grammar) over which constraints can be stated. These constraints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure.We state the conditions under which context-free based grammars can be 'lexicalized' without changing the linguistic structures originally produced. We argue that even if one extends the domain of locality of CFGs to trees, using only substitution does not give the freedom to choose the head of each structure. We show how adjunction allows us to 'lexicalize' a CFG freely.We then show how a 'lexicalized' grammar naturally follows from the extended domain of locality of TAGs and present some of the linguistic advantages of our approach.A novel general parsing strategy for 'lexicalized' grammars is discussed. In a first stage, the parser builds a set structures corresponding to the input sentence and in a second stage, the sentence is parsed with respect to this set. The strategy is independent of the linguistic theory adopted and of the underlying grammar formalism. However, we focus our attention on TAGs. Since the set of trees needed to parse an input sentence is supposed to be finite, the parser can use in principle any search strategy. Thus, in particular, a top-down strategy can be used since problems due to recursive structures are eliminated. The parser is also able to use non-local information to guide the search.We then explain how the Earley-type parser for TAGs can be modified to take advantage of this approach."
T87-1011,Unification and Some New Grammatical Formalisms,1987,5,4,1,1,33217,aravind joshi,Theoretical Issues in Natural Language Processing 3,0,"The key idea in the unification-based approaches to grammar is that we deal with informational structures (called feature structures) which encode a variety of linguistic information (lexical, syntactic, semantic, discourse, perhaps even cross-linguistic) in a uniform way and then manipulate (combine) these structures by means of a few (one, if possible) welldefined operations (unification being the primary one). The feature structures consist of features and associated values, which can be atomic or complex i.e., feature structures themselves. In other'words, the values can be from a structured set. The unification operation builds new structures and together with some string combining operation (concatenation being the primary one) pairs the feature structures with strings (Schieber, 1986)."
T87-1041,Generation - A New Frontier of Natural Language Processing?,1987,0,7,1,1,33217,aravind joshi,Theoretical Issues in Natural Language Processing 3,0,None
P87-1015,Characterizing Structural Descriptions Produced by Various Grammatical Formalisms,1987,13,217,3,1,12137,vijayshanker,25th Annual Meeting of the Association for Computational Linguistics,1,"We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate. In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties of their derivation trees. We find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars. On the basis of this observation, we describe a class of formalisms which we call Linear Context-Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages."
J86-1015,Three Titles from the {C}ambridge Series: {S}TUDIES IN {N}ATURAL {L}ANGUAGE {P}ROCESSING,1986,-1,-1,1,1,33217,aravind joshi,Computational Linguistics,0,None
H86-1005,Research in Natural Language Processing,1986,0,0,1,1,33217,aravind joshi,"Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",0,"The main objective is to develop robust methods for the understanding and generation of both written and spoken human language, including but not limited to English. Penn is pursuing development of: (1) New mathematical and computational frameworks which are highly constrained, yet adequate to allow a simple, concise description of complex linguistic phenomena. These new frameworks are tested by the explicit encoding within each framework of a wide range of phenomena across a diverse set of human languages. (2) Both statistical and symbolic learning methods which automatically extract and effectively utilize the implicit linguistic knowledge in the Penn Treebank and the corpora of the Linguistic Data Consortium. These techniques have been tested against the performance of the best current methods."
H86-1017,Living Up to Expectations: Computing Expert Responses,1986,8,3,1,1,33217,aravind joshi,"Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",0,"In cooperative man-machine interaction, it is necessary but not sufficient for a system to respond truthfully and informatively to a user's question. In particular, if the system has reason to believe that its planned response might mislead the user, then it must block that conclusion by modifying its response. This paper focusses on identifying and avoiding potentially misleading responses by acknowledging types of informing behavior usually expected of an expert. We attempt to give a formal account of several types of assertions that should be included in response to questions concerning the achievement of some goal (in addition to the simple answer), lest the questioner otherwise be misled."
H86-1020,Some Computational Properties of {T}ree {A}djoining {G}rammars,1986,4,66,2,1,55792,vijayshankar,"Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",0,"Tree Adjoining Grammar (TAG) is a formalism for natural language grammars. Some of the basic notions of TAG's were introduced in [Joshi, Levy, and Takahashi 1975] and by [Joshi, 1983]. A detailed investigation of the linguistic relevance of TAG's has been carried out in [Kroch and Joshi, 1985]. In this paper, we will describe some new results for TAG's, especially in the following areas: (1) parsing complexity of TAG's, (2) some closure results for TAG's, and (3) the relationship to Head grammars."
C86-1048,Tree Adjoining and Head Wrapping,1986,4,30,3,1,12137,vijayshanker,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"In this paper we discuss the formal relationship between thw classes of languages generated by Tree Adjoining Grammars and Head Grammars. In particular, we show that Head Languages are included in Tree Adjoining Languages and that Tree Adjoining Grammars are equivalent to a modification of Head Grammars called Modified Head Grammars. The inclusion of MHL in HL, and thus the equivalence of HG's and TAG's in the most general case remains to be established."
P85-1011,Some Computational Properties of {T}ree {A}djoining {G}rammars,1985,3,113,2,1,55792,vijayshankar,23rd Annual Meeting of the Association for Computational Linguistics,1,"Tree Adjoining Grammar (TAG) is a formalism for natural language grammars. Some of the basic notions of TAG's were introduced in [Joshi, Levy, and Takahashi 1975] and by [Joshi, 1983]. A detailed investigation of the linguistic relevance of TAG's has been carried out in [Kroch and Joshi, 1985]. In this paper, we will describe some new results for TAG's, especially in the following areas: (1) parsing complexity of TAG's, (2) some closure results for TAG's, and (3) the relationship to Head grammars."
P84-1029,Preventing False Inferences,1984,11,33,1,1,33217,aravind joshi,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,None
P83-1002,"Factoring Recursion and Dependencies: An Aspect of {T}ree {A}djoining {G}rammars ({TAG}) and a Comparison of Some Formal Properties of {TAG}s, {GPSG}s, {PLG}s, and {LPG}s",1983,2,16,1,1,33217,aravind joshi,21st Annual Meeting of the Association for Computational Linguistics,1,D u r i n g t h e l a s t few y e a r s t h e r e i s v i g o r o u s activity In constructing highly constrained grammatical systems by eliminating the transformational component either totally or partially. There is increasing recognition of t he f a c t t h a t the e n t i r e r a n g e o f d e p e n d e n c i e s t h a t t r a n s f o r m a t i o n a l g r a m m a r s i n t h e i r v a r i o u s i n c a r n a t i o n s have t r i e d t o a c c o u n t f o r c a n be satisfactorily captured by classes of rules that are non-transformational and at the same Clme highly constrlaned in terms of the classes of g r a m m a r s and l a n g u a g e s t h a t t h e y de f i n e .
P83-1007,Providing a Unified Account of Definite Noun Phrases in Discourse,1983,13,309,2,0.428571,30392,barbara grosz,21st Annual Meeting of the Association for Computational Linguistics,1,"Citation Grosz, Barbara J., Aravind K. Joshi, and Scott Weinstein. 1983. Providing a unified account of definite noun phrases in discourse. In 21st Annual Meeting of the Association for Computational Linguistics: proceedings of the conference : 15-17 June 1983, Massachusetts Institute of Technology, Cambridge, Massachusetts, ed. Association for Computational Linguistics, 44-50. Morristown, N.J.: Association for Computational Linguistics."
A83-1001,Domain-Independent Natural Language Interfaces: Session Introduction,1983,0,1,1,1,33217,aravind joshi,First Conference on Applied Natural Language Processing,0,None
P82-1023,Twenty Years of Reflections,1982,0,0,1,1,33217,aravind joshi,20th Annual Meeting of the Association for Computational Linguistics,1,None
J82-1001,Phrase Structure Trees Bear More Fruit than You Would Have Thought,1982,7,29,1,1,33217,aravind joshi,American Journal of Computational Linguistics,0,"In this paper we will present several results concerning phrase structure trees. These results show that phrase structure trees, when viewed in certain ways, have much more descriptive power than one would have thought. We have given a brief account of local constraints on structural descriptions and an intuitive proof of a theorem about local constraints. We have compared the local constraints approach to some aspects of Gazdar's framework and that of Peters and Ritchie and of Karttunen. We have also presented some results on skeletons (phrase structure trees without labels) which show that phrase structure trees, even when deprived of the labels, retain in a certain sense all the structural information. This result has implications for grammatical inference procedures."
C82-1023,Processing of Sentences With Intra-Sentential Code-Switching,1982,1,163,1,1,33217,aravind joshi,{C}oling 1982: Proceedings of the {N}inth {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Speakers of certain bilingual communities systematically produce utterances in which they switch from one language to another, suggesting that the two language systems systematically interact with each other in the production (and recognition) of these sentences. We have investigated this phenomenon in a formal or computational framework which consists of two grammatical systems and a mechanism for switching between the two systems. A variety of constraints apparent in these sentences are then explained in terms of constraints on the switching mechanism, especially, those on closed class items."
C82-1066,Taking the Initiative in Natural Language Data Base Interactions: Justifying Why,1982,10,21,2,0,9578,bonnie webber,{C}oling 1982: Proceedings of the {N}inth {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
P80-1009,Parasession on Topics in Interactive Discourse Influence of the Problem Context,1980,0,0,1,1,33217,aravind joshi,18th Annual Meeting of the Association for Computational Linguistics,1,"My comments are organized within the framework suggested by the Panel Chair, Barbara Grosz, which I find very appropriate. All of my comments pertain to the various issues raised by her; however, wherever possible I will discuss these issues more in the context of the information seeking interaction and the data base domain.The primary question is how the purpose of the interaction or the problem context affects what is said and how it is interpreted. The two separate aspects of this question that must be considered are the function and the domain of the discourse."
P80-1012,Phrase Structure Trees Bear More Fruit Than You Would Have Thought,1980,7,2,1,1,33217,aravind joshi,18th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we will present several results concerning phrase structure trees. These results show that phrase structure trees, when viewed in certain ways, have much more descriptive power than one would have thought. We have given a brief account of local constraints on structural descriptions and an intuitive proof of a theorem about local constraints. We have compared the local constraints approach to some aspects of Gazdar's framework and that of Peters and Ritchie and of Karttunen. We have also presented some results on skeletons (phrase structure trees without labels) which show that phrase structure trees, even when deprived of the labels, retain in a certain sense all the structural information. This result has implications for grammatical inference procedures."
P79-1008,Knowledge Organization and Application: Brief Comments on Papers in the Session,1979,0,0,1,1,33217,aravind joshi,17th Annual Meeting of the Association for Computational Linguistics,1,Brackman describes a l a t t i ce l i ke structured inheritance network (KLONE) as a language for exp l i c i t representation of natural language conceptual information. Multiple descriptions can be represented. How does the f a c i l i t y d i f fe r from a similar one in KRL? Belief representations appear to be only impl ic i t . Quantification is handled through a set of structural descriptions. I t is not clear how negation is handled. The main application is for the command and control of advanced graphics manioulators through natural language. Is there an impl ic i t claim here that the KLONE representations are suitable for both natural language concepts as well as for those in the visual domain?
T78-1026,A Note on Partial Match of Descriptions. Can One Simultaneously Question (Retrieve) and Inform (Update)?,1978,0,4,1,1,33217,aravind joshi,Theoretical Issues in Natural Language Processing-2,0,None
J78-3034,A Note on Partial Match of Descriptions. Can One Simultaneously Question (Retrieve) and Inform (Update)?,1978,0,4,1,1,33217,aravind joshi,American Journal of Computational Linguistics,0,None
J77-1008,Computation of a Subclass of Inferences: Presupposition and Entailment,1977,-1,-1,1,1,33217,aravind joshi,American Journal of Computational Linguistics,0,None
T75-2017,A Formalism for Relating Lexical and Pragmatic Information: Its Relevance to Recognition and Generation,1975,3,4,1,1,33217,aravind joshi,Theoretical Issues in Natural Language Processing,0,None
C69-4701,Properties of Formal Grammars With Mixed Type of Rules and Their Linguistic Relevance,1969,10,13,1,1,33217,aravind joshi,{I}nternational {C}onference on {C}omputational {L}inguistics {COLING} 1969: Preprint No. 47,0,"In this paper, we will study a class of formal grammars with mixed types of rules. The reason for considering such grammars is that no single style (i.e., formal character of rules) of formal grammars is able to represent the various aspects of language structure in a natural way. Various considerations for setting up such grammars have been discussed. Generation schemes which map strings in the language of one mixed grammar into strings in the language of another mixed grammar (both strings being 'well-formed') have been studied. Linguistic relevance of these concepts has also been discussed."
C67-1007,Transformational Decomposition: A Simple Description of an Algorithm for Transformational Analysis of {E}nglish Sentences,1967,0,3,2,0,59200,danuta hiz,{COLING} 1967 Volume 1: Conference Internationale Sur Le Traitement Automatique Des Langues,0,"In this paper, we will present a rather simplified description of an algorithm for transformational analysis (decomposition) of English sentences. Our purpose here is not to discuss the transformational theory, the full details of the theoretical formulations of the algorithm, or of the grammar. Rather, we will present a set of examples of the decomposition and some discussion of them with the hope that it will give enough insight into the capability of the algorithm and indicate to some extent the power of transformational analysis."
