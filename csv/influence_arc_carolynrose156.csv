1995.tmi-1.13,P95-1016,0,0.062519,"rman, and English) spoken language translation system. The techniques involve statistical models as well as knowledge-based models including discourse plan inference. This work is carried out in the context of the Janus project at Carnegie Mellon University and the University of Karlsruhe ([1]). There has been much recent work on using context to constrain spoken language processing. Most of this work involves making predictions about possible sequences of utterances and using these predictions to limit the search space of the speech recognizer or some other component (See [2], [3], [4], [5], [6], [7], [8], [9]). The goal of such an approach is to increase the accuracy of the top best hypothesis of the speech recognizer, which is then passed on to the language processing components of the system. The underlying assumption being made is that design and complexity considerations require that each component of the system pass on a single hypothesis to the following stage, and that this can achieve sufficiently accurate translation results. However, this approach forces components to make disambiguation choices based solely on the level of knowledge available at that stage of processing."
1995.tmi-1.13,E95-1026,0,0.0140163,"English) spoken language translation system. The techniques involve statistical models as well as knowledge-based models including discourse plan inference. This work is carried out in the context of the Janus project at Carnegie Mellon University and the University of Karlsruhe ([1]). There has been much recent work on using context to constrain spoken language processing. Most of this work involves making predictions about possible sequences of utterances and using these predictions to limit the search space of the speech recognizer or some other component (See [2], [3], [4], [5], [6], [7], [8], [9]). The goal of such an approach is to increase the accuracy of the top best hypothesis of the speech recognizer, which is then passed on to the language processing components of the system. The underlying assumption being made is that design and complexity considerations require that each component of the system pass on a single hypothesis to the following stage, and that this can achieve sufficiently accurate translation results. However, this approach forces components to make disambiguation choices based solely on the level of knowledge available at that stage of processing. Thus, comp"
1995.tmi-1.13,1995.tmi-1.15,0,0.606961,"agram is shown in Figure 2.1 Processing starts with speech input in the source language. Recognition of the speech signal is done with acoustic modeling methods, constrained by a language model. The output of speech recognition is a word lattice. We prefer working with word lattices rather than the more common approach of processing N-best lists of hypotheses. An N-best list may be largely redundant and can be efficiently represented in the form of a lattice. Using a lattice parser can thus reduce time and space 1 Another approach being pursued in parallel in the Janus project is described in [10] 174 175 complexity relative to parsing a corresponding N-best list. Selection of the correct path through the lattice is accomplished during parsing when more information is available. Lattices, however, are potentially inefficient because of their size. We apply four steps to make them more tractable ([11]). The first step involves cleaning the lattice by mapping all non-human noises and pauses into a generic pause. Consecutive pauses are then adjoined to one long pause. The resulting lattice contains only linguistically meaningful information. The lattice is then broken at points where no h"
1995.tmi-1.13,1993.iwpt-1.12,1,0.826786,"et of sub-lattices which are highly correspondent to sentence breaks in the utterance. Each of the sub-lattices is then re-scored using a new language model. Finally the lattices are pruned to a size that the parser can process in reasonable time and space. The re-scoring raises the probability that the correct hypothesis will not be lost during the pruning stage. Each of the resulting sub-lattices are passed on to the parser, the first component of the translation process. Parsing a word lattice involves finding all paths of connecting words within the lattice that are grammatical. The GLR* ([12], [13]) parser skips parts of the utterance that it cannot incorporate into a well-formed structure. Thus it is well-suited to domains in which extra-grammaticality is common. The parser can identify additional sentence breaks within each sub-lattice with the help of a statistical method that determines the probability of sentence breaks at each point in the utterance. The output of parsing a sub-lattice is a set of interlingua texts, or ILTs, representing all of the grammatical paths through the sub-lattice and all of the ambiguities in each grammatical path. The ILTs from each sub-lattice ar"
1995.tmi-1.13,P94-1045,1,0.890553,"sub-lattices which are highly correspondent to sentence breaks in the utterance. Each of the sub-lattices is then re-scored using a new language model. Finally the lattices are pruned to a size that the parser can process in reasonable time and space. The re-scoring raises the probability that the correct hypothesis will not be lost during the pruning stage. Each of the resulting sub-lattices are passed on to the parser, the first component of the translation process. Parsing a word lattice involves finding all paths of connecting words within the lattice that are grammatical. The GLR* ([12], [13]) parser skips parts of the utterance that it cannot incorporate into a well-formed structure. Thus it is well-suited to domains in which extra-grammaticality is common. The parser can identify additional sentence breaks within each sub-lattice with the help of a statistical method that determines the probability of sentence breaks at each point in the utterance. The output of parsing a sub-lattice is a set of interlingua texts, or ILTs, representing all of the grammatical paths through the sub-lattice and all of the ambiguities in each grammatical path. The ILTs from each sub-lattice are comb"
1995.tmi-1.13,P92-1025,0,0.0443447,"a statistical method that determines the probability of sentence breaks at each point in the utterance. The output of parsing a sub-lattice is a set of interlingua texts, or ILTs, representing all of the grammatical paths through the sub-lattice and all of the ambiguities in each grammatical path. The ILTs from each sub-lattice are combined, yielding a list of ILT sequences that represent the possible sentences in a full multi-sentence turn. An ILT n-gram is applied to each such list to determine the probability of each sequence of sentences. The discourse processor, based on Lambert’s work ([14, 15]), disambiguates the speech act of each sentence, normalizes temporal expressions, and incorporates the sentence into a discourse plan tree. The discourse processor's focusing heuristics and plan operators eliminate some ambiguity by filtering out hypotheses that do not fit into the current discourse context. The discourse component also updates a calendar in the dynamic discourse memory to keep track of what the speakers have said about their schedules. As processing continues, the N-best hypotheses for sequences of ILTs in a multisentence turn are sent to the generator. The generation output"
2000.iwpt-1.16,P89-1018,0,0.399379,"ses individually. As a result , an exponential number of parse trees can be succinctly represented , and parsing can still be performed in polynomial time. Obviously, in order to achieve optimal parsing efficiency, the parsing algorithm must identify all pos sible local ambiguities and pack them together. Certain context-free parsing algorithms are inherently better at this task than others. Tabular parsing algorithms such as CKY [21] by design synchronize processing in a way that supports easy identification of local ambiguities. On the other hand, as has been pointed out by Billot and Lang [1] , the Generalized LR Parsing algorithm (GLR) [18] is not capable of performing full ambiguity packing, due to the fact that stacks that end in different states must be kept distinct. There has been some debate in the parsing community regarding the relative efficiency of GLR and chart parsers, with conflicting evidence in both directions [16] , [20} , [2] , [14] . The relative effectiveness of performing ambiguity packing has not received full attention in this debate, and may in fact account for some of the conflicting evidence 1 . 2.2 The Problem: Ambiguity Packing in CFG Parsing with Inter"
2000.iwpt-1.16,J93-1002,0,0.0624449,"Missing"
2000.iwpt-1.16,P88-1035,0,0.0586349,"ith a significant effect on the performance of the parser. It should be noted that effective local ambiguity packing when parsing with unification-augmented I 1 CFGs requires not only a compact representation for the packed c-structure node, but also an ef fective representation of the associated f-structures. The issue of how to effectively pack ambiguous 1 For example, van Noord [20] compares left corner and chart parsers that do apply ambiguity packing with a GLR parser without ambiguity packing. 149 f-structures has received quite a bit of attention in the literature over the last decade [4] [11] [12] [13] [6] . While the two issues are related , we focus here on how to achieve optimal packing of c-structures, so that unification operations do not have to be re-executed due to the later detection of an additional local ambiguity. W hile there is much to be gained from improved packing on the f-structure level, one should note that effective ambiguity packing on the c-structure level is a necessary pre-condition for efficient parsing, regardless of how well f-structures are packed. 3 The Rule Priorit ization Heuristic In order to ensure effective ambiguity packing in unification-a"
2000.iwpt-1.16,P99-1061,0,0.163013,"Missing"
2000.iwpt-1.16,P94-1045,1,0.840585,"ture bundles that are associated with the non-terminals of the rules. Feature structure computation is, for the most part , specified and implemented via unification operations. This allows the grammar to constrain the applicability of context-free rules. A reduction by a context-free rule succeeds only if the associated feature structure unification is successful as well. The Generalized LR Parser/Compiler is implemented in Common Lisp, and has been used as the analysis component of several different projects at the Center for Machine Translation at CMU in the course of the last decade. GLR* [7] , [9] , [8] , the robust version of the parser, was constructed as an extended version of the unification-based Generalized LR Parser/Compiler. The parser skips parts of the utterance that it cannot incorporate into a well-formed sentence structure. Thus it is well-suited to domains in which non-grammaticality is common. The parser conducts a search for the maximal subset of the original input that is covered by the grammar. This is done using a beam search heuristic that limits the combinations of skipped words considered by the parser, and ensures that it operates within feasible time and s"
2000.iwpt-1.16,C96-1075,1,0.830539,"rtion of the input sentence may be reduced to a non-terminal symbol in many different ways, when considering different subsets of the input that may be skipped. Thus, efficient runtime performance of the robust versions of the parsers is even more dependent on effective local ambiguity packing. We therefore conducted evaluations with the two parsers in both robust and non-robust modes, in order to quantify the effect of the rule selection heuristic under both scenarios. All of the described experiments were conducted on a common test set of 520 sentences from the JANUS English Scheduling Task [10], using a general English syntactic grammar developed at Carnegie Mellon University. The grammar has 412 rules and 71 non-terminals, and produces a full predicate-argument structure analysis in the form of a feature structure. For the GLR parser, the 153 35 .-----.-----..------�-----. Sentenoo Length Dlatributlon -+30 25 20 15 10 o _____._____...______._______. 10 Sentence l.englh 15 20 Figure 2: Distribution of evaluation set sentence lengths grammar compiles into an SLR parsing table with 628 states and 8822 parsing actions. Figure 2 shows the distribution of sentence lengths in the evaluati"
2000.iwpt-1.16,J93-4001,0,0.0821809,"Missing"
2000.iwpt-1.16,P99-1075,0,0.0804833,"ant effect on the performance of the parser. It should be noted that effective local ambiguity packing when parsing with unification-augmented I 1 CFGs requires not only a compact representation for the packed c-structure node, but also an ef fective representation of the associated f-structures. The issue of how to effectively pack ambiguous 1 For example, van Noord [20] compares left corner and chart parsers that do apply ambiguity packing with a GLR parser without ambiguity packing. 149 f-structures has received quite a bit of attention in the literature over the last decade [4] [11] [12] [13] [6] . While the two issues are related , we focus here on how to achieve optimal packing of c-structures, so that unification operations do not have to be re-executed due to the later detection of an additional local ambiguity. W hile there is much to be gained from improved packing on the f-structure level, one should note that effective ambiguity packing on the c-structure level is a necessary pre-condition for efficient parsing, regardless of how well f-structures are packed. 3 The Rule Priorit ization Heuristic In order to ensure effective ambiguity packing in unification-augmented contex"
2000.iwpt-1.16,J87-1004,0,0.171613,"mber of parse trees can be succinctly represented , and parsing can still be performed in polynomial time. Obviously, in order to achieve optimal parsing efficiency, the parsing algorithm must identify all pos sible local ambiguities and pack them together. Certain context-free parsing algorithms are inherently better at this task than others. Tabular parsing algorithms such as CKY [21] by design synchronize processing in a way that supports easy identification of local ambiguities. On the other hand, as has been pointed out by Billot and Lang [1] , the Generalized LR Parsing algorithm (GLR) [18] is not capable of performing full ambiguity packing, due to the fact that stacks that end in different states must be kept distinct. There has been some debate in the parsing community regarding the relative efficiency of GLR and chart parsers, with conflicting evidence in both directions [16] , [20} , [2] , [14] . The relative effectiveness of performing ambiguity packing has not received full attention in this debate, and may in fact account for some of the conflicting evidence 1 . 2.2 The Problem: Ambiguity Packing in CFG Parsing with Interleaved Unification respect to the order in which v"
2000.iwpt-1.16,J97-3004,0,0.0375142,"cessed . Instead, the parser creates a new parse node for the newly discovered local ambiguity and processes the new node separately. As a result, the parser&apos;s overall local ambiguity packing is less than optimal, with a significant effect on the performance of the parser. It should be noted that effective local ambiguity packing when parsing with unification-augmented I 1 CFGs requires not only a compact representation for the packed c-structure node, but also an ef fective representation of the associated f-structures. The issue of how to effectively pack ambiguous 1 For example, van Noord [20] compares left corner and chart parsers that do apply ambiguity packing with a GLR parser without ambiguity packing. 149 f-structures has received quite a bit of attention in the literature over the last decade [4] [11] [12] [13] [6] . While the two issues are related , we focus here on how to achieve optimal packing of c-structures, so that unification operations do not have to be re-executed due to the later detection of an additional local ambiguity. W hile there is much to be gained from improved packing on the f-structure level, one should note that effective ambiguity packing on the c-st"
2020.acl-main.681,C18-1075,0,0.0182605,"a from a different domain or containing different event types (Keith et al., 2017). Conversely, event extractors that incorporate syntactic rule-based modules (Saur´ı et al., 2005; Chambers et al., 2014) tend to overgenerate, labeling most verbs and nouns as events. Achieving a balance between these extremes will help in building generalizable event extractors, a crucial problem since annotated training data may be expensive to obtain for every new domain. Prior work has explored unsupervised (Huang et al., 2016; Yuan et al., 2018), distantly supervised (Keith et al., 2017; Chen et al., 2017; Araki and Mitamura, 2018; Zeng et al., 2018) and semisupervised approaches (Liao and Grishman, 2010; Huang and Riloff, 2012; Ferguson et al., 2018), which largely focus on automatically generating in-domain training data. In our work, we try to leverage annotated training data from other domains. Motivated by the hypothesis that events, despite being domain/ task-specific, often occur in similar contextual patterns, we try to inject lexical domain-invariance into supervised models, improving generalization, while not overpredicting events. Concretely, we focus on event trigger identification, which aims to identify t"
2020.acl-main.681,Q14-1022,0,0.0244542,"9/ODETTE ercise habits) which may have bearing on the patient’s illness? To circumvent this, prior work has mainly focused on annotating specific categories of events (Grishman and Sundheim, 1996; Doddington et al., 2004; Kim et al., 2008) or narratives from specific domains (Pustejovsky et al., 2003; Sims et al., 2019). This has an important implication for supervised event extractors: they do not generalize to data from a different domain or containing different event types (Keith et al., 2017). Conversely, event extractors that incorporate syntactic rule-based modules (Saur´ı et al., 2005; Chambers et al., 2014) tend to overgenerate, labeling most verbs and nouns as events. Achieving a balance between these extremes will help in building generalizable event extractors, a crucial problem since annotated training data may be expensive to obtain for every new domain. Prior work has explored unsupervised (Huang et al., 2016; Yuan et al., 2018), distantly supervised (Keith et al., 2017; Chen et al., 2017; Araki and Mitamura, 2018; Zeng et al., 2018) and semisupervised approaches (Liao and Grishman, 2010; Huang and Riloff, 2012; Ferguson et al., 2018), which largely focus on automatically generating in-dom"
2020.acl-main.681,Q18-1039,0,0.040644,"leads to substantial improvement, reaching 51.5 and 67.2 F1 on literature and news respectively. 2 Approaching Open Domain Event Trigger Identification Throughout this work, we treat the task of event trigger identification as a token-level classification task. For each token in a sequence, we predict whether it is an event trigger. To ensure that our trigger identification model can transfer across domains, we leverage the adversarial domain adaptation (ADA) framework (Ganin and Lempitsky, 2015), which has been used in several NLP tasks (Ganin et al., 2016; Li et al., 2017; Liu et al., 2017; Chen et al., 2018; Shah et al., 2018; Yu et al., 2018). 2.1 Labeled Source Domain Data Figure 1 gives an overview of the ADA framework for event trigger identification. It consists of three components: i) representation learner (R) ii) event classifier (E) and iii) domain predictor (D). The representation learner generates tokenlevel representations, while the event classifier and domain predictor use these representations to identify event triggers and predict the domain to which the sequence belongs. The key idea is to train the representation learner to generate representations which are predictive for trig"
2020.acl-main.681,P17-1038,0,0.0231969,"t generalize to data from a different domain or containing different event types (Keith et al., 2017). Conversely, event extractors that incorporate syntactic rule-based modules (Saur´ı et al., 2005; Chambers et al., 2014) tend to overgenerate, labeling most verbs and nouns as events. Achieving a balance between these extremes will help in building generalizable event extractors, a crucial problem since annotated training data may be expensive to obtain for every new domain. Prior work has explored unsupervised (Huang et al., 2016; Yuan et al., 2018), distantly supervised (Keith et al., 2017; Chen et al., 2017; Araki and Mitamura, 2018; Zeng et al., 2018) and semisupervised approaches (Liao and Grishman, 2010; Huang and Riloff, 2012; Ferguson et al., 2018), which largely focus on automatically generating in-domain training data. In our work, we try to leverage annotated training data from other domains. Motivated by the hypothesis that events, despite being domain/ task-specific, often occur in similar contextual patterns, we try to inject lexical domain-invariance into supervised models, improving generalization, while not overpredicting events. Concretely, we focus on event trigger identification"
2020.acl-main.681,W03-0502,0,0.236848,"Missing"
2020.acl-main.681,P07-1033,0,0.234002,"Missing"
2020.acl-main.681,N19-1423,0,0.0241455,".5 25.6 75.2 72.9 35.8 37.9 BiLSTM BiLSTM-A 75.4 74.2 76.3 79.4 75.9 76.7 27.6 26.3 68.8 72.0 39.4 38.6 POS POS-A 77.4 76.4 81.1 83.0 79.2 79.6 26.4 27.3 79.8 81.9 39.6 40.9 BERT BERT-A 79.6 79.8 84.3 85.6 81.9 82.6 28.1 30.3 84.8 80.8 42.2 44.1 3.2 BiLSTM: A bidirectional LSTM over word embeddings to incorporate both left and right context. POS: A BiLSTM over token representations constructed by concatenating word embeddings with embeddings corresponding to part-of-speech tags. This model explicitly introduces syntax. BERT: A BiLSTM over contextual token representations extracted using BERT (Devlin et al., 2019), similar to the best-performing model on LitBank, reported by Sims et al. (2019). 3.1 Out-of-Domain Table 3: Model performance on domain transfer experiments from TimeBank to LitBank. Presence of the -A suffix indicates that the model uses adversarial training. Table 2: Model performance on domain transfer experiments from LitBank to TimeBank. Presence of the -A suffix indicates that the model uses adversarial training. 3 In-Domain Unlike prior work, we cannot use the ACE-2005 dataset since it tags specific categories of events, whereas we focus on tagging all possible events. Results and Ana"
2020.acl-main.681,doddington-etal-2004-automatic,0,0.117671,"ent extraction remains an onerous task. A major reason for this is that the notion of what counts as an “event” depends heavily on the domain and task at hand. For example, should a system which extracts events from doctor notes only focus on medical events (eg: symptoms, treatments), or also annotate lifestyle events (eg: dietary changes, ex1 Our system is available at https://github.com/ aakanksha19/ODETTE ercise habits) which may have bearing on the patient’s illness? To circumvent this, prior work has mainly focused on annotating specific categories of events (Grishman and Sundheim, 1996; Doddington et al., 2004; Kim et al., 2008) or narratives from specific domains (Pustejovsky et al., 2003; Sims et al., 2019). This has an important implication for supervised event extractors: they do not generalize to data from a different domain or containing different event types (Keith et al., 2017). Conversely, event extractors that incorporate syntactic rule-based modules (Saur´ı et al., 2005; Chambers et al., 2014) tend to overgenerate, labeling most verbs and nouns as events. Achieving a balance between these extremes will help in building generalizable event extractors, a crucial problem since annotated tra"
2020.acl-main.681,N18-2058,0,0.0176126,"rate syntactic rule-based modules (Saur´ı et al., 2005; Chambers et al., 2014) tend to overgenerate, labeling most verbs and nouns as events. Achieving a balance between these extremes will help in building generalizable event extractors, a crucial problem since annotated training data may be expensive to obtain for every new domain. Prior work has explored unsupervised (Huang et al., 2016; Yuan et al., 2018), distantly supervised (Keith et al., 2017; Chen et al., 2017; Araki and Mitamura, 2018; Zeng et al., 2018) and semisupervised approaches (Liao and Grishman, 2010; Huang and Riloff, 2012; Ferguson et al., 2018), which largely focus on automatically generating in-domain training data. In our work, we try to leverage annotated training data from other domains. Motivated by the hypothesis that events, despite being domain/ task-specific, often occur in similar contextual patterns, we try to inject lexical domain-invariance into supervised models, improving generalization, while not overpredicting events. Concretely, we focus on event trigger identification, which aims to identify triggers (words) that instantiate an event. For example, in “John was born in Sussex”, born is a trigger, invoking a BIRTH e"
2020.acl-main.681,C96-1079,0,0.397671,"3). Despite their utility, event extraction remains an onerous task. A major reason for this is that the notion of what counts as an “event” depends heavily on the domain and task at hand. For example, should a system which extracts events from doctor notes only focus on medical events (eg: symptoms, treatments), or also annotate lifestyle events (eg: dietary changes, ex1 Our system is available at https://github.com/ aakanksha19/ODETTE ercise habits) which may have bearing on the patient’s illness? To circumvent this, prior work has mainly focused on annotating specific categories of events (Grishman and Sundheim, 1996; Doddington et al., 2004; Kim et al., 2008) or narratives from specific domains (Pustejovsky et al., 2003; Sims et al., 2019). This has an important implication for supervised event extractors: they do not generalize to data from a different domain or containing different event types (Keith et al., 2017). Conversely, event extractors that incorporate syntactic rule-based modules (Saur´ı et al., 2005; Chambers et al., 2014) tend to overgenerate, labeling most verbs and nouns as events. Achieving a balance between these extremes will help in building generalizable event extractors, a crucial pr"
2020.acl-main.681,P16-1025,0,0.0291554,"19). This has an important implication for supervised event extractors: they do not generalize to data from a different domain or containing different event types (Keith et al., 2017). Conversely, event extractors that incorporate syntactic rule-based modules (Saur´ı et al., 2005; Chambers et al., 2014) tend to overgenerate, labeling most verbs and nouns as events. Achieving a balance between these extremes will help in building generalizable event extractors, a crucial problem since annotated training data may be expensive to obtain for every new domain. Prior work has explored unsupervised (Huang et al., 2016; Yuan et al., 2018), distantly supervised (Keith et al., 2017; Chen et al., 2017; Araki and Mitamura, 2018; Zeng et al., 2018) and semisupervised approaches (Liao and Grishman, 2010; Huang and Riloff, 2012; Ferguson et al., 2018), which largely focus on automatically generating in-domain training data. In our work, we try to leverage annotated training data from other domains. Motivated by the hypothesis that events, despite being domain/ task-specific, often occur in similar contextual patterns, we try to inject lexical domain-invariance into supervised models, improving generalization, whil"
2020.acl-main.681,E12-1029,0,0.0289495,"extractors that incorporate syntactic rule-based modules (Saur´ı et al., 2005; Chambers et al., 2014) tend to overgenerate, labeling most verbs and nouns as events. Achieving a balance between these extremes will help in building generalizable event extractors, a crucial problem since annotated training data may be expensive to obtain for every new domain. Prior work has explored unsupervised (Huang et al., 2016; Yuan et al., 2018), distantly supervised (Keith et al., 2017; Chen et al., 2017; Araki and Mitamura, 2018; Zeng et al., 2018) and semisupervised approaches (Liao and Grishman, 2010; Huang and Riloff, 2012; Ferguson et al., 2018), which largely focus on automatically generating in-domain training data. In our work, we try to leverage annotated training data from other domains. Motivated by the hypothesis that events, despite being domain/ task-specific, often occur in similar contextual patterns, we try to inject lexical domain-invariance into supervised models, improving generalization, while not overpredicting events. Concretely, we focus on event trigger identification, which aims to identify triggers (words) that instantiate an event. For example, in “John was born in Sussex”, born is a tri"
2020.acl-main.681,D17-1163,0,0.112542,"Missing"
2020.acl-main.681,C16-1038,0,0.0759557,"Missing"
2020.acl-main.681,C10-1077,0,0.0224584,"2017). Conversely, event extractors that incorporate syntactic rule-based modules (Saur´ı et al., 2005; Chambers et al., 2014) tend to overgenerate, labeling most verbs and nouns as events. Achieving a balance between these extremes will help in building generalizable event extractors, a crucial problem since annotated training data may be expensive to obtain for every new domain. Prior work has explored unsupervised (Huang et al., 2016; Yuan et al., 2018), distantly supervised (Keith et al., 2017; Chen et al., 2017; Araki and Mitamura, 2018; Zeng et al., 2018) and semisupervised approaches (Liao and Grishman, 2010; Huang and Riloff, 2012; Ferguson et al., 2018), which largely focus on automatically generating in-domain training data. In our work, we try to leverage annotated training data from other domains. Motivated by the hypothesis that events, despite being domain/ task-specific, often occur in similar contextual patterns, we try to inject lexical domain-invariance into supervised models, improving generalization, while not overpredicting events. Concretely, we focus on event trigger identification, which aims to identify triggers (words) that instantiate an event. For example, in “John was born i"
2020.acl-main.681,P17-1001,0,0.0442522,"by self-training leads to substantial improvement, reaching 51.5 and 67.2 F1 on literature and news respectively. 2 Approaching Open Domain Event Trigger Identification Throughout this work, we treat the task of event trigger identification as a token-level classification task. For each token in a sequence, we predict whether it is an event trigger. To ensure that our trigger identification model can transfer across domains, we leverage the adversarial domain adaptation (ADA) framework (Ganin and Lempitsky, 2015), which has been used in several NLP tasks (Ganin et al., 2016; Li et al., 2017; Liu et al., 2017; Chen et al., 2018; Shah et al., 2018; Yu et al., 2018). 2.1 Labeled Source Domain Data Figure 1 gives an overview of the ADA framework for event trigger identification. It consists of three components: i) representation learner (R) ii) event classifier (E) and iii) domain predictor (D). The representation learner generates tokenlevel representations, while the event classifier and domain predictor use these representations to identify event triggers and predict the domain to which the sequence belongs. The key idea is to train the representation learner to generate representations which are"
2020.acl-main.681,W03-1014,0,0.0602804,"Missing"
2020.acl-main.681,H05-1088,0,0.250788,"Missing"
2020.acl-main.681,D18-1131,0,0.0570096,"l improvement, reaching 51.5 and 67.2 F1 on literature and news respectively. 2 Approaching Open Domain Event Trigger Identification Throughout this work, we treat the task of event trigger identification as a token-level classification task. For each token in a sequence, we predict whether it is an event trigger. To ensure that our trigger identification model can transfer across domains, we leverage the adversarial domain adaptation (ADA) framework (Ganin and Lempitsky, 2015), which has been used in several NLP tasks (Ganin et al., 2016; Li et al., 2017; Liu et al., 2017; Chen et al., 2018; Shah et al., 2018; Yu et al., 2018). 2.1 Labeled Source Domain Data Figure 1 gives an overview of the ADA framework for event trigger identification. It consists of three components: i) representation learner (R) ii) event classifier (E) and iii) domain predictor (D). The representation learner generates tokenlevel representations, while the event classifier and domain predictor use these representations to identify event triggers and predict the domain to which the sequence belongs. The key idea is to train the representation learner to generate representations which are predictive for trigger identification"
2020.acl-main.681,P19-1353,0,0.243665,"vent” depends heavily on the domain and task at hand. For example, should a system which extracts events from doctor notes only focus on medical events (eg: symptoms, treatments), or also annotate lifestyle events (eg: dietary changes, ex1 Our system is available at https://github.com/ aakanksha19/ODETTE ercise habits) which may have bearing on the patient’s illness? To circumvent this, prior work has mainly focused on annotating specific categories of events (Grishman and Sundheim, 1996; Doddington et al., 2004; Kim et al., 2008) or narratives from specific domains (Pustejovsky et al., 2003; Sims et al., 2019). This has an important implication for supervised event extractors: they do not generalize to data from a different domain or containing different event types (Keith et al., 2017). Conversely, event extractors that incorporate syntactic rule-based modules (Saur´ı et al., 2005; Chambers et al., 2014) tend to overgenerate, labeling most verbs and nouns as events. Achieving a balance between these extremes will help in building generalizable event extractors, a crucial problem since annotated training data may be expensive to obtain for every new domain. Prior work has explored unsupervised (Hua"
2020.acl-main.681,P95-1026,0,0.809424,"Missing"
2020.codi-1.3,2020.lrec-1.6,0,0.0578382,"Missing"
2020.codi-1.3,D08-1031,0,0.0634849,"rrect predictions in order to increase the probability of a correct prediction. In this work, we evaluate the benefits of using explicit type information for CR. We show that a model that leverages entity types associated with the anaphoric/ antecedent mentions significantly reduces the problem of type inconsistency in the output coreference clusters and thus improves the overall performance of the neural baseline on four datasets. Type Information for CR: Multiple prior works have shown type-information to be a useful feature for shallow coreference resolution classifiers (Soon et al., 2001; Bengtson and Roth, 2008; Ponzetto and Strube, 2006; Haghighi and Klein, 2010; Durrett and Klein, 2014). (Soon et al., 2001) take the most frequent sense for each noun in WordNet as the semantic class for that noun and use a decision-tree for pairwise classification of whether two samples co-refer each other. (Bengtson and Roth, 2008) use a hypernym tree to extract the type information for different common nouns, and compare the proper names against a predefined list to determine if the mention is a person. They, then, pass this and many other features (like distance, agreement, etc.) through a regularized average pe"
2020.codi-1.3,D16-1245,0,0.0205132,"This paper offers the first such model and evaluation, demonstrating modest gains in accuracy by introducing either gold standard or predicted types. In the proposed approach, type information serves both to (1) improve mention representation and (2) create a soft type consistency check between coreference candidate mentions. Our evaluation covers two different grain sizes of types over four different benchmark corpora. 1 Introduction Coreference resolution (CR) is an extensively studied problem in computational linguistics and NLP (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 1999; Ng, 2017; Clark and Manning, 2016; Lee et al., 2017). Solutions to this problem allow us to make meaningful links between concepts and entities within a discourse and therefore serves as a valuable pre-processing step for downstream tasks like summarization and questionanswering (Steinberger et al., 2007; Dasigi et al., 2019; Sukthanker et al., 2020a). Recently, multiple datasets including Ontonotes (Pradhan et al., 2012), Litbank (Bamman et al., 2020), EmailCoref (Dakle et al., 2020), and WikiCoref (Ghaddar and Langlais, 2016) have been proposed as benchmark datasets for CR, especially in the sub-area of entity anaphora (Suk"
2020.codi-1.3,2020.lrec-1.8,0,0.0146002,"n (CR) is an extensively studied problem in computational linguistics and NLP (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 1999; Ng, 2017; Clark and Manning, 2016; Lee et al., 2017). Solutions to this problem allow us to make meaningful links between concepts and entities within a discourse and therefore serves as a valuable pre-processing step for downstream tasks like summarization and questionanswering (Steinberger et al., 2007; Dasigi et al., 2019; Sukthanker et al., 2020a). Recently, multiple datasets including Ontonotes (Pradhan et al., 2012), Litbank (Bamman et al., 2020), EmailCoref (Dakle et al., 2020), and WikiCoref (Ghaddar and Langlais, 2016) have been proposed as benchmark datasets for CR, especially in the sub-area of entity anaphora (Sukthanker et al., 2020b). Entity anaphora is a simpler starting place for work on anaphora because unlike abstract anaphora (Webber, 1991), entity anaphora are pronouns or noun phrases that refer to an explicitly mentioned entity in the 20 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 20–31 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Figure 1: We improve B"
2020.codi-1.3,D19-1606,0,0.0542077,"Missing"
2020.codi-1.3,Q14-1037,0,0.355665,". In this work, we consider CR datasets that contain generic entitytypes. One challenge is that the different corpora do not utilize the same set of type tags. For example, OntoNotes includes 18 types while EmailCoref includes only 4. Thus, we evaluate the performance of the proposed modeling approach on each dataset both with the set of type tags germaine to the dataset as well as a common set of four basic types (person, org, location, facility) inspired from research on Named Entity Recognition (NER) (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Our motivation is similar to (Durrett and Klein, 2014), which used a structured CRF with handcurated features to jointly-model the tasks of CR, entity typing, and entity linking. Their joint architecture showed an improved performance on CR over the independent baseline. However, our work differs from there’s as we show the benefits of entity-type information in neural models that use contextualized representations like BERT (Peters et al., 2018). Some prior art (Petroni et al., 2019; Roberts et al., 2020) argues that contextualCoreference resolution (CR) is an essential part of discourse analysis. Most recently, neural approaches have been propo"
2020.codi-1.3,L16-1021,0,0.0267592,"blem in computational linguistics and NLP (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 1999; Ng, 2017; Clark and Manning, 2016; Lee et al., 2017). Solutions to this problem allow us to make meaningful links between concepts and entities within a discourse and therefore serves as a valuable pre-processing step for downstream tasks like summarization and questionanswering (Steinberger et al., 2007; Dasigi et al., 2019; Sukthanker et al., 2020a). Recently, multiple datasets including Ontonotes (Pradhan et al., 2012), Litbank (Bamman et al., 2020), EmailCoref (Dakle et al., 2020), and WikiCoref (Ghaddar and Langlais, 2016) have been proposed as benchmark datasets for CR, especially in the sub-area of entity anaphora (Sukthanker et al., 2020b). Entity anaphora is a simpler starting place for work on anaphora because unlike abstract anaphora (Webber, 1991), entity anaphora are pronouns or noun phrases that refer to an explicitly mentioned entity in the 20 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 20–31 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Figure 1: We improve Bamman et al. (2020) for entity coreference r"
2020.codi-1.3,N18-1202,0,0.246579,"four basic types (person, org, location, facility) inspired from research on Named Entity Recognition (NER) (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Our motivation is similar to (Durrett and Klein, 2014), which used a structured CRF with handcurated features to jointly-model the tasks of CR, entity typing, and entity linking. Their joint architecture showed an improved performance on CR over the independent baseline. However, our work differs from there’s as we show the benefits of entity-type information in neural models that use contextualized representations like BERT (Peters et al., 2018). Some prior art (Petroni et al., 2019; Roberts et al., 2020) argues that contextualCoreference resolution (CR) is an essential part of discourse analysis. Most recently, neural approaches have been proposed to improve over SOTA models from earlier paradigms. So far none of the published neural models leverage external semantic knowledge such as type information. This paper offers the first such model and evaluation, demonstrating modest gains in accuracy by introducing either gold standard or predicted types. In the proposed approach, type information serves both to (1) improve mention repres"
2020.codi-1.3,N10-1061,0,0.0427049,"ty of a correct prediction. In this work, we evaluate the benefits of using explicit type information for CR. We show that a model that leverages entity types associated with the anaphoric/ antecedent mentions significantly reduces the problem of type inconsistency in the output coreference clusters and thus improves the overall performance of the neural baseline on four datasets. Type Information for CR: Multiple prior works have shown type-information to be a useful feature for shallow coreference resolution classifiers (Soon et al., 2001; Bengtson and Roth, 2008; Ponzetto and Strube, 2006; Haghighi and Klein, 2010; Durrett and Klein, 2014). (Soon et al., 2001) take the most frequent sense for each noun in WordNet as the semantic class for that noun and use a decision-tree for pairwise classification of whether two samples co-refer each other. (Bengtson and Roth, 2008) use a hypernym tree to extract the type information for different common nouns, and compare the proper names against a predefined list to determine if the mention is a person. They, then, pass this and many other features (like distance, agreement, etc.) through a regularized average perceptron for pairwise classification. This paper expa"
2020.codi-1.3,2020.tacl-1.5,0,0.0116767,"entity-types benefits neural models that use BERT for CR. We show a consistent improvement in performance on four different coreference datasets from varied domains. Our contribution is that we evaluate the impact of the introduction of type information in neural entity coreference at two different levels of granularity (which we refer to as original vs common), demonstrating their utility both in the case where gold standard type information is available, and the more typical case where it is predicted. 2 Related Work Neural Coreference Resolution: Recently, neural approaches to coreference (Joshi et al., 2020, 2019; Lee et al., 2018, 2017) have begun to show their prowess. The SOTA models show impressive performance on state-of-the-art datasets like OntoNotes (Pradhan et al., 2012) and GAP (Webster et al., 2018). The notable architecture proposed by Lee et al. (2017) scores pairs of entity mentions independently and later uses a clustering algorithm to find coreference clusters. On the other hand, Lee et al. (2018) improve upon this foundation by introducing an approximated higher-order inference that iteratively updates the 21 goal is to evaluate the benefits of type information, we too separate"
2020.codi-1.3,D19-1250,0,0.0304505,"Missing"
2020.codi-1.3,D19-1588,0,0.0463923,"o Improve Entity Coreference Resolution Sopan Khosla Carolyn Rose Language Technologies Institute Carnegie Mellon University, USA {sopank, cprose}@cs.cmu.edu Abstract discourse rather than an abstract idea that must be constructed from a repackaging of information revealed over an extended text. An affordance of entity anaphora is that they have easily articulated semantic types. Most of the entity CR datasets are extensively annotated for syntactic features (like constituency parse etc.) and semantic features (like entity-types). However, none of the published SOTA methods (Lee et al., 2017; Joshi et al., 2019, 2020) explicitly leverage the type information. In this paper, we present a proof of concept to portray the benefits of using type information in neural approaches for CR. Named entities are generally divided generically (e.g. person, organization etc.) or in a domain-specific manner (e.g. symptom, drug, test etc.). In this work, we consider CR datasets that contain generic entitytypes. One challenge is that the different corpora do not utilize the same set of type tags. For example, OntoNotes includes 18 types while EmailCoref includes only 4. Thus, we evaluate the performance of the propos"
2020.codi-1.3,J94-4002,0,0.0912061,"al semantic knowledge such as type information. This paper offers the first such model and evaluation, demonstrating modest gains in accuracy by introducing either gold standard or predicted types. In the proposed approach, type information serves both to (1) improve mention representation and (2) create a soft type consistency check between coreference candidate mentions. Our evaluation covers two different grain sizes of types over four different benchmark corpora. 1 Introduction Coreference resolution (CR) is an extensively studied problem in computational linguistics and NLP (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 1999; Ng, 2017; Clark and Manning, 2016; Lee et al., 2017). Solutions to this problem allow us to make meaningful links between concepts and entities within a discourse and therefore serves as a valuable pre-processing step for downstream tasks like summarization and questionanswering (Steinberger et al., 2007; Dasigi et al., 2019; Sukthanker et al., 2020a). Recently, multiple datasets including Ontonotes (Pradhan et al., 2012), Litbank (Bamman et al., 2020), EmailCoref (Dakle et al., 2020), and WikiCoref (Ghaddar and Langlais, 2016) have been proposed as benchmark datasets for CR, e"
2020.codi-1.3,N06-1025,0,0.104457,"r to increase the probability of a correct prediction. In this work, we evaluate the benefits of using explicit type information for CR. We show that a model that leverages entity types associated with the anaphoric/ antecedent mentions significantly reduces the problem of type inconsistency in the output coreference clusters and thus improves the overall performance of the neural baseline on four datasets. Type Information for CR: Multiple prior works have shown type-information to be a useful feature for shallow coreference resolution classifiers (Soon et al., 2001; Bengtson and Roth, 2008; Ponzetto and Strube, 2006; Haghighi and Klein, 2010; Durrett and Klein, 2014). (Soon et al., 2001) take the most frequent sense for each noun in WordNet as the semantic class for that noun and use a decision-tree for pairwise classification of whether two samples co-refer each other. (Bengtson and Roth, 2008) use a hypernym tree to extract the type information for different common nouns, and compare the proper names against a predefined list to determine if the mention is a person. They, then, pass this and many other features (like distance, agreement, etc.) through a regularized average perceptron for pairwise class"
2020.codi-1.3,D17-1018,0,0.2622,"Type Information to Improve Entity Coreference Resolution Sopan Khosla Carolyn Rose Language Technologies Institute Carnegie Mellon University, USA {sopank, cprose}@cs.cmu.edu Abstract discourse rather than an abstract idea that must be constructed from a repackaging of information revealed over an extended text. An affordance of entity anaphora is that they have easily articulated semantic types. Most of the entity CR datasets are extensively annotated for syntactic features (like constituency parse etc.) and semantic features (like entity-types). However, none of the published SOTA methods (Lee et al., 2017; Joshi et al., 2019, 2020) explicitly leverage the type information. In this paper, we present a proof of concept to portray the benefits of using type information in neural approaches for CR. Named entities are generally divided generically (e.g. person, organization etc.) or in a domain-specific manner (e.g. symptom, drug, test etc.). In this work, we consider CR datasets that contain generic entitytypes. One challenge is that the different corpora do not utilize the same set of type tags. For example, OntoNotes includes 18 types while EmailCoref includes only 4. Thus, we evaluate the perfo"
2020.codi-1.3,W12-4501,0,0.270368,"different benchmark corpora. 1 Introduction Coreference resolution (CR) is an extensively studied problem in computational linguistics and NLP (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 1999; Ng, 2017; Clark and Manning, 2016; Lee et al., 2017). Solutions to this problem allow us to make meaningful links between concepts and entities within a discourse and therefore serves as a valuable pre-processing step for downstream tasks like summarization and questionanswering (Steinberger et al., 2007; Dasigi et al., 2019; Sukthanker et al., 2020a). Recently, multiple datasets including Ontonotes (Pradhan et al., 2012), Litbank (Bamman et al., 2020), EmailCoref (Dakle et al., 2020), and WikiCoref (Ghaddar and Langlais, 2016) have been proposed as benchmark datasets for CR, especially in the sub-area of entity anaphora (Sukthanker et al., 2020b). Entity anaphora is a simpler starting place for work on anaphora because unlike abstract anaphora (Webber, 1991), entity anaphora are pronouns or noun phrases that refer to an explicitly mentioned entity in the 20 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 20–31 c Online, November 20, 2020. 2020 Association for Computational Li"
2020.codi-1.3,N18-2108,0,0.028726,"Missing"
2020.codi-1.3,2020.emnlp-main.437,0,0.0312484,"Missing"
2020.codi-1.3,P19-1279,0,0.0217897,"ong Kim Sang and De Meulder, 2003; Li et al., 2016) often group entity mentions into different types (or categories) depending on the domain and the potential downstream applications of the corpus. For example, the medical corpus used in the i2b2 Challenge 2010 (Uzuner et al., 2011) annotates domain-specific types like problem, test, symptom etc., whereas, a more general-domain dataset like CoNLL-2002 (Tjong Kim Sang, 2002) uses generic types like person, organization, and location. Type information as a predictive signal has been shown to be beneficial for NLP tasks like relation extraction (Soares et al., 2019) and entitylinking (Chen et al., 2020). It affords some level ized embeddings implicitly capture facts and relationships between real-world entities. However, in this work, we empirically show that access to explicit knowledge about entity-types benefits neural models that use BERT for CR. We show a consistent improvement in performance on four different coreference datasets from varied domains. Our contribution is that we evaluate the impact of the introduction of type information in neural entity coreference at two different levels of granularity (which we refer to as original vs common), de"
2020.codi-1.3,J01-4004,0,0.724658,"ering out some incorrect predictions in order to increase the probability of a correct prediction. In this work, we evaluate the benefits of using explicit type information for CR. We show that a model that leverages entity types associated with the anaphoric/ antecedent mentions significantly reduces the problem of type inconsistency in the output coreference clusters and thus improves the overall performance of the neural baseline on four datasets. Type Information for CR: Multiple prior works have shown type-information to be a useful feature for shallow coreference resolution classifiers (Soon et al., 2001; Bengtson and Roth, 2008; Ponzetto and Strube, 2006; Haghighi and Klein, 2010; Durrett and Klein, 2014). (Soon et al., 2001) take the most frequent sense for each noun in WordNet as the semantic class for that noun and use a decision-tree for pairwise classification of whether two samples co-refer each other. (Bengtson and Roth, 2008) use a hypernym tree to extract the type information for different common nouns, and compare the proper names against a predefined list to determine if the mention is a person. They, then, pass this and many other features (like distance, agreement, etc.) through"
2020.codi-1.3,D14-1162,0,0.0813567,"with the mention span representation created by their model; and (2) A consistency check is incorporated that compares the types of two mentions under consideration to calculate the coreference score. Please refer to Section 3 for details. existing span representation using its antecedent distribution. Moreover, they propose a coarseto-fine grained approach to pairwise scoring for tackling the computational challenges caused due to the iterative higher-order inference. More recently, Joshi et al. (2019, 2020) showed that use of contextual representations instead of wordembeddings like GloVe (Pennington et al., 2014) can further boost the results over and above those just mentioned. Our work offers additional improvement by building on the model proposed in Bamman et al. (2020), which is based on Lee et al. (2017), and adds additional nuanced information grounded in semantic types. Type Information: Named Entity Recognition datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003; Li et al., 2016) often group entity mentions into different types (or categories) depending on the domain and the potential downstream applications of the corpus. For example, the medical corpus used in the i2b2 Chall"
2020.codi-1.3,W02-2024,0,0.58614,"rganization etc.) or in a domain-specific manner (e.g. symptom, drug, test etc.). In this work, we consider CR datasets that contain generic entitytypes. One challenge is that the different corpora do not utilize the same set of type tags. For example, OntoNotes includes 18 types while EmailCoref includes only 4. Thus, we evaluate the performance of the proposed modeling approach on each dataset both with the set of type tags germaine to the dataset as well as a common set of four basic types (person, org, location, facility) inspired from research on Named Entity Recognition (NER) (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Our motivation is similar to (Durrett and Klein, 2014), which used a structured CRF with handcurated features to jointly-model the tasks of CR, entity typing, and entity linking. Their joint architecture showed an improved performance on CR over the independent baseline. However, our work differs from there’s as we show the benefits of entity-type information in neural models that use contextualized representations like BERT (Peters et al., 2018). Some prior art (Petroni et al., 2019; Roberts et al., 2020) argues that contextualCoreference resolution (CR"
2020.codi-1.3,W03-0419,0,0.770377,"Missing"
2020.codi-1.3,Q18-1042,0,0.0144035,"he impact of the introduction of type information in neural entity coreference at two different levels of granularity (which we refer to as original vs common), demonstrating their utility both in the case where gold standard type information is available, and the more typical case where it is predicted. 2 Related Work Neural Coreference Resolution: Recently, neural approaches to coreference (Joshi et al., 2020, 2019; Lee et al., 2018, 2017) have begun to show their prowess. The SOTA models show impressive performance on state-of-the-art datasets like OntoNotes (Pradhan et al., 2012) and GAP (Webster et al., 2018). The notable architecture proposed by Lee et al. (2017) scores pairs of entity mentions independently and later uses a clustering algorithm to find coreference clusters. On the other hand, Lee et al. (2018) improve upon this foundation by introducing an approximated higher-order inference that iteratively updates the 21 goal is to evaluate the benefits of type information, we too separate mention-linking from mentionidentification and only show results computed over gold-standard mentions. This controls for the effects of the mention-identification module’s performance on our experiments. Imp"
2020.emnlp-main.140,N19-1070,0,0.0202237,"Missing"
2020.emnlp-main.140,D18-1439,0,0.0395055,"Missing"
2020.emnlp-main.140,P14-1119,0,0.638488,"n demonstrates that a combination of effective strategy induction and strategy selection within this approach for the KPE task outperforms state-of-the-art models. A qualitative post-hoc analysis illustrates how these features function within the model. 1 Introduction We present a novel multi-modal approach to KeyPhrase Extraction (KPE), which is the task of automatically extracting salient phrases from a given document. The KPE task is a foundational task, which plays a facilitating role in many Information Retrieval (IR) tasks, including classification, summarization, and document indexing (Hasan and Ng, 2014). Specifically, the KPE task requires accurate selection of the phrases that best capture the web document’s topic. Wellperforming approaches take advantage of document structure and entity co-occurrences. Over the history of work in this area, there have been a variety of benchmarks (Medelyan and Witten, 2002; Nguyen and Kan, 2007; Wan and Xiao, ∗ Equally contributed. 2008; Meng et al., 2017) and an equally wide variety of both non-neural (Grineva et al., 2009; Liu et al., 2009, 2010) and neural modeling approaches (Meng et al., 2017; Zhang et al., 2017; Chen et al., 2018). The earliest KPE a"
2020.emnlp-main.140,W03-1028,0,0.0759914,"r knowledge, Strategy-based Multimodal Architecture for Keyphrase Extraction is the most comprehensive treatment of multimodality in open-domain KPE. 2 2.1 Related Work Development of Open-domain Web Keyphrase Extraction Originally, the concept keyphrase was first used by authors of scientific papers when they indicated by hand a few phrases they decided best summarized their paper (C ¸ ano and Bojar, 2019). The first corpora for automated keyphrase extraction were likewise assembled out of publications from scientific fields including technical reports (Witten et al., 1999), paper abstracts (Hulth, 2003), and scientific papers (Nguyen and Kan, 2007; Medelyan et al., 2009; Kim et al., 2010). To this day, scientific publications still serve as a fundamental fixed-domain benchmark for neural KPE methods (Meng et al., 2017; Alzaidy et al., 2019; Sahrawat et al., 2019) due to the availability of ample data of this kind. However, experiments have revealed that KPE methods trained directly on such corpora do not generalize well to other web-related genres or other types of documents (Chen et al., 2018; Xiong et al., 2019), where there may be far more heterogeneity in topics, content and structure, a"
2020.emnlp-main.140,P06-1068,0,0.042282,"ver, experiments have revealed that KPE methods trained directly on such corpora do not generalize well to other web-related genres or other types of documents (Chen et al., 2018; Xiong et al., 2019), where there may be far more heterogeneity in topics, content and structure, and there may be more variation in terms of where a key phrase may appear. Past researchers have collected corpora for KPE in Internet and social media environments, including web pages (Yih et al., 2006; Hammouda et al., 2005), blogs (Grineva et al., 2009), email (Dredze et al., 2008), news articles (Wan and Xiao, 2008; Hulth and Megyesi, 2006) and live chats (Kim and Baldwin, 2012), but most of these existing corpora fall prey to similar problems with respect to robust model training for neural models due to data sparsity and lack of representativeness in topic distribution. The recently released OpenKP (Xiong et al., 2019) is the first large-scale KPE dataset with a broad distribution of topic domains. This recent dataset facilitates work on model generalization and the opportunity to develop nuanced models that can adapt their performance based on the type of document they are applied to. This property of the dataset has inspired"
2020.emnlp-main.140,Y12-1021,0,0.02148,"ethods trained directly on such corpora do not generalize well to other web-related genres or other types of documents (Chen et al., 2018; Xiong et al., 2019), where there may be far more heterogeneity in topics, content and structure, and there may be more variation in terms of where a key phrase may appear. Past researchers have collected corpora for KPE in Internet and social media environments, including web pages (Yih et al., 2006; Hammouda et al., 2005), blogs (Grineva et al., 2009), email (Dredze et al., 2008), news articles (Wan and Xiao, 2008; Hulth and Megyesi, 2006) and live chats (Kim and Baldwin, 2012), but most of these existing corpora fall prey to similar problems with respect to robust model training for neural models due to data sparsity and lack of representativeness in topic distribution. The recently released OpenKP (Xiong et al., 2019) is the first large-scale KPE dataset with a broad distribution of topic domains. This recent dataset facilitates work on model generalization and the opportunity to develop nuanced models that can adapt their performance based on the type of document they are applied to. This property of the dataset has inspired our proposed method where strategies a"
2020.emnlp-main.140,S10-1004,0,0.0974014,"e most comprehensive treatment of multimodality in open-domain KPE. 2 2.1 Related Work Development of Open-domain Web Keyphrase Extraction Originally, the concept keyphrase was first used by authors of scientific papers when they indicated by hand a few phrases they decided best summarized their paper (C ¸ ano and Bojar, 2019). The first corpora for automated keyphrase extraction were likewise assembled out of publications from scientific fields including technical reports (Witten et al., 1999), paper abstracts (Hulth, 2003), and scientific papers (Nguyen and Kan, 2007; Medelyan et al., 2009; Kim et al., 2010). To this day, scientific publications still serve as a fundamental fixed-domain benchmark for neural KPE methods (Meng et al., 2017; Alzaidy et al., 2019; Sahrawat et al., 2019) due to the availability of ample data of this kind. However, experiments have revealed that KPE methods trained directly on such corpora do not generalize well to other web-related genres or other types of documents (Chen et al., 2018; Xiong et al., 2019), where there may be far more heterogeneity in topics, content and structure, and there may be more variation in terms of where a key phrase may appear. Past research"
2020.emnlp-main.140,D10-1036,0,0.366592,"Missing"
2020.emnlp-main.140,D09-1027,0,0.273994,"ng role in many Information Retrieval (IR) tasks, including classification, summarization, and document indexing (Hasan and Ng, 2014). Specifically, the KPE task requires accurate selection of the phrases that best capture the web document’s topic. Wellperforming approaches take advantage of document structure and entity co-occurrences. Over the history of work in this area, there have been a variety of benchmarks (Medelyan and Witten, 2002; Nguyen and Kan, 2007; Wan and Xiao, ∗ Equally contributed. 2008; Meng et al., 2017) and an equally wide variety of both non-neural (Grineva et al., 2009; Liu et al., 2009, 2010) and neural modeling approaches (Meng et al., 2017; Zhang et al., 2017; Chen et al., 2018). The earliest KPE approaches were mainly limited to domain-specific keyphrase extraction. The recent release of OpenKP (Xiong et al., 2019), a large-scale feature-rich dataset specifically developed for open-domain web-page keyphrase extraction, has encouraged further research related to the KPE task. A novel characteristic of this data set is the inclusion of features related to visual properties. Visual properties of words and web page layout offer a KPE model utility in at least two respects. F"
2020.emnlp-main.140,D09-1137,0,0.589355,"phrase Extraction is the most comprehensive treatment of multimodality in open-domain KPE. 2 2.1 Related Work Development of Open-domain Web Keyphrase Extraction Originally, the concept keyphrase was first used by authors of scientific papers when they indicated by hand a few phrases they decided best summarized their paper (C ¸ ano and Bojar, 2019). The first corpora for automated keyphrase extraction were likewise assembled out of publications from scientific fields including technical reports (Witten et al., 1999), paper abstracts (Hulth, 2003), and scientific papers (Nguyen and Kan, 2007; Medelyan et al., 2009; Kim et al., 2010). To this day, scientific publications still serve as a fundamental fixed-domain benchmark for neural KPE methods (Meng et al., 2017; Alzaidy et al., 2019; Sahrawat et al., 2019) due to the availability of ample data of this kind. However, experiments have revealed that KPE methods trained directly on such corpora do not generalize well to other web-related genres or other types of documents (Chen et al., 2018; Xiong et al., 2019), where there may be far more heterogeneity in topics, content and structure, and there may be more variation in terms of where a key phrase may ap"
2020.emnlp-main.140,P17-1054,0,0.833943,"ases from a given document. The KPE task is a foundational task, which plays a facilitating role in many Information Retrieval (IR) tasks, including classification, summarization, and document indexing (Hasan and Ng, 2014). Specifically, the KPE task requires accurate selection of the phrases that best capture the web document’s topic. Wellperforming approaches take advantage of document structure and entity co-occurrences. Over the history of work in this area, there have been a variety of benchmarks (Medelyan and Witten, 2002; Nguyen and Kan, 2007; Wan and Xiao, ∗ Equally contributed. 2008; Meng et al., 2017) and an equally wide variety of both non-neural (Grineva et al., 2009; Liu et al., 2009, 2010) and neural modeling approaches (Meng et al., 2017; Zhang et al., 2017; Chen et al., 2018). The earliest KPE approaches were mainly limited to domain-specific keyphrase extraction. The recent release of OpenKP (Xiong et al., 2019), a large-scale feature-rich dataset specifically developed for open-domain web-page keyphrase extraction, has encouraged further research related to the KPE task. A novel characteristic of this data set is the inclusion of features related to visual properties. Visual proper"
2020.emnlp-main.140,N18-1202,0,0.030749,"public leaderboard for the KPE task held by Microsoft1 . Built from Web data, it serves as the first large-scale benchmark for open-domain neural keyphrase extraction. In addition to providing the raw text of each document, OpenKP also includes various visual features associated with each text term, such as position, size, font, etc. Along with OpenKP, Xiong et al. (2019) also proposed BLING-KPE, the first neural model baseline for open-domain keyphrase extraction using visual features along with text. BLING-KPE first generates a hybrid embedding for each term by concatenating: (1) the ELMo (Peters et al., 2018) representation of the term, (2) standard sinusoidal position embedding (Vaswani et al., 2017), and (3) 18 of the 20 visual features of the term available in the OpenKP dataset. It models n-grams using multiple CNNs, and utilizes a Transformer (Vaswani et al., 2017) layer and feed-forward layer for scoring. This approach represents the first attempt at multimodal KPE. Recently, Sun et al. (2020) achieved greater success on the OpenKP task by modeling keyphrase extraction as multiple traditional text tasks, including sequence labeling, chunking, salience ranking, etc. From this work we adopt th"
2020.emnlp-main.140,C08-1122,0,0.727579,"a of this kind. However, experiments have revealed that KPE methods trained directly on such corpora do not generalize well to other web-related genres or other types of documents (Chen et al., 2018; Xiong et al., 2019), where there may be far more heterogeneity in topics, content and structure, and there may be more variation in terms of where a key phrase may appear. Past researchers have collected corpora for KPE in Internet and social media environments, including web pages (Yih et al., 2006; Hammouda et al., 2005), blogs (Grineva et al., 2009), email (Dredze et al., 2008), news articles (Wan and Xiao, 2008; Hulth and Megyesi, 2006) and live chats (Kim and Baldwin, 2012), but most of these existing corpora fall prey to similar problems with respect to robust model training for neural models due to data sparsity and lack of representativeness in topic distribution. The recently released OpenKP (Xiong et al., 2019) is the first large-scale KPE dataset with a broad distribution of topic domains. This recent dataset facilitates work on model generalization and the opportunity to develop nuanced models that can adapt their performance based on the type of document they are applied to. This property o"
2020.emnlp-main.140,D19-1521,0,0.753432,"ument’s topic. Wellperforming approaches take advantage of document structure and entity co-occurrences. Over the history of work in this area, there have been a variety of benchmarks (Medelyan and Witten, 2002; Nguyen and Kan, 2007; Wan and Xiao, ∗ Equally contributed. 2008; Meng et al., 2017) and an equally wide variety of both non-neural (Grineva et al., 2009; Liu et al., 2009, 2010) and neural modeling approaches (Meng et al., 2017; Zhang et al., 2017; Chen et al., 2018). The earliest KPE approaches were mainly limited to domain-specific keyphrase extraction. The recent release of OpenKP (Xiong et al., 2019), a large-scale feature-rich dataset specifically developed for open-domain web-page keyphrase extraction, has encouraged further research related to the KPE task. A novel characteristic of this data set is the inclusion of features related to visual properties. Visual properties of words and web page layout offer a KPE model utility in at least two respects. First, micro-level features operating at the word level, including lexical features as well as features related to size, font, color, and position of words, signal relative importance of words within extended texts. Intuitively, texts tha"
2020.emnlp-main.605,abdul-mageed-diab-2012-awatif,0,0.0372304,", it is possible to analyze how interlocutors make decisions about where and how these devices should be used based on an intricate cost-benefit analysis (Brown et al., 1987). We refer to these component actions here as face acts. The idea of face acts appears quite attractive from a computational standpoint for their potential role in understanding what is “meant” from what is “said” (Grice et al., 1975; Brown et al., 1987; Leech, 2016). Consequently, politeness has been widely researched in various domains of language technologies (Walker et al., 1997; Gupta et al., 2007; Wang et al., 2012; Abdul-Mageed and Diab, 2012; Danescu-Niculescu-Mizil et al., 2013) in addition to foundational work in pragmatics and sociolinguistics (Brown et al., 1987; Grice et al., 1975; Leech, 2016). However, much prior work modeling politeness reduces the problem to a rating task or binary prediction task, separating polite and impolite behavior. Consequently, what the models end up learning is mainly overt markers of politeness or rudeness, rather than the underlying indirect strategies for achieving politeness or rudeness through raising or attacking face, even in the indirect case where no overt markers of rudeness or politen"
2020.emnlp-main.605,W14-5001,0,0.0113102,"al., 2013), the prior work is distinguished from our own in that they do not explicitly model face changes of both parties over time. Rather, DanescuNiculescu-Mizil et al. (2013) utilizes requests annotated for politeness to create a framework specifically to relate politeness and social power. Other previous work attempt to computationally model politeness, using politeness as a feature to identify conversations that appear to go awry in online discussions (Zhang et al., 2018a). Previous work has also explored indirect speech acts as potential sources of face-threatening acts through blame (Briggs and Scheutz, 2014) and as face-saving acts in parliamentary debates (Naderi and Hirst, 2018). The closest semblance of our work is with Kl¨uwer (2011, 2015), which builds upon the notion of face provided by Goffman (1967) and invents its own set of face acts specifically in the context of “small-talk” conversations. In contrast, our work specifically operationalizes the notion of the positive and negative face of Brown et al. (1987); Brown and Levinson (1978), which is well established in the Pragmatics literature and heavily acknowledged in the NLP community (DanescuNiculescu-Mizil et al., 2013; Zhang et al.,"
2020.emnlp-main.605,D18-1547,0,0.0233161,"Description We use the pre-existing persuasion corpus of Wang et al. (2019). Each conversation comprises a series of exchanges where the persuader (ER) has to convince the persuadee (EE) to donate a part of their task earnings to the charity, Save the Children. This selected corpus is well-situated for our task since each conversation is guaranteed to have a potential face threat (i.e., a request for money) and hence, we can expect face act exchanges between the two participants. It also sets itself apart from other goal-oriented conversations such as restaurant reservations and cab booking (Budzianowski et al., 2018) since in those cases the hearer is obligated to address what might otherwise come across as an FTA (request/ booking), and thus in those cases non-compliance can be assumed to be due to logistic issues rather than an unwillingness to co-operate. In the selected corpus, the participants are Amazon Mechanical Turk workers who are anonymous to each other, which controls for the ‘social distance’ variable. Moreover, the participants have similar ‘power’, with one role having some appearance of authority in that it represents an organization, but the other role representing possession of some desi"
2020.emnlp-main.605,D19-1481,0,0.0405732,"Missing"
2020.emnlp-main.605,P13-1025,0,0.160236,"ow interlocutors make decisions about where and how these devices should be used based on an intricate cost-benefit analysis (Brown et al., 1987). We refer to these component actions here as face acts. The idea of face acts appears quite attractive from a computational standpoint for their potential role in understanding what is “meant” from what is “said” (Grice et al., 1975; Brown et al., 1987; Leech, 2016). Consequently, politeness has been widely researched in various domains of language technologies (Walker et al., 1997; Gupta et al., 2007; Wang et al., 2012; Abdul-Mageed and Diab, 2012; Danescu-Niculescu-Mizil et al., 2013) in addition to foundational work in pragmatics and sociolinguistics (Brown et al., 1987; Grice et al., 1975; Leech, 2016). However, much prior work modeling politeness reduces the problem to a rating task or binary prediction task, separating polite and impolite behavior. Consequently, what the models end up learning is mainly overt markers of politeness or rudeness, rather than the underlying indirect strategies for achieving politeness or rudeness through raising or attacking face, even in the indirect case where no overt markers of rudeness or politeness might be explicitly displayed. In c"
2020.emnlp-main.605,N19-1423,0,0.00881854,"rk. Consequently, we also adopt a modified hierarchical neural network architecture of Jiao et al. (2019) that leverages both the contextualized utterance embedding and the previous conversational context for classification. We hereby adopt this as the foundation architecture for our work and refer to our instantiation of the architecture as BERT-H I GRU. Architecture of BERT-H I GRU: An utterance uj is composed of tokens [w0 , w1 , ..., wK ], which are represented by their corresponding embeddings [e(w0 ), e(w1 ), ..., e(wK )]. In BERT-H I GRU, we obtain these using a pre-trained BERT model (Devlin et al., 2019). We pass these contextualized word representations through a BiGRU to obtain − → ← − the forward hk and backward hk hidden states of each word, before passing them into a SelfAttention layer. This gives us corresponding atten−−→ ←−− tion outputs, ahk and ahk . Finally, we concatenate the contextualized word embedding with the GRU hidden states and Attention outputs in our fusion layer to obtain the final representation of the word. We perform max-pooling over the fused word embeddings to obtain the j th utterance embedding, e(uj ). Formally,  − → −−→ hk = GRU e (wk ) , hk−1  ← − ←−− hk ="
2020.emnlp-main.605,N19-1037,0,0.231985,"ification of predicting a single face-act for each utterance2 . Several tasks in the dialogue domain, such as emotion recognition (Majumder et al., 2019; Jiao et al., 2 For each utterance with multiple labels, one is randomly select from that set to be treated as the Gold label. 2019), dialogue act prediction (Chen et al., 2018; Raheja and Tetreault, 2019) and open domain chitchat (Zhang et al., 2018b; Kumar et al., 2020), have achieved state-of-the-art results using a hierarchical sequence labelling framework. Consequently, we also adopt a modified hierarchical neural network architecture of Jiao et al. (2019) that leverages both the contextualized utterance embedding and the previous conversational context for classification. We hereby adopt this as the foundation architecture for our work and refer to our instantiation of the architecture as BERT-H I GRU. Architecture of BERT-H I GRU: An utterance uj is composed of tokens [w0 , w1 , ..., wK ], which are represented by their corresponding embeddings [e(w0 ), e(w1 ), ..., e(wK )]. In BERT-H I GRU, we obtain these using a pre-trained BERT model (Devlin et al., 2019). We pass these contextualized word representations through a BiGRU to obtain − → ← −"
2020.emnlp-main.605,2020.lrec-1.94,1,0.394496,"Missing"
2020.emnlp-main.605,W18-5214,0,0.0192538,"t explicitly model face changes of both parties over time. Rather, DanescuNiculescu-Mizil et al. (2013) utilizes requests annotated for politeness to create a framework specifically to relate politeness and social power. Other previous work attempt to computationally model politeness, using politeness as a feature to identify conversations that appear to go awry in online discussions (Zhang et al., 2018a). Previous work has also explored indirect speech acts as potential sources of face-threatening acts through blame (Briggs and Scheutz, 2014) and as face-saving acts in parliamentary debates (Naderi and Hirst, 2018). The closest semblance of our work is with Kl¨uwer (2011, 2015), which builds upon the notion of face provided by Goffman (1967) and invents its own set of face acts specifically in the context of “small-talk” conversations. In contrast, our work specifically operationalizes the notion of the positive and negative face of Brown et al. (1987); Brown and Levinson (1978), which is well established in the Pragmatics literature and heavily acknowledged in the NLP community (DanescuNiculescu-Mizil et al., 2013; Zhang et al., 2018a; Wang et al., 2012; Musi et al., 2018). Moreover, we focus on analys"
2020.emnlp-main.605,D14-1162,0,0.0833514,"in Section 3, namely the vanilla BERT-H I GRU, BERT-H I GRU-f (with residual connections (fusion)) and BERT-H I GRUsf with self-attention and fusion. To observe the effect of incorporating conversation context, we pass the utterance embedding e(uj ) obtained from the utterance encoder directly into the face act classifier. We denote the different variants of utterance encoder employed as BERTBiGRU, BERT-BiGRU-f, and BERT-BiGRU-sf with the same notation as the hierarchical variants. To explore the impact of embedding choice on model performance, we experiment with pretrained Glove embeddings (Pennington et al., 2014) in addition to BERT tokens. We denote the hierarchical models with GloVe embeddings as HiGRU and those without conversation context as BiGRU. Donation Outcome Prediction: We use the baselines mentioned above for predicting face acts and augment them with the donation loss component. We explore different values of α for weighing the two losses as described in Equation 4. 4.2 Evaluation Metrics Face act prediction: We observe the model performance in predicting the face acts for (i) only the persuader (ER), (ii) only the persuadee (EE), and (iii) both ER and EE (All). We perform five-fold cross"
2020.emnlp-main.605,N19-1373,0,0.219882,"Missing"
2020.emnlp-main.605,P18-1125,0,0.0228014,"in dialogue have been previously investigated in the NLP literature (Chang and DanescuNiculescu-Mizil, 2019; Danescu-Niculescu-Mizil et al., 2013), the prior work is distinguished from our own in that they do not explicitly model face changes of both parties over time. Rather, DanescuNiculescu-Mizil et al. (2013) utilizes requests annotated for politeness to create a framework specifically to relate politeness and social power. Other previous work attempt to computationally model politeness, using politeness as a feature to identify conversations that appear to go awry in online discussions (Zhang et al., 2018a). Previous work has also explored indirect speech acts as potential sources of face-threatening acts through blame (Briggs and Scheutz, 2014) and as face-saving acts in parliamentary debates (Naderi and Hirst, 2018). The closest semblance of our work is with Kl¨uwer (2011, 2015), which builds upon the notion of face provided by Goffman (1967) and invents its own set of face acts specifically in the context of “small-talk” conversations. In contrast, our work specifically operationalizes the notion of the positive and negative face of Brown et al. (1987); Brown and Levinson (1978), which is w"
2020.emnlp-main.605,P18-1205,0,0.0460844,"in dialogue have been previously investigated in the NLP literature (Chang and DanescuNiculescu-Mizil, 2019; Danescu-Niculescu-Mizil et al., 2013), the prior work is distinguished from our own in that they do not explicitly model face changes of both parties over time. Rather, DanescuNiculescu-Mizil et al. (2013) utilizes requests annotated for politeness to create a framework specifically to relate politeness and social power. Other previous work attempt to computationally model politeness, using politeness as a feature to identify conversations that appear to go awry in online discussions (Zhang et al., 2018a). Previous work has also explored indirect speech acts as potential sources of face-threatening acts through blame (Briggs and Scheutz, 2014) and as face-saving acts in parliamentary debates (Naderi and Hirst, 2018). The closest semblance of our work is with Kl¨uwer (2011, 2015), which builds upon the notion of face provided by Goffman (1967) and invents its own set of face acts specifically in the context of “small-talk” conversations. In contrast, our work specifically operationalizes the notion of the positive and negative face of Brown et al. (1987); Brown and Levinson (1978), which is w"
2020.emnlp-main.605,W12-1603,0,0.203991,"sing this framework, it is possible to analyze how interlocutors make decisions about where and how these devices should be used based on an intricate cost-benefit analysis (Brown et al., 1987). We refer to these component actions here as face acts. The idea of face acts appears quite attractive from a computational standpoint for their potential role in understanding what is “meant” from what is “said” (Grice et al., 1975; Brown et al., 1987; Leech, 2016). Consequently, politeness has been widely researched in various domains of language technologies (Walker et al., 1997; Gupta et al., 2007; Wang et al., 2012; Abdul-Mageed and Diab, 2012; Danescu-Niculescu-Mizil et al., 2013) in addition to foundational work in pragmatics and sociolinguistics (Brown et al., 1987; Grice et al., 1975; Leech, 2016). However, much prior work modeling politeness reduces the problem to a rating task or binary prediction task, separating polite and impolite behavior. Consequently, what the models end up learning is mainly overt markers of politeness or rudeness, rather than the underlying indirect strategies for achieving politeness or rudeness through raising or attacking face, even in the indirect case where no overt m"
2020.emnlp-main.605,P19-1566,0,0.453109,"rating task or binary prediction task, separating polite and impolite behavior. Consequently, what the models end up learning is mainly overt markers of politeness or rudeness, rather than the underlying indirect strategies for achieving politeness or rudeness through raising or attacking face, even in the indirect case where no overt markers of rudeness or politeness might be explicitly displayed. In contrast, the main contribution of this work is the investigation of eight major face acts, similar to dialogue acts, including an investigation of their usage in a publicly available corpus of Wang et al. (2019). In the selected corpus, a persuader (ER) is tasked with convincing a persuadee (EE) to donate money to a charity. The nature of the task prompts frequent utilization of face acts in interaction, and thus these face acts are abundantly present in the chosen dataset. We also provide a generalized framework for operationalizing face acts in conversations as well as design an annotation scheme to instantiate these face acts in the context of persuasion conversations (§2.1, §2.3). We offer the annotations we have added to this public dataset as another contribution of this work 7473 Proceedings o"
2020.emnlp-main.605,N19-1364,0,0.203847,"tual utterance embeddings ec (uj ) till the j th utterance and project it through a linear layer with tanh activation to obtain the donation score donj . The tanh non-linearity ensures that the donation score remains between -1 and 1 and intuitively denotes the delta change in scores from the previous step. We finally compute the probability of donation o0j at the j th step, by applying sigmoid activation over the sum of probability at the previous step and the delta change donj . This ensures that the o0th j probability is restricted between 0 and 1. We obtain the donation loss Ld similar to Yang et al. (2019) by taking the mean squared error of the donation probability at the last step o0n and the actual donation outcome on . on is 1 if successful, otherwise 0. We also exper7477 iment with Binary Cross Entropy loss and obtain similar results. ed (uj ) = MaskSelfAttention(ec (uj )) donj = tanh(Wd [ed (uj )] + bd ) o0j =σ(o0j−1 + donj ) Ld =(o0n − on )2 The donation loss is combined with the original face-act loss in a weighted fashion using some hyperparameter α, such that α  [0, 1]. Ltot = αLf + (1 − α)Ld (4) Correlating face acts with donation outcome: The aforementioned formulation enables us t"
2020.emnlp-main.626,W19-1909,0,0.0505075,"Missing"
2021.acl-long.82,D12-1091,0,0.0342146,"s only knowledge distillation, uses only ensembling, and uses neither. This means that in the most naive setting, we train the re-ranker using the hard training labels and re-rank the candidates using only the re-ranker. 5.2 Evaluation We report standard ranking metrics: Mean Rank (MR), Mean Reciprocal Rank (MRR), Hits at 1 (H@1), Hits at 3 (H@3), and Hits at 10 (H@10). We follow past work and use the filtered setting (Bordes et al., 2013), removing all positive entities other than the target entity before calculating the target entity’s rank. We utilize paired bootstrap significance testing (Berg-Kirkpatrick et al., 2012) with the MRR to validate the statistical significance of improvements. To account for the large number of comparisons 1022 being performed, we apply the Holm–Bonferroni method (Holm, 1979) to correct for multiple hypothesis testing. We define families for the three primary hypotheses that we tested with our experiments. They are as follows: (1) The deep convolutional BERT models outperform the shallow convolutional BERT models. (2) BERT-ResNet improves upon our BERT-DeepConv baseline. (3) The re-ranking procedure improves the original rankings. This selection has the benefit of allowing for a"
2021.acl-long.82,D14-1067,0,0.0247007,"ons and demonstrate that our model outperforms recent KG completion methods in this challenging setting. We find that our model’s performance improvements stem primarily from its robustness to sparsity. We then distill the knowledge from the convolutional network into a student network that re-ranks promising candidate entities. This re-ranking stage leads to further improvements in performance and demonstrates the effectiveness of entity re-ranking for KG completion.1 1 Introduction Knowledge graphs (KGs) have been shown to be useful for a wide range of NLP tasks, such as question answering (Bordes et al., 2014a,b), dialog systems (Ma et al., 2015), relation extraction (Mintz et al., 2009; Vashishth et al., 2018), and recommender systems (Zhang et al., 2016). However, because scaling the collection of facts to provide coverage for all the true relations that hold between entities is difficult, most existing KGs are incomplete (Dong et al., 2014), limiting their utility for downstream applications. Because of this problem, KG completion (KGC) has come to be a widely studied task (Yang et al., 2015; Trouillon et al., 2016; Shang et al., 2018; Dettmers et al., 2018; ∗ Work performed while at Carnegie M"
2021.acl-long.82,2020.tacl-1.28,0,0.0299502,"cation model to the ranking paradigm by exhaustively evaluating all possible triples for a given query, (e1 , r, ?). However, the ranking performance was not competitive2 , and such an approach is not scalable to large KG datasets like those explored in this work. Exhaustively applying BERT to compute all rankings for the test set for our largest dataset would take over two months. In our re-ranking setting, we reduce the number of triples that need to be evaluated by over 7700×, reducing the evaluation time to less than 15 minutes. BERT as a Knowledge Base: Recent work (Petroni et al., 2019; Jiang et al., 2020; Rogers et al., 2020) has utilized the masked-languagemodeling (MLM) objective to probe the knowledge contained within pre-trained models using fill-in-the-blank prompts (e.g. “Dante was born in [MASK]”). This body of work has found that pre-trained language models such as BERT capture some of the relational knowledge contained within their pre-training corpora. This motivates us to utilize these models to develop entity representations that are well-suited for KGC. Re-Ranking: Wang et al. (2011) introduced cascade re-ranking for document retrieval. This approach applies inexpensive models to"
2021.acl-long.82,P16-1137,0,0.0254202,"a sequence of two-dimensional convolutions. The final feature map is then average pooled and projected to a query vector, which is used to rank candidate entities. We extract promising candidates and train a re-ranking model utilizing knowledge distilled from the original ranking model. The final candidate ranking is generated by ensembling the ranking and re-ranking models. 3.3 ConceptNet-100K ConceptNet (Speer and Havasi, 2013) is a KG that contains commonsense knowledge about the world such as the fact (go to dentist, motivatedBy, prevent tooth decay). We utilize ConceptNet-100k (CN-100K) (Li et al., 2016) which consists of the Open Mind Common Sense entries in the ConceptNet dataset. This KG is much sparser than benchmark datasets like FB15k-237, which makes it well-suited for our purpose. We use the training, validation, and testing splits of Malaviya et al. (2020) to allow for direct comparison. We also use the textual descriptions released by Malaviya et al. (2020) to represent the KG entities. 4 Methods We provide an overview of our model architecture in Figure 2. We first extract feature representations from BERT (Devlin et al., 2019) to develop textual entity embeddings. Motivated by our"
2021.acl-long.82,W19-4302,0,0.029134,"Missing"
2021.acl-long.82,D19-1250,0,0.0604604,"Missing"
2021.acl-long.82,D17-1184,0,0.0657975,"Missing"
2021.codi-sharedtask.1,D19-1422,0,0.0559867,"variant that is only trained on bridging annotations. We evaluated their best-performing model, which was trained on the RST sub-corpus of ARRAU, on CODI - CRAC 2021 data.19 The baseline for Task 3 leverages a simple heuristic that only considers demonstrative pronouns (this, that) as anaphors and considers the immediately preceding clause/utterance in the conversation to be their antecedent. Although simplistic, the algorithm achieves respectable scores on the CODI CRAC 2021 development corpus. The performance KU_NLP submitted results for tasks 1 and 2. For identity anaphora, they leveraged Cui and Zhang (2019)’s model with an ELECTRA-large backbone 18 20 The necessary scripts are available from https:// github.com/sopankhosla/codi2021_scripts 21 Participants were allowed to create teams. https://github.com/lxucs/coref-hoi/ https://github.com/juntaoy/ dali-bridging 19 8 9 Discourse Deixis Resolution Bridging Resolution Anaphora Resolution Track - Joshi et al. Leverage baseline’s architecture to find the correct (2019) bridging antecedent in the gold setting. KU_NLP INRIA - Yu and Poe- A multi-pass sieve approach which used the baseline sio (2020) as one of the sieves and consists of a set of learnin"
2021.codi-sharedtask.1,E89-1022,0,0.449685,"v., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect"
2021.codi-sharedtask.1,J18-3007,0,0.521559,"chael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialog"
2021.codi-sharedtask.1,D17-1018,0,0.0879633,"iple tasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, h"
2021.codi-sharedtask.1,N18-2108,0,0.205145,"Missing"
2021.codi-sharedtask.1,2020.acl-main.132,0,0.135367,"urse Deixis in Dialogue Sopan Khosla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on an"
2021.codi-sharedtask.1,J18-2002,1,0.943726,"ridging, and Discourse Deixis in Dialogue Sopan Khosla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the liter"
2021.codi-sharedtask.1,L16-1145,0,0.382038,"Missing"
2021.codi-sharedtask.1,2020.tacl-1.5,0,0.344926,"ons. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity coreference. In addit"
2021.codi-sharedtask.1,D19-1588,0,0.0733603,"om the end-to-end neural coreference resolution model of Joshi et al. (2020). They recognized singletons, encoded speakers for all turns, and leveraged other out-of-domain datasets during training. Eval DD (Pred) UTD_NLP DFKI 42.70 20.97 35.35 17.43 39.64 23.76 35.43 23.86 38.3 21.5 Baseline 12.12 15.75 18.27 13.55 14.9 Table 5: Performance on Task 3 (Evaluation Phase) – Discourse Deixis (CoNLL Avg. F1) INRIA submitted an end-to-end transformerbased model fine-tuned for the bridging resolution task. They formulated the bridging problem as antecedent selection, and leveraged Lee et al. (2018); Joshi et al. (2019)’s architecture to find the correct antecedent. for mention detection. The resulting mention representation, created from the constituent token representations, is then fed to a pointer-network (Vinyals et al., 2015) based coreference resolution model for clustering. They solved the bridging resolution problem using a machine reading comprehension framework, where they constructed a query for each entity of the form – ""What is related of ENTITY?"". The input of their model is the query and the document (i.e., all utterances of dialogue), and the output is the entity span that is the answer for"
2021.codi-sharedtask.1,2021.crac-1.2,1,0.739024,".edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect language suffering from disfluencies. Dialogue involves much more deictic reference, vaguer anaphoric and discourse deictic reference, speaker grounding of pronouns and long-distance conversation structure. These are complexities that are often missing in news or Wikipedia articles,"
2021.codi-sharedtask.1,P19-1066,0,0.0336391,"vidually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity"
2021.codi-sharedtask.1,D17-1021,0,0.24606,"Missing"
2021.codi-sharedtask.1,2020.coling-main.331,1,0.631559,"atasets: the AMI corpus (Carletta, 2006), the LIGHT corpus (Urbanek et al., 2019), the PERSUASION corpus (Wang et al., 2019) and SWITCHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on experiments that were started by Vieira and Poesio (Poesio and Vieira, 1998) and continued in the GNOME project (Poesio, 2004). The ARRAU Release 1 guidelines followed the GNOME guidelines, but with an extension and a simplification. Annotators were asked to mark a markable as related to a particular antecedent if it stood to that antecedent in one of the relations identified in GNOME (indeed, the same examples Switchboard SWITCHBOARD8 (Godfrey et al., 1992) is one of the best known dialogue corpora. It consists of 1,155 five-minute spo"
2021.codi-sharedtask.1,J93-2004,0,0.0756707,"ther markable and thus form a singleton coreference chain. Moreover, in ARRAU non-referring markables are manually sub-classified into expletives, predicative, and quantifiers. In addition, all generic references are marked, including premodifiers when the entity referred to is mentioned again, e.g., in the case of ARRAU: Corpus and Annotation Scheme Genres The ARRAU corpus4 (Poesio and Artstein, 2008; Uryupina et al., 2020) was designed to cover a variety of genres. It includes a substantial amount of news text in a sub-corpus called RST, consisting of the entire subset of the Penn Treebank (Marcus et al., 1993) that was annotated in the RST treebank (Carlson et al., 2003). In addition to the news data, ARRAU includes three more sub-corpora. The TRAINS sub-corpus includes all the task-oriented dialogues in the TRAINS-93 corpus5 as well as the pilot dialogues in the socalled TRAINS-91 corpus. The PEAR sub-corpus consists of the complete collection of spoken nar6 The original intention had been to use the soon-to-bereleased ARRAU 3, but as the work on this version was still under way by the time the training data had to be released, ARRAU 2 was used instead–i.e., the exact same version used for the CRA"
2021.codi-sharedtask.1,2021.naacl-main.131,1,0.669111,"osla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important"
2021.codi-sharedtask.1,D14-1056,0,0.107024,"2018). That shared task was articulated around three tasks: identity coreference (including identification of nonreferring expressions), bridging references, and discourse deixis. The organization of the shared task resulted in the development of an extended version of the Coreference Reference Scorer (Pradhan et al., 2014), which also scores non-referring expressions. Separate scorers were developed for bridging reference resolution, carrying out both mention-based evaluation and entity-based evaluation of bridging references, as done by Hou et al. (2018), and for discourse deixis, based on Kolhatkar and Hirst (2014). The present shared task was modeled on that. (5) There was not a moment to be lost: away went Alice like the wind, and was just in time to hear it say, as it turned a corner, ’Oh my ears and whiskers, how late it’s getting!’ She was close behind it when she turned the corner, but the Rabbit was no longer to be seen: she found herself in [a long, low hall, which was lit up by a row of lamps hanging from [the roof]]. There were doors all round the hall, but they were all locked; and when Alice had been all the way down one side and up the other, trying every door, she walked sadly down [the mi"
2021.codi-sharedtask.1,P12-1084,1,0.830857,"es of identity anaphora exist, as well as other types of anaphoric relations that are not annotated in ONTONOTES but are annotated in other corpora. Split-antecedent anaphora In ONTONOTES, plural reference is only marked when the antecedent is mentioned by a single noun phrase. However, split-antecedent anaphors are also possible (Eschenbach et al., 1989; Kamp and Reyle, 1 https://competitions.codalab.org/ competitions/30312 2 just in time to see it pop down a large rabbithole under the hedge. and Martí, 2010), ARRAU (Uryupina et al., 2020), GNOME (Poesio, 2004), GUM (Zeldes, 2017), IS NOTES (Markert et al., 2012), the Prague Dependency Treebank (Nedoluzhko, 2013), and TÜBA DZ (Versley, 2008). (See Poesio et al. (2016) for a more detailed survey and Nedoluzhko et al. (2021) for a more recent, extensive update.) Bridging references There are other forms of anaphoric reference besides identity, and there are now a number of corpora annotating (a subset of) these forms. Possibly the most studied of nonidentity anaphora is bridging reference or associative anaphora (Clark, 1977; Hawkins, 1978; Prince, 1981) as in (5), where bridging reference / associative anaphora the roof refers to an object which is rel"
2021.codi-sharedtask.1,J98-2001,1,0.56338,"CHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on experiments that were started by Vieira and Poesio (Poesio and Vieira, 1998) and continued in the GNOME project (Poesio, 2004). The ARRAU Release 1 guidelines followed the GNOME guidelines, but with an extension and a simplification. Annotators were asked to mark a markable as related to a particular antecedent if it stood to that antecedent in one of the relations identified in GNOME (indeed, the same examples Switchboard SWITCHBOARD8 (Godfrey et al., 1992) is one of the best known dialogue corpora. It consists of 1,155 five-minute spontaneous telephone conversations between two participants not previously acquainted with each other. In these conversations, callers q"
2021.codi-sharedtask.1,W00-1007,0,0.805051,"nt. . . The statement was the [US]1 ’s government first acknowledgment of what other groups, such as the International Monetary Fund, have been predicting for months. • an undersp-rel relation for ‘obvious cases of bridging that didn’t fit any other category’. Discourse deixis Discourse deixis in its full form is a very complex form of reference, both to annotate (Kolhatkar et al., 2018) and to resolve. Very few anaphoric annotation projects have attempted to annotate discourse deixis in its entirety (Kolhatkar et al., 2018). More typical is a partial annotation, as in (Byron and Allen, 1998; Navarretta, 2000), who annotated pronominal reference to abstract objects; in O NTO N OTES, where event anaphora was marked (Pradhan et al., 2007); and in the work of Kolhatkar and Hirst (2014), which focused on so-called shell nouns. In ARRAU, A coder specifying that a referring expression is discourseold is asked whether its antecedent was introduced using a phrase (markable) or a segment (discourse segment). Coders who choose segment have to mark a sequence of predefined clauses. (9) The Treasury report, which is required annually by a provision of the 1988 trade act, again took South Korea to task for its"
2021.codi-sharedtask.1,P14-2006,1,0.93955,"association such as identity of sense anaphora, etc. (Poesio, 2016). Some of these resources are of a sufficient size to support shared tasks. In particular, the AR RAU corpus was used as the dataset for the Shared Task on Anaphora Resolution with ARRAU in the CRAC 2018 Workshop (Poesio et al., 2018). That shared task was articulated around three tasks: identity coreference (including identification of nonreferring expressions), bridging references, and discourse deixis. The organization of the shared task resulted in the development of an extended version of the Coreference Reference Scorer (Pradhan et al., 2014), which also scores non-referring expressions. Separate scorers were developed for bridging reference resolution, carrying out both mention-based evaluation and entity-based evaluation of bridging references, as done by Hou et al. (2018), and for discourse deixis, based on Kolhatkar and Hirst (2014). The present shared task was modeled on that. (5) There was not a moment to be lost: away went Alice like the wind, and was just in time to hear it say, as it turned a corner, ’Oh my ears and whiskers, how late it’s getting!’ She was close behind it when she turned the corner, but the Rabbit was no"
2021.codi-sharedtask.1,W13-2313,0,0.0303269,"f anaphoric relations that are not annotated in ONTONOTES but are annotated in other corpora. Split-antecedent anaphora In ONTONOTES, plural reference is only marked when the antecedent is mentioned by a single noun phrase. However, split-antecedent anaphors are also possible (Eschenbach et al., 1989; Kamp and Reyle, 1 https://competitions.codalab.org/ competitions/30312 2 just in time to see it pop down a large rabbithole under the hedge. and Martí, 2010), ARRAU (Uryupina et al., 2020), GNOME (Poesio, 2004), GUM (Zeldes, 2017), IS NOTES (Markert et al., 2012), the Prague Dependency Treebank (Nedoluzhko, 2013), and TÜBA DZ (Versley, 2008). (See Poesio et al. (2016) for a more detailed survey and Nedoluzhko et al. (2021) for a more recent, extensive update.) Bridging references There are other forms of anaphoric reference besides identity, and there are now a number of corpora annotating (a subset of) these forms. Possibly the most studied of nonidentity anaphora is bridging reference or associative anaphora (Clark, 1977; Hawkins, 1978; Prince, 1981) as in (5), where bridging reference / associative anaphora the roof refers to an object which is related to / associated with, but not identical to, th"
2021.codi-sharedtask.1,W12-4501,0,0.677797,"ric relations: identity, bridging references and discourse deixis, we defined multiple tasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for wh"
2021.codi-sharedtask.1,nissim-etal-2004-annotation,0,0.152087,"2) is one of the best known dialogue corpora. It consists of 1,155 five-minute spontaneous telephone conversations between two participants not previously acquainted with each other. In these conversations, callers question receivers on provided topics, such as child care, recycling, and news media. 440 speakers participate in these 1,155 conversations, producing 221,616 utterances. It was 7 Identity anaphora also includes split antecedent plural anaphoric reference. 8 https://catalog.ldc.upenn.edu/ LDC97S62 5 annotated for dialogue acts by Stolcke et al. (1997)9 and for information status by Nissim et al. (2004). exactly the same MMAX style – and by the same two annotators from the DALI team at Queen Mary University and University of Essex, Dr. Maris Camilleri and Dr. Paloma Carretero Garcia, who annotated and checked ARRAU Release 3, which is currently being prepared for release. However, due to time constraints, each document was only annotated by a single annotator, with spot checks carried out by the other annotator and Massimo Poesio (in ARRAU 3 each document was looked at by both annotators, and most documents were also independently checked by Massimo Poesio). To prepare the data for the share"
2021.codi-sharedtask.1,W04-0210,1,0.761359,"j didn’t fit [her]i . However, many other types of identity anaphora exist, as well as other types of anaphoric relations that are not annotated in ONTONOTES but are annotated in other corpora. Split-antecedent anaphora In ONTONOTES, plural reference is only marked when the antecedent is mentioned by a single noun phrase. However, split-antecedent anaphors are also possible (Eschenbach et al., 1989; Kamp and Reyle, 1 https://competitions.codalab.org/ competitions/30312 2 just in time to see it pop down a large rabbithole under the hedge. and Martí, 2010), ARRAU (Uryupina et al., 2020), GNOME (Poesio, 2004), GUM (Zeldes, 2017), IS NOTES (Markert et al., 2012), the Prague Dependency Treebank (Nedoluzhko, 2013), and TÜBA DZ (Versley, 2008). (See Poesio et al. (2016) for a more detailed survey and Nedoluzhko et al. (2021) for a more recent, extensive update.) Bridging references There are other forms of anaphoric reference besides identity, and there are now a number of corpora annotating (a subset of) these forms. Possibly the most studied of nonidentity anaphora is bridging reference or associative anaphora (Clark, 1977; Hawkins, 1978; Prince, 1981) as in (5), where bridging reference / associati"
2021.codi-sharedtask.1,D12-1071,1,0.763158,"e-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity coreference. In addition, more research has been carried out on aspects of anaphoric interpretation that go beyond identity anaphora but are covered by datasets ∗ Work done when the author was a student at CMU 1 Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue, pages 1–15 Punta Cana, Dominican Republic, November"
2021.codi-sharedtask.1,poesio-artstein-2008-anaphoric,1,0.800941,"lopment in the trial phase, whereas all other datasets were used for training.6 A limitation of most resources annotated for anaphora is that they mostly focus on expository text. The one substantial dataset of anaphoric relations in dialogue is ANCOR for French (Muzerelle et al., 2014), in which identity and bridging anaphora are annotated. Among the small number of English corpora that cover dialogue include ONTONOTES (Pradhan et al., 2012), which contains a small number of conversations annotated for identity anaphora and a small subtype of discourse deixis (as discussed earlier). ARRAU’s (Poesio and Artstein, 2008; Uryupina et al., 2020) TRAINS sub-corpus consists of task-oriented dialogues for identity, bridging, and discourse deixis. We include TRAINS in CODI - CRAC 2021 training data. The more recently released ONTOGUM (Zhu et al., 2021) builds upon the ONTONOTES schema and adds several new genres (including more spoken data) to the ONTONOTES family. Both identity anaphora and bridging are annotated in the dataset. 3 Annotation scheme The original annotation scheme used for Release 1 (Poesio and Artstein, 2008) is distributed with the dataset and is also available from the ARRAU corpus page. For the"
2021.codi-sharedtask.1,N19-1176,1,0.849468,"tings, and evaluation metrics in Section 4, and submission details in Section 5. This is followed by details of the baselines in Section 6 and participating systems in Section 7. We present a discussion of the performance of the systems on different tasks and sub-corpora in Section 8, and finally conclude this paper in Section 9. 2 1993), as in (2). These are also cases of plural identity coreference, but to sets composed of two or more entities introduced by separate noun phrases. Such references are annotated in, e.g., ARRAU (Uryupina et al., 2020), GUM (Zeldes, 2017) and Phrase Detectives (Poesio et al., 2019). (2) [John]1 met [Mary]2 . [He]1 greeted [her]2 . [They]1,2 went to the movies. Discourse deixis In ONTONOTES, event anaphora, a subtype of discourse deixis (Webber, 1991; Kolhatkar et al., 2018) is marked, as in (3) (where [that] arguably refers to the event of a white rabbit with pink ears running past Alice) but not the whole range of abstract anaphora, illustrated by, e.g., (4), where again arguably [this] refers to the fact that the Rabbit was able to talk. (Both examples from the Phrase Detectives corpus (Poesio et al., 2019).) (3) So she was considering in her own mind (as well as she"
2021.codi-sharedtask.1,2020.emnlp-main.686,0,0.790191,"f 55 individual participants registered for the CODI - CRAC 2021 shared task on CodaLab.21 Among them, five teams submitted results for Task 1, three submitted results for Task 2, and two submitted results for Task 3. Teams UTD_NLP, KU_NLP, DFKI_TalkingRobots, Emory_NLP, and INRIA submitted system description papers. We summarize their approaches below (and in Table 2): Baselines UTD_NLP participated in all three tasks. For identity anaphora, they deployed a pipeline architecture consisting of a mention detection component and an entity coreference component. The coreference component extends Xu and Choi (2020)’s implementation of Lee et al. (2018) by modifying the objective so that it can output singleton clusters, and enforces dialogue-specific constraints. They setup a similar architecture for discourse deixis. However, they slightly modified the objective function in Xu and Choi (2020) by classifying each span as a candidate anaphor, a candidate antecedent, or a non-mention in the mention detection stage, and resolving only candidate anaphors to candidate antecedents later. The team used a multi-pass sieve approach for bridging resolution to target same-head bridging links, with Yu and Poesio (2"
2021.codi-sharedtask.1,2020.lrec-1.1,1,0.560581,"ities that are often missing in news or Wikipedia articles, which form a large chunk of current datasets for coreference resolution. There has been some research on coreference in dialogue (Byron, 2002; Eckert and Strube, 2001; Müller, 2008), but very limited in scope (primarily related to pronominal interpretation), due to the lack of suitable corpora. The one language for which substantial corpora of coreference in dialogue exist is French: the ANCOR corpus (Muzerelle et al., 2014) has enabled the development of an end-to-end neural model for coreference interpretation in dialogue by Grobol (2020). For English, the one resource we are aware of fully annotated for anaphoric reference is the TRAINS corpora included in the ARRAU corpus (Uryupina et al., 2020). The objective of the CODI - CRAC 2021 Shared In this paper, we provide an overview of the CODI - CRAC 2021 Shared Task. The shared task focuses on detecting anaphoric relations in different genres of conversations. Using five conversational datasets, four of which have been newly annotated with a wide range of anaphoric relations: identity, bridging references and discourse deixis, we defined multiple tasks focusing individually on"
2021.codi-sharedtask.1,2020.coling-main.538,1,0.858079,"A; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect language suffering from disfluencies. Dialogue involves m"
2021.codi-sharedtask.1,D19-1062,0,0.248851,"duced (antecedent). For discourse-old mentions, an antecedent can be identified, either of type phrase (if the antecedent was introduced using a nominal markable) or segment (not introduced by a nominal markable, for discourse deixis).7 In addition, referring NPs can be marked as related to a previously mentioned discourse entity to identify them as examples of associative (bridging) anaphora. 3.2 New Data The annotated corpus prepared for the CODI - CRAC 2021 shared task consists of conversations from four well-known conversational datasets: the AMI corpus (Carletta, 2006), the LIGHT corpus (Urbanek et al., 2019), the PERSUASION corpus (Wang et al., 2019) and SWITCHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on"
2021.codi-sharedtask.1,2021.naacl-main.329,1,0.738872,"Missing"
2021.codi-sharedtask.1,P16-1216,0,0.0628664,"Missing"
2021.codi-sharedtask.1,2020.coling-main.315,1,0.910842,"in Dialogue Sopan Khosla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Ar"
2021.codi-sharedtask.1,C18-1003,0,0.0784215,"USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect language suffering from disfluencies. Di"
2021.codi-sharedtask.1,P19-1566,0,0.250523,", an antecedent can be identified, either of type phrase (if the antecedent was introduced using a nominal markable) or segment (not introduced by a nominal markable, for discourse deixis).7 In addition, referring NPs can be marked as related to a previously mentioned discourse entity to identify them as examples of associative (bridging) anaphora. 3.2 New Data The annotated corpus prepared for the CODI - CRAC 2021 shared task consists of conversations from four well-known conversational datasets: the AMI corpus (Carletta, 2006), the LIGHT corpus (Urbanek et al., 2019), the PERSUASION corpus (Wang et al., 2019) and SWITCHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on experiments that were started by Vieira and"
2021.codi-sharedtask.1,2021.acl-short.59,0,0.0207201,"gue is ANCOR for French (Muzerelle et al., 2014), in which identity and bridging anaphora are annotated. Among the small number of English corpora that cover dialogue include ONTONOTES (Pradhan et al., 2012), which contains a small number of conversations annotated for identity anaphora and a small subtype of discourse deixis (as discussed earlier). ARRAU’s (Poesio and Artstein, 2008; Uryupina et al., 2020) TRAINS sub-corpus consists of task-oriented dialogues for identity, bridging, and discourse deixis. We include TRAINS in CODI - CRAC 2021 training data. The more recently released ONTOGUM (Zhu et al., 2021) builds upon the ONTONOTES schema and adds several new genres (including more spoken data) to the ONTONOTES family. Both identity anaphora and bridging are annotated in the dataset. 3 Annotation scheme The original annotation scheme used for Release 1 (Poesio and Artstein, 2008) is distributed with the dataset and is also available from the ARRAU corpus page. For the second release (Uryupina et al., 2020), the guidelines for bridging were extended and genericity was also annotated using the GNOME guidelines, but a complete new manual was not produced. However, a fairly extensive description ca"
2021.codi-sharedtask.1,Q18-1042,0,0.0216415,"t al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity coreference. In addition, more research has been carried out on aspects of anaphoric interpretation that go beyond identity anaphora but are covered by datasets ∗ Work done when the author was a student at CMU 1 Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue, pages 1–15 Punta Cana, Dominican Republic, November 10, 2021. ©2021 Association for Computational Linguistics Task in Anaphora Resolution in Dialogue1 was to provide participants with the opportunity to develop automated approaches for corefer"
2021.codi-sharedtask.1,P15-1137,0,0.0532448,"eixis, we defined multiple tasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et"
2021.eacl-main.152,D15-1008,0,0.0186187,"mps from May to September 2019 from the Reddit Pushshift API (Baumgartner et al., 2020), we collect all submissions and comments from the top political subreddits2 by subscriber count. The collected subreddits were manually labeled as left or right, based on the subreddit description and top posts. We then select the top 12 left and top 12 right subreddits from the monthly dumps where discussion is primarily focused on U.S. politics.3 The selected subreddits are shown in Table 3 (Supplementary Material). 2.1 Paired Ideology Ranking Task Prior work on annotating viewpoints (Iyyer et al., 2014; Bamman and Smith, 2015) generally presents annotators with texts in isolation to label with an ideology of interest. One drawback of this approach is the high degree of political expertise annotators are required to have to recognize that a text matches an ideology. To reduce the amount of overhead in recruiting and training political annotators, we present annotators instead with a paired ideology ranking task. Rather than examining texts in isolation, annotators are shown two texts and asked to select the text that is more likely to be authored by someone with the ideology of interest. For our setup, our goal is t"
2021.eacl-main.152,P14-1105,0,0.0242817,"Using the monthly dumps from May to September 2019 from the Reddit Pushshift API (Baumgartner et al., 2020), we collect all submissions and comments from the top political subreddits2 by subscriber count. The collected subreddits were manually labeled as left or right, based on the subreddit description and top posts. We then select the top 12 left and top 12 right subreddits from the monthly dumps where discussion is primarily focused on U.S. politics.3 The selected subreddits are shown in Table 3 (Supplementary Material). 2.1 Paired Ideology Ranking Task Prior work on annotating viewpoints (Iyyer et al., 2014; Bamman and Smith, 2015) generally presents annotators with texts in isolation to label with an ideology of interest. One drawback of this approach is the high degree of political expertise annotators are required to have to recognize that a text matches an ideology. To reduce the amount of overhead in recruiting and training political annotators, we present annotators instead with a paired ideology ranking task. Rather than examining texts in isolation, annotators are shown two texts and asked to select the text that is more likely to be authored by someone with the ideology of interest. For"
2021.eacl-main.152,D17-1116,0,0.13141,"explicit ideology label to a text with less contextual knowledge about how the text was produced. Thus, annotators may rely heavily on their own experiential factors, such as one’s own beliefs or level of political engagement, when considering ideology. As a result, this process may introduce inconsistencies and biases in ideological labels used for political analysis. In this paper, we present an exploration of how experiential factors play a role in how annotators perceive ideology in text. Building upon prior work investigating annotation bias (Zaidan and CallisonBurch, 2011; Waseem, 2016; Joseph et al., 2017; Ross et al., 2017; Schwartz et al., 2017; Geva et al., 2019), we construct an annotated corpus of posts from political subcommunities on Reddit but incorporate additional contextual information about the annotators making ideology judgments.1 While previous work (Joseph et al., 2017) has shown that source-side contextual features, such as user profiles and previous tweets, can influence label quality in stance annotation, we focus our analyses on contextual factors on the side of annotators. Most similar to our work, Carpenter et al. (2017) and Carpenter et al. (2018) examine the impact of a"
2021.eacl-main.258,S17-2093,0,0.059351,"Missing"
2021.eacl-main.258,P07-1056,0,0.503146,"Missing"
2021.eacl-main.258,W06-1615,0,0.297617,"guidelines (Styler IV et al., 2014) used by the Clinical TempEval challenges on event ordering in clinical notes (Bethard et al., 2015, 2016, 2017),2 but also cover event extraction in a novel domain: doctor-patient conversations. 2.2 Unsupervised Domain Adaptation Unsupervised domain adaptation is the task of transferring a model from a source domain to a target domain, using only unlabeled data from the target domain, by aligning source and target distri2964 2 We provide a detailed comparison with this work in §3.1. butions. Early approaches such as structural correspondence learning (SCL) (Blitzer et al., 2006, 2007) tried to solve this by mapping source and target examples into a shared pivot feature space, where pivot features are selected to be features that behave the same way for discriminative learning in both domains (e.g., sentiment terms such as amazing and great show similar behavior for sentiment analysis across domains). With advances in neural representation learning, autoencoder-based methods (Glorot et al., 2011; Chen et al., 2014), neural SCL (Ziser and Reichart, 2017), adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016) and LM fine-tuning methods (Han and"
2021.eacl-main.258,Q14-1022,0,0.0115155,"tem that can track a patient’s disease progression from clinical notes, event extractors only need to focus on extracting medical events relevant to that illness. This task/domain specificity has encouraged prior work to focus on specific event types (Grishman and Sundheim, 1996; Doddington et al., 2004; Kim et al., 2008) or domains (Pustejovsky et al., 2003b; Sims et al., 2019). Owing to this narrow focus, supervised event extractors often fail to adapt to new domains or event types (Keith et al., 2017). Unsupervised event extractors that use syntactic rulebased modules (Saur´ı et al., 2005; Chambers et al., 2014), conversely, have a tendency to over-generate by labeling most verbs and nouns as events. In this work, we try to achieve a balance between these extremes by adapting event extractors using unsupervised domain adaptation techniques. We also study the behavior of these techniques under various types of linguistic shifts (e.g., lexical shift, semantic shift) to gain insight into differences among them. Exploring adaptability under no (or little) supervision is crucial, since sourcing annotated data for new domains, especially medical texts, can be expensive and time-consuming. Following prior w"
2021.eacl-main.258,cybulska-vossen-2014-using,0,0.0266558,"ers interesting patterns such as varying ability of models to leverage subword morphology to generalize to technical terms, and LIW’s performance improvement on long-term state events (e.g., chronic illnesses). Our best models achieve F1 scores of 70.0 and 72.9 on notes and conversations respectively with no training data.1 2 Related Work 2.1 Event Extraction Most prior event extraction work has focused on news articles, resulting in the development of several datasets (Onyshkevych et al., 1993; Grishman and Sundheim, 1996; Pustejovsky et al., 2003b; Doddington et al., 2004; Lee et al., 2012; Cybulska and Vossen, 2014; Mitamura et al., 2016). Recently, event extraction has also been explored in other domains such as biology (Wattarujeekrit et al., 2004; Kim et al., 2008, 2009; Berant et al., 2014), Wikipedia articles (Araki and Mitamura, 2018), social media data (Ritter et al., 2012; Li et al., 2014; Jain et al., 2016) and literary novels (Sims et al., 2019). Aside from data domain, event extraction paradigms (both datasets and tools) differ along three major axes: (i) event extraction granularity, (ii) event representation, and (iii) event categoriza1 Annotated clinical notes and all code associated with"
2021.eacl-main.258,N19-1423,0,0.0253945,"5 5.1 Experimental Setup Model Details The goal of our evaluation is to identify which alignment technique works best for each domain, as well as analyze whether there are specific kinds of source-target shifts that some techniques are better equipped to handle. We choose a strong BERTbased baseline model with no transfer, and evaluate the performance of each alignment technique when applied to this baseline. VERB: Baseline labeling all verbs as events. DELEX: Fully-delexicalized baseline using POS tag embeddings as features, followed by an MLP. BERT: Single-layer BiLSTM over BERT embeddings (Devlin et al., 2019), followed by an MLP, similar to the best-performing model on LitBank 2968 Model In-Domain Out-of-Domain P R F1 P R F1 VERB DELEX BERT CBERT 58.8 75.0 80.6 79.2 66.5 66.3 86.0 83.3 62.5 70.4 83.2 81.2 49.4 74.4 85.7 85.8 41.4 42.2 55.9 52.9 45.0 53.8 67.6 65.4 BERT-ADA BERT-LIW BERT-DAFT BERT-DAFT-SYN 81.2 81.9 79.1 76.9 86.3 86.6 85.9 80.7 83.7 84.1 82.3 78.7 83.2 86.7 83.9 70.7 60.4 56.0 58.6 56.8 70.0 68.1 69.0 63.0 Table 4: Model performance on domain transfer experiments from news to clinical notes. Model In-Domain Out-of-Domain P R F1 P R F1 VERB DELEX BERT CBERT 58.8 75.0 80.6 79.2 66.5"
2021.eacl-main.258,doddington-etal-2004-automatic,0,0.449166,"s in multiple domains. Despite its importance, building high-performing and generalizable systems for event extraction has remained an elusive goal. One of the major hurdles is that the notion of what counts as an important event is usually task-specific or domain-specific (sometimes both). For example, to build a system that can track a patient’s disease progression from clinical notes, event extractors only need to focus on extracting medical events relevant to that illness. This task/domain specificity has encouraged prior work to focus on specific event types (Grishman and Sundheim, 1996; Doddington et al., 2004; Kim et al., 2008) or domains (Pustejovsky et al., 2003b; Sims et al., 2019). Owing to this narrow focus, supervised event extractors often fail to adapt to new domains or event types (Keith et al., 2017). Unsupervised event extractors that use syntactic rulebased modules (Saur´ı et al., 2005; Chambers et al., 2014), conversely, have a tendency to over-generate by labeling most verbs and nouns as events. In this work, we try to achieve a balance between these extremes by adapting event extractors using unsupervised domain adaptation techniques. We also study the behavior of these techniques u"
2021.eacl-main.258,K15-1012,0,0.0302272,"his variant is two-fold. First, we observe that event annotation is heavily syntax-driven, allowing delexicalized models (i.e., models using POS tags instead of words) to achieve high performance (§5.2). This indicates that infusing more syntactic awareness into embeddings might help performance on the task. Second, syntax might offer an additional basis for generalization, since sentences that look very different lexically, might follow similar syntactic structures. Intuitively, this variant is similar to syntactic relexicalization which has shown success in cross-lingual dependency parsing (Duong et al., 2015). Likelihood-based Instance Weighting: We develop a new instance weighting procedure which uses likelihood scores computed by a language model. Instance selection and instance weighting strategies have frequently been used to perform domain adaptation by correcting for distributional differences (Jiang and Zhai, 2007; Foster et al., 2010; Axelrod et al., 2011; Wang et al., 2017). The main premise is that some samples from out-of-domain data and in-domain data often share some characteristics. Training only on these samples (pruning), or biasing training to focus more on these samples (weightin"
2021.eacl-main.258,D10-1044,0,0.0796013,"Missing"
2021.eacl-main.258,C96-1079,0,0.863914,"r text understanding pipelines in multiple domains. Despite its importance, building high-performing and generalizable systems for event extraction has remained an elusive goal. One of the major hurdles is that the notion of what counts as an important event is usually task-specific or domain-specific (sometimes both). For example, to build a system that can track a patient’s disease progression from clinical notes, event extractors only need to focus on extracting medical events relevant to that illness. This task/domain specificity has encouraged prior work to focus on specific event types (Grishman and Sundheim, 1996; Doddington et al., 2004; Kim et al., 2008) or domains (Pustejovsky et al., 2003b; Sims et al., 2019). Owing to this narrow focus, supervised event extractors often fail to adapt to new domains or event types (Keith et al., 2017). Unsupervised event extractors that use syntactic rulebased modules (Saur´ı et al., 2005; Chambers et al., 2014), conversely, have a tendency to over-generate by labeling most verbs and nouns as events. In this work, we try to achieve a balance between these extremes by adapting event extractors using unsupervised domain adaptation techniques. We also study the behav"
2021.eacl-main.258,D17-1256,0,0.0284763,"ethods (Glorot et al., 2011; Chen et al., 2014), neural SCL (Ziser and Reichart, 2017), adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016) and LM fine-tuning methods (Han and Eisenstein, 2019; Gururangan et al., 2020) have shown success in learning a shared space in which source and target domains are aligned. We propose a new method (LIW) which relies on instance weighting via language model likelihood, and contrast it with adversarial domain adaptation (ADA) and domain adaptive fine-tuning (DAFT). These two techniques have shown promise on sequence labeling tasks (Gui et al., 2017; Han and Eisenstein, 2019; Naik and Ros´e, 2020), and offer an interesting contrast between approaches that jointly perform alignment and task training (ADA) and approaches that perform these steps sequentially (DAFT). Comparing all three techniques also provides us the opportunity to study which methods adapt better to different kinds of shifts between source and target domains (e.g., shifts in vocabulary, syntax, etc.). 3 Dataset Creation To test adaptability of event extraction models, we create a testbed using data from two domains: 1. Clinical Notes: Clinical notes are records documentin"
2021.eacl-main.258,2020.acl-main.740,0,0.0516813,"Missing"
2021.eacl-main.258,D19-1433,0,0.318176,"Association for Computational Linguistics we train a BERT-based event extraction model on English news articles from TimeBank (Pustejovsky et al., 2003b), which is labeled using TimeML, and test its performance on our datasets. To improve this out-of-domain baseline performance, we tackle the problem of covariate shift, i.e., differences between marginal distributions of source (news) and target domains (notes or conversations). We experiment with three marginal alignment techniques: (i) adversarial domain adaptation (ADA) (Ganin and Lempitsky, 2015), (ii) domain-adaptive fine-tuning (DAFT) (Han and Eisenstein, 2019), and (iii) a new instance weighting scheme using language model likelihood scores (LIW). Our results show that DAFT and LIW improve over BERT on both domains, whereas ADA only improves on notes. Across domains, ADA and DAFT perform best on notes and conversations respectively. To probe why some techniques are better at addressing certain source-target domain pairs, we analyze model performance on various types of covariate shifts (e.g., lexical shift, semantic shift). Our analysis uncovers interesting patterns such as varying ability of models to leverage subword morphology to generalize to t"
2021.eacl-main.258,W16-3911,0,0.0173979,"ning data.1 2 Related Work 2.1 Event Extraction Most prior event extraction work has focused on news articles, resulting in the development of several datasets (Onyshkevych et al., 1993; Grishman and Sundheim, 1996; Pustejovsky et al., 2003b; Doddington et al., 2004; Lee et al., 2012; Cybulska and Vossen, 2014; Mitamura et al., 2016). Recently, event extraction has also been explored in other domains such as biology (Wattarujeekrit et al., 2004; Kim et al., 2008, 2009; Berant et al., 2014), Wikipedia articles (Araki and Mitamura, 2018), social media data (Ritter et al., 2012; Li et al., 2014; Jain et al., 2016) and literary novels (Sims et al., 2019). Aside from data domain, event extraction paradigms (both datasets and tools) differ along three major axes: (i) event extraction granularity, (ii) event representation, and (iii) event categoriza1 Annotated clinical notes and all code associated with this work can be found at: https://github.com/ aakanksha19/MedicalEventExtraction. tion (ontology). We briefly describe these axes to contextualize our choice of event paradigm. Event extraction granularity divides extraction paradigms into two types: (i) document-level paradigms that assume that a piece o"
2021.eacl-main.258,P07-1034,0,0.176395,"syntax might offer an additional basis for generalization, since sentences that look very different lexically, might follow similar syntactic structures. Intuitively, this variant is similar to syntactic relexicalization which has shown success in cross-lingual dependency parsing (Duong et al., 2015). Likelihood-based Instance Weighting: We develop a new instance weighting procedure which uses likelihood scores computed by a language model. Instance selection and instance weighting strategies have frequently been used to perform domain adaptation by correcting for distributional differences (Jiang and Zhai, 2007; Foster et al., 2010; Axelrod et al., 2011; Wang et al., 2017). The main premise is that some samples from out-of-domain data and in-domain data often share some characteristics. Training only on these samples (pruning), or biasing training to focus more on these samples (weighting) can produce models that perform better on out-of-domain data. Motivated by this, our instance weighting procedure works as follows. Let St = w1 w2 ...wn be a sentence from the indomain training set. Let O be a language model trained on raw text from the target domain. We first compute the likelihood of sentence St"
2021.eacl-main.258,D17-1163,0,0.0456254,"Missing"
2021.eacl-main.258,W09-1401,0,0.084788,"Missing"
2021.eacl-main.258,D12-1045,0,0.0416298,"Our analysis uncovers interesting patterns such as varying ability of models to leverage subword morphology to generalize to technical terms, and LIW’s performance improvement on long-term state events (e.g., chronic illnesses). Our best models achieve F1 scores of 70.0 and 72.9 on notes and conversations respectively with no training data.1 2 Related Work 2.1 Event Extraction Most prior event extraction work has focused on news articles, resulting in the development of several datasets (Onyshkevych et al., 1993; Grishman and Sundheim, 1996; Pustejovsky et al., 2003b; Doddington et al., 2004; Lee et al., 2012; Cybulska and Vossen, 2014; Mitamura et al., 2016). Recently, event extraction has also been explored in other domains such as biology (Wattarujeekrit et al., 2004; Kim et al., 2008, 2009; Berant et al., 2014), Wikipedia articles (Araki and Mitamura, 2018), social media data (Ritter et al., 2012; Li et al., 2014; Jain et al., 2016) and literary novels (Sims et al., 2019). Aside from data domain, event extraction paradigms (both datasets and tools) differ along three major axes: (i) event extraction granularity, (ii) event representation, and (iii) event categoriza1 Annotated clinical notes an"
2021.eacl-main.258,D14-1214,0,0.0130197,"vely with no training data.1 2 Related Work 2.1 Event Extraction Most prior event extraction work has focused on news articles, resulting in the development of several datasets (Onyshkevych et al., 1993; Grishman and Sundheim, 1996; Pustejovsky et al., 2003b; Doddington et al., 2004; Lee et al., 2012; Cybulska and Vossen, 2014; Mitamura et al., 2016). Recently, event extraction has also been explored in other domains such as biology (Wattarujeekrit et al., 2004; Kim et al., 2008, 2009; Berant et al., 2014), Wikipedia articles (Araki and Mitamura, 2018), social media data (Ritter et al., 2012; Li et al., 2014; Jain et al., 2016) and literary novels (Sims et al., 2019). Aside from data domain, event extraction paradigms (both datasets and tools) differ along three major axes: (i) event extraction granularity, (ii) event representation, and (iii) event categoriza1 Annotated clinical notes and all code associated with this work can be found at: https://github.com/ aakanksha19/MedicalEventExtraction. tion (ontology). We briefly describe these axes to contextualize our choice of event paradigm. Event extraction granularity divides extraction paradigms into two types: (i) document-level paradigms that a"
2021.eacl-main.258,P14-5010,0,0.00438612,"Missing"
2021.eacl-main.258,W15-0809,0,0.0189771,"logy). We briefly describe these axes to contextualize our choice of event paradigm. Event extraction granularity divides extraction paradigms into two types: (i) document-level paradigms that assume that a piece of text refers to a single event (Grishman and Sundheim, 1996), and (ii) sentence-level paradigms that assume that a single sentence describes one or more events. Event representation also divides extraction paradigms into two types: (i) span-based paradigms that represent events by marking text spans that refer to events, called triggers or nuggets (Linguistic Data Consortium, 2005; Mitamura et al., 2015; O’Gorman et al., 2016), and (ii) structured paradigms that represent events by marking text spans and adding additional arguments (e.g., participants, location etc.) to create a structured template (Grishman and Sundheim, 1996). Event categorization divides extraction paradigms into: (i) ontologydriven paradigms that are limited to specific event types (Grishman and Sundheim, 1996; Doddington et al., 2004), and (ii) ontology-free paradigms that do not place type restrictions (Pustejovsky et al., 2003b; Araki and Mitamura, 2018). We use a sentence-level, span-based, ontologyfree event extract"
2021.eacl-main.258,W17-2346,1,0.760444,"xtraction paradigm. Sentence-level extraction suits our domains of interest since notes and conversations tend to discuss multiple events. Span-based and ontology-free extraction allows us to develop adaptable coding guidelines since event arguments and types are usually domain-specific or task-specific. This adaptability sets our work apart from other prior work on medical event extraction such as adverse drug event extraction (Nikfarjam et al., 2015; Sarker and Gonzalez, 2015; Cocos et al., 2017; Henry et al., 2020) and personal event extraction from online support groups (Wen et al., 2013; Naik et al., 2017), which focus on specific event types. Our guidelines draw heavily from the Thyme-TimeML guidelines (Styler IV et al., 2014) used by the Clinical TempEval challenges on event ordering in clinical notes (Bethard et al., 2015, 2016, 2017),2 but also cover event extraction in a novel domain: doctor-patient conversations. 2.2 Unsupervised Domain Adaptation Unsupervised domain adaptation is the task of transferring a model from a source domain to a target domain, using only unlabeled data from the target domain, by aligning source and target distri2964 2 We provide a detailed comparison with this w"
2021.eacl-main.258,2020.acl-main.681,1,0.879606,"Missing"
2021.eacl-main.258,W16-5706,0,0.0603347,"Missing"
2021.eacl-main.258,X93-1013,0,0.189391,", we analyze model performance on various types of covariate shifts (e.g., lexical shift, semantic shift). Our analysis uncovers interesting patterns such as varying ability of models to leverage subword morphology to generalize to technical terms, and LIW’s performance improvement on long-term state events (e.g., chronic illnesses). Our best models achieve F1 scores of 70.0 and 72.9 on notes and conversations respectively with no training data.1 2 Related Work 2.1 Event Extraction Most prior event extraction work has focused on news articles, resulting in the development of several datasets (Onyshkevych et al., 1993; Grishman and Sundheim, 1996; Pustejovsky et al., 2003b; Doddington et al., 2004; Lee et al., 2012; Cybulska and Vossen, 2014; Mitamura et al., 2016). Recently, event extraction has also been explored in other domains such as biology (Wattarujeekrit et al., 2004; Kim et al., 2008, 2009; Berant et al., 2014), Wikipedia articles (Araki and Mitamura, 2018), social media data (Ritter et al., 2012; Li et al., 2014; Jain et al., 2016) and literary novels (Sims et al., 2019). Aside from data domain, event extraction paradigms (both datasets and tools) differ along three major axes: (i) event extract"
2021.eacl-main.258,H05-1088,0,0.2795,"Missing"
2021.eacl-main.258,P19-1353,0,0.348911,"lizable systems for event extraction has remained an elusive goal. One of the major hurdles is that the notion of what counts as an important event is usually task-specific or domain-specific (sometimes both). For example, to build a system that can track a patient’s disease progression from clinical notes, event extractors only need to focus on extracting medical events relevant to that illness. This task/domain specificity has encouraged prior work to focus on specific event types (Grishman and Sundheim, 1996; Doddington et al., 2004; Kim et al., 2008) or domains (Pustejovsky et al., 2003b; Sims et al., 2019). Owing to this narrow focus, supervised event extractors often fail to adapt to new domains or event types (Keith et al., 2017). Unsupervised event extractors that use syntactic rulebased modules (Saur´ı et al., 2005; Chambers et al., 2014), conversely, have a tendency to over-generate by labeling most verbs and nouns as events. In this work, we try to achieve a balance between these extremes by adapting event extractors using unsupervised domain adaptation techniques. We also study the behavior of these techniques under various types of linguistic shifts (e.g., lexical shift, semantic shift)"
2021.eacl-main.258,E12-2021,0,0.0204252,"Missing"
2021.eacl-main.258,D17-1155,0,0.0207588,"sentences that look very different lexically, might follow similar syntactic structures. Intuitively, this variant is similar to syntactic relexicalization which has shown success in cross-lingual dependency parsing (Duong et al., 2015). Likelihood-based Instance Weighting: We develop a new instance weighting procedure which uses likelihood scores computed by a language model. Instance selection and instance weighting strategies have frequently been used to perform domain adaptation by correcting for distributional differences (Jiang and Zhai, 2007; Foster et al., 2010; Axelrod et al., 2011; Wang et al., 2017). The main premise is that some samples from out-of-domain data and in-domain data often share some characteristics. Training only on these samples (pruning), or biasing training to focus more on these samples (weighting) can produce models that perform better on out-of-domain data. Motivated by this, our instance weighting procedure works as follows. Let St = w1 w2 ...wn be a sentence from the indomain training set. Let O be a language model trained on raw text from the target domain. We first compute the likelihood of sentence St under O as Lt = PO (w1 )Πni=2 PO (wi |w1 ...wi−1 ), where PO i"
2021.eacl-main.258,P13-2145,1,0.849917,"Missing"
2021.eacl-main.258,K17-1040,0,0.019151,"detailed comparison with this work in §3.1. butions. Early approaches such as structural correspondence learning (SCL) (Blitzer et al., 2006, 2007) tried to solve this by mapping source and target examples into a shared pivot feature space, where pivot features are selected to be features that behave the same way for discriminative learning in both domains (e.g., sentiment terms such as amazing and great show similar behavior for sentiment analysis across domains). With advances in neural representation learning, autoencoder-based methods (Glorot et al., 2011; Chen et al., 2014), neural SCL (Ziser and Reichart, 2017), adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016) and LM fine-tuning methods (Han and Eisenstein, 2019; Gururangan et al., 2020) have shown success in learning a shared space in which source and target domains are aligned. We propose a new method (LIW) which relies on instance weighting via language model likelihood, and contrast it with adversarial domain adaptation (ADA) and domain adaptive fine-tuning (DAFT). These two techniques have shown promise on sequence labeling tasks (Gui et al., 2017; Han and Eisenstein, 2019; Naik and Ros´e, 2020), and offer an intere"
2021.eacl-main.7,D19-1565,0,0.0395864,"Missing"
2021.eacl-main.7,2020.acl-main.632,0,0.0639056,"Missing"
2021.eacl-main.7,N19-1423,0,0.0771146,"ions of a token as input and passes it through a BiGRU layer followed by Self Attention. The outputs from BERT, BiGRU and self-attention are then concatenated to form the output. Max-pooling over this output yields the corresponding utterance embedding. This utterance representation is passed through a uni-directional GRU followed by Masked-Self-Attention and fusion to yield the contextualised utterance embedding. is composed of tokens [w0 , w1 , ..., wK ] represented by their corresponding embeddings [e(w0 ), e(w1 ), ..., e(wK )]. In R ES P ER, we obtain these using a pre-trained BERT model (Devlin et al., 2019). We pass these contextualised word representations through a bidirectional GRU to ob− → ← − tain the forward hk and backward hk hidden states of each word, before passing them into a SelfAttention layer. This gives us the corresponding −−→ ←−− attention outputs, ahk and ahk as described below.  − → −−→ hk = GRU e (wk ) , hk−1  ← − ←−− hk = GRU e (wk ) , hk+1 −−→ − → ahk = Self Attention(hk ) ←−− ← − ahk = Self Attention(hk ) Finally, we concatenate the contextualised word embedding with the GRU hidden states and Attention outputs in the fusion layer to obtain the final representation of t"
2021.eacl-main.7,P18-1237,0,0.018394,"and the design of a framework to measure the impact of resistance (Tormala, 2008). However, these works have mostly relied on qualitative methods, unlike ours, which adopts a data-driven approach. We propose a generalised framework to characterise resisting strategies and employ state-of-the-art neural models to infer them automatically. Thus our work can be considered complementary to past research. The closest semblance to our work in NLP literature ties in with argumentation, be it essays (Carlile et al., 2018), debates (Cano-Basave and He, 2016), or discussions on social media platforms (Al-Khatib et al., 2018; Zeng et al., 2020). Such works have revolved mostly on analysing argumentative strategies and their effect on others. Recently, Al Khatib et al. (2020) demonstrated that incorporating the personality traits of the resis3 Framework In this section, we describe the datasets, the resisting strategies employed, and the annotation framework to instantiate the strategies. 3.1 Dataset Employed We choose persuasion-oriented conversations, rather than essays or advertisements (Yang et al., 2019), since we can observe how the participants respond to the persuasion attempts in real-time. To that end, w"
2021.eacl-main.7,P18-1128,0,0.0135633,"sting strategies employed by EE and (iii) both the persuasion and resisting strategies. Likewise, for CB, we encode the resisting strategies of only (i) the buyer (BU) (ii) the seller (SE) (iii) both. These experiments would enable us to investigate which party has a greater influence on conversation success. Weighted F1 Scores are calculated by taking the average of the F1 scores for each label weighted by the number of true instances for each label. We estimate the statistical significance using the paired bootstrapped test of Berg-Kirkpatrick et al. (2012), due to the small number of data (Dror et al., 2018). 84 0.01 0 HS 0.49 0.4 0 0 0.05 0.03 0.01 0.03 CA 0.58 0 0.26 0.04 0.06 0.01 0.04 0 PC 0.46 0 0.03 0.39 0.03 0.03 0.04 0.01 SD 0.22 0 0.01 0.01 0.74 0.01 0 0 II 0.28 0.01 0.05 0.05 0.08 0.51 0 0.04 SP 0.35 0.01 0.01 0.07 0.05 0.03 0.47 0.01 SA 0.15 0.05 0.03 0.01 0 0.01 0.12 0.63 0.6 0.4 0.2 0.02 0.02 0.02 0.04 0.04 0.04 0.01 HS 0.1 0.61 0.06 0.01 0.02 0.18 0.01 0.01 CA 0.11 0.06 0.6 0.02 0.1 0.01 0.08 0.02 PC 0.11 0 0.04 0.62 0.12 0.02 0.06 0.03 SD 0.11 0.01 0.06 0.06 0.65 0.01 0.07 0.04 II 0.07 0.05 0 0.01 0.01 0.84 0.01 0 SP 0.11 0 0.03 0.02 0.05 0.03 0.74 0.01 SA 0.04 0 0.03 0.05 0.13 0.0"
2021.eacl-main.7,N18-1094,0,0.0654367,"Missing"
2021.eacl-main.7,D12-1091,0,0.0105048,"rategies of ER as identified by Wang et al. (2019), (ii) the resisting strategies employed by EE and (iii) both the persuasion and resisting strategies. Likewise, for CB, we encode the resisting strategies of only (i) the buyer (BU) (ii) the seller (SE) (iii) both. These experiments would enable us to investigate which party has a greater influence on conversation success. Weighted F1 Scores are calculated by taking the average of the F1 scores for each label weighted by the number of true instances for each label. We estimate the statistical significance using the paired bootstrapped test of Berg-Kirkpatrick et al. (2012), due to the small number of data (Dror et al., 2018). 84 0.01 0 HS 0.49 0.4 0 0 0.05 0.03 0.01 0.03 CA 0.58 0 0.26 0.04 0.06 0.01 0.04 0 PC 0.46 0 0.03 0.39 0.03 0.03 0.04 0.01 SD 0.22 0 0.01 0.01 0.74 0.01 0 0 II 0.28 0.01 0.05 0.05 0.08 0.51 0 0.04 SP 0.35 0.01 0.01 0.07 0.05 0.03 0.47 0.01 SA 0.15 0.05 0.03 0.01 0 0.01 0.12 0.63 0.6 0.4 0.2 0.02 0.02 0.02 0.04 0.04 0.04 0.01 HS 0.1 0.61 0.06 0.01 0.02 0.18 0.01 0.01 CA 0.11 0.06 0.6 0.02 0.1 0.01 0.08 0.02 PC 0.11 0 0.04 0.62 0.12 0.02 0.06 0.03 SD 0.11 0.01 0.06 0.06 0.65 0.01 0.07 0.04 II 0.07 0.05 0 0.01 0.01 0.84 0.01 0 SP 0.11 0 0.03"
2021.eacl-main.7,2020.emnlp-main.605,1,0.800868,"ical neural network architecture, similar to Jiao et al. (2019), to infer the corresponding resisting strategy. The architecture leverages the previous conversational context in addition to the current contextualised utterance embedding. Our choice is motivated by the recent successes of hierarchical sequence labelling frameworks in achieving state-of-the-art performance on several dialogueoriented tasks. Some myriad examples include emotion recognition (Majumder et al., 2019; Jiao et al., 2019), dialogue act classification (Chen et al., 2018; Raheja and Tetreault, 2019), face act prediction (Dutt et al., 2020), open domain chit-chat (Zhang et al., 2018; Kumar et al., 2020) and the like. We hereby adopt this as the foundation architecture for our work and refer to our instantiation of the architecture as R ES P ER. Architecture of R ES P ER: An utterance uj Table 3: Examples of annotation snippets for the Persuasion (P4G) and Negotiation (CB). The utterances of the EE and the SE are highlighted in cyan. Some strategies are shortened, like Info Inquiry, and Per Choice for Information Inquiry and Personal Choice. The datasets cover two distinct persuasion scenarios and also illustrate the rights and o"
2021.eacl-main.7,N16-1166,0,0.0669655,"Missing"
2021.eacl-main.7,P18-1058,0,0.0262846,"on, 2003), the use of psychological metrics to predict resistance (San Jos´e, 2019; Ahluwalia, 2000), and the design of a framework to measure the impact of resistance (Tormala, 2008). However, these works have mostly relied on qualitative methods, unlike ours, which adopts a data-driven approach. We propose a generalised framework to characterise resisting strategies and employ state-of-the-art neural models to infer them automatically. Thus our work can be considered complementary to past research. The closest semblance to our work in NLP literature ties in with argumentation, be it essays (Carlile et al., 2018), debates (Cano-Basave and He, 2016), or discussions on social media platforms (Al-Khatib et al., 2018; Zeng et al., 2020). Such works have revolved mostly on analysing argumentative strategies and their effect on others. Recently, Al Khatib et al. (2020) demonstrated that incorporating the personality traits of the resis3 Framework In this section, we describe the datasets, the resisting strategies employed, and the annotation framework to instantiate the strategies. 3.1 Dataset Employed We choose persuasion-oriented conversations, rather than essays or advertisements (Yang et al., 2019), sin"
2021.eacl-main.7,D18-1256,0,0.375037,"uasion strategies to change a person’s view or achieve a desired outcome finds several real-world applications, such as in election campaigns (Knobloch-Westerwick and Meng, 2009; Bartels, 2006), advertisements (Speck and Elliott, 1997), and mediation (Cooley, 1993). Consequently, several seminal NLP research have focused on operationalising and automatically identifying persuasion strategies (Wang et al., 2019), propaganda techniques (Da San Martino et al., 2019), and negotiation tactics (Zhou et al., 2019), as well as the impact of such strategies on the outcome of a task (Yang et al., 2019; He et al., 2018; Joshi et al., 2021). However, there is still a dearth of research from a computational linguistic perspective investigating resisting strategies to foil persuasion. Resisting strategies have been widely discussed in literature from various aspects such as marketing (Heath et al., 2017), cognitive psychology (Zuwerink Jacks and Cameron, 2003), and political communication (Fransen et al., 2015b) . Some notable works include the identification and motivation of commonly-used resisting strategies (Fransen et al., 2015a; Zuwerink Jacks and Cameron, 2003), the use of psychological metrics to predi"
2021.eacl-main.7,N19-1373,0,0.0261464,"sational history. To that end, we adopt a hierarchical neural network architecture, similar to Jiao et al. (2019), to infer the corresponding resisting strategy. The architecture leverages the previous conversational context in addition to the current contextualised utterance embedding. Our choice is motivated by the recent successes of hierarchical sequence labelling frameworks in achieving state-of-the-art performance on several dialogueoriented tasks. Some myriad examples include emotion recognition (Majumder et al., 2019; Jiao et al., 2019), dialogue act classification (Chen et al., 2018; Raheja and Tetreault, 2019), face act prediction (Dutt et al., 2020), open domain chit-chat (Zhang et al., 2018; Kumar et al., 2020) and the like. We hereby adopt this as the foundation architecture for our work and refer to our instantiation of the architecture as R ES P ER. Architecture of R ES P ER: An utterance uj Table 3: Examples of annotation snippets for the Persuasion (P4G) and Negotiation (CB). The utterances of the EE and the SE are highlighted in cyan. Some strategies are shortened, like Info Inquiry, and Per Choice for Information Inquiry and Personal Choice. The datasets cover two distinct persuasion scena"
2021.eacl-main.7,N19-1037,0,0.132684,"s and how they can be leveraged to determine the dialogue’s outcome. Source Derogation - 4.1 .. No Strategy Resisting Strategy prediction We model the task of identifying resisting strategies as a sequence labelling task. We assign each utterance in the dialogues with a label representing either one of the seven resisting strategies or Not-A-Strategy. Since the resisting strategies, by definition, occur in response to the persuasion attempts, our model architecture needs to be cognizant of the conversational history. To that end, we adopt a hierarchical neural network architecture, similar to Jiao et al. (2019), to infer the corresponding resisting strategy. The architecture leverages the previous conversational context in addition to the current contextualised utterance embedding. Our choice is motivated by the recent successes of hierarchical sequence labelling frameworks in achieving state-of-the-art performance on several dialogueoriented tasks. Some myriad examples include emotion recognition (Majumder et al., 2019; Jiao et al., 2019), dialogue act classification (Chen et al., 2018; Raheja and Tetreault, 2019), face act prediction (Dutt et al., 2020), open domain chit-chat (Zhang et al., 2018;"
2021.eacl-main.7,D14-1181,0,0.00318724,"00 Experiments Table 5: Here we describe the search-space of all the hyper-parameters used in our experiments and describe the search space we used to find the hyper-parameters. dh1 , dh2 represents the hidden dimensions of the Utterance GRU and the Conversation GRU. In this section, we describe the baselines and evaluation metrics. We present the experimental details of our model in Table 5. 5.1 Search space Baselines Resisting strategy prediction: We experiment with standard neural baselines for text classification, which have also been used in classifying persuasion strategies, namely CNN (Kim, 2014; Wang et al., 2019) and BiGRU (Yang et al., 2019). To ensure a fair comparison, we introduce pre-trained BERT-embeddings (Devlin et al., 2019) as input to the baselines, henceforth denoted as BERT-CNN and BERT-BiGRU. Furthermore, to inspect the impact of conversational history, we remove the conversational GRU from R ES P ER such that the utterance embedding e(uj ) is directly used for prediction. We refer to this architecture as BERT-BiGRUsf, since it employs self-attention(s) and fusion (f) on top of BERT-BiGRU. Finally, we experiment with the best performing HiGRU-sf model of Jiao et al. ("
2021.eacl-main.7,2020.lrec-1.94,1,0.734274,", to infer the corresponding resisting strategy. The architecture leverages the previous conversational context in addition to the current contextualised utterance embedding. Our choice is motivated by the recent successes of hierarchical sequence labelling frameworks in achieving state-of-the-art performance on several dialogueoriented tasks. Some myriad examples include emotion recognition (Majumder et al., 2019; Jiao et al., 2019), dialogue act classification (Chen et al., 2018; Raheja and Tetreault, 2019), face act prediction (Dutt et al., 2020), open domain chit-chat (Zhang et al., 2018; Kumar et al., 2020) and the like. We hereby adopt this as the foundation architecture for our work and refer to our instantiation of the architecture as R ES P ER. Architecture of R ES P ER: An utterance uj Table 3: Examples of annotation snippets for the Persuasion (P4G) and Negotiation (CB). The utterances of the EE and the SE are highlighted in cyan. Some strategies are shortened, like Info Inquiry, and Per Choice for Information Inquiry and Personal Choice. The datasets cover two distinct persuasion scenarios and also illustrate the rights and obligations shown by the participants. For example, in P4G, EE co"
2021.eacl-main.7,P19-1566,0,0.115583,"of this work publicly available at https://github.com/americast/ resper. 1 Introduction Persuasion is pervasive in everyday human interactions. People are often exposed to scenarios that challenge their existing beliefs and opinions, such as medical advice, election campaigns, and advertisements (Knobloch-Westerwick and Meng, 2009; Bartels, 2006; Speck and Elliott, 1997). Of late, huge strides have been taken by the Computational Linguistics community to advance research in persuasion. Some seminal works include identifying persuasive strategies in text (Yang et al., 2019) and conversations (Wang et al., 2019), investigating the ∗ denotes equal contribution 78 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 78–90 April 19 - 23, 2021. ©2021 Association for Computational Linguistics strategies employed by the persuader. We also observe that the buyer’s strategies are more influential in negotiating the final price. Our findings highlight the asymmetric nature of power roles arising in non-collaborative dialogue scenarios and form motivation for this work. 2 tor was influential in determining their resistance to persuasion. Such an obs"
2021.eacl-main.7,W19-4519,0,0.0436754,"Missing"
2021.eacl-main.7,N19-1364,0,0.26842,"We also make the code and the dataset of this work publicly available at https://github.com/americast/ resper. 1 Introduction Persuasion is pervasive in everyday human interactions. People are often exposed to scenarios that challenge their existing beliefs and opinions, such as medical advice, election campaigns, and advertisements (Knobloch-Westerwick and Meng, 2009; Bartels, 2006; Speck and Elliott, 1997). Of late, huge strides have been taken by the Computational Linguistics community to advance research in persuasion. Some seminal works include identifying persuasive strategies in text (Yang et al., 2019) and conversations (Wang et al., 2019), investigating the ∗ denotes equal contribution 78 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 78–90 April 19 - 23, 2021. ©2021 Association for Computational Linguistics strategies employed by the persuader. We also observe that the buyer’s strategies are more influential in negotiating the final price. Our findings highlight the asymmetric nature of power roles arising in non-collaborative dialogue scenarios and form motivation for this work. 2 tor was influential in determining their"
2021.eacl-main.7,P18-1205,0,0.0304561,"o Jiao et al. (2019), to infer the corresponding resisting strategy. The architecture leverages the previous conversational context in addition to the current contextualised utterance embedding. Our choice is motivated by the recent successes of hierarchical sequence labelling frameworks in achieving state-of-the-art performance on several dialogueoriented tasks. Some myriad examples include emotion recognition (Majumder et al., 2019; Jiao et al., 2019), dialogue act classification (Chen et al., 2018; Raheja and Tetreault, 2019), face act prediction (Dutt et al., 2020), open domain chit-chat (Zhang et al., 2018; Kumar et al., 2020) and the like. We hereby adopt this as the foundation architecture for our work and refer to our instantiation of the architecture as R ES P ER. Architecture of R ES P ER: An utterance uj Table 3: Examples of annotation snippets for the Persuasion (P4G) and Negotiation (CB). The utterances of the EE and the SE are highlighted in cyan. Some strategies are shortened, like Info Inquiry, and Per Choice for Information Inquiry and Personal Choice. The datasets cover two distinct persuasion scenarios and also illustrate the rights and obligations shown by the participants. For e"
2021.eacl-main.7,W19-5943,0,0.12426,"parties is imperative to get a complete picture of persuasive conversations. Related Works The use of persuasion strategies to change a person’s view or achieve a desired outcome finds several real-world applications, such as in election campaigns (Knobloch-Westerwick and Meng, 2009; Bartels, 2006), advertisements (Speck and Elliott, 1997), and mediation (Cooley, 1993). Consequently, several seminal NLP research have focused on operationalising and automatically identifying persuasion strategies (Wang et al., 2019), propaganda techniques (Da San Martino et al., 2019), and negotiation tactics (Zhou et al., 2019), as well as the impact of such strategies on the outcome of a task (Yang et al., 2019; He et al., 2018; Joshi et al., 2021). However, there is still a dearth of research from a computational linguistic perspective investigating resisting strategies to foil persuasion. Resisting strategies have been widely discussed in literature from various aspects such as marketing (Heath et al., 2017), cognitive psychology (Zuwerink Jacks and Cameron, 2003), and political communication (Fransen et al., 2015b) . Some notable works include the identification and motivation of commonly-used resisting strategi"
2021.naacl-main.325,2020.lt4gov-1.1,1,0.75554,"hallenges for basic NLP research that can inform many other applications beyond this case study. 4 Case Study: NLP for Disability Review Of course, these studies are far from the only We illustrate our Translational NLP framework us- examples of Translational NLP research. Many ing our recent line of research on developing NLP studies tackle translational questions, from domain tools to assist US Social Security Administration adaptation (shifts in Data Characteristics) and low(SSA) officials in reviewing applications for dis- resource learning (limited Available Resources), ability benefits (Desmet et al., 2020). The goal of and the growing NLP literature in domain-specific this effort was to help identify relevant pieces of venues such as medical research, law, finance, and medical evidence for making decisions about dis- more involves all aspects of the translational proability benefits, analyzing vast quantities of medi- cess. Rather, this case study is simply one illustracal records collected during the review process. tion of how an explicitly translational perspective The stakeholders in this setting included: NLP in study design can identify and connect broad opresearchers (interested in devel"
2021.naacl-main.325,N19-1423,0,0.0323144,"Missing"
2021.naacl-main.325,D19-1222,0,0.0115585,"ating innovations in basic ceeds while another fails, and to develop general, NLP methods to successful applications remains a reusable processes to facilitate more (and easier) difficult task in which failure points often appear translation between basic NLP advances and reallate in the development process, delaying or pre- world application settings. Much NLP research venting potential impact in research and industry. already includes translational insights, but often Application challenges range widely, from changes considers them properties of a specific application in data distributions (Elsahar and Gallé, 2019) to rather than generalizable findings that can advance computational bottlenecks (Desai et al., 2020) and the field. This paper illustrates why general prinintegration with domain expertise (Rahman et al., ciples of the translational process enhance mutual 2020). When unanticipated, such challenges can exchange between linguistic inquiry, model develbe fatal to applications of new NLP methodologies, opment, and application research (illustrated in Figleaving exciting innovations with minimal practical ure 1), and are key drivers of NLP advances. 4125 Proceedings of the 2021 Conference of the"
2021.naacl-main.325,W18-2501,0,0.0201535,"hen the overall goal is factorized into multiple NLP tasks, optimization often involves Ex. 1: Evidence identification, model audits. 4131 Ex. 2: Criteria visualization, model audits. Application Engineering Last but not least, the translational process must also be concerned with the implementation of NLP solutions, both in terms of the specific technologies used and how they can fit in to broader information processing pipelines. The development of general-purpose NLP architectures such as the Stanford CoreNLP Toolkit (Manning et al., 2014), spaCy (Honnibal and Montani, 2017), and AllenNLP (Gardner et al., 2018), as well as more targeted architectures such as the clinical NLP framework presented by Wen et al. (2019), provide well-engineered frameworks for implementing new technologies in a way that is easy for others to both adopt and adapt for use in their own pipelines. Standardized data exchange frameworks such as UIMA (Ferrucci and Lally, 2004) and JSON make implementations more modular and easier to wire together. Leveraging tools and frameworks like these, together with good software design principles, makes NLP tools both easier to apply downstream and easier for other researchers to incorpora"
2021.naacl-main.325,M95-1001,0,0.751413,"Missing"
2021.naacl-main.325,2020.emnlp-main.473,0,0.0138643,"ifies the sublanguage(s) of interest (Grchecklist, with key questions for each. Items are ishman and Kittredge, 1986), which determine the loosely ordered from initial design to application deavailability and development of appropriate NLP tails, but should be regularly revisited in a feedback tools (Grishman, 2001). Corporate disclosures, filoop between application stakeholders. nancial news reports, and tweets all require different processing strategies (Xing et al., 2018), as do re-evaluated via a feedback loop between the ap- tweets written by different communities (Blodgett et al., 2016; Groenwold et al., 2020). plication stakeholders. While many of these items Ex. 1: clinical texts, lab reports. will be familiar to NLP researchers, each represents Ex. 2: clinical texts, legal guidelines. potential points of failure in translation. Designing the research process with these variables in mind Task Paradigms To address the overall goal will produce basic innovations that are more eas- with an NLP solution, it must be formulated in ily adopted for application and more directly con- terms of one or more well-defined NLP problems. nected to the challenges of real-world use cases. Many real-world applicati"
2021.naacl-main.325,P14-5010,0,0.00731523,"basic research on interpretability within the target domain. tectures. When the overall goal is factorized into multiple NLP tasks, optimization often involves Ex. 1: Evidence identification, model audits. 4131 Ex. 2: Criteria visualization, model audits. Application Engineering Last but not least, the translational process must also be concerned with the implementation of NLP solutions, both in terms of the specific technologies used and how they can fit in to broader information processing pipelines. The development of general-purpose NLP architectures such as the Stanford CoreNLP Toolkit (Manning et al., 2014), spaCy (Honnibal and Montani, 2017), and AllenNLP (Gardner et al., 2018), as well as more targeted architectures such as the clinical NLP framework presented by Wen et al. (2019), provide well-engineered frameworks for implementing new technologies in a way that is easy for others to both adopt and adapt for use in their own pipelines. Standardized data exchange frameworks such as UIMA (Ferrucci and Lally, 2004) and JSON make implementations more modular and easier to wire together. Leveraging tools and frameworks like these, together with good software design principles, makes NLP tools both"
2021.naacl-main.325,W19-5929,1,0.875869,"sform advances in basic knowledge (biological or clinical) to applications to human health (Butte, 2008; Rubio et al., 2010). Translational research is a distinct discipline bridging basic science and applications (Pober et al., 2001; Reis et al., 2010). We adopt the term Translational NLP to describe research bridging the gap between basic and applied NLP research, and aiming to understand the processes by which each informs the other. Section 4 presents one in-depth example; other salient examples include comparing the efficacy of domain adaptation methods for different application domains (Naik et al., 2019) and developing reusable software for processing specific text genres (Neumann et al., 2019). Translational research occupies a middle ground in the timeframe and complexity of solutions: it develops processes to rapidly and effectively integrate new innovations into applications to address emerging needs, and facilitates integration between pipelines of NLP tools. A long history of distinguishing between basic and applied research (Bush, 1945; Shneiderman, 2016) has noted that these terms are often relative; one researcher’s basic study is the application of another’s theory. In practice, bas"
2021.naacl-main.325,W19-5034,0,0.0216283,"h (Butte, 2008; Rubio et al., 2010). Translational research is a distinct discipline bridging basic science and applications (Pober et al., 2001; Reis et al., 2010). We adopt the term Translational NLP to describe research bridging the gap between basic and applied NLP research, and aiming to understand the processes by which each informs the other. Section 4 presents one in-depth example; other salient examples include comparing the efficacy of domain adaptation methods for different application domains (Naik et al., 2019) and developing reusable software for processing specific text genres (Neumann et al., 2019). Translational research occupies a middle ground in the timeframe and complexity of solutions: it develops processes to rapidly and effectively integrate new innovations into applications to address emerging needs, and facilitates integration between pipelines of NLP tools. A long history of distinguishing between basic and applied research (Bush, 1945; Shneiderman, 2016) has noted that these terms are often relative; one researcher’s basic study is the application of another’s theory. In practice, basic and applied research in NLP are endpoints of a spectrum, rather than discrete categories."
2021.naacl-main.325,D19-3015,1,0.873985,"ng of where relevant information use NLP technologies to address their own inforcan be found (e.g., document sources (Fisher et al., mation needs according to the priorities of their 2016) and sections (Afzal et al., 2018)), which can organizations. These organizational priorities may help identify new types of language for basic re- conflict with existing modeling assumptions, highsearchers to study (Burstein, 2009; Crossley et al., lighting new opportunities for basic research to 2014) and new challenges such as sparse complex expand model capabilities. For example, Shah et al. information (Newman-Griffis and Fosler-Lussier, (2019) highlight the conceptual gap between pre2019) and higher-level structure in complex docu- dictive model performance in medicine and clinical ments (Naik et al., 2019). In addition, the context utility to call for new research on utility-driven that domain experts offer in terms of the needs of model evaluation. Spector et al. (2012) make a simtarget applications feeds back into evaluation meth- ilar point about Google’s mission-driven research ods in the basic research setting (Graham, 2015). identifying unseen gaps for new basic research. The role of the Translational NLP researcher SMEs are"
2021.naacl-main.325,W18-3026,1,0.871701,"Missing"
2021.naacl-main.325,W18-2301,1,0.797906,", SMEs, and the end users to determine how to incorporate the new innovation into the existing solution. methods); subject matter experts in disability and rehabilitation; and SSA end users (limited computing, large data but strictly controlled, overall priorities of efficiency and accuracy). The Translational NLP checklist for this setting is shown in Table 1. This combination of factors has led to several translational studies, including: • Newman-Griffis et al. (2018) developed a lowresource entity embedding method for domains with minimal knowledge sources (lack of Available Resources). • Newman-Griffis and Zirikly (2018) analyzed the data size and representativeness tradeoff for information extraction in domains lacking large corpora (Available Resources). • Newman-Griffis and Fosler-Lussier (2019) developed a flexible method for identifying sparse health information that is syntactically complex (challenging Data Characteristics). • Newman-Griffis and Fosler-Lussier (2021) compared the Task Paradigms of classification and candidate selection paradigms for medical coding in a new domain. While these studies do not systematically explore Evaluation, Interpretation, or Application Engineering, they illustrate h"
2021.naacl-main.325,D14-1162,0,0.0913968,"hroughput requirements (Nityasya et al., 2020). methods may be constantly revised (Grishman and An application environment with a high maxi- Sundheim, 1995). Critically for the translational mum resource load but low median availability researcher, some metrics may be preferred over is amenable to batch processing architectures or others (e.g., precision over recall), and standardapproaches with high pretraining cost and low ized evaluation metrics may not reflect the goals test-time cost. Pretrained word representstions and needs of applications (Friedman and Hripcsak, (Mikolov et al., 2013; Pennington et al., 2014) and 1998). Improvements on standardized evaluation language models (Peters et al., 2018; Devlin et al., metrics (such as increased AUC) may even obscure 2019) are one example of fundamental technolo- degradations in application-relevant performance gies that address such a need. Throughput require- measures (such as decreased process efficiency). ments, i.e., how much language input needs to be Translational researchers thus have the opportunity analyzed in a fixed amount of time, often require to work with NLP experts and SMEs to identify engineering optimization for specific environments or"
2021.naacl-main.325,N18-1202,0,0.0181564,"nd An application environment with a high maxi- Sundheim, 1995). Critically for the translational mum resource load but low median availability researcher, some metrics may be preferred over is amenable to batch processing architectures or others (e.g., precision over recall), and standardapproaches with high pretraining cost and low ized evaluation metrics may not reflect the goals test-time cost. Pretrained word representstions and needs of applications (Friedman and Hripcsak, (Mikolov et al., 2013; Pennington et al., 2014) and 1998). Improvements on standardized evaluation language models (Peters et al., 2018; Devlin et al., metrics (such as increased AUC) may even obscure 2019) are one example of fundamental technolo- degradations in application-relevant performance gies that address such a need. Throughput require- measures (such as decreased process efficiency). ments, i.e., how much language input needs to be Translational researchers thus have the opportunity analyzed in a fixed amount of time, often require to work with NLP experts and SMEs to identify engineering optimization for specific environments or develop metrics that capture both the effective(Afshar et al., 2019), but the need for"
2021.naacl-main.325,D16-1264,0,0.018956,"le. The need to index and monetize vast quantities of textual information led to an explosion in information retrieval research, and the NLP field and ever-growing web data continue to co-develop. In a more recent example, IBM identified automated question answering (QA) as a new business opportunity in a high-information world, and developed the Watson project (Ferrucci et al., 2010). Watson’s early successes catapulted QA into the center of NLP research, where it has continued to drive both novel technology development and benchmark evaluation datasets used in hundreds of basic NLP studies (Rajpurkar et al., 2016). These and other examples illustrate the key role that application needs have played in driving inno1 While workflows will vary for different classes of NLP vation in NLP research. This reflects not only the problems, dialogue between NLP experts and subject matter history of the field but the role that integrating basic experts is at the heart of developing almost all NLP solutions. 4127 tools in specific use cases. Implementing these tools and processes, and engaging with basic NLP experts and SMEs in their use, is the role of the Translational NLP scientist. Although every use case has uni"
2021.naacl-main.325,J85-1001,0,0.694176,"(2017)). Given this gap, we define Translational NLP as the development of theories, tools, and processes to enable the direct application of advanced NLP The NLP field has always lain at the nexus of basic and applied research. Application needs have driven some of the most fundamental developments in the field, leading to explosions in basic research in new topics and on long-standing challenges. The need to automatically translate Russian scientific papers in the early years of the Cold War led to some of the earliest NLP research, creating the still-thriving field of machine translation (Slocum, 1985). Machine translation has since helped drive many significant advances in basic NLP research, from the adoption of statistical models in the 1980s (Dorr et al., 1999) to neural sequence-to-sequence modeling (Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). Similarly, the rapid growth of the World Wide Web in the 1990s created an acute need for technologies to search the growing sea of information, leading to the development of NLP-based search engines such as Lycos (Mauldin, 1997), followed by PageRank (Page et al., 1999) and the growth of Google. The need to index and"
2021.naacl-main.325,2020.acl-main.686,0,0.0363696,"Missing"
2021.naacl-main.325,D19-1002,0,0.0182215,"comes. (Kingma and Welling, 2014) and the Transformer Ex. 2: F-1, billing rates. architecture (Vaswani et al., 2017). Interpretation Interpretability and analysis of Ex. 1: UMLS, high GPU compute. NLP and other machine learning systems has been Ex. 2: UMLS, guideline criteria, low compute. the focus of extensive research in recent years (Gilpin et al., 2018; Belinkov and Glass, 2019), NLP Technologies The interaction between task with debate over what constitutes an interpretation paradigms, data characteristics, and available resources helps to determine what types of implemen- (Rudin, 2019; Wiegreffe and Pinter, 2019) and detations are appropriate to the task. Implementa- velopment of broad-coverage software packages tions can be further broken down into representa- for ease of use (Nori et al., 2019). For the translational researcher, the first step is to engage with tion technologies, for mathematically representing the language units to be analyzed; modeling ar- SMEs to determine what constitutes an acceptable interpretation of an NLP system’s output in the apchitectures, for capturing regularities within that language; and optimization strategies (when us- plication domain (which may be subject to spec"
2021.nuse-1.2,N19-1423,0,0.0195828,"and Strube, 2016). We compare our approach against different stateof-the-art approaches used for coreference resolution in the past. Along with BookNLP’s approach, we consider the Stanford CoreNLP deterministic coreference model (CoreNLP (dcoref); Raghunathan et al., 2010; Recasens et al., 2013; Lee et al., 2011) and the CoreNLP statistical model (CoreNLP (coref); Clark and Manning, 2015) as traditional baselines. As a neural baseline, we evaluate the more recently proposed BERT-base model (Joshi et al., 2019), which replaces the original GloVe embeddings (Pennington et al., 2014) with BERT (Devlin et al., 2019) in Lee et al. (2017)’s coreference resolution approach. Micro-averaged results across the 10 annotated stories are shown in Table 4. The FanfictionNLP approach is SpanBERT-base fine-tuned on LitBank, with the post-hoc removal of non-person and plural mentions and clusters (as described in Section 3.1). Note that these results are without the quote pronoun resolution module described in Section 3.3. Traditional approaches like BookNLP and CoreNLP (dcoref, coref) perform significantly worse than the neural models, especially on recall. Neural models that are further fine-tuned on LitBank (OL) o"
2021.nuse-1.2,I13-1171,0,0.0243489,"tification and coreference, as well as the attribution of quotes and narration to those characters. Additionally, the pipeline contains a novel approach to character coreference that uses knowledge from quote attribution to resolve pronouns within quotes. For each module, we evaluate the effectiveness of various approaches on 10 annotated fanfiction stories. This pipeline outperforms tools developed for formal fiction on the tasks of character coreference and quote attribution. 1 Introduction A growing number of natural language processing tools and approaches have been developed for fiction (Agarwal et al., 2013; Bamman et al., 2014; Iyyer et al., 2016; Sims et al., 2019). These tools generally focus on published literary works, such as collections of novels. We present an NLP pipeline for processing fanfiction, amateur writing from fans of TV shows, movies, books, games, and comics. Fanfiction writers creatively change and expand on plots, settings, and characters from original media, an example of “participatory culture” (Jenkins, 1992; Tosenberger, 2008). The community of fanfiction readers and writers, now largely online, has been studied for its mentorship and support for writers (Evans et al.,"
2021.nuse-1.2,E14-1005,0,0.0248961,"compute mention representations and to cluster these mentions from pairwise or higher-order comparisons. They also concatenate features such as the distance between the compared mentions to their representations. However, these approaches to not capture the change in point of view caused by quotes within narratives, so they suffer when resolving first- and second-person pronouns within quotes. To alleviate this issue, we introduce an optional step in the pipeline that uses the output from quote attribution to inform the resolution of firstand second-person pronouns within quotes. Prior work (Almeida et al., 2014) proposed a joint model for entity-level quotation attribution and coreference resolution, exploiting correlations between the two tasks. However, in this work, we propose an interleaved setup that is modular and allows the user of the pipeline to use independent off-the-shelf pre-trained models of their choice for both coreference resolution and quote attribution. More specifically, once the quote attribution module predicts the position of each quote (qi ) and its associated speaker (si ), the first-person pronouns within the quote (e.g. I, my, mine, me) are resolved to the speaker of that q"
2021.nuse-1.2,2020.lrec-1.6,0,0.693536,"2017)’s approach assigns quotes to character mentions and then to character clusters. We simply assign quotes to the names of these selected character clusters. Coreference Resolution. We use SpanBERTbase (Joshi et al., 2020), a neural method with stateof-the-art performance on formal text, for coreference resolution. This model uses SpanBERT-base embeddings to create mention representations and employs Lee et al. (2017)’s approach to calculate the coreferent pairs. SpanBERT-base is originally trained on OntoNotes (Pradhan et al., 2012). However, we further fine-tune SpanBERT-base on LitBank (Bamman et al., 2020), a dataset with coreference annotations for works of literature in English, a domain more similar to fanfiction. The model takes the raw story text as input, identifies spans of text that mention characters, and outputs clusters of mentions that refer to the same character. 3.3 Quote Pronoun Resolution Module Recent advances in coreference resolution, such as the SpanBERT-base system incorporated in the pipeline, leverage contextualized word embeddings to compute mention representations and to cluster these mentions from pairwise or higher-order comparisons. They also concatenate features suc"
2021.nuse-1.2,P14-1035,0,0.136086,"ence, as well as the attribution of quotes and narration to those characters. Additionally, the pipeline contains a novel approach to character coreference that uses knowledge from quote attribution to resolve pronouns within quotes. For each module, we evaluate the effectiveness of various approaches on 10 annotated fanfiction stories. This pipeline outperforms tools developed for formal fiction on the tasks of character coreference and quote attribution. 1 Introduction A growing number of natural language processing tools and approaches have been developed for fiction (Agarwal et al., 2013; Bamman et al., 2014; Iyyer et al., 2016; Sims et al., 2019). These tools generally focus on published literary works, such as collections of novels. We present an NLP pipeline for processing fanfiction, amateur writing from fans of TV shows, movies, books, games, and comics. Fanfiction writers creatively change and expand on plots, settings, and characters from original media, an example of “participatory culture” (Jenkins, 1992; Tosenberger, 2008). The community of fanfiction readers and writers, now largely online, has been studied for its mentorship and support for writers (Evans et al., 2017) and for the bro"
2021.nuse-1.2,P13-1129,0,0.201051,"gold attribution name for that quote. A match is assigned if a) an assigned name has only one word, which matches any word in the gold cluster name (such as Tony and Tony Stark), or b) if more than half of the words in the name match between the two character names, excluding titles such as Ms. and Dr. Namematching is manually checked to ensure no system is penalized for selecting the wrong name within a correct character cluster. Any quote that a system fails to extract is considered a mis-attribution (an attribution to a NULL character). As baselines, we consider BookNLP and the approach of He et al. (2013), who train a RankSVM model supervised on annotations from the novel 18 BookNLP He et al. (2013) Muzny et al. (2017) (FanfictionNLP) With system coreference P R F1 With gold coreference P R F1 With gold quote extraction P R F1 54.6 54.0 25.4 53.3 34.7 53.6 66.8 56.5 38.9 55.7 49.2 56.1 65.0 56.7 49.7 56.0 56.3 56.3 68.7 67.0 67.8 73.5 75.4 74.4 77.5 77.5 77.5 Table 5: Quote attribution evaluation scores. Scores are reported using the respective system’s coreference (system coreference), with gold character coreference supplied (gold coreference) and with gold character and gold quote spans sup"
2021.nuse-1.2,J97-1003,0,0.811994,"eline also extracts what we describe as “assertions”, topically coherent segments of text that mention a character. The motivation for this is to identify longer spans of exposition and narrative that relate to characters for building embedding representations for these characters. Parsing these assertions would also facilitate the extraction of descriptive features such as verbs for which characters are subjects and adjectives used to describe characters. To identify such spans of texts that relate to characters, we first segment the text with a topic segmentation approach called TextTiling (Hearst, 1997). We then assign segments (with quotes removed) to characters if they contain at least one mention of the character within the span. If multiple characters are mentioned, the span is included in extracted assertions for each of the characters. 2 16 http://archiveofourown.org/ tives such as “she said”. This can be a source of ambiguity in published fiction as well, but we find a large variety of styles in fanfiction. One fanfiction story in our evaluation dataset, for example, contains many implicit quotes in conversations among three or more characters, which can be difficult for quote attribu"
2021.nuse-1.2,2020.emnlp-demos.14,0,0.0209698,"proach from Muzny et al. (2017) (Section 3.2). These quote attribution results are optionally used to aid the resolution of first- and second-person pronouns within quotes to improve coreference output (Section 3.3). In parallel with quote attribution, the pipeline extracts “assertions”, topically coherent segments of text that mention a character (Section 3.4). Fanfiction and NLP Data from fanfiction has been used in NLP research for a variety of tasks, including authorship attribution (Kestemont et al., 2018), action prediction (Vilares and Gómez-Rodríguez, 2019), finegrained entity typing (Chu et al., 2020), and tracing the sources of derivative texts (Shen et al., 2018). Computational work focusing on characterization in fanfiction includes the work of Milli and Bamman (2016), who found that fanfiction writers are more likely to emphasize female and secondary characters. Using data from WattPad, a platform 1 The pipeline is available at https://github.com/ michaelmilleryoder/fanfiction-nlp. 14 3.1 3.2 Character Coreference Module The story text is first passed through the coreference resolution module, which extracts mentions of characters and attributes them to character clusters. These mentio"
2021.nuse-1.2,N16-1180,0,0.0143464,"attribution of quotes and narration to those characters. Additionally, the pipeline contains a novel approach to character coreference that uses knowledge from quote attribution to resolve pronouns within quotes. For each module, we evaluate the effectiveness of various approaches on 10 annotated fanfiction stories. This pipeline outperforms tools developed for formal fiction on the tasks of character coreference and quote attribution. 1 Introduction A growing number of natural language processing tools and approaches have been developed for fiction (Agarwal et al., 2013; Bamman et al., 2014; Iyyer et al., 2016; Sims et al., 2019). These tools generally focus on published literary works, such as collections of novels. We present an NLP pipeline for processing fanfiction, amateur writing from fans of TV shows, movies, books, games, and comics. Fanfiction writers creatively change and expand on plots, settings, and characters from original media, an example of “participatory culture” (Jenkins, 1992; Tosenberger, 2008). The community of fanfiction readers and writers, now largely online, has been studied for its mentorship and support for writers (Evans et al., 2017) and for the broad representation of"
2021.nuse-1.2,D16-1218,0,0.0202461,"otes to improve coreference output (Section 3.3). In parallel with quote attribution, the pipeline extracts “assertions”, topically coherent segments of text that mention a character (Section 3.4). Fanfiction and NLP Data from fanfiction has been used in NLP research for a variety of tasks, including authorship attribution (Kestemont et al., 2018), action prediction (Vilares and Gómez-Rodríguez, 2019), finegrained entity typing (Chu et al., 2020), and tracing the sources of derivative texts (Shen et al., 2018). Computational work focusing on characterization in fanfiction includes the work of Milli and Bamman (2016), who found that fanfiction writers are more likely to emphasize female and secondary characters. Using data from WattPad, a platform 1 The pipeline is available at https://github.com/ michaelmilleryoder/fanfiction-nlp. 14 3.1 3.2 Character Coreference Module The story text is first passed through the coreference resolution module, which extracts mentions of characters and attributes them to character clusters. These mentions include alternative forms of names, pronouns, and anaphoric references such as “the bartender”. Each cluster is then given a single standardized character name. Quote Att"
2021.nuse-1.2,2020.tacl-1.5,0,0.0118654,"approach for formal fiction. The pipeline attributes quotes to characters with the deterministic approach of Muzny et al. (2017), which uses sieves such as looking for character mentions that are the head words of known speech verbs. We use a standalone re-implementation of this approach by Sims and Bamman (2020) that allows using the pipeline’s character coreference as input. Muzny et al. (2017)’s approach assigns quotes to character mentions and then to character clusters. We simply assign quotes to the names of these selected character clusters. Coreference Resolution. We use SpanBERTbase (Joshi et al., 2020), a neural method with stateof-the-art performance on formal text, for coreference resolution. This model uses SpanBERT-base embeddings to create mention representations and employs Lee et al. (2017)’s approach to calculate the coreferent pairs. SpanBERT-base is originally trained on OntoNotes (Pradhan et al., 2012). However, we further fine-tune SpanBERT-base on LitBank (Bamman et al., 2020), a dataset with coreference annotations for works of literature in English, a domain more similar to fanfiction. The model takes the raw story text as input, identifies spans of text that mention characte"
2021.nuse-1.2,P16-1060,0,0.023072,"performance on CoNLL and LEA metrics. O: Model is trained on OntoNotes. L: Model is also fine-tuned on LitBank corpus. FanfictionNLP is the SpanBERT-base OL model with posthoc removal of non-person entities. Note that none of the approaches had access to our fanfiction data. These results are without the quote pronoun resolultion module described in Section 3.3. Character Coreference Evaluation We evaluate the performance of the character coreference module on our 10 annotated fanfiction stories using the CoNLL metric (Pradhan et al., 2012; the average of MUC, B 3 , and CEAFE) and LEA metric (Moosavi and Strube, 2016). We compare our approach against different stateof-the-art approaches used for coreference resolution in the past. Along with BookNLP’s approach, we consider the Stanford CoreNLP deterministic coreference model (CoreNLP (dcoref); Raghunathan et al., 2010; Recasens et al., 2013; Lee et al., 2011) and the CoreNLP statistical model (CoreNLP (coref); Clark and Manning, 2015) as traditional baselines. As a neural baseline, we evaluate the more recently proposed BERT-base model (Joshi et al., 2019), which replaces the original GloVe embeddings (Pennington et al., 2014) with BERT (Devlin et al., 201"
2021.nuse-1.2,D19-1588,0,0.0489975,"ng the CoNLL metric (Pradhan et al., 2012; the average of MUC, B 3 , and CEAFE) and LEA metric (Moosavi and Strube, 2016). We compare our approach against different stateof-the-art approaches used for coreference resolution in the past. Along with BookNLP’s approach, we consider the Stanford CoreNLP deterministic coreference model (CoreNLP (dcoref); Raghunathan et al., 2010; Recasens et al., 2013; Lee et al., 2011) and the CoreNLP statistical model (CoreNLP (coref); Clark and Manning, 2015) as traditional baselines. As a neural baseline, we evaluate the more recently proposed BERT-base model (Joshi et al., 2019), which replaces the original GloVe embeddings (Pennington et al., 2014) with BERT (Devlin et al., 2019) in Lee et al. (2017)’s coreference resolution approach. Micro-averaged results across the 10 annotated stories are shown in Table 4. The FanfictionNLP approach is SpanBERT-base fine-tuned on LitBank, with the post-hoc removal of non-person and plural mentions and clusters (as described in Section 3.1). Note that these results are without the quote pronoun resolution module described in Section 3.3. Traditional approaches like BookNLP and CoreNLP (dcoref, coref) perform significantly worse t"
2021.nuse-1.2,E17-1044,0,0.304362,"a publicly available pipeline for processing fanfiction.1 This pipeline is a commandline tool developed in Python. From the text of a fanfiction story, the pipeline extracts a list of characters, each mention of a character, as well as what each character does and says (Figure 1). More specifically, the pipeline first performs character coreference resolution, extracting character mentions and attributing them to character clusters with a single standardized character name (Section 3.1). After coreference, the pipeline outputs quotes uttered by each character using a sieve-based approach from Muzny et al. (2017) (Section 3.2). These quote attribution results are optionally used to aid the resolution of first- and second-person pronouns within quotes to improve coreference output (Section 3.3). In parallel with quote attribution, the pipeline extracts “assertions”, topically coherent segments of text that mention a character (Section 3.4). Fanfiction and NLP Data from fanfiction has been used in NLP research for a variety of tasks, including authorship attribution (Kestemont et al., 2018), action prediction (Vilares and Gómez-Rodríguez, 2019), finegrained entity typing (Chu et al., 2020), and tracing"
2021.nuse-1.2,W18-1501,0,0.0390302,"n larger sizes (Yin et al., 2017). We present a pipeline that enables structured insight into this vast amount of text by identifying sets of characters in fanfiction stories and attributing narration and quotes to these characters. Knowing who the characters are and what they do and say is essential for understanding story structure (Bruce, 1981; Wall, 1984). Such processing is also useful for researchers in the humanities and social sciences investigating identification with characters and the representation of characters of diverse genders, sexualities, and ethnicities (Green et al., 2004; Kasunic and Kaufman, 2018; Felski, 2020). The presented pipeline, which extracts text related to characters in fanfiction, can assist researchers building NLP tools for literary domains, as well those analyzing characterization in fields such as digital humanities. For example, the pipeline could be used to explore how characters are voiced and described differently when cast in queer versus straight relationships. The presented pipeline contains three main modules: character coreference resolution, quote attribution, and extraction of “assertions”, narration that relates to particular characters. We incorporate new a"
2021.nuse-1.2,D12-1072,0,0.0697058,"Missing"
2021.nuse-1.2,D14-1162,0,0.0872354,", and CEAFE) and LEA metric (Moosavi and Strube, 2016). We compare our approach against different stateof-the-art approaches used for coreference resolution in the past. Along with BookNLP’s approach, we consider the Stanford CoreNLP deterministic coreference model (CoreNLP (dcoref); Raghunathan et al., 2010; Recasens et al., 2013; Lee et al., 2011) and the CoreNLP statistical model (CoreNLP (coref); Clark and Manning, 2015) as traditional baselines. As a neural baseline, we evaluate the more recently proposed BERT-base model (Joshi et al., 2019), which replaces the original GloVe embeddings (Pennington et al., 2014) with BERT (Devlin et al., 2019) in Lee et al. (2017)’s coreference resolution approach. Micro-averaged results across the 10 annotated stories are shown in Table 4. The FanfictionNLP approach is SpanBERT-base fine-tuned on LitBank, with the post-hoc removal of non-person and plural mentions and clusters (as described in Section 3.1). Note that these results are without the quote pronoun resolution module described in Section 3.3. Traditional approaches like BookNLP and CoreNLP (dcoref, coref) perform significantly worse than the neural models, especially on recall. Neural models that are furt"
2021.nuse-1.2,N19-1067,0,0.017456,"dharan, Carolyn P. Rosé Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA {yoder,sopank,qinlans,anaik,huimingj,hmuralid,cprose}@cs.cmu.edu Abstract a data source for research in a variety of fields, from those studying learning in online communities to social science analysis of how community norms develop in an LGBTQ-friendly environment. For NLP researchers, fanfiction provides a large source of literary text with metadata, and has already been used in applications such as authorship attribution (Kestemont et al., 2018) and character relationship classification (Kim and Klinger, 2019). There is an vast amount of fanfiction in online archives. As of March 2021, over 7 million stories were hosted on just one fanfiction website, Archive of Our Own, and there exist other online archives of similar or even larger sizes (Yin et al., 2017). We present a pipeline that enables structured insight into this vast amount of text by identifying sets of characters in fanfiction stories and attributing narration and quotes to these characters. Knowing who the characters are and what they do and say is essential for understanding story structure (Bruce, 1981; Wall, 1984). Such processing i"
2021.nuse-1.2,W12-4501,0,0.413775,"that allows using the pipeline’s character coreference as input. Muzny et al. (2017)’s approach assigns quotes to character mentions and then to character clusters. We simply assign quotes to the names of these selected character clusters. Coreference Resolution. We use SpanBERTbase (Joshi et al., 2020), a neural method with stateof-the-art performance on formal text, for coreference resolution. This model uses SpanBERT-base embeddings to create mention representations and employs Lee et al. (2017)’s approach to calculate the coreferent pairs. SpanBERT-base is originally trained on OntoNotes (Pradhan et al., 2012). However, we further fine-tune SpanBERT-base on LitBank (Bamman et al., 2020), a dataset with coreference annotations for works of literature in English, a domain more similar to fanfiction. The model takes the raw story text as input, identifies spans of text that mention characters, and outputs clusters of mentions that refer to the same character. 3.3 Quote Pronoun Resolution Module Recent advances in coreference resolution, such as the SpanBERT-base system incorporated in the pipeline, leverage contextualized word embeddings to compute mention representations and to cluster these mentions"
2021.nuse-1.2,W11-1902,0,0.0562395,"quote pronoun resolultion module described in Section 3.3. Character Coreference Evaluation We evaluate the performance of the character coreference module on our 10 annotated fanfiction stories using the CoNLL metric (Pradhan et al., 2012; the average of MUC, B 3 , and CEAFE) and LEA metric (Moosavi and Strube, 2016). We compare our approach against different stateof-the-art approaches used for coreference resolution in the past. Along with BookNLP’s approach, we consider the Stanford CoreNLP deterministic coreference model (CoreNLP (dcoref); Raghunathan et al., 2010; Recasens et al., 2013; Lee et al., 2011) and the CoreNLP statistical model (CoreNLP (coref); Clark and Manning, 2015) as traditional baselines. As a neural baseline, we evaluate the more recently proposed BERT-base model (Joshi et al., 2019), which replaces the original GloVe embeddings (Pennington et al., 2014) with BERT (Devlin et al., 2019) in Lee et al. (2017)’s coreference resolution approach. Micro-averaged results across the 10 annotated stories are shown in Table 4. The FanfictionNLP approach is SpanBERT-base fine-tuned on LitBank, with the post-hoc removal of non-person and plural mentions and clusters (as described in Sect"
2021.nuse-1.2,D10-1048,0,0.0373171,"ur fanfiction data. These results are without the quote pronoun resolultion module described in Section 3.3. Character Coreference Evaluation We evaluate the performance of the character coreference module on our 10 annotated fanfiction stories using the CoNLL metric (Pradhan et al., 2012; the average of MUC, B 3 , and CEAFE) and LEA metric (Moosavi and Strube, 2016). We compare our approach against different stateof-the-art approaches used for coreference resolution in the past. Along with BookNLP’s approach, we consider the Stanford CoreNLP deterministic coreference model (CoreNLP (dcoref); Raghunathan et al., 2010; Recasens et al., 2013; Lee et al., 2011) and the CoreNLP statistical model (CoreNLP (coref); Clark and Manning, 2015) as traditional baselines. As a neural baseline, we evaluate the more recently proposed BERT-base model (Joshi et al., 2019), which replaces the original GloVe embeddings (Pennington et al., 2014) with BERT (Devlin et al., 2019) in Lee et al. (2017)’s coreference resolution approach. Micro-averaged results across the 10 annotated stories are shown in Table 4. The FanfictionNLP approach is SpanBERT-base fine-tuned on LitBank, with the post-hoc removal of non-person and plural m"
2021.nuse-1.2,D17-1018,0,0.120914,"ead words of known speech verbs. We use a standalone re-implementation of this approach by Sims and Bamman (2020) that allows using the pipeline’s character coreference as input. Muzny et al. (2017)’s approach assigns quotes to character mentions and then to character clusters. We simply assign quotes to the names of these selected character clusters. Coreference Resolution. We use SpanBERTbase (Joshi et al., 2020), a neural method with stateof-the-art performance on formal text, for coreference resolution. This model uses SpanBERT-base embeddings to create mention representations and employs Lee et al. (2017)’s approach to calculate the coreferent pairs. SpanBERT-base is originally trained on OntoNotes (Pradhan et al., 2012). However, we further fine-tune SpanBERT-base on LitBank (Bamman et al., 2020), a dataset with coreference annotations for works of literature in English, a domain more similar to fanfiction. The model takes the raw story text as input, identifies spans of text that mention characters, and outputs clusters of mentions that refer to the same character. 3.3 Quote Pronoun Resolution Module Recent advances in coreference resolution, such as the SpanBERT-base system incorporated in"
2021.nuse-1.2,N13-1071,0,0.0743424,"Missing"
2021.nuse-1.2,2020.emnlp-main.47,0,0.0109272,"single standardized character name. Quote Attribution Module To extract quotes, we simply extract any spans between quotation marks, a common approach in literary texts (O’Keefe et al., 2012). For the wide variety of fanfiction, we recognize a broader set of quotation marks than are recognized in BookNLP’s approach for formal fiction. The pipeline attributes quotes to characters with the deterministic approach of Muzny et al. (2017), which uses sieves such as looking for character mentions that are the head words of known speech verbs. We use a standalone re-implementation of this approach by Sims and Bamman (2020) that allows using the pipeline’s character coreference as input. Muzny et al. (2017)’s approach assigns quotes to character mentions and then to character clusters. We simply assign quotes to the names of these selected character clusters. Coreference Resolution. We use SpanBERTbase (Joshi et al., 2020), a neural method with stateof-the-art performance on formal text, for coreference resolution. This model uses SpanBERT-base embeddings to create mention representations and employs Lee et al. (2017)’s approach to calculate the coreferent pairs. SpanBERT-base is originally trained on OntoNotes"
2021.nuse-1.2,P19-1353,0,0.0225872,"s and narration to those characters. Additionally, the pipeline contains a novel approach to character coreference that uses knowledge from quote attribution to resolve pronouns within quotes. For each module, we evaluate the effectiveness of various approaches on 10 annotated fanfiction stories. This pipeline outperforms tools developed for formal fiction on the tasks of character coreference and quote attribution. 1 Introduction A growing number of natural language processing tools and approaches have been developed for fiction (Agarwal et al., 2013; Bamman et al., 2014; Iyyer et al., 2016; Sims et al., 2019). These tools generally focus on published literary works, such as collections of novels. We present an NLP pipeline for processing fanfiction, amateur writing from fans of TV shows, movies, books, games, and comics. Fanfiction writers creatively change and expand on plots, settings, and characters from original media, an example of “participatory culture” (Jenkins, 1992; Tosenberger, 2008). The community of fanfiction readers and writers, now largely online, has been studied for its mentorship and support for writers (Evans et al., 2017) and for the broad representation of LGBTQ+ characters a"
2021.nuse-1.2,N19-1218,0,0.0333294,"Missing"
A00-2041,P99-1052,0,0.115103,"s can be combined with other partial analyses in a semantically meaningful way. Genetic programming search (Koza, 1992; Koza, 1994) is used to efficiently compose the fragments produced by the parser. The function definitions compiled from the meaning representation specification allow the genetic search to use semantic constraints to make effective use of its search space. Thus, AUTOSEM operates efficiently, free of any hand coded repair rules or any knowledge specifically dedicated to repair unlike other approaches to recovery from parser failure (Danieli and Gerbino, 1995; Van Noord, 1997; Kasper et al., 1999). 2 The Meaning Specification Representation At the heart of AUTOSEM is its interpretation framework composed of semantic constructor functions compiled from a meaning representation specification. These semantic constructor functions can be used at parse time to build up semantic representations. These same constructor functions can then be used in a repair stage to compose the fragments returned by the parser in the cases where the parser is not able to obtain a complete analysis for an ex(:type <*state> :isa (<>) :instances nil :vars (entity time duration polarity) :spec ((who <*entity> ent"
A00-2041,A97-1012,0,0.023776,"would have been removed. From the remaining list, a quadruple was selected randomly. The corresponding crossover operation was then executed and the resulting two new programs were returned. While this typed vetsion of crossover is more complex than the original crossover operation, it can be executed very rapidly in practice because the programs are relatively small and the semantic type restrictions ensure than the initial lists generated are correspondingly small. 5 Related Work Recent approaches to robust parsing focus on shallow or partial parsing techniques (Van Noord, 1997; Worm, 1998; Ait-Mokhtar and Chanod, 1997; Abney, 1996). Rather than a t t e m p t i n g to construct a parse covering an entire ungrammatical sentence as in (Lehman, 1989; Hipp, 1992), these approaches att e m p t to construct analyses for maximal contiguous portions of the input. The weakness of these partial parsing approaches is that part of the original meaning of the utterance m a y be discarded with the portion(s) of the utterance that are skipped in order to find a parsable subset. Information communicated by the relationships between these fragments within the original text is lost if these fragments are not combined. Thus,"
A00-2041,C94-1042,0,0.510019,"emantics approach to interpretation (Dalrymple, 1999). A U T O S E M &apos; s lexicon formalism allows semantic constructor functions to be linked into lexical entries by means of the semtag feature. Each semtag feature value corresponds to a semantic constructor function and mappings between syntactic functional roles such as s u b j e c t , d i r e c t o b j e c t , and i n d i r e c t object and semantic roles such as a g e n t , a c t i v i t y , or time. See Figures 2 and 3 discussed further below. Note that the syntactic features that appear in this example are taken from the COMLEX lexicon (Grishman et al., 1994). In order to provide consistent input to the semantic constructor functious, AUTOSEM assumes a syntactic approach in which deep syntactic functional roles are assigned as in C A R M E L &apos; s syntactic parsing g r a m m a r evaluated in (Freedman et al., to appear). In this way, for example, the roles assigned within an active sentence and its corresponding passive sentence remain the same. Next, when &quot;by you&quot; is attached, &quot;you&quot; is assigned the deep syntactic role of subject of &quot;cancel&quot;. The s u b j e c t role is associated with the a g e n t argument in the definition of c a n c e l l . Thus,"
A00-2041,W94-0113,1,0.889678,"Missing"
A00-2041,J97-3004,0,0.0708272,"Missing"
A00-2041,P98-2229,0,0.0616116,"rogram that would have been removed. From the remaining list, a quadruple was selected randomly. The corresponding crossover operation was then executed and the resulting two new programs were returned. While this typed vetsion of crossover is more complex than the original crossover operation, it can be executed very rapidly in practice because the programs are relatively small and the semantic type restrictions ensure than the initial lists generated are correspondingly small. 5 Related Work Recent approaches to robust parsing focus on shallow or partial parsing techniques (Van Noord, 1997; Worm, 1998; Ait-Mokhtar and Chanod, 1997; Abney, 1996). Rather than a t t e m p t i n g to construct a parse covering an entire ungrammatical sentence as in (Lehman, 1989; Hipp, 1992), these approaches att e m p t to construct analyses for maximal contiguous portions of the input. The weakness of these partial parsing approaches is that part of the original meaning of the utterance m a y be discarded with the portion(s) of the utterance that are skipped in order to find a parsable subset. Information communicated by the relationships between these fragments within the original text is lost if these frag"
C18-1198,D17-1263,0,0.0599234,"Missing"
C18-1198,D17-1215,0,0.145006,"Missing"
C18-1198,P17-1015,0,0.0422534,"Missing"
C18-1198,P11-2057,0,0.0357943,"Missing"
C18-1198,W17-5405,0,0.0695799,"Missing"
C18-1198,W07-1407,0,0.140553,"Missing"
C18-1198,S14-2001,0,0.0573409,"Missing"
C18-1198,marelli-etal-2014-sick,0,0.137032,"Missing"
C18-1198,W17-5301,0,0.0529109,"Missing"
C18-1198,W17-5308,0,0.0824878,"Missing"
C18-1198,S18-2023,0,0.15601,"Missing"
C18-1198,D09-1085,0,0.13395,"Missing"
C18-1198,E06-1052,0,0.0638059,"Missing"
C18-1198,W17-5402,0,0.0661211,"Missing"
C18-1198,W17-5410,0,0.0813283,"Missing"
C18-1198,H05-1049,0,\N,Missing
C18-1198,P06-1114,0,\N,Missing
C18-1198,W07-1401,0,\N,Missing
C18-1198,W17-5307,0,\N,Missing
C18-1198,W17-4705,0,\N,Missing
C18-1198,N18-2017,0,\N,Missing
C96-1061,P92-1025,0,0.0612146,"Missing"
C96-1061,1995.tmi-1.13,1,0.858993,"Missing"
C96-1061,J95-3001,0,\N,Missing
C96-1061,P95-1016,0,\N,Missing
C96-1061,J81-4001,0,\N,Missing
C96-1061,P95-1005,1,\N,Missing
C96-1061,H93-1041,0,\N,Missing
C96-1061,1993.tmi-1.16,0,\N,Missing
C96-1061,J93-1002,0,\N,Missing
C98-2180,C96-1075,1,0.897687,"Missing"
C98-2180,W97-0303,1,0.891754,"Missing"
C98-2180,W94-0113,1,0.866328,"Missing"
C98-2180,P95-1005,1,0.891709,"Missing"
D12-1119,P08-1029,1,0.814281,"ly distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1 . While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would hope, or are other fa"
D12-1119,W06-1615,0,0.15196,"ecific distribution Ddi , and yi is the label (e.g. yi ∈ {−1, +1} for binary labels). Standard learning ignores di , but MDL uses these to improve learning accuracy. Why should we care about the domain label? Domain differences can introduce errors in a number of ways (Ben-David et al., 2007; Ben-David et al., 2009). First, the domain-specific distributions Ddi can differ such that they favor different features, i.e. p(x) changes between domains. As a result, some features may only appear in one domain. This aspect of domain difference is typically the focus of unsupervised domain adaptation (Blitzer et al., 2006; Blitzer et al., 2007). Second, the features may behave differently with respect to the label in each domain, i.e. p(y|x) changes between domains. As a result, a learning algorithm cannot generalize the behavior of features from one domain to another. The key idea behind many MDL algorithms is to target one or both of these properties of domain difference 1303 to improve performance. Prior approaches to MDL can be broadly categorized into two classes. The first set of approaches (Daum´e III, 2007; Dredze et al., 2008) introduce parameters to capture domain-specific behaviors while preserving"
D12-1119,P07-1056,1,0.862631,"has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art. 1 Introduction Research efforts in recent years have demonstrated the importance of domains in statistical natural language processing. A mismatch between training and test domains can negatively impact system accuracy as it violates a core assumption in many machine learning algorithms: that data points are independent and identically distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product r"
D12-1119,W04-3237,0,0.00951458,"ssues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art. 1 Introduction Research efforts in recent years have demonstrated the importance of domains in statistical natural language processing. A mismatch between training and test domains can negatively impact system accuracy as it violates a core assumption in many machine learning algorithms: that data points are independent and identically distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One su"
D12-1119,W10-2608,0,0.0251531,"Missing"
D12-1119,P07-1033,0,0.72823,"Missing"
D12-1119,D08-1072,1,0.960015,"independent and identically distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1 . While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would"
D12-1119,N09-1068,0,0.0948268,".). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1 . While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would hope, or are other factors contributing to lear"
D12-1119,D07-1111,0,0.0119993,"ety of tasks. The key idea behind ensemble learning, that of combining a diverse array of models, has been applied to settings in which data preprocessing is used to create many different classifiers. Examples include instance bagging and feature bagging (Dietterich, 2000). The core idea of using diverse inputs in making classification decisions is common in the MDL literature. In fact, the top performing and only successful entry to the 2007 CoNLL shared task on domain adaptation for dependency parsing was a straightforward implementation of ensemble learning by creating variants of parsers (Sagae and Tsujii, 2007). Many MDL algorithms, among them Dredze and Crammer (2008), Daum´e III (2009), Zhang and Yeung (2010) and Saha et al. (2011), all include some notion of learning domain-specific classifiers on the training data, and combining them in the best way possible. To be clear, we do not claim that these approaches can be reduced to an existing ensemble learning algorithm. There are crucial elements in each of these algorithms that separate them from existing ensemble learning algorithms. One example of such a distinction is the learning of domain relationships by both Zhang and Yeung (2010) and Saha"
D12-1119,W06-1639,0,0.0122179,", electronics and kitchen appliances. The original dataset contained 2,000 reviews for each of the four domains, with 1,000 positive and 1,000 negative reviews per domain. Feature extraction follows Blitzer et al. (2007): we use case insensitive unigrams and bigrams, although we remove rare features (those that appear less than five times in the training set). The reduced feature set was selected given the sensitivity to feature size of some of the MDL methods. ConVote (C ONVOTE) Our second dataset is taken from segments of speech from United States Congress floor debates, first introduced by Thomas et al. (2006). The binary classification task on this dataset is that of predicting whether a given speech segment supports or opposes a bill under discussion in the floor debate. We select this dataset because, unlike the A MAZON data, C ONVOTE can be divided into domains in several ways based on different metadata attributes available with the dataset. We consider two types of domain divisions: the bill identifier and the political party of the speaker. Division based on the bill creates domain differences in that each bill has its own topic. Division based on political party implies preference for diffe"
D17-1232,W11-0133,0,0.0268937,"Missing"
D17-1232,E17-2078,0,0.0201596,"Missing"
D17-1232,W10-2923,0,0.0204574,"different corpora. 4.2 Corpora and Preprocessing We evaluate our model in terms of accuracy in utterance-level DA recognition. Since the output of the model is assignments to discovered states for utterances, not pre-determined DA labels, we use a clustering evaluation method, as adopted by previous work on unsupervised DA modeling. Specifically, we use homogeneity, completeness, and v-measure as metrics (Rosenberg and Hirschberg, 2007). Homogeneity represents the degree to which utterances assigned to the same We evaluate on two corpora: CNET and NPS Chat (see Table 3 for statistics). CNET (Kim et al., 2010) is a set of post threads from the Operating System, Software, Hardware, and Web Development sub-forums of CNET. This corpus is tagged with 12 DAs, including QuestionQuestion, Question-Confirmation, Answer-Add, Resolution, and Other (Table 4). Note that question- and answer-related DAs are two-level. Most posts are tagged with one DA; in case a post is tagged with multiple DAs, we choose the first DA in the meta-data3 . Each post is considered an 2 https://github.com/yohanjo/ Dialogue-Acts 3 Some tagging systems, such as the DAMSL-style, break down an utterance that has multiple DAs. 4.1 Task"
D17-1232,W13-4006,0,0.0282249,"ilizing speaker tendencies is advantageous on the NPS corpus. The components of our model complement one another to achieve robust performance on both corpora and outperform state-of-the-art baseline models. 1 Introduction Dialogue acts (DAs), or speech acts, represent the intention behind an utterance in conversation to achieve a conversational goal (Austin, 1975). Modeling conversations as structured DA sequences is a step toward the automated understanding of dialogue, useful for dialogue agents (Traum, 1999; Louwerse et al., 2002) and the processing of informal online conversational data (Misra and Walker, 2013; Vosoughi and Roy, 2016). Distributions of DAs can also be used as predictors of conversational outcome measures such as student learning in tutoring systems (Litman and Forbes-Riley, 2006) and engagement in meetings (Wrede and Shriberg, 2003). Unsupervised models for DA recognition may substitute or aid costly human annotation. We present an unsupervised model of DA sequences in conversation that overcomes limitations of prior models. The first improvement our model offers is separating out content-related words to emphasize words more relevant to DAs. DAs are associated more closely with st"
D17-1232,D12-1009,0,0.182814,"n or cross-task generalizations about conversational processes. Our model filters out content words by implementing the assumption that conversations proceed against a backdrop of underlying topics that transition more slowly than DAs or that are constant throughout. Based on a difference in transition speed, two types of language models are learned: foreground language models that capture DA-related words and background language models for content words. Although some existing models assume a background or domain-specific language model to filter out words unrelated to DAs (Lee et al., 2013; Paul, 2012; Ritter et al., 2010), they either require domain labels or do not learn topics underlying conversations. The second improvement offered by our model is inclusion of speaker preferences, or tendencies to use some DAs more than others. Prior mod2179 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2179–2189 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics els of DAs in conversation often rely on the discourse property of conditional relevance (Levinson, 1983; Martin and Rose, 2003), i.e., tendencies for seq"
D17-1232,D07-1043,0,0.0123588,"d completeness. These metrics are easy to interpret and have been demonstrated to be invariant to dataset size and number of clusters. This enables a meaningful comparison of accuracy across different corpora. 4.2 Corpora and Preprocessing We evaluate our model in terms of accuracy in utterance-level DA recognition. Since the output of the model is assignments to discovered states for utterances, not pre-determined DA labels, we use a clustering evaluation method, as adopted by previous work on unsupervised DA modeling. Specifically, we use homogeneity, completeness, and v-measure as metrics (Rosenberg and Hirschberg, 2007). Homogeneity represents the degree to which utterances assigned to the same We evaluate on two corpora: CNET and NPS Chat (see Table 3 for statistics). CNET (Kim et al., 2010) is a set of post threads from the Operating System, Software, Hardware, and Web Development sub-forums of CNET. This corpus is tagged with 12 DAs, including QuestionQuestion, Question-Confirmation, Answer-Add, Resolution, and Other (Table 4). Note that question- and answer-related DAs are two-level. Most posts are tagged with one DA; in case a post is tagged with multiple DAs, we choose the first DA in the meta-data3 ."
D17-1232,N10-1020,0,\N,Missing
E12-1080,W06-3403,0,0.304697,"etwork Model for Conversation Prior research has attempted to quantify accommodation computationally by measuring similarSpeech stylistic information is reflected in prosodic features such as pitch, energy, speakAccommodation could be measured either from textual or speech content of a conversation. The former relates to ”what” people say whereas the latter to ’how’ they say it. We are only interested in measuring accommodation from speech in this work. There has been work on convergence in text such as syntactic adaptation (Reitter et al., 2006) and language similarity in online communities (Huffaker et al., 2006). 2.2 Social Interpretation of Speech Style Accommodation 789 ing rate etc. In this work, we leverage on several of these speech features to quantify accommodation. We propose a series of models that can be trained unsupervised from speech features and can be used for predicting accommodation. The models attempt to capture the dependence of speech features on speaking style, as well as the effect of persistence and accommodation on style. We use a dynamic Bayesian network (DBN) formalism to capture these relationships. Below we briefly review DBNs, and subsequently describe the speech features"
E12-1080,P11-2020,0,0.771719,"tly focused on the ways in which shifts in style are used to achieve strategic goals within interactions, for example the ways in which speakers may adapt their speaking style to suppress differences and accentuate similarities between themselves and their interlocutors in order to build solidarity (Coupland, 2007; Eckert & Rickford, 2001; Sanders, 1987). We refer to this stylistic convergence as speech style accommodation. In the language technologies community, one targeted practical benefit of such modeling has been the achievement of more natural interactions with speech dialogue systems (Levitan et al., 2011). Monitoring social processes from speech or language data has other practical benefits as well, such as enabling monitoring how beneficial an interaction is for group learning (Ward & Litman, 2007; Gweon, 2011), how equal participation is within a group (DiMicco et al., 2004), or how conducive an environment is for fostering a sense of belonging and identification with a community (Wang et al., 2011). Typical work on computational models of speech style accommodation have focused on specific aspects of style that may be accommodated, such as the frequency or timing of pauses or backchannels ("
E12-1080,D09-1035,0,0.0457197,"Missing"
E12-1080,N06-2031,0,0.0207724,"utational models of speech style accommodation 3 A Dynamic Bayesian Network Model for Conversation Prior research has attempted to quantify accommodation computationally by measuring similarSpeech stylistic information is reflected in prosodic features such as pitch, energy, speakAccommodation could be measured either from textual or speech content of a conversation. The former relates to ”what” people say whereas the latter to ’how’ they say it. We are only interested in measuring accommodation from speech in this work. There has been work on convergence in text such as syntactic adaptation (Reitter et al., 2006) and language similarity in online communities (Huffaker et al., 2006). 2.2 Social Interpretation of Speech Style Accommodation 789 ing rate etc. In this work, we leverage on several of these speech features to quantify accommodation. We propose a series of models that can be trained unsupervised from speech features and can be used for predicting accommodation. The models attempt to capture the dependence of speech features on speaking style, as well as the effect of persistence and accommodation on style. We use a dynamic Bayesian network (DBN) formalism to capture these relationships. Below"
E12-1080,P08-2043,0,\N,Missing
E14-1012,N09-1068,0,0.0216054,"ned previously, plurals, feminine forms, etc. Additionally, in the Blood and Hoover features, they sometimes use numbers to replace the „o‟s representing the street that their gang is located on. So the Bloods from 34th Street, say, might write “Bl34d”. 111 3.2 Computational domain learning Paradigm: MultiThe key to training an interpretable model in our work is to pair a rich feature representation with a model that enables accounting for the structure of the social context explicitly. Recent work in the area of multi-domain learning offers such an opportunity (Arnold, 2009; Daumé III, 2007; Finkel & Manning, 2009). In our work, we treat the dominant gang of a thread as a domain for the purpose of detecting thread composition. This decision is based on the observation that while it is a common practice across gangs to express their attitudes towards allied and opposing gangs using stylistic features like the Graffiti style features, the particular features that serve the purpose of showing affiliation or opposition differ by gang. Thus, it is not the features themselves that carry significance, but rather a combination of who is saying it and how it is being said. As a paradigm for multi-domain learning"
E14-1012,W11-2606,1,0.942444,"Practices at the Feature Level Gender-based language variation arises from multiple sources. Among these, it has been noted that within a single corpus comprised of samples of male and female language that the two genders do not speak or write about the same topics. This is problematic because word-based features such as unigrams and bigrams, which are very frequently used, are highly likely to pick up on differences in topic (Schler, 2006) and possibly perspective. Thus, in cases where linguistic style variation is specifically of interest, these features do not offer good generalizability (Gianfortoni et al., 2011). Similarly, in our work, members of different 109 gangs are located in different areas associated with different concerns and levels of socioeconomic status. Thus, in working to model the stylistic choices of gang forum members, it is important to consider how to avoid overfitting to content-level distinctions. Typical kinds of features that have been used in gender prediction apart from unigram features include part-of-speech (POS) ngrams (Argamon et al., 2003), word-structure features that cluster words according to endings that indicate part of speech (Zhang et al., 2009), features that in"
E14-1012,J04-3002,0,\N,Missing
E14-1012,P07-1033,0,\N,Missing
I17-1103,W11-0707,0,0.32183,"revealing subgroups of users with similar attitudes (Hassan et al., 2012). 1026 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1026–1035, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Our work fits with research on editor behavior on Wikipedia, which is relatively wellstudied on article pages and somewhat less studied on talk pages. Wikipedia has been a popular source of data for modeling social interaction and other issues of language behavior from multiple perspectives including collaboration (Ferschke et al., 2012), authority (Bender et al., 2011), influence (Bracewell et al., 2012; Swayamdipta and Rambow, 2012), and collegiality and adversity (Bracewell et al., 2012). Much work analyzing behavior in Wikipedia has focused on types of edit behavior. Yang et al. (2016) use an LDA-based model to derive editor roles from edit behaviors. They then find correlations between certain editor roles and article quality improvements. Their approach differs from ours in that our model is supervised with an outcome measure and that we define editor roles based on talk page behavior. Behavior in discussion can be characterized at multiple levels of g"
I17-1103,E12-1079,0,0.0176544,"thers’ minds (Tan et al., 2016) and revealing subgroups of users with similar attitudes (Hassan et al., 2012). 1026 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1026–1035, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Our work fits with research on editor behavior on Wikipedia, which is relatively wellstudied on article pages and somewhat less studied on talk pages. Wikipedia has been a popular source of data for modeling social interaction and other issues of language behavior from multiple perspectives including collaboration (Ferschke et al., 2012), authority (Bender et al., 2011), influence (Bracewell et al., 2012; Swayamdipta and Rambow, 2012), and collegiality and adversity (Bracewell et al., 2012). Much work analyzing behavior in Wikipedia has focused on types of edit behavior. Yang et al. (2016) use an LDA-based model to derive editor roles from edit behaviors. They then find correlations between certain editor roles and article quality improvements. Their approach differs from ours in that our model is supervised with an outcome measure and that we define editor roles based on talk page behavior. Behavior in discussion can be char"
I17-1103,P11-4017,0,0.0141439,"page. It is this assessment that forms the class value of our predictive task. In this study we explore negotiation strategies and role configurations that affect article editing; each data point in our task provides both discussion and an article edit success value for each editor involved. 2 This dataset is available at http://github.com/ michaelmilleryoder/wikipedia-talk-scores 1027 3.2 Data Acquisition To form our dataset, we extracted all versions (revisions) of English Wikipedia articles from 2004 to 2014 and removed much of the Mediawiki markup using the Java Wikipedia Library (JWPL) (Ferschke et al., 2011). The most recent revisions of talk pages corresponding to the articles were split into turns using paragraph boundaries and edit history. We grouped discussion posts under the same section headings as discussion threads. We sampled 100,000 articles with talk page discussions and filtered to include discussion threads with 2 or more participants who made edits to the article page from 24 hours before the discussion began to 24 hours after the discussion ended. Discussion thread beginnings and endings are defined as the time of the first post and last post, respectively. Statistics on our discu"
I17-1103,D12-1006,0,0.0290285,"editors. We find that the greatest success is achieved by detail-oriented editors working in cooperation with editors who play more abstract organizational roles. 2 Related Work This work investigates influence in discussion as part of the collaborative editing process of Wikipedia, but achieving influence through discussion has also been studied in online environments other than Wikipedia. For example, other work in language technologies has studied the effectiveness of argumentative speech in changing others’ minds (Tan et al., 2016) and revealing subgroups of users with similar attitudes (Hassan et al., 2012). 1026 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1026–1035, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Our work fits with research on editor behavior on Wikipedia, which is relatively wellstudied on article pages and somewhat less studied on talk pages. Wikipedia has been a popular source of data for modeling social interaction and other issues of language behavior from multiple perspectives including collaboration (Ferschke et al., 2012), authority (Bender et al., 2011), influence (Bracewell et al., 2012; Swayamdipta and R"
I17-1103,D17-1232,1,0.878394,"Missing"
I17-1103,P15-1161,1,0.847068,"Missing"
J16-3007,P12-2012,0,0.0240577,"Missing"
J16-3007,I13-1041,0,0.0146601,"ng and analysis of historical texts (see Piotrowski [2012] for an overview), which also contain many spelling variations. In this domain, normalization is often applied as well to facilitate the use of tools such as parsers. However, some approaches first normalize the text, but then replace the modernized word forms with the original word forms to retain the original text. Another issue with social media data is that many social media studies have so far focused primarily on one data source. A comparison of the online data sources in terms of language use has only been done in a few studies (Baldwin et al. 2013; Hu, Talamadupula, and Kambhampati 2013). Another up-and-coming promising resource for studying language from a social perspective is crowdsourcing. So far, crowdsourcing is mostly used to obtain large numbers of annotations (e.g., Snow et al. 2008). However, “crowds” can also be used for large-scale perception studies (i.e., to study how non-linguists interpret messages and identify social characteristics of speakers [Clopper 2013]), and for the collection of linguistic data, such as the use of variants of linguistic variables. Within sociolinguistics, surveys have been one of the instrument"
J16-3007,N10-1027,0,0.0180866,"Missing"
J16-3007,P14-2134,0,0.0507187,"Missing"
J16-3007,W11-0707,0,0.28973,"abhakaran, Rambow, and Diab 2012b; Prabhakaran, Reid, and Rambow 2014), since Enron’s organizational structure is known and can be integrated in studies on hierarchical power structures connected with quantitative capacity theories of power. Such theories treat power as a stable characteristic that inheres in a person. An example theory within this space is Resource Dependency Theory (Pfeffer and Salancik 1978). For studies that involve more dynamic notions of power (e.g., identifying individuals who are pursuing power), other resources have also been explored, including Wikipedia Talk Pages (Bender et al. 2011; Bracewell, Tomlinson, and Wang 2012; Danescu-Niculescu-Mizil et al. 2012; Swayamdipta and Rambow 2012), transcripts of political debates (Prabhakaran, John, and Seligmann 2013; Prabhakaran, Arora, and Rambow 2014), and transcripts of Supreme Court arguments (Danescu-Niculescu-Mizil et al. 2012). 4.2 Shaping Social Relationships Language is not only a means to exchange information but language also contributes to the performance of action within interaction. Language serves simultaneously as a reflection of the relative positioning of speakers to their conversation partners as well as actions"
J16-3007,N12-1033,0,0.0205081,"Missing"
J16-3007,W12-2105,0,0.0257779,"nron corpus, they compared manual annotations of situational power with the organization hierarchy and found that these were not well aligned. Other studies have focused on a more dynamic view of power as arising through asymmetries with respect to needed resources or other goals, as characterized in consent-based theories of power such as exchange theory (Guinote and Vescio 2010). This would include such investigations as identifying persons who are pursuing power (Bracewell, Tomlinson, and Wang 2012; Swayamdipta and Rambow 2012) and detecting influencers (Huffaker 2010; Quercia et al. 2011; Biran et al. 2012; Nguyen et al. 2014). This could also include studying how language use changes when users change their status in online communities (Danescu-Niculescu-Mizil et al. 2012). Depending on the conceptualization of power and the used data set, labels for the relations or roles of individuals have been collected in different ways, such as based on the organizational structure of Enron (Bramsen et al. 2011; Gilbert 2012), the number of followers in Twitter (Danescu-Niculescu-Mizil, Gamon, and Dumais 2011), standings in state and national polls to study power in political debates (Prabhakaran, John,"
J16-3007,P05-1054,0,0.0903046,"ording to gender (Section 3.2), age (Section 3.3), and location (Section 3.4), we conclude with a discussion of how various NLP tasks, such as sentiment detection, can be improved by accounting for language variation related to the social identity of speakers (Section 3.5). 3.1 Data Sources Early computational studies on social identity and language use were based on formal texts, such as the British National Corpus (Koppel, Argamon, and Shimoni 2002; Argamon et al. 2003), or data sets collected from controlled settings, such as recorded conversations (Singh 2001) and telephone conversations (Boulis and Ostendorf 2005; Garera and Yarowsky 2009; Van Durme 2012), where protocols were used to coordinate the conversations (such as the topic). With the advent of social media, a shift is observed towards more informal texts collected from uncontrolled settings. Much of the initial work in this domain focused on blogs. The Blog Authorship Corpus (Schler et al. 2006), collected in 2004 from blogger.com, has been used in various studies on gender and age (Argamon et al. 2007; Goswami, Sarkar, and Rustagi 2009; Gianfortoni, Adamson, and Rosé 2011; Nguyen, Smith, and Rosé 2011; Sap et al. 2014). Others have created t"
J16-3007,P11-1078,0,0.0253323,"he locus of meaning is treated as though it is in the text itself rather than an emergent property of the interaction between speakers. Though some references to external power structures and transient power relationships are mentioned, much room remains for deeper reflection on the connection between power and language. Research in the computational linguistics community related to these issues is normally centered around classification tasks. Earlier studies have focused on hierarchical power relations based on the organizational structure, thereby frequently making use of the Enron corpus. Bramsen et al. (2011) extracted messages between pairs of participants and developed a machine learning classifier to automatically determine whether the messages of an author were UpSpeak (directed towards a person of higher status) or DownSpeak (directed towards a person of lower status). With a slightly different formulation of the task, Gilbert (2012) used logistic regression to classify power relationships in the Enron corpus and identified the most predictive phrases. Besides formulating the task as a classification task, ranking approaches have been explored as well (Diehl, Namata, and Getoor 2007; Prabhaka"
J16-3007,D11-1052,0,0.057984,"types of data that have been used within computational linguistics. Many developed tools (e.g., parsers, named entity recognizers) do not work well because of the informal nature of many social media texts. Although the dominant response has been to focus on text normalization and domain adaptation, Eisenstein (2013b) argues that doing so is throwing away meaningful variation. For example, building on work on text normalization, Gouws et al. (2011) showed how various transformations (e.g., dropping the last character of a word) vary across different user groups on Twitter. As another example, Brody and Diakopoulos (2011) find that lengthening of words (e.g., cooooll) is often applied to subjective words. They build on this observation to detect sentimentbearing words. The tension between normalizing and preserving the variation in text also arises in the processing and analysis of historical texts (see Piotrowski [2012] for an overview), which also contain many spelling variations. In this domain, normalization is often applied as well to facilitate the use of tools such as parsers. However, some approaches first normalize the text, but then replace the modernized word forms with the original word forms to re"
J16-3007,D14-1154,0,0.0146682,"Missing"
J16-3007,D10-1124,0,0.0223199,"Missing"
J16-3007,K15-1011,0,0.0707771,"Missing"
J16-3007,N13-1131,0,0.0302125,"Missing"
J16-3007,W14-5317,0,0.022764,"Missing"
J16-3007,N15-1185,0,0.0649597,"ebruary 2016. doi:10.1162/COLI_a_00258 © 2016 Association for Computational Linguistics Computational Linguistics Volume 42, Number 3 computational science, data-driven exploration and discovery have become a dominant ingredient of many methodological frameworks. In line with these developments, the field of computational linguistics (CL) has also evolved. Human communication occurs in both verbal and nonverbal form. Research on computational linguistics has primarily focused on capturing the informational dimension of language and the structure of verbal information transfer. In the words of Krishnan and Eisenstein (2015), computational linguistics has made great progress in modeling language’s informational dimension, but with a few notable exceptions, computation has had little to contribute to our understanding of language’s social dimension. The recent increase in interest of computational linguists to study language in social contexts is partly driven by the ever increasing availability of social media data. Data from social media platforms provide a strong incentive for innovation in the CL research agenda and the surge in relevant data opens up methodological possibilities for studying text as social da"
J16-3007,P11-2020,0,0.0638038,"Missing"
J16-3007,C12-1102,0,0.0117135,"ained by including the output of the monolingual parsers as features in a machine learning algorithm. Vyas et al. (2014) studied the impact of different preprocessing steps on POS tagging of English–Hindi data collected from Facebook. Language identification and transliteration were the major challenges that impacted POS performance. Language and Topic Models. Language models have been developed to improve speech recognition for mixed-language speech, by adding POS and language information to the language models (Adel, Vu, and Schultz 2013) or by incorporating syntactic inversion constraints (Li and Fung 2012). Peng, Wang, and Dredze (2014) developed a topic model 573 Computational Linguistics Volume 42, Number 3 that infers language-specific topic distributions based on mixed-language text. The main challenge for their model was aligning the inferred topics across languages. 5.3 Analysis and Prediction of Multilingual Communication According to Thomason (2001), Gardner-Chloros and Edwards (2004), and Bhatt and Bolonyai (2011), social factors (e.g., attitudes and motives of the speakers, social and political context) are as important as linguistic factors in multilingual settings. Largescale analys"
J16-3007,P13-1018,0,0.0665914,"r their data was limited to three speakers. Language pairs that have been studied for multilingual communication include English–Hindi (Vyas et al. 2014), Spanish–English (Solorio and Liu 2008a, 2008b; Peng, Wang, and Dredze 2014), Turkish–Dutch (Nguyen and Dogruöz ˘ 2013), Mandarin–English (Adel, Vu, and Schultz 2013; Peng, Wang, and Dredze 2014), and French–English (Jurgens, Dimitrov, and Ruths 2014). Besides being a valuable resource for studies on multilingual social interaction, multilingual texts in social media have also been used to improve general purpose machine translation systems (Ling et al. 2013; Huang and Yates 2014). Processing and analyzing mixed-language data often requires identification of languages at the word level. Language identification is a well-researched problem in CL and we discussed it in the context of dialect identification in Section 3.4.1. Here, we discuss language identification for mixed-language texts. Several data sets are publicly available to stimulate research on language identification in mixed-language texts, including data from the shared task on Language Identification in Code-Switched Data (Solorio et al. 2014) covering four different language pairs on"
J16-3007,Q14-1003,0,0.0217726,"Missing"
J16-3007,W11-2032,0,0.027886,"rception studies (i.e., to study how non-linguists interpret messages and identify social characteristics of speakers [Clopper 2013]), and for the collection of linguistic data, such as the use of variants of linguistic variables. Within sociolinguistics, surveys have been one of the instruments to collect data and crowdsourcing is an emerging alternative to traditional methods for collecting survey data. Crowdsourcing has already been used to obtain perception data for sociolinguistic research—for example, to study how English utterances are perceived differently across language communities (Makatchev and Simmons 2011) and to obtain native-likeness ratings of speech samples (Wieling et al. 2014). For some studies, games have been developed to collect data. Nguyen et al. (2014) studied how Twitter users are perceived based on their tweets by asking players to guess the gender and age based on displayed tweets. Leemann et al. (2016) developed a mobile app that predicted the user’s location based on a 16-question survey. By also collecting user feedback on the predictions, the 549 Computational Linguistics Volume 42, Number 3 authors compared their data with the Linguistic Atlas of German-speaking Switzerland,"
J16-3007,W14-3601,0,0.0189136,"Missing"
J16-3007,D10-1021,0,0.0376377,"Missing"
J16-3007,D13-1084,1,0.0854041,"well as information on the social context, and the development or refinement of NLP tools based on sociolinguistic insights. 540 Nguyen et al. Computational Sociolinguistics: A Survey 1.2 Scope of the Discussion Given the breadth of this field, we will limit the scope of this survey as follows. First of all, the coverage of sociolinguistics topics will be selective and primarily determined by the work within computational linguistics that touches on sociolinguistic topics. For readers with a wish for a more complete overview of sociolinguistics, we recommend the introductory readings by Bell (2013), Holmes (2013), and Meyerhoff (2011). The availability of social media and other online language data in computermediated formats is one of the primary driving factors for the emergence of computational sociolinguistics. A relevant research area is therefore the study of computer-mediated communication (Herring 1996). Considering the strong focus on speech data within sociolinguistics, there is much potential for computational approaches to be applied to spoken language as well. Moreover, the increased availability of recordings of spontaneous speech and transcribed speech has inspired a revi"
J16-3007,W11-0710,1,0.833038,"Missing"
J16-3007,W11-1515,1,0.407265,"Missing"
J16-3007,C14-1184,1,0.754196,"Missing"
J16-3007,C14-2014,1,0.130922,"eveloped by sociolinguists could influence the field of CL also in more fundamental ways. The boundaries of communities are often not as clear-cut as they may seem and the impact of agency has not been sufficiently taken into account in many computational studies. For example, an understanding of linguistic agency can explain why and when there might be more or less of a problem when making inferences about people based on their linguistic choices. This issue is discussed in depth in some recent computational work related to gender, specifically, Bamman, Eisenstein, and Schnoebelen (2014) and Nguyen et al. (2014), who provide a critical reflection on the operationalization of gender in CL studies. The increasing interest in analyzing and modeling the social dimension of language within CL encourages collaboration between sociolinguistics and CL in various ways. However, the potential for synergy between the two fields has not been explored systematically so far (Eisenstein 2013b) and to date there is no overview of the common and complementary aspects of the two fields. This article aims to present an integrated overview of research published in the two communities and to describe the state-ofthe-art"
J16-3007,W15-1104,0,0.0347649,"nd orientations taken up by speakers influence how conversations play out over time and computational approaches to measure accommodation have been used to study power dynamics (Danescu-Niculescu-Mizil, Gamon, and Dumais 2011; Danescu-Niculescu-Mizil et al. 2012; Jones et al. 2014). In a study on power dynamics in Wikipedia Talk pages and Supreme court debates, Danescu-Niculescu-Mizil et al. (2012) found that people with a lower status accommodated more than people with a higher status. In addition, users accommodated less once they became an admin in Wikipedia. Using the same Wikipedia data, Noble and Fernández (2015) found that users accommodated more towards users who occupied a more central position, based on eigenvector and betweenness centrality, in the social network. Furthermore, whether a user was an admin did not have a significant effect on the amount of coordination that highly central users received. From a different angle, Gweon et al. (2013) studied transactive exchange in debate contexts. Transactivity is a property of an assertion that requires that it displays reasoning (e.g., a causal mechanism) and refers to or integrates an idea expressed earlier in the discussion. In this context, high"
J16-3007,W14-3905,1,0.0541563,"practice developed by sociolinguists could influence the field of CL also in more fundamental ways. The boundaries of communities are often not as clear-cut as they may seem and the impact of agency has not been sufficiently taken into account in many computational studies. For example, an understanding of linguistic agency can explain why and when there might be more or less of a problem when making inferences about people based on their linguistic choices. This issue is discussed in depth in some recent computational work related to gender, specifically, Bamman, Eisenstein, and Schnoebelen (2014) and Nguyen et al. (2014), who provide a critical reflection on the operationalization of gender in CL studies. The increasing interest in analyzing and modeling the social dimension of language within CL encourages collaboration between sociolinguistics and CL in various ways. However, the potential for synergy between the two fields has not been explored systematically so far (Eisenstein 2013b) and to date there is no overview of the common and complementary aspects of the two fields. This article aims to present an integrated overview of research published in the two communities and to desc"
J16-3007,D15-1256,0,0.144086,"ssages and threads) that do not correspond entirely with traditional analysis units (such as sentences and turns) (Androutsopoulos 2013). This raises the question about valid application of findings from prior work. Another complicating factor is that in social media the target audience of a message is often not explicitly indicated—namely, multiple audiences (e.g., friends, colleagues) are collapsed into a single context (Marwick and boyd 2011). Some studies have therefore treated the use of hashtags and user mentions as proxies for the target audience (Nguyen, Trieschnigg, and Cornips 2015; Pavalanathan and Eisenstein 2015a). Furthermore, although historically the field of sociolinguistics 548 Nguyen et al. Computational Sociolinguistics: A Survey started with a major focus on phonological variation (e.g., Labov 1966), the use of social media data has led to a higher focus on lexical variation in computational sociolinguistics. However, there are concerns that a focus on lexical variation without regard to other aspects may threaten the validity of conclusions. Phonology does impact social media orthography at both the word level and structural level (Eisenstein 2013a), suggesting that studies on phonological v"
J16-3007,P14-2110,0,0.0723707,"Missing"
J16-3007,W11-0711,0,0.0633704,"Missing"
J16-3007,E14-1012,1,0.868257,"Missing"
J16-3007,I13-1042,0,0.0204336,"Missing"
J16-3007,I13-1025,0,0.0221468,"il et al. 2012). Depending on the conceptualization of power and the used data set, labels for the relations or roles of individuals have been collected in different ways, such as based on the organizational structure of Enron (Bramsen et al. 2011; Gilbert 2012), the number of followers in Twitter (Danescu-Niculescu-Mizil, Gamon, and Dumais 2011), standings in state and national polls to study power in political debates (Prabhakaran, John, and Seligmann 2013), admins and non-admins in Wikipedia (Bender et al. 2011; DanescuNiculescu-Mizil et al. 2012), and manual annotation (Biran et al. 2012; Prabhakaran and Rambow 2013; Nguyen et al. 2014). Many computational approaches within this sphere build on a foundation from pragmatics related to speech act theory (Searle 1969; Austin 1975), which has most commonly been represented in what are typically referred to as conversation, dialog or social acts (Bender et al. 2011; Ferschke, Gurevych, and Chebotar 2012). Such categories can also be combined into sequences (Bracewell, Tomlinson, and Wang 2012). Other specialized representations are also used, such as features related to turn-taking 566 Nguyen et al. Computational Sociolinguistics: A Survey style (Swayamdipta"
J16-3007,N12-1057,0,0.0911042,"Missing"
J16-3007,C12-1138,0,0.0525122,"Missing"
J16-3007,D14-1211,0,0.0439416,"Missing"
J16-3007,P15-1169,0,0.0685626,"Missing"
J16-3007,W12-0211,0,0.0191169,"Missing"
J16-3007,P10-2008,0,0.0354676,"Missing"
J16-3007,P11-1077,0,0.0723896,"rved towards more informal texts collected from uncontrolled settings. Much of the initial work in this domain focused on blogs. The Blog Authorship Corpus (Schler et al. 2006), collected in 2004 from blogger.com, has been used in various studies on gender and age (Argamon et al. 2007; Goswami, Sarkar, and Rustagi 2009; Gianfortoni, Adamson, and Rosé 2011; Nguyen, Smith, and Rosé 2011; Sap et al. 2014). Others have created their own blog corpus from various sources including LiveJournal and Xanga (Burger and Henderson 2006; Nowson and Oberlander 2006; Yan and Yan 2006; Mukherjee and Liu 2010; Rosenthal and McKeown 2011; Sarawgi, Gajulapalli, and Choi 2011). More recent studies are focusing on Twitter data, which contain richer interactions than blogs. Burger et al. (2011) created a large corpus by following links to blogs that contained author information provided by the authors themselves. The data set has been used in various subsequent studies (Van Durme 2012; Bergsma and Van Durme 2013; Volkova, Wilson, and Yarowsky 2013). Others created their own Twitter data set (Rao et al. 2010; Eisenstein, Smith, and Xing 2011; Zamal, Liu, and Ruths 2012; Kokkos and Tzouramanis 2014; Liao et al. 2014). Whereas early"
J16-3007,D14-1121,0,0.0452005,"Missing"
J16-3007,W11-0310,0,0.0543141,"Missing"
J16-3007,D10-1112,0,0.0174647,"equently used in dialect identification (Trieschnigg et al. 2012; Zaidan and Callison-Burch 2013; King, Radev, and Abney 2014). Similarly, many text-based location prediction systems make use of unigram word features (Eisenstein et al. 2010; Wing and Baldridge 2011; Han, Cook, and Baldwin 2012). Features inspired by sociolinguistics could potentially improve performance. Darwish, Sajjad, and Mubarak (2014) showed that for identifying Arabic dialects a better classification performance could be obtained by incorporating known lexical, morphological, and phonological differences in their model. Scherrer and Rambow (2010) also found that using linguistic knowledge improves over an n-gram approach. Their method is based on a linguistic atlas for the extraction of lexical, morphological, and phonetic rules and the likelihood of these forms across German-speaking Switzerland. Dogruöz ˘ and Nakov (2014) explored the use of light verb constructions to distinguish between two Turkish dialects. To support the discovery of new sociolinguistic patterns and to improve prediction performance, several studies have focused on automatically identifying characteristic features of dialects. Han, Cook, and Baldwin (2012) explo"
J16-3007,W04-2003,0,0.0153653,"Missing"
J16-3007,D08-1027,0,0.0190425,"rst normalize the text, but then replace the modernized word forms with the original word forms to retain the original text. Another issue with social media data is that many social media studies have so far focused primarily on one data source. A comparison of the online data sources in terms of language use has only been done in a few studies (Baldwin et al. 2013; Hu, Talamadupula, and Kambhampati 2013). Another up-and-coming promising resource for studying language from a social perspective is crowdsourcing. So far, crowdsourcing is mostly used to obtain large numbers of annotations (e.g., Snow et al. 2008). However, “crowds” can also be used for large-scale perception studies (i.e., to study how non-linguists interpret messages and identify social characteristics of speakers [Clopper 2013]), and for the collection of linguistic data, such as the use of variants of linguistic variables. Within sociolinguistics, surveys have been one of the instruments to collect data and crowdsourcing is an emerging alternative to traditional methods for collecting survey data. Crowdsourcing has already been used to obtain perception data for sociolinguistic research—for example, to study how English utterances"
J16-3007,W14-3907,0,0.123779,"rces or technical support are lacking. Within computational linguistics, there is a growing interest in the automatic processing of mixed-language texts. Lui, Lau, and Baldwin (2014) and Yamaguchi and Tanaka-Ishii (2012) studied automatic language identification in mixed-language documents from Wikipedia by artificially concatenating texts from monolingual sources into multilingual documents. However, such approaches lead to artificial language boundaries. More recently, social media (such as Facebook [Vyas et al. 2014], Twitter [Jurgens, Dimitrov, and Ruths 2014; Peng, Wang, and Dredze 2014; Solorio et al. 2014] and online forums [Nguyen and Dogruöz ˘ 2013]) provide large volumes of data for analyzing multilingual communication in social interaction. Transcriptions of conversations have been explored by Solorio and Liu (2008b), however their data was limited to three speakers. Language pairs that have been studied for multilingual communication include English–Hindi (Vyas et al. 2014), Spanish–English (Solorio and Liu 2008a, 2008b; Peng, Wang, and Dredze 2014), Turkish–Dutch (Nguyen and Dogruöz ˘ 2013), Mandarin–English (Adel, Vu, and Schultz 2013; Peng, Wang, and Dredze 2014), and French–English (J"
J16-3007,D08-1102,0,0.0161943,"Missing"
J16-3007,D08-1110,0,0.014047,"Missing"
J16-3007,E14-1034,0,0.0473728,"Missing"
J16-3007,C12-1155,0,0.0259337,"build on a foundation from pragmatics related to speech act theory (Searle 1969; Austin 1975), which has most commonly been represented in what are typically referred to as conversation, dialog or social acts (Bender et al. 2011; Ferschke, Gurevych, and Chebotar 2012). Such categories can also be combined into sequences (Bracewell, Tomlinson, and Wang 2012). Other specialized representations are also used, such as features related to turn-taking 566 Nguyen et al. Computational Sociolinguistics: A Survey style (Swayamdipta and Rambow 2012; Prabhakaran, John, and Seligmann 2013), topic control (Strzalkowski et al. 2012; Nguyen et al. 2014; Prabhakaran, Arora, and Rambow 2014), and “overt displays of power,” which Prabhakaran, Rambow, and Diab (2012a) define as utterances that constrain the addressee’s actions beyond what the underlying dialog act imposes. Politeness. Polite behavior contributes to maintaining social harmony and avoiding social conflict (Holmes 2013). Automatic classifiers to detect politeness have been developed to study politeness strategies on a large scale. According to a politeness theory by Brown and Levinson (1987), three social factors influence linguistically polite behavior: social"
J16-3007,D12-1005,0,0.0723941,"Missing"
J16-3007,D13-1187,0,0.03298,"Missing"
J16-3007,voss-etal-2014-finding,0,0.0135448,"erns in multilingual communication (Jurgens, Dimitrov, and Ruths 2014; Kim et al. 2014; Papalexakis, Nguyen, and Dogruöz ˘ 2014). Most of the earlier research on automatic language identification focused on documentlevel identification of a single language (Baldwin and Lui 2010). To handle mixedlanguage texts, more fine-grained approaches have been explored, ranging from language identification at the sentence (Elfardy and Diab 2013; Zaidan and CallisonBurch 2013; Zampieri et al. 2014) and word level (Elfardy and Diab 2012b; King and Abney 2013; Nguyen and Dogruöz ˘ 2013; Solorio et al. 2014; Voss et al. 2014), approaches for text segmentation (Yamaguchi and Tanaka-Ishii 2012), and estimating the proportion of the various languages used within documents (Prager 1999; Lui, Lau, and Baldwin 2014). Depending on the application, different approaches may be suitable, but studies that analyze patterns in multilingual communication have mostly focused on wordlevel identification (Nguyen and Dogruöz ˘ 2013; Solorio et al. 2014). Off-the-shelf tools developed for language identification at the document level (e.g., the TextCat program [Cavnar and Trenkle 1994]) are not effective for word-level identificatio"
J16-3007,D14-1105,0,0.0873769,"c analysis of this type of data has been difficult for most languages, especially when resources or technical support are lacking. Within computational linguistics, there is a growing interest in the automatic processing of mixed-language texts. Lui, Lau, and Baldwin (2014) and Yamaguchi and Tanaka-Ishii (2012) studied automatic language identification in mixed-language documents from Wikipedia by artificially concatenating texts from monolingual sources into multilingual documents. However, such approaches lead to artificial language boundaries. More recently, social media (such as Facebook [Vyas et al. 2014], Twitter [Jurgens, Dimitrov, and Ruths 2014; Peng, Wang, and Dredze 2014; Solorio et al. 2014] and online forums [Nguyen and Dogruöz ˘ 2013]) provide large volumes of data for analyzing multilingual communication in social interaction. Transcriptions of conversations have been explored by Solorio and Liu (2008b), however their data was limited to three speakers. Language pairs that have been studied for multilingual communication include English–Hindi (Vyas et al. 2014), Spanish–English (Solorio and Liu 2008a, 2008b; Peng, Wang, and Dredze 2014), Turkish–Dutch (Nguyen and Dogruöz ˘ 2013), Ma"
J16-3007,Q14-1024,0,0.0135156,"berg 2010), which involves edges that are directed and reflect status differences. Hassan, Abu-Jbara, and Radev (2012) developed a machine learning classifier to extract signed social networks and found that the extracted network structure mostly agreed with Structural Balance Theory. Krishnan and Eisenstein (2015) proposed an unsupervised model for extracting signed social networks, which they used to extract formal and informal relations in a movie-script corpus. Furthermore, their model also induced the social function of address terms (e.g., dude). To infer edge signs in a social network, West et al. (2014) formulated an optimization problem that combined two objectives, capturing the extent to which the inferred signs agreed with the predictions of a sentiment analysis model, and the extent to which the resulting triangles corresponded with Status and Structural Balance Theory. Power. Work on power relations draws from social psychological concepts of relative power in social situations (Guinote and Vescio 2010), in particular, aspects of relative power that operate at the level of individuals in relation to specific others within groups or communities. Relative power may be thought of as opera"
J16-3007,W10-2305,0,0.0180829,"ed, referred to as dialectometry (Wieling and Nerbonne 2015). In contrast to dialectology, which focuses on individual linguistic variables, dialectometry involves aggregating linguistic variables to examine linguistic differences between regions. Nerbonne (2009) argues that studies that focus on individual variables are sensitive to noise and that therefore aggregating linguistic variables will result in more reliable signals. This aggregation step has led to the introduction of various statistical methods, including clustering, dimensionality reduction techniques, and regression approaches (Wieling and Nerbonne 2010; Heeringa and Nerbonne 2013; Nerbonne and Wieling 2015). Recently, researchers within dialectometry have explored the automatic identification of characteristic features of dialect regions (Wieling and Nerbonne 2010), a task that aligns more closely with the approaches taken by dialectologists. Although the data sets typically used in dialectology and dialectometry studies are still small compared to data sets used in computational linguistics, similar statistical methods have been explored. This has created a promising starting point for closer collaboration with computational linguistics. 3"
J16-3007,P11-1096,0,0.0273277,"f the research that starts with location-tagged data is done with the aim of automatically predicting the location of speakers. The set-up is thus similar to the set-up for the other tasks that we have surveyed in this section (e.g., gender and age prediction). Eisenstein et al. (2010) developed a topic model to identify geographically coherent linguistic regions and words that are highly associated with these regions. The model was tested by predicting the locations of Twitter users based on their tweets. Although the topic of text-based location prediction has received increasing attention (Wing and Baldridge 2011; Han, Cook, and Baldwin 2012), using these models for the discovery of new sociolinguistic patterns is an option that has not been fully explored yet, since most studies primarily focus on prediction performance. Various approaches have been explored to model the location of speakers, an aspect that is essential in many of the studies that start with location-tagged data. In Wing and Baldridge (2011), locations are modeled using geodesic grids, but these grids do not always correspond to administrative or language boundaries. Users can also be grouped based on cities (Han, Cook, and Baldwin 2"
J16-3007,W02-0110,0,0.0160678,"rge-scale statistical modeling, symbolic- and knowledge-driven methods have been largely left aside, though the presence of linguistics as an active force can still be seen in some areas of computational linguistics, such as tree banking. Along with older symbolic methods that required carefully crafted grammars and lexicons, the concept of knowledge source has become strongly associated with the notion of theory, which is consistent with the philosophical notion of linguistic theory advocated by Chomskyan linguistics and other formal linguistic theories (Green 1992; Backofen and Smolka 1993; Wintner 2002; Schneider, Dowdall, and Rinaldi 2004). As knowledge-based methods have to a large extent been replaced with statistical models, a grounding in linguistic theory has become less and less valued. A desire to replace theory with empiricism dominated the zeitgeist and drove progress within the field. Currently, the term theory seems to be associated with old and outdated approaches. It often has a negative connotation in contrast to the positive reception of empiricism, and contemporary modeling approaches are believed to have a greater ability to offer insights into language than symbolic model"
J16-3007,P12-1102,0,0.143114,"times (Dogruöz ˘ and Backus 2007, 2009) or from the same group of speakers longitudinally (Milroy and Milroy 1978; Trudgill 2003). The manual transcription and annotation of data is time-intensive and costly. Multilingual data from online environments is usually extracted in small volumes and for short periods. Automatic analysis of this type of data has been difficult for most languages, especially when resources or technical support are lacking. Within computational linguistics, there is a growing interest in the automatic processing of mixed-language texts. Lui, Lau, and Baldwin (2014) and Yamaguchi and Tanaka-Ishii (2012) studied automatic language identification in mixed-language documents from Wikipedia by artificially concatenating texts from monolingual sources into multilingual documents. However, such approaches lead to artificial language boundaries. More recently, social media (such as Facebook [Vyas et al. 2014], Twitter [Jurgens, Dimitrov, and Ruths 2014; Peng, Wang, and Dredze 2014; Solorio et al. 2014] and online forums [Nguyen and Dogruöz ˘ 2013]) provide large volumes of data for analyzing multilingual communication in social interaction. Transcriptions of conversations have been explored by Solo"
J16-3007,P15-1161,1,0.874302,"Missing"
J16-3007,W14-5307,0,0.029959,"Missing"
J16-3007,W15-5401,0,0.0195697,"Missing"
J16-3007,C12-2029,0,\N,Missing
J16-3007,E12-1079,0,\N,Missing
J16-3007,E12-1080,1,\N,Missing
J16-3007,W06-3403,0,\N,Missing
J16-3007,C82-1023,0,\N,Missing
J16-3007,N09-1072,0,\N,Missing
J16-3007,N13-1037,0,\N,Missing
J16-3007,P09-1080,0,\N,Missing
J16-3007,W11-0609,0,\N,Missing
J16-3007,C14-1044,0,\N,Missing
J16-3007,P12-2027,0,\N,Missing
J16-3007,P11-1137,0,\N,Missing
J16-3007,J96-2004,0,\N,Missing
J16-3007,P13-1025,0,\N,Missing
J16-3007,J14-1006,0,\N,Missing
J16-3007,D12-1135,0,\N,Missing
J16-3007,D14-1157,0,\N,Missing
J16-3007,P15-2079,0,\N,Missing
J16-3007,W14-3906,0,\N,Missing
J16-3007,W11-2606,1,\N,Missing
J16-3007,D14-1145,1,\N,Missing
J16-3007,E14-1011,0,\N,Missing
J16-3007,elfardy-diab-2012-simplified,0,\N,Missing
J16-3007,E14-1001,0,\N,Missing
J16-3007,D15-1254,0,\N,Missing
J16-3007,P13-2081,0,\N,Missing
J16-3007,W15-4302,0,\N,Missing
J16-3007,P13-2037,0,\N,Missing
J16-3007,P15-1073,0,\N,Missing
J16-3007,P05-2023,0,\N,Missing
J16-3007,D11-1120,0,\N,Missing
J16-3007,N13-1080,1,\N,Missing
J16-3007,D12-1119,1,\N,Missing
J16-3007,hughes-etal-2006-reconsidering,0,\N,Missing
J16-3007,D12-1006,0,\N,Missing
J16-3007,P93-1026,0,\N,Missing
J16-3007,P13-1070,0,\N,Missing
J16-3007,J15-4006,0,\N,Missing
J16-3007,D13-1114,0,\N,Missing
J16-3007,C12-1064,0,\N,Missing
J16-3007,W13-1102,0,\N,Missing
J16-3007,N15-1019,0,\N,Missing
K19-1033,D14-1059,0,0.0211661,", straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we nee"
K19-1033,W03-0906,0,0.209086,"Missing"
K19-1033,W17-5310,0,0.012966,"ide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge tasks has already made headway in defining tasks for subproblems such as lexical inference with hypernymy, co-hyponymy, antonymy (Glockner et al., 2018; Naik et"
K19-1033,D17-1070,0,0.0604839,"attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge tasks has already made headway in defining tasks for subproblems such as lexical inference with hypernymy, co-hyponymy, antonymy (Glockner e"
K19-1033,bentivogli-etal-2010-building,0,0.0745212,"Missing"
K19-1033,H05-1079,0,0.113473,"Missing"
K19-1033,P08-1118,0,0.109261,"Missing"
K19-1033,D15-1075,0,0.0536693,"ovides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge task"
K19-1033,P17-1152,0,0.282638,"ingent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge tasks has already made headway in defining tasks for subproblems such as lexical inference with hypernymy, co-hyponymy, antonymy (Glockner et al., 2018; Naik et al., 2018). In this"
K19-1033,N19-1246,0,0.152828,"Missing"
K19-1033,P14-1026,0,0.142336,"lenge area for textual entailment stands. 2 While to the best of our knowledge, prior work has not studied quantitative reasoning in NLI, Roy (2017) propose a model for a related subtask called quantity entailment, which aims to determine if a given quantity can be inferred from a sentence. In contrast, our work is concerned with general-purpose textual entailment which considers if a given sentence can be inferred from another. Our work also relates to solving arithmetic word problems (Hosseini et al., 2014; Mitra and Baral, 2016; Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017; Kushman et al., 2014a; Koncel-Kedziorski et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glic"
K19-1033,W07-1401,0,0.0672176,"focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the f"
K19-1033,P18-2103,0,0.0574716,"al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge tasks has already made headway in defining tasks for subproblems such as lexical inference with hypernymy, co-hyponymy, antonymy (Glockner et al., 2018; Naik et al., 2018). In this work, we specifically probe into quantitative reasoning. Recently, Dua et al. (2019) also recognize the importance of quantitative reasoning for text understanding. They propose DROP, a reading comprehension dataset focused on a limited set of discrete operations such as counting, comparison, sorting and arithmetic. In contrast, EQUATE features diverse phenomena that occur naturally in text, including reasoning with approximation, ordinals, implicit quantities and quantifiers, requiring NLI models to reason comprehensively about the interplay between quantities an"
K19-1033,N18-2017,0,0.0321211,"list (Pc ∪ Pr ∪ H ∪ O) TL Type list (set of types from Pc , Pr , H) N Length of symbol list K Index of first range quantity in symbol list M Index of first operator in symbol list OUTPUT ei Index of symbol assigned to ith position in postfix equation VARIABLES xi Main ILP variable for position i ci Indicator variable: is ei a single value? ri Indicator variable: is ei a range? oi Indicator variable: is ei an operator? di Stack depth of ei ti Type index for ei Figure 1: Overview of Q-REAS baseline. 2) Hypothesis-Only (HYP): FastText classifier (Mikolov et al., 2018) trained on only hypotheses (Gururangan et al., 2018). 3) ALIGN: A bag-of-words alignment model inspired by MacCartney (2009).10 4) CBOW: A simple bag-of-embeddings sentence representation model (wil). 5) BiLSTM: The simple BiLSTM model described by wil. 6) Chen (CH): Stacked BiLSTM-RNNs with shortcut connections and character word embeddings (Chen et al., 2017b). 7) InferSent: A single-layer BiLSTM-RNN model with max-pooling (Conneau et al., 2017). 8) SSEN: Stacked BiLSTM-RNNs with shortcut connections (Nie and Bansal, 2017). 9) ESIM: Sequential inference model proposed by Chen et al. (2017a) which uses BiLSTMs with an attention mechanism. 10)"
K19-1033,P17-1015,0,0.268598,"owledge, prior work has not studied quantitative reasoning in NLI, Roy (2017) propose a model for a related subtask called quantity entailment, which aims to determine if a given quantity can be inferred from a sentence. In contrast, our work is concerned with general-purpose textual entailment which considers if a given sentence can be inferred from another. Our work also relates to solving arithmetic word problems (Hosseini et al., 2014; Mitra and Baral, 2016; Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017; Kushman et al., 2014a; Koncel-Kedziorski et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006;"
K19-1033,H05-1049,0,0.0345695,"i et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task lik"
K19-1033,P06-1114,0,0.0668325,"y, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which"
K19-1033,W07-1407,0,0.0184018,"ment for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones"
K19-1033,marelli-etal-2014-sick,0,0.0373596,"f-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will"
K19-1033,D14-1058,0,0.0373675,"tween linguistic and numerical reasoning. The EQUATE evaluation framework makes it clear where this new challenge area for textual entailment stands. 2 While to the best of our knowledge, prior work has not studied quantitative reasoning in NLI, Roy (2017) propose a model for a related subtask called quantity entailment, which aims to determine if a given quantity can be inferred from a sentence. In contrast, our work is concerned with general-purpose textual entailment which considers if a given sentence can be inferred from another. Our work also relates to solving arithmetic word problems (Hosseini et al., 2014; Mitra and Baral, 2016; Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017; Kushman et al., 2014a; Koncel-Kedziorski et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted communit"
K19-1033,L18-1008,0,0.0338838,"⊆} L Length of equation to be generated SL Symbol list (Pc ∪ Pr ∪ H ∪ O) TL Type list (set of types from Pc , Pr , H) N Length of symbol list K Index of first range quantity in symbol list M Index of first operator in symbol list OUTPUT ei Index of symbol assigned to ith position in postfix equation VARIABLES xi Main ILP variable for position i ci Indicator variable: is ei a single value? ri Indicator variable: is ei a range? oi Indicator variable: is ei an operator? di Stack depth of ei ti Type index for ei Figure 1: Overview of Q-REAS baseline. 2) Hypothesis-Only (HYP): FastText classifier (Mikolov et al., 2018) trained on only hypotheses (Gururangan et al., 2018). 3) ALIGN: A bag-of-words alignment model inspired by MacCartney (2009).10 4) CBOW: A simple bag-of-embeddings sentence representation model (wil). 5) BiLSTM: The simple BiLSTM model described by wil. 6) Chen (CH): Stacked BiLSTM-RNNs with shortcut connections and character word embeddings (Chen et al., 2017b). 7) InferSent: A single-layer BiLSTM-RNN model with max-pooling (Conneau et al., 2017). 8) SSEN: Stacked BiLSTM-RNNs with shortcut connections (Nie and Bansal, 2017). 9) ESIM: Sequential inference model proposed by Chen et al. (2017a)"
K19-1033,D17-1084,0,0.0146358,"where this new challenge area for textual entailment stands. 2 While to the best of our knowledge, prior work has not studied quantitative reasoning in NLI, Roy (2017) propose a model for a related subtask called quantity entailment, which aims to determine if a given quantity can be inferred from a sentence. In contrast, our work is concerned with general-purpose textual entailment which considers if a given sentence can be inferred from another. Our work also relates to solving arithmetic word problems (Hosseini et al., 2014; Mitra and Baral, 2016; Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017; Kushman et al., 2014a; Koncel-Kedziorski et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al."
K19-1033,P16-1202,0,0.0313731,"merical reasoning. The EQUATE evaluation framework makes it clear where this new challenge area for textual entailment stands. 2 While to the best of our knowledge, prior work has not studied quantitative reasoning in NLI, Roy (2017) propose a model for a related subtask called quantity entailment, which aims to determine if a given quantity can be inferred from a sentence. In contrast, our work is concerned with general-purpose textual entailment which considers if a given sentence can be inferred from another. Our work also relates to solving arithmetic word problems (Hosseini et al., 2014; Mitra and Baral, 2016; Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017; Kushman et al., 2014a; Koncel-Kedziorski et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a st"
K19-1033,C18-1198,1,0.931097,"l., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge tasks has already made headway in defining tasks for subproblems such as lexical inference with hypernymy, co-hyponymy, antonymy (Glockner et al., 2018; Naik et al., 2018). In this work, we specifically probe into quantitative reasoning. Recently, Dua et al. (2019) also recognize the importance of quantitative reasoning for text understanding. They propose DROP, a reading comprehension dataset focused on a limited set of discrete operations such as counting, comparison, sorting and arithmetic. In contrast, EQUATE features diverse phenomena that occur naturally in text, including reasoning with approximation, ordinals, implicit quantities and quantifiers, requiring NLI models to reason comprehensively about the interplay between quantities and language. Ad350 di"
K19-1033,W17-5308,0,0.0711571,". Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge tasks has already made headway in defining tasks for subproblems such as lexical inference with hypernymy, co-hyponymy"
K19-1033,D16-1244,0,0.108172,"Missing"
K19-1033,P10-1122,0,0.0774486,"Missing"
K19-1033,D16-1029,0,0.0127033,"ramework makes it clear where this new challenge area for textual entailment stands. 2 While to the best of our knowledge, prior work has not studied quantitative reasoning in NLI, Roy (2017) propose a model for a related subtask called quantity entailment, which aims to determine if a given quantity can be inferred from a sentence. In contrast, our work is concerned with general-purpose textual entailment which considers if a given sentence can be inferred from another. Our work also relates to solving arithmetic word problems (Hosseini et al., 2014; Mitra and Baral, 2016; Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017; Kushman et al., 2014a; Koncel-Kedziorski et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understa"
K19-1033,D15-1096,0,0.0349587,"Missing"
K19-1033,E06-1052,0,\N,Missing
K19-1033,Q15-1001,0,\N,Missing
K19-1033,W09-2501,0,\N,Missing
K19-1033,Q15-1042,0,\N,Missing
K19-1033,W17-5301,0,\N,Missing
K19-1033,S18-2023,0,\N,Missing
N03-2030,P98-1032,0,0.0245837,"We present CarmelTC, a novel hybrid text classification approach for automatic essay grading. Our evaluation demonstrates that the hybrid CarmelTC approach outperforms two “bag of words” approaches, namely LSA and a Naive Bayes, as well as a purely symbolic approach. 1 Introduction In this paper we describe CarmelTC , a novel automatic essay grading approach using a hybrid text classification technique for analyzing essay answers to qualitative physics questions inside the Why2 tutorial dialogue system (VanLehn et al., 2002). In contrast to many previous approaches to automated essay grading (Burstein et al., 1998; Foltz et al., 1998; Larkey, 1998), our goal is not to assign a letter grade to student essays. Instead, our purpose is to tally which set of “correct answer aspects” are present in student essays. Previously, tutorial dialogue systems such as A UTO -T UTOR (Wiemer-Hastings et al., 1998) and Research Methods Tutor (Malatesta et al., 2002) have used LSA (Landauer et al., 1998) to perform the same type of content analysis for student essays that we do in Why2. While Bag of Words approaches such as LSA have performed successfully on the content analysis task in domains such as Computer Literacy"
N03-2030,A00-2041,1,0.881074,"Missing"
N03-2030,C98-1032,0,\N,Missing
N06-2003,N04-1015,0,0.0130412,"xtTiling and Foltz’s approach measure coherence as a function of the repetition of thematically-related terms. TextTiling looks for cooccurrences of terms or term-stems and Foltz uses LSA to measure semantic relatedness between terms. Olney and Cai’s orthonormal basis approach also uses LSA, but allows a richer representation of discourse coherence, which is that coherence is a function of how much new information a discourse unit (e.g. a dialogue contribution) adds (informativity) and how relevant it is to the local context (relevance) (Olney and Cai, 2005). Content-oriented models, such as (Barzilay and Lee, 2004), rely on the re-occurrence of patterns of topics over multiple realizations of thematically similar discourses, such as a series of newspaper articles about similar events. Their approach utilizes a hidden Markov model where states correspond to topics, and state transition probabilities correspond to topic shifts. To obtain the desired 9 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 9–12, c New York, June 2006. 2006 Association for Computational Linguistics number of topics (states), text spans of uniform length (individual contributi"
N06-2003,J97-1003,0,0.100889,"to dialogue. We demonstrate a significant advantage of Museli over competing approaches. We then discuss why models based entirely on lexical cohesion fail on dialogue and how our algorithm compensates with other topic shift indicators. 2 Previous Work Existing topic segmentation approaches can be loosely classified into two types: (1) lexical cohesion models, and (2) content-oriented models. The underlying assumption in lexical cohesion models is that a shift in term distribution signals a shift in topic (Halliday and Hassan, 1976). The best known algorithm based on this idea is TextTiling (Hearst, 1997). In TextTiling, a sliding window is passed over the vector-space representation of the text. At each position, the cosine correlation between the upper and lower region of the sliding window is compared with that of the peak cosine correlation values to the left and right of the window. A segment boundary is predicted when the magnitude of the difference exceeds a threshold. One drawback to relying on term co-occurrence to signal topic continuity is that synonyms or related terms are treated as thematically-unrelated. One solution to this problem is using a dimensionality reduction technique"
N06-2003,H05-1122,0,0.159994,"n the upper and lower region of the sliding window is compared with that of the peak cosine correlation values to the left and right of the window. A segment boundary is predicted when the magnitude of the difference exceeds a threshold. One drawback to relying on term co-occurrence to signal topic continuity is that synonyms or related terms are treated as thematically-unrelated. One solution to this problem is using a dimensionality reduction technique such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997). Two such algorithms for segmentation are described in (Foltz, 1998) and (Olney and Cai, 2005). Both TextTiling and Foltz’s approach measure coherence as a function of the repetition of thematically-related terms. TextTiling looks for cooccurrences of terms or term-stems and Foltz uses LSA to measure semantic relatedness between terms. Olney and Cai’s orthonormal basis approach also uses LSA, but allows a richer representation of discourse coherence, which is that coherence is a function of how much new information a discourse unit (e.g. a dialogue contribution) adds (informativity) and how relevant it is to the local context (relevance) (Olney and Cai, 2005). Content-oriented models,"
N06-2003,P93-1020,0,0.121643,"Missing"
N06-4001,W05-0208,1,\N,Missing
N09-2010,P08-2004,0,0.0250882,"our definition, 1. The camera comes with a lexar 16mb starter card, which stores about 10 images in fine mode at the highest resolution. 2. I sent my camera to nikon for servicing, took them a whole 6 weeks to diagnose the problem. 3. I find this to be a great feature. The first example is a fact about the camera. The second example is a report of an event. The third example is a self-attributed opinion of the reviewer. Bald claims on the other hand are non-factual claims that are open to interpretation and thus cannot 38 The two categories for gradable words defined above are similar to what Chen (2008) describes as vagueness, non-objective measurability and imprecision. 4 Related work Initial work by Hu and Liu (2004) on the product review data that we have used in this paper focuses on the task of opinion mining. They propose an approach to summarize product reviews by identifying opinionated statements about the features of a product. In our annotation scheme however, we classify 2 www.cs.cmu.edu/˜shilpaa/datasets/ opinion-claims/qbclaims-manual-v1.0.pdf 3 http://en.wikipedia.org/wiki/English_ grammar#Semantic_gradability all claims in a review, not restricting to comments with feature me"
N09-2010,P06-2063,0,0.0613693,"not apply to a wide customer base, versus qualified claims that limit their scope by making some assumptions explicit. Research in analyzing subjectivity of text by Wiebe et al. (2005) involves identifying expression of private states that cannot be objectively verified (and are therefore open to interpretation). However, our task differs from subjectivity analysis, since both bald as well as qualified claims can involve subjective language. Specifically, objective statements are always categorized as qualified claims, but subjective statements can be either bald or qualified claims. Work by Kim and Hovy (2006) involves extracting pros and cons from customer reviews and as in the case of our task, these pros and cons can be either subjective or objective. In supervised machine learning approaches to opinion mining, the results using longer n-grams and syntactic knowledge as features have been both positive as well as negative (Gamon, 2004; Dave et al., 2003). In our work, we show that the qualified vs. bald claims distinction can benefit from using syntactic features. 5 Data and Annotation Procedure We applied our annotation scheme to the product review dataset4 released by Hu and Liu (2004). We ann"
N09-2010,P08-2037,0,\N,Missing
N09-2010,C04-1121,0,\N,Missing
N09-2037,P06-2019,0,0.0144398,"uding additional, more sophisticated syntactically motivated features than those included in previously published models. In this paper, we evaluate the gold standard corpus itself using similar syntactic grammar policies. 3 Grammar Policy Extraction In the domain of Sentence Compression, the corpus consists of source sentences each paired with a gold standard compressed sentence. Most of the above related work has been evaluated using the following 2 corpora, namely the Ziff-Davis (ZD) set (Knight and Marcu, 2002) consisting of 1055 sentences, and a partial Broadcast News Corpus (CL Corpus) (Clarke and Lapata, 2006) originally consisting of 1619 sentences, of which we used 1070 as the training set in our development work as well as in the error analysis below. Hence, we use these two popular corpora to present our work. We hypothesize certain grammar policies that intuitively should be followed while deriving the target-compressed sentence from the source sen146 tence if the mapping between source and target sentences is produced via grammatical transformations. The basic idea behind these policies grows out of the same ideas motivating the syntactic features used in McDonald (2006). These policies, extr"
N09-2037,P06-1048,0,0.0139328,"uding additional, more sophisticated syntactically motivated features than those included in previously published models. In this paper, we evaluate the gold standard corpus itself using similar syntactic grammar policies. 3 Grammar Policy Extraction In the domain of Sentence Compression, the corpus consists of source sentences each paired with a gold standard compressed sentence. Most of the above related work has been evaluated using the following 2 corpora, namely the Ziff-Davis (ZD) set (Knight and Marcu, 2002) consisting of 1055 sentences, and a partial Broadcast News Corpus (CL Corpus) (Clarke and Lapata, 2006) originally consisting of 1619 sentences, of which we used 1070 as the training set in our development work as well as in the error analysis below. Hence, we use these two popular corpora to present our work. We hypothesize certain grammar policies that intuitively should be followed while deriving the target-compressed sentence from the source sen146 tence if the mapping between source and target sentences is produced via grammatical transformations. The basic idea behind these policies grows out of the same ideas motivating the syntactic features used in McDonald (2006). These policies, extr"
N09-2037,P05-1012,0,0.0894033,"Missing"
N09-2037,E06-1038,0,0.189569,"Missing"
N09-2037,P05-1036,0,0.0352409,"ity Pittsburgh, PA 15213, USA {nkgupta,sourishc,cprose}@cs.cmu.edu Abstract We present a policy-based error analysis approach that demonstrates a limitation to the current commonly adopted paradigm for sentence compression. We demonstrate that these limitations arise from the strong assumption of locality of the decision making process in the search for an acceptable derivation in this paradigm. 1 Introduction In this paper we present a policy-based error analysis approach that demonstrates a limitation to the current commonly adopted paradigm for sentence compression (Knight and Marcu, 2000; Turner and Charniak, 2005; McDonald, 2006; Clark and Lapata 2006). Specifically, in typical statistical compression approaches, a simplifying assumption is made that compression is accomplished strictly by means of word deletion. Furthermore, each sequence of contiguous words that are dropped from a source sentence is considered independently of other sequences of words dropped from other portions of the sentence, so that the features that predict whether deleting a sequence of words is preferred or not is based solely on local considerations. This simplistic approach allows all possible derivations to be modeled and"
N09-5002,H05-1029,0,0.0292001,"ccessing information from a database over the telephone. Other common applications of conversational agents include computer aided instruction (CAI) and human-robot interaction (HRI). Conversational Agents in most of today’s SDS, CAI and HRI are designed to work within the scope of specific task domains which allows the scientists and engineers working on such systems to ensure satisfactory and relevant interaction with the user most of the time. Within the task domain, such agents can display intelligent interactive behavior like helping the user use the interface, asking remedial questions (Bohus and Rudnicky, 2005), shaping the user behavior (Tomko and Rosenfeld, 2004) by using alternative phrasing of utterances, responding to user affect (D’Mello et al., 2008) through text, voice and gesture, engaging the user through the display of presence via backchannels (Ward, 1996) and embodiment (Cassell et al., 1999). As more and more of these intelligent interactive agents get built for many task domains (Raux et al., 2005; Bohus et al., 2007; Gockley et al., 2005; Amtrak Julie; …) that surround our everyday life, we observe a gradual transition in the use of the conversational agent technology to be a form of"
N09-5002,W08-0114,0,\N,Missing
N09-5002,N07-2003,1,\N,Missing
N09-5002,W07-0305,0,\N,Missing
N10-1098,N09-5002,1,\N,Missing
N10-1098,W09-3932,0,\N,Missing
N13-1080,P07-1056,1,0.766444,"t the domains induced by that metadata attribute. Each instance xi is drawn from a distribution xi ∼ Da specific to a set of attribute values Ai associated with each instance. Additionally, each unique set of attributes indexes a function fA .1 Ai could contain a value for each attribute, or no values for any attribute (which would index a domain-agnostic “background” distribution and labeling function). Just as a domain can change a feature’s probability and behavior, so can each metadata attribute. Examples of data for MAMD learning abound. The commonly used Amazon product reviews data set (Blitzer et al., 2007) only includes product types, but the original reviews can be attributed with author, product price, brand, and so on. Additional examples include congressional floor debate records (e.g. political party, speaker, bill) (Joshi et al., 2012). In this paper, we use restaurant reviews (Chahuneau et al., 2012), which have upto 20 metadata attributes that define domains, and congressional floor debates, with two attributes that define domains. It is difficult to apply multi-domain learning algorithms when it is unclear which metadata attribute to choose for defining the “domains”. It is possible th"
N13-1080,D12-1124,0,0.333936,"metadata attributes). We introduce the multi-attribute multi-domain (MAMD) learning problem, in which each learning instance is associated with multiple metadata attributes, each of which may impact feature behavior. We present extensions to two popular multi-domain learning algorithms, FEDA (Daum´e III, 2007) and MDR (Dredze et al., 2009). Rather than selecting a single domain division, our algorithms consider all attributes as possible distinctions and discover changes in features across attributes. We evaluate our algorithms using two different data sets – a data set of restaurant reviews (Chahuneau et al., 2012), and a dataset of transcribed speech segments from floor debates in the United States Congress (Thomas et al., 2006). We demonstrate that multi-attribute algorithms improve over their multi-domain counterparts, which can learn distinctions from only a single attribute. 2 MAMD Learning In multi-domain learning, each instance x is drawn from a domain d with distribution x ∼ Dd over a vectors space RD and labeled with a domain specific function fd with label y ∈ {−1, +1} (for binary classification). In multi-attribute multi-domain 685 Proceedings of NAACL-HLT 2013, pages 685–690, c Atlanta, Geor"
N13-1080,P07-1033,0,0.690089,"Missing"
N13-1080,D08-1072,1,0.890708,"-world datasets often have multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute. 1 Introduction Multi-Domain Learning (Evgeniou and Pontil, 2004; Daum´e III, 2007; Dredze and Crammer, 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011) algorithms learn when training instances are spread across many domains, which impact model parameters. These algorithms use examples from each domain to learn a general model that is also sensitive to individual domain differences. However, many data sets include a host of metadata attributes, many of which can potentially define the domains to use. Consider the case of restaurant reviews, which can be categorized into domains corresponding to the cuisine, location, price range, or several other factors. For multi-domain le"
N13-1080,N09-1068,0,0.194056,"multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute. 1 Introduction Multi-Domain Learning (Evgeniou and Pontil, 2004; Daum´e III, 2007; Dredze and Crammer, 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011) algorithms learn when training instances are spread across many domains, which impact model parameters. These algorithms use examples from each domain to learn a general model that is also sensitive to individual domain differences. However, many data sets include a host of metadata attributes, many of which can potentially define the domains to use. Consider the case of restaurant reviews, which can be categorized into domains corresponding to the cuisine, location, price range, or several other factors. For multi-domain learning, we should use the"
N13-1080,D12-1119,1,0.876767,"Missing"
N13-1080,W06-1639,0,0.150433,"ance is associated with multiple metadata attributes, each of which may impact feature behavior. We present extensions to two popular multi-domain learning algorithms, FEDA (Daum´e III, 2007) and MDR (Dredze et al., 2009). Rather than selecting a single domain division, our algorithms consider all attributes as possible distinctions and discover changes in features across attributes. We evaluate our algorithms using two different data sets – a data set of restaurant reviews (Chahuneau et al., 2012), and a dataset of transcribed speech segments from floor debates in the United States Congress (Thomas et al., 2006). We demonstrate that multi-attribute algorithms improve over their multi-domain counterparts, which can learn distinctions from only a single attribute. 2 MAMD Learning In multi-domain learning, each instance x is drawn from a domain d with distribution x ∼ Dd over a vectors space RD and labeled with a domain specific function fd with label y ∈ {−1, +1} (for binary classification). In multi-attribute multi-domain 685 Proceedings of NAACL-HLT 2013, pages 685–690, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics (MAMD) learning, we have M metadata attributes in"
N13-1080,P12-1078,0,0.0137892,"is possible that there is a single “best” attribute to use for defining domains, one that when used in multi-domain learning will yield the best classifier. To find this attribute, one must rely on one’s intuition about the problem,2 or perform an exhaustive empirical search over all attributes using some validation set. Both these strategies can be brittle, because as the nature of data changes over time so may the “best” domain distinction. Additionally, multi-domain learning was not designed to benefit from multiple helpful attributes. We note here that Eisenstein et al. (2011), as well as Wang et al. (2012), worked with a “multifaceted topic model” using the framework of sparse additive generative models (SAGE). Both those models capture interactions between topics and multiple as1 Distributions and functions that share attributes could share parameters. 2 Intuition is often critical for learning and in some cases can help, such as in the Amazon product reviews data set, where product type clearly corresponds to domain. However, for other data sets the choice may be less clear. 686 pects, and can be adapted to the case of MAMD. While our problem formulation has significant conceptual overlap wit"
N18-1010,P17-1091,0,0.0392487,"? If so, what properties constitute vulnerability? Changing an OH’s view depends highly on argumentation quality, which has been the focus of much prior work. Wachsmuth et al. (2017) reviewed theories of argumentation quality assessment and suggested a unified framework. Prior research has focused mainly on the presentation of an argument and some aspects in this framework without considering the OH’s reasoning. Specific examples include politeness, sentiment (Tan et al., 2016; Wei et al., 2016), grammaticality, factuality, topic-relatedness (Habernal and Gurevych, 2016b), argument structure (Niculae et al., 2017), topics (Wang et al., 2017), and argumentative strategies (e.g., anecdote, testimony, statistics) (Al Khatib et al., 2017). Some of these aspects have been used as features to predict debate winners (Wang et al., 2017) and view changes (Tan et al., 2016). Habernal and Gurevych (2016a) used crowdsourcing to develop an ontology of reasons for strong/weak arguments. • RQ3. What kinds of interactions between arguments are captured by the model? We use our model to predict whether a challenger’s argument has impacted the OH’s view and compare the result with several baseline models. We also presen"
N18-1010,D16-1129,0,0.0407885,"more likely to change the OH’s view when addressed? If so, what properties constitute vulnerability? Changing an OH’s view depends highly on argumentation quality, which has been the focus of much prior work. Wachsmuth et al. (2017) reviewed theories of argumentation quality assessment and suggested a unified framework. Prior research has focused mainly on the presentation of an argument and some aspects in this framework without considering the OH’s reasoning. Specific examples include politeness, sentiment (Tan et al., 2016; Wei et al., 2016), grammaticality, factuality, topic-relatedness (Habernal and Gurevych, 2016b), argument structure (Niculae et al., 2017), topics (Wang et al., 2017), and argumentative strategies (e.g., anecdote, testimony, statistics) (Al Khatib et al., 2017). Some of these aspects have been used as features to predict debate winners (Wang et al., 2017) and view changes (Tan et al., 2016). Habernal and Gurevych (2016a) used crowdsourcing to develop an ontology of reasons for strong/weak arguments. • RQ3. What kinds of interactions between arguments are captured by the model? We use our model to predict whether a challenger’s argument has impacted the OH’s view and compare the result"
N18-1010,D17-1261,0,0.205193,"he joint construction of knowledge. Especially modeling the knowledge co-construction process requires understanding of both the substance of viewpoints and how the substance of an argument connects with what it is arguing against. Prior work on argumentation in the NLP community, however, has focused mainly on the first goal and has often reduced the concept of a viewpoint as a discrete 1 Our code is available at https://github.com/ yohanjo/aim. 103 Proceedings of NAACL-HLT 2018, pages 103–116 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics winners (Potash and Rumshisky, 2017; Zhang et al., 2016; Wang et al., 2017; Prabhakaran et al., 2013) and winning negotiation games (Keizer et al., 2017), this paper addresses a different angle: predicting whether an argument against an OH’s reasoning will successfully impact the OH’s view. Some prior work investigates factors that underlie viewpoint changes (Tan et al., 2016; Lukin et al., 2017; Hidey et al., 2017; Wei et al., 2016), but none target our task of identifying the specific arguments that impact an OH’s view. ipate in discussion with challengers who try to change the OH’s view, and acknowledge when their views have"
N18-1010,P16-1150,0,0.0442761,"more likely to change the OH’s view when addressed? If so, what properties constitute vulnerability? Changing an OH’s view depends highly on argumentation quality, which has been the focus of much prior work. Wachsmuth et al. (2017) reviewed theories of argumentation quality assessment and suggested a unified framework. Prior research has focused mainly on the presentation of an argument and some aspects in this framework without considering the OH’s reasoning. Specific examples include politeness, sentiment (Tan et al., 2016; Wei et al., 2016), grammaticality, factuality, topic-relatedness (Habernal and Gurevych, 2016b), argument structure (Niculae et al., 2017), topics (Wang et al., 2017), and argumentative strategies (e.g., anecdote, testimony, statistics) (Al Khatib et al., 2017). Some of these aspects have been used as features to predict debate winners (Wang et al., 2017) and view changes (Tan et al., 2016). Habernal and Gurevych (2016a) used crowdsourcing to develop an ontology of reasons for strong/weak arguments. • RQ3. What kinds of interactions between arguments are captured by the model? We use our model to predict whether a challenger’s argument has impacted the OH’s view and compare the result"
N18-1010,I13-1042,0,0.024531,"ge co-construction process requires understanding of both the substance of viewpoints and how the substance of an argument connects with what it is arguing against. Prior work on argumentation in the NLP community, however, has focused mainly on the first goal and has often reduced the concept of a viewpoint as a discrete 1 Our code is available at https://github.com/ yohanjo/aim. 103 Proceedings of NAACL-HLT 2018, pages 103–116 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics winners (Potash and Rumshisky, 2017; Zhang et al., 2016; Wang et al., 2017; Prabhakaran et al., 2013) and winning negotiation games (Keizer et al., 2017), this paper addresses a different angle: predicting whether an argument against an OH’s reasoning will successfully impact the OH’s view. Some prior work investigates factors that underlie viewpoint changes (Tan et al., 2016; Lukin et al., 2017; Hidey et al., 2017; Wei et al., 2016), but none target our task of identifying the specific arguments that impact an OH’s view. ipate in discussion with challengers who try to change the OH’s view, and acknowledge when their views have been impacted. Particularly, we aim to answer the following quest"
N18-1010,E17-1017,0,0.0130562,"ments that impact an OH’s view. ipate in discussion with challengers who try to change the OH’s view, and acknowledge when their views have been impacted. Particularly, we aim to answer the following questions: • RQ1. Does the architecture of vulnerable region detection and interaction encoding help to predict changes in view? • RQ2. Can the model identify vulnerable sentences, which are more likely to change the OH’s view when addressed? If so, what properties constitute vulnerability? Changing an OH’s view depends highly on argumentation quality, which has been the focus of much prior work. Wachsmuth et al. (2017) reviewed theories of argumentation quality assessment and suggested a unified framework. Prior research has focused mainly on the presentation of an argument and some aspects in this framework without considering the OH’s reasoning. Specific examples include politeness, sentiment (Tan et al., 2016; Wei et al., 2016), grammaticality, factuality, topic-relatedness (Habernal and Gurevych, 2016b), argument structure (Niculae et al., 2017), topics (Wang et al., 2017), and argumentative strategies (e.g., anecdote, testimony, statistics) (Al Khatib et al., 2017). Some of these aspects have been used"
N18-1010,Q17-1016,0,0.247771,"odeling the knowledge co-construction process requires understanding of both the substance of viewpoints and how the substance of an argument connects with what it is arguing against. Prior work on argumentation in the NLP community, however, has focused mainly on the first goal and has often reduced the concept of a viewpoint as a discrete 1 Our code is available at https://github.com/ yohanjo/aim. 103 Proceedings of NAACL-HLT 2018, pages 103–116 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics winners (Potash and Rumshisky, 2017; Zhang et al., 2016; Wang et al., 2017; Prabhakaran et al., 2013) and winning negotiation games (Keizer et al., 2017), this paper addresses a different angle: predicting whether an argument against an OH’s reasoning will successfully impact the OH’s view. Some prior work investigates factors that underlie viewpoint changes (Tan et al., 2016; Lukin et al., 2017; Hidey et al., 2017; Wei et al., 2016), but none target our task of identifying the specific arguments that impact an OH’s view. ipate in discussion with challengers who try to change the OH’s view, and acknowledge when their views have been impacted. Particularly, we aim to"
N18-1010,P16-2032,0,0.0473329,"ble at https://github.com/ yohanjo/aim. 103 Proceedings of NAACL-HLT 2018, pages 103–116 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics winners (Potash and Rumshisky, 2017; Zhang et al., 2016; Wang et al., 2017; Prabhakaran et al., 2013) and winning negotiation games (Keizer et al., 2017), this paper addresses a different angle: predicting whether an argument against an OH’s reasoning will successfully impact the OH’s view. Some prior work investigates factors that underlie viewpoint changes (Tan et al., 2016; Lukin et al., 2017; Hidey et al., 2017; Wei et al., 2016), but none target our task of identifying the specific arguments that impact an OH’s view. ipate in discussion with challengers who try to change the OH’s view, and acknowledge when their views have been impacted. Particularly, we aim to answer the following questions: • RQ1. Does the architecture of vulnerable region detection and interaction encoding help to predict changes in view? • RQ2. Can the model identify vulnerable sentences, which are more likely to change the OH’s view when addressed? If so, what properties constitute vulnerability? Changing an OH’s view depends highly on argumenta"
N18-1010,P14-5010,0,\N,Missing
N18-1010,W14-4012,0,\N,Missing
N18-1010,D17-1070,0,\N,Missing
N18-1010,E17-1070,0,\N,Missing
N18-1010,W17-5102,0,\N,Missing
N18-1010,D17-1141,0,\N,Missing
N18-1010,N18-1036,0,\N,Missing
P07-2019,W02-1001,0,0.0963626,"tial state (q0) at the top of a message. It makes a transition to state (q1) when it encounters a quoted span of text. Once in state (q1), the automaton remains in this state until it encounters a prompt. On encountering a prompt it makes a transition back to the initial state (q0). The purpose is to indicate places where users are likely to make a comment in reference to something another participant in the conversation has already contributed. Evaluation 4.1 The purpose of our evaluation is to contrast our proposed feature based approach with a state-ofthe-art sequential learning technique (Collins, 2002). Both approaches are designed to leverage context for the purpose of increasing classification accuracy on a classification task where the codes refer to the role a span of text plays in context. We evaluate these two approaches alone and in combination over the same data but with three different sets of codes, namely the three relevant dimensions of the Weinberger and Fischer annotation scheme. In all cases, we employ a 10-fold cross-validation methodology, where we apply a feature selection wrapper in such as way as to select the 100 best features over the training set on each fold, and the"
P08-4007,J02-4002,0,\N,Missing
P09-2026,A00-2018,0,0.0079424,"riminative online learning to train feature weights. A key 2 Experimental Paradigm Supervised approaches to sentence compression typically use parallel corpora consisting of original and compressed sentences (paired corpus, henceforth). In this paper, we will refer to these pairs as a 2-tuple <x, y&gt;, where x is the original sentence and y is the compressed sentence. We implemented the M06 system as an experimental framework in which to conduct our investigation. The system uses as input the paired corpus, the corresponding POS tagged corpus, the paired corpus parsed using the Charniak parser (Charniak, 2000), and dependency parses from the MST parser (McDonald et al., 2005). Features are extracted over adjacent pairs of words in the compressed sentence and weights are learnt at training time using the MIRA algorithm (Crammer and Singer, 2003). We decode as follows to find the best compression: Let the score of a compression y for a sentence x be s(x, y). This score is factored using a first-order Markov assumption over the words in the compressed sentence, and is defined by the dot product between a high dimensional feature representation and a corresponding weight vector (for details, refer to M"
P09-2026,N09-2037,1,0.847565,"Missing"
P09-2026,N03-1020,0,0.136449,"Missing"
P09-2026,E06-1038,0,0.0665047,"), and dependency parses from the MST parser (McDonald et al., 2005). Features are extracted over adjacent pairs of words in the compressed sentence and weights are learnt at training time using the MIRA algorithm (Crammer and Singer, 2003). We decode as follows to find the best compression: Let the score of a compression y for a sentence x be s(x, y). This score is factored using a first-order Markov assumption over the words in the compressed sentence, and is defined by the dot product between a high dimensional feature representation and a corresponding weight vector (for details, refer to McDonald, 2006). The equations for decoding are as follows: C[1] 0.0 C[i] max j i C[ j ] s( x, j, i), i 1 101 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 101–104, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP where C is the dynamic programming table and C[i] represents the highest score for compressions ending at word i for the sentence x. The M06 system takes the best scoring compression from the set of all possible compressions. In the ARC system, the model determines the compression rate and enforces a target compression length by altering the dynamic programming algorithm a"
P09-2026,P05-1012,0,0.0167782,"Experimental Paradigm Supervised approaches to sentence compression typically use parallel corpora consisting of original and compressed sentences (paired corpus, henceforth). In this paper, we will refer to these pairs as a 2-tuple <x, y&gt;, where x is the original sentence and y is the compressed sentence. We implemented the M06 system as an experimental framework in which to conduct our investigation. The system uses as input the paired corpus, the corresponding POS tagged corpus, the paired corpus parsed using the Charniak parser (Charniak, 2000), and dependency parses from the MST parser (McDonald et al., 2005). Features are extracted over adjacent pairs of words in the compressed sentence and weights are learnt at training time using the MIRA algorithm (Crammer and Singer, 2003). We decode as follows to find the best compression: Let the score of a compression y for a sentence x be s(x, y). This score is factored using a first-order Markov assumption over the words in the compressed sentence, and is defined by the dot product between a high dimensional feature representation and a corresponding weight vector (for details, refer to McDonald, 2006). The equations for decoding are as follows: C[1] 0.0"
P09-2026,N03-1026,0,0.0311585,"oduces significantly improved compressions across a range of compression rates compared to existing state-of-the-art approaches. Thus, we name our system for generating compressions the Adjustable Rate Compressor (ARC). Knight and Marcu (2000) (K&M, henceforth) presented two approaches to the sentence compression problem: one using a noisy channel model, the other using a decision-based model. The performances of the two models were comparable though their experiments suggested that the noisy channel model degraded more smoothly than the decision-based model when tested on out-of-domain data. Riezler et al. (2003) applied linguistically rich LFG grammars to a sentence compression system. Turner and Charniak (2005) achieved similar performance to K&M using an unsupervised approach that induced rules from the Penn Treebank. A variety of feature encodings have previously been explored for the problem of sentence compression. Clarke and Lapata (2007) included discourse level features in their framework to leverage context for enhancing coherence. McDonald’s (2006) model (M06, henceforth) is similar to K&M except that it uses discriminative online learning to train feature weights. A key 2 Experimental Para"
P09-2026,D07-1001,0,\N,Missing
P11-1102,E03-1072,0,0.0449649,"Missing"
P11-1102,N10-2007,1,0.349045,"Missing"
P11-1102,J02-1002,0,0.0323581,"Missing"
P11-1102,rizzolo-roth-2010-learning,0,0.0381585,"Missing"
P11-1102,W04-2401,0,0.0327169,"the Association for Computational Linguistics, pages 1018–1026, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics dimensional classification task. One dimension is a set of codes describing the authoritative status of a contribution1 . The other dimension is a segmentation task. We impose constraints on both of these models based on the structure observed in the work of SFL. These constraints are formulated as boolean statements describing what a correct label sequence looks like, and are imposed on our model using an Integer Linear Programming formulation (Roth and Yih, 2004). In section 5, this model is evaluated on a subset of the MapTask corpus (Anderson et al., 1991) and shows a high correlation with human judgements of authoritativeness (r2 = 0.947). After a detailed error analysis, we will conclude the paper in section 6 with a discussion of our future work. 2 Background The Negotiation framework, as formulated by the SFL community, places a special emphasis on how speakers function in a discourse as sources or recipients of information or action. We break down this concept into a set of codes, one code per contribution. Before we break down the coding schem"
P11-1102,P10-1019,0,0.0124786,"at each line in our corpus as a single contribution. 1019 versation similarly transfers from one speaker to another (Walker and Whittaker, 1990). This relation is often considered synchronous, though evidence suggests that the reality is not straightforward (Jordan and Di Eugenio, 1997). Research in initiative and control has been applied in the form of mixed-initiative dialogue systems (Smith, 1992). This is a large and active field, with applications in tutorial dialogues (Core, 2003), human-robot interactions (Peltason and Wrede, 2010), and more general approaches to effective turn-taking (Selfridge and Heeman, 2010). However, that body of work focuses on influencing discourse structure through positioning. The question that we are asking instead focuses on how speakers view their authority as a source of information about the topic of the discourse. In particular, consider questioning in discourse. In mixed-initiative analysis of discourse, asking a question always gives you control of a discourse. There is an expectation that your question will be followed by an answer. A speaker might already know the answer to a question they asked - for instance, when a teacher is verifying a student’s knowledge. How"
P11-1102,J00-3003,0,0.314922,"1988), which attempt to operationalize the authority over a discourse’s structure, fall under the umbrella of positioning. As we construe positioning, it also includes work on detecting certainty and confusion in speech (Liscombe et al., 2005), which models a speaker’s understanding of the information in their statements. Work in dialogue act tagging is also relevant, as it seeks to describe the ac1018 Carolyn Penstein Ros´e Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 cprose@cs.cmu.edu tions and moves with which speakers display these types of positioning (Stolcke et al., 2000). To complement these bodies of work, we choose to focus on the question of how speakers position themselves as authoritative in a discourse. This means that we must describe the way speakers introduce new topics or discussions into the discourse; the way they position themselves relative to that topic; and how these functions interact with each other. While all of the tasks mentioned above focus on specific problems in the larger rhetorical question of speaker positioning, none explicitly address this framing of authority. Each does have valuable ties to the work that we would like to do, and"
P11-1102,P88-1015,0,0.738918,"ined model’s analyses of speaker authority correlates very strongly with expert human judgments (r2 coefficient of 0.947). 1 Introduction In this work, we seek to formalize the ways speakers position themselves in discourse. We do this in a way that maintains a notion of discourse structure, and which can be aggregated to evaluate a speaker’s overall stance in a dialogue. We define the body of work in positioning to include any attempt to formalize the processes by which speakers attempt to influence or give evidence of their relations to each other. Constructs such as Initiative and Control (Whittaker and Stenton, 1988), which attempt to operationalize the authority over a discourse’s structure, fall under the umbrella of positioning. As we construe positioning, it also includes work on detecting certainty and confusion in speech (Liscombe et al., 2005), which models a speaker’s understanding of the information in their statements. Work in dialogue act tagging is also relevant, as it seeks to describe the ac1018 Carolyn Penstein Ros´e Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 cprose@cs.cmu.edu tions and moves with which speakers display these types of positioning (Stolck"
P11-1102,J86-3001,0,\N,Missing
P11-1102,P90-1010,0,\N,Missing
P11-4020,J97-1003,0,0.354385,"played in Figure 1. First, the Text Tiling module takes care of obtaining tiles of text relevant to the citation context. Next, the clustering module is used to generate labelled clusters using the text tiles extracted from the co-cited papers. The clusters are ordered according to relevance with respect to the generated query. This is accomplished by the Ranking Module. In the following sections, we discuss each of the main modules in detail. Figure 1: SciSumm summarization pipeline 3.2 Texttiling efficient in terms of space requirements. The Text Tiling module uses the TextTiling algorithm (Hearst, 1997) for segmenting the text of each article. We have used text tiles as the basic unit for our summary since individual sentences are too short to stand on their own. This happens as a sideeffect of the length of scientific articles. Sentences picked from different parts of several articles assembled together would make an incoherent summary. Once computed, text tiles are used to expand on the content viewed within the context associated with a co-citation. The intuition is that an embedded cocitation in a text tile is connected with the topic distribution of its context. Thus, we can use a compu"
P11-4020,W00-0403,0,0.188546,"Missing"
P11-4020,radev-etal-2004-mead,0,0.0892933,"to the specific information needs of the users in context. SciSumm captures a user’s contextual needs when a user clicks on a co-citation. Using the context of the co-citation in the source article, we generate a query that allows us to create a summary in a query-oriented fashion. The extracted portions of the co-cited articles are then assembled into clusters that represent the main themes of the articles that relate to the context in which they were cited. Our evaluation demonstrates that SciSumm achieves higher quality summaries than a state-of-the-art multidocument summarization system (Radev, 2004). The rest of the paper is organized as follows. We first describe the design goals for SciSumm in 2 to motivate the need for the system and its usefulness. The end-to-end summarization pipeline has been described in Section 3. Section 4 presents an evaluation of summaries generated from the system. We present an overview of relevant literature in Section 5. We end the paper with conclusions and some interesting further research directions in Section 6. 2 Design Goals Consider that as a researcher reads a scientific article, she/he encounters numerous citations, most of them citing the foundat"
P11-4020,J02-4002,0,0.154733,"Missing"
P11-4020,D08-1035,0,0.0728799,"Missing"
P11-4020,N04-1015,0,0.0893262,"Missing"
P11-4020,councill-etal-2008-parscit,0,\N,Missing
P11-4020,P06-1039,0,\N,Missing
P11-4020,C08-1087,0,\N,Missing
P13-1011,C08-1040,0,0.0563752,"et al., 2012b). These transitional works have used limited analysis methodology; in the absence of sophisticated natural language processing, their conclusions often rely on coarse measures, such as word counts and proportions of annotations in a text. Users, of course, do not express empowerment in every thread in which they participate, which leads to a challenge for machine learning. Threads often focus on a single user’s experiences, in which most participants in a chat are merely commentators, if they participate at all, matching previous research on shifts in speaker salience over time (Hassan et al., 2008). This leads to many user threads which are annotated as not applicable (N/A). We move to our proposed approach with these skewed distributions in mind. Background We ground this paper’s discussion of machine learning with a real problem, turning to the annotation of empowerment language in chat1 . The concept of empowerment, while a prolific area of research, lacks a broad definition across professionals, but broadly relates to “the power to act efficaciously to bring about desired results” (Boehm and Staples, 2002) and “experiencing personal growth as a result of developing skills and abilit"
P13-1011,J93-3003,0,0.294425,"tion by learning a set of cue features. Each of these features indicates some linguistic function within the discourse which should downplay the importance of features either before or after that discourse marker. Our algorithm allows us to evaluate the impact of rules against a baseline, and to iteratively judge each rule atop the changes made by previous rules. This algorithm fits into existing language technologies research which has attempted to partition documents into sections which are more or less relevant for classification. Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993), especially for segmentation both in prose (Hearst, 1997) and conversation (Galley et al., 2003). The approach of content selection, meanwhile, has been explored for sentiment analysis (Pang and Lee, 2004), where individual sentences may be less subjective and therefore less relevant to the sentiment classification task. It is also similar conceptually to content selection algorithms that have been used for text summarization (Teufel and Moens, 2002) and text generation (Sauper and Barzilay, 2009), both of which rely on finding highly-relevant passages within source texts. Our work is distinc"
P13-1011,W12-1607,1,0.870038,"Missing"
P13-1011,P09-1024,0,0.0232612,"less relevant for classification. Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993), especially for segmentation both in prose (Hearst, 1997) and conversation (Galley et al., 2003). The approach of content selection, meanwhile, has been explored for sentiment analysis (Pang and Lee, 2004), where individual sentences may be less subjective and therefore less relevant to the sentiment classification task. It is also similar conceptually to content selection algorithms that have been used for text summarization (Teufel and Moens, 2002) and text generation (Sauper and Barzilay, 2009), both of which rely on finding highly-relevant passages within source texts. Our work is distinct from these approaches. While we have coarse-grained annotations of empowerment, there is no direct annotation of what makes a good cue for content selection. With our cues, we hope to take advantage of shallow discourse structure in conversation, such as contrastive markers, making use of implicit structure in the conversational domain. 4.1 • Ignore Local Future (A): Ignore all features from the two lines after each occurrence of c. • Ignore All Future (B): Ignore all features occurring after the"
P13-1011,P04-1035,0,0.00417972,"ur algorithm allows us to evaluate the impact of rules against a baseline, and to iteratively judge each rule atop the changes made by previous rules. This algorithm fits into existing language technologies research which has attempted to partition documents into sections which are more or less relevant for classification. Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993), especially for segmentation both in prose (Hearst, 1997) and conversation (Galley et al., 2003). The approach of content selection, meanwhile, has been explored for sentiment analysis (Pang and Lee, 2004), where individual sentences may be less subjective and therefore less relevant to the sentiment classification task. It is also similar conceptually to content selection algorithms that have been used for text summarization (Teufel and Moens, 2002) and text generation (Sauper and Barzilay, 2009), both of which rely on finding highly-relevant passages within source texts. Our work is distinct from these approaches. While we have coarse-grained annotations of empowerment, there is no direct annotation of what makes a good cue for content selection. With our cues, we hope to take advantage of sh"
P13-1011,N12-1057,0,0.023984,"ettings and into natural environments. With this influx of interest comes new methodology, and the inevitable question arises of how to move towards testable hypotheses, using these uncontrolled sources of data as scientific lenses into the real world. The study of conversational transcripts is a key domain in this new frontier. There are certain social and behavioral phenomena in conversation that cannot be easily identified through questionnaire data, self-reported surveys, or easily extracted user metadata. Examples of these social phenomena in conversation include overt displays of power (Prabhakaran et al., 2012) or indicators of rapport and relationship building (Wang et al., 104 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 104–113, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics we define a modification of confidence-based ensemble voting which encourages minority class labeling. Using these techniques, we demonstrate a significant improvement in classifier performance when recognizing the language of empowerment in support group chatrooms, a critical application area for researchers studying conversational interact"
P13-1011,prasad-etal-2008-penn,0,0.0308854,"boundaries allows for context to be meaningfully taken into account, without crossing topic boundaries. We choose to take advantage of these attributes of threads. We know from research in discourse analysis that many sections of conversations are formulaic and rote, like introductions and greetings (Schegloff, 1968). We additionally know that polarity often shifts in dialogue through the use of discourse connectives such as conjunctions and transitional phrases. These issues have been addressed in work in the language technologies community, most notably through the Penn Discourse Treebank (Prasad et al., 2008); however, their applications to noisier synchronous conversation has beenrare in computational linguistics. With these linguistic insights in mind, we examine how we can make best use of them for machine learning performance. While techniques for predicting rare events (Weiss and Hirsh, 1998) and compensating for class imbalance (Frank and 106 data is arranged hierarchically. We assume that we have a collection of d training documents Tr = {D1 . . . Dd }, each of which contains many training instances (in our task, an instance consists of all lines of chat from one user in one thread). Our to"
P13-1011,W12-1603,0,0.0606573,"Missing"
P13-1011,J97-1003,0,\N,Missing
P13-1011,P03-1071,0,\N,Missing
P13-1011,J02-4002,0,\N,Missing
P13-1011,J10-3004,0,\N,Missing
P13-2145,P09-1113,0,0.0161949,"Missing"
P13-2145,P07-2044,0,0.0684976,"Missing"
P13-2145,P12-1011,0,0.0762238,"have not been born yet. A notable characteristic of our task is that constraints are softer. Diseases may occur in very different ways across patients. Recurring illnesses falsely appear to have an unpredictable order. Thus, there can be no universal logical constraints on the order of cancer events. Our approach to using temporal constraints is a variant on previously published approaches. Garrido et al. (2012) made use of DCT (document creation time) as well, however, they have assumed the DCT is within the time-range of the event stated in the document, which is often not true in our data. Chambers (2012) utilized the withinsentence time-DCT relation to learn constrains for predicting DCT. We learn the event-DCT relations to produce constrains for the event date. a candidate event and a posting history. The output is the event date (month and year) for the event if it occurred, or “unknown” if it did not occur. The process iterates through a list of 10 cancer events (CEs). This list includes breast cancer Diagnosis, Metastasis, Recurrence, Mastectomy, Lumpectomy, Reconstruction, Chemotherapy-Start, Chemotherapy-End, Radiation-Start and Radiation-End. For each of these target CEs, we manually d"
P13-2145,chang-manning-2012-sutime,0,0.06249,"et of gold event relations are provided as input, so that the task is only to identify a date for an event that is guaranteed to have been mentioned. In our task, we provide a set of potential events. However, most of the candidate events won’t have ever been reported within a user’s posting history. Temporal constraints have proven to be useful for producing a globally consistent timeline. In most temporal relation bound extraction systems, the constraints are included as input rather than learned by the system (Talukdar et al., 2012; Wang et al., 2011). A notable exception is McCloskyet al. (2012) who developed an approach to learning constraints such as that people cannot attend school if they have not been born yet. A notable characteristic of our task is that constraints are softer. Diseases may occur in very different ways across patients. Recurring illnesses falsely appear to have an unpredictable order. Thus, there can be no universal logical constraints on the order of cancer events. Our approach to using temporal constraints is a variant on previously published approaches. Garrido et al. (2012) made use of DCT (document creation time) as well, however, they have assumed the DCT"
P13-2145,P12-1012,0,0.186058,"ned by the system (Talukdar et al., 2012; Wang et al., 2011). A notable exception is McCloskyet al. (2012) who developed an approach to learning constraints such as that people cannot attend school if they have not been born yet. A notable characteristic of our task is that constraints are softer. Diseases may occur in very different ways across patients. Recurring illnesses falsely appear to have an unpredictable order. Thus, there can be no universal logical constraints on the order of cancer events. Our approach to using temporal constraints is a variant on previously published approaches. Garrido et al. (2012) made use of DCT (document creation time) as well, however, they have assumed the DCT is within the time-range of the event stated in the document, which is often not true in our data. Chambers (2012) utilized the withinsentence time-DCT relation to learn constrains for predicting DCT. We learn the event-DCT relations to produce constrains for the event date. a candidate event and a posting history. The output is the event date (month and year) for the event if it occurred, or “unknown” if it did not occur. The process iterates through a list of 10 cancer events (CEs). This list includes breas"
P13-2145,P12-2014,0,0.0316498,"Missing"
P13-2145,W11-0219,0,0.0382245,"Missing"
P13-2145,strotgen-gertz-2012-temporal,0,0.030033,"people are instead communicating informally about their lives, they refer to time more informally and frequently from their personal frame of reference rather than from an impersonal third person frame of reference. For example, they may use their own birthday as a time reference. The proportion of relative (e.g., “last week”, “two days from now”), or personal time references in our data is more than one and a half times as high as in newswire and Wikipedia. Therefore, it is not surprising that there would be difficulty in applying a temporal tagger designed for newswire to social media data (Strotgen and Gertz, 2012; Kolomiyets et al., 2011). Recent behavioral studies (Choudhury et al., 2013; Park and Choi, 2012; Wen et al., 2012) demonstrate that user-focused event mentions extracted from social media data can provide a useful timeline-like tool for studying how behavior patterns change over time in response to mentioned events. Our research contributes towards automating this work. Introduction In this paper we present a challenging new event date extraction task. Our technical contribution is a temporal tagger that outperforms previously published baseline approaches in its ability to identify informa"
P13-2145,P11-2047,0,0.0282033,"cating informally about their lives, they refer to time more informally and frequently from their personal frame of reference rather than from an impersonal third person frame of reference. For example, they may use their own birthday as a time reference. The proportion of relative (e.g., “last week”, “two days from now”), or personal time references in our data is more than one and a half times as high as in newswire and Wikipedia. Therefore, it is not surprising that there would be difficulty in applying a temporal tagger designed for newswire to social media data (Strotgen and Gertz, 2012; Kolomiyets et al., 2011). Recent behavioral studies (Choudhury et al., 2013; Park and Choi, 2012; Wen et al., 2012) demonstrate that user-focused event mentions extracted from social media data can provide a useful timeline-like tool for studying how behavior patterns change over time in response to mentioned events. Our research contributes towards automating this work. Introduction In this paper we present a challenging new event date extraction task. Our technical contribution is a temporal tagger that outperforms previously published baseline approaches in its ability to identify informal temporal expressions (TE"
P13-2145,P12-1010,0,0.0376552,"Missing"
P13-2145,S10-1062,0,0.107041,"ents (Wen et al., 2013). We achieved .94 Kappa on identification of whether an event has a reported event date in a user’s history or not. In evaluation of agreement on extracted dates, we achieved a .99 Cronbach’s alpha. From this corpus, 509 events were annotated with occurrence dates (year and month). In our evaluation, we use data from 250 users for training, and 50 for testing. 5 5.2 We train a MaxEnt classifier to predict the temporal relationship between the retrieved TE and the event date as overlap or no-overlap, similar to the within-sentence event-time anchoring task in TempEval-2 (UzZaman and Allen, 2010). Features for the classifier include many of those in (McClosky and Manning, 2012; Yoshikawa et al., 2009): namely, event keyword and its dominant verb, verb and preposition that dominate TE, dependency path between TE and keyword and its length, unigram and bigram word and POS features. New features include the Event-Subject, Negative and Modality features. In online support groups, users not only tell stories about themselves, they also share other patients’ stories (as shown in Figure 1). So we add subject features to remove this kind of noise, which includes the governing subject of the e"
P13-2145,P09-1046,0,0.0312506,"te in a user’s history or not. In evaluation of agreement on extracted dates, we achieved a .99 Cronbach’s alpha. From this corpus, 509 events were annotated with occurrence dates (year and month). In our evaluation, we use data from 250 users for training, and 50 for testing. 5 5.2 We train a MaxEnt classifier to predict the temporal relationship between the retrieved TE and the event date as overlap or no-overlap, similar to the within-sentence event-time anchoring task in TempEval-2 (UzZaman and Allen, 2010). Features for the classifier include many of those in (McClosky and Manning, 2012; Yoshikawa et al., 2009): namely, event keyword and its dominant verb, verb and preposition that dominate TE, dependency path between TE and keyword and its length, unigram and bigram word and POS features. New features include the Event-Subject, Negative and Modality features. In online support groups, users not only tell stories about themselves, they also share other patients’ stories (as shown in Figure 1). So we add subject features to remove this kind of noise, which includes the governing subject of the event keyword and its POS tag. Modality features include the appearance of modals before the event keyword ("
P13-2145,D12-1080,0,\N,Missing
P15-1161,P13-1035,0,0.038167,"Missing"
P15-1161,P14-1035,0,0.0135673,"icular contexts. Theory on coordination in groups and organizations emphasizes role differentiation, division of labor and formal and informal management (Kittur and Kraut, 2010). However, identification of roles as such has not had a corresponding strong emphasis in the language technologies community, although there has been work on related notions. For example, there has been much previous work modeling disagreement and debate framed as stance classification (Thomas et al., 2006; Walker et al., 2012). Another similar line of work studies the identification of personas (Bamman et al., 2013; Bamman et al., 2014) in the context of a social network, e.g. celebrity, newbie, lurker, flamer, troll and ranter, etc, which evolve through user interaction (Forestier et al., 2012). What is similar between stances and personas on the one hand and roles on the other is that the unit of analysis is the person. On the other hand, they are distinct in that stances (e.g., liberal) and personas (e.g., lurker) are not typically defined in terms of what they are meant to accomplish, although they may be associated with kinds of things they do. Teamwork roles are defined in terms of what the role holder is meant to acco"
P15-1161,D14-1226,0,0.0181563,"an be used effectively to predict the teamwork outcome. An empirical evaluation applied to two Massive Open Online Course (MOOCs) datasets demonstrates that this approach yields superior performance in learning representations for predicting the teamwork outcome over several baselines. 1 Carolyn Penstein Ros´e School of Computer Science Carnegie Mellon University Pittsburgh, PA, 15213, USA cprose@cs.cmu.edu Introduction In language technologies research seeking to model conversational interactions, modeling approaches have aimed to identify conversation acts (Paul, 2012; Wallace et al., 2013; Bhatia et al., 2014) on a per turn basis, or to identify stances (Germesin and Wilson, 2009; Mukherjee et al., 2013; Piergallini et al., 2014; Hasan and Ng, 2014) that characterize the nature of a speaker’s orientation within an interaction over several turns. What neither of these two perspectives quite offer is a notion of a conversational role. And yet, conversational role is a concept with great utility in current real world applications where language technologies may be applied. Important teamwork is achieved through collaboration where discussion is an important medium We present a modeling approach that l"
P15-1161,D13-1035,0,0.0291271,"ough they may be associated with kinds of things they do. Teamwork roles are defined in terms of what the role holder is meant to accomplish. The notion of a natural outcome associated with a role suggests a modeling approach utilizing the outcome as light supervision towards identification of the latent roles. However, representations of other notions such as stances or strategies can similarly be used to predict outcomes. Cadilhac et al. maps strategies based on verbal contributions of participants in a win-lose game into a prediction of exactly which players, if any, trade with each other (Cadilhac et al., 2013). Hu et al. (Hu et al., 2009) predict the outcome of featured article nominations based on user activeness, discussion consensus and user co-review relations. In other work, the authors of (Somasundaran and Wiebe, 2009) adopt manually annotated characters and leaders to predict which participants will achieve success in online debates. The difference is the interpretation of the latent constructs. The latent construct of a role, such as team leader, is defined in terms of a distribution of characteristics that describe how that role should ideally be carried out. However, in the case of stance"
P15-1161,D14-1083,0,0.0135337,"nstrates that this approach yields superior performance in learning representations for predicting the teamwork outcome over several baselines. 1 Carolyn Penstein Ros´e School of Computer Science Carnegie Mellon University Pittsburgh, PA, 15213, USA cprose@cs.cmu.edu Introduction In language technologies research seeking to model conversational interactions, modeling approaches have aimed to identify conversation acts (Paul, 2012; Wallace et al., 2013; Bhatia et al., 2014) on a per turn basis, or to identify stances (Germesin and Wilson, 2009; Mukherjee et al., 2013; Piergallini et al., 2014; Hasan and Ng, 2014) that characterize the nature of a speaker’s orientation within an interaction over several turns. What neither of these two perspectives quite offer is a notion of a conversational role. And yet, conversational role is a concept with great utility in current real world applications where language technologies may be applied. Important teamwork is achieved through collaboration where discussion is an important medium We present a modeling approach that leverages the concept of latent conversational roles as an intermediary between observed discussions and a measure of interaction success. Whil"
P15-1161,P09-1026,0,0.0156101,"ling approach utilizing the outcome as light supervision towards identification of the latent roles. However, representations of other notions such as stances or strategies can similarly be used to predict outcomes. Cadilhac et al. maps strategies based on verbal contributions of participants in a win-lose game into a prediction of exactly which players, if any, trade with each other (Cadilhac et al., 2013). Hu et al. (Hu et al., 2009) predict the outcome of featured article nominations based on user activeness, discussion consensus and user co-review relations. In other work, the authors of (Somasundaran and Wiebe, 2009) adopt manually annotated characters and leaders to predict which participants will achieve success in online debates. The difference is the interpretation of the latent constructs. The latent construct of a role, such as team leader, is defined in terms of a distribution of characteristics that describe how that role should ideally be carried out. However, in the case of stances, the latent constructs are learned in order to distinguish one stance from another or in order to predict who will win. This approach will not necessarily offer insight into what marks the most staunch proponents of a"
P15-1161,W06-1639,0,0.0507883,"n social science fields to describe the intersection of behavioral, symbolic, and structural attributes that emerge regularly in particular contexts. Theory on coordination in groups and organizations emphasizes role differentiation, division of labor and formal and informal management (Kittur and Kraut, 2010). However, identification of roles as such has not had a corresponding strong emphasis in the language technologies community, although there has been work on related notions. For example, there has been much previous work modeling disagreement and debate framed as stance classification (Thomas et al., 2006; Walker et al., 2012). Another similar line of work studies the identification of personas (Bamman et al., 2013; Bamman et al., 2014) in the context of a social network, e.g. celebrity, newbie, lurker, flamer, troll and ranter, etc, which evolve through user interaction (Forestier et al., 2012). What is similar between stances and personas on the one hand and roles on the other is that the unit of analysis is the person. On the other hand, they are distinct in that stances (e.g., liberal) and personas (e.g., lurker) are not typically defined in terms of what they are meant to accomplish, alth"
P15-1161,P13-1165,0,0.0183751,"o Massive Open Online Course (MOOCs) datasets demonstrates that this approach yields superior performance in learning representations for predicting the teamwork outcome over several baselines. 1 Carolyn Penstein Ros´e School of Computer Science Carnegie Mellon University Pittsburgh, PA, 15213, USA cprose@cs.cmu.edu Introduction In language technologies research seeking to model conversational interactions, modeling approaches have aimed to identify conversation acts (Paul, 2012; Wallace et al., 2013; Bhatia et al., 2014) on a per turn basis, or to identify stances (Germesin and Wilson, 2009; Mukherjee et al., 2013; Piergallini et al., 2014; Hasan and Ng, 2014) that characterize the nature of a speaker’s orientation within an interaction over several turns. What neither of these two perspectives quite offer is a notion of a conversational role. And yet, conversational role is a concept with great utility in current real world applications where language technologies may be applied. Important teamwork is achieved through collaboration where discussion is an important medium We present a modeling approach that leverages the concept of latent conversational roles as an intermediary between observed discuss"
P15-1161,D12-1009,0,0.0161744,"mited sized feature vectors that can be used effectively to predict the teamwork outcome. An empirical evaluation applied to two Massive Open Online Course (MOOCs) datasets demonstrates that this approach yields superior performance in learning representations for predicting the teamwork outcome over several baselines. 1 Carolyn Penstein Ros´e School of Computer Science Carnegie Mellon University Pittsburgh, PA, 15213, USA cprose@cs.cmu.edu Introduction In language technologies research seeking to model conversational interactions, modeling approaches have aimed to identify conversation acts (Paul, 2012; Wallace et al., 2013; Bhatia et al., 2014) on a per turn basis, or to identify stances (Germesin and Wilson, 2009; Mukherjee et al., 2013; Piergallini et al., 2014; Hasan and Ng, 2014) that characterize the nature of a speaker’s orientation within an interaction over several turns. What neither of these two perspectives quite offer is a notion of a conversational role. And yet, conversational role is a concept with great utility in current real world applications where language technologies may be applied. Important teamwork is achieved through collaboration where discussion is an important"
P15-1161,E14-1012,1,0.734433,"Missing"
P15-1161,N12-1072,0,0.0182326,"ds to describe the intersection of behavioral, symbolic, and structural attributes that emerge regularly in particular contexts. Theory on coordination in groups and organizations emphasizes role differentiation, division of labor and formal and informal management (Kittur and Kraut, 2010). However, identification of roles as such has not had a corresponding strong emphasis in the language technologies community, although there has been work on related notions. For example, there has been much previous work modeling disagreement and debate framed as stance classification (Thomas et al., 2006; Walker et al., 2012). Another similar line of work studies the identification of personas (Bamman et al., 2013; Bamman et al., 2014) in the context of a social network, e.g. celebrity, newbie, lurker, flamer, troll and ranter, etc, which evolve through user interaction (Forestier et al., 2012). What is similar between stances and personas on the one hand and roles on the other is that the unit of analysis is the person. On the other hand, they are distinct in that stances (e.g., liberal) and personas (e.g., lurker) are not typically defined in terms of what they are meant to accomplish, although they may be assoc"
P15-1161,D13-1182,0,0.0220451,"feature vectors that can be used effectively to predict the teamwork outcome. An empirical evaluation applied to two Massive Open Online Course (MOOCs) datasets demonstrates that this approach yields superior performance in learning representations for predicting the teamwork outcome over several baselines. 1 Carolyn Penstein Ros´e School of Computer Science Carnegie Mellon University Pittsburgh, PA, 15213, USA cprose@cs.cmu.edu Introduction In language technologies research seeking to model conversational interactions, modeling approaches have aimed to identify conversation acts (Paul, 2012; Wallace et al., 2013; Bhatia et al., 2014) on a per turn basis, or to identify stances (Germesin and Wilson, 2009; Mukherjee et al., 2013; Piergallini et al., 2014; Hasan and Ng, 2014) that characterize the nature of a speaker’s orientation within an interaction over several turns. What neither of these two perspectives quite offer is a notion of a conversational role. And yet, conversational role is a concept with great utility in current real world applications where language technologies may be applied. Important teamwork is achieved through collaboration where discussion is an important medium We present a mo"
P15-1161,H05-1044,0,0.00709015,"on Words: counts of question related words in the posts, e.g. why, what, question, problem, how, answer, etc. 8. Discrepancy: number of occurrences of words, such as should, would, could, etc as defined in LIWC (Tausczik and Pennebaker, 2010). 9. Social Process: number of words that denote social processes and suggest human interaction, e.g. talking, sharing, etc. 10. Cognitive Process: number of occurrences of words that reflect thinking and reasoning, e.g. cause, because, thus, etc. 11-14. Polarity: four variables that measure the portion of Positive, Negative, Neutral, Both polarity words (Wilson et al., 2005) in the posts. 15-16. Subjectivity: two count variables of occurrences of Strong Subjectivity words and Weak Subjectivity words. Activities: We also introduce several variables to measure the activeness level of team members. 17-18. Messages: two variables that measure the total number of messages sent, and the number of tokens contained in the messages. 19-20. Videos: the number of videos a student has watched and total duration of watched videos. 21. Login Times: times that a student logins to the course. 4 Experiments In this section, we begin with the dataset description, and then we compa"
P16-1021,W15-4650,1,0.777385,"Missing"
P16-1021,W15-1402,0,0.0664444,"Missing"
P16-1021,N10-1039,0,0.0211661,"roach the problem of detecting semantic outliers in different ways. Li and Sporleder (2009; 2010) identify metaphorical idioms using the idea that non-literal expressions break lexical cohesion of a text. Li and Sporleder (2009) approached the problem by constructing a lexical cohesion graph. In the graph, content words in a text are represented as vertices, which are connected by edges representing semantic relatedness. The intuition behind their approach was that non-literal expressions would lower the average semantic relatedness of the graph. To classify a word as literal or metaphorical, Li and Sporleder (2010) use Gaussian Mixture Models with semantic similarity features, such as the relatedness between this target word and words in its context. Broadwell et al. (2013) and Strzalkowski et al. (2013) base their approach on the idea that metaphors are likely to be concrete words that are Jang et al. (2015) model context by using both global context, the context of an entire post, and local context, the context within a sentence, in relationship to a word being classified as metaphorical or literal. They used word categories from FrameNet, topic distribution, and lexical chain information (similar in"
P16-1021,N10-2007,0,0.327172,"Missing"
P16-1021,W13-0904,0,0.0395048,"that are Jang et al. (2015) model context by using both global context, the context of an entire post, and local context, the context within a sentence, in relationship to a word being classified as metaphorical or literal. They used word categories from FrameNet, topic distribution, and lexical chain information (similar in concept to the topic chain information in (Broadwell et al., 2013)) to model the contrast between a word and its global context. To model the contrast between a word and its local context, they used lexical concreteness, word categories and semantic relatedness features. Mohler et al. (2013) built a domain-aware semantic signature for a text to capture the context surrounding a metaphorical candidate. Unlike other approaches that try to discriminate metaphors from their context, their approach uses binary classifiers to compare the semantic signature for a text with that of known metaphors. The above approaches attempted to capture governing context in various ways and were effective when applied to the problem of metaphor detection. However, these methods tend to overclassify literal instances as metaphorical when semantic cohesion is violated within their governing contexts. Ad"
P16-1021,W14-2303,0,0.0510288,"etection with Topic Transition, Emotion and Cognition in Context Hyeju Jang, Yohan Jo, Qinlan Shen, Michael Miller, Seungwhan Moon, Carolyn P. Ros´e Language Technologies Institute Carnegie Mellon University 5000 Forbes Ave, Pittsburgh, PA 15213, USA {hyejuj,yohanj,qinlans,millerm,seungwhm,cprose}@cs.cmu.edu Abstract Previous approaches to modeling metaphor have used either the semantic and syntactic information in just the sentence that contains a metaphor (Turney et al., 2011; Tsvetkov et al., 2014), or the context beyond a single sentence (Broadwell et al., 2013; Strzalkowski et al., 2013; Schulder and Hovy, 2014; Klebanov et al., 2015; Jang et al., 2015) to detect topical discrepancy between a candidate metaphor and the dominant theme (See Section 2 for more detailed literature review). Although previous approaches were effective at capturing some aspects of the governing context of a metaphor, the space of how to best use the contextual information is still wide open. Previous context-based models tend to overclassify literal words as metaphorical if they find semantic contrast with the governing context. These cases manifested in the work by Schulder and Hovy (2014) and Jang et al. (2015) as high r"
P16-1021,J15-4002,0,0.0361474,"of this approach is its accommodation of common words without discriminative power, which often confuse contextbased models. Relation to Prior Work Research in automatic metaphor detection has spanned from detecting metaphor in limited sets of syntactic constructions to studying the use of metaphor in discourse, with approaches ranging from rule-based methods using lexical resources to statistical machine learning models. Here, we focus in particular on approaches that use context wider than a sentence for metaphor detection. For a more thorough review of metaphor processing systems, refer to Shutova (2015). The main idea behind using context in metaphor detection is that metaphorically used words tend to violate lexical cohesion in text. Different methods, however, approach the problem of detecting semantic outliers in different ways. Li and Sporleder (2009; 2010) identify metaphorical idioms using the idea that non-literal expressions break lexical cohesion of a text. Li and Sporleder (2009) approached the problem by constructing a lexical cohesion graph. In the graph, content words in a text are represented as vertices, which are connected by edges representing semantic relatedness. The intui"
P16-1021,E09-1086,0,0.0802012,"Missing"
P16-1021,P07-1033,0,0.066727,"Missing"
P16-1021,W13-0909,0,0.220433,"time is valuable. Bringing in information from another domain allows more effective ways of expressing thoughts, feelings, and ideas than only using literal language. 216 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 216–225, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics not semantically associated with the surrounding context. Broadwell et al. (2013) implemented this idea using topic chains, which consist of noun phrases that are connected by pronominal mention, repetition, synonym, or hyponym relations. Strzalkowski et al. (2013) build on this idea by taking nouns and adjectives around the target concept as candidate source relations. They filtered out candidate sources that were in the same topical chain as the target concept or were not linked to the word being classified by a direct dependency path. evaluate our system on the metaphor detection task presented by Jang et al. (2015) using a breast cancer discussion forum dataset. This dataset is distinct in that it features metaphors occurring in conversational text, unlike news corpora or other formal texts typical in computational linguistics. Our contributions are"
P16-1021,P14-1024,0,0.147762,"Missing"
P16-1021,D11-1063,0,0.148676,"Missing"
P19-1329,N13-1090,0,0.268214,"embeddings capture two essential properties of numbers: magnitude (e.g. 3&lt;4) and numeration (e.g. 3=three). Our experiments reveal that most models capture an approximate notion of magnitude, but are inadequate at capturing numeration. We hope that our observations provide a starting point for the development of methods which better capture numeracy in NLP systems. 1 Introduction Word embeddings operationalize the distributional hypothesis, where a word is characterized by “the company it keeps” (Harris, 1954; Firth, 1957), and have been shown to capture semantic regularities in vector space (Mikolov et al., 2013c). They have been a driving force in NLP in recent years, and enjoy widespread use in a variety of semantic tasks (Rumelhart et al.; Mikolov et al., 2013a,b; Collobert and Weston, 2008; Glorot et al., 2011; Turney and Pantel, 2010; Turney, 2013). However, to what extent do these word representations capture numeric properties? Numbers often need to be dealt with precisely, and understanding the meaning of text also requires a careful understanding of the quantities involved. They have been identified to play an important role in textual entailment, a benchmark natural language ∗ *The first tw"
P19-1329,D09-1045,0,0.204085,"Missing"
P19-1329,C18-1198,1,0.895984,"Missing"
P19-1329,D14-1162,0,0.0838474,"Missing"
P19-1329,P18-2100,0,0.0591297,"Missing"
P19-1575,D15-1075,0,0.0129861,"se in a different text (Dagan and Glickman, 2004). The Multi-genre Natural Language Inference (MultiNLI) corpus (Williams et al., 2018) follows this definition and contains annotated pairs of sentences which are labeled as entailment if the hypothesis sentence is definitely true given the premise sentence, contradiction if the hypothesis is definitely false given the premise, and neutral if the hypothesis could be true, but is not guaranteed to be given the premise. 5758 Models and Data: We implemented two neural models for this task: a bidirectional version of the simple LSTM classifier from Bowman et al. (2015) and the decomposable attention model (DAM) (Figure 2) from Parikh et al. (2016). We use Keras (Chollet et al., 2015) with the TensorFlow (Abadi et al., 2015) backend for our implementations of both of the entailment models. Metrics of Interest: For purposes of this work, the metric of interest used is simply the class value for each data instance. For this task, the activations in the representations for each text segment learned by the model just prior to the classification step are used in the analysis. Task Knowledge: Our external knowledge for this task comes from a stress test dataset de"
P19-1575,P18-1198,0,0.179573,"rming the task. Furthermore, it enables comparison across very different architectures in terms of the extent and the manner in which each architecture has approximated use of such knowledge. In so doing, the method can also be used to formulate explanations for differences in performance between models based on relevant linguistic or task knowledge that is identified as learned or not learned by the models. This approach builds on and extends prior work using linguistic and task knowledge to understand the behavior and the results of modern neural models (Shi et al., 2016b; Adi et al., 2016; Conneau et al., 2018). In the remainder of the paper we review common techniques for network interpretation followed by a detailed description of the neural pathways approach. Next, we apply the neural pathways approach to previously published neural models, 5754 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5754–5764 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics namely models for the task of named entity recognition (NER) (Ma and Hovy, 2016) on CoNLL 2003 data for English (Sang and Meulder, 2003) and recognizing textual"
P19-1575,P18-2103,0,0.052283,"), and many others that are too numerous to list. As our work focuses on interpretation, we are not presenting new state-of-theart performance on a given task, but rather a new method to understand and compare neural models. Our evaluation is a demonstration that focuses on models trained for the Named Entity Recognition and Recognizing Textual Entailment tasks. The specific goal of our evaluation will be to demonstrate the broad applicability of the approach, and position it as building on and extending the existing body of work exploring interpretability of previously defined neural models (Glockner et al., 2018; Mudrakarta et al., 2018). We observe that neural interpretation approaches fall within several broad categories: visualizations and heatmaps (Karpathy et al., 2015; Strobelt et al., 2016), gradient-based analyses (Potapenko et al., 2017; Samek et al., 2017b; Bach et al., 2015; Arras et al., 2017), learning disentangled representations during training (Whitney, 2016; Siddharth et al., 2017; Esmaeili et al., 2018), and model probes (Shi et al., 2016a; Adi et al., 2016; Conneau et al., 2018; Zhu et al., 2018; Kuncoro et al., 2018; Khandelwal et al., 2018). Our work uses linear probes as a metho"
P19-1575,P18-1027,0,0.0316263,"ility of previously defined neural models (Glockner et al., 2018; Mudrakarta et al., 2018). We observe that neural interpretation approaches fall within several broad categories: visualizations and heatmaps (Karpathy et al., 2015; Strobelt et al., 2016), gradient-based analyses (Potapenko et al., 2017; Samek et al., 2017b; Bach et al., 2015; Arras et al., 2017), learning disentangled representations during training (Whitney, 2016; Siddharth et al., 2017; Esmaeili et al., 2018), and model probes (Shi et al., 2016a; Adi et al., 2016; Conneau et al., 2018; Zhu et al., 2018; Kuncoro et al., 2018; Khandelwal et al., 2018). Our work uses linear probes as a method to identify the function of groups of neurons that are correlated with linguistic and tasklevel features, rather than for interpretation of individual neurons. Through correlation with the pathway analysis, we can furthermore reason about the role that those linguistic and task-level features have in the network’s predictions. Recent attempts to understand the functioning of trained neural models have limited themselves to investigations of the function of individual neurons or individual architectural components. An early way to probe the function of"
P19-1575,P18-1132,0,0.056844,"Missing"
P19-1575,N19-1225,0,0.0227284,"Missing"
P19-1575,P16-1101,0,0.109815,"nd the results of modern neural models (Shi et al., 2016b; Adi et al., 2016; Conneau et al., 2018). In the remainder of the paper we review common techniques for network interpretation followed by a detailed description of the neural pathways approach. Next, we apply the neural pathways approach to previously published neural models, 5754 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5754–5764 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics namely models for the task of named entity recognition (NER) (Ma and Hovy, 2016) on CoNLL 2003 data for English (Sang and Meulder, 2003) and recognizing textual entailment (Dagan and Glickman, 2004). We compare across different neural architectures through a shared lens comprising linguistic and task-level heuristics for the two target tasks and draw conclusions about learning outcomes on those tasks. 2 Related Work Our work falls under the broad topic of neural network interpretation. Recently, in this area of research a wide variety of models have been the target of investigation, including additive classifiers (Poulin et al., 2006), kernel-based classifiers (Baehrens e"
P19-1575,N18-2017,0,0.0127802,"which neurons behave cohesively, indicating a subprocess within the network. However, these subprocesses do not correspond strongly to any of the tested features. Consequences of this finding could be an indication that the model is ‘cheating’ on the task and has some inductive bias that is beneficial to the task independent from the task as envisioned by the creators. Otherwise, if many models demonstrate this behavior, the task or dataset may be insufficient to induce the desired learning behavior in neural models. This is consistent with recent highly domain specific analyses of this task (Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018). The NER model analysis was set up to understand the factors contributing to the differences between the two models rather than the factors influencing the prediction accuracy. Many of the surface features that were tested were present in the models, although there were not significant differences as to which of these features were encoded in one model or the other. Examination of the correlation of each pathway to the prediction differences between the models indicate that the differences were primarily explained by pathways that had high amounts"
P19-1575,P18-1176,0,0.0150818,"are too numerous to list. As our work focuses on interpretation, we are not presenting new state-of-theart performance on a given task, but rather a new method to understand and compare neural models. Our evaluation is a demonstration that focuses on models trained for the Named Entity Recognition and Recognizing Textual Entailment tasks. The specific goal of our evaluation will be to demonstrate the broad applicability of the approach, and position it as building on and extending the existing body of work exploring interpretability of previously defined neural models (Glockner et al., 2018; Mudrakarta et al., 2018). We observe that neural interpretation approaches fall within several broad categories: visualizations and heatmaps (Karpathy et al., 2015; Strobelt et al., 2016), gradient-based analyses (Potapenko et al., 2017; Samek et al., 2017b; Bach et al., 2015; Arras et al., 2017), learning disentangled representations during training (Whitney, 2016; Siddharth et al., 2017; Esmaeili et al., 2018), and model probes (Shi et al., 2016a; Adi et al., 2016; Conneau et al., 2018; Zhu et al., 2018; Kuncoro et al., 2018; Khandelwal et al., 2018). Our work uses linear probes as a method to identify the function"
P19-1575,C18-1198,1,0.843039,"2) from Parikh et al. (2016). We use Keras (Chollet et al., 2015) with the TensorFlow (Abadi et al., 2015) backend for our implementations of both of the entailment models. Metrics of Interest: For purposes of this work, the metric of interest used is simply the class value for each data instance. For this task, the activations in the representations for each text segment learned by the model just prior to the classification step are used in the analysis. Task Knowledge: Our external knowledge for this task comes from a stress test dataset developed for models trained on the MultiNLI corpus (Naik et al., 2018). There are nine categories and subcategories, each of which contains data instances that require a specific type or reasoning to correctly identify the entailment relationship. We combine all of the data instances in the stress test and tag each with the category or subcategory it belongs to. The entailment models’ representations are analyzed in terms of the type of reasoning they can perform. While we acknowledge that recent work by Liu et al. (2019) has found limitations in this dataset with respect to the reasoning that is required for the models to achieve, we use it as a foundation for"
P19-1575,D16-1244,0,0.0718689,"Missing"
P19-1575,S18-2023,0,0.0384633,"Missing"
P19-1575,N16-3020,0,0.045435,"omponents. An early way to probe the function of target components, as Karpathy et al. (2015) and Strobelt et al. (2016) have each proposed, is by visualizing patterns of activation through the target components, for example using heatmaps. However, making meaningful patterns apparent in these visualizations can be highly dependent on the artful arrangement of the data presented within them, and it is easy to overlook patterns that are not immediately obvious. There have also been approaches that made use of simpler classifiers to predict and then explain mistakes made by more complex models (Ribeiro et al., 2016; Krishnan and Wu, 2017). In a similar vein, linear classifier probes have been used by Alain and Bengio (2016) to co-train simple linear models to illustrate functions performed by particular layers in arbitrarily deep models, and then later by associating the learned patterns in the linear models with task or linguistic knowledge determined by hand or through some other means to be relevant or not instance-by-instance. More recently, Montavon et al. (2017) published a detailed tutorial on the recent approaches and techniques of interpreting deep neural networks. They identified cross-cutting"
P19-1575,W03-0419,0,0.317975,"Missing"
P19-1575,D16-1248,0,0.176577,"derstood non-neural methods for performing the task. Furthermore, it enables comparison across very different architectures in terms of the extent and the manner in which each architecture has approximated use of such knowledge. In so doing, the method can also be used to formulate explanations for differences in performance between models based on relevant linguistic or task knowledge that is identified as learned or not learned by the models. This approach builds on and extends prior work using linguistic and task knowledge to understand the behavior and the results of modern neural models (Shi et al., 2016b; Adi et al., 2016; Conneau et al., 2018). In the remainder of the paper we review common techniques for network interpretation followed by a detailed description of the neural pathways approach. Next, we apply the neural pathways approach to previously published neural models, 5754 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5754–5764 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics namely models for the task of named entity recognition (NER) (Ma and Hovy, 2016) on CoNLL 2003 data for English (Sang"
P19-1575,D16-1159,0,0.193042,"derstood non-neural methods for performing the task. Furthermore, it enables comparison across very different architectures in terms of the extent and the manner in which each architecture has approximated use of such knowledge. In so doing, the method can also be used to formulate explanations for differences in performance between models based on relevant linguistic or task knowledge that is identified as learned or not learned by the models. This approach builds on and extends prior work using linguistic and task knowledge to understand the behavior and the results of modern neural models (Shi et al., 2016b; Adi et al., 2016; Conneau et al., 2018). In the remainder of the paper we review common techniques for network interpretation followed by a detailed description of the neural pathways approach. Next, we apply the neural pathways approach to previously published neural models, 5754 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5754–5764 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics namely models for the task of named entity recognition (NER) (Ma and Hovy, 2016) on CoNLL 2003 data for English (Sang"
P19-1575,N18-1101,0,0.0120147,"(a) b2 ....... bn F(b) G(a, β ) G(b, α) H ( ∑ G( a, β ), ∑ G( b, α)) ˆ y Figure 2: Decomposable Attention Model. Dotted arrows indicate networks with shared weights. is performing. The linear probe and linear comparison performance are likewise related to how likely the information is stored within the pathway and how influential that pathway is on the metric respectively. 4 Experiments To evaluate our interpretation technique on real world data, we applied our method on four trained models over two tasks: recognizing textual entailment using the Multi-genre Natural Language Inference corpus (Williams et al., 2018) and named entity recognition using the CoNLL 2003 data (Sang and Meulder, 2003) for English NER. The analysis was implemented using Scikit-Learn (Pedregosa et al., 2011) and SciPy (Jones et al., 2001–) and unless otherwise noted used default hyperparameters. 4.1 Recognizing Textual Entailment Recognizing textual entailment is a task comprised of deciding whether the concepts presented in one text can be determined to be true given some context or premise in a different text (Dagan and Glickman, 2004). The Multi-genre Natural Language Inference (MultiNLI) corpus (Williams et al., 2018) follows"
P19-1575,P18-2100,0,0.0179635,"sting body of work exploring interpretability of previously defined neural models (Glockner et al., 2018; Mudrakarta et al., 2018). We observe that neural interpretation approaches fall within several broad categories: visualizations and heatmaps (Karpathy et al., 2015; Strobelt et al., 2016), gradient-based analyses (Potapenko et al., 2017; Samek et al., 2017b; Bach et al., 2015; Arras et al., 2017), learning disentangled representations during training (Whitney, 2016; Siddharth et al., 2017; Esmaeili et al., 2018), and model probes (Shi et al., 2016a; Adi et al., 2016; Conneau et al., 2018; Zhu et al., 2018; Kuncoro et al., 2018; Khandelwal et al., 2018). Our work uses linear probes as a method to identify the function of groups of neurons that are correlated with linguistic and tasklevel features, rather than for interpretation of individual neurons. Through correlation with the pathway analysis, we can furthermore reason about the role that those linguistic and task-level features have in the network’s predictions. Recent attempts to understand the functioning of trained neural models have limited themselves to investigations of the function of individual neurons or individual architectural co"
P95-1005,1993.iwpt-1.12,0,0.0588063,"Missing"
P95-1005,C88-2120,0,0.0306128,"w is the next week? (13) If all else fails there is always video conferencing. (14) S 1: Monday, Tuesday, and Wednesday I am out of town. (15) But Thursday and Friday are both good. (16) How about Thursday at twelve? (17) $2: Sounds good. (18) See you then. Figure 1: E x a m p l e o f D e l i b e r a t i n g O v e r A M e e t i n g T i m e purpose of making our argument easy to follow. Notice that in both of these examples, the speakers negotiate over multiple alternatives in parallel. We challenge an assumption underlying the best known theories of discourse structure (Grosz and Sidner 1986; Scha and Polanyi 1988; Polanyi 1988; Mann and Thompson 1986), namely that discourse has a recursive, tree-like structure. Webber (1991) points out that Attentional State i is modeled equivalently as a stack, as in Grosz and Sidner&apos;s approach, or by constraining the current discourse segment to attach on the rightmost frontier of the discourse structure, as in Polanyi and Scha&apos;s approach. This is because attaching a leaf node corresponds to pushing a new element on the stack; adjoining a node Di to a node Dj corresponds to popping all the stack elements through the one corresponding to Dj and pushing Di on the stac"
P95-1005,J86-3001,0,\N,Missing
P98-2185,C96-1075,1,0.898713,"Missing"
P98-2185,W97-0303,1,0.892031,"Missing"
P98-2185,W94-0113,1,0.861543,"Missing"
P98-2185,P95-1005,1,0.873607,"Missing"
W03-0205,W01-1609,0,0.112852,"ialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then in text based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available for spontaneous self-explanation. Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states (Ang et al., 2002; Batliner et al., 2000) and user responses to system errors (Litman et al., 2001) that are useful for triggering system adaptation. We are thus currently developing a speech based dialogue system that uses a text based system (VanLehn et al., 2002) as its “back-end”. These systems and their goals will be discussed in"
W03-0205,H93-1016,0,0.0372439,"hysics problem. A computer tutor then engages the student in a natural language dialogue to provide feedback, correct misconceptions, and to elicit more complete explanations. The first version of Why2-Atlas was deployed and evaluated with undergraduate students in the spring of 2002; the system is continuing to be actively developed (Graesser et al., 2002). We are currently developing a speech-enabled version of Why2-ATLAS, called ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), that uses the Why2-Atlas system as its “back-end”. To date we have interfaced the Sphinx2 speech recognizer (Huang et al., 1993) with stochastic language models trained from example user utterances, and the Festival speech synthesizer (Black and Taylor, 1997) for text-to-speech, to the Why2-Atlas backend. The rest of the needed natural language processing components, e.g. the sentence-level syntactic and semantic analysis modules (Ros´e, 2000), discourse and domain level processors (Makatchev et al., 2002), and a finite-state dialogue manager (Ros´e et al., 2001), are provided by a toolkit that is part of the Why2-Atlas backend. The student speech is digitized from microphone input, while the tutor’s synthesized speech"
W03-0205,P01-1048,1,0.792039,"spoken tutoring then in text based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available for spontaneous self-explanation. Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states (Ang et al., 2002; Batliner et al., 2000) and user responses to system errors (Litman et al., 2001) that are useful for triggering system adaptation. We are thus currently developing a speech based dialogue system that uses a text based system (VanLehn et al., 2002) as its “back-end”. These systems and their goals will be discussed in Section 2. We expect that the different modalities used by these systems (e.g. text based vs speech based) will display interesting differences with respect to the characteristics of dialogue interaction that may determine their relative merits with respect to increasing student performance. Although human-computer data from the speech based system is not yet"
W03-0205,N03-2018,1,0.797483,"isn’t present in typed dialogue. Connections between learning and emotion have been well documented (Coles, 1999), so it seems likely that the success of computer-based tutoring systems could be greatly increased if they were capable of predicting and adapting to student emotional states, e.g. reinforcing positive states, while rectifying negative states (Evens, 2002). Preliminary machine learning experiments involving emotion annotation and automatic feature extraction from our corpus suggest that ITSPOKE can indeed be enhanced to automatically predict and adapt to student emotional states (Litman et al., 2003). 3 Typed Human-Human Tutoring Corpus The Why2-Atlas Human-Human Typed Tutoring Corpus is a collection of typed tutoring dialogues between (human) tutor and student collected via typed interface, which the tutor plays the same role that Why2-Atlas is designed to perform. The experimental procedure is as follows: 1) students are given a pretest measuring their knowledge of physics, 2) students are asked to read through a small document of background material, 3) students work through a set of up to 10 Why2-Atlas training problems with the human tutor, and 4) students are given a post-test that"
W03-0205,W02-0211,1,0.832815,"eveloping a speech-enabled version of Why2-ATLAS, called ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), that uses the Why2-Atlas system as its “back-end”. To date we have interfaced the Sphinx2 speech recognizer (Huang et al., 1993) with stochastic language models trained from example user utterances, and the Festival speech synthesizer (Black and Taylor, 1997) for text-to-speech, to the Why2-Atlas backend. The rest of the needed natural language processing components, e.g. the sentence-level syntactic and semantic analysis modules (Ros´e, 2000), discourse and domain level processors (Makatchev et al., 2002), and a finite-state dialogue manager (Ros´e et al., 2001), are provided by a toolkit that is part of the Why2-Atlas backend. The student speech is digitized from microphone input, while the tutor’s synthesized speech is played to the student using a speaker and/or headphone. We are now in the process of adapting the knowledge sources needed by the spoken language components to our application domain. For example, we have developed a set of dialogue dependent language models using the experimental human-computer typed corpus (4551 student utterances) obtained during the Why2-Atlas 2002 evaluat"
W03-0210,P98-1032,0,0.0137608,"rom students, such as “heavier objects exert more force.” (Hake, 1998; Halloun and Hestenes, 1985). In Why2-Atlas, a student first types an essay answering a qualitative physics problem. A computer tutor then engages the student in a natural language dialogue to provide feedback, correct misconceptions, and to elicit more complete explanations. The first version of Why2-Atlas was deployed and evaluated with undergraduate students in the spring of 2002; the system is continuing to be actively developed (Graesser et al., 2002). In contrast to many previous approaches to automated essay grading (Burstein et al., 1998; Foltz et al., 1998; Larkey, 1998), our goal is not to assign a letter grade to student essays. Instead, our purpose is to tally which set of “correct answer aspects” are present in student essays. For example, we expect satisfactory answers to the example question above to include a detailed explanation of how Newton’s first law applies to this scenario. From Newton’s first law, the student should infer that the pumpkin and the man will continue at the same constant horizontal velocity that they both had before the release. Thus, they will always have the same displacement from the point of"
W03-0210,P01-1014,0,0.0193714,"iled explanation of how Newton’s first law applies to this scenario. From Newton’s first law, the student should infer that the pumpkin and the man will continue at the same constant horizontal velocity that they both had before the release. Thus, they will always have the same displacement from the point of release. Therefore, after the pumpkin rises and falls, it will land back in the man’s hands. Our goal is to coach students through the process of constructing good physics explanations. Thus, our focus is on the physics content and not the quality of the student’s writing, in contrast to (Burstein et al., 2001). 2 Student Essay Analysis We cast the Student Essay Analysis problem as a text classification problem where we classify each sentence in the student’s essay as an expression one of a set of “correct answer aspects”, or “nothing” in the case where no “correct answer aspect” was expressed. After a student attempts an initial answer to the question, the system analyzes the student’s essay to assess which key points are missing from the student’s argument. The system then uses its analysis of the student’s essay to determine which help to offer that student. In order to do an effective job at sel"
W03-0210,C94-1042,0,0.0194465,"e “bag of words” Naive Bayes classification of the input sentence, CarmelTC builds a vector representation of each input sentence, with each vector position corresponding to one of these features. We then use the ID3 decision tree learning algorithm (Mitchell, 1997; Quinlin, 1993) to induce rules for identifying sentence classes based on these feature vectors. The symbolic features used for the CarmelTC approach are extracted from a deep syntactic functional analysis constructed using the CARMEL broad coverage English syntactic parsing grammar (Ros´e, 2000) and the large scale COMLEX lexicon (Grishman et al., 1994), containing 40,000 lexical items. For parsing we use an incremental version of the LCF LEX robust parser (Ros´e et al., 2002b; Ros´e and Lavie, 2001), which was designed for efficient, robust interpretation. While computing a deep syntactic analysis is more computationally expensive than computing a shallow syntactic analysis, we can do so very efficiently using the incrementalized version of LCF LEX because it takes advantage of student typing time to reduce the time delay between when students submit their essays and when the system is prepared to respond. Syntactic feature structures produ"
W03-0210,A00-2041,1,0.927413,"Missing"
W04-2803,P98-1032,0,0.08013,"Missing"
W04-2803,C94-1042,0,0.0204715,"the man and the keys Figure 2: Example of how deep syntactic analysis facilitates uncovering complex relationships encoded syntactically within a sentence One of the goals behind the design of Carmel-Tools is to leverage off of the normalization over surface syntactic variation that deep syntactic analysis provides. While our approach is not specific to a particular framework for deep syntactic analysis, we have chosen to build upon the publicly available LCF LEX robust parser (Ros´e et al., 2002), the CARMEL grammar and semantic interpretation framework (Ros´e, 2000), and the COMLEX lexicon (Grishman et al., 1994). This same broad coverage, domain general interpretation framework has already been used in a number of educational applications including (Zinn et al., 2002; VanLehn et al., 2002). Syntactic feature structures produced by the CARMEL grammar normalize those aspects of syntax that modify the surface realization of a sentence but do not change its deep functional analysis. These aspects include tense, negation, mood, modality, and syntactic transformations such as passivization and extraction. Thus, a sentence and it’s otherwise equivalent passive counterpart would be encoded with the same set"
W04-2803,P85-1008,0,0.55906,"ols offers greater flexibility in output representation than the context-free rewrite rules produced by previous semantic authoring tools, allowing authors to design their own predicate language representations that are not constrained to follow the structure of the input text (See Figure 1 for a simple example and Figure 2 for a more complex example.). See Section 3 and (Ros´e, 2000; Ros´e et al., 2002) for more details about CARMEL’s knowledge source representation. Note that the predicate language representation utilized by Carmel-Tools is in the style of Davidsonian event based semantics (Hobbs, 1985). For example, in Figure 1 notice that the first argument of each predicate is an identification token that represents the whole predicate. These identification tokens can then be bound to arguments of other predicates, and in that way be used to represent relationships between predicates. For example, the rel-value predicate expresses the idea that the predicates indicated by id1 and id2 are equal in value. While language understanding systems with this style of analysis are not a new idea, the contribution of this work is a set of authoring tools that simplify the semantic knowledge sources"
W04-2803,A97-1051,0,0.0244068,"ontal constant non-zero) (velocity id2 pumpkin ?dir ?mag-change ?mag-zero) (rel-value id3 id1 id2 equal)) Gloss: The constant, nonzero, horizontal velocity of the man is equal to the velocity of the pumpkin. Figure 1: Simple example of how Carmel-Tools builds knowledge sources capable of assigning representations to sentences that are not constrained to mirror the exact wording, structure, or literal surface meaning of the text. While much work has been done in the area of robust semantic interpretation, current authoring tools for building semantic knowledge sources (Cunningham et al., 2003; Jay et al., 1997) are tailored for information extraction tasks that emphasize the identification of named entities such as people, locations, and organizations. While regular expression based recognizers, such as JAPE (Cunningham et al., 2000), used for information extraction systems, are not strictly limited to these standard entity types, it is not clear how they would handle concepts expressing complex relationships between entities, where the complexity in the meaning can be realized with a much greater degree of surface syntactic variation. Outside of the information extraction domain, a concept acquisit"
W04-2803,W03-0209,0,0.0390926,"Missing"
W04-2803,W03-0210,1,0.775569,"Missing"
W04-2803,A00-2041,1,0.900995,"Missing"
W05-0208,W04-2803,1,0.78151,"uthoring environment developed with the long term goal of enabling the authoring of effective tutorial dialogue agents. It was designed for developers without expertise in knowledge representation, artificial intelligence, or computational linguistics. In our previous work we have reported progress to45 Proceedings of the 2nd Workshop on Building Educational Applications Using NLP, c pages 45–52, Ann Arbor, June 2005. Association for Computational Linguistics, 2005 velopers of educational technology, the system should involve minimal programming. The design of Carmel-Tools (Rosé et al., 2003; Rosé & Hall, 2004), the first generation of our authoring tools, was based on these obvious desiderata and not on any in-depth analysis of data collected from our target user population. While an evaluation of the underlying computational linguistics technology showed promise (Rosé & Hall, 2004), the results from actual authoring use were tremendously disappointing. A formal study reported in (Rosé, et al., 2005) demonstrates that even individuals with expertise in computational linguistics have difficulty predicting the coverage of knowledge sources that would be generated automatically from example texts anno"
W05-0208,A00-2041,1,0.836111,"the computational linguistics community, our experience tells us that insufficient attention to these details leads to the development of tools that are unusable, particularly to the user population that we target with our work. Some desiderata related to the design of our system are obvious based on our target user population. Currently, many educational technology oriented research groups do not have computational linguists on their staff with the expertise required to author domain specific knowledge sources for use with sophisticated state-of-the-art understanding systems, such as CARMEL (Rosé, 2000) or TRIPS (Allen et al., 2001). However, previous studies have shown that, while scaffolding and guidance is required to support the authoring process, noncomputational linguists possess many of the basic skills required to author conversational interfaces (Rosé, Pai, & Arguello, 2005). Because the main barrier of entry to such sophisticated tools are expertise in understanding the underlying data structures and linguistically motivated representation, our tools should have an interface that masks the unnecessary details and provides intuitive widgets that manipulate the data in ways that are"
W05-1524,C04-1055,0,0.0195129,"October 2005. 2005 Association for Computational Linguistics of similar constituents, and the parser frequently reaches the chart size limit before finding the correct constituent to use. Ambiguity packing in TF LEX helps chose the best constituents to prune by pruning competing interpretations which cover the same span and have the same non-local features, thus making it less likely that a constituent essential for building a parse will be pruned. 3 Evaluation Our evaluation data is an excerpt from the Monroe corpus that has been used in previous TRIPS research on parsing speed and accuracy (Swift et al., 2004). The test contained 1042 utterances, from 1 to 45 words in length (mean 5.38 words/utt, st. dev. 5.7 words/utt). Using a hold-out set, we determined that a beam width of 3 was an optimal setting for TF LEX. We then compared TF LEX at beam width 3 to the TRIPS parser with chart size limits of 1500, 5000, and 10000. As our evaluation metrics we report are average parse time per sentence and probability of finding at least one parse, the latter being a measure approximating parsing accuracy. The results are presented in Figure 1. We grouped sentences into equivalence classes based on length with"
W05-1524,W04-0214,1,0.889578,"Missing"
W06-3407,N04-1015,0,0.0627095,"A-based algorithms for segmentation are described in (Foltz, 1998) and (Olney and Cai, 2005). Foltz’s approach differs from TextTiling mainly in its use of an LSA-based vector space model. Olney and Cai address a problem not addressed by TextTiling or Foltz’s approach, which is that cohesion is not just a function of the repetition of thematically-related terms, but also a function of the presentation of new information in reference to information already presented. Their orthonormal basis approach allows for segmentation based on relevance and informativity. Content-oriented models, such as (Barzilay and Lee, 2004), rely on the re-occurrence of patterns of topics over multiple realizations of thematically similar discourses, such as a series of newspaper articles about similar events. Their approach utilizes a hidden Markov model where states correspond to topics and state transition probabilities correspond to topic shifts. To obtain the desired number of topics (states), text spans of uniform length (individual contributions, in our case) are clustered. Then, state emission probabilities are induced using smoothed cluster-specific language models. Transition probabilities are induced by considering th"
W06-3407,H05-1122,0,0.248217,"on, the cosine correlation between the upper and lower regions of the sliding window is compared with that of the peak cosine correlation values to the left and right of the window. A seg43 ment boundary is predicted when the magnitude of the difference exceeds a threshold. One drawback to relying on term co-occurrence to signal topic continuity is that synonyms or related terms are treated as thematically-unrelated. One proposed solution to this problem is Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997). Two LSA-based algorithms for segmentation are described in (Foltz, 1998) and (Olney and Cai, 2005). Foltz’s approach differs from TextTiling mainly in its use of an LSA-based vector space model. Olney and Cai address a problem not addressed by TextTiling or Foltz’s approach, which is that cohesion is not just a function of the repetition of thematically-related terms, but also a function of the presentation of new information in reference to information already presented. Their orthonormal basis approach allows for segmentation based on relevance and informativity. Content-oriented models, such as (Barzilay and Lee, 2004), rely on the re-occurrence of patterns of topics over multiple reali"
W06-3407,P93-1020,0,0.232901,"contributions improves topic segmentation quality. 1 2 Introduction In this paper we explore the problem of topic segmentation of dialogue. Use of topic-based models of dialogue has played a role in information retrieval (Oard et al., 2004), information extraction (Baufaden, 2001), and summarization (Zechner, 2001), just to name a few applications. However, most previous work on automatic topic segmentation has focused primarily on segmentation of expository text. This paper presents a survey of the state-of-the-art in topic segmentation technology. Using the definition of topic segment from (Passonneau and Litman, 1993) applied to two different dialogue corpora, we present an evaluation including a detailed error analysis, illustrating why approaches designed for expository text do not generalize well to dialogue. We first demonstrate a significant advantage of our hybrid, supervised learning approach called Museli, a multi-source evidence integration approach, over competing algorithms. We then extend the basic Museli algorithm by introducing an intermediate level of analysis based on Sinclair and Coulthard’s notion of a dialogue exchange (SinDefining Topic In the most general sense, the challenge of topic"
W06-3407,J97-1003,0,\N,Missing
W06-3502,P94-1040,0,0.324891,"ep interpretation systems (Copestake and Flickinger, 2000; Swift et al., 2004; Maxwell and Kaplan, 1994), which forms the foundation that we build on in this work. For example, techniques for speeding up unification in HPSG lead to dramatic improvements in efficiency (Kiefer et al., 1999). Likewise ambiguity packing and CFG backbone parsing (Maxwell and Kaplan, 1994; van Noord, 1997) are known to increase parsing efficiency. However, as we show in this paper, these techniques depend on specific grammar properties that do not hold for all grammars. This claim is consistent with observations of Carroll (1994) that parsing software optimisation techniques tend to be limited in their applicability to the individual grammars they were developed for. While we used TRIPS as our example unification-based grammar, this investigation is important not only for this project, but in the general context of speeding up a wide-coverage unification grammar which incorporates fragment 9 Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 9–16, c New York City, June 2006. 2006 Association for Computational Linguistics rules and lexical semantics, which may not be immediately provided"
W06-3502,copestake-flickinger-2000-open,0,0.266849,"ng its speed and coverage for longer utterances. While typical sentences in dialogue contexts are shorter than in expository text domains, longer utterances are important in discussion oriented domains. For example, in educational applications of dialogue it is important to elicit deep explanation from students and then offer focused feedback based on the details of what students say. The choice of instructional dialogue as a target application influenced the choice of parser we needed to use for interpretation in a dialogue system. Several deep, wide-coverage parsers are currently available (Copestake and Flickinger, 2000; Ros´e, 2000; Baldridge, 2002; Maxwell and Kaplan, 1994), but many of these have not been designed with issues related to interpretation in a dialogue context in mind. Carolyn P. Ros´e Carnegie Mellon University Language Technologies Institute, Pittsburgh, PA 15213, cprose@cs.cmu.edu The TRIPS grammar (Dzikovska et al., 2005) is a wide-coverage unification grammar that has been used very successfully in several task-oriented dialogue systems. It supports interpretation of fragments and lexical semantic features (see Section 2 for a more detailed discussion), and provides additional robustness"
W06-3502,W05-1525,1,0.864164,"Missing"
W06-3502,W05-1526,0,0.0206713,"table for the ambiguity level in the TRIPS grammar, but that by introducing a pruning method that uses ambiguity packing to guide pruning decisions, we can achieve significant improvements in both speed and coverage compared to the original TRIPS parser. 3 The TRIPS and LCFLEX algorithms 3.1 The TRIPS parser The TRIPS parser we use as a baseline is a bottomup chart parser with lexical entries and rules represented as attribute-value structures. To achieve parsing efficiency, TRIPS uses a best-first beam search algorithm based on the scores from a parse selection model (Dzikovska et al., 2005; Elsner et al., 2005). The constituents on the parser’s agenda are grouped into buckets based on their scores. At each step, the bucket with the highest scoring constituents is selected to build/extend chart edges. The parsing stops once N requested analyses are found. This guarantees that the parser returns the N -best list of analyses according to the parse selection model used, unless the parser reaches the chart size limit. 1 Other enhancements used by LINGO depend on disallowing disjunctive features, and relying instead on the type system. The TRIPS grammar is untyped and uses disjunctive features, and conver"
W06-3502,P03-1014,0,0.0235456,". This guarantees that the parser returns the N -best list of analyses according to the parse selection model used, unless the parser reaches the chart size limit. 1 Other enhancements used by LINGO depend on disallowing disjunctive features, and relying instead on the type system. The TRIPS grammar is untyped and uses disjunctive features, and converting it to a typed system would require as yet undetermined amount of additional work. In addition to best-first parsing, the TRIPS parser uses a chart size limit, to prevent the parser from running too long on unparseable utterances, similar to (Frank et al., 2003). TRIPS is much slower processing utterances not covered in the grammar, because it continues its search until it reaches the chart limit. Thus, a lower chart limit improves parsing efficiency. However, we show in our evaluation that the chart limit necessary to obtain good performance in most cases is too low to find parses for utterances with 15 or more words, even if they are covered by the grammar. The integration of lexical semantics in the TRIPS lexicon has a major impact on parsing in TRIPS. Each word in the TRIPS lexicon is associated with a semantic type from a domain-independent onto"
W06-3502,P99-1061,0,0.0609653,"Missing"
W06-3502,C04-1100,0,0.0166149,"n, supports parsing fragmentary utterance in dialogue, and could be used to start development without large quantities of corpus data. TRIPS fulfilled our requirements better than similar alternatives, such as LINGO ERG (Copestake and Flickinger, 2000) or XLE (Maxwell and Kaplan, 1994). TRIPS produces logical forms which include semantic classes and roles in a domain-independent frame-based formalism derived from FrameNet and VerbNet (Dzikovska et al., 2004; Kipper et al., 2000). Lexical semantic features are known to be helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu, 2004). Apart from TRIPS, these have not been integrated into existing deep grammars. While both LINGO-ERG and XLE include semantic features related to scoping, in our applications the availability of semantic classes and semantic role assignments was more important to interpretation, and these features are not currently available from those parsers. Finally, TRIPS provides a domainindependent parse selection model, as well as rules for interpreting discourse fragments (as was also done in HPSG (Schlangen and Lascarides, 2003), a feature actively used in interpretation. While TRIPS provides the capa"
W06-3502,A00-2022,0,0.0265264,"a train” had over 700 possible CFG analyses, and took 910 msec to parse compared to 10 msec with interleaved unification. CFG ambiguity is so high because noun phrase fragments are allowed as top-level categories, and lexical ambiguity is compounded with semantic ambiguity and robust rules normally disallowed by features during unification. Thus, in our combined algorithm we had to use unification interleaved with parsing to filter out the CFG constituents. 4.2 Ambiguity Packing For building semantic representations in parallel with parsing, ambiguity packing presents a set of known problems (Oepen and Carroll, 2000). One possible solution is to exclude semantic features during an initial unification stage, use ambiguity packing, and re-unify with semantic features in a postprocessing stage. In our case, we found this strategy difficult to implement, since selectional restrictions are used to limit the ambiguity created by multiple word senses during syntactic parsing. Therefore, we chose to do ambiguity packing on the CFG structure only, keeping the multiple feature structures associated with each packed CFG constituent. To begin to evaluate the contribution of ambiguity packing on efficiency, we ran a t"
W06-3502,A00-2041,1,0.833634,"Missing"
W06-3502,W03-2106,0,0.0183856,"e helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu, 2004). Apart from TRIPS, these have not been integrated into existing deep grammars. While both LINGO-ERG and XLE include semantic features related to scoping, in our applications the availability of semantic classes and semantic role assignments was more important to interpretation, and these features are not currently available from those parsers. Finally, TRIPS provides a domainindependent parse selection model, as well as rules for interpreting discourse fragments (as was also done in HPSG (Schlangen and Lascarides, 2003), a feature actively used in interpretation. While TRIPS provides the capabilities we need, its parse times for long sentences (above 15 words long) are intolerably long. We considered two pos10 sible techniques for speeding up parsing: speeding up unification using the techniques similar to the LINGO system (Copestake and Flickinger, 2000), or using backbone extraction (Maxwell and Kaplan, 1994; Ros´e and Lavie, 2001; Briscoe and Carroll, 1994). TRIPS already uses a fast unification algorithm similar to quasi-destructive unification, avoiding copying during unification.1 However, the TRIPS gr"
W06-3502,P98-2196,0,0.0216696,"ar (Dzikovska et al., 2005) is a wide-coverage unification grammar that has been used very successfully in several task-oriented dialogue systems. It supports interpretation of fragments and lexical semantic features (see Section 2 for a more detailed discussion), and provides additional robustness through “robust” rules that cover common grammar mistakes found in dialogue such as missing articles or incorrect agreement. These enhancements help parsing dialogue (both spoken and typed), but they significantly increase grammar ambiguity, a common concern in building grammars for robust parsing (Schneider and McCoy, 1998). It is specifically these robustness-efficiency trade-offs that we address in this paper. Much work has been done related to enhancing the efficiency of deep interpretation systems (Copestake and Flickinger, 2000; Swift et al., 2004; Maxwell and Kaplan, 1994), which forms the foundation that we build on in this work. For example, techniques for speeding up unification in HPSG lead to dramatic improvements in efficiency (Kiefer et al., 1999). Likewise ambiguity packing and CFG backbone parsing (Maxwell and Kaplan, 1994; van Noord, 1997) are known to increase parsing efficiency. However, as we"
W06-3502,C04-1055,0,0.0606253,"e detailed discussion), and provides additional robustness through “robust” rules that cover common grammar mistakes found in dialogue such as missing articles or incorrect agreement. These enhancements help parsing dialogue (both spoken and typed), but they significantly increase grammar ambiguity, a common concern in building grammars for robust parsing (Schneider and McCoy, 1998). It is specifically these robustness-efficiency trade-offs that we address in this paper. Much work has been done related to enhancing the efficiency of deep interpretation systems (Copestake and Flickinger, 2000; Swift et al., 2004; Maxwell and Kaplan, 1994), which forms the foundation that we build on in this work. For example, techniques for speeding up unification in HPSG lead to dramatic improvements in efficiency (Kiefer et al., 1999). Likewise ambiguity packing and CFG backbone parsing (Maxwell and Kaplan, 1994; van Noord, 1997) are known to increase parsing efficiency. However, as we show in this paper, these techniques depend on specific grammar properties that do not hold for all grammars. This claim is consistent with observations of Carroll (1994) that parsing software optimisation techniques tend to be limit"
W06-3502,J97-3004,0,0.0892484,"Missing"
W06-3502,J93-4001,0,\N,Missing
W06-3502,J93-1002,0,\N,Missing
W06-3502,C98-2191,0,\N,Missing
W09-1903,N09-3010,1,0.82093,"perience, programming experience, etc. We used the first 200 movie reviews from the dataset provided by Zaidan et al. (2007), with an equal distribution of positive and negative examples. Each group annotated 25 movie reviews randomly selected from the 200 reviews and all annotators in each group annotated all 25 reviews. In addition to voting positive or negative for a review, annotators also annotated rationales (Zaidan et al., 2007), spans of text in the review that support their vote. Rationales can be used to guide the model by identifying the most discriminant features. In related work (Arora and Nyberg, 2009), we ascertain that with rationales the same performance can be achieved with less annotated data. The annotation task with rationales involved a variety of user actions: voting positive or negative, highlighting spans of text and adding rationale annotations. We used the same annotation guidelines as Zaidan et al. (2007). The data has been made available for research purposes1 . Figure 1 shows a screenshot of the GUI used. We performed an analysis of our data similar to that conducted by Settles et al. (2008). We address the following main questions. Are the annotation times variable enough?"
W09-1903,W04-2412,0,0.0934527,"Missing"
W09-1903,W00-1306,0,0.0297815,"calculated exactly before querying the user. For example, in biological experiments it might be calculable from the cost of the equipment and the material used (King et al., 2004). In NLP, sometimes a simplifying assumption is made that the annotation cost for an example can be measured in terms of its length (e.g. seconds of voicemail annotated (Kapoor et al., 2007); number of tokens annotated (Tomanek et al., 2007)). Another assumption is that the number of user annotation actions can be used as a proxy for annotation cost of an example (e.g. number of brackets added for parsing a sentence (Hwa, 2000); number of clicks for correcting named entities (Kristjannson et al., 2004)). While these are important factors in determining the annotation cost, none of them alone can fully substitute for the actual annotation cost. For example, a short sentence with a lot of embedded clauses may be more costly to annotate than a longer sentence with simpler grammatical structure. Similarly, a short sentence with multiple verbs and discontinuous arguments may take more time to annotate with semantic roles than a longer sentence with a single verb and simple subject-verb-object structure (Carreras and M´ar"
W09-1903,ringger-etal-2008-assessing,0,0.693434,"annotator and annotation task characteristics are important for developing an accurate estimator, and argue that both correlation coefficient and root mean square error should be used for evaluating annotation cost estimators. 1 Introduction Active Learning is the process of selectively querying the user to annotate examples with the goal of minimizing the total annotation cost. Annotation cost has been traditionally measured in terms of the number of examples annotated, but it has been widely acknowledged that different examples may require different annotation effort (Settles et al., 2008; Ringger et al., 2008). Ideally, we would use actual human annotation cost for evaluating selective sampling strategies, but this will require conducting several user studies, one per strategy on the same dataset. Alternatively, we may be able to simulate the real user by an annotation cost estimator that can then be used to evaluate several selective sampling strategies without having to run a new user study each time. An annotation cost estimator models the characteristics that can 18 differentiate the examples in terms of their annotation time. The characteristics that strongly correlate with the annotation time"
W09-1903,D07-1051,0,0.224082,"relate with the annotation time can be used as a criterion in selective sampling strategies to minimize the total annotation cost. In some domains, the annotation cost of an example is known or can be calculated exactly before querying the user. For example, in biological experiments it might be calculable from the cost of the equipment and the material used (King et al., 2004). In NLP, sometimes a simplifying assumption is made that the annotation cost for an example can be measured in terms of its length (e.g. seconds of voicemail annotated (Kapoor et al., 2007); number of tokens annotated (Tomanek et al., 2007)). Another assumption is that the number of user annotation actions can be used as a proxy for annotation cost of an example (e.g. number of brackets added for parsing a sentence (Hwa, 2000); number of clicks for correcting named entities (Kristjannson et al., 2004)). While these are important factors in determining the annotation cost, none of them alone can fully substitute for the actual annotation cost. For example, a short sentence with a lot of embedded clauses may be more costly to annotate than a longer sentence with simpler grammatical structure. Similarly, a short sentence with multi"
W09-1903,H05-1044,0,0.045174,"Missing"
W09-1903,N07-1033,0,0.0117728,"used.We then present our experimental setup followed by a discussion of our results. 3.1 Annotation Methodology and Data Analysis In this work, we estimate the annotation cost for a movie review classification task. The data we used were collected as part of a graduate course. Twenty annotators (students and instructors) were grouped into five groups of four each. The groups were created such that each group had similar variance in annotator characteristics such as department, educational experience, programming experience, etc. We used the first 200 movie reviews from the dataset provided by Zaidan et al. (2007), with an equal distribution of positive and negative examples. Each group annotated 25 movie reviews randomly selected from the 200 reviews and all annotators in each group annotated all 25 reviews. In addition to voting positive or negative for a review, annotators also annotated rationales (Zaidan et al., 2007), spans of text in the review that support their vote. Rationales can be used to guide the model by identifying the most discriminant features. In related work (Arora and Nyberg, 2009), we ascertain that with rationales the same performance can be achieved with less annotated data. Th"
W11-0502,J97-1003,0,0.421023,"hlighted are interesting terms which are either part of the label of the cluster or show a low IDF (Inverse Document Frequency) amongst the tiles generated from the co-cited papers. These words are presented as hyper-links to the search interface and can be further used as search queries for finding articles on related topics. 3.1 System Description SciSumm has four primary modules that are central to the functionality of the system, as displayed in Figure 1. First, the Text Tiling module takes care of obtaining tiles of text relevant to the citation context. It uses the Texttiling algorithm (Hearst, 1997), to segment the co-cited papers into text tiles based on topic shifts identified using a term overlap measure computed between fixed-length blocks of text. Next, the clustering module is used to generate labelled clusters using the text tiles extracted from the cocited papers. The labels provide a conveniently comprehensible and yet terse description of each cluster. We have used a Frequent Term Based Clustering algorithm (Beil et al., 2002) for clustering. The clusters are ordered according to relevance with respect to the generated query. This is accomplished by the 11 Ranking Module. Final"
W11-0502,W00-0403,0,0.111343,"Missing"
W11-0502,radev-etal-2004-mead,0,0.344015,"ion in the source article, we generate a query that allows us to generate a summary in a query-oriented fashion. The extracted por8 Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 8–15, c Portland, Oregon, June 23, 2011. 2011 Association for Computational Linguistics tions of the co-cited articles are then assembled into clusters that represent the main themes of the articles that relate to the context in which they were cited. Our evaluation demonstrates that SciSumm achieves higher quality summaries than the MEAD summarization system (Radev, 2004). The rest of the paper is organized as follows. We present an overview of relevant literature in Section 2. The end-to-end summarization pipeline has been described in Section 3 . Section 4 presents an evaluation of summaries generated from the system. We end the paper with conclusions and some interesting further research directions in Section 5. 2 Literature Review We begin our literature review by thinking about some common use cases for multi-document summarization of scientific articles. First consider that as a researcher reads a scientific article, she/he encounters numerous citations,"
W11-0502,J02-4002,0,0.147869,"Missing"
W11-0502,N09-1066,0,0.101706,"of what we are working to achieve. Rather than summarizing multiple papers cited in the same source article, they summarize different viewpoints expressed towards the same article from different citing articles. Some of the insights they use in their work also apply to our problem. They used a clustering approach over different citations for the same target article for discovery of different ways of thinking about that article. Citation text has been already shown to contain important concepts about the article that might be absent from other important sections of an article e.g. an Abstract (Mohammad et al., 2009) . Template based generation of summaries possessing similar hierarchical topic structure as the Related Work section in an article has also been proposed (Hoang et al., 2010). In our work, we consider a flat topic structure in the form of topic clusters. More specifically, we discover the comparable attributes of the co-cited articles using Frequent Term Based Clustering (Beil et al., 2002). The clusters generated in this process contain a set of topically related text fragments called tiles, which are extracted from the set of co-cited articles. Each cluster is indexed with a label, which is"
W11-0502,C10-2049,0,\N,Missing
W11-0502,councill-etal-2008-parscit,0,\N,Missing
W11-0502,P10-1057,0,\N,Missing
W11-0502,C08-1087,0,\N,Missing
W11-0710,P07-1104,0,0.0212855,"l with high explanatory value, we use Linear Regression, with L1 regularization (Tibshirani, 1996). This minimizes the sum of P squared errors, but in addition adds a penalty term λ m j=1 |βj |, the sum of absolute values of the coefficients. λ is a constant and can be found by optimizing over the development data. As a result, this method delivers sparse models. We use Orthant-Wise Limited-memory Quasi-Newton Optimizer (Andrew and Gao, 2007) as our optimization method. This method has proven to establish competitive performances with other optimization methods, while producing sparse models (Gao et al., 2007). Because our observations suggest that language change decreases as members have been active longer, we also experimented with applying a log transformation on the number of weeks. 5.3 Features For all features, we only use information that has been available for that particular week. We explore different types of features related to the qualitative differences in language we discussed in Section 3: textual, behavioral, subforum and meta-features. 5.3.1 Textual features We explore the following textual features: • Unigrams and bigrams. • Part of Speech (POS) bigrams. Text was tagged using the"
W11-0710,W06-3403,0,0.46123,"e groups. Over time, conformity to these norms increased. Similarly, Cassell and Tversky (2005) looked at evolution of language patterns in an online community. In this work, the participants were students from around the world participating in the Junior Summit forum ’98. Cassell and Tversky found that participants converged on style, topics, goals and strategies. Analyses were computed using word frequencies of common classes (such as self references) and Table 1: Statistics dataset. Posts Threads Users (at least one post) Time-span 1,562,590 68,226 31,307 Oct 2002 - Jan 2011 manual coding. Huffaker et al. (2006) examined a subset of the same data. When comparing consecutive weeks over a 6 week time period, they found that the language diverged. They hypothesized that this was caused by external events leading to the introduction of new words. Our research differs from the research by Cassell and Tversky (2005), Huffaker et al. (2006) and Postmes et al. (2000) in several respects. For example, in all of this work, participants joined the community simultaneously at the inception of the community. In contrast, our community of inquiry has evolved over time, with members joining intermittently throughou"
W11-0710,W10-1908,0,0.0151215,"rast, our community of inquiry has evolved over time, with members joining intermittently throughout the history of the community. Additionally, our analysis spans much more time, specifically 2 years of data rather than 3 or 4 months. Thus, this research addresses a different question from the way community norms are first established at the inception of a community. In contrast, what we investigate is how new users are socialized into an existing community in which norms have already been established prior to their arrival. We are not the first researchers to study our community of inquiry (Jha and Elhadad, 2010). However, prior work on data from this forum was focused on predicting the cancer stage of a patient rather than issues related to language change that we investigate. 3 Data description We analyze one of the largest breast cancer forums on the web (http://community.breastcancer.org/). All posts and user profiles of the forum were crawled in January 2011. The forum serves as a platform for many different kinds of interactions, and serving the needs of a variety of types of users. For example, a large proportion of users only join to ask some medical questions, and therefore do not stay active"
W11-0710,N03-1033,0,0.0146165,"ions suggest that language change decreases as members have been active longer, we also experimented with applying a log transformation on the number of weeks. 5.3 Features For all features, we only use information that has been available for that particular week. We explore different types of features related to the qualitative differences in language we discussed in Section 3: textual, behavioral, subforum and meta-features. 5.3.1 Textual features We explore the following textual features: • Unigrams and bigrams. • Part of Speech (POS) bigrams. Text was tagged using the Stanford POS tagger (Toutanova et al., 2003). • LIWC (Pennebaker et al., 2001), a word counting program that captures word classes and stylistic features. • Usernames. Because some of the usernames are common words, we only consider usernames of users active in the same thread. • Proper names. We obtained a list containing common female names. We ranked them according to frequency in our dataset, and manually deleted common words in our dataset, such as happy, hope, tuesday and may, from our list. • Slang words. We manually compile a list of common abbreviations and their whole words counterpart. We then count the number of abbreviation"
W11-1515,cieri-etal-2004-fisher,0,0.0966874,"construed along a variety of dimensions, including age, gender, socioeconomic status and political affiliation. A person is a member of a multiplicity of communities, and thus the person’s identity and language are influenced by many factors. In this paper we focus on the relationship between age and language use. Recently, machine learning Our first contribution to this literature is an investigation of age prediction using a multi-corpus approach. We present results and analysis across three very different corpora: a blog corpus (Schler et al., 2006), a transcribed telephone speech corpus (Cieri et al., 2004) and posts from an online forum on breast cancer. By using the domain adaptation approach of Daum´e III (2007), we train a model on all these corpora together and separate the global features from corpus-specific features that are associated with age. A second contribution is the investigation of age prediction with age modeled as a continuous variable rather than as a categorical variable. Most prior research on age prediction has framed this as a two-class or three-class classification problem (e.g., Schler et al., 2006 and Garera and Yarowsky, 2009). In our work, modeling age as a continuou"
W11-1515,P07-1033,0,0.0151607,"Missing"
W11-1515,P07-1104,0,0.0431598,"model: yˆ = β0 + x&gt; β where β0 and β are the parameters to estimate. Usually, the parameters are learned by minimizing the sum of squared errors. In order to strive for a model with high explanatory value, we use a linear regression model with Lasso (also called L1 ) regularization (Tibshirani, 1996). This minimizes the sum of P squared errors, but in addition adds a penalty term λ m j=1 |βj |. λ is a constant and can be found by optimizing over the development data. As a result, this method delivers sparse models. We use OWLQN to optimize the regularized empirical risk (Andrew and Gao, 2007; Gao et al., 2007). We evaluate the models by reporting the correlation and mean absolute error (MAE). Besides experimenting with the joint model, we are also interested in the performance using only the discovered global features. This can be achieved by applying the weights for the global features directly as learned by the joint model, or retraining the model on the individual datasets using only the global features. In summary, we have the following models: 4.2 4.4 Joint model To discover which features are important across datasets and which are corpus-specific, we train a model on the data of all corpora"
W11-1515,P09-1080,0,0.474427,"., 2006), a transcribed telephone speech corpus (Cieri et al., 2004) and posts from an online forum on breast cancer. By using the domain adaptation approach of Daum´e III (2007), we train a model on all these corpora together and separate the global features from corpus-specific features that are associated with age. A second contribution is the investigation of age prediction with age modeled as a continuous variable rather than as a categorical variable. Most prior research on age prediction has framed this as a two-class or three-class classification problem (e.g., Schler et al., 2006 and Garera and Yarowsky, 2009). In our work, modeling age as a continuous variable is interesting not only as a more realistic representation of age, but also for practical benefits of joint modeling of age across corpora since the bound115 Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 115–123, c Portland, OR, USA, 24 June 2011. 2011 Association for Computational Linguistics aries for discretizing age into a categorical variable in prior work have been chosen heuristically and in a corpus-dependent way, making it hard to compare performance acro"
W11-1515,N03-1033,0,0.0106484,"rained on the three corpora individually. • JOINT: Model trained on all three corpora with features represented as in Daum´e III (2007). • JOINT-Global: Using the learned JOINT model but only keeping the global features. • JOINT-Global-Retrained: Using the discovered global features by the JOINT model, but retrained on each specific dataset. Features 4.4.1 Textual features We explore the following textual features; all features are frequency counts normalized by the length (number of tokens) of the document. • Unigrams. • POS unigrams and bigrams. Text is tagged using the Stanford POS tagger (Toutanova et al., 2003). • LIWC (Pennebaker et al., 2001). This is a word counting program that captures word classes such as inclusion words (LIWC-incl: “with,” “and,” “include,” etc.), causation words (LIWCcause: “because,” “hence,” etc.), and stylistic characteristics such as percentage of words longer than 6 letters (LIWC-Sixltr). of training instances, the noisy text, and the fact that the ages lie very close to each other. Predicted age Figure 2: Scatterplot of true and predicted age. 90 80 70 60 50 40 30 20 10 0 -10 -20 Overall, the joint model using all features performed best (condition 10). In Figure 2 a p"
W11-2025,W08-0101,0,\N,Missing
W11-2025,P05-1012,0,\N,Missing
W11-2025,W09-3932,0,\N,Missing
W11-2606,N09-2010,1,0.787159,"that are adept at capturing interesting language variation without overfitting to content based variation, with the hope of leading to more generalizable models. POS n-grams, which have frequently been utilized in genre analysis models (Argamon et al., 2003), are a strategic balance between informativity and simplicity. They are able to estimate syntactic structure and style without modeling it directly. In an attempt to capture syntactic structure more faithfully, there has been experimentation within the area of sentiment analysis on using syntactic dependency features (Joshi & Rosé, 2009; Arora, Joshi, & Rosé, 2009). However, results have been mixed. In practice, the added richness of the features comes at a tremendous cost in terms of dramatic increases in feature space size. What has been more successful in practice is templatizing the dependency features in order to capture the same amount of structure without creating features that are so specific. Syntactic dependency based features are able to capture more structure than POS bigrams, however, they are still limited to representing relationships between pairs of words within a text. Thus, they still leave much to be desired in terms of representati"
W11-2606,W10-0216,0,0.0126788,"f dramatic increases in feature space size. What has been more successful in practice is templatizing the dependency features in order to capture the same amount of structure without creating features that are so specific. Syntactic dependency based features are able to capture more structure than POS bigrams, however, they are still limited to representing relationships between pairs of words within a text. Thus, they still leave much to be desired in terms of representation power. Experimentation with graph mining from dependency parses has also been used for generating rich feature spaces (Arora et al., 2010). However, results with these features has also been disappointing. In practice, the rich features with real predictive power end up being difficult to find amidst myriads of useless features that simply add noise to the model. One direction that has proven successful at exceeding the representational power and performance of POS bigrams with only a very modest increase in feature space size has been a genetic programming based approach to learning to build a strategic set of rich features so that the benefits of rich features can be obtained without the expense in terms of feature space expan"
W11-2606,cieri-etal-2004-fisher,0,0.0123966,"only achieves high performance, but generalizes across domains better than alternative techniques. Building on an earlier template based approach for modeling sarcasm (Tsur et al., 2010), we investigate the use of what we have termed “stretchy” features to model stylistic variation related to sociolects, which can be thought of as a form of dialect. Specifically, we focus on the problem of gender based classification. Gender classification and age classification have both received increased attention in the social media analysis community in recent years (Goswami et al., 2009; Barbieri, 2008; Cieri et al., 2004), most likely because large data sets annotated with these variables have recently become available. Machine learning technology provides a lens with which to explore linguistic variation that complements earlier statistical techniques used by variationist sociolinguists in their work mapping out the space of dialect variation and its accompanying social interpretation (Labov, 2010a; Labov, 2010b; Eckert & Rickford, 2001). These complementary approaches share a common foundation in numerical methods, however while descriptive statistics and inferential statistics mainly serve the purpose of de"
W11-2606,P07-1033,0,0.0393981,"Missing"
W11-2606,N09-1068,0,0.0289583,"ion techniques, which have also had a significant impact on success (Mukherjee & Liu, 2010; Zhang, Dang, & Chen, 2009). Of these features, the only ones that capture stylistic elements that extend beyond individual words at a time are the POS ngram features. The inclusion of these features has been motivated by their hypothesized generality, although in practice, the generality of gender prediction models has not been formally evaluated in the gender prediction literature. 2.2 Domain Adaptation in Social Media Recent work in the area of domain adaptation (Arnold et al., 2008; Daumé III, 2007; Finkel & Manning, 2009) raises awareness of the difficulties with generality of trained models and offers insight into the reasons for the difficulty with generalization. We consider these issues specifically in connection with the problem of modeling gender based variation. One problem, also noted by variationist sociolinguists, is that similar language variation is associated with different variables (McEnery, 2006). For example, linguistic features associated with older age are also more associated with male communication style than female communication style for people of the same age (Argamon et al., 2007). Ano"
W11-2606,P09-2079,0,0.00484139,"truct feature spaces that are adept at capturing interesting language variation without overfitting to content based variation, with the hope of leading to more generalizable models. POS n-grams, which have frequently been utilized in genre analysis models (Argamon et al., 2003), are a strategic balance between informativity and simplicity. They are able to estimate syntactic structure and style without modeling it directly. In an attempt to capture syntactic structure more faithfully, there has been experimentation within the area of sentiment analysis on using syntactic dependency features (Joshi & Rosé, 2009; Arora, Joshi, & Rosé, 2009). However, results have been mixed. In practice, the added richness of the features comes at a tremendous cost in terms of dramatic increases in feature space size. What has been more successful in practice is templatizing the dependency features in order to capture the same amount of structure without creating features that are so specific. Syntactic dependency based features are able to capture more structure than POS bigrams, however, they are still limited to representing relationships between pairs of words within a text. Thus, they still leave much to be desi"
W11-2606,N03-1033,0,0.0063526,"patterns beat the state-of-the-art at the predictive task of gender classification, it would be essential to implement one of these approaches as our baseline. However, our purpose here is instead to address two more specific research questions instead, and for that we argue that we can learn something from comparing with three more simplistic baselines, which differ only in terms of feature extraction. The three baseline models we tested included a unigram model, a unigram+bigram model, and a Part-of-Speech bigram model. For part-of-speech tagging we used the Stanford part-of-speech tagger4 (Toutanova et al., 2003). Our three baseline feature spaces have been very commonly used in the language technologies community for a variety of social media analysis tasks, the most common of which in recent years has been sentiment analysis. While these feature spaces are simple, they have remained surprisingly strong baseline approaches when testing is done 3 4 http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm http://nlp.stanford.edu/software/tagger.shtml within domain, and with large enough training sets. However, these relatively weak, low level features are notorious for low performance when datasets are too small a"
W11-2606,D10-1021,0,\N,Missing
W11-2606,J04-3002,0,\N,Missing
W12-1607,P11-1118,0,0.0957857,"s is designed with the goal of separating out sub-conversations which are independently coherent. There is a common ground emerging in the thread detection literature on best practices for automated prediction. Early work viewed the problem as a time series analysis task (Bingham et al., 2003). Treating thread detection as a clustering problem, with lines representing instances, was given great attention in Shen et al. (2006). Subsequent researchers have treated the thread detection task as based in discourse coherence, and have pursued topic modelling (Adams, 2008) or entity reference grids (Elsner and Charniak, 2011) to define that concept of coherence. Other work integrates local discourse structure with the topic-based threads of discourse. Ai et al. (2007) utilizes information state, a dialogue management component which loosely parallels thread structure, to improve dialogue act tagging. In the context of Twitter conversations, Ritter et al. (2010) suggests using dialogue act tags as a middle layer towards conversation reconstruction. Low-level structure between utterances has also been used as a foundation for modelling larger-level sociological phenomena between speakers in a dialogue, for instance,"
W12-1607,P04-1085,0,0.0307766,"he number of variations remains in the hundreds (Jurafsky et al., 1998). Situated work has jointly modelled speech act and domain-specific topics (Laws et al., 2012). Additional structure inspired by linguistics, such as adjacency pairs (Schegloff, 2007) or dialogue games (Carlson, 1983), has been used to build discourse relations between turns. This additional structure has been shown to improve performance of automated analysis (Poesio and Mikheev, 1998). Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al., 2004), addressee detection (op den Akker and Traum, 61 2009), and real-world applications, such as customer service conversations (Kim et al., 2010). Higher-order structure has also been explored in dialogue, from complex graph-like relations (Wolf and Gibson, 2005) to simpler segmentation-based approaches (Malioutov and Barzilay, 2006). Utterance level-tagging can take into account nearby structure, e.g. forward-looking and backward-looking functions in DAMSL (Core and Allen, 1997), while dialogue management systems in intelligent agents often have a plan unfolding over a whole dialogue (Ferguson"
W12-1607,D10-1084,0,0.0174352,"(Laws et al., 2012). Additional structure inspired by linguistics, such as adjacency pairs (Schegloff, 2007) or dialogue games (Carlson, 1983), has been used to build discourse relations between turns. This additional structure has been shown to improve performance of automated analysis (Poesio and Mikheev, 1998). Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al., 2004), addressee detection (op den Akker and Traum, 61 2009), and real-world applications, such as customer service conversations (Kim et al., 2010). Higher-order structure has also been explored in dialogue, from complex graph-like relations (Wolf and Gibson, 2005) to simpler segmentation-based approaches (Malioutov and Barzilay, 2006). Utterance level-tagging can take into account nearby structure, e.g. forward-looking and backward-looking functions in DAMSL (Core and Allen, 1997), while dialogue management systems in intelligent agents often have a plan unfolding over a whole dialogue (Ferguson and Allen, 1998). In recent years, threading and maintaining of multiple “floors” has grown in popularity (Elsner and Charniak, 2010), especial"
W12-1607,P06-1004,0,0.0370592,"elations between turns. This additional structure has been shown to improve performance of automated analysis (Poesio and Mikheev, 1998). Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al., 2004), addressee detection (op den Akker and Traum, 61 2009), and real-world applications, such as customer service conversations (Kim et al., 2010). Higher-order structure has also been explored in dialogue, from complex graph-like relations (Wolf and Gibson, 2005) to simpler segmentation-based approaches (Malioutov and Barzilay, 2006). Utterance level-tagging can take into account nearby structure, e.g. forward-looking and backward-looking functions in DAMSL (Core and Allen, 1997), while dialogue management systems in intelligent agents often have a plan unfolding over a whole dialogue (Ferguson and Allen, 1998). In recent years, threading and maintaining of multiple “floors” has grown in popularity (Elsner and Charniak, 2010), especially in text-based media. This level of analysis is designed with the goal of separating out sub-conversations which are independently coherent. There is a common ground emerging in the thread"
W12-1607,N10-2007,1,0.837882,"Missing"
W12-1607,P11-1102,1,0.893246,"Missing"
W12-1607,N10-1020,0,0.0542002,"ith lines representing instances, was given great attention in Shen et al. (2006). Subsequent researchers have treated the thread detection task as based in discourse coherence, and have pursued topic modelling (Adams, 2008) or entity reference grids (Elsner and Charniak, 2011) to define that concept of coherence. Other work integrates local discourse structure with the topic-based threads of discourse. Ai et al. (2007) utilizes information state, a dialogue management component which loosely parallels thread structure, to improve dialogue act tagging. In the context of Twitter conversations, Ritter et al. (2010) suggests using dialogue act tags as a middle layer towards conversation reconstruction. Low-level structure between utterances has also been used as a foundation for modelling larger-level sociological phenomena between speakers in a dialogue, for instance, identifying leadership (Strzalkowski et al., 2011) and rapport between providers and patients in support groups (Ogura et al., 2008). These works have all pointed to the utility of incorporating sentence-level annotations, low-level interaction structure, and overarching themes into a unified system. To our knowledge, however, this work is"
W12-1607,rizzolo-roth-2010-learning,0,0.0152433,"classifier in either pass is a set of probabilities corresponding to possible cluster assignments, including that of creating a new cluster. In the second pass, the input is a set of sentences (a sequence) rather than a single sentence, and output assignments are to threads rather than sequences. Ros´e, 2011). Constraints on the structure of annotations are easily defined using Integer Linear Programming. Recent work has used boolean logic (Chang et al., 2008) to allow intuitive rules about a domain to be enforced at classification time. ILP inference was performed using Learning-Based Java (Rizzolo and Roth, 2010). First, we define the classification task. Optimization is performed given the set of probabilities N (s) as the distribution output of the Neg classifier given sentence s as input, and the set of probabilities C(s) = pnc (s) ∪ psc (s, c), ∀c ∈ C. Instance classification requires maximizing the objective function: arg max n + c 3. Functionally, therefore, f moves may not initiate a sequence). (cs = c0 ) → (ns 6= f) 4. Speakers do not respond to their own requests for information (the speakers of K2 and K1 moves in the same sequence must be different). ((cs = c) ∧ (ns = K1)) → (∀i ∈ Sc , ((nc,"
W12-1607,W04-2319,0,0.0417013,"r matches human reliability on all tasks. We conclude with discussion of the utility of this conversation structuring algorithm for new analyses of conversation. 2 Related Work Research on multi-party conversation structure is widely varied, due to the multifunctional nature of language. These structures have been used in diverse fields such as computer-supported collaborative work (O’Neill and Martin, 2003), dialogue systems (Bohus and Horvitz, 2011), and research on meetings (Renals et al., 2012). Much work in annotation has been inspired by speech act theory and dialogue acts (Traum, 1994; Shriberg et al., 2004), which operate primarily on the granularity of individual utterances. A challenge of tagging is the issue of specificity of tags, as previous work has shown that most utterances have multiple functions (Bunt, 2011). General tagsets have attempted to capture multi-functionality through independent dimensions which produce potentially millions of possible annotations, though in practice the number of variations remains in the hundreds (Jurafsky et al., 1998). Situated work has jointly modelled speech act and domain-specific topics (Laws et al., 2012). Additional structure inspired by linguistic"
W12-1607,N03-1033,0,0.0263302,"Missing"
W12-1607,J05-2005,0,0.0390206,"ialogue games (Carlson, 1983), has been used to build discourse relations between turns. This additional structure has been shown to improve performance of automated analysis (Poesio and Mikheev, 1998). Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al., 2004), addressee detection (op den Akker and Traum, 61 2009), and real-world applications, such as customer service conversations (Kim et al., 2010). Higher-order structure has also been explored in dialogue, from complex graph-like relations (Wolf and Gibson, 2005) to simpler segmentation-based approaches (Malioutov and Barzilay, 2006). Utterance level-tagging can take into account nearby structure, e.g. forward-looking and backward-looking functions in DAMSL (Core and Allen, 1997), while dialogue management systems in intelligent agents often have a plan unfolding over a whole dialogue (Ferguson and Allen, 1998). In recent years, threading and maintaining of multiple “floors” has grown in popularity (Elsner and Charniak, 2010), especially in text-based media. This level of analysis is designed with the goal of separating out sub-conversations which are"
W12-1607,W11-2013,0,\N,Missing
W12-1607,J10-3004,0,\N,Missing
W14-2301,N10-1039,0,0.0160258,"taphor processing and metaphor annotation. 2.1 Metaphor Annotation Computational Work on Metaphor Much of of the computational work on metaphor can be classified into two tasks: automatic identification and interpretation of metaphors. Metaphor identification has been done using different approaches: violation of selectional preferences (Fass, 1991), linguistic cues (Goatly, 1997), source and target domain words (Stefanowitsch and Gries, 2006), clustering (Birke and Sarkar, 2006; Shutova et al., 2010), and lexical relations in WordNet (Krishnakumaran and Zhu, 2007). Gedigian et al. (2006) and Li and Sporleder (2010) distinguished the literal and nonliteral use of a target expression in text. In addition, Mason (2004) performed source-target domain mappings. Metaphor interpretation is another large part of the computational work on metaphor. Starting with Martin (1990), a number of researchers including Narayanan (1999), Barnden and Lee (2002), Agerri et al. (2007), and Shutova (2010) have worked on the task. Metaphor identification and interpretation was performed simultaneously in (Shutova, 2013; 2 fer significantly in purpose, demographics and the participation trajectory of members. Therefore, we expe"
W14-2301,J04-1002,0,0.0337113,"omputational work on metaphor can be classified into two tasks: automatic identification and interpretation of metaphors. Metaphor identification has been done using different approaches: violation of selectional preferences (Fass, 1991), linguistic cues (Goatly, 1997), source and target domain words (Stefanowitsch and Gries, 2006), clustering (Birke and Sarkar, 2006; Shutova et al., 2010), and lexical relations in WordNet (Krishnakumaran and Zhu, 2007). Gedigian et al. (2006) and Li and Sporleder (2010) distinguished the literal and nonliteral use of a target expression in text. In addition, Mason (2004) performed source-target domain mappings. Metaphor interpretation is another large part of the computational work on metaphor. Starting with Martin (1990), a number of researchers including Narayanan (1999), Barnden and Lee (2002), Agerri et al. (2007), and Shutova (2010) have worked on the task. Metaphor identification and interpretation was performed simultaneously in (Shutova, 2013; 2 fer significantly in purpose, demographics and the participation trajectory of members. Therefore, we expect that people will use language differently in the three sets, especially related to metaphorical expr"
W14-2301,E06-1042,0,0.0119891,"wing the defIn this section, we introduce the two main bodies of relevant prior work on metaphor in language technologies: computational metaphor processing and metaphor annotation. 2.1 Metaphor Annotation Computational Work on Metaphor Much of of the computational work on metaphor can be classified into two tasks: automatic identification and interpretation of metaphors. Metaphor identification has been done using different approaches: violation of selectional preferences (Fass, 1991), linguistic cues (Goatly, 1997), source and target domain words (Stefanowitsch and Gries, 2006), clustering (Birke and Sarkar, 2006; Shutova et al., 2010), and lexical relations in WordNet (Krishnakumaran and Zhu, 2007). Gedigian et al. (2006) and Li and Sporleder (2010) distinguished the literal and nonliteral use of a target expression in text. In addition, Mason (2004) performed source-target domain mappings. Metaphor interpretation is another large part of the computational work on metaphor. Starting with Martin (1990), a number of researchers including Narayanan (1999), Barnden and Lee (2002), Agerri et al. (2007), and Shutova (2010) have worked on the task. Metaphor identification and interpretation was performed si"
W14-2301,shutova-teufel-2010-metaphor,0,0.0954773,"stinct subculture prior to joining, whereas Breastcancer and MOOC members have less shared identity before entering the forum. This forum is purely social. There is no clear endpoint for participation; members leave the forum whenever they are not interested in it any more. Users may stay for a week or two, or for years. inition of basic meaning introduced in the paper tends to result in a large proportion of words being annotated as metaphor. Many of the annotated words would not be considered to be metaphors by a layperson due to their long and widespread usage. Later works by Steen (2010), Shutova and Teufel (2010), and Shutova et al. (2013a) expanded upon MIP. Steen (2010) discussed the strengths and weaknesses of MIP, and introduced the Metaphor Identification Procedure VU University Amsterdam (MIPVU). Shutova and Teufel (2010) and and Shutova et al. (2013a) added a procedure for identifying underlying conceptual mappings between source and target domains. So far, these presented schemes do not distinguish between degrees of metaphoricity, and were not specifically designed for considering motivations behind metaphor use. Unlike the annotation schemes described above, Klebanov and Flor (2013) built a"
W14-2301,J91-1003,0,0.507222,"his scheme can be used for different applications. However, defining the basic meaning of a word is nontrivial, and following the defIn this section, we introduce the two main bodies of relevant prior work on metaphor in language technologies: computational metaphor processing and metaphor annotation. 2.1 Metaphor Annotation Computational Work on Metaphor Much of of the computational work on metaphor can be classified into two tasks: automatic identification and interpretation of metaphors. Metaphor identification has been done using different approaches: violation of selectional preferences (Fass, 1991), linguistic cues (Goatly, 1997), source and target domain words (Stefanowitsch and Gries, 2006), clustering (Birke and Sarkar, 2006; Shutova et al., 2010), and lexical relations in WordNet (Krishnakumaran and Zhu, 2007). Gedigian et al. (2006) and Li and Sporleder (2010) distinguished the literal and nonliteral use of a target expression in text. In addition, Mason (2004) performed source-target domain mappings. Metaphor interpretation is another large part of the computational work on metaphor. Starting with Martin (1990), a number of researchers including Narayanan (1999), Barnden and Lee ("
W14-2301,C10-1113,0,0.0882632,"ion, we introduce the two main bodies of relevant prior work on metaphor in language technologies: computational metaphor processing and metaphor annotation. 2.1 Metaphor Annotation Computational Work on Metaphor Much of of the computational work on metaphor can be classified into two tasks: automatic identification and interpretation of metaphors. Metaphor identification has been done using different approaches: violation of selectional preferences (Fass, 1991), linguistic cues (Goatly, 1997), source and target domain words (Stefanowitsch and Gries, 2006), clustering (Birke and Sarkar, 2006; Shutova et al., 2010), and lexical relations in WordNet (Krishnakumaran and Zhu, 2007). Gedigian et al. (2006) and Li and Sporleder (2010) distinguished the literal and nonliteral use of a target expression in text. In addition, Mason (2004) performed source-target domain mappings. Metaphor interpretation is another large part of the computational work on metaphor. Starting with Martin (1990), a number of researchers including Narayanan (1999), Barnden and Lee (2002), Agerri et al. (2007), and Shutova (2010) have worked on the task. Metaphor identification and interpretation was performed simultaneously in (Shutov"
W14-2301,W06-3506,0,0.0251494,"hnologies: computational metaphor processing and metaphor annotation. 2.1 Metaphor Annotation Computational Work on Metaphor Much of of the computational work on metaphor can be classified into two tasks: automatic identification and interpretation of metaphors. Metaphor identification has been done using different approaches: violation of selectional preferences (Fass, 1991), linguistic cues (Goatly, 1997), source and target domain words (Stefanowitsch and Gries, 2006), clustering (Birke and Sarkar, 2006; Shutova et al., 2010), and lexical relations in WordNet (Krishnakumaran and Zhu, 2007). Gedigian et al. (2006) and Li and Sporleder (2010) distinguished the literal and nonliteral use of a target expression in text. In addition, Mason (2004) performed source-target domain mappings. Metaphor interpretation is another large part of the computational work on metaphor. Starting with Martin (1990), a number of researchers including Narayanan (1999), Barnden and Lee (2002), Agerri et al. (2007), and Shutova (2010) have worked on the task. Metaphor identification and interpretation was performed simultaneously in (Shutova, 2013; 2 fer significantly in purpose, demographics and the participation trajectory of"
W14-2301,W13-0902,0,0.243691,"on 2 relates our work to prior work on annotation and a corpus study. Section 3 describes the data used for annotation. Section 4 illustrates the functions metaphor serves in discourse through a qualitative analysis of our data. Section 5 explains our annotation scheme. Section 6 presents our annotation and MTurk experiments. Section 7 discusses the results. Section 8 concludes the paper. Shutova et al., 2013b). As we have seen so far, much of the computation work has focused on detecting and uncovering the intended meaning behind metaphorical expressions. On the other hand, Klebanov and Flor (2013) paid attention to motivations behind metaphor use, specifically metaphors used for argumentation in essays. They showed a moderate-to-strong correlation between percentage of metaphorically used words in an essay and the writing quality score. We will introduce their annotation protocol in Section 2.2. However, to the best of our knowledge, not much computational work has been done on understanding the motivation behind the use of metaphor besides that of Klebanov and Flor (2013). Our work hopefully lays additional foundation for the needed computational work. 2 2.2 Relation to Prior Work One"
W14-2301,W07-0103,0,0.0213244,"ork on metaphor in language technologies: computational metaphor processing and metaphor annotation. 2.1 Metaphor Annotation Computational Work on Metaphor Much of of the computational work on metaphor can be classified into two tasks: automatic identification and interpretation of metaphors. Metaphor identification has been done using different approaches: violation of selectional preferences (Fass, 1991), linguistic cues (Goatly, 1997), source and target domain words (Stefanowitsch and Gries, 2006), clustering (Birke and Sarkar, 2006; Shutova et al., 2010), and lexical relations in WordNet (Krishnakumaran and Zhu, 2007). Gedigian et al. (2006) and Li and Sporleder (2010) distinguished the literal and nonliteral use of a target expression in text. In addition, Mason (2004) performed source-target domain mappings. Metaphor interpretation is another large part of the computational work on metaphor. Starting with Martin (1990), a number of researchers including Narayanan (1999), Barnden and Lee (2002), Agerri et al. (2007), and Shutova (2010) have worked on the task. Metaphor identification and interpretation was performed simultaneously in (Shutova, 2013; 2 fer significantly in purpose, demographics and the par"
W14-2301,S13-1040,0,0.0387651,"2010), and lexical relations in WordNet (Krishnakumaran and Zhu, 2007). Gedigian et al. (2006) and Li and Sporleder (2010) distinguished the literal and nonliteral use of a target expression in text. In addition, Mason (2004) performed source-target domain mappings. Metaphor interpretation is another large part of the computational work on metaphor. Starting with Martin (1990), a number of researchers including Narayanan (1999), Barnden and Lee (2002), Agerri et al. (2007), and Shutova (2010) have worked on the task. Metaphor identification and interpretation was performed simultaneously in (Shutova, 2013; 2 fer significantly in purpose, demographics and the participation trajectory of members. Therefore, we expect that people will use language differently in the three sets, especially related to metaphorical expressions. MOOC: This forum is used primarily for taskbased reasons rather than socializing. People participate in the forum for a course, and leave when the course ends. As a result, the forum does not have continuity over time; participants do not spend long time with the same people. Breastcancer: People join this forum for both task-based and social reasons: to receive informational"
W14-2301,N10-1147,0,\N,Missing
W14-2301,J13-2003,0,\N,Missing
W14-4104,P08-2025,0,0.0201771,"t al., 2012) has focused on understanding the dynamics of the surrounding community activity, like the process through which answers and voters arrive over time. Based on understanding of such factors, a prediction can be made about the long term value for the community of a question being answered. Similarly, Agichtein and colleagues (Agichtein et al., 2009) presented a general prediction model of information seeker satisfaction in community question answering, and developed content, structure and community focused features for the question answering task. A collection of other related work (Liu and Agichtein, 2008) has developed personalized models of asker satisfaction to predict whether a particular question starter will be satisfied with the answers given 22 by others. This is solved by exploring content, structure and interaction features using standard prediction models. Work on automated question answering systems can also be seen as relevant since questions that can be answered automatically do not need a human response, and therefore might reduce the load on available human effort. Instead of predicting whether a problem is answered, strategies for predicting are explored when a question answeri"
W14-4104,W02-1033,0,0.0105395,"faction to predict whether a particular question starter will be satisfied with the answers given 22 by others. This is solved by exploring content, structure and interaction features using standard prediction models. Work on automated question answering systems can also be seen as relevant since questions that can be answered automatically do not need a human response, and therefore might reduce the load on available human effort. Instead of predicting whether a problem is answered, strategies for predicting are explored when a question answering system is likely to give an incorrect answer (Brill et al., 2002). To further understand how a question is answered, researchers (Yih et al., 2013) have studied the answer sentence selection problem for question answering and improves the model performance by using lexical semantic resources. That is, they construct semantic matches between question and answers. In terms of the extent to which the question is answered, Shah and colleagues (Shah and Pomerantz, 2010) evaluated answer quality by manually rating the quality of each answer. Then they extracted various features to train classifiers to select the best answer for that question. Liu et al. (Liu et a"
W14-4104,P13-1025,0,0.0190337,"of thread popularity. The Total UpVotes xT vt and Max UpVotes xM vt are used to represent the credit this thread has received and how others recognize the current discussion. Based on our analysis, people rarely give a downvote to others’ posts. The Question Votes xSvt indicates whether the starter formulates a problem that wins recognition from others. For Total Upvotes, we find that in resolved threads, it is 6.10 compared to 3.15 in unresolved thread. Thus, intuitively, thread popularity has the potential to give a useful prediction of thread resolveability. 4.4 Friendliness Friendliness (Danescu-Niculescu-Mizil et al., 2013; Burke and Kraut, 2008) concerns whether the current conversation is conducive for others to discuss ideas. This has not been considered in existing question answering work, and we thus discuss our operationalization of politeness here. We hypothesize that resolved threads posses more polite words, such as ’thank’. For example, a resolved thread might end with gratitude to thank others for providing help, and indeed we see this. Thus, we specify a set of observed indicators that may be useful in a latent variable model of politeness. (1) Start with Thanks: xStx , Expert Participation Who part"
W14-4104,P14-1092,0,0.0146572,"rious features to train classifiers to select the best answer for that question. Liu et al. (Liu et al., 2011) proposed to use a mutual reinforcement based propagation algorithm to predict question quality based. The model makes its prediction based on the connection between askers and topics, and how those connections predict differences in quality. The above question answering work is all about general discussion forums (Qu et al., 2009; Kabutoya et al., 2010), such as Yahoo! Answers2 . In our work, in addition to taking advantage of existing QA work, we also adopt a linguistic perspective (Jansen et al., 2014) and take semantic matching into account using a latent semantic approach. To the best of our knowledge, this is the first work on thread resolvability analysis in a MOOC context. 3 readings or lectures, students have the opportunity to initiate a thread in the course forum, in order to engage other students in the class as well as the teaching staff. For example, if a student were confused about the distinction between an argument and a parameter in Python, he/she would post the question to the variables subforum, marking it unresolved at the same time. In the ideal case, another participant"
W14-4104,P13-1171,0,0.0155598,"answers given 22 by others. This is solved by exploring content, structure and interaction features using standard prediction models. Work on automated question answering systems can also be seen as relevant since questions that can be answered automatically do not need a human response, and therefore might reduce the load on available human effort. Instead of predicting whether a problem is answered, strategies for predicting are explored when a question answering system is likely to give an incorrect answer (Brill et al., 2002). To further understand how a question is answered, researchers (Yih et al., 2013) have studied the answer sentence selection problem for question answering and improves the model performance by using lexical semantic resources. That is, they construct semantic matches between question and answers. In terms of the extent to which the question is answered, Shah and colleagues (Shah and Pomerantz, 2010) evaluated answer quality by manually rating the quality of each answer. Then they extracted various features to train classifiers to select the best answer for that question. Liu et al. (Liu et al., 2011) proposed to use a mutual reinforcement based propagation algorithm to pr"
W14-4107,W14-4110,0,0.0290047,"nces that are more conducive to learning can be offered. Beyond that, automated analyses enable adaptive technology to tailor the experience of users in real time (Rosé et al., 2014a). This paper describes a shared task designed to enlist the involvement of the language technologies community in this endeavor and to identify what value expertise within the field might bring. 1 12 research teams signed up for the shared task, including an international assortment of academic and industrial teams. Out of these 12 teams, only 4 submitted final models (Sinha et al., 2014; Sharkey & Sanders, 2014; Amnueypornsakul et al., 2014; Kloft et al., 2014 ). In the remainder of this paper we describe the shared task in greater detail and discuss plans for future related research. http://www.moocresearch.com/reports 39 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39–41, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2 C++ 4.7, Java 1.6, or Python 2.7. The script was required to be able to run within 24 hours on a 2400 MHz machine with 6 cores. Shared Task Participants in the shared task were given a complete SQL dump and clickstream"
W14-4107,W14-4111,0,0.0268353,"o learning can be offered. Beyond that, automated analyses enable adaptive technology to tailor the experience of users in real time (Rosé et al., 2014a). This paper describes a shared task designed to enlist the involvement of the language technologies community in this endeavor and to identify what value expertise within the field might bring. 1 12 research teams signed up for the shared task, including an international assortment of academic and industrial teams. Out of these 12 teams, only 4 submitted final models (Sinha et al., 2014; Sharkey & Sanders, 2014; Amnueypornsakul et al., 2014; Kloft et al., 2014 ). In the remainder of this paper we describe the shared task in greater detail and discuss plans for future related research. http://www.moocresearch.com/reports 39 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39–41, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2 C++ 4.7, Java 1.6, or Python 2.7. The script was required to be able to run within 24 hours on a 2400 MHz machine with 6 cores. Shared Task Participants in the shared task were given a complete SQL dump and clickstream dump from one Course"
W14-4107,W14-4108,0,0.0238221,"derstanding user needs better so that experiences that are more conducive to learning can be offered. Beyond that, automated analyses enable adaptive technology to tailor the experience of users in real time (Rosé et al., 2014a). This paper describes a shared task designed to enlist the involvement of the language technologies community in this endeavor and to identify what value expertise within the field might bring. 1 12 research teams signed up for the shared task, including an international assortment of academic and industrial teams. Out of these 12 teams, only 4 submitted final models (Sinha et al., 2014; Sharkey & Sanders, 2014; Amnueypornsakul et al., 2014; Kloft et al., 2014 ). In the remainder of this paper we describe the shared task in greater detail and discuss plans for future related research. http://www.moocresearch.com/reports 39 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39–41, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2 C++ 4.7, Java 1.6, or Python 2.7. The script was required to be able to run within 24 hours on a 2400 MHz machine with 6 cores. Shared Task Participants in the shar"
W14-4107,W14-4109,0,\N,Missing
W15-1401,O14-3001,0,0.0246168,"emantically compatible predicates are with particular arguments. For example, the verb eat prefers food as an object over chair. The idea of using selectional preferences for metaphor detection is that metaphorically used words tend to break selectional preferences. In the example of The clouds sailed across the sky, sailed is determined to be a metaphor since clouds as a subject violates its selectional preferences. Selectional preferences have been considered in a variety of studies about metaphor detection (Martin, 1996; Shutova and Teufel, 2010; Shutova et al., 2010; Shutova et al., 2013; Huang, 2014) The abstractness/concreteness approach associates metaphorical use with the degree of abstractness and concreteness within the components of a phrase. In an phrase of adjective and noun such as green idea and green frog, the former is considered metaphorical since an abstract word (idea) is modified by a concrete word (green), while the latter is considered literal since both words are concrete (Turney et al., 2011). Broadwell et al. (2013) use measures of imageability to detect metaphor, a similar concept to abstractness and concreteness. The lexical coherence approach uses the fact that met"
W15-1401,W14-2301,1,0.878697,"Missing"
W15-1401,W14-2302,0,0.0299864,"Missing"
W15-1401,N10-2007,0,0.0355775,"Missing"
W15-1401,W11-0710,1,0.736301,"Missing"
W15-1401,shutova-teufel-2010-metaphor,0,0.0301574,"ness, and lexical incoherence. Selectional preferences relate to how semantically compatible predicates are with particular arguments. For example, the verb eat prefers food as an object over chair. The idea of using selectional preferences for metaphor detection is that metaphorically used words tend to break selectional preferences. In the example of The clouds sailed across the sky, sailed is determined to be a metaphor since clouds as a subject violates its selectional preferences. Selectional preferences have been considered in a variety of studies about metaphor detection (Martin, 1996; Shutova and Teufel, 2010; Shutova et al., 2010; Shutova et al., 2013; Huang, 2014) The abstractness/concreteness approach associates metaphorical use with the degree of abstractness and concreteness within the components of a phrase. In an phrase of adjective and noun such as green idea and green frog, the former is considered metaphorical since an abstract word (idea) is modified by a concrete word (green), while the latter is considered literal since both words are concrete (Turney et al., 2011). Broadwell et al. (2013) use measures of imageability to detect metaphor, a similar concept to abstractness and concreten"
W15-1401,C10-1113,0,0.0667985,"Missing"
W15-1401,J13-2003,0,0.0241648,"rences relate to how semantically compatible predicates are with particular arguments. For example, the verb eat prefers food as an object over chair. The idea of using selectional preferences for metaphor detection is that metaphorically used words tend to break selectional preferences. In the example of The clouds sailed across the sky, sailed is determined to be a metaphor since clouds as a subject violates its selectional preferences. Selectional preferences have been considered in a variety of studies about metaphor detection (Martin, 1996; Shutova and Teufel, 2010; Shutova et al., 2010; Shutova et al., 2013; Huang, 2014) The abstractness/concreteness approach associates metaphorical use with the degree of abstractness and concreteness within the components of a phrase. In an phrase of adjective and noun such as green idea and green frog, the former is considered metaphorical since an abstract word (idea) is modified by a concrete word (green), while the latter is considered literal since both words are concrete (Turney et al., 2011). Broadwell et al. (2013) use measures of imageability to detect metaphor, a similar concept to abstractness and concreteness. The lexical coherence approach uses the"
W15-1401,E09-1086,0,0.0317667,"en frog, the former is considered metaphorical since an abstract word (idea) is modified by a concrete word (green), while the latter is considered literal since both words are concrete (Turney et al., 2011). Broadwell et al. (2013) use measures of imageability to detect metaphor, a similar concept to abstractness and concreteness. The lexical coherence approach uses the fact that metaphorically used words are semantically not coherent with context words. Broadwell et al. (2013) use topic chaining to categorize words as nonmetaphorical when they have a semantic relationship to the main topic. Sporleder and Li (2009) also use lexical chains and semantic cohesion graphs to detect metaphors. To the best of our knowledge, there has been no computational work on the effect of situational fac3 3 Data Meghan, I was diagnosed this pst 09/02/07. I was upset for a day when I realized after I had two mammograms and the ultrasound that I had cancer-I didn’t have a diagnosis, but I knew. After the ultrasound came the biopsy and then the diagnosis, I was fine. I did research. I made up my mind about what treatement I thought I wanted. I was good...I really was fine up to my visit with the surgeon last week. That made"
W15-1401,D11-1063,0,0.0176155,"erences. Selectional preferences have been considered in a variety of studies about metaphor detection (Martin, 1996; Shutova and Teufel, 2010; Shutova et al., 2010; Shutova et al., 2013; Huang, 2014) The abstractness/concreteness approach associates metaphorical use with the degree of abstractness and concreteness within the components of a phrase. In an phrase of adjective and noun such as green idea and green frog, the former is considered metaphorical since an abstract word (idea) is modified by a concrete word (green), while the latter is considered literal since both words are concrete (Turney et al., 2011). Broadwell et al. (2013) use measures of imageability to detect metaphor, a similar concept to abstractness and concreteness. The lexical coherence approach uses the fact that metaphorically used words are semantically not coherent with context words. Broadwell et al. (2013) use topic chaining to categorize words as nonmetaphorical when they have a semantic relationship to the main topic. Sporleder and Li (2009) also use lexical chains and semantic cohesion graphs to detect metaphors. To the best of our knowledge, there has been no computational work on the effect of situational fac3 3 Data M"
W15-1401,P13-2145,1,0.874099,"Missing"
W15-4650,P98-1013,0,0.181065,"al Features We use the following features to represent global contexts of a given text. Semantic Category: Lexico-semantic resources (e.g. FrameNet, WordNet) provide categorical information for much of the English lexicon. If a target word is used literally, the document may have a high proportion of words in the same semantic category. If the word is used metaphorically, the document may contain more words that share different semantic categories. To implement this intuition, we use SEMAFOR (Das et al., 2014) to assign each word to one of the categories provided by the FrameNet 1.5 taxonomy (Baker et al., 1998). Then, we compute the relative proportion of the target word’s category with regards to categories appearing in the document to measure the alignment of categories of the target word The intuition for our main idea is that metaphorically-used words would often break lexical cohesion of text, while literal expressions would maintain a single connected graph of topically or semantically related words. Therefore, we identify that these incohesive words may serve as cues for nonliteral expressions. The following two examples illustrate the described phenomenon, both of which contain the same phra"
W15-4650,1996.eamt-1.16,0,0.604438,"Missing"
W15-4650,J14-1002,0,0.0312143,"pic of discussion, whereas w7 is only indirectly related to the topic through w2 . 3.1 Global Contextual Features We use the following features to represent global contexts of a given text. Semantic Category: Lexico-semantic resources (e.g. FrameNet, WordNet) provide categorical information for much of the English lexicon. If a target word is used literally, the document may have a high proportion of words in the same semantic category. If the word is used metaphorically, the document may contain more words that share different semantic categories. To implement this intuition, we use SEMAFOR (Das et al., 2014) to assign each word to one of the categories provided by the FrameNet 1.5 taxonomy (Baker et al., 1998). Then, we compute the relative proportion of the target word’s category with regards to categories appearing in the document to measure the alignment of categories of the target word The intuition for our main idea is that metaphorically-used words would often break lexical cohesion of text, while literal expressions would maintain a single connected graph of topically or semantically related words. Therefore, we identify that these incohesive words may serve as cues for nonliteral expressi"
W15-4650,N13-1119,0,0.0133664,"ing our model for 2,000 iterations on a large data set. Then, for the estimation on test documents we apply this model to our test data set for 100 iterations of Gibbs sampling. The original LDA computes P (word|topic) instead of P (topic|word). In order to compute P (topic|word), the first 20 iterations out of 100 are used as a burn-in phase, and then we collect sample topic assignments for each word in every other iteration. This process results in a total of 40 topic assignments for a word in a document, and we use these topic assignments to estimate the topic distributions per word as in (Remus and Biemann, 2013). We used the GibbsC++ toolkit (Phan and Nguyen, 2007) with default parameters to train the model. Finally, we use the cosine similarity between P (topic|document) and P (topic|word) as features that represent the global alignment of topics between the target word and the document. Lexical Chain: We use lexical chains (Morris and Hirst, 1991) to obtain multiple sequences of 1 semantically related words in a text. From the intuition that metaphorical words would not belong to dominant lexical chains of the given text, we use the lexical chain membership of a target word as a cue for its non-lit"
W15-4650,O14-3001,0,0.215776,"text, implying that contextual incoherence might serve as a cue for detecting metaphors. Based on this observation, the idea of leveraging textual context to detect metaphors has been recently proposed by some researchers (Broadwell et al., 2013; Sporleder and Li, 2009). Introduction Detecting metaphors in text is an active line of research which has attracted attention in recent years. To date, most of the previous literature has looked at lexical semantic features such as selectional restriction violations (Martin, 1996; Shutova and Teufel, 2010; Shutova et al., 2010; Shutova et al., 2013; Huang, 2014) or contrast in lexical concreteness and abstractness (Turney et al., 2011; Broadwell et al., 2013; Tsvetkov et al., 2013). While these approaches have been shown to be successful in detecting metaphors given a single sentence, metaphor detection in discourse brings a new dimension to the task. Consider the following excerpt from an online Breast Cancer discussion forum as an example: welcome, glad for the company .... just sad to see that there are so many of us. Here is a thought that I have been thinking since I was diagnosed. This disease should be called the “Hurry up 384 Proceedings of t"
W15-4650,W14-2303,0,0.0443615,"d by the observation that metaphorical words are often semantically incoherent with context words. There have been several approaches proposed to compute lexical coherence. Broadwell et al. (2013), for instance, employed topic chaining to categorize metaphors, whereas Sporleder and Li (2009) have proposed to use lexical chains and semantic cohesion graphs to detect metaphors. Shutova and Sun (2013) and Shutova et al. (2013) have formulated the metaphor detection problem similar to outlier detection or anomaly detection tasks, and proposed to use topic signatures as lexical coherence features. Schulder and Hovy (2014) used TFIDF to obtain domain term relevance, and applied this feature to detect metaphors. Relation to Prior Work The main approaches to computationally detecting metaphors can be categorized into work that considers the following three classes of features: selectional preferences, abstractness and concreteness, and lexical cohesion. Selectional preferences relate to how semantically compatible predicates are with particular arguments. For example, the verb drink prefers beer as an object over computer. The idea behind using selectional preferences for metaphor detection is that metaphorical w"
W15-4650,N13-1118,0,0.0727567,"O or A+N structures) for their experiments, in order to test their hypothesis in a controlled way. Another line of work considers lexical coherence of text as a cue for metaphor. The lexical coherence approach is motivated by the observation that metaphorical words are often semantically incoherent with context words. There have been several approaches proposed to compute lexical coherence. Broadwell et al. (2013), for instance, employed topic chaining to categorize metaphors, whereas Sporleder and Li (2009) have proposed to use lexical chains and semantic cohesion graphs to detect metaphors. Shutova and Sun (2013) and Shutova et al. (2013) have formulated the metaphor detection problem similar to outlier detection or anomaly detection tasks, and proposed to use topic signatures as lexical coherence features. Schulder and Hovy (2014) used TFIDF to obtain domain term relevance, and applied this feature to detect metaphors. Relation to Prior Work The main approaches to computationally detecting metaphors can be categorized into work that considers the following three classes of features: selectional preferences, abstractness and concreteness, and lexical cohesion. Selectional preferences relate to how sem"
W15-4650,W14-2301,1,0.899458,"Missing"
W15-4650,shutova-teufel-2010-metaphor,0,0.0772922,"c of discussion are less likely to be metaphorical than other words in text, implying that contextual incoherence might serve as a cue for detecting metaphors. Based on this observation, the idea of leveraging textual context to detect metaphors has been recently proposed by some researchers (Broadwell et al., 2013; Sporleder and Li, 2009). Introduction Detecting metaphors in text is an active line of research which has attracted attention in recent years. To date, most of the previous literature has looked at lexical semantic features such as selectional restriction violations (Martin, 1996; Shutova and Teufel, 2010; Shutova et al., 2010; Shutova et al., 2013; Huang, 2014) or contrast in lexical concreteness and abstractness (Turney et al., 2011; Broadwell et al., 2013; Tsvetkov et al., 2013). While these approaches have been shown to be successful in detecting metaphors given a single sentence, metaphor detection in discourse brings a new dimension to the task. Consider the following excerpt from an online Breast Cancer discussion forum as an example: welcome, glad for the company .... just sad to see that there are so many of us. Here is a thought that I have been thinking since I was diagnosed. This d"
W15-4650,C10-1113,0,0.297184,"ikely to be metaphorical than other words in text, implying that contextual incoherence might serve as a cue for detecting metaphors. Based on this observation, the idea of leveraging textual context to detect metaphors has been recently proposed by some researchers (Broadwell et al., 2013; Sporleder and Li, 2009). Introduction Detecting metaphors in text is an active line of research which has attracted attention in recent years. To date, most of the previous literature has looked at lexical semantic features such as selectional restriction violations (Martin, 1996; Shutova and Teufel, 2010; Shutova et al., 2010; Shutova et al., 2013; Huang, 2014) or contrast in lexical concreteness and abstractness (Turney et al., 2011; Broadwell et al., 2013; Tsvetkov et al., 2013). While these approaches have been shown to be successful in detecting metaphors given a single sentence, metaphor detection in discourse brings a new dimension to the task. Consider the following excerpt from an online Breast Cancer discussion forum as an example: welcome, glad for the company .... just sad to see that there are so many of us. Here is a thought that I have been thinking since I was diagnosed. This disease should be calle"
W15-4650,W09-2001,0,0.0170719,"iscourse (e.g. diagonsed, disease, biopsy are semantically contrasted with train, rushes, and station). This clearly demonstrates the need for a new set of computational tools to represent context beyond a single sentence, in order to better detect metaphorical expressions that have contextual connections outside the sentence in which they are used. Context for metaphor detection. Metaphor is a semantic phenomenon that describes objects often with a view borrowed from a different domain. As such, it is natural that metaphors inherently break the lexical coherence of a sentence or a discourse. Klebanov et al. (2009), for example, showed in their study that words related to the topic of discussion are less likely to be metaphorical than other words in text, implying that contextual incoherence might serve as a cue for detecting metaphors. Based on this observation, the idea of leveraging textual context to detect metaphors has been recently proposed by some researchers (Broadwell et al., 2013; Sporleder and Li, 2009). Introduction Detecting metaphors in text is an active line of research which has attracted attention in recent years. To date, most of the previous literature has looked at lexical semantic"
W15-4650,J13-2003,0,0.199638,"al than other words in text, implying that contextual incoherence might serve as a cue for detecting metaphors. Based on this observation, the idea of leveraging textual context to detect metaphors has been recently proposed by some researchers (Broadwell et al., 2013; Sporleder and Li, 2009). Introduction Detecting metaphors in text is an active line of research which has attracted attention in recent years. To date, most of the previous literature has looked at lexical semantic features such as selectional restriction violations (Martin, 1996; Shutova and Teufel, 2010; Shutova et al., 2010; Shutova et al., 2013; Huang, 2014) or contrast in lexical concreteness and abstractness (Turney et al., 2011; Broadwell et al., 2013; Tsvetkov et al., 2013). While these approaches have been shown to be successful in detecting metaphors given a single sentence, metaphor detection in discourse brings a new dimension to the task. Consider the following excerpt from an online Breast Cancer discussion forum as an example: welcome, glad for the company .... just sad to see that there are so many of us. Here is a thought that I have been thinking since I was diagnosed. This disease should be called the “Hurry up 384 Pr"
W15-4650,E09-1086,0,0.573776,"phenomenon that describes objects often with a view borrowed from a different domain. As such, it is natural that metaphors inherently break the lexical coherence of a sentence or a discourse. Klebanov et al. (2009), for example, showed in their study that words related to the topic of discussion are less likely to be metaphorical than other words in text, implying that contextual incoherence might serve as a cue for detecting metaphors. Based on this observation, the idea of leveraging textual context to detect metaphors has been recently proposed by some researchers (Broadwell et al., 2013; Sporleder and Li, 2009). Introduction Detecting metaphors in text is an active line of research which has attracted attention in recent years. To date, most of the previous literature has looked at lexical semantic features such as selectional restriction violations (Martin, 1996; Shutova and Teufel, 2010; Shutova et al., 2010; Shutova et al., 2013; Huang, 2014) or contrast in lexical concreteness and abstractness (Turney et al., 2011; Broadwell et al., 2013; Tsvetkov et al., 2013). While these approaches have been shown to be successful in detecting metaphors given a single sentence, metaphor detection in discourse"
W15-4650,W14-2302,0,0.159524,"thus twofold: first, we propose several textual descriptors that can capture global contextual shifts among a discourse, such as semantic word category distribution obtained from a framesemantic parser, homogeneity in topic distributions, and lexical chains. Second, we show that global and local contextual information are complimentary in detecting metaphors, and that leveraging syntactic features is crucial in better describing lexico-semantic information in a local context. Our method achieves higher performance on a metaphor disambiguation task than state-ofthe-art systems from prior work (Klebanov et al., 2014; Tsvetkov et al., 2013) on our newly created dataset from an online discussion forum. The rest of the paper is organized as follows. Section 2 relates our work to prior work. Section 3 explains our method in detail, specifically in regards to how we use global context and local context for metaphor detection. Section 4 describes the Breast Cancer dataset annotated and used for our experiment. In Section 5, we present our experimental results and show the effectiveness of our method with the task of metaphor disambiguation. Section 6 analyzes the results and identifies potential areas of impro"
W15-4650,P14-5010,0,0.004255,"ncept that is easy to grasp. With this intuition, Turney et al. (2011) showed that the word abstractness/concreteness measure is a useful clue for detecting metaphors. To represent the concreteness of a word, we used Brysbaert’s database of concreteness ratings for about 40,000 English words (Brysbaert et al., 2014). We use the mean ratings in the database as a numerical feature for the target word. In addition, we also use the concreteness ratings of the words in grammatical relations to the target word as local context features. Grammatical Dependencies: We use the stanford-corenlp toolkit (Manning et al., 2014) to parse dependency relations of our data and apply grammatical dependencies as described above for each semantic feature. We use grammatical dependencies only between content words (e.g. words with syntactic categories of noun, verb, adjective, and adverb). 4 We built an annotated dataset for our experiments as follows. We first picked seven metaphor candidates that appear either metaphorically or literally in the Breast Cancer corpus: boat, candle, light, ride, road, spice, and train. We then retrieved all the posts in the corpus that contain these candidate words, and annotated each post a"
W15-4650,W13-0906,0,0.144132,"ion, the idea of leveraging textual context to detect metaphors has been recently proposed by some researchers (Broadwell et al., 2013; Sporleder and Li, 2009). Introduction Detecting metaphors in text is an active line of research which has attracted attention in recent years. To date, most of the previous literature has looked at lexical semantic features such as selectional restriction violations (Martin, 1996; Shutova and Teufel, 2010; Shutova et al., 2010; Shutova et al., 2013; Huang, 2014) or contrast in lexical concreteness and abstractness (Turney et al., 2011; Broadwell et al., 2013; Tsvetkov et al., 2013). While these approaches have been shown to be successful in detecting metaphors given a single sentence, metaphor detection in discourse brings a new dimension to the task. Consider the following excerpt from an online Breast Cancer discussion forum as an example: welcome, glad for the company .... just sad to see that there are so many of us. Here is a thought that I have been thinking since I was diagnosed. This disease should be called the “Hurry up 384 Proceedings of the SIGDIAL 2015 Conference, pages 384–392, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computationa"
W15-4650,D11-1063,0,0.680939,"r detecting metaphors. Based on this observation, the idea of leveraging textual context to detect metaphors has been recently proposed by some researchers (Broadwell et al., 2013; Sporleder and Li, 2009). Introduction Detecting metaphors in text is an active line of research which has attracted attention in recent years. To date, most of the previous literature has looked at lexical semantic features such as selectional restriction violations (Martin, 1996; Shutova and Teufel, 2010; Shutova et al., 2010; Shutova et al., 2013; Huang, 2014) or contrast in lexical concreteness and abstractness (Turney et al., 2011; Broadwell et al., 2013; Tsvetkov et al., 2013). While these approaches have been shown to be successful in detecting metaphors given a single sentence, metaphor detection in discourse brings a new dimension to the task. Consider the following excerpt from an online Breast Cancer discussion forum as an example: welcome, glad for the company .... just sad to see that there are so many of us. Here is a thought that I have been thinking since I was diagnosed. This disease should be called the “Hurry up 384 Proceedings of the SIGDIAL 2015 Conference, pages 384–392, c Prague, Czech Republic, 2-4 S"
W15-4650,N10-2007,0,0.409052,"Missing"
W15-4650,J91-1002,0,\N,Missing
W15-4650,C98-1013,0,\N,Missing
W15-4650,W13-4203,0,\N,Missing
W17-2346,S13-2012,0,0.0471108,"k in event extraction and temporal resolution which we leverage, while section 3 describes our datasets. Section 4 introduces the architecture of our proposed system and section 5 talks about the system modules in more detail. Section 6 describes our experiments and evaluation, while section 7 presents a brief error analysis and describes possible future extensions. Section 8 concludes the paper. 2 Related Work Event extraction is a well-studied topic in natural language processing. This has resulted in the development of several off-the-shelf tools for event extraction (Saur´ı et al. (2005), Chambers (2013), Derczynski et al. (2016)). All these tools have been developed for extraction of public events from news corpora. Some prior work has also studied extraction of public events from social media (Sakaki et al. (2010), Becker et al. (2010), Ritter et al. (2012)). However, in this work, we want to focus on extracting personal medical events for users from their posts on online support groups using minimal supervision. There has been some prior work on personal event extraction from social media, especially twitter )Li and Cardie (2014); Li et al. (2014)). Li et al. (2014) developed a system for"
W17-2346,chang-manning-2012-sutime,0,0.019039,"om online support groups. Their system used manually constructed keyword sets for event extraction. We propose a minimally supervised medical event detection pipeline which can remove the need to create these manual keyword sets. Since we want to create event timelines for users from their posts in online support groups, we also need to perform temporal expression detection and resolution as well as linking of temporal expressions to events. Temporal expression extraction and normalization is also a well-studied area and several off-the-shelf systems are available (Str¨otgen and Gertz (2010), Chang and Manning (2012), Derczynski et al. (2016)). Moreover, some systems perform both temporal resolution and linking of events with temporal expressions (Chambers (2013)). However, most of these systems are developed for news data and do not work very well with the informal writing style used on social media. But there have been some efforts to develop systems which work well for this space. Wen et al. (2013) developed a temporal tagger and resolver for informal temporal references on social media, but the system is not available for use. The HeidelTime system Str¨otgen and Gertz (2010) also has a ”colloquial eng"
W17-2346,P16-1025,0,0.0282875,"lso want to build a system for personal event extraction from online support groups, our focus is on identifying medical events. Hence, the techniques used by Li et al. (2014) do not work very well for us. Online support groups are not as personfocused as twitter, so the presence of congratulations/ condolence speech acts is not a strong signal for personal medical event detection. Moreover, as we show in section 6, LDA is unable to perform well on personal medical event type detection. So, we use a different technique for event type detection, which is partly similar to the technique used by Huang et al. (2016). Our overall system pipeline for data-driven medical event detection with minimal supervision is partly inspired by Li et al. (2014). On the other hand, there has not been extensive research on personal medical event extraction from online support groups. Wen and Ros´e (2012) developed a system for medical event extraction from online support groups. Their system used manually constructed keyword sets for event extraction. We propose a minimally supervised medical event detection pipeline which can remove the need to create these manual keyword sets. Since we want to create event timelines fo"
W17-2346,D14-1214,0,0.178263,"for event extraction (Saur´ı et al. (2005), Chambers (2013), Derczynski et al. (2016)). All these tools have been developed for extraction of public events from news corpora. Some prior work has also studied extraction of public events from social media (Sakaki et al. (2010), Becker et al. (2010), Ritter et al. (2012)). However, in this work, we want to focus on extracting personal medical events for users from their posts on online support groups using minimal supervision. There has been some prior work on personal event extraction from social media, especially twitter )Li and Cardie (2014); Li et al. (2014)). Li et al. (2014) developed a system for personal event extraction from twitter using minimal supervision. They used the presence of congratulations/ condolence speech acts to detect personal event mentions in tweets and clustering based on the Latent Dirichlet Algorithm (Blei et al. (2003)) to detect personal event types. However, they did not focus specifically on medical events. While we also want to build a system for personal event extraction from online support groups, our focus is on identifying medical events. Hence, the techniques used by Li et al. (2014) do not work very well for u"
W17-2346,H05-1088,0,0.702827,"Missing"
W17-2346,S10-1071,0,0.0607427,"Missing"
W17-2346,P13-2145,1,0.801526,"Missing"
W17-2908,W12-2105,0,0.377903,"the aforementioned theoretical principles and their correlation with influence. We attempt to extend the prior computational efforts on social influence, by using insights from the Social Sciences. Influence can be defined and operationalized in different settings. A majority of computational work on interpersonal influence focuses on the analysis of social networks that employ probabilistic methods to analyze and maximize the flow of influence in these networks. There have been recent efforts in understanding influence in social media conversations with the aim of finding influential people (Biran et al., 2012; Quercia et al., 2011; Rosenthal and McKeown, 2016). We investigate what we can learn from language about influence from informal interactions where there is no explicit motivation to influence others. We look at user interactions in a social networking website for people interested in knitting, weaving, crocheting and fiber arts called Ravelry 1 , which is a large DIY online community with tens of thousands of sub-communities within it. In the following sections we talk about prior work on social influence and the approaches taken to study it. We describe our dataset and the task setup that"
W17-2908,W16-5604,0,0.173046,"and their correlation with influence. We attempt to extend the prior computational efforts on social influence, by using insights from the Social Sciences. Influence can be defined and operationalized in different settings. A majority of computational work on interpersonal influence focuses on the analysis of social networks that employ probabilistic methods to analyze and maximize the flow of influence in these networks. There have been recent efforts in understanding influence in social media conversations with the aim of finding influential people (Biran et al., 2012; Quercia et al., 2011; Rosenthal and McKeown, 2016). We investigate what we can learn from language about influence from informal interactions where there is no explicit motivation to influence others. We look at user interactions in a social networking website for people interested in knitting, weaving, crocheting and fiber arts called Ravelry 1 , which is a large DIY online community with tens of thousands of sub-communities within it. In the following sections we talk about prior work on social influence and the approaches taken to study it. We describe our dataset and the task setup that allows us to measure influence. We give an overview"
W17-2908,D16-1178,0,0.109749,"heir study of social influence research, Cialdini and Goldstein (2002) identify six basic principles that govern how one person might influence another. They are: liking, reciprocation, consistency, scarcity, social validation and authority. These principles control how influence plays out in different social situations. The above mentioned principles constitute a solid basis for most of the work in this domain. Prior computational approaches for understanding influence, have primarily focused on influence as an explicit intention of the people involved (Tan et al., 2016a; Biran et al., 2012; Sim et al., 2016). ∗ Institute for Software Research Carnegie Mellon University Pittsburgh, PA 15213. 2 Related Work There has been a substantial amount of computational work on modeling and detecting influence that can be broadly divided in two categories: ‘In1 Both authors contributed equally to this work. https://www.ravelry.com/ 53 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 53–62, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics seed nodes) that would maximize the diffusion or the spread of influence. Chen et"
W17-2911,P13-1025,0,0.0790416,"Missing"
W17-2911,W14-3629,0,0.0202044,"choices have in other contexts? These questions motivate our study. Terms and definitions for code-switching and code-mixing across studies vary considerably (Gardner-Chloros, 2009). Since we are interested in all deviation from the likely norm of Arabic, we accept any instances of switching between languages in a conversation as code-switching. We also include “script-switching”, since we assume most editors can use Arabic characters and there may be social significance attached to writing Arabic in Latin script (something called Arabizi), especially since such language is usually dialectal (Darwish, 2014). Table 1 presents a few motivating examples of language variety in Arabic Wikipedia talk pages. Most CS we see is Arabic-English, but there are examples of French and Arabizi, the romanized Arabic seen in the third example in Table 1. We also note apologies for using English, including a longer exchange on the Israel talk page where an editor is confronted about language choice and replies in Arabizi: 4 Data and Task To capture the social effect of code-switching, we choose a task predicting social influence from CS features in discussion. In the context of the Arabic Wikipedia, we measure so"
W17-2911,W14-3911,0,0.0238548,"maxims, but instead are using them to understand meaning in interaction and to more fully explain natural language data. We assume a community norm of Arabic on the Arabic Wikipedia and expect CS to be marked and have some sort of social effect. However, MyersScotton (1998) allows the possibility of contexts where CS is itself unmarked; this would also be 3 Code-Switching on Arabic Wikipedia Talk Pages Though many language Wikipedias contain codeswitching on their talk pages, we select the Arabic Wikipedia for the variation we observe and previous Arabic CS work in NLP (Solorio et al., 2014; Elfardy et al., 2014). 74 Talk page GNU/Linux Oran, Algeria Said Aouita Lebanon Text English translation threaded fs. Salam, Les missions principales du centre sont: la recherche... hafid hassan ana fakhour dh TfO b3outa Sorry for talking english I notice you use the image... threaded fs. Greetings, the main missions of the center are: research... Target page [name] I am proud to... CAm F®F and it has a multi- ... a string used, and it has a multiSorry for talking english I notice you use the image... Table 1: Observations of non-Arabic text in Arabic Wikipedia talk pages Arabic is leveled as grounds for n"
W17-2911,P11-4017,0,0.0746933,"Missing"
W17-2911,P16-1021,1,0.833207,"ttings (Safi-Stagni, 1991). Codeswitching could demonstrate a level of expertise or world knowledge and have a positive effect on the acceptance of an editor’s contributions. 73 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 73–82, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics possible in our case. Recent computational analyses of style, metaphor, framing and politeness have investigated how language is used to achieve social goals in online communities (Danescu-Niculescu-Mizil et al., 2012, 2013; Jang et al., 2016; Tsur et al., 2015). We examine CS in a similar fashion. Interactional, discourse-level features are contextspecific, and the relationship between social and linguistic features is fluid and often difficult to computationalize (Nguyen et al., 2016). Codeswitching may not carry clear social meaning at all in a given context (Auer, 2013), much less a predictable signal. Our work enters this conversation by exploring the effect of code-switching on social influence in an online community. The NLP community has largely studied codeswitching apart from its social context. Much work has focused on"
W17-2911,L16-1260,0,0.0592635,", 2016). Others have worked to predict code-switch points from preceding text. Solorio and Liu (2008) predict codeswitch points with features including the previous n-grams’ identified language, POS tag, and location in constituent parses in both languages. Piergallini et al. (2016) tackle the same task in combination with language identification on a SwahiliEnglish online forum dataset. They note the possibility of using discourse structure and social variables for predicting code-switch points. Interest in computational models of the social and pragmatic nature of code-switching is growing. Begum et al. (2016) present an annotation scheme for the pragmatic functions of Hindi-English code-switched tweets, which includes reinforcement, sarcasm, reported speech, and changes from narration to evaluation. Rudra et al. (2016) study language preference for the expression of sentiment among Hindi-English multilinguals, finding that speakers more commonly use Hindi to express negative sentiment and English for positive sentiment on Twitter. To determine which of these hypotheses is a more likely explanation for CS in this context, we construct a publicly released dataset that pairs discussion between Wikipe"
W17-2911,P12-3005,0,0.0285116,"editor’s concatenated text in the entire thread (all their posts), along with the combination of all other editors’ text as separate features. 4.2 Editor Success Scores Language Identification Pn ||ci || s(u, t) = 1 − Pni=1 We find a diversity of language on Arabic Wikipedia talk pages not written in the Arabic script, including English, French, Hebrew, Turkish, Chinese and even a few words written in the Tifinagh and Syriac scripts. To initially survey the distribution of languages, we run all spans of tokens without Arabic characters (and that are not wholly punctuation) through langid.py (Lui and Baldwin, 2012), a language identification tool that can detect 97 languages. It is trained in a supervised fashion with Naive Bayes on byte n-grams, using cross-domain training data. langid.py finds 66 languages present within the dataset, but a qualitative analysis finds that named entities and noise in the dataset (special characters, usernames that passed through our prei=1 ||ei || Each editor’s score is the proportion of tokens they changed that remain changed, so s ∈ [0, 1]. In a qualitative evaluation, this editor score formulation was found to accurately reflect an editor’s impact on the revision of"
W17-2911,W14-3907,0,0.136327,"onal, discourse-level features are contextspecific, and the relationship between social and linguistic features is fluid and often difficult to computationalize (Nguyen et al., 2016). Codeswitching may not carry clear social meaning at all in a given context (Auer, 2013), much less a predictable signal. Our work enters this conversation by exploring the effect of code-switching on social influence in an online community. The NLP community has largely studied codeswitching apart from its social context. Much work has focused on word-level CS language identification, encouraged by shared tasks (Solorio et al., 2014; Molina et al., 2016). Others have worked to predict code-switch points from preceding text. Solorio and Liu (2008) predict codeswitch points with features including the previous n-grams’ identified language, POS tag, and location in constituent parses in both languages. Piergallini et al. (2016) tackle the same task in combination with language identification on a SwahiliEnglish online forum dataset. They note the possibility of using discourse structure and social variables for predicting code-switch points. Interest in computational models of the social and pragmatic nature of code-switchi"
W17-2911,W16-5805,0,0.0312643,"features are contextspecific, and the relationship between social and linguistic features is fluid and often difficult to computationalize (Nguyen et al., 2016). Codeswitching may not carry clear social meaning at all in a given context (Auer, 2013), much less a predictable signal. Our work enters this conversation by exploring the effect of code-switching on social influence in an online community. The NLP community has largely studied codeswitching apart from its social context. Much work has focused on word-level CS language identification, encouraged by shared tasks (Solorio et al., 2014; Molina et al., 2016). Others have worked to predict code-switch points from preceding text. Solorio and Liu (2008) predict codeswitch points with features including the previous n-grams’ identified language, POS tag, and location in constituent parses in both languages. Piergallini et al. (2016) tackle the same task in combination with language identification on a SwahiliEnglish online forum dataset. They note the possibility of using discourse structure and social variables for predicting code-switch points. Interest in computational models of the social and pragmatic nature of code-switching is growing. Begum e"
W17-2911,D08-1102,0,0.0515102,"fluid and often difficult to computationalize (Nguyen et al., 2016). Codeswitching may not carry clear social meaning at all in a given context (Auer, 2013), much less a predictable signal. Our work enters this conversation by exploring the effect of code-switching on social influence in an online community. The NLP community has largely studied codeswitching apart from its social context. Much work has focused on word-level CS language identification, encouraged by shared tasks (Solorio et al., 2014; Molina et al., 2016). Others have worked to predict code-switch points from preceding text. Solorio and Liu (2008) predict codeswitch points with features including the previous n-grams’ identified language, POS tag, and location in constituent parses in both languages. Piergallini et al. (2016) tackle the same task in combination with language identification on a SwahiliEnglish online forum dataset. They note the possibility of using discourse structure and social variables for predicting code-switch points. Interest in computational models of the social and pragmatic nature of code-switching is growing. Begum et al. (2016) present an annotation scheme for the pragmatic functions of Hindi-English code-sw"
W17-2911,P15-1157,0,0.0149864,", 1991). Codeswitching could demonstrate a level of expertise or world knowledge and have a positive effect on the acceptance of an editor’s contributions. 73 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 73–82, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics possible in our case. Recent computational analyses of style, metaphor, framing and politeness have investigated how language is used to achieve social goals in online communities (Danescu-Niculescu-Mizil et al., 2012, 2013; Jang et al., 2016; Tsur et al., 2015). We examine CS in a similar fashion. Interactional, discourse-level features are contextspecific, and the relationship between social and linguistic features is fluid and often difficult to computationalize (Nguyen et al., 2016). Codeswitching may not carry clear social meaning at all in a given context (Auer, 2013), much less a predictable signal. Our work enters this conversation by exploring the effect of code-switching on social influence in an online community. The NLP community has largely studied codeswitching apart from its social context. Much work has focused on word-level CS langua"
W17-2911,D16-1121,1,0.879023,"Missing"
W17-5538,D14-1162,0,0.0809725,"each pattern cluster are facet instance candidates. Although we have clusters of similar facet instance candidates, there are many noisy instances in each cluster. To determine which instances are most reliable, we score each instance based on how far its generating patterns are from the center of the cluster. Specifically, an instance is scored high if it is found in more patterns in the cluster, and in patterns with higher within-cluster scores. We also take into account how semantically close each instance is to the other words in the same cluster. We use the GloVe vector representations (Pennington et al., 2014) to compute cosine similarity between two words. The scoring formula is shown below, where Ni is the number of different patterns that extracted wordi , Sim is the average cosine similarity with all other words in the same cluster, score patternk is within-cluster score computed by NMF. Cluster Lexico-Grammar Patterns score(wordi ) = Sim∗ Using the idea that lexico-grammar patterns can approximate semantic relations, we first cluster collected lexico-grammar patterns so that each cluster may represent a different relation (facet slot). The feature representation of each pattern is based on all"
W17-5538,W13-0907,1,0.881605,"ce concept occur in similar lexicosyntactic settings. They cluster nouns (target domain) and verbs (source domain), and search the corpus for metaphors that use the verbs in the source domain lexicon to represent the target domain concepts. Extending Shutova et al. (2010), (Shutova and Sun, 2013) find metaphorical mappings by building and traversing a graph of concepts. Then, they generate lists of salient features for the metaphorically connected clusters, and search the corpus for metaphors that use the verbs in the salient features to represent the target domain concepts. Another approach, Hovy et al. (2013) detected metaphors using certain semantic patterns appearing in metaphor manifestations. For example, “sweet” with food is literal, but is metaphorical with people. By finding these patterns on different levels, they extended the application of this mapping information from a narrow focus on verb relations to other syntactic relations. Along the same lines, Mohler et al. (2013) presented a domain-aware semantic signature to capture source and target domains for a text. A semantic signature represents the placement of a text on a semantic space by using a set of related WordNet senses, and it"
W17-5538,W13-1602,0,0.023732,"e her status (EX(7)) and her wish to the other person with the extension of get back on after you fall. Although falling off the wagon and on the wagon are metaphorical idioms, get back on after you fall is a novel metaphor created by the following speaker. This novel metaphor is drawn from the wagon frame that has been brought into this conversation. In this way, a metaphor that is taken up by multiple speakers may increase empathetic understanding as well as add creative opportunities (e.g., for “fun”) to the conversation. 323 Figure 1: System flow diagram. 1. as (Riloff et al., 1999, 2003; Qadir and Riloff, 2013). In our model, we assume that a sentence tends to contain more than one important facet of a metaphor frame. In other words, if a sentence contains one facet of a metaphor frame, the sentence is likely to contain additional facets. Additionally, we assume that facets and dependency relations have some relationship. There are certain grammatical patterns that represent semantic relations that connect facets in context. Note that we disregard frame facet instances that do not cooccur with a keyword (e.g., journey) within the same sentence. This can be considered as a limitation of this approach"
W17-5538,O14-3001,0,0.0117225,"ism through which the frame and topic information enable the more accurate metaphor detection. 1 Introduction Computational work on metaphor has largely focused on metaphor detection within individual sentences, for the purpose of identification of literal meaning, with an eye towards improvement of downstream applications like Machine Translation. This limited conceptualization of metaphor within these restricted contexts has allowed prior work to leverage local indicators to identify metaphorical language, such as the violation of selectional preferences (Martin, 1996; Shutova et al., 2010; Huang, 2014) or the use of abstract vs concrete descriptors (Turney et al., 2011; Brysbaert et al., 2014; Tsvetkov et al., 2013). When detecting metaphor in an extended discourse, and especially for the purpose of modeling the use of metaphor in interaction, however, a broader conceptualization of metaphor is needed in order to accommodate the many places where these simplifying assumptions break down (Jang 320 Proceedings of the SIGDIAL 2017 Conference, pages 320–330, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics metaphorically, or what target and source domai"
W17-5538,N16-1146,0,0.0226168,"“All the world’s a stage and men and women merely players.” (Shakespeare, Twelfth Night) EX(3) “Bobby Holloway says my imagination is a three-hundred-ring circus. Currently I was in ring two hundred and ninety-nine, with elephants dancing and clowns cart wheeling and tigers leaping through rings of fire. The time had come to step back, leave the main tent, go buy some popcorn and a Coke, bliss out, cool down.” (Dean Koontz, Seize the Night. Bantam, 1999) Extraction of Properties So far very little computational work has focused on facets, or properties, of metaphor specifically. However, the Qadir et al. (2016) approach auto322 EX(5) “falling off the wagon is no big thing in my opinion, the psychological good feelings of enjoyment weigh in big for feeling good.” EX(6) “Tina falling off is part of this journey, it is stupid to deny yourself everything.” EX(7) “I am on the wagon so far today . . . ongoing battle.” EX(8) “Tina — hope you stay on the wagon, or at least get back on after you fall!” In the breast cancer discussion forum we use in our work, community participants frequently bring in journey and battle frames when talking about their cancer experience. Depending on what aspects of the cance"
W17-5538,P16-1021,1,0.863785,"Missing"
W17-5538,W03-0404,0,0.0365829,"Missing"
W17-5538,W15-4650,1,0.851491,"Missing"
W17-5538,P14-5010,0,0.00267755,"ontain the frame (e.g. journey) and at least one example seed facet instance. Note that the sentences in the corpus are not annotated metaphorical or literal. Since we are building a frame that can be used either metaphorically or literally, we do not require sen324 As a solution, we propose using lexicogrammatical patterns generated from dependency paths between a domain word and facet words via the ROOT. The lexico-grammatical patterns are defined as the shortest path that passes through the ROOT in dependencies between the domain name and seed facet instances. For example, StanfordCoreNLP (Manning et al., 2014) outputs the dependencies in Table 2 for the sentence She resumed her journey through the city. The lexicogrammatical pattern that connects journey with other candidate property words such as she and city is defined as the reverse path from journey to ROOT combined with the path from ROOT to journey. The paths for the example are shown in Table 3. Words are lemmatized to reduce sparsity. This lexico-grammatical pattern representation has advantages. First, it allows representing patterns connecting pairs of words in a position invariant manner. For example, in our baseline bootstrapping model,"
W17-5538,J15-4002,0,0.0133813,"opting the concept of a frame may be useful for studying metaphor in discourse from a social perspective. Section 4 explains our semi-supervised approach of template induction to model a metaphor frame in detail. Section 5 presents the effectiveness of the frame information through metaphor detection experiments. Section 6 analyzes the results and identifies when the frame information is beneficial. Section 7 concludes the paper. 2 Relation to Prior Work In this section, we discuss previous computational work on metaphor that is most relevant to our study. (For more thorough review, refer to (Shutova, 2015).) Next, Section 2.1 introduces approaches to metaphor detection by modeling metaphorical mapping patterns instead of relying on the idea of violation of linguistic expectations. Section 2.2 reviews work that specifically aims to address problems of metaphor detection in discourse. As a direction related to metaphor detection, Section 2.3 introduces computational work that extracts properties of similes, which provides inspiration for our template induction approach used to induce properties (facets) of a metaphor frame. 2.1 Modeling Metaphorical Mapping There are many different types of metap"
W17-5538,C10-1113,0,0.103882,"illustrate the mechanism through which the frame and topic information enable the more accurate metaphor detection. 1 Introduction Computational work on metaphor has largely focused on metaphor detection within individual sentences, for the purpose of identification of literal meaning, with an eye towards improvement of downstream applications like Machine Translation. This limited conceptualization of metaphor within these restricted contexts has allowed prior work to leverage local indicators to identify metaphorical language, such as the violation of selectional preferences (Martin, 1996; Shutova et al., 2010; Huang, 2014) or the use of abstract vs concrete descriptors (Turney et al., 2011; Brysbaert et al., 2014; Tsvetkov et al., 2013). When detecting metaphor in an extended discourse, and especially for the purpose of modeling the use of metaphor in interaction, however, a broader conceptualization of metaphor is needed in order to accommodate the many places where these simplifying assumptions break down (Jang 320 Proceedings of the SIGDIAL 2017 Conference, pages 320–330, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics metaphorically, or what target an"
W17-5538,N13-1118,0,0.0207064,"or what target and source domains are frequently used together in metaphors. Within these approaches that model frequent target and source domain mappings, Shutova et al. (2010) identified new metaphors by expanding seed metaphors. The idea in this approach is that target concepts that are frequently used with the same source concept occur in similar lexicosyntactic settings. They cluster nouns (target domain) and verbs (source domain), and search the corpus for metaphors that use the verbs in the source domain lexicon to represent the target domain concepts. Extending Shutova et al. (2010), (Shutova and Sun, 2013) find metaphorical mappings by building and traversing a graph of concepts. Then, they generate lists of salient features for the metaphorically connected clusters, and search the corpus for metaphors that use the verbs in the salient features to represent the target domain concepts. Another approach, Hovy et al. (2013) detected metaphors using certain semantic patterns appearing in metaphor manifestations. For example, “sweet” with food is literal, but is metaphorical with people. By finding these patterns on different levels, they extended the application of this mapping information from a n"
W17-5538,N10-2007,0,0.0296799,"Missing"
W17-5538,W13-0904,0,0.260283,"they generate lists of salient features for the metaphorically connected clusters, and search the corpus for metaphors that use the verbs in the salient features to represent the target domain concepts. Another approach, Hovy et al. (2013) detected metaphors using certain semantic patterns appearing in metaphor manifestations. For example, “sweet” with food is literal, but is metaphorical with people. By finding these patterns on different levels, they extended the application of this mapping information from a narrow focus on verb relations to other syntactic relations. Along the same lines, Mohler et al. (2013) presented a domain-aware semantic signature to capture source and target domains for a text. A semantic signature represents the placement of a text on a semantic space by using a set of related WordNet senses, and it includes source concept dimensions and target concept dimensions. The primary idea is that the signature of a known metaphor is used to detect the same conceptual metaphor. These approaches are effective for capturing frequent domain specific metaphorical mappings, and in appropriate contexts are helpful for metaphor detection. They also provided valuable insight to our approach"
W17-5538,W13-0906,0,0.0994871,"ion Computational work on metaphor has largely focused on metaphor detection within individual sentences, for the purpose of identification of literal meaning, with an eye towards improvement of downstream applications like Machine Translation. This limited conceptualization of metaphor within these restricted contexts has allowed prior work to leverage local indicators to identify metaphorical language, such as the violation of selectional preferences (Martin, 1996; Shutova et al., 2010; Huang, 2014) or the use of abstract vs concrete descriptors (Turney et al., 2011; Brysbaert et al., 2014; Tsvetkov et al., 2013). When detecting metaphor in an extended discourse, and especially for the purpose of modeling the use of metaphor in interaction, however, a broader conceptualization of metaphor is needed in order to accommodate the many places where these simplifying assumptions break down (Jang 320 Proceedings of the SIGDIAL 2017 Conference, pages 320–330, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics metaphorically, or what target and source domains are frequently used together in metaphors. Within these approaches that model frequent target and source domain m"
W17-5538,D11-1063,0,0.106048,"more accurate metaphor detection. 1 Introduction Computational work on metaphor has largely focused on metaphor detection within individual sentences, for the purpose of identification of literal meaning, with an eye towards improvement of downstream applications like Machine Translation. This limited conceptualization of metaphor within these restricted contexts has allowed prior work to leverage local indicators to identify metaphorical language, such as the violation of selectional preferences (Martin, 1996; Shutova et al., 2010; Huang, 2014) or the use of abstract vs concrete descriptors (Turney et al., 2011; Brysbaert et al., 2014; Tsvetkov et al., 2013). When detecting metaphor in an extended discourse, and especially for the purpose of modeling the use of metaphor in interaction, however, a broader conceptualization of metaphor is needed in order to accommodate the many places where these simplifying assumptions break down (Jang 320 Proceedings of the SIGDIAL 2017 Conference, pages 320–330, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics metaphorically, or what target and source domains are frequently used together in metaphors. Within these approache"
W17-5538,P10-1072,0,\N,Missing
W19-2720,C14-1045,0,0.0695606,"Missing"
W19-2720,D14-1220,0,0.0527077,"Missing"
W19-2720,C18-2025,0,0.0284083,". Introduction 2.1 Recent work in automated essay scoring focuses on local features of writing, often simply to predict grades, though sometimes to offer feedback (Burstein et al., 2003; Wilson et al., 2017). Our focus is specifically at the rhetorical structure level. Structural writing feedback is designed for helping writers to develop a clear structure in which sentences and paragraphs are wellorganized (Huang et al., 2017). Researchers have made much progress in providing feedback for enhancing writing structure with the development of intelligent writing systems, such as Writing Mentor (Madnani et al., 2018) and Writing Pal (Roscoe and McNamara, 2013). However, structural writing feedback generated from existing systems is either locally situated in individual sentences or not specific enough for students to take actions. This paper presents how RST can be used to provide global structural feedback for improving writing quality and discusses future work about providing automated writing feedback with deep learning technology. Our contributions are 1) presenting RST annotation resources that can be used to annotate student essays and 2) highlighting the huge potential of using RST annotation for p"
W19-2720,N16-3001,0,0.264466,"Missing"
W19-3403,P14-1035,0,0.0424402,"Missing"
W19-3403,N04-1015,0,0.0770175,"resting line of research has focused on constructing “plot units”, which are story representations consisting of affect states of characters and tensions between them. Plot units were first proposed by 2 All data and code are available at https://github. com/xinru1414/Reddit 23 models impose structure on transitions between latent themes, typically using an HMM. This uncovers latent themes that account for interactions among themselves, helping to identify dialogue acts, which these models aim to extract. A similar HMM-based framework has been used to extract story schemas from news articles (Barzilay and Lee, 2004). Among many conversation models, we use the Content word filtering and Speaker preferences Model (CSM), which recently offered the best performance at unsupervised dialogue act identification (Jo et al., 2017). We choose this model because it has some characteristics which make it especially useful for capturing functional structures. Above all, it automatically distinguishes between topical themes and functional structures, which have different behavior. For example, a functional structure that represents asking a question would be characterized by wh-adverbs and question marks, rather than"
W19-3403,D17-1168,0,0.0138602,"d Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recent research identified a key problem with the narrative cloze test, namely that language modeling approaches perform well without learning about events (Pichotta and Mooney, 2014; Rudinger et al., 2015). This drove the establishment of a new task: the story cloze test where the goal was to select the correct ending for a story given two endings (Mostafazadeh et al., 2016a; Sharma et al., 2018). Several works showed that incorporating event sequence information provides improvement in this task (Peng et al., 2017; Chaturvedi et al., 2017b). Additionally, some work has focused on defining new script annotation schemes (Mostafazadeh et al., 2016b; Wanzare et al., 2016; Modi et al., 2016) and domain-specific script-based story understanding (Mueller, 2004; McIntyre and Lapata, 2009). Background & Related Work Narrative Understanding Much prior work on narrative understanding has focused on extracting structured knowledge representations (“templates” or “schemas”) from narratives. These works can be divided into two major classes based on the narrative aspect they attend to: event-centric and character-centric. Event-centric appr"
W19-3403,J18-4012,0,0.0125711,"location (LDA) (Blei et al., 2003), can be used for automatic induction of such structures since they identify latent themes, which may be treated as functions, from a set of documents. However, vanilla topic models do not model transitions between themes, whereas stories tend to follow stereotypical sequences of functional structures. For example, the conflict in a story must be set up before the resolution. Hence, to account for the order of functional structures, conversation models can be employed (Ritter et al., 2010; Lee et al., 2013; Ezen-Can and Boyer, 2015; Brychc´ın and Kr´al, 2017; Joty and Mohiuddin, 2018; Paul, 2012; Wallace et al., 2013; Jo et al., 2017). These 3.1 Functional Structure Identification The first step in our pipeline is to identify the typical sequences of functional structures in the corpus, which will then be clustered to form several functional schemas. Specifically, we utilize CSM to identify underlying functional structures from the corpus. CSM is a generative model originally applied to conversation – a sequence of utterances by speakers. The model assumes that a corpus of conversations has a set of functional structures undertaken by individual sentences. Each structure"
W19-3403,D14-1181,0,0.00242222,"osts in each subreddit indeed exhibit unique structures. 4.3 Schema-1 4.3.1 Baseline Models We set up the following baseline models, which use only word-level information, for text classification: • LR: A logistic regression classifier with two feature settings (bag-of-words or tf-idf) • NB: A naive bayes classifier with two feature settings (bag-of-words or tf-idf) • SVM: A support vector machine classifier with unigram bag-of-word features • BiLSTM: A bi-directional LSTM with mean-pooling (Yang et al., 2016), followed by an MLP classifier • CNN: A CNN with filter sizes 3,4,5 and maxpooling (Kim, 2014), followed by an MLP Using Schemas for Text Classification In addition to manual interpretation, we demonstrate the practical utility of our schema extraction pipeline by applying it in a downstream task: multi-label text classification. In our task setup, we treat each post as a document and the subreddit it belongs to as the document label. Since 26 Structure 0 Label Requesting help 1 Asking for feedback & thanking 2 Disclosing personal stories 3 Presenting news/statements 4 Catch-all for questions 5 Presenting news/facts (numbers) 6 Expressing personal opinions 7 Providing motivation 8 Non-"
W19-3403,N18-2106,0,0.0716192,"ication and structure grouping for schema formation. The first stage uses the Content word filtering and Introduction Narrative understanding has long been considered a central, yet challenging task in natural language understanding (Winograd, 1972). Recent advances in NLP have revived interest in this area, especially the task of story understanding (Mostafazadeh et al., 2016a). Most computational work has focused on extracting structured story representations (often called “schemas”) from literary novels, folktales, movie plots or news articles (Chambers and Jurafsky, 2009; Finlayson, 2012; Chaturvedi et al., 2018). In our work, we shift the focus to understanding the structure of stories from a different data source: narratives found on social media. Table 1 provides an example story from the popular online discussion forum Reddit 1 . Prior work has studied stories of personal experiences found on social media, identifying new storytelling patterns. However, these studies have focused on how storyteller identity is conveyed (Page, 2013). In our work, we instead 1 https://www.reddit.com/ 22 Proceedings of the Second Storytelling Workshop, pages 22–33 c Florence, Italy, August 1, 2019. 2019 Association f"
W19-3403,C16-1038,0,0.0151211,"n. • AllSent: Both general and schema domains contain non-zero feature values computed using sentences from the entire document. For each document, only one schema domain (i.e. assigned schema) contains non-zero values. • SchemaSent: General domain feature values are computed using the entire document, while schema domain feature values are computed using only sentences which contain structures present in the assigned schema. Schema-based Extension Models To incorporate schema features alongside wordlevel features, we adopt a strategy inspired by domain adaptation techniques (Daume III, 2007; Kim et al., 2016). Daume III (2007) proposed a feature augmentation strategy for domain adaptation, which was extended to neural models by (Kim et al., 2016). It works as described: given two domains (“source” and “target”), each feature is duplicated thrice creating three versions – a general version, a source-specific version and a targetspecific version. We follow the same intuition considering each schema to be a separate domain. Hence, we duplicate each feature 5 times (a general version and 4 schema-specific versions). For example, if a document contains the word “plastic”, our feature space includes “ge"
W19-3403,N13-1104,0,0.0259443,"d the narrative cloze test aimed at predicting a missing event in the script given all other events. Chambers and Jurafsky (2009) broadened the scope of event chains by defining “narrative schemas” which model all actors involved in a set of events along with their role. These inspired several script learning approaches (Regneri et al., 2010; Balasubramanian et al., 2013). A related line of research focused on extracting “event schemas”, which store semantic roles for typical entities involved in an event. Several works proposed unsupervised methods for this task (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recent research identified a key problem with the narrative cloze test, namely that language modeling approaches perform well without learning about events (Pichotta and Mooney, 2014; Rudinger et al., 2015). This drove the establishment of a new task: the story cloze test where the goal was to select the correct ending for a story given two endings (Mostafazadeh et al., 2016a; Sharma et al., 2018). Several works showed that incorporating event sequence information provides improvement in this task (Peng et al., 2017; Chaturvedi et al., 2017b). Additional"
W19-3403,N15-1185,0,0.0277561,"f Mooney and DeJong (1985)) attempted to build models for this task due to its complexity. However, it has garnered more interest in recent years. Chambers and Jurafsky (2008) modeled scripts as narrative event chains, defined as partially ordered Character-centric approaches adopt the outlook that characters make a narrative compelling and drive the story. While no standard paradigms have been established for character representation, a common approach concentrated on learning character types or personas (Bamman et al., 2013, 2014). Other work proposed to model inter-character relationships (Krishnan and Eisenstein, 2015; Chaturvedi et al., 2016, 2017a). Information about character types and their relationships has been demonstrated to be useful for story understanding tasks such as identifying incorrect narratives (e.g., reordered or reversed stories) (Elsner, 2012) and detecting narrative similarity (Chaturvedi et al., 2018). Finally, an interesting line of research has focused on constructing “plot units”, which are story representations consisting of affect states of characters and tensions between them. Plot units were first proposed by 2 All data and code are available at https://github. com/xinru1414/R"
W19-3403,P07-1033,0,0.10837,"Missing"
W19-3403,E12-1065,0,0.0292078,"pproaches adopt the outlook that characters make a narrative compelling and drive the story. While no standard paradigms have been established for character representation, a common approach concentrated on learning character types or personas (Bamman et al., 2013, 2014). Other work proposed to model inter-character relationships (Krishnan and Eisenstein, 2015; Chaturvedi et al., 2016, 2017a). Information about character types and their relationships has been demonstrated to be useful for story understanding tasks such as identifying incorrect narratives (e.g., reordered or reversed stories) (Elsner, 2012) and detecting narrative similarity (Chaturvedi et al., 2018). Finally, an interesting line of research has focused on constructing “plot units”, which are story representations consisting of affect states of characters and tensions between them. Plot units were first proposed by 2 All data and code are available at https://github. com/xinru1414/Reddit 23 models impose structure on transitions between latent themes, typically using an HMM. This uncovers latent themes that account for interactions among themselves, helping to identify dialogue acts, which these models aim to extract. A similar"
W19-3403,D14-1220,0,0.0156779,"typical sets of functional structures observed in stories. The key difference between functional schemas and scripts is that scripts contain events present in the narrative, while functional schemas consist of phases in a story arc. For example, for a crime story, a script representation may contain a “murder” event, but a functional schema could represent that event as “inciting incident”, based on its role in the arc. Functional structures are key to rhetorical structure theory for discourse analysis (Labov, 1996; Labov and Waletzky, 1997) and have been operationalized in discourse parsing (Li et al., 2014; Xue et al., 2015). However, not much work has explored their utility in uncovering novel narrative structures. One exception is Finlayson (2012), which learned functional structures from folktales, indicating that computational techniques could recover patterns described in Propp’s theory of folktale structure (Propp, 2010). Our work differs since we aim to uncover new schemas instead of validating existing structural theories. We take this perspective because we are interested in studying stories told on social media which may not conform to existing theories of narrative structure. 2.2 3 M"
W19-3403,D10-1008,0,0.0684059,"Missing"
W19-3403,P09-1025,0,0.0439439,"nd Mooney, 2014; Rudinger et al., 2015). This drove the establishment of a new task: the story cloze test where the goal was to select the correct ending for a story given two endings (Mostafazadeh et al., 2016a; Sharma et al., 2018). Several works showed that incorporating event sequence information provides improvement in this task (Peng et al., 2017; Chaturvedi et al., 2017b). Additionally, some work has focused on defining new script annotation schemes (Mostafazadeh et al., 2016b; Wanzare et al., 2016; Modi et al., 2016) and domain-specific script-based story understanding (Mueller, 2004; McIntyre and Lapata, 2009). Background & Related Work Narrative Understanding Much prior work on narrative understanding has focused on extracting structured knowledge representations (“templates” or “schemas”) from narratives. These works can be divided into two major classes based on the narrative aspect they attend to: event-centric and character-centric. Event-centric approaches primarily focus on learning “scripts”, which are stereotypical sequences of events occurring in the narrative along with their participants (Schank and Abelson, 1977). While scripts were introduced in the 1970s, not much early work (with th"
W19-3403,D17-1232,1,0.861834,"Missing"
W19-3403,L16-1555,0,0.0154767,"that language modeling approaches perform well without learning about events (Pichotta and Mooney, 2014; Rudinger et al., 2015). This drove the establishment of a new task: the story cloze test where the goal was to select the correct ending for a story given two endings (Mostafazadeh et al., 2016a; Sharma et al., 2018). Several works showed that incorporating event sequence information provides improvement in this task (Peng et al., 2017; Chaturvedi et al., 2017b). Additionally, some work has focused on defining new script annotation schemes (Mostafazadeh et al., 2016b; Wanzare et al., 2016; Modi et al., 2016) and domain-specific script-based story understanding (Mueller, 2004; McIntyre and Lapata, 2009). Background & Related Work Narrative Understanding Much prior work on narrative understanding has focused on extracting structured knowledge representations (“templates” or “schemas”) from narratives. These works can be divided into two major classes based on the narrative aspect they attend to: event-centric and character-centric. Event-centric approaches primarily focus on learning “scripts”, which are stereotypical sequences of events occurring in the narrative along with their participants (Sch"
W19-3403,1985.tmi-1.17,0,0.115238,"Related Work Narrative Understanding Much prior work on narrative understanding has focused on extracting structured knowledge representations (“templates” or “schemas”) from narratives. These works can be divided into two major classes based on the narrative aspect they attend to: event-centric and character-centric. Event-centric approaches primarily focus on learning “scripts”, which are stereotypical sequences of events occurring in the narrative along with their participants (Schank and Abelson, 1977). While scripts were introduced in the 1970s, not much early work (with the exception of Mooney and DeJong (1985)) attempted to build models for this task due to its complexity. However, it has garnered more interest in recent years. Chambers and Jurafsky (2008) modeled scripts as narrative event chains, defined as partially ordered Character-centric approaches adopt the outlook that characters make a narrative compelling and drive the story. While no standard paradigms have been established for character representation, a common approach concentrated on learning character types or personas (Bamman et al., 2013, 2014). Other work proposed to model inter-character relationships (Krishnan and Eisenstein, 2"
W19-3403,E14-1024,0,0.0176904,"tors involved in a set of events along with their role. These inspired several script learning approaches (Regneri et al., 2010; Balasubramanian et al., 2013). A related line of research focused on extracting “event schemas”, which store semantic roles for typical entities involved in an event. Several works proposed unsupervised methods for this task (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recent research identified a key problem with the narrative cloze test, namely that language modeling approaches perform well without learning about events (Pichotta and Mooney, 2014; Rudinger et al., 2015). This drove the establishment of a new task: the story cloze test where the goal was to select the correct ending for a story given two endings (Mostafazadeh et al., 2016a; Sharma et al., 2018). Several works showed that incorporating event sequence information provides improvement in this task (Peng et al., 2017; Chaturvedi et al., 2017b). Additionally, some work has focused on defining new script annotation schemes (Mostafazadeh et al., 2016b; Wanzare et al., 2016; Modi et al., 2016) and domain-specific script-based story understanding (Mueller, 2004; McIntyre and La"
W19-3403,N16-1098,0,0.0574121,"al in forming stories and play an important role in story understanding (Brewer and Lichtenstein, 1980, 1982). We develop a novel unsupervised pipeline to extract functional schemas (§3), which consists of two stages: functional structure identification and structure grouping for schema formation. The first stage uses the Content word filtering and Introduction Narrative understanding has long been considered a central, yet challenging task in natural language understanding (Winograd, 1972). Recent advances in NLP have revived interest in this area, especially the task of story understanding (Mostafazadeh et al., 2016a). Most computational work has focused on extracting structured story representations (often called “schemas”) from literary novels, folktales, movie plots or news articles (Chambers and Jurafsky, 2009; Finlayson, 2012; Chaturvedi et al., 2018). In our work, we shift the focus to understanding the structure of stories from a different data source: narratives found on social media. Table 1 provides an example story from the popular online discussion forum Reddit 1 . Prior work has studied stories of personal experiences found on social media, identifying new storytelling patterns. However, the"
W19-3403,P10-1100,0,0.0401331,"subreddit. We hope that our conceptualization of functional story schemas provides an interesting research direction for future work on story understanding, especially stories on social media. 2 2.1 sets of events related to a single common actor, and built an evaluation called the narrative cloze test aimed at predicting a missing event in the script given all other events. Chambers and Jurafsky (2009) broadened the scope of event chains by defining “narrative schemas” which model all actors involved in a set of events along with their role. These inspired several script learning approaches (Regneri et al., 2010; Balasubramanian et al., 2013). A related line of research focused on extracting “event schemas”, which store semantic roles for typical entities involved in an event. Several works proposed unsupervised methods for this task (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recent research identified a key problem with the narrative cloze test, namely that language modeling approaches perform well without learning about events (Pichotta and Mooney, 2014; Rudinger et al., 2015). This drove the establishment of a new task: the story cloze test where the g"
W19-3403,W16-1007,0,0.0724816,"al in forming stories and play an important role in story understanding (Brewer and Lichtenstein, 1980, 1982). We develop a novel unsupervised pipeline to extract functional schemas (§3), which consists of two stages: functional structure identification and structure grouping for schema formation. The first stage uses the Content word filtering and Introduction Narrative understanding has long been considered a central, yet challenging task in natural language understanding (Winograd, 1972). Recent advances in NLP have revived interest in this area, especially the task of story understanding (Mostafazadeh et al., 2016a). Most computational work has focused on extracting structured story representations (often called “schemas”) from literary novels, folktales, movie plots or news articles (Chambers and Jurafsky, 2009; Finlayson, 2012; Chaturvedi et al., 2018). In our work, we shift the focus to understanding the structure of stories from a different data source: narratives found on social media. Table 1 provides an example story from the popular online discussion forum Reddit 1 . Prior work has studied stories of personal experiences found on social media, identifying new storytelling patterns. However, the"
W19-3403,N10-1020,0,0.0367918,"characteristic functional structures from stories. Topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), can be used for automatic induction of such structures since they identify latent themes, which may be treated as functions, from a set of documents. However, vanilla topic models do not model transitions between themes, whereas stories tend to follow stereotypical sequences of functional structures. For example, the conflict in a story must be set up before the resolution. Hence, to account for the order of functional structures, conversation models can be employed (Ritter et al., 2010; Lee et al., 2013; Ezen-Can and Boyer, 2015; Brychc´ın and Kr´al, 2017; Joty and Mohiuddin, 2018; Paul, 2012; Wallace et al., 2013; Jo et al., 2017). These 3.1 Functional Structure Identification The first step in our pipeline is to identify the typical sequences of functional structures in the corpus, which will then be clustered to form several functional schemas. Specifically, we utilize CSM to identify underlying functional structures from the corpus. CSM is a generative model originally applied to conversation – a sequence of utterances by speakers. The model assumes that a corpus of con"
W19-3403,J16-3007,1,0.77525,"Missing"
W19-3403,D15-1195,0,0.0427478,"Missing"
W19-3403,P15-1019,0,0.050684,"Missing"
W19-3403,P18-2119,0,0.0169095,"”, which store semantic roles for typical entities involved in an event. Several works proposed unsupervised methods for this task (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recent research identified a key problem with the narrative cloze test, namely that language modeling approaches perform well without learning about events (Pichotta and Mooney, 2014; Rudinger et al., 2015). This drove the establishment of a new task: the story cloze test where the goal was to select the correct ending for a story given two endings (Mostafazadeh et al., 2016a; Sharma et al., 2018). Several works showed that incorporating event sequence information provides improvement in this task (Peng et al., 2017; Chaturvedi et al., 2017b). Additionally, some work has focused on defining new script annotation schemes (Mostafazadeh et al., 2016b; Wanzare et al., 2016; Modi et al., 2016) and domain-specific script-based story understanding (Mueller, 2004; McIntyre and Lapata, 2009). Background & Related Work Narrative Understanding Much prior work on narrative understanding has focused on extracting structured knowledge representations (“templates” or “schemas”) from narratives. These"
W19-3403,D12-1009,0,0.015625,"., 2003), can be used for automatic induction of such structures since they identify latent themes, which may be treated as functions, from a set of documents. However, vanilla topic models do not model transitions between themes, whereas stories tend to follow stereotypical sequences of functional structures. For example, the conflict in a story must be set up before the resolution. Hence, to account for the order of functional structures, conversation models can be employed (Ritter et al., 2010; Lee et al., 2013; Ezen-Can and Boyer, 2015; Brychc´ın and Kr´al, 2017; Joty and Mohiuddin, 2018; Paul, 2012; Wallace et al., 2013; Jo et al., 2017). These 3.1 Functional Structure Identification The first step in our pipeline is to identify the typical sequences of functional structures in the corpus, which will then be clustered to form several functional schemas. Specifically, we utilize CSM to identify underlying functional structures from the corpus. CSM is a generative model originally applied to conversation – a sequence of utterances by speakers. The model assumes that a corpus of conversations has a set of functional structures undertaken by individual sentences. Each structure is represent"
W19-3403,D16-1108,0,0.0256209,"mas not only capture specific narrative structures existing in subreddits, More importantly, these narrative structures unique to each subreddit, as captured by functional schemas, can act as a lens and provide insight into community posting norms. This is analogous with previous work on computational sociolinguistics, where researchers have demonstrated that online discussion forums create community norms about language usage, and members adapt their language to conform to those norms (Nguyen et al., 2016). Especially on Reddit, language style is an essential indicator of community identity (Tran and Ostendorf, 2016; Chancellor et al., 2018). Our 29 but also reveal online community norms, which helps us better understand how stories function in social media. A limitation of our work is that PCA-based grouping loses information about ordering of functional structures within each schema. Moving forward, we plan to tackle this to form ordered schemas. Possible applications of our work include using extracted schemas to study evolution of community norms and changes in user compliance to these norms over time. 7 William F Brewer and Edward H Lichtenstein. 1980. Event schemas, story schemas, and story grammar"
W19-3403,K17-1019,0,0.0146862,"s task (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recent research identified a key problem with the narrative cloze test, namely that language modeling approaches perform well without learning about events (Pichotta and Mooney, 2014; Rudinger et al., 2015). This drove the establishment of a new task: the story cloze test where the goal was to select the correct ending for a story given two endings (Mostafazadeh et al., 2016a; Sharma et al., 2018). Several works showed that incorporating event sequence information provides improvement in this task (Peng et al., 2017; Chaturvedi et al., 2017b). Additionally, some work has focused on defining new script annotation schemes (Mostafazadeh et al., 2016b; Wanzare et al., 2016; Modi et al., 2016) and domain-specific script-based story understanding (Mueller, 2004; McIntyre and Lapata, 2009). Background & Related Work Narrative Understanding Much prior work on narrative understanding has focused on extracting structured knowledge representations (“templates” or “schemas”) from narratives. These works can be divided into two major classes based on the narrative aspect they attend to: event-centric and character-ce"
W19-3403,D13-1182,0,0.0262816,"n be used for automatic induction of such structures since they identify latent themes, which may be treated as functions, from a set of documents. However, vanilla topic models do not model transitions between themes, whereas stories tend to follow stereotypical sequences of functional structures. For example, the conflict in a story must be set up before the resolution. Hence, to account for the order of functional structures, conversation models can be employed (Ritter et al., 2010; Lee et al., 2013; Ezen-Can and Boyer, 2015; Brychc´ın and Kr´al, 2017; Joty and Mohiuddin, 2018; Paul, 2012; Wallace et al., 2013; Jo et al., 2017). These 3.1 Functional Structure Identification The first step in our pipeline is to identify the typical sequences of functional structures in the corpus, which will then be clustered to form several functional schemas. Specifically, we utilize CSM to identify underlying functional structures from the corpus. CSM is a generative model originally applied to conversation – a sequence of utterances by speakers. The model assumes that a corpus of conversations has a set of functional structures undertaken by individual sentences. Each structure is represented as a language model"
W19-3403,D14-1162,0,0.0821795,"... all i wanted to do was use a cup to get some coffee... Table 3: 10 functional structures extracted by CSM along with examples. These structures are more general than narrative primitives appearing in classic theoretical frameworks such as Propp’s theory, but we believe that they provide a reasonable approximation. with several feature duplication strategies, resulting in the following settings for each model: classifier For all models using bag-of-words or tf-idf features, we restrict the vocabulary to the most frequent 2, 000 words. All neural models use 300-dimensional GloVe embeddings (Pennington et al., 2014). 4.3.2 • Vanilla: Only the general domain features contain non-zero values. All schema domain features are set to zero, hence this setting contains no schema information. • AllSent: Both general and schema domains contain non-zero feature values computed using sentences from the entire document. For each document, only one schema domain (i.e. assigned schema) contains non-zero values. • SchemaSent: General domain feature values are computed using the entire document, while schema domain feature values are computed using only sentences which contain structures present in the assigned schema. S"
W19-3403,H89-1033,0,0.748965,"hich reduces domain-specificity in the found schemas. Studies have shown that functional narrative structures are critical in forming stories and play an important role in story understanding (Brewer and Lichtenstein, 1980, 1982). We develop a novel unsupervised pipeline to extract functional schemas (§3), which consists of two stages: functional structure identification and structure grouping for schema formation. The first stage uses the Content word filtering and Introduction Narrative understanding has long been considered a central, yet challenging task in natural language understanding (Winograd, 1972). Recent advances in NLP have revived interest in this area, especially the task of story understanding (Mostafazadeh et al., 2016a). Most computational work has focused on extracting structured story representations (often called “schemas”) from literary novels, folktales, movie plots or news articles (Chambers and Jurafsky, 2009; Finlayson, 2012; Chaturvedi et al., 2018). In our work, we shift the focus to understanding the structure of stories from a different data source: narratives found on social media. Table 1 provides an example story from the popular online discussion forum Reddit 1 ."
W19-3403,K15-2001,0,0.0123997,"unctional structures observed in stories. The key difference between functional schemas and scripts is that scripts contain events present in the narrative, while functional schemas consist of phases in a story arc. For example, for a crime story, a script representation may contain a “murder” event, but a functional schema could represent that event as “inciting incident”, based on its role in the arc. Functional structures are key to rhetorical structure theory for discourse analysis (Labov, 1996; Labov and Waletzky, 1997) and have been operationalized in discourse parsing (Li et al., 2014; Xue et al., 2015). However, not much work has explored their utility in uncovering novel narrative structures. One exception is Finlayson (2012), which learned functional structures from folktales, indicating that computational techniques could recover patterns described in Propp’s theory of folktale structure (Propp, 2010). Our work differs since we aim to uncover new schemas instead of validating existing structural theories. We take this perspective because we are interested in studying stories told on social media which may not conform to existing theories of narrative structure. 2.2 3 Method We use unsupe"
W19-3403,N16-1174,0,0.0115676,"emonstrates that our extracted schemas do capture typical structures present in Reddit posts and that posts in each subreddit indeed exhibit unique structures. 4.3 Schema-1 4.3.1 Baseline Models We set up the following baseline models, which use only word-level information, for text classification: • LR: A logistic regression classifier with two feature settings (bag-of-words or tf-idf) • NB: A naive bayes classifier with two feature settings (bag-of-words or tf-idf) • SVM: A support vector machine classifier with unigram bag-of-word features • BiLSTM: A bi-directional LSTM with mean-pooling (Yang et al., 2016), followed by an MLP classifier • CNN: A CNN with filter sizes 3,4,5 and maxpooling (Kim, 2014), followed by an MLP Using Schemas for Text Classification In addition to manual interpretation, we demonstrate the practical utility of our schema extraction pipeline by applying it in a downstream task: multi-label text classification. In our task setup, we treat each post as a document and the subreddit it belongs to as the document label. Since 26 Structure 0 Label Requesting help 1 Asking for feedback & thanking 2 Disclosing personal stories 3 Presenting news/statements 4 Catch-all for questions"
W19-3507,D16-1148,0,0.0168721,". Liberal Politics and T6: Far-Right/Far-Left Ideologies center around broader ideologies associated with controversial content, while T4: Censorship of Political Views/Debate, T5: Moderation/Free Speech on Social Media Platforms, and T8: Laws/Government-Level Policies discuss the legal implications of online content moderation. points may highlight different aspects within the general topics discussed here. 5 Characterizing User Participation on Reddit In order to better understand how different users highlight or frame particular aspects within each topic (Entman, 2007; Nguyen et al., 2013; Card et al., 2016), we first want to characterize the types of users who participated in the r/announcements discussion. Because subreddits on Reddit represent interest-based subcommunities, previous work has used participation across subreddits as a signal of user interests or viewpoint (Olson and Neal, 2015; Chandrasekharan et al., 2017). We follow in the lines of this work by characterizing users using their participation in subreddits prior to the announcement. In this section, we describe a graph-partitioning approach for characterizing common interests across subreddits. One notable topic in our model was"
W19-3507,N19-1304,0,0.0150372,"m people, things, talking, thing, time, men, matter, person, real, years, talk, life, made, lot, world people, society, violence, person, power, words, point, world, rights, groups, political, majority, control, argue, definition, part good, make, ca, yeah, read, back, man, money, question, side, wo, big, end, full, care Table 1: Identified topics, proportion in our dataset, and top 15 associated words. Topic names were assigned after examining both the top words and the top comments associated with each topic. Topic choice has been commonly used in NLP (Tsur et al., 2015; Field et al., 2018; Demszky et al., 2019) as a proxy for agenda-setting, the strategic highlighting of what aspects of a subject are worth discussing (McCombs, 2002). Here, we first describe our preliminary topic analysis for discovering the range of topics discussed. For the LDA models, we considered each comment to be a document. Comments were tokenized using SpaCy (Honnibal and Montani, 2017) and stopwords and punctuation-only tokens were removed. We trained models with 5, 10, 15, 20, 25, 30, 40, and 50 topics. We selected the model with 10 topics for further analysis for having the highest CV coherence, which has been shown to mo"
W19-3507,D18-1393,0,0.012644,"try, claim, socialism people, things, talking, thing, time, men, matter, person, real, years, talk, life, made, lot, world people, society, violence, person, power, words, point, world, rights, groups, political, majority, control, argue, definition, part good, make, ca, yeah, read, back, man, money, question, side, wo, big, end, full, care Table 1: Identified topics, proportion in our dataset, and top 15 associated words. Topic names were assigned after examining both the top words and the top comments associated with each topic. Topic choice has been commonly used in NLP (Tsur et al., 2015; Field et al., 2018; Demszky et al., 2019) as a proxy for agenda-setting, the strategic highlighting of what aspects of a subject are worth discussing (McCombs, 2002). Here, we first describe our preliminary topic analysis for discovering the range of topics discussed. For the LDA models, we considered each comment to be a document. Comments were tokenized using SpaCy (Honnibal and Montani, 2017) and stopwords and punctuation-only tokens were removed. We trained models with 5, 10, 15, 20, 25, 30, 40, and 50 topics. We selected the model with 10 topics for further analysis for having the highest CV coherence, whi"
W19-3507,D11-1024,0,0.0503041,"g (McCombs, 2002). Here, we first describe our preliminary topic analysis for discovering the range of topics discussed. For the LDA models, we considered each comment to be a document. Comments were tokenized using SpaCy (Honnibal and Montani, 2017) and stopwords and punctuation-only tokens were removed. We trained models with 5, 10, 15, 20, 25, 30, 40, and 50 topics. We selected the model with 10 topics for further analysis for having the highest CV coherence, which has been shown to more closely correlate with human ratings of interpretability (R¨oder et al., 2015) than semantic coherence (Mimno et al., 2011). When analyzing and interpreting the topics discovered, we examined both the highest weighted words and example comments associated with each topic. 4.1 4.2 as they are usually formulaic and unrelated to the content of our analyses (e.g. “Good bot”, complaints about bot responses), leaving us with a final announcement dataset containing 9,836 posts from 3,640 users. 4 Topical Analysis Models Results Table 1 presents the topics discovered by the model. The most prevalent topic (T0) in the discussion thread focuses on accessibility to quarantined subreddits. This is unsurprising, as this We use"
W19-3507,P15-1157,0,0.029559,"m, capitalism, country, claim, socialism people, things, talking, thing, time, men, matter, person, real, years, talk, life, made, lot, world people, society, violence, person, power, words, point, world, rights, groups, political, majority, control, argue, definition, part good, make, ca, yeah, read, back, man, money, question, side, wo, big, end, full, care Table 1: Identified topics, proportion in our dataset, and top 15 associated words. Topic names were assigned after examining both the top words and the top comments associated with each topic. Topic choice has been commonly used in NLP (Tsur et al., 2015; Field et al., 2018; Demszky et al., 2019) as a proxy for agenda-setting, the strategic highlighting of what aspects of a subject are worth discussing (McCombs, 2002). Here, we first describe our preliminary topic analysis for discovering the range of topics discussed. For the LDA models, we considered each comment to be a document. Comments were tokenized using SpaCy (Honnibal and Montani, 2017) and stopwords and punctuation-only tokens were removed. We trained models with 5, 10, 15, 20, 25, 30, 40, and 50 topics. We selected the model with 10 topics for further analysis for having the highe"
W19-5929,W06-1623,0,0.0584694,"tejovsky et al., 2003) and TimeBank (Pustejovsky et al.) marked the first attempt towards creating a corpus for temporal ordering of events. TimeML uses temporal links (TLINKs) (Setzer, 2002), to represent ordering. A TLINK expresses the temporal relation between two events. For example, an event e1 can occur before another event e2. TimeBank is annotated using TLINKs, but the number of possible TLINKs in a document is large (quadratic in number of events). So annotation is restricted to a subset of TLINKs, leading to sparsity. To combat this, several works attempted to create denser corpora (Bramsen et al., 2006; Kolomiyets et al., 2012; Do et al., 2012; Cassidy et al., 2014), but still focused largely on local TLINKs. 2.2 Prior Temporal Ordering Systems TimeBank and the TempEval tasks (Verhagen et al., 2007, 2010; UzZaman et al., 2013) spurred the development of many temporal ordering systems (UzZaman and Allen, 2010; Llorens et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012; Chambers, 2013; Bethard, 2013). More recently, TimeBank-Dense and EventTime prompted development of newer models (Chambers et al., 2014; Mirza and Tonelli, 2016; Cheng and Miyao, 2017; Reimers et al., 2018). Most"
W19-5929,S13-2012,0,0.0543387,"Missing"
W19-5929,Q14-1022,0,0.0170313,"event pair systematic, and ensure that experts do not miss important cues. The final step is guaranteed to assign a label. We choose not to allow annotators to leave event pairs unlabeled or label them “vague”, to keep them from overusing this option. Owing to this decision, we need to develop mechanisms for handling TLINLKs containing events which have not actually occurred (eg: negated, hypothetical or conditional events). Drawing from prior work, we interpret these events using a possible worlds analysis, in which the event is treated as if it has occurred. We refer interested readers to (Chambers et al., 2014) for a more detailed discussion. 4.1.2 4.1.1 Using textual cues Using world knowledge This step uses real world knowledge to determine causal/prerequisite links which are used to label a TLINK. We consider both events in the TLINK and determine whether they possess one or both of the following: • Causal Link: Two events have a causal link if the occurrence of one event results in the other event coming about. For example, in the sentence “The paper got wet when I spilled water on it”, the event pair (spilled, wet) have a causal link. • Prerequisite Link: Two events have a prerequisite link if"
W19-5929,D08-1073,0,0.105476,"Missing"
W19-5929,chang-manning-2012-sutime,0,0.0921049,"Missing"
W19-5929,P17-2001,0,0.0169533,"lunteers with no vested interest in the corpus 244 Dataset TB-Dense TDD-Man TDD-Auto a 0.18 0.13 0.28 b 0.22 0.27 0.32 s 0.02 0.03 0.16 i 0.05 0.38 0.11 ii 0.06 0.19 0.13 consists of specialized learners (sieves) which include heuristic rules and trained models. For each document, sieves run in decreasing order of precision. Decisions made by earlier sieves constrain following ones. This framework integrates transitive reasoning, but decisions made by earlier sieves cannot be overturned, causing error cascades. To extend CAEVO, we increase window sizes and remove the AllVague sieve.11 BiLSTM (Cheng and Miyao, 2017): Inspired by Xu et al. (2015), this model uses a BiLSTM classifier. For each pair, dependency paths from source and target events to the sentence root are fed to a BiLSTM. For events in adjacent sentences, source and target event sentences are assumed to be connected to a ”common root”. We follow the same framework to build a BiLSTM. SP+ILP (Ning et al., 2017): CAEVO and BiLSTM make separate local decisions for each TLINK, which may result in global inconsistency. For example, for events A, B and C, if A occurs before B and B occurs before C, transitivity implies that A occurs before C. Model"
W19-5929,S13-2002,0,0.0383376,"Missing"
W19-5929,P12-1010,0,0.0191648,"and TimeBank (Pustejovsky et al.) marked the first attempt towards creating a corpus for temporal ordering of events. TimeML uses temporal links (TLINKs) (Setzer, 2002), to represent ordering. A TLINK expresses the temporal relation between two events. For example, an event e1 can occur before another event e2. TimeBank is annotated using TLINKs, but the number of possible TLINKs in a document is large (quadratic in number of events). So annotation is restricted to a subset of TLINKs, leading to sparsity. To combat this, several works attempted to create denser corpora (Bramsen et al., 2006; Kolomiyets et al., 2012; Do et al., 2012; Cassidy et al., 2014), but still focused largely on local TLINKs. 2.2 Prior Temporal Ordering Systems TimeBank and the TempEval tasks (Verhagen et al., 2007, 2010; UzZaman et al., 2013) spurred the development of many temporal ordering systems (UzZaman and Allen, 2010; Llorens et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012; Chambers, 2013; Bethard, 2013). More recently, TimeBank-Dense and EventTime prompted development of newer models (Chambers et al., 2014; Mirza and Tonelli, 2016; Cheng and Miyao, 2017; Reimers et al., 2018). Most systems built for TimeBa"
W19-5929,S15-2134,0,0.0630529,"al features rather than document-level structure, with some exceptions. Chambers and Jurafsky (2008); Denis and Muller (2011); Ning et al. (2017) introduce documentlevel consistency via integer linear programming constraints. Bramsen et al. (2006); Do et al. (2012) also incorporate document-level structure, but focus on different corpora. Reimers et al. (2018) develop a model for EventTime, which uses a decision tree of CNNs to associate each event from a document with a time. Several works have explored techniques to incorporate document-level cues such as event coreference (Do et al., 2012; Llorens et al., 2015) and causality (Do et al., 2012; Ning et al., 2018a) in temporal ordering systems. However, due to a lack of standard datasets focusing on global discourse-level links, most work has been evaluated on datasets of their own creation or standard datasets with mainly local TLINKs. This further stresses the need for a standardized benchmarking effort, which we address by evaluating adaptations of three state-of-the-art systems on our dataset (§8). 3 Relation e1 occurs after e2 e1 occurs before e2 e1 and e2 are simultaneous e1 includes e2 e1 is included in e2 Table 1: Temporal relation set used in"
W19-5929,Q18-1006,0,0.0217942,"Missing"
W19-5929,S10-1063,0,0.0862284,"Missing"
W19-5929,W15-0812,0,0.0268726,"cues to label a TLINK. The cues used are similar to those used in previous datasets (Cassidy et al., 2014). Table 2 gives an example of the types of cues used. A key textual cue we use here is event coreference. Event coreference has not been used for annotation because the occurrence of coreferent events in adjacent sentences is rare. However, this cue is crucial for global discourse-level annotation. Since TimeBank does not contain event coreference annotation, we develop a procedure to annotate our document subset. Our procedure is based on the ERE (Entities, Relations, and Events) scheme (Song et al., 2015), which cannot be directly used for TimeBank due to differing notions of what constitutes an event and different metadata. In our procedure, events are considered coreferent iff they share the following: • Entities involved in the event • Temporal attributes • Location attributes • Realis (whether event is real or hypothetical) Events which are synonymous in context are also 6 242 Examples in the appendix Rule TLINK=(A, B), A=P TLINK=(A, B), A=I TLINK=(B, A), A=P TLINK=(B, A), A=I Label Before Includes After Is Included Dataset TimeBank TimeBank-Dense TDD-Man Table 3: Labels assigned to event"
W19-5929,C16-1265,0,0.0384721,"Missing"
W19-5929,S10-1071,0,0.193731,"Missing"
W19-5929,D17-1108,0,0.305799,"in following ones. This framework integrates transitive reasoning, but decisions made by earlier sieves cannot be overturned, causing error cascades. To extend CAEVO, we increase window sizes and remove the AllVague sieve.11 BiLSTM (Cheng and Miyao, 2017): Inspired by Xu et al. (2015), this model uses a BiLSTM classifier. For each pair, dependency paths from source and target events to the sentence root are fed to a BiLSTM. For events in adjacent sentences, source and target event sentences are assumed to be connected to a ”common root”. We follow the same framework to build a BiLSTM. SP+ILP (Ning et al., 2017): CAEVO and BiLSTM make separate local decisions for each TLINK, which may result in global inconsistency. For example, for events A, B and C, if A occurs before B and B occurs before C, transitivity implies that A occurs before C. Models classifying each pair independently may assign a different relation to A-C. To correct this, Ning et al. (2017) proposed SP+ILP, which uses a structured perceptron with ILP constraints, explicitly enforcing global consistency. This model was trained on TimeBank-Dense which contains fewer TLINKs per document, making joint learning tractable with loose transiti"
W19-5929,S10-1062,0,0.0680406,"Missing"
W19-5929,P18-1212,0,0.15679,"he temporal relation between dismembered and kidnapped is clear because the kidnapping should have happened before dismembering. Based on this, we address the drawback in EventTime, by using TLINK-based annotation, which is expensive but allows more expressive power. Following TimeML, we augment TimeBank-Dense (Cassidy et al., 2014) with global discourse-level TLINKs. To optimize manual effort, we automatically generate all TLINKs that can be inferred from EventTime. Then, we manually annotate a large subset of missing TLINKs involving events not associated with specific dates. Most recently, Ning et al. (2018b) proposed a new scheme, which labels TLINKs based only on event start time. This improved inter-annotator agreement allowing for crowdsourcing of longdistance annotations at lower cost. However, they focused only on verb events, whereas our work is broader in scope and poses no such restrictions. Related Work 2.1 Prior Work on Temporal Annotation The development of TimeML (Pustejovsky et al., 2003) and TimeBank (Pustejovsky et al.) marked the first attempt towards creating a corpus for temporal ordering of events. TimeML uses temporal links (TLINKs) (Setzer, 2002), to represent ordering. A T"
W19-5929,S13-2001,0,0.365901,"Missing"
W19-5929,P18-1122,0,0.144548,"he temporal relation between dismembered and kidnapped is clear because the kidnapping should have happened before dismembering. Based on this, we address the drawback in EventTime, by using TLINK-based annotation, which is expensive but allows more expressive power. Following TimeML, we augment TimeBank-Dense (Cassidy et al., 2014) with global discourse-level TLINKs. To optimize manual effort, we automatically generate all TLINKs that can be inferred from EventTime. Then, we manually annotate a large subset of missing TLINKs involving events not associated with specific dates. Most recently, Ning et al. (2018b) proposed a new scheme, which labels TLINKs based only on event start time. This improved inter-annotator agreement allowing for crowdsourcing of longdistance annotations at lower cost. However, they focused only on verb events, whereas our work is broader in scope and poses no such restrictions. Related Work 2.1 Prior Work on Temporal Annotation The development of TimeML (Pustejovsky et al., 2003) and TimeBank (Pustejovsky et al.) marked the first attempt towards creating a corpus for temporal ordering of events. TimeML uses temporal links (TLINKs) (Setzer, 2002), to represent ordering. A T"
W19-5929,S07-1014,0,0.468563,"Missing"
W19-5929,P16-1207,0,0.694651,"ating event coreference and causality/prerequisite links arising from world knowledge. To handle these, we design a careful coding scheme that achieves high inter-annotator agreement (Cohen’s Kappa of 0.69 on the test set). However, getting expert manual annotation for all possible long-distance event pairs is expensive. Moreover, it is possible to leverage annotations from existing datasets to automatically infer temporal relations for certain event pairs. To make optimal use of expert annotation, we develop a heuristic algorithm for automatic inference of temporal relations using EventTime (Reimers et al., 2016) and apply this Introduction Temporal ordering of events is a crucial problem in automated text analysis. Systems capable of performing this task find widespread applicability in areas such as time-aware summarization, temporal information extraction or event timeline construction. Prior work has focused extensively on creating annotated corpora for temporal ordering, some notable efforts being the development of the TimeML annotation schema (Pustejovsky et al., 2003), TimeBank (Pustejovsky et al.) and 239 Proceedings of the SIGDial 2019 Conference, pages 239–249 c Stockholm, Sweden, 11-13 Sep"
W19-5929,D15-1206,0,0.0303654,"the corpus 244 Dataset TB-Dense TDD-Man TDD-Auto a 0.18 0.13 0.28 b 0.22 0.27 0.32 s 0.02 0.03 0.16 i 0.05 0.38 0.11 ii 0.06 0.19 0.13 consists of specialized learners (sieves) which include heuristic rules and trained models. For each document, sieves run in decreasing order of precision. Decisions made by earlier sieves constrain following ones. This framework integrates transitive reasoning, but decisions made by earlier sieves cannot be overturned, causing error cascades. To extend CAEVO, we increase window sizes and remove the AllVague sieve.11 BiLSTM (Cheng and Miyao, 2017): Inspired by Xu et al. (2015), this model uses a BiLSTM classifier. For each pair, dependency paths from source and target events to the sentence root are fed to a BiLSTM. For events in adjacent sentences, source and target event sentences are assumed to be connected to a ”common root”. We follow the same framework to build a BiLSTM. SP+ILP (Ning et al., 2017): CAEVO and BiLSTM make separate local decisions for each TLINK, which may result in global inconsistency. For example, for events A, B and C, if A occurs before B and B occurs before C, transitivity implies that A occurs before C. Models classifying each pair indepe"
W94-0113,1993.iwpt-1.12,0,0.0286201,"Missing"
W94-0113,1993.iwpt-1.15,0,0.0295865,"Missing"
W94-0113,H93-1041,0,0.0272365,"Missing"
W94-0113,J83-3001,0,\N,Missing
W94-0113,1993.tmi-1.16,0,\N,Missing
W97-0303,C96-1075,1,0.623663,"ng representation hypothesis. In this paper, only the Hypothesis Formation phase is described and evaluated. Since repairs beyond those made possible by the partial parser are performed during the Combination stage, we refer to the implementation of the Combination stage as the repair module. Though a set of hypotheses are produced by during the Combination stage, in the evaluation presented in this paper, only the repair hypothesis scored by the repair module as best is returned. The ROSE approach was developed in the context of the JANUS large-scale multi-lingual machine translation system (Lavie et al., 1996; Woszcyna et al., 1993; Woszcyna et al., 1994). Currently, the JANUS system deals with the scheduling domain where two speakers a t t e m p t to schedule a meeting together over the phone. The system is composed of four language independent and domain independent modules including speech-recognition, parsing, discourse processing, and generation. The repair module described in this paper is similarly language Although Minimum Distance Parsing (MDP) offers a theoretically attractive solution to the problem of extragrammaticality, it is often computationally infeasible in large scale practical"
W97-0303,1993.iwpt-1.12,1,0.903264,"llon University Baker Hall 135F Pittsburgh, PA 15213 cprose@cs.cmu.edu Carnegie Mellon University Center for Machine Translation Pittsburgh, PA 15213 alavie~cs.cmu.edu Abstract pletely automatic portion of the ROSE 1 approach. ROSE, RObustness with Structural Evolution, repairs extragrammatical input in two phases. The first phase, Repair Hypothesis Formation, is responsible for assembling a set of hypotheses about the meaning of the ungrammatical utterance. This phase is itself divided into two stages, Partial Parsing and Combination. A restricted version of Lavie&apos;s GLR* parser (Lavie, 1995; Lavie and Tomita, 1993) is used to obtain an analysis of islands of the speaker&apos;s sentence in cases where it is not possible to obtain an analysis for the entire sentence. In the Combination stage, the fragments from the partial parse are assembled into a set of alternative meaning representation hypotheses. A genetic programming approach is used to search for different ways to combine the fragments in order to avoid requiring any hand-crafted repair rules. In ROSE&apos;s second phase, Interaction with the User, the system generates a set of queries, negotiating with the speaker in order to narrow down to a single best m"
W97-0303,H93-1041,0,0.0278991,"Missing"
W97-0303,1993.tmi-1.16,0,\N,Missing
