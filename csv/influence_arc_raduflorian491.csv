2020.acl-main.117,N19-1423,0,0.136108,"), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Koˇcisk´y et al., 2018) and H OTPOT QA. A common problem of the earlier MRC datasets is observation bias: annotators first read a paragraph and then wrote appropriate questions and answers, which, as a result, have substantial lexical overlap with the paragraph. Also, systems trained on SQuAD 1.1 could be easily fooled by the insertion of distractor sentences that should not change the answer, as shown in (Jia and Liang, 2017). Based on these considerations, SQuAD 2.0 added “unanswerable” questions. However, large pretrained language models (Devlin et al., 2019; Liu et al., 2019) were able to achieve super-human performance in less than a year on SQuAD 2.0 as well; this suggests that the evidence needed to correctly identify unanswerable questions also are present as specific patterns in the paragraphs. Recently, the NQ dataset has been introduced which overcomes the above problems and constitutes a much harder and realistic benchmark. The questions came from a commercial search engine and were asked by humans who had actual information needs. The answers were manually extracted from a Wikipedia page which the user may have selected among the search"
2020.acl-main.117,D17-1215,0,0.0214012,"on 6. 2 Related Work Recent notable datasets for Machine Reading Comprehension (henceforth, MRC) include SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Koˇcisk´y et al., 2018) and H OTPOT QA. A common problem of the earlier MRC datasets is observation bias: annotators first read a paragraph and then wrote appropriate questions and answers, which, as a result, have substantial lexical overlap with the paragraph. Also, systems trained on SQuAD 1.1 could be easily fooled by the insertion of distractor sentences that should not change the answer, as shown in (Jia and Liang, 2017). Based on these considerations, SQuAD 2.0 added “unanswerable” questions. However, large pretrained language models (Devlin et al., 2019; Liu et al., 2019) were able to achieve super-human performance in less than a year on SQuAD 2.0 as well; this suggests that the evidence needed to correctly identify unanswerable questions also are present as specific patterns in the paragraphs. Recently, the NQ dataset has been introduced which overcomes the above problems and constitutes a much harder and realistic benchmark. The questions came from a commercial search engine and were asked by humans who"
2020.acl-main.117,Q18-1023,0,0.0458961,"Missing"
2020.acl-main.117,Q19-1026,0,0.0194103,"uninstall all products including Install Manager (IM) then reinstall IM and Data Studio 4.1.2. Figure 1: Examples of questions in the TechQA dataset. We briefly review related work in Section 2; we then describe the process of collecting the data for T ECH QA in Section 3, where we detail the automatic filtering, human filtering, annotation guidelines, and annotation procedure. We present statistics of the dataset in Section 4, introduce the associated leaderboard task in Section 5 and present baseline results obtained by fine-tuning MRC systems built for Natural Questions (hence-forth, NQ) (Kwiatkowski et al., 2019) and H OTPOT QA (Yang et al., 2018) in Section 6. 2 Related Work Recent notable datasets for Machine Reading Comprehension (henceforth, MRC) include SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Koˇcisk´y et al., 2018) and H OTPOT QA. A common problem of the earlier MRC datasets is observation bias: annotators first read a paragraph and then wrote appropriate questions and answers, which, as a result, have substantial lexical overlap with the paragraph. Also, systems trained on SQuAD 1.1 could be easily fooled by the insertion of distractor sentences that"
2020.acl-main.117,2021.ccl-1.108,0,0.135289,"Missing"
2020.acl-main.117,P18-2124,0,0.105149,"Missing"
2020.acl-main.117,D16-1264,0,0.100497,"Section 2; we then describe the process of collecting the data for T ECH QA in Section 3, where we detail the automatic filtering, human filtering, annotation guidelines, and annotation procedure. We present statistics of the dataset in Section 4, introduce the associated leaderboard task in Section 5 and present baseline results obtained by fine-tuning MRC systems built for Natural Questions (hence-forth, NQ) (Kwiatkowski et al., 2019) and H OTPOT QA (Yang et al., 2018) in Section 6. 2 Related Work Recent notable datasets for Machine Reading Comprehension (henceforth, MRC) include SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Koˇcisk´y et al., 2018) and H OTPOT QA. A common problem of the earlier MRC datasets is observation bias: annotators first read a paragraph and then wrote appropriate questions and answers, which, as a result, have substantial lexical overlap with the paragraph. Also, systems trained on SQuAD 1.1 could be easily fooled by the insertion of distractor sentences that should not change the answer, as shown in (Jia and Liang, 2017). Based on these considerations, SQuAD 2.0 added “unanswerable” questions. However, large pretrained language models (D"
2020.acl-main.117,K17-1029,0,0.0179664,"rum by technical users who 1270 Questions Total retrieved With Accepted Answers With link to Technote in Accepted Answer Count 276,968 57,990 15,918 Table 1: Statistics of questions from the forums. The questions with a Technote link in the Accepted Answer were manually annotated by our annotators. had a specific information need, and answers from technical documents mentioned in the ”Accepted Answer” to the post. In Section 4 we will contrast structural properties of T ECH QA to those of some of the datasets mentioned here. Datasets for specialized domain require effective domain adaptation (Wiese et al., 2017), because they contain a much smaller number of labeled examples than open-domain datasets like (Bajaj et al., 2016). Having a limited number of quality labeled examples is a real-world situation: domain experts are much more expensive than crowd-sourcing participants. 3 T ECH QA Dataset Collection The questions for the T ECH QA dataset were posed by real users on public forums maintained and hosted by IBM at the developer.ibm.com answers2 and IBM developerworks3 sites. The questions are related to products running in environments supported by IBM and mostly fall into three categories: i) gene"
2020.acl-main.117,D18-1259,0,0.0667405,"Missing"
2020.acl-main.167,P19-1620,0,0.0402586,"time, we provide the AMR as context as in conventional conditional text generation: w ˆj = arg max{pGPT-2 (wj |w1:j−1 , a1:M )} wj 3 Re-scoring via Cycle Consistency The general idea of cycle consistency is to assess the quality of a system’s output based on how well an external ‘reverse’ system can reconstruct the input from it. In previous works, cycle-consistency based losses have been used as part of the training objective in machine translation (He et al., 2016) and speech recognition (Hori et al., 2019). It has also been used for filtering synthetic training data for question answering (Alberti et al., 2019). Here we propose the use of a cycle consistency measure to re-score the system outputs. In particular, we take the top k sentences generated by our system from each gold AMR graph and parse them using an off-the-shelf parser to obtain a second AMR graph. We then re-score each sentence using the standard AMR parsing metric Smatch (Cai and Knight, 2013) by comparing the gold and parsed AMRs. 4 Experimental setup Following Previous works on AMR-to-text, we Use the standard LDC2017T10 AMR corpus for evaluation of the proposed model. This Corpus contains 36,521 training instances of AMR graphs in"
2020.acl-main.167,W13-2322,0,0.0845262,"graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Grap"
2020.acl-main.167,W05-0909,0,0.0296777,"ent with greedy decoding, beam search, and nucleus sampling (Holtzman et al., 2019). For beam search, we explore beam sizes of 5, 10 and 15. As the system, in some cases, produces repetitive output at the end of the text, we additionally perform a post-processing step to remove these occurrences. Metrics. We considered the three automatic evaluation metrics commonly used in previous works. We compute BLEU (Papineni et al., 2002) using SacreBLEU (Ma et al., 2019). We compute chrF++ (Popovi´c, 2017) using both SacreBLEU and the scripts used by authors of the baseline systems. We compute METEOR (Banerjee and Lavie, 2005) with the default values for English of the CMU implementation.2 In addition to the standard automatic metrics, we also carry out human evaluation experiments and use the semantic similarity metric BERTScore (Zhang et al., 2020). Both metrics arguably have less dependency on the surface symbols of the reference text used for evaluation. This is particularly relevant for the AMR-to-text task, since one single AMR graph corresponds to multiple sentences with the same semantic meaning. Conventional metrics for AMR-to-text are are strongly influenced by surface symbols and thus do not capture well"
2020.acl-main.167,P18-1026,0,0.134477,"our approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying"
2020.acl-main.167,2020.acl-main.640,0,0.0938116,"Missing"
2020.acl-main.167,P13-2131,0,0.0643467,"le-consistency based losses have been used as part of the training objective in machine translation (He et al., 2016) and speech recognition (Hori et al., 2019). It has also been used for filtering synthetic training data for question answering (Alberti et al., 2019). Here we propose the use of a cycle consistency measure to re-score the system outputs. In particular, we take the top k sentences generated by our system from each gold AMR graph and parse them using an off-the-shelf parser to obtain a second AMR graph. We then re-score each sentence using the standard AMR parsing metric Smatch (Cai and Knight, 2013) by comparing the gold and parsed AMRs. 4 Experimental setup Following Previous works on AMR-to-text, we Use the standard LDC2017T10 AMR corpus for evaluation of the proposed model. This Corpus contains 36,521 training instances of AMR graphs in PENMAN notation and the corresponding texts. It also includes 1368 and 1371 development and test instances, respectively. We tokenize each input text using The JAMR toolkit (Flanigan et al., 2014). The concatenation of an AMR graph and the corresponding text is split into words, special symbols and sub-word units using the GPT-2 tokenizer. We add all a"
2020.acl-main.167,N19-1223,0,0.148268,"n evaluation experiments that further substantiate the strength of our approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (20"
2020.acl-main.167,N19-1366,0,0.129568,"ted, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al."
2020.acl-main.167,N19-1423,0,0.0327542,"se graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism. Pre-trained transformer representations (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019) use transfer learning to yield powerful language models that considerably outperform the prior art. They have also shown great success when fine-tuned to particular text generation tasks (See et al., 2019; Zhang et al., 2019; Keskar et al., 2019). Given their success, it would be desirable to apply pre-trained transformer models to a graph-to-text task like AMR-to-text, but the need for graph encoding precludes in principle that option. Feeding the network with some sequential representation of the graph, such as a topological sorting, looses some of the graphs represen"
2020.acl-main.167,N16-1087,0,0.0518772,"orm all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research"
2020.acl-main.167,P14-1134,0,0.165504,"graph and parse them using an off-the-shelf parser to obtain a second AMR graph. We then re-score each sentence using the standard AMR parsing metric Smatch (Cai and Knight, 2013) by comparing the gold and parsed AMRs. 4 Experimental setup Following Previous works on AMR-to-text, we Use the standard LDC2017T10 AMR corpus for evaluation of the proposed model. This Corpus contains 36,521 training instances of AMR graphs in PENMAN notation and the corresponding texts. It also includes 1368 and 1371 development and test instances, respectively. We tokenize each input text using The JAMR toolkit (Flanigan et al., 2014). The concatenation of an AMR graph and the corresponding text is split into words, special symbols and sub-word units using the GPT-2 tokenizer. We add all arc labels seen in the training set and the root node :root to the vocabulary of the GPT-2model, but we freeze the embedding layer for training. We use the Hugging Face implementation of (Wolf et al., 2019) for GPT-2 small (GPT-2S), medium (GPT-2M) and large (GPT-2L). Fine-tuning converges after 6 epochs, which takes just a few hours on a V100 GPU1 . For cycle-consistency re-scoring we use an implementation of Naseem et al. (2019) in PyTor"
2020.acl-main.167,Q19-1019,0,0.103533,"at to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism. Pre-trained transformer represe"
2020.acl-main.167,P17-1014,0,0.156544,"etrics, we provide human evaluation experiments that further substantiate the strength of our approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh,"
2020.acl-main.167,W19-5302,0,0.0214036,":ARG1 it :manner vigorous Penman (r / recommend-01 :ARG1 (a / advocate-01 :ARG1 (i / it) :manner (v / vigorous))) Decoding. For generation, we experiment with greedy decoding, beam search, and nucleus sampling (Holtzman et al., 2019). For beam search, we explore beam sizes of 5, 10 and 15. As the system, in some cases, produces repetitive output at the end of the text, we additionally perform a post-processing step to remove these occurrences. Metrics. We considered the three automatic evaluation metrics commonly used in previous works. We compute BLEU (Papineni et al., 2002) using SacreBLEU (Ma et al., 2019). We compute chrF++ (Popovi´c, 2017) using both SacreBLEU and the scripts used by authors of the baseline systems. We compute METEOR (Banerjee and Lavie, 2005) with the default values for English of the CMU implementation.2 In addition to the standard automatic metrics, we also carry out human evaluation experiments and use the semantic similarity metric BERTScore (Zhang et al., 2020). Both metrics arguably have less dependency on the surface symbols of the reference text used for evaluation. This is particularly relevant for the AMR-to-text task, since one single AMR graph corresponds to mult"
2020.acl-main.167,P19-1451,1,0.829397,"olkit (Flanigan et al., 2014). The concatenation of an AMR graph and the corresponding text is split into words, special symbols and sub-word units using the GPT-2 tokenizer. We add all arc labels seen in the training set and the root node :root to the vocabulary of the GPT-2model, but we freeze the embedding layer for training. We use the Hugging Face implementation of (Wolf et al., 2019) for GPT-2 small (GPT-2S), medium (GPT-2M) and large (GPT-2L). Fine-tuning converges after 6 epochs, which takes just a few hours on a V100 GPU1 . For cycle-consistency re-scoring we use an implementation of Naseem et al. (2019) in PyTorch. For re-scoring experiments, we use a beam size of 15. AMR input representation. we test three variants of AMR representation. First, a depth-first search (DFS) through the graph following Konstas et al. (2017), where the input sequence is the path followed in the graph. Second, to see if GPT-2 is in fact learning from the graph structure, we remove all the edges from the DFS, keeping only the concept nodes. This has the effect of removing the relation information between concepts, such as subject/object relations. As a third option, we use the PENMAN representation without any mod"
2020.acl-main.167,P02-1040,1,0.124304,"igorous DFS recommend :ARG1 advocate-01 :ARG1 it :manner vigorous Penman (r / recommend-01 :ARG1 (a / advocate-01 :ARG1 (i / it) :manner (v / vigorous))) Decoding. For generation, we experiment with greedy decoding, beam search, and nucleus sampling (Holtzman et al., 2019). For beam search, we explore beam sizes of 5, 10 and 15. As the system, in some cases, produces repetitive output at the end of the text, we additionally perform a post-processing step to remove these occurrences. Metrics. We considered the three automatic evaluation metrics commonly used in previous works. We compute BLEU (Papineni et al., 2002) using SacreBLEU (Ma et al., 2019). We compute chrF++ (Popovi´c, 2017) using both SacreBLEU and the scripts used by authors of the baseline systems. We compute METEOR (Banerjee and Lavie, 2005) with the default values for English of the CMU implementation.2 In addition to the standard automatic metrics, we also carry out human evaluation experiments and use the semantic similarity metric BERTScore (Zhang et al., 2020). Both metrics arguably have less dependency on the surface symbols of the reference text used for evaluation. This is particularly relevant for the AMR-to-text task, since one si"
2020.acl-main.167,W17-4770,0,0.0426759,"Missing"
2020.acl-main.167,W16-6603,0,0.201065,"set, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transfor"
2020.acl-main.167,D19-1548,0,0.705729,"d Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism. Pre-trained transformer representations (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019) use transfer learning to yield powerful language models that considerably outperform the prior art. They have also shown great success when fine-tuned to particular text generation tasks (See et al., 2019; Zhang et al., 2019; Keskar et al., 2019). Given th"
2020.acl-main.167,D19-1314,0,0.229439,"an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism. Pre-trained transformer representations (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019) use transfe"
2020.acl-main.167,K19-1079,0,0.131093,"pplied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism. Pre-trained transformer representations (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019) use transfer learning to yield powerful language models that considerably outperform the prior art. They have also shown great success when fine-tuned to particular text generation tasks (See et al., 2019; Zhang et al., 2019; Keskar et al., 2019). Given their success, it would be desirable to apply pre-trained transformer models to a graph-to-text task like AMR-to-text, but the need for graph encoding precludes in principle that option. Feeding the network with some sequential representation of the graph, such as a topological sorting, looses some of the graphs representational power. Complex graph annotations, such as AMR, also contain many special symbols and special constructs that departure from natural language and may by not interpretable by a pretrained language model. In this paper we"
2020.acl-main.167,P18-1150,0,0.0585758,"on (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using t"
2020.acl-main.167,2020.tacl-1.2,0,0.103704,"ohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism. Pre-trained transformer representations (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019) use transfer learning to yield powerful language models that considerably outperform the prior art. They have also shown great success when fine-tuned to particular text generation tasks (See et al., 2019; Zhang et al., 2019; Keskar et al., 2019). Given their success, it would be desirable to apply pre-trained transformer models to a graph-to-text task like AMR-to-text, but the need for graph encoding precludes in principle that option."
2020.coling-demos.8,D19-3006,1,0.588997,"C) demo which is able to answer questions in over 100 languages. M-GAAMA answers questions from a given passage in the same or a different language. It incorporates several existing multilingual models that can be used interchangeably in the demo such as M-BERT and XLM-R. The M-GAAMA demo also improves language accessibility by incorporating the IBM Watson machine translation widget to provide additional capabilities to the user to see an answer in their desired language. We also show how M-GAAMA can be used in downstream tasks by incorporating it into an E ND - TO -E ND -QA system using CFO (Chakravarti et al., 2019). We experiment with our system architecture on the Multi-Lingual Question Answering (MLQA) and the CORD-19 COVID (Wang et al., 2020; Tang et al., 2020) datasets to provide insights into the performance of the system. 1 Introduction Recent advances in open domain question answering (QA) have mostly revolved around machine reading comprehension (MRC) (Rajpurkar et al., 2018; Yang et al., 2018). The MRC task is to read and comprehend a given text and then answer questions based on it. Our monolingual MRC approach (Pan et al., 2019) has the capability of being applied to train many Language Model"
2020.coling-demos.8,2020.tacl-1.30,0,0.0177366,"ncluding question answering (Lewis et al., 2019) (Conneau et al., 2019). We train our underlying MRC system with these pre-trained language models and achieve results that are consistently as strong as prior work. Many datasets for English MRC have been introduced with annotated Wikipedia documents including (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Yang et al., 2018; Kwiatkowski et al., 2019). Fewer resources are available for the cross-lingual setting. The MLQA (Lewis et al., 2019) dataset contains parallel instances in 7 languages where the context is found in Wikipedia. The TyDiQA (Clark et al., 2020) dataset containes instances in 11 languages. However, TyDiQA is not parallel and it only has instances where the question and context are in the same language. 42 Quelle est la période d’incubation du virus? What is the incubation period of the virus? For an individual case with exposure lying between 1 E and 2 E , the likelihood function for an incubation observation was 12 (
Commission of China, reporting an incubation time -------------------------------. Statistical estimation of the distribution of ??????? 1 ??? 14 ???? s incubation periods has been done in two other studies. ENGLISH Pa"
2020.coling-demos.8,P19-4007,0,0.0279245,"Missing"
2020.coling-demos.8,N19-1423,0,0.491563,"ur system architecture on the Multi-Lingual Question Answering (MLQA) and the CORD-19 COVID (Wang et al., 2020; Tang et al., 2020) datasets to provide insights into the performance of the system. 1 Introduction Recent advances in open domain question answering (QA) have mostly revolved around machine reading comprehension (MRC) (Rajpurkar et al., 2018; Yang et al., 2018). The MRC task is to read and comprehend a given text and then answer questions based on it. Our monolingual MRC approach (Pan et al., 2019) has the capability of being applied to train many Language Models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). We achieve the 2nd rank1 on the Google Natural Questions (Kwiatkowski et al., 2019) leaderboard2 . In this paper, we expand our approach by introducing new multilingual capabilities using models such as Multilingual-BERT (M-BERT) (Devlin et al., 2019) and XLM-R (Conneau et al., 2019). This addition has the capability of transcending language boundaries to 104 languages. Figure 1 shows examples of QA pairs from the MLQA dataset (Lewis et al., 2019). To the best of our knowledge, this is the first published demo of a Multi-Lingual QA system. We achieve this by in"
2020.coling-demos.8,Q19-1026,0,0.0507249,"Tang et al., 2020) datasets to provide insights into the performance of the system. 1 Introduction Recent advances in open domain question answering (QA) have mostly revolved around machine reading comprehension (MRC) (Rajpurkar et al., 2018; Yang et al., 2018). The MRC task is to read and comprehend a given text and then answer questions based on it. Our monolingual MRC approach (Pan et al., 2019) has the capability of being applied to train many Language Models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). We achieve the 2nd rank1 on the Google Natural Questions (Kwiatkowski et al., 2019) leaderboard2 . In this paper, we expand our approach by introducing new multilingual capabilities using models such as Multilingual-BERT (M-BERT) (Devlin et al., 2019) and XLM-R (Conneau et al., 2019). This addition has the capability of transcending language boundaries to 104 languages. Figure 1 shows examples of QA pairs from the MLQA dataset (Lewis et al., 2019). To the best of our knowledge, this is the first published demo of a Multi-Lingual QA system. We achieve this by introducing a novel multilingual component to our QA GAAMA (Go Ahead, Ask Me Anything) (Chakravarti et al., 2019) pipe"
2020.coling-demos.8,P12-3005,0,0.0336218,"omponents together using the ReactJS framework5 . Providing M-GAAMA as a gRPC server allows it to be quite flexible. This enables it to seamlessly transition between being a standalone system and integrating with larger systems. We show this via the downstream E ND - TO -E ND -QA task described below. E ND - TO -E ND -QA builds upon M-GAAMA, with a full IR-MRC pipeline. Information Retrieval is obtained using an Elasticsearch index6 for each language7 . The user can ask a question in any language for which an index exists. The language of the question is identified using the ‘langid’ toolkit (Lui and Baldwin, 2012) to determine the appropriate index. The appropriate index is then searched for documents in the target language. These documents are then evaluated together with the user’s question 4 https://www.ibm.com/watson/services/language-translator/ https://reactjs.org/ 6 https://hub.docker.com/_/elasticsearch/ 7 In our implementation we built an index in English and Spanish as a proof of concept. 5 43 F1 ROBERTAL M-BERT XLM-RB XLM-RL en 84.4 80.4 80.1 83.9 es 66.7 67.6 74.0 de 61.3 63.0 69.9 ar 51.9 56.3 66.3 MLQA hi vi 50.7 61.6 61.1 66.2 71.2 74.0 zh 60.2 61.6 69.9 XLT 61.8 65.1 72.7 G-XLT 52.1 41."
2020.coling-demos.8,D16-1264,0,0.501214,"er (Vaswani et al., 2017) based masked language model on text in multiple languages. The use of pretrained multilingual language models such as M-BERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019), and XLM-R (Conneau et al., 2019) achieve the previous SOTA on cross-lingual tasks including question answering (Lewis et al., 2019) (Conneau et al., 2019). We train our underlying MRC system with these pre-trained language models and achieve results that are consistently as strong as prior work. Many datasets for English MRC have been introduced with annotated Wikipedia documents including (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Yang et al., 2018; Kwiatkowski et al., 2019). Fewer resources are available for the cross-lingual setting. The MLQA (Lewis et al., 2019) dataset contains parallel instances in 7 languages where the context is found in Wikipedia. The TyDiQA (Clark et al., 2020) dataset containes instances in 11 languages. However, TyDiQA is not parallel and it only has instances where the question and context are in the same language. 42 Quelle est la période d’incubation du virus? What is the incubation period of the virus? For an individual case with exposure lying between 1 E and 2"
2020.coling-demos.8,P18-2124,0,0.173969,"widget to provide additional capabilities to the user to see an answer in their desired language. We also show how M-GAAMA can be used in downstream tasks by incorporating it into an E ND - TO -E ND -QA system using CFO (Chakravarti et al., 2019). We experiment with our system architecture on the Multi-Lingual Question Answering (MLQA) and the CORD-19 COVID (Wang et al., 2020; Tang et al., 2020) datasets to provide insights into the performance of the system. 1 Introduction Recent advances in open domain question answering (QA) have mostly revolved around machine reading comprehension (MRC) (Rajpurkar et al., 2018; Yang et al., 2018). The MRC task is to read and comprehend a given text and then answer questions based on it. Our monolingual MRC approach (Pan et al., 2019) has the capability of being applied to train many Language Models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). We achieve the 2nd rank1 on the Google Natural Questions (Kwiatkowski et al., 2019) leaderboard2 . In this paper, we expand our approach by introducing new multilingual capabilities using models such as Multilingual-BERT (M-BERT) (Devlin et al., 2019) and XLM-R (Conneau et al., 2019). This addition"
2020.coling-demos.8,2020.nlpcovid19-acl.1,0,0.0254579,"Missing"
2020.coling-demos.8,D18-1259,0,0.135037,"ional capabilities to the user to see an answer in their desired language. We also show how M-GAAMA can be used in downstream tasks by incorporating it into an E ND - TO -E ND -QA system using CFO (Chakravarti et al., 2019). We experiment with our system architecture on the Multi-Lingual Question Answering (MLQA) and the CORD-19 COVID (Wang et al., 2020; Tang et al., 2020) datasets to provide insights into the performance of the system. 1 Introduction Recent advances in open domain question answering (QA) have mostly revolved around machine reading comprehension (MRC) (Rajpurkar et al., 2018; Yang et al., 2018). The MRC task is to read and comprehend a given text and then answer questions based on it. Our monolingual MRC approach (Pan et al., 2019) has the capability of being applied to train many Language Models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). We achieve the 2nd rank1 on the Google Natural Questions (Kwiatkowski et al., 2019) leaderboard2 . In this paper, we expand our approach by introducing new multilingual capabilities using models such as Multilingual-BERT (M-BERT) (Devlin et al., 2019) and XLM-R (Conneau et al., 2019). This addition has the capability o"
2020.coling-demos.8,N19-4013,0,0.0210743,"he question was originally asked. It incorporates several multilingual components including multilingual LMs, machine translation, and indexed corpora in multiple languages. The rest of the paper is organized as follows: We first discuss related work, then talk about the data used in our experiments and models. Sections 4 and 5 discuss the demo system and model architecture. Finally, we discuss the Model and Runtime Experiments on MLQA (Lewis et al., 2019) and the COVID-19 CORD-19 dataset (Wang et al., 2020; Tang et al., 2020) in Section 6. 2 Related Work Few other QA demos exist; BERTSerini (Yang et al., 2019), leverages the Anserini IR toolkit (Yang et al., 2017) to extract relevant documents given a question, then uses BERT-based techniques (Devlin et al., 2019) to extract the correct answer. However, their demo is designed to perform only mono-lingual English QA. The GAAMA and CFO (Chakravarti et al., 2019) demos also only performs English QA. In contrast, M-GAAMA and our downstream E ND - TO -E ND -QA task perform cross-lingual QA. Several cross lingual large scale representations have been created by training a large scale transformer (Vaswani et al., 2017) based masked language model on text"
2020.coling-industry.16,D14-1082,0,0.020091,"improved for these languages due to both the abundance and specialization of their monolingual data. Multilingual Transformer models, such as Multilingual-BERT (M-BERT) are trained on more than 100 languages. When using M-BERT, both monolingual as well as multilingual treebanks can be used to train the parser. Low resource languages can particularly benefit from cross-lingual transfer learning. 2.3 Fast Production Parser Our production parser should meet rigid criteria regarding runtime speed; thus, we choose the arc-eager algorithm (Nivre, 2004) trained with features similar to those used by Chen and Manning (2014). To maintain UD compatability for existing downstream tasks, the tokenization and PoS tagging should not be modified even if they do not completely follow the definitions from UD. As shown in Figure 2, the dependency parser takes the tokenizer and PoS tagger’s results as input in order to produce UD-based syntactic structures. 174 UD Parser Transformer Model SNLP TBAP BERT-base-en BERT-large-en en ewt Multilingual-BERT Albert-xxlarge Roberta-large XLM SNLP de gsd TBAP BERT-base-de Multilingual-BERT LAS 89.50 91.36 92.38 91.14 92.12 91.02 91.56 86.16 87.92 87.35 UD nl alpino fr gsd it isdt pt"
2020.coling-industry.16,D19-1279,0,0.023425,"Mikolov, 2014; Devlin et al., 2018), were trained on massive amounts of unlabeled multilingual text, greatly enabling transfer learning opportunities for NLP tasks. Particularly, models such as BERT (Devlin et al., 2018), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019) and XLM (Lample and Conneau, 2019) employ a masked language modeling objective (Taylor, 1953) on a bidirectional self-attention encoder (Vaswani et al., 2017) enabling such models to utilize both left and right context for each word representation. Pretrained multilingual BERT (M-BERT) was used for dependency parsing in (Kondratyuk and Straka, 2019) aiming to create a single multilingual model. This work, in contrast, shows that parsing performance for a particular language can considerably be improved when adapting the biaffine-attention parser (Qi et al., 2018) with a selected set of pretrained Transformer models while training on multilingual subsets of selected language family treebanks. We then use this novel parser to project synthetic treebanks, which are used in a teacher-student technique to improve the accuracy of a fast production parser. Our approach can generally be described as a form of model compression which was introduc"
2020.coling-industry.16,D17-1159,0,0.0123926,"mprove production-grade parsers. The synthetic treebanks are generated using a state-of-the-art biaffine parser adapted with pretrained Transformer models, such as Multilingual BERT (M-BERT). The new parser improves LAS by up to two points on seven languages. The production models’ LAS performance improves as the augmented treebanks scale in size, surpassing performance of production models trained on originally annotated UD treebanks. 1 Introduction Dependency parsers are important components in many NLP systems, such as language understanding, semantic role labeling and relation extraction (Marcheggiani and Titov, 2017; Zhang et al., 2018). Universal Dependencies (UD) (Nivre et al., 2020; Zeman et al., 2018) are becoming a widely accepted standard among many NLP practitioners for definition of syntactic structures and treebanks. However, production parsers require custom tokenization policies and Part of Speech (PoS) tagging, mostly dictated by supported downstream applications. In addition, parsers in production environments require fine balancing of demands for model accuracy, service performance, response time and constraints on hardware resources, making the design of an industrial-grade parser a challe"
2020.coling-industry.16,2020.lrec-1.497,0,0.044784,"Missing"
2020.coling-industry.16,W04-0308,0,0.0718599,"Japanese as well as others. Performance can particularly be improved for these languages due to both the abundance and specialization of their monolingual data. Multilingual Transformer models, such as Multilingual-BERT (M-BERT) are trained on more than 100 languages. When using M-BERT, both monolingual as well as multilingual treebanks can be used to train the parser. Low resource languages can particularly benefit from cross-lingual transfer learning. 2.3 Fast Production Parser Our production parser should meet rigid criteria regarding runtime speed; thus, we choose the arc-eager algorithm (Nivre, 2004) trained with features similar to those used by Chen and Manning (2014). To maintain UD compatability for existing downstream tasks, the tokenization and PoS tagging should not be modified even if they do not completely follow the definitions from UD. As shown in Figure 2, the dependency parser takes the tokenizer and PoS tagger’s results as input in order to produce UD-based syntactic structures. 174 UD Parser Transformer Model SNLP TBAP BERT-base-en BERT-large-en en ewt Multilingual-BERT Albert-xxlarge Roberta-large XLM SNLP de gsd TBAP BERT-base-de Multilingual-BERT LAS 89.50 91.36 92.38 91"
2020.coling-industry.16,D14-1162,0,0.0916441,"tion environments require fine balancing of demands for model accuracy, service performance, response time and constraints on hardware resources, making the design of an industrial-grade parser a challenge. Hereby, we introduce data augmentation techniques to improve production parsers without violating their architectural constraints. Since their early inception, advances in language representation modeling lead to major improvements in many NLP tasks (Wang et al., 2018; Moon et al., 2019). Representations trained on various language modeling objectives, ranging from context free embeddings (Pennington et al., 2014; Mikolov et al., 2013), to deep context aware representations (Peters et al., 2018; Le and Mikolov, 2014; Devlin et al., 2018), were trained on massive amounts of unlabeled multilingual text, greatly enabling transfer learning opportunities for NLP tasks. Particularly, models such as BERT (Devlin et al., 2018), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019) and XLM (Lample and Conneau, 2019) employ a masked language modeling objective (Taylor, 1953) on a bidirectional self-attention encoder (Vaswani et al., 2017) enabling such models to utilize both left and right context for each word"
2020.coling-industry.16,N18-1202,0,0.00874482,"ance, response time and constraints on hardware resources, making the design of an industrial-grade parser a challenge. Hereby, we introduce data augmentation techniques to improve production parsers without violating their architectural constraints. Since their early inception, advances in language representation modeling lead to major improvements in many NLP tasks (Wang et al., 2018; Moon et al., 2019). Representations trained on various language modeling objectives, ranging from context free embeddings (Pennington et al., 2014; Mikolov et al., 2013), to deep context aware representations (Peters et al., 2018; Le and Mikolov, 2014; Devlin et al., 2018), were trained on massive amounts of unlabeled multilingual text, greatly enabling transfer learning opportunities for NLP tasks. Particularly, models such as BERT (Devlin et al., 2018), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019) and XLM (Lample and Conneau, 2019) employ a masked language modeling objective (Taylor, 1953) on a bidirectional self-attention encoder (Vaswani et al., 2017) enabling such models to utilize both left and right context for each word representation. Pretrained multilingual BERT (M-BERT) was used for dependency pars"
2020.coling-industry.16,K18-2016,0,0.120616,"(Lan et al., 2020), RoBERTa (Liu et al., 2019) and XLM (Lample and Conneau, 2019) employ a masked language modeling objective (Taylor, 1953) on a bidirectional self-attention encoder (Vaswani et al., 2017) enabling such models to utilize both left and right context for each word representation. Pretrained multilingual BERT (M-BERT) was used for dependency parsing in (Kondratyuk and Straka, 2019) aiming to create a single multilingual model. This work, in contrast, shows that parsing performance for a particular language can considerably be improved when adapting the biaffine-attention parser (Qi et al., 2018) with a selected set of pretrained Transformer models while training on multilingual subsets of selected language family treebanks. We then use this novel parser to project synthetic treebanks, which are used in a teacher-student technique to improve the accuracy of a fast production parser. Our approach can generally be described as a form of model compression which was introduced by (Bucilu et al., 2006), and later reformulated and generalized as neural network knowledge distillation by ∗ Work done during AI Residency at IBM Research. This work is licensed under a Creative Commons Attributio"
2020.coling-industry.16,P16-1162,0,0.006315,"data augmentation process as well as the new parser architecture. 2.1 Transformer Enhanced Biaffine-Attention Parser Figure 1 shows the architecture of the Transformer enhanced Biaffine-Attention Parser (TBAP). The Transformer provides contextualized word representations for each input sentence to the BiLSTM layer of the biaffine parser. First, a tokenized input sentence is passed through the Transformer. The Transformer further breaks word tokens into subword tokens. This is done in order to significantly reduce the size of the fixed vocabulary representation in the output prediction layer (Sennrich et al., 2016) overcom173 Figure 2: The data augumentation process for training a production parser. ing the open-vocabulary problem. We then take the sum of the last four encoder layers of the Transformer as the output representation, which is comprised of the contextualized subword representations of the input sentence. Afterwords, two operations are performed on the Transformer’s output. Subword token representations (also referred to as WordPiece tokens for BERT) are merged back into word-based representations. Merging the subword representations can either be done by averaging, maxpooling, or simply ta"
2020.coling-industry.16,W18-5446,0,0.0113626,"m tokenization policies and Part of Speech (PoS) tagging, mostly dictated by supported downstream applications. In addition, parsers in production environments require fine balancing of demands for model accuracy, service performance, response time and constraints on hardware resources, making the design of an industrial-grade parser a challenge. Hereby, we introduce data augmentation techniques to improve production parsers without violating their architectural constraints. Since their early inception, advances in language representation modeling lead to major improvements in many NLP tasks (Wang et al., 2018; Moon et al., 2019). Representations trained on various language modeling objectives, ranging from context free embeddings (Pennington et al., 2014; Mikolov et al., 2013), to deep context aware representations (Peters et al., 2018; Le and Mikolov, 2014; Devlin et al., 2018), were trained on massive amounts of unlabeled multilingual text, greatly enabling transfer learning opportunities for NLP tasks. Particularly, models such as BERT (Devlin et al., 2018), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019) and XLM (Lample and Conneau, 2019) employ a masked language modeling objective (Tayl"
2020.coling-industry.16,K18-2001,0,0.0287212,"Missing"
2020.coling-industry.16,D18-1244,0,0.0124701,"s. The synthetic treebanks are generated using a state-of-the-art biaffine parser adapted with pretrained Transformer models, such as Multilingual BERT (M-BERT). The new parser improves LAS by up to two points on seven languages. The production models’ LAS performance improves as the augmented treebanks scale in size, surpassing performance of production models trained on originally annotated UD treebanks. 1 Introduction Dependency parsers are important components in many NLP systems, such as language understanding, semantic role labeling and relation extraction (Marcheggiani and Titov, 2017; Zhang et al., 2018). Universal Dependencies (UD) (Nivre et al., 2020; Zeman et al., 2018) are becoming a widely accepted standard among many NLP practitioners for definition of syntactic structures and treebanks. However, production parsers require custom tokenization policies and Part of Speech (PoS) tagging, mostly dictated by supported downstream applications. In addition, parsers in production environments require fine balancing of demands for model accuracy, service performance, response time and constraints on hardware resources, making the design of an industrial-grade parser a challenge. Hereby, we intro"
2020.coling-industry.9,P19-1620,0,0.163254,"Wikipedia Page: Total Quality Management Gold Long Answer: The exact origin of the term “total quality management” is uncertain. It is almost certainly inspired by Armand V. Feigenbaum’s multi-edition book Total Quality Control... Gold Short Answer: NULL BERTQA : Armand V. Feigenbaum Figure 1: Examples of questions in the NQ dataset. Example 1 contains the short answer in the long answer whereas Example 2 has none. We propose G AA M A that possesses several MRC technologies that are necessary to perform well on NQ and achieve significant boosts over another industry setting competitor system (Alberti et al., 2019a) pre-trained on a large language model (LM) and then over millions of synthetic examples. Specifically, G AA M A builds on top of a large pre-trained LM and focusses on two broad dimensions: 1. Improved Attention: With the reduction of observation bias in NQ, we find a distinct lack of lexical and grammatical alignment between answer contexts and the questions. For example, here is a question to identify the date of an event from the SQuAD 2.0 dataset: According to business journalist Kimberly Amadeo, when did the first signs of decline in real estate occur? This question can be aligned almo"
2020.coling-industry.9,P17-1171,0,0.0458488,"for supervised machine learning. CovidQA evaluates the zero-shot transfer capabilities of existing models on topics specifically related to COVID-19. One difference of CovidQA from the other QA datasets we evaluate is that it requires systems to predict the correct sentence that answers the question. Hence we intuitively report the P@1, R@3, and MRR based on the official evaluation metric. 5.2 Competitors We compare G AA M A against three strong competitors from the industry research: 1) A hybrid of a decomposable attention model for Natural Language Inference (Parikh et al., 2016) and DrQA (Chen et al., 2017), a retrieve and rank QA model, which obtains commendable results on SQuAD. 2) The NQ baseline system (Alberti et al., 2019b) and 3) The current industry SOTA on NQ (Alberti et al., 2019a) which utilizes 4 million synthetic examples as pre-training. Architecturally, the latter is similar to us but we propose more technical novelty in terms of both improved attention and data augmentation. We note there is very recent academic work (Zheng et al., 2020)which we omit as G AA M A outperforms them on short answers and more importantly we compare against large scale industry SOTA for the scope of th"
2020.coling-industry.9,P17-1055,0,0.105412,"are one-hot vectors for the ground-truth beginning 0.4 and end positions, and 1(a) for the ground-truth answer type. During decoding, the span over argmax of `b and argmax of `e is picked Similarity as the predicted short answer. 0.2 3.2 Attention Strategies 1995 2000 In this section, we outline our investigation of the attention mechanisms on top of the above BERTQA model. Our main question: BERT already computes self-attention over the question and the passage in several layers—can we improve on top that? 3.2.1 Attention-over-Attention (AoA) 1 Our first approach is AoA: originally designed (Cui et al., 2017) for cloze-style question answering, where a phrase in a short passage of text is removed in forming a question. We seek to explore whether AoA helps in a more traditional MRC setting. Let Q be a sequence of question tokens [q1 , . . . , qm ], and C a sequence of context tokens [c1 , . . . , cn ]. AoA first computes an attention matrix: M = CQT , (1) n×h m×h where C ∈ R ,Q ∈ R , and M ∈ Rn×m . In our case, the hidden dimension is h = 1024. Next, it separately performs on M a column-wise softmax α = sof tmax(MT ) and a row-wise softmax β = sof tmax(M). Each row i of matrix α represents the docu"
2020.coling-industry.9,N19-1423,0,0.134633,"performance when compared to baselines either trained on the target domain or zero-shot transferred to the target. Overall, our contributions can be summarized as follows: 1. We propose a novel system that investigates several improved attention and enhanced data augmentation strategies, 2. Outperforms the previous industry-scale QA system on NQ, 3. Provides ZSTL capabilities on two unseen domains and 4. Achieves competitive performance compared to the respective corresponding baselines. 2 Related Work Most recent MRC systems either achieve SOTA by adding additional components on top of BERT (Devlin et al., 2019) such as syntax (Zhang et al., 2019) or perform attention fusion (Wang et al., 2018) without using BERT. However, we argue that additional attention mechanisms should be explored on top of BERT such as computing additional cross-attention between the question and the passage and maximizing the diversity among different attention heads in BERT. Our work is also generic enough to be applied on recently introduced transformer based language models such as ALBERT (Lan et al., 2019) and REFORMER (Kitaev et al., 2020). Another common technique is DA (Zhang and Bansal, 2019) by artificially generatin"
2020.coling-industry.9,N18-2092,0,0.0233721,"018) without using BERT. However, we argue that additional attention mechanisms should be explored on top of BERT such as computing additional cross-attention between the question and the passage and maximizing the diversity among different attention heads in BERT. Our work is also generic enough to be applied on recently introduced transformer based language models such as ALBERT (Lan et al., 2019) and REFORMER (Kitaev et al., 2020). Another common technique is DA (Zhang and Bansal, 2019) by artificially generating more questions to enhance the training data or in a MTL setup (Yatskar, 2018; Dhingra et al., 2018; Zhou et al., 2019). (Alberti et al., 2019a; Alberti et al., 2019b) combine models of question generation with answer extraction and filter results to ensure round-trip consistency to get the SOTA on NQ. Contrary to this, we explore several strategies for DA that either involve diverse question generation from a dynamic nucleus (Holtzman et al., 2019) of the probability distribution over question tokens or shuffling the existing dataset to produce adversarial examples. Recently (Lee et al., 2019; Min et al., 2019) focus on “open” NQ, a modified version of the full NQ dataset for document retr"
2020.coling-industry.9,Q18-1023,0,0.0285251,"Missing"
2020.coling-industry.9,D19-1445,0,0.0218723,"C Hi = H Wi , (5) M ×1024 ; HC , HC ∈ RN ×1024 ; and WQ , WC ∈ R1024×1024 . Therefore, the AoA where HQ , HQ ∈ R i i i i layer adds about 2.1 million parameters on top of BERT which already has 340 million. Next, we feed Q HC 1 and H1 into the AoA calculation specified in Equations (1) - (3) to get the attention vector s1 for C head 1. The same procedure is applied to HQ 2 and H2 to get s2 for head 2. Lastly, s1 and s2 are combined with `b and `e respectively via two weighted sum operations for answer extraction. 3.2.2 Attention Diversity (AD) layer It has been shown through ablation studies (Kovaleva et al., 2019; Michel et al., 2019) that removing BERT attention heads can achieve comparable or better performance on some tasks. Our objective is to find out if we can diversify the information captured and train a better BERT model by enforcing diversity among the attention heads. In a Transformer model, (Li et al., 2018) examine a few methods to enforce such diversity and see an improvement on machine translation tasks. Contrary to that we start with a pre-trained BERT model, take the attention output from scaled dot-product attention and compute the cosine similarity between all pairs of heads: D= Hea"
2020.coling-industry.9,Q19-1026,0,0.147651,"leaderboard submission). For each question, crowd sourced annotators also provide start and end offsets for short answer spans5 within the Wikipedia article, if available, as well as long answer spans (which is generally the most immediate HTML paragraph, table, or list span containing the short answer), if available. The dataset also forces models to make an attempt at “knowing what they don’t know” (Rajpurkar et al., 2018) by requiring a confidence score with each prediction. For evaluation, we report the offset-based F1 overlap score. For additional details on the data and evaluation see (Kwiatkowski et al., 2019). Target Domain To test G AA M A’s ZSTL transfer capability, we choose two academic6 benchmark datasets on a related domain: Bio-medical. The first one uses a subset of the questions and annotations from task 8b of the BioASQ competition (Tsatsaronis et al., 2015). Specifically, we extract 1,266 factoid biomedical questions for which exact answers can be extracted from one of the PubMED abstracts marked as relevant by the annotators. We report the Factoid Mean Reciprocal Rank (MRR) as the evaluation metric. Secondly, we choose the very recent CovidQA (Tang et al., 2020) benchmark to illustrate"
2020.coling-industry.9,P19-1612,0,0.0354479,"ially generating more questions to enhance the training data or in a MTL setup (Yatskar, 2018; Dhingra et al., 2018; Zhou et al., 2019). (Alberti et al., 2019a; Alberti et al., 2019b) combine models of question generation with answer extraction and filter results to ensure round-trip consistency to get the SOTA on NQ. Contrary to this, we explore several strategies for DA that either involve diverse question generation from a dynamic nucleus (Holtzman et al., 2019) of the probability distribution over question tokens or shuffling the existing dataset to produce adversarial examples. Recently (Lee et al., 2019; Min et al., 2019) focus on “open” NQ, a modified version of the full NQ dataset for document retrieval QA that discards unanswerable questions. Contrary to that, we specifically focus on the full NQ dataset and believe there is room for improvement from a MRC research standpoint. 3 Model Architecture In this section, we first describe BERTQA , G AA M A’s underlying QA model, and two additional attention layers on top of it. Figure 2 shows our overall model architecture with details explained below. 3.1 Underlying QA model: BERTQA Given a token sequence X = [x1 , x2 , . . . , xT ]: BERT, a de"
2020.coling-industry.9,D18-1317,0,0.0247664,"or s1 for C head 1. The same procedure is applied to HQ 2 and H2 to get s2 for head 2. Lastly, s1 and s2 are combined with `b and `e respectively via two weighted sum operations for answer extraction. 3.2.2 Attention Diversity (AD) layer It has been shown through ablation studies (Kovaleva et al., 2019; Michel et al., 2019) that removing BERT attention heads can achieve comparable or better performance on some tasks. Our objective is to find out if we can diversify the information captured and train a better BERT model by enforcing diversity among the attention heads. In a Transformer model, (Li et al., 2018) examine a few methods to enforce such diversity and see an improvement on machine translation tasks. Contrary to that we start with a pre-trained BERT model, take the attention output from scaled dot-product attention and compute the cosine similarity between all pairs of heads: D= Head X Head X i=1 j=1 Oi · Oj . ||Oi |Oj || We then average D for the per-token similarity and add it as an additional loss term. For each token, there are 16 + 15 + ... + 2 total similarity calculations, 16 being the number of heads in BERTQA . Figure 3 shows the modified structure of Multi-head Attention in the T"
2020.coling-industry.9,D19-1284,0,0.0137809,"ore questions to enhance the training data or in a MTL setup (Yatskar, 2018; Dhingra et al., 2018; Zhou et al., 2019). (Alberti et al., 2019a; Alberti et al., 2019b) combine models of question generation with answer extraction and filter results to ensure round-trip consistency to get the SOTA on NQ. Contrary to this, we explore several strategies for DA that either involve diverse question generation from a dynamic nucleus (Holtzman et al., 2019) of the probability distribution over question tokens or shuffling the existing dataset to produce adversarial examples. Recently (Lee et al., 2019; Min et al., 2019) focus on “open” NQ, a modified version of the full NQ dataset for document retrieval QA that discards unanswerable questions. Contrary to that, we specifically focus on the full NQ dataset and believe there is room for improvement from a MRC research standpoint. 3 Model Architecture In this section, we first describe BERTQA , G AA M A’s underlying QA model, and two additional attention layers on top of it. Figure 2 shows our overall model architecture with details explained below. 3.1 Underlying QA model: BERTQA Given a token sequence X = [x1 , x2 , . . . , xT ]: BERT, a deep Transformer (Vas"
2020.coling-industry.9,D16-1244,0,0.106908,"Missing"
2020.coling-industry.9,D16-1264,0,0.136795,"Missing"
2020.coling-industry.9,P18-2124,0,0.218824,"n NQ. Further, we show that G AA M A transfers zero-shot to unseen real life and important domains as it yields respectable performance on two benchmarks: the BioASQ and the newly introduced CovidQA datasets. 1 Introduction A relatively new task in open domain question answering (QA) is machine reading comprehension (MRC), which aims to read and comprehend a given text and then answer questions based on it. Recent work on transfer learning, from large pre-trained language models like BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) has practically solved SQuAD (Rajpurkar et al., 2016; Rajpurkar et al., 2018), the most widely used MRC benchmark. This necessitates harder QA benchmarks for the field to advance. Additionally, SQuAD and other existing datasets like NarrativeQA (Koˇcisk`y et al., 2018) and HotpotQA (Yang et al., 2018) suffer from observation bias: annotators had read the passages before creating their questions. In industry research, there is an urgent demand to build a usable MRC QA system that not only provides very good performance on academic benchmarks but also real life industry applications (Tang et al., 2020) in a ZSTL environment. In this paper, to build such a system, we firs"
2020.coling-industry.9,N19-5004,0,0.0377738,"Missing"
2020.coling-industry.9,2020.acl-main.500,0,0.0540436,"Missing"
2020.coling-industry.9,P18-1158,0,0.0118638,"t transferred to the target. Overall, our contributions can be summarized as follows: 1. We propose a novel system that investigates several improved attention and enhanced data augmentation strategies, 2. Outperforms the previous industry-scale QA system on NQ, 3. Provides ZSTL capabilities on two unseen domains and 4. Achieves competitive performance compared to the respective corresponding baselines. 2 Related Work Most recent MRC systems either achieve SOTA by adding additional components on top of BERT (Devlin et al., 2019) such as syntax (Zhang et al., 2019) or perform attention fusion (Wang et al., 2018) without using BERT. However, we argue that additional attention mechanisms should be explored on top of BERT such as computing additional cross-attention between the question and the passage and maximizing the diversity among different attention heads in BERT. Our work is also generic enough to be applied on recently introduced transformer based language models such as ALBERT (Lan et al., 2019) and REFORMER (Kitaev et al., 2020). Another common technique is DA (Zhang and Bansal, 2019) by artificially generating more questions to enhance the training data or in a MTL setup (Yatskar, 2018; Dhin"
2020.coling-industry.9,2020.nlpcovid19-acl.1,0,0.0256188,"Missing"
2020.coling-industry.9,D18-1259,0,0.0913112,"Missing"
2020.coling-industry.9,D19-1253,0,0.0218363,"l components on top of BERT (Devlin et al., 2019) such as syntax (Zhang et al., 2019) or perform attention fusion (Wang et al., 2018) without using BERT. However, we argue that additional attention mechanisms should be explored on top of BERT such as computing additional cross-attention between the question and the passage and maximizing the diversity among different attention heads in BERT. Our work is also generic enough to be applied on recently introduced transformer based language models such as ALBERT (Lan et al., 2019) and REFORMER (Kitaev et al., 2020). Another common technique is DA (Zhang and Bansal, 2019) by artificially generating more questions to enhance the training data or in a MTL setup (Yatskar, 2018; Dhingra et al., 2018; Zhou et al., 2019). (Alberti et al., 2019a; Alberti et al., 2019b) combine models of question generation with answer extraction and filter results to ensure round-trip consistency to get the SOTA on NQ. Contrary to this, we explore several strategies for DA that either involve diverse question generation from a dynamic nucleus (Holtzman et al., 2019) of the probability distribution over question tokens or shuffling the existing dataset to produce adversarial examples."
2020.coling-industry.9,D19-1169,0,0.0121634,"s either trained on the target domain or zero-shot transferred to the target. Overall, our contributions can be summarized as follows: 1. We propose a novel system that investigates several improved attention and enhanced data augmentation strategies, 2. Outperforms the previous industry-scale QA system on NQ, 3. Provides ZSTL capabilities on two unseen domains and 4. Achieves competitive performance compared to the respective corresponding baselines. 2 Related Work Most recent MRC systems either achieve SOTA by adding additional components on top of BERT (Devlin et al., 2019) such as syntax (Zhang et al., 2019) or perform attention fusion (Wang et al., 2018) without using BERT. However, we argue that additional attention mechanisms should be explored on top of BERT such as computing additional cross-attention between the question and the passage and maximizing the diversity among different attention heads in BERT. Our work is also generic enough to be applied on recently introduced transformer based language models such as ALBERT (Lan et al., 2019) and REFORMER (Kitaev et al., 2020). Another common technique is DA (Zhang and Bansal, 2019) by artificially generating more questions to enhance the trai"
2020.coling-industry.9,2020.acl-main.599,0,0.60604,"Missing"
2020.emnlp-demos.5,D19-3006,1,0.826158,"m.com Abstract and explore how to ensemble multiple MRC models from GAAMA1 . We evaluate these techniques on the NQ short answer task. Using our ensemble of models, for each example (question, passage pair), we take the top predictions per system, group by span (answer extracted from the passage), normalize and aggregate the scores, take the mean score across systems for each span, and then take the highest scoring short and long answer spans as our final prediction. These improved ensembling techniques are applied to our MRC systems to produce stronger answers. Whereas other systems such as (Chakravarti et al., 2019; Yang et al., 2019a) and Allen NLP’s2 make use of a single model, we are able to use multiple models to produce a stronger result. We further take advantage of the fact that both the individual model predictions and the ensembed predictions are returned to help increase explainability for the user. For the graphical interface we use a heatmap to show the level of (dis)agreement between the underlying models along with the “best ensemble” answer. An example of this can be seen in Figure 1. More completely, our contributions include: We introduce ARES (A Reading Comprehension Ensembling Service"
2020.emnlp-demos.5,Q19-1026,0,0.0155355,"ARES leverages the CFO (Chakravarti et al., 2019) and ReactJS distributed frameworks to provide a scalable interactive Question Answering experience that capitalizes on the agreement (or lack thereof) between models to improve the answer visualization experience. 1 Introduction Machine Reading Comprension (MRC) involves computer systems that can take a question and some text and produce an answer to that question using the content in that text. This field has recently received considerable attention, yielding popular leaderboard challenges such as SQuAD (Rajpurkar et al., 2016, 2018) and NQ (Kwiatkowski et al., 2019). Currently, the top submissions on both the SQuAD and NQ leaderboards combine multiple system outputs. These ensembled systems traditionally outperform single models by 1-4 Fmeasure. Unfortunately, many of the papers for these systems provide little to no information about the ensembling techniques they use. In this work, we use GAAMA, a prototype question-answering system using the MRC techniques of (Pan et al., 2019), as our starting point ∗ • A novel MRC demonstration system, which leverages multiple underlying MRC model predictions and ensembles them for the user. • A system architecture"
2020.emnlp-demos.5,N19-4013,0,0.0971586,"how to ensemble multiple MRC models from GAAMA1 . We evaluate these techniques on the NQ short answer task. Using our ensemble of models, for each example (question, passage pair), we take the top predictions per system, group by span (answer extracted from the passage), normalize and aggregate the scores, take the mean score across systems for each span, and then take the highest scoring short and long answer spans as our final prediction. These improved ensembling techniques are applied to our MRC systems to produce stronger answers. Whereas other systems such as (Chakravarti et al., 2019; Yang et al., 2019a) and Allen NLP’s2 make use of a single model, we are able to use multiple models to produce a stronger result. We further take advantage of the fact that both the individual model predictions and the ensembed predictions are returned to help increase explainability for the user. For the graphical interface we use a heatmap to show the level of (dis)agreement between the underlying models along with the “best ensemble” answer. An example of this can be seen in Figure 1. More completely, our contributions include: We introduce ARES (A Reading Comprehension Ensembling Service): a novel Machine"
2020.emnlp-demos.5,2020.acl-main.604,0,0.0495456,"s the most important regions of the passage from the perspective of different models in addition to boxing in the ensembled answer as seen in Figure 1. 2.2 Many of the top recent MRC systems publish few details on their ensembling strategies. Systems such as (Devlin et al., 2019; Alberti et al., 2019; Liu et al., 2019; Wang et al., 2019; Lan et al., 2019; Group, 2017; Seo et al., 2016) report using ensembles of 5 to 18 models to gain 1.3 - 4 F1 points on tasks such as GLUE, SQuAD 1.0, and SQuAD 2.0; unfortunately most of these systems report little information on their ensembling techniques. (Liu et al., 2020) reports slightly more information: gaining 1.8 and 0.6 F1 points short answer (SA) and long answer (LA) respectively on the NQ dev set with an ensemble of three models with different hyperparameters. We also consider work in the field of information retrieval (IR) as a way to aggregate multiple scores for the same span. Similar to the popular CombSUM and CombMNZ (Kurland and Culpepper, 2018; Wu, 2012) methods, considering the spans as the “documents”, we use span-score weighted aggregation in our noisy-or aggregator. Futher, we additionally incorporate the use of rank-based scoring from Borda"
2020.emnlp-demos.5,2021.ccl-1.108,0,0.0475564,"Missing"
2020.emnlp-demos.5,P18-2124,0,0.0636688,"Missing"
2020.emnlp-demos.5,D16-1264,0,0.0423104,"bling strategies using the NQ dataset. ARES leverages the CFO (Chakravarti et al., 2019) and ReactJS distributed frameworks to provide a scalable interactive Question Answering experience that capitalizes on the agreement (or lack thereof) between models to improve the answer visualization experience. 1 Introduction Machine Reading Comprension (MRC) involves computer systems that can take a question and some text and produce an answer to that question using the content in that text. This field has recently received considerable attention, yielding popular leaderboard challenges such as SQuAD (Rajpurkar et al., 2016, 2018) and NQ (Kwiatkowski et al., 2019). Currently, the top submissions on both the SQuAD and NQ leaderboards combine multiple system outputs. These ensembled systems traditionally outperform single models by 1-4 Fmeasure. Unfortunately, many of the papers for these systems provide little to no information about the ensembling techniques they use. In this work, we use GAAMA, a prototype question-answering system using the MRC techniques of (Pan et al., 2019), as our starting point ∗ • A novel MRC demonstration system, which leverages multiple underlying MRC model predictions and ensembles th"
2020.emnlp-main.440,N16-1153,0,0.0652316,"Missing"
2020.emnlp-main.440,P19-1484,0,0.0166446,"ta in many domains can have implicit structure which can be taken advantage of. For example, in the IT domain, technical documents are often created using predefined templates, and support forums have data in the form of questions and accepted answers. In this work, we propose to make use of the structure in such unlabeled domain data to create synthetic data that can provide additional domain knowledge to the model. Augmenting training data with generated synthetic examples has been found to be effective in improving performance on low-resource tasks. Golub et al. (2017), Yang et al. (2017), Lewis et al. (2019) and Dhingra et al. (2018) develop approaches to generate natural questions that can aid downstream question answering tasks. However, when it is not possible to obtain synthetic data that exactly fits the target task description, we show that creating auxiliary tasks from such unlabeled data can be 5461 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5461–5468, c November 16–20, 2020. 2020 Association for Computational Linguistics Dataset TechQA AskUbuntu useful to the downstream task in a transfer learning setting. For preliminary experiments in"
2020.emnlp-main.440,2020.tacl-1.47,0,0.0222883,"in content and terminology from the pre-training corpora. To address this language mismatch problem, recent work (Alsentzer et al., 2019; Lee et al., 2019; ∗ Both authors contributed equally. Work done during AI Residency at IBM Research. ‡ Corresponding author. † Beltagy et al., 2019; Gururangan et al., 2020) has adapted pre-trained LMs to specific domains by continuing to train the same LM on target domain text. Similar approaches are also used in multilingual adaptation, where the representations learned from multilingual pre-training are further optimized for a particular target language (Liu et al., 2020; Bapna and Firat, 2019). However, many specialized domains contain their own specific terms that are not part of the pre-trained LM vocabulary. Furthermore, in many such domains, large enough corpora may not be available to support LM training from scratch. To resolve this out-of-vocabulary issue, in this work, we extend the open-domain vocabulary with in-domain terms while adapting the LM, and show that it helps improve performance on downstream tasks. While language modeling can help the model better encode the domain language, it might not be sufficient to gain the domain knowledge necessa"
2020.emnlp-main.440,2021.ccl-1.108,0,0.114872,"Missing"
2020.emnlp-main.440,P17-2081,0,0.0204121,"ion Detection. 1 Introduction Pre-trained language models (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019) have pushed performance in many natural language processing tasks to new heights. The process of model construction has effectively been reduced to extending the pre-trained LM architecture with simpler taskspecific layers, while fine-tuning on labeled target data. In cases where the target task has limited labeled data, prior work has also employed transfer learning by pre-training on a source dataset with abundant labeled data before fine-tuning on the target task dataset (Min et al., 2017; Chung et al., 2018; Wiese et al., 2017). However, directly fine-tuning to a task in a new domain may not be optimal when the domain is distant in content and terminology from the pre-training corpora. To address this language mismatch problem, recent work (Alsentzer et al., 2019; Lee et al., 2019; ∗ Both authors contributed equally. Work done during AI Residency at IBM Research. ‡ Corresponding author. † Beltagy et al., 2019; Gururangan et al., 2020) has adapted pre-trained LMs to specific domains by continuing to train the same LM on target domain text. Similar approaches are also used in m"
2020.emnlp-main.440,N19-4009,0,0.059706,"Missing"
2020.emnlp-main.440,D16-1264,0,0.0406929,"se to use the inherent structure in unlabeled data to formulate synthetic tasks that can transfer to downstream tasks in a lowresource setting. (3) In our experiments, we show considerable improvements in performance over directly fine-tuning an underlying RoBERTa-large LM (Liu et al., 2019) on multiple tasks in the IT domain: extractive reading comprehension (RC), document ranking (DR) and duplicate question detection (DQD).1 2 Datasets We use two publicly available IT domain datasets. Table 1 shows their size statistics. TechQA (Castelli et al., 2019) is an extractive reading comprehension (Rajpurkar et al., 2016) dataset developed from real user questions in the customer support domain. Each question is accompanied by 50 documents, at most one of which has the answer. A companion collection of 801K unlabeled Technotes is provided to support LM training. In addition to the primary reading comprehension task (TechQA-RC), we also evaluate on a new document ranking task (TechQA-DR). Given the question, the task is to find the document that contains the answer. AskUbuntu2 (Lei et al., 2016) is a dataset containing user-marked pairs of similar questions from Stack Exchange3 , which was developed for a dupli"
2020.emnlp-main.440,D19-1171,0,0.0427042,"Missing"
2020.emnlp-main.440,K17-1029,0,0.0192711,"ned language models (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019) have pushed performance in many natural language processing tasks to new heights. The process of model construction has effectively been reduced to extending the pre-trained LM architecture with simpler taskspecific layers, while fine-tuning on labeled target data. In cases where the target task has limited labeled data, prior work has also employed transfer learning by pre-training on a source dataset with abundant labeled data before fine-tuning on the target task dataset (Min et al., 2017; Chung et al., 2018; Wiese et al., 2017). However, directly fine-tuning to a task in a new domain may not be optimal when the domain is distant in content and terminology from the pre-training corpora. To address this language mismatch problem, recent work (Alsentzer et al., 2019; Lee et al., 2019; ∗ Both authors contributed equally. Work done during AI Residency at IBM Research. ‡ Corresponding author. † Beltagy et al., 2019; Gururangan et al., 2020) has adapted pre-trained LMs to specific domains by continuing to train the same LM on target domain text. Similar approaches are also used in multilingual adaptation, where the represe"
2020.emnlp-main.440,P17-1096,0,0.02229,"at such unlabeled data in many domains can have implicit structure which can be taken advantage of. For example, in the IT domain, technical documents are often created using predefined templates, and support forums have data in the form of questions and accepted answers. In this work, we propose to make use of the structure in such unlabeled domain data to create synthetic data that can provide additional domain knowledge to the model. Augmenting training data with generated synthetic examples has been found to be effective in improving performance on low-resource tasks. Golub et al. (2017), Yang et al. (2017), Lewis et al. (2019) and Dhingra et al. (2018) develop approaches to generate natural questions that can aid downstream question answering tasks. However, when it is not possible to obtain synthetic data that exactly fits the target task description, we show that creating auxiliary tasks from such unlabeled data can be 5461 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5461–5468, c November 16–20, 2020. 2020 Association for Computational Linguistics Dataset TechQA AskUbuntu useful to the downstream task in a transfer learning setting. For prelim"
2020.findings-emnlp.288,P19-1620,0,0.0215709,"ess to its own errors by exposing it to actions that are often inferior to the oracle sequence in score. The approach presented here seeks only the small set of sequences improving over the oracle and uses them for conventional maximum likelihood training. Synthetic text, introduced in Section 4, is related to Back-translation in Machine Translation (Sennrich et al., 2016). The approach presented here exploits however the fact that multiple sentences correspond to a single AMR and thus needs no external data. This is closer to recent work on question generation for question answering systems (Alberti et al., 2019), which also uses cycle consistency filtering. Finally, regarding synthetic AMR, discussed in Section 5, with respect to prior work (Konstas et al., 2017b; van Noord and Bos, 2017) we show that synthetic AMR parsing still can yield improvements for high performance baselines, and introduce the cycle-consistency filtering. 8 Conclusions In this work6 , we explored different ways in which trained models can be applied to improve AMR parsing performance via self-learning. Despite the recent strong improvements in performance through novel architectures, we show that the proposed techniques improv"
2020.findings-emnlp.288,D16-1211,0,0.015395,"need for graph post-processing using Core-NLP. Compared to this, transition-based approaches provide a more uniform performance across categories, and in this context the presented self-learning methods are able to improve in all categories. One aspect that merits further study, is the increase in the Negation category when using synTxt, which improves 5.4 points, probably due to generation of additional negation examples. 7 Related Works Mining for gold, introduced in Section 3, can be related to previous works addressing oracle limitations such as dynamic oracles (Goldberg and Nivre, 2012; Ballesteros et al., 2016), imitation learning (Goodman et al., 2016) and minimum risk training (Naseem et al., 2019). All these approaches increase parser robustness to its own errors by exposing it to actions that are often inferior to the oracle sequence in score. The approach presented here seeks only the small set of sequences improving over the oracle and uses them for conventional maximum likelihood training. Synthetic text, introduced in Section 4, is related to Back-translation in Machine Translation (Sennrich et al., 2016). The approach presented here exploits however the fact that multiple sentences correspo"
2020.findings-emnlp.288,2020.acl-main.119,0,0.463735,"e rise of pre-trained transformer models (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019), but also due to AMR-specific architecture improvements. A non-exhaustive list includes latent node-word alignments through learned permutations (Lyu and Titov, 2018a), minimum risk training via REINFORCE (Naseem et al., 2019), a sequence-to-graph modeling of linearized trees with copy mechanisms and re-entrance features ∗ † Equal contribution. Work done during AI Residency at IBM Research. (Zhang et al., 2019a) and more recently a highly performant graph-sequence iterative refinement model (Cai and Lam, 2020) and a hard-attention transition-based parser (F. A. et al., 2020), both based on the Transformer architecture. Given the strong improvements in architectures for AMR, it becomes interesting to explore alternative avenues to push performance even further. AMR annotations are relatively expensive to produce and thus typical corpora have on the order of tens of thousands of sentences. In this work we explore the use self-learning techniques as a means to escape this limitation. We explore the use of a trained parser to iteratively refine a rule-based AMR oracle (Ballesteros and Al-Onaizan, 2017;"
2020.findings-emnlp.288,P13-2131,0,0.203945,"ousands of sentences. In this work we explore the use self-learning techniques as a means to escape this limitation. We explore the use of a trained parser to iteratively refine a rule-based AMR oracle (Ballesteros and Al-Onaizan, 2017; F. A. et al., 2020) to yield better action sequences. We also exploit the fact that a single AMR graph maps to multiple sentences in combination with AMR-to-text (Mager et al., 2020), to generate additional training samples without using external data. Finally we revisit silver data training (Konstas et al., 2017a). These techniques reach 77.3 and 80.7 Smatch (Cai and Knight, 2013) on AMR1.0 and AMR2.0 respectively using only gold data as well as 78.2 and 81.3 with silver data. 2 Baseline Parser and Setup To test the proposed ideas, we used the AMR setup and parser from (F. A. et al., 2020) with improved embedding representations. This is a transitionbased parsing approach, following the original AMR oracle in (Ballesteros and Al-Onaizan, 2017) and further improvements in (Naseem et al., 2019). Briefly, rather than predicting a graph g from a sentence s directly, transition-based parsers predict instead an action sequence a. This action sequence, when applied to a state"
2020.findings-emnlp.288,E17-1051,0,0.0332596,"+synAMRU 80.9 84.9 81.4 88.4 88.0 66.0 79.3 70.9 78.9 synTxt+synAMRU 81.0 84.9 81.5 88.6 88.3 67.4 78.9 71.5 79.1 mining+synTxt+synAMRU 81.3 85.3 81.8 88.7 88.7 66.3 79.2 71.9 79.4 Table 5: Detailed scoring of the final system on AMR2.0 test sets which is the best result obtained at the time of submission for AMR2.0, improving 1.1 over (Cai and Lam, 2020). It also obtains 78.2 for AMR1.0, which is 2.8 points above best previous results. Excluding silver data training, synTxt achieves 80.7 (+0.5) in AMR2.0 and 77.5 (+2.1) with minining in AMR1.0. We also provide the detailed AMR analysis from (Damonte et al., 2017) for the best previously published system, baseline and the proposed methods in Table 5. This analysis computes Smatch for sub-sets of AMR to loosely reflect particular subtasks, such as Word Sense Disambiguation (WSD), Named Entity recognition or Semantic Role Labeling (SRL). The proposed approaches and the baseline consistently outperform prior art in a majority of categories and the main observable differences seems due to differences between the transitionbased and graph recategorization approaches. Wikification and negation, the only categories where the proposed methods do not outperform"
2020.findings-emnlp.288,N19-1423,0,0.104904,"mprove an already performant parser and achieve state-ofthe-art results on AMR 1.0 and AMR 2.0. 1 Introduction Abstract Meaning Representation (AMR) are broad-coverage sentence-level semantic representations expressing who does what to whom. Nodes in an AMR graph correspond to concepts such as entities or predicates and are not always directly related to words. Edges in AMR represent relations between concepts such as subject/object. AMR has experienced unprecedented performance improvements in the last two years, partly due to the rise of pre-trained transformer models (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019), but also due to AMR-specific architecture improvements. A non-exhaustive list includes latent node-word alignments through learned permutations (Lyu and Titov, 2018a), minimum risk training via REINFORCE (Naseem et al., 2019), a sequence-to-graph modeling of linearized trees with copy mechanisms and re-entrance features ∗ † Equal contribution. Work done during AI Residency at IBM Research. (Zhang et al., 2019a) and more recently a highly performant graph-sequence iterative refinement model (Cai and Lam, 2020) and a hard-attention transition-based parser (F. A. et al., 2020"
2020.findings-emnlp.288,2020.findings-emnlp.89,1,0.79522,"Missing"
2020.findings-emnlp.288,P14-1134,0,0.10189,"onal Linguistics Figure 1: Role of sentence s, AMR graph g and oracle actions a in the different self-learning strategies. Left: Replacing rule-based actions by machine generated ones. Middle: synthetic text generation for existing graph annotations. Right: synthetic AMR generation for external data. Generated data ( ). External data ( ). predicting the graph into a sequence to sequence problem, but introduces the need for an oracle to determine the action sequence a = O(g, s). As in previous works, the oracle in (F. A. et al., 2020) is rule-based, relying on external word-to-node alignments (Flanigan et al., 2014; Pourdamghani et al., 2016) to determine action sequences. It however force-aligns unaligned nodes to suitable words, notably improving oracle performance. As parser, (F. A. et al., 2020) introduces the stack-Transformer model. This is a modification of the sequence to sequence Transformer (Vaswani et al., 2017) to account for the parser state. It modifies the cross-attention mechanism dedicating two heads to attend the stack and buffer of the state machine M (a, s). This parser is highly performant achieving the best results for a transition-based parser as of date and second overall for AMR"
2020.findings-emnlp.288,C12-1059,0,0.0151358,"category, probably due to need for graph post-processing using Core-NLP. Compared to this, transition-based approaches provide a more uniform performance across categories, and in this context the presented self-learning methods are able to improve in all categories. One aspect that merits further study, is the increase in the Negation category when using synTxt, which improves 5.4 points, probably due to generation of additional negation examples. 7 Related Works Mining for gold, introduced in Section 3, can be related to previous works addressing oracle limitations such as dynamic oracles (Goldberg and Nivre, 2012; Ballesteros et al., 2016), imitation learning (Goodman et al., 2016) and minimum risk training (Naseem et al., 2019). All these approaches increase parser robustness to its own errors by exposing it to actions that are often inferior to the oracle sequence in score. The approach presented here seeks only the small set of sequences improving over the oracle and uses them for conventional maximum likelihood training. Synthetic text, introduced in Section 4, is related to Back-translation in Machine Translation (Sennrich et al., 2016). The approach presented here exploits however the fact that"
2020.findings-emnlp.288,P16-1001,0,0.0159831,"Compared to this, transition-based approaches provide a more uniform performance across categories, and in this context the presented self-learning methods are able to improve in all categories. One aspect that merits further study, is the increase in the Negation category when using synTxt, which improves 5.4 points, probably due to generation of additional negation examples. 7 Related Works Mining for gold, introduced in Section 3, can be related to previous works addressing oracle limitations such as dynamic oracles (Goldberg and Nivre, 2012; Ballesteros et al., 2016), imitation learning (Goodman et al., 2016) and minimum risk training (Naseem et al., 2019). All these approaches increase parser robustness to its own errors by exposing it to actions that are often inferior to the oracle sequence in score. The approach presented here seeks only the small set of sequences improving over the oracle and uses them for conventional maximum likelihood training. Synthetic text, introduced in Section 4, is related to Back-translation in Machine Translation (Sennrich et al., 2016). The approach presented here exploits however the fact that multiple sentences correspond to a single AMR and thus needs no extern"
2020.findings-emnlp.288,W16-2316,0,0.0222026,"Missing"
2020.findings-emnlp.288,P17-1014,0,0.14619,"e to produce and thus typical corpora have on the order of tens of thousands of sentences. In this work we explore the use self-learning techniques as a means to escape this limitation. We explore the use of a trained parser to iteratively refine a rule-based AMR oracle (Ballesteros and Al-Onaizan, 2017; F. A. et al., 2020) to yield better action sequences. We also exploit the fact that a single AMR graph maps to multiple sentences in combination with AMR-to-text (Mager et al., 2020), to generate additional training samples without using external data. Finally we revisit silver data training (Konstas et al., 2017a). These techniques reach 77.3 and 80.7 Smatch (Cai and Knight, 2013) on AMR1.0 and AMR2.0 respectively using only gold data as well as 78.2 and 81.3 with silver data. 2 Baseline Parser and Setup To test the proposed ideas, we used the AMR setup and parser from (F. A. et al., 2020) with improved embedding representations. This is a transitionbased parsing approach, following the original AMR oracle in (Ballesteros and Al-Onaizan, 2017) and further improvements in (Naseem et al., 2019). Briefly, rather than predicting a graph g from a sentence s directly, transition-based parsers predict inste"
2020.findings-emnlp.288,2021.ccl-1.108,0,0.0417005,"Missing"
2020.findings-emnlp.288,P18-1037,0,0.371916,"mantic representations expressing who does what to whom. Nodes in an AMR graph correspond to concepts such as entities or predicates and are not always directly related to words. Edges in AMR represent relations between concepts such as subject/object. AMR has experienced unprecedented performance improvements in the last two years, partly due to the rise of pre-trained transformer models (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019), but also due to AMR-specific architecture improvements. A non-exhaustive list includes latent node-word alignments through learned permutations (Lyu and Titov, 2018a), minimum risk training via REINFORCE (Naseem et al., 2019), a sequence-to-graph modeling of linearized trees with copy mechanisms and re-entrance features ∗ † Equal contribution. Work done during AI Residency at IBM Research. (Zhang et al., 2019a) and more recently a highly performant graph-sequence iterative refinement model (Cai and Lam, 2020) and a hard-attention transition-based parser (F. A. et al., 2020), both based on the Transformer architecture. Given the strong improvements in architectures for AMR, it becomes interesting to explore alternative avenues to push performance even fur"
2020.findings-emnlp.288,2020.acl-main.167,1,0.915499,"MR, it becomes interesting to explore alternative avenues to push performance even further. AMR annotations are relatively expensive to produce and thus typical corpora have on the order of tens of thousands of sentences. In this work we explore the use self-learning techniques as a means to escape this limitation. We explore the use of a trained parser to iteratively refine a rule-based AMR oracle (Ballesteros and Al-Onaizan, 2017; F. A. et al., 2020) to yield better action sequences. We also exploit the fact that a single AMR graph maps to multiple sentences in combination with AMR-to-text (Mager et al., 2020), to generate additional training samples without using external data. Finally we revisit silver data training (Konstas et al., 2017a). These techniques reach 77.3 and 80.7 Smatch (Cai and Knight, 2013) on AMR1.0 and AMR2.0 respectively using only gold data as well as 78.2 and 81.3 with silver data. 2 Baseline Parser and Setup To test the proposed ideas, we used the AMR setup and parser from (F. A. et al., 2020) with improved embedding representations. This is a transitionbased parsing approach, following the original AMR oracle in (Ballesteros and Al-Onaizan, 2017) and further improvements in"
2020.findings-emnlp.288,P19-1451,1,0.306011,"es in an AMR graph correspond to concepts such as entities or predicates and are not always directly related to words. Edges in AMR represent relations between concepts such as subject/object. AMR has experienced unprecedented performance improvements in the last two years, partly due to the rise of pre-trained transformer models (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019), but also due to AMR-specific architecture improvements. A non-exhaustive list includes latent node-word alignments through learned permutations (Lyu and Titov, 2018a), minimum risk training via REINFORCE (Naseem et al., 2019), a sequence-to-graph modeling of linearized trees with copy mechanisms and re-entrance features ∗ † Equal contribution. Work done during AI Residency at IBM Research. (Zhang et al., 2019a) and more recently a highly performant graph-sequence iterative refinement model (Cai and Lam, 2020) and a hard-attention transition-based parser (F. A. et al., 2020), both based on the Transformer architecture. Given the strong improvements in architectures for AMR, it becomes interesting to explore alternative avenues to push performance even further. AMR annotations are relatively expensive to produce and"
2020.findings-emnlp.288,P02-1040,1,0.124689,"synAMR corpus. This set is optionally filtered to reduce the training corpus size for AMR 2.0 experiments and is left unfiltered for AMR 1.0, due to its smaller size. The filtering combines two criteria. First, it is easy to detect when the transition-based system produces disconnected AMR graphs. Outputs with disconnected graphs are therefore filtered out. Second, we use a cycle-consistency criteria as in Section 4 whereby synthetic text is generated for each synthetic AMR with (Mager et al., 2020). For each pair of original text and generated text, the synAMR is filtered out if BLEU score (Papineni et al., 2002) is lower than a pre-specified 4 synTxt training takes 17h for AMR 2.0 and 5h hours for AMR 1.0 on a Tesla V100. AMR-to-text training for 15 epochs takes 4.5h on AMR 1.0 and 15h on AMR 2.0. 3210 threshold, 5 in our experiments. Because the AMRto-text generation system is trained on the humanannotated AMR only, generation performance may be worse on synthetic AMR and out of domain data. Consequently we apply BLEU-based filtering only to the input texts with no out of vocabulary (OOV) tokens with respect to the original humanannotated corpus. After filtering, the synAMR data is reduced to 58k se"
2020.findings-emnlp.288,W16-6603,0,0.0267822,"1: Role of sentence s, AMR graph g and oracle actions a in the different self-learning strategies. Left: Replacing rule-based actions by machine generated ones. Middle: synthetic text generation for existing graph annotations. Right: synthetic AMR generation for external data. Generated data ( ). External data ( ). predicting the graph into a sequence to sequence problem, but introduces the need for an oracle to determine the action sequence a = O(g, s). As in previous works, the oracle in (F. A. et al., 2020) is rule-based, relying on external word-to-node alignments (Flanigan et al., 2014; Pourdamghani et al., 2016) to determine action sequences. It however force-aligns unaligned nodes to suitable words, notably improving oracle performance. As parser, (F. A. et al., 2020) introduces the stack-Transformer model. This is a modification of the sequence to sequence Transformer (Vaswani et al., 2017) to account for the parser state. It modifies the cross-attention mechanism dedicating two heads to attend the stack and buffer of the state machine M (a, s). This parser is highly performant achieving the best results for a transition-based parser as of date and second overall for AMR2.0 and tied with the best f"
2020.findings-emnlp.288,P16-1009,0,0.0358422,"rks addressing oracle limitations such as dynamic oracles (Goldberg and Nivre, 2012; Ballesteros et al., 2016), imitation learning (Goodman et al., 2016) and minimum risk training (Naseem et al., 2019). All these approaches increase parser robustness to its own errors by exposing it to actions that are often inferior to the oracle sequence in score. The approach presented here seeks only the small set of sequences improving over the oracle and uses them for conventional maximum likelihood training. Synthetic text, introduced in Section 4, is related to Back-translation in Machine Translation (Sennrich et al., 2016). The approach presented here exploits however the fact that multiple sentences correspond to a single AMR and thus needs no external data. This is closer to recent work on question generation for question answering systems (Alberti et al., 2019), which also uses cycle consistency filtering. Finally, regarding synthetic AMR, discussed in Section 5, with respect to prior work (Konstas et al., 2017b; van Noord and Bos, 2017) we show that synthetic AMR parsing still can yield improvements for high performance baselines, and introduce the cycle-consistency filtering. 8 Conclusions In this work6 ,"
2020.findings-emnlp.288,P19-1009,0,0.273105,"Missing"
2020.findings-emnlp.288,D19-1392,0,0.429197,"Missing"
2020.findings-emnlp.89,N19-1253,0,0.111422,"tate such as the stack-LSTM (Dyer et al., 2015) allowed encoding the entire buffer and ∗ Miguel’s and Austin’s contributions were carried out while at IBM Research. stack. It was later shown that local features of the stack and buffer extracted from contextual word representations, such as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this work we revisit the local versus global paradigms of state modeling in the context of sequence-to-sequence Transformers applied to action prediction for transition-based parsing. Similarly to previous works for RNN sequence to sequence (Liu and Zhang, 2017; Zhang et al., 2017), we propose a modification of the cross-attention mechanism of the Transformer to provide global parser state modeling. We analyze the role of local versus global parser state modeling, stack and buffer modeling, effects model size as well as task complexity and amount of training data. Resu"
2020.findings-emnlp.89,D17-1130,1,0.92647,"Missing"
2020.findings-emnlp.89,D12-1133,0,0.0148225,"mti is a {−∞, 0} mask, pti are the position embeddings for elements in the stack, h = f (w) is the output of the Transformer encoder. The attention would be computed from the score function as αti = softmax(et )i Both mask and positions change for each word and time-step as the parser state changes, but they imply little computation overhead and can be precomputed for training. Henceforth this modification will be referred to as stack-Transformer. 3.2 Labeled SHIFT Multi-task It is common practice for transition-based systems to add an additional Part of Speech (POS) or word prediction task (Bohnet and Nivre, 2012). This is achieved by labeling the SHIFT action, that moves a word from the buffer to the stack, with the word’s tag. This decorated actions become part of the action history a&lt;t , which was expected to give better visibility into stack/buffer content and exploit Transformer’s attentional encoding of history. In initial experiments, POS tags produced a small improvement while word prediction led to performance decrease. It was observed, however, that prediction of only 100 − 300 most frequent words, leaving SHIFT undecorated otherwise, led to large performance increases. This is thus the metho"
2020.findings-emnlp.89,2020.acl-main.119,0,0.476243,"ck-LSTM (Dyer et al., 2015) allowed encoding the entire buffer and ∗ Miguel’s and Austin’s contributions were carried out while at IBM Research. stack. It was later shown that local features of the stack and buffer extracted from contextual word representations, such as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this work we revisit the local versus global paradigms of state modeling in the context of sequence-to-sequence Transformers applied to action prediction for transition-based parsing. Similarly to previous works for RNN sequence to sequence (Liu and Zhang, 2017; Zhang et al., 2017), we propose a modification of the cross-attention mechanism of the Transformer to provide global parser state modeling. We analyze the role of local versus global parser state modeling, stack and buffer modeling, effects model size as well as task complexity and amount of training data. Results show that local"
2020.findings-emnlp.89,P13-2131,0,0.17666,"f entity sub-graphs, this increased oracle Smatch from 93.7 to 98.1 and notably improved model performance. We therefore provide results for the Naseem et al. (2019) oracle for comparison. Both previous works predict a node creation action and then a node label, or call a lemmatizer if no label is found. Instead, we directly predicted the label and added COPY actions to construct node names from lemmas1 or surface words, resulting in a maximum of 9K actions. Node label predictions were limited to those seen during training for the word on the top of the stack. Results were measured in Smatch (Cai and Knight, 2013) using the latest version 1.0.42 . Regarding model implementation, all models were implemented on the fairseq toolkit and trained with only minor modifications over the MT model hyper-parameters (Ott et al., 2018). This used crossentropy training with learning rate 5e−4 , inverse square root scheduling with min. 1e−9 , 4000 warmup updates with learning rate 1e−7 , and maximum 3584 tokens per batch. Adam parameters 0.9 and 0.98, label smoothing was reduced to 0.013 . All models used 6 layers of encoding and decoding with size 256 and 4 attention heads, except the normal Transformers in AMR, whi"
2020.findings-emnlp.89,N19-1423,0,0.0410949,"for global (full buffer, full stack) variants being more performant. Modeling of the buffer seems also more important than modeling of the stack. One possible explanation for this is that, since the total number of heads is kept fixed, it may be more useful to gain an additional free head than modeling the stack content. Furthermore without recursive representation building, as in stack-LSTMs, the role of the stack can be expected to be less important. Tables 2 and 3 compare with prior works. Pretrained embeddings used are indicated as XLnet-largeX (Yang et al., 2019), BERT baseβ and largeB (Devlin et al., 2019), Graph Recategorization, which utilizes an external entity recognizer (Lyu and Titov, 2018; Zhang et al., 2019) as (G.R.) and a∗ indicates the Naseem et al. (2019) oracle. Overall, the stack-Transformer is competitive against recent works particularly for AMR, likely due to the higher complexity of the task. Compared to prior AMR systems, it is worth noting the large performance increase against stack-LSTM (Naseem et al., 2019), while sharing a similar oracle and embeddings and not using reinforcement learning fine-tuning. The stack-Transformer also matches the best reported AMR system (Cai a"
2020.findings-emnlp.89,P81-1022,0,0.716276,"Missing"
2020.findings-emnlp.89,P15-1033,1,0.771904,"e attractive for their linear inference time and interpretability, however, their performance hinges on effective modeling of the parser state at every decision step. Parser states typically comprise two memories, a buffer and a stack, from which tokens can be pushed or popped (Kubler et al., 2009). Traditionally, parser states were modeled using hand selected local features pertaining only to the words on the top of the stack or buffer (Nivre et al., 2007; Zhang and Nivre, 2011, inter-alia). With the widespread use of neural networks, global models of the parser state such as the stack-LSTM (Dyer et al., 2015) allowed encoding the entire buffer and ∗ Miguel’s and Austin’s contributions were carried out while at IBM Research. stack. It was later shown that local features of the stack and buffer extracted from contextual word representations, such as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this"
2020.findings-emnlp.89,N16-1024,1,0.865879,"Missing"
2020.findings-emnlp.89,N19-1076,0,0.0269193,"Missing"
2020.findings-emnlp.89,J13-4006,0,0.0304049,"tions of the sequence-to-sequence Transformer architecture to model either global or local parser states in transition-based parsing. We show that modifications of the cross attention mechanism of the Transformer considerably strengthen performance both on dependency and Abstract Meaning Representation (AMR) parsing tasks, particularly for smaller models or limited training data. 1 Introduction Transition-based Parsing transforms the task of predicting a graph from a sentence into predicting an action sequence of a state machine that produces the graph (Nivre, 2003, 2004; Kubler et al., 2009; Henderson et al., 2013). These parsers are attractive for their linear inference time and interpretability, however, their performance hinges on effective modeling of the parser state at every decision step. Parser states typically comprise two memories, a buffer and a stack, from which tokens can be pushed or popped (Kubler et al., 2009). Traditionally, parser states were modeled using hand selected local features pertaining only to the words on the top of the stack or buffer (Nivre et al., 2007; Zhang and Nivre, 2011, inter-alia). With the widespread use of neural networks, global models of the parser state such a"
2020.findings-emnlp.89,W16-2316,0,0.111522,"Missing"
2020.findings-emnlp.89,Q16-1023,0,0.397406,"tionally, parser states were modeled using hand selected local features pertaining only to the words on the top of the stack or buffer (Nivre et al., 2007; Zhang and Nivre, 2011, inter-alia). With the widespread use of neural networks, global models of the parser state such as the stack-LSTM (Dyer et al., 2015) allowed encoding the entire buffer and ∗ Miguel’s and Austin’s contributions were carried out while at IBM Research. stack. It was later shown that local features of the stack and buffer extracted from contextual word representations, such as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this work we revisit the local versus global paradigms of state modeling in the context of sequence-to-sequence Transformers applied to action prediction for transition-based parsing. Similarly to previous works for RNN sequence to sequence (Liu and Zhang, 2017; Zhang et al., 2017), we propose a modification of the cross"
2020.findings-emnlp.89,D19-1279,0,0.02522,"espread use of neural networks, global models of the parser state such as the stack-LSTM (Dyer et al., 2015) allowed encoding the entire buffer and ∗ Miguel’s and Austin’s contributions were carried out while at IBM Research. stack. It was later shown that local features of the stack and buffer extracted from contextual word representations, such as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this work we revisit the local versus global paradigms of state modeling in the context of sequence-to-sequence Transformers applied to action prediction for transition-based parsing. Similarly to previous works for RNN sequence to sequence (Liu and Zhang, 2017; Zhang et al., 2017), we propose a modification of the cross-attention mechanism of the Transformer to provide global parser state modeling. We analyze the role of local versus global parser state modeling, stack and buffer modeling, effects model si"
2020.findings-emnlp.89,D19-1277,0,0.0397653,"Missing"
2020.findings-emnlp.89,W17-6315,0,0.284317,"ch as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this work we revisit the local versus global paradigms of state modeling in the context of sequence-to-sequence Transformers applied to action prediction for transition-based parsing. Similarly to previous works for RNN sequence to sequence (Liu and Zhang, 2017; Zhang et al., 2017), we propose a modification of the cross-attention mechanism of the Transformer to provide global parser state modeling. We analyze the role of local versus global parser state modeling, stack and buffer modeling, effects model size as well as task complexity and amount of training data. Results show that local and global state modeling of the parser state yield more than 2 percentage points absolute improvement over a strong Transformer baseline, both for dependency and Abstract Meaning Representation (AMR) parsing. Gains are also particularly large for smaller train sets"
2020.findings-emnlp.89,2021.ccl-1.108,0,0.0504955,"Missing"
2020.findings-emnlp.89,P18-1037,0,0.145188,"seems also more important than modeling of the stack. One possible explanation for this is that, since the total number of heads is kept fixed, it may be more useful to gain an additional free head than modeling the stack content. Furthermore without recursive representation building, as in stack-LSTMs, the role of the stack can be expected to be less important. Tables 2 and 3 compare with prior works. Pretrained embeddings used are indicated as XLnet-largeX (Yang et al., 2019), BERT baseβ and largeB (Devlin et al., 2019), Graph Recategorization, which utilizes an external entity recognizer (Lyu and Titov, 2018; Zhang et al., 2019) as (G.R.) and a∗ indicates the Naseem et al. (2019) oracle. Overall, the stack-Transformer is competitive against recent works particularly for AMR, likely due to the higher complexity of the task. Compared to prior AMR systems, it is worth noting the large performance increase against stack-LSTM (Naseem et al., 2019), while sharing a similar oracle and embeddings and not using reinforcement learning fine-tuning. The stack-Transformer also matches the best reported AMR system (Cai and Lam, 2020) on AMR1.0 without graph recategorization, but using RoBERTa instead of BERT e"
2020.findings-emnlp.89,P18-1130,0,0.0181131,"nd buffer attentions. While simple, this precludes the use of SWAP actions needed for AMR parsing and non-projective parsing. Zhang et al. (2017) mask out reduced words and add a bias to the attention weights for words in the stack. While being the closest to the proposed technique, this method does not separately model stack and buffer nor retains free attention heads, which we consider a fundamental advantage. We also provide evidence that modeling the parser state still produces gains when using pre-trained Transformer embeddings and provide a detailed analysis of components. Finally, RNN (Ma et al., 2018) and selfattention (Ahmad et al., 2019) Stack-Pointer networks sum encoder representations based on local graph structure, which can be interpreted as masked uniform attention over 3 words and is related to the previous methods. 7 Conclusions We have explored modifications of sequence-tosequence Transformers to encode the parser state for transition-based parsing, inspired by stackLSTM’s global modeling of the parser state. While simple, these modifications consistently provide improvements against a normal sequence to sequence Transformer in transition-based parsing, both for dependency parsi"
2020.findings-emnlp.89,J93-2004,0,0.0707626,"Missing"
2020.findings-emnlp.89,2021.tacl-1.8,0,0.0860829,"Missing"
2020.findings-emnlp.89,P19-1451,1,0.724394,"ks, also well resourced (36K sentences). AMR1.0 has around 10K sentences and can be considered as AMR with limited train data. The dependency parsing setup followed Dyer et al. (2015), in the setting with no POS tags. This has only SHIFT, LEFT-ARC(label), and RIGHTARC(label) base action with a total of 82 different actions. Results were measured in terms of (Un)labeled Attachment Scores (UAS/LAS). The AMR setup followed Ballesteros and AlOnaizan (2017a), which introduced new actions to segment text and derive nodes or entity sub-graphs. In addition, we use the alignments and wikification from Naseem et al. (2019). Unlike previous works, we force-aligned the unaligned nodes to neighbouring words and allowed attachment to the leaf nodes of entity sub-graphs, this increased oracle Smatch from 93.7 to 98.1 and notably improved model performance. We therefore provide results for the Naseem et al. (2019) oracle for comparison. Both previous works predict a node creation action and then a node label, or call a lemmatizer if no label is found. Instead, we directly predicted the label and added COPY actions to construct node names from lemmas1 or surface words, resulting in a maximum of 9K actions. Node label"
2020.findings-emnlp.89,W18-6301,0,0.0213418,"a node creation action and then a node label, or call a lemmatizer if no label is found. Instead, we directly predicted the label and added COPY actions to construct node names from lemmas1 or surface words, resulting in a maximum of 9K actions. Node label predictions were limited to those seen during training for the word on the top of the stack. Results were measured in Smatch (Cai and Knight, 2013) using the latest version 1.0.42 . Regarding model implementation, all models were implemented on the fairseq toolkit and trained with only minor modifications over the MT model hyper-parameters (Ott et al., 2018). This used crossentropy training with learning rate 5e−4 , inverse square root scheduling with min. 1e−9 , 4000 warmup updates with learning rate 1e−7 , and maximum 3584 tokens per batch. Adam parameters 0.9 and 0.98, label smoothing was reduced to 0.013 . All models used 6 layers of encoding and decoding with size 256 and 4 attention heads, except the normal Transformers in AMR, which performed better on a 3/8 layer configuration instead of 6/6. To study the effect of model size, small versions of all models using a 2/2 configuration were also tested. 1 We used https://spacy.io/ as lemmatize"
2020.findings-emnlp.89,D19-1392,0,0.204029,"Missing"
2020.findings-emnlp.89,P11-2033,0,0.23849,"ion sequence of a state machine that produces the graph (Nivre, 2003, 2004; Kubler et al., 2009; Henderson et al., 2013). These parsers are attractive for their linear inference time and interpretability, however, their performance hinges on effective modeling of the parser state at every decision step. Parser states typically comprise two memories, a buffer and a stack, from which tokens can be pushed or popped (Kubler et al., 2009). Traditionally, parser states were modeled using hand selected local features pertaining only to the words on the top of the stack or buffer (Nivre et al., 2007; Zhang and Nivre, 2011, inter-alia). With the widespread use of neural networks, global models of the parser state such as the stack-LSTM (Dyer et al., 2015) allowed encoding the entire buffer and ∗ Miguel’s and Austin’s contributions were carried out while at IBM Research. stack. It was later shown that local features of the stack and buffer extracted from contextual word representations, such as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this"
2020.findings-emnlp.89,D17-1175,0,0.232117,"outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this work we revisit the local versus global paradigms of state modeling in the context of sequence-to-sequence Transformers applied to action prediction for transition-based parsing. Similarly to previous works for RNN sequence to sequence (Liu and Zhang, 2017; Zhang et al., 2017), we propose a modification of the cross-attention mechanism of the Transformer to provide global parser state modeling. We analyze the role of local versus global parser state modeling, stack and buffer modeling, effects model size as well as task complexity and amount of training data. Results show that local and global state modeling of the parser state yield more than 2 percentage points absolute improvement over a strong Transformer baseline, both for dependency and Abstract Meaning Representation (AMR) parsing. Gains are also particularly large for smaller train sets and smaller model si"
2020.findings-emnlp.89,W03-3017,0,0.140844,"ing systems, this work explores modifications of the sequence-to-sequence Transformer architecture to model either global or local parser states in transition-based parsing. We show that modifications of the cross attention mechanism of the Transformer considerably strengthen performance both on dependency and Abstract Meaning Representation (AMR) parsing tasks, particularly for smaller models or limited training data. 1 Introduction Transition-based Parsing transforms the task of predicting a graph from a sentence into predicting an action sequence of a state machine that produces the graph (Nivre, 2003, 2004; Kubler et al., 2009; Henderson et al., 2013). These parsers are attractive for their linear inference time and interpretability, however, their performance hinges on effective modeling of the parser state at every decision step. Parser states typically comprise two memories, a buffer and a stack, from which tokens can be pushed or popped (Kubler et al., 2009). Traditionally, parser states were modeled using hand selected local features pertaining only to the words on the top of the stack or buffer (Nivre et al., 2007; Zhang and Nivre, 2011, inter-alia). With the widespread use of neura"
2020.findings-emnlp.89,W04-0308,0,0.141912,"Missing"
2020.sustainlp-1.15,H94-1010,0,0.069331,"Missing"
2020.sustainlp-1.15,N19-1423,0,0.0184735,"ion encoding layer (e.g., a BERT model), and the classifier layer (e.g, a linear classifier). Once the source model with the highest WSS is selected, we use it as the base for transfer learning. To capture the knowledge of the source model, we use the context representation layer of the source model, but replace its classifier layer with a new classifier mapped to the target model space. We then fine-tune this new model on the target dataset. 5 5.1 our target sets. We use the full dataset as a source, and the small sampled sets as target sets. We train NER models using the method described in Devlin et al. (2019) on full source datasets, using the setup described in Section 5.4. Experimental Evaluation Datasets and Source Models We test our method on the various datasets shown in Table 1, all comprising of named entity annotated data, with different number of types as described in the lined citations. Alchemy1 and Alchemy2 are newswire datasets labeled internally with 47 and 54 types (person, organization, company, etc), respectively. Cybersecurity, the other dataset that is not cited, is a dataset of cybersecurity related articles (descriptions of virus attacks, etc.), labeled internally. For each of"
2020.sustainlp-1.15,N04-1001,0,0.272555,"Missing"
2020.sustainlp-1.15,P09-1114,0,0.109371,"Missing"
2020.sustainlp-1.15,W16-1618,0,0.034619,"Missing"
2020.sustainlp-1.15,W03-0419,0,0.552547,"Missing"
2021.acl-short.131,2020.acl-main.117,1,0.866357,"istics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 1035–1042 August 1–6, 2021. ©2021 Association for Computational Linguistics answer span from this identified paragraph. While previous MRC methods (Chen et al., 2017; Devlin et al., 2019) use ground-truth start and end span positions exclusively as training objectives when extracting answer spans from the context and consider all other positions as incorrect instances equally. However, spans that overlap with the ground-truth should be considered as partially correct. Motivated by Li et al. (2020) which proposes a new optimization criteria based on constructing prior distribution over synonyms for machine translation, we further propose to improve the above base model by considering the start and end positions of ground-truth answer spans as Gaussian-like distributions, instead of single points, and optimize our model using statistical distance. We call this final model, VAULT (VAriable Unified Long Text representation) as it can handle a variable number and lengths of paragraphs at any position with the same unified model structure to handle long texts. To evaluate the performance of"
2021.acl-short.131,2020.coling-industry.9,1,0.841803,"l structure to handle long texts. To evaluate the performance of VAULT, we select the new Natural Questions (NQ, Kwiatkowski et al., 2019) and TechQA (Castelli et al., 2020) datasets. NQ attempts to make Machine Reading Comprehension (MRC) more realistic by providing longer Wikipedia documents as contexts and real user search-engine queries as questions, and aims at avoiding observation bias: high lexical overlap between the question and the answer context which can happen frequently if the question is created after the user sees the paragraph (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Chakravarti et al., 2020; Karpukhin et al., 2020; Lee et al., 2019; Murdock et al., 2018). The task introduces the extraction of long answers (henceforth LA; typically paragraphs) besides also requiring short answers (henceforth SA) similar to SQuAD (Rajpurkar et al., 2016). In Figure 1 we examine an example from NQ along with the answers of VAULT and (Zheng et al., 2020). We see that while VAULT can extract answers from the very bottom of a page – if relevant – the existing system suffers from positional bias. It often predicts answers from the first paragraph of Wikipedia (a region which often contains the most rel"
2021.acl-short.131,P17-1171,0,0.373142,"is approach allows us to encode paragraph-level position in the text and teach the model to impute information on each paragraph into the hidden outputs for these tokens that we can exploit to determine in which paragraph the answer resides. We then predict the 1035 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 1035–1042 August 1–6, 2021. ©2021 Association for Computational Linguistics answer span from this identified paragraph. While previous MRC methods (Chen et al., 2017; Devlin et al., 2019) use ground-truth start and end span positions exclusively as training objectives when extracting answer spans from the context and consider all other positions as incorrect instances equally. However, spans that overlap with the ground-truth should be considered as partially correct. Motivated by Li et al. (2020) which proposes a new optimization criteria based on constructing prior distribution over synonyms for machine translation, we further propose to improve the above base model by considering the start and end positions of ground-truth answer spans as Gaussian-like"
2021.acl-short.131,P17-1020,0,0.0216472,"al., 2019a) adapt a span extraction model for short answer extraction. (Zheng et al., 2020; Liu et al., 2020) construct complex networks for paragraph-level representation to enhance long answer classification along with span extraction for short answers. In this work, we propose a more light-weight and parallel-efficient way for constructing paragraph-level representation and classification by using longer context and 1036 modeling the negative instance through Gaussian prior optimization. Using the hierarchical nature of a long document for question answering has been previously studied by (Choi et al., 2017), where they use a hierarchical approach to select candidate sentences and extract answers in those candidates. However, due to the limit of input length for large PLMs, existing methods (Alberti et al., 2019b; Zheng et al., 2020; Chakravarti et al., 2020) slice long documents into document pieces and perform prediction for each piece separately. In our work, we show that by modeling longer input with position-aware paragraph representation coupled with Gaussian prior optimization (which is novel for MRC), we can achieve comparable performance using much simpler architecture compared to previo"
2021.acl-short.131,P17-1055,0,0.0315606,"TechQA outperforming a standard fine-tuned model trained on a large PLM such as RoBERTa. To summarize, our contributions include: 1. We introduce a novel and effective yet simple paragraph representation. 2. We introduce soft labels to leverage information from local contexts near ground-truth during training which is novel for MRC. 3. Our model provides similar performance to a SOTA system on NQ while being 16 times faster and also effectively adapts to a new domain: TechQA. 2 Related Work Machine reading comprehension has been widely modeled as cloze-type span extraction (Chen et al., 2017; Cui et al., 2017; Devlin et al., 2019). In NQ, we need to identify answers in two levels, long and short answers. (Alberti et al., 2019a) adapt a span extraction model for short answer extraction. (Zheng et al., 2020; Liu et al., 2020) construct complex networks for paragraph-level representation to enhance long answer classification along with span extraction for short answers. In this work, we propose a more light-weight and parallel-efficient way for constructing paragraph-level representation and classification by using longer context and 1036 modeling the negative instance through Gaussian prior optimiza"
2021.acl-short.131,P17-1147,0,0.0930564,"the-art (SOTA) complex document modeling approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain – TechQA – with large improvement over a model fine-tuned on a previously published large PLM. 1 Introduction Machine Reading Comprehension (MRC) has seen great advances in recent years with the rise of pre-trained language models (PLM) (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) and public leaderboards (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Joshi et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019). While some challenges (Rajpurkar et al., 2016, 2018) focus on reading comprehension with shorter contexts, many others ∗ † Work done during an internship at IBM Research AI. Equal contributions. (Welbl et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019; Tanaka et al., 2021) focus on longer contexts that cannot fit into a typical 512 sub-token transformer window. Motivated by this, we focus on reading comprehension with long contexts. One newer approach to this task (Zheng et al., 2020) focuses on modeling document hierarchy to represent"
2021.acl-short.131,2020.emnlp-main.550,0,0.0157458,"texts. To evaluate the performance of VAULT, we select the new Natural Questions (NQ, Kwiatkowski et al., 2019) and TechQA (Castelli et al., 2020) datasets. NQ attempts to make Machine Reading Comprehension (MRC) more realistic by providing longer Wikipedia documents as contexts and real user search-engine queries as questions, and aims at avoiding observation bias: high lexical overlap between the question and the answer context which can happen frequently if the question is created after the user sees the paragraph (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Chakravarti et al., 2020; Karpukhin et al., 2020; Lee et al., 2019; Murdock et al., 2018). The task introduces the extraction of long answers (henceforth LA; typically paragraphs) besides also requiring short answers (henceforth SA) similar to SQuAD (Rajpurkar et al., 2016). In Figure 1 we examine an example from NQ along with the answers of VAULT and (Zheng et al., 2020). We see that while VAULT can extract answers from the very bottom of a page – if relevant – the existing system suffers from positional bias. It often predicts answers from the first paragraph of Wikipedia (a region which often contains the most relevant information). We e"
2021.acl-short.131,Q19-1026,0,0.151544,"approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain – TechQA – with large improvement over a model fine-tuned on a previously published large PLM. 1 Introduction Machine Reading Comprehension (MRC) has seen great advances in recent years with the rise of pre-trained language models (PLM) (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) and public leaderboards (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Joshi et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019). While some challenges (Rajpurkar et al., 2016, 2018) focus on reading comprehension with shorter contexts, many others ∗ † Work done during an internship at IBM Research AI. Equal contributions. (Welbl et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019; Tanaka et al., 2021) focus on longer contexts that cannot fit into a typical 512 sub-token transformer window. Motivated by this, we focus on reading comprehension with long contexts. One newer approach to this task (Zheng et al., 2020) focuses on modeling document hierarchy to represent multi-grained information for answer extraction"
2021.acl-short.131,P19-1612,0,0.0111992,"performance of VAULT, we select the new Natural Questions (NQ, Kwiatkowski et al., 2019) and TechQA (Castelli et al., 2020) datasets. NQ attempts to make Machine Reading Comprehension (MRC) more realistic by providing longer Wikipedia documents as contexts and real user search-engine queries as questions, and aims at avoiding observation bias: high lexical overlap between the question and the answer context which can happen frequently if the question is created after the user sees the paragraph (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Chakravarti et al., 2020; Karpukhin et al., 2020; Lee et al., 2019; Murdock et al., 2018). The task introduces the extraction of long answers (henceforth LA; typically paragraphs) besides also requiring short answers (henceforth SA) similar to SQuAD (Rajpurkar et al., 2016). In Figure 1 we examine an example from NQ along with the answers of VAULT and (Zheng et al., 2020). We see that while VAULT can extract answers from the very bottom of a page – if relevant – the existing system suffers from positional bias. It often predicts answers from the first paragraph of Wikipedia (a region which often contains the most relevant information). We evaluate our model"
2021.acl-short.131,2020.acl-main.604,0,0.119455,"ce soft labels to leverage information from local contexts near ground-truth during training which is novel for MRC. 3. Our model provides similar performance to a SOTA system on NQ while being 16 times faster and also effectively adapts to a new domain: TechQA. 2 Related Work Machine reading comprehension has been widely modeled as cloze-type span extraction (Chen et al., 2017; Cui et al., 2017; Devlin et al., 2019). In NQ, we need to identify answers in two levels, long and short answers. (Alberti et al., 2019a) adapt a span extraction model for short answer extraction. (Zheng et al., 2020; Liu et al., 2020) construct complex networks for paragraph-level representation to enhance long answer classification along with span extraction for short answers. In this work, we propose a more light-weight and parallel-efficient way for constructing paragraph-level representation and classification by using longer context and 1036 modeling the negative instance through Gaussian prior optimization. Using the hierarchical nature of a long document for question answering has been previously studied by (Choi et al., 2017), where they use a hierarchical approach to select candidate sentences and extract answers"
2021.acl-short.131,2021.ccl-1.108,0,0.0485891,"Missing"
2021.acl-short.131,P18-2124,0,0.060427,"Missing"
2021.acl-short.131,D16-1264,0,0.348374,"eve comparable performance on NQ with a state-of-the-art (SOTA) complex document modeling approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain – TechQA – with large improvement over a model fine-tuned on a previously published large PLM. 1 Introduction Machine Reading Comprehension (MRC) has seen great advances in recent years with the rise of pre-trained language models (PLM) (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) and public leaderboards (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Joshi et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019). While some challenges (Rajpurkar et al., 2016, 2018) focus on reading comprehension with shorter contexts, many others ∗ † Work done during an internship at IBM Research AI. Equal contributions. (Welbl et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019; Tanaka et al., 2021) focus on longer contexts that cannot fit into a typical 512 sub-token transformer window. Motivated by this, we focus on reading comprehension with long contexts. One newer approach to this task (Zheng et al., 2020) focuses"
2021.acl-short.131,Q18-1021,0,0.153703,"ex document modeling approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain – TechQA – with large improvement over a model fine-tuned on a previously published large PLM. 1 Introduction Machine Reading Comprehension (MRC) has seen great advances in recent years with the rise of pre-trained language models (PLM) (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) and public leaderboards (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Joshi et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019). While some challenges (Rajpurkar et al., 2016, 2018) focus on reading comprehension with shorter contexts, many others ∗ † Work done during an internship at IBM Research AI. Equal contributions. (Welbl et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019; Tanaka et al., 2021) focus on longer contexts that cannot fit into a typical 512 sub-token transformer window. Motivated by this, we focus on reading comprehension with long contexts. One newer approach to this task (Zheng et al., 2020) focuses on modeling document hierarchy to represent multi-grained inform"
2021.acl-short.131,D18-1259,0,0.172173,"NQ with a state-of-the-art (SOTA) complex document modeling approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain – TechQA – with large improvement over a model fine-tuned on a previously published large PLM. 1 Introduction Machine Reading Comprehension (MRC) has seen great advances in recent years with the rise of pre-trained language models (PLM) (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) and public leaderboards (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Joshi et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019). While some challenges (Rajpurkar et al., 2016, 2018) focus on reading comprehension with shorter contexts, many others ∗ † Work done during an internship at IBM Research AI. Equal contributions. (Welbl et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019; Tanaka et al., 2021) focus on longer contexts that cannot fit into a typical 512 sub-token transformer window. Motivated by this, we focus on reading comprehension with long contexts. One newer approach to this task (Zheng et al., 2020) focuses on modeling document hie"
2021.acl-short.131,2020.acl-main.599,1,0.563418,"rds (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Joshi et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019). While some challenges (Rajpurkar et al., 2016, 2018) focus on reading comprehension with shorter contexts, many others ∗ † Work done during an internship at IBM Research AI. Equal contributions. (Welbl et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019; Tanaka et al., 2021) focus on longer contexts that cannot fit into a typical 512 sub-token transformer window. Motivated by this, we focus on reading comprehension with long contexts. One newer approach to this task (Zheng et al., 2020) focuses on modeling document hierarchy to represent multi-grained information for answer extraction. Although this approach creates a strong representation of the text, it suffers from a significant drawback. The graph-based methods (Veliˇckovi´c et al., 2018) are inefficient on parallel hardware, such as GPUs, resulting in slow inference speed (Zhou et al., 2018; Zheng et al., 2020). Motivated by this, in this paper, we propose a reading comprehension model that addresses the above issue and uses a more light-weight, parallelefficient (i.e. efficient on parallel hardware) paragraph represent"
2021.case-1.18,2021.case-1.1,0,0.0833205,"Missing"
2021.case-1.18,P19-1409,0,0.0147027,"eferent when they refer to the same event. In this task, however, the gold event triggers are not provided; the sentences are deemed coreferent, possibly, on the basis of any of the multiple triggers that occur in the sentences being coreferent, or if the sentences are about the same general event that is occurring. Given a document, this event coreference subtask aims to create clusters of coreferent sentences. There is good variety in the research for coreference detection. Cattan et al. (2020) rely only on raw text without access to triggers or entity mentions to build coreference systems. Barhom et al. (2019) do joint entity and event extraction using a feature-based approach. Yu et al. (2020) use transformers to compute the event trigger and argument representation for the task. Following the recent work on event coreference, our system is comprised of two parts: the classification module and the clustering module. The classification module uses a binary classifier to make pair-wise binary decisions on whether two sentences are coreferent. Once all sentence pairs have been classified as coreferent or not, the clustering module clusters the “closest” sentences with each other with agglomerative cl"
2021.case-1.18,P15-1017,0,0.0228104,"el S1 S2 S3 en-dev 80.57 80.25 80.87 es-dev 64.09 - pt-dev 69.67 - Lt = − n X log(P (lwi )) (2) i=1 7.1 Table 5: CoNLL F1 score on the development sets for subtask 4: Event Extraction. in such events. The arguments are to be extracted and classified as one of the following types: time, facility, organizer, participant, place or target of the event. Formally the Event Extraction task can be summarized as follows: given a sentence s = {w1 , w2 , .., wn } and an event label set T = {t1 , t2 ..., tj }, identify contiguous phrases (ws , ..., we ) such that l(ws , .., we ) ∈ T . Most previous work (Chen et al. (2015); Nguyen et al. (2016); Nguyen and Grishman (2018)) for event extraction has treated event and argument extraction as separate tasks. But some systems (Li et al., 2013) treat the problem as structured prediction and train joint models for event triggers and arguments. Lin et al. (2020) built a joint system for many information extraction tasks including event trigger and arguments. Experiments The evaluation of the event extraction task is the CoNLL macro-F1 score. Since there is little Spanish and Portuguese data, we use it either as train in our multilingual model or as a held out developmen"
2021.case-1.18,D17-1226,0,0.0438801,"Missing"
2021.case-1.18,2020.acl-main.747,0,0.0970033,"Missing"
2021.case-1.18,N19-1423,0,0.195216,"as the training data and keep the remaining as the development data. Since there is much less data for Spanish and Portuguese, for some subtasks, such as subtask 3, we use the Spanish and Portuguese data for development only; and for subMultilingual Transformer-Based Framework For all the subtasks we use transformer-based language models (Vaswani et al., 2017) as the input text encoder. Recent studies show that deep transformer-based language models, when pretrained on a large text corpus, can achieve better generalization performance and attain state-ofthe-art performance for many NLP tasks (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020). One key success of transformer-based models is a multi-head self-attention mechanism that can model global dependencies between tokens in input and output sequences. Due to the multilingual nature of this shared task, we have applied several multilingual transformerbased language models, including multilingual BERT (mBERT) (Devlin et al., 2019), XLMRoBERTa (XLM-R) (Conneau et al., 2020), and multilingual BART (mBART) (Liu et al., 2020). Our preliminary experiments showed that XLM-R based models achieved better accuracy than other models. Hence we deci"
2021.case-1.18,2021.case-1.11,0,0.0783622,"Missing"
2021.case-1.18,D18-2012,0,0.0245053,"d task, we have applied several multilingual transformerbased language models, including multilingual BERT (mBERT) (Devlin et al., 2019), XLMRoBERTa (XLM-R) (Conneau et al., 2020), and multilingual BART (mBART) (Liu et al., 2020). Our preliminary experiments showed that XLM-R based models achieved better accuracy than other models. Hence we decided to use XLM-R as the text encoder. We use HuggingFace’s pytorch implementation of transformers (Wolf et al., 2019). XLM-R was pre-trained with unlabeled Wikipedia text and the CommonCrawl Corpus of 100 languages. It uses the SentencePiece tokenizer (Kudo and Richardson, 2018) with a vocabulary size of 250,000. Since XLM-R does not use any cross-lingual resources, it belongs to the unsupervised representation learning framework. For this work, we fine-tune the pre-trained XLM-R model on a specific task by training all layers of the model. 4 Subtask 1: Document Classification To detect protest events at the document level, the problem can be formulated as a binary text classification problem where a document is assigned label “1” if it contains one or more protest event(s) and label “0” otherwise. Various models have been developed for text classification in general"
2021.case-1.18,P13-1008,0,0.0385304,"nt Extraction. in such events. The arguments are to be extracted and classified as one of the following types: time, facility, organizer, participant, place or target of the event. Formally the Event Extraction task can be summarized as follows: given a sentence s = {w1 , w2 , .., wn } and an event label set T = {t1 , t2 ..., tj }, identify contiguous phrases (ws , ..., we ) such that l(ws , .., we ) ∈ T . Most previous work (Chen et al. (2015); Nguyen et al. (2016); Nguyen and Grishman (2018)) for event extraction has treated event and argument extraction as separate tasks. But some systems (Li et al., 2013) treat the problem as structured prediction and train joint models for event triggers and arguments. Lin et al. (2020) built a joint system for many information extraction tasks including event trigger and arguments. Experiments The evaluation of the event extraction task is the CoNLL macro-F1 score. Since there is little Spanish and Portuguese data, we use it either as train in our multilingual model or as a held out development set for our English-only model. For contextualized word embeddings, we use the XLM-R large pretrained model. The dense layer output size is same as its input size. We"
2021.case-1.18,2020.acl-main.713,0,0.0338627,"facility, organizer, participant, place or target of the event. Formally the Event Extraction task can be summarized as follows: given a sentence s = {w1 , w2 , .., wn } and an event label set T = {t1 , t2 ..., tj }, identify contiguous phrases (ws , ..., we ) such that l(ws , .., we ) ∈ T . Most previous work (Chen et al. (2015); Nguyen et al. (2016); Nguyen and Grishman (2018)) for event extraction has treated event and argument extraction as separate tasks. But some systems (Li et al., 2013) treat the problem as structured prediction and train joint models for event triggers and arguments. Lin et al. (2020) built a joint system for many information extraction tasks including event trigger and arguments. Experiments The evaluation of the event extraction task is the CoNLL macro-F1 score. Since there is little Spanish and Portuguese data, we use it either as train in our multilingual model or as a held out development set for our English-only model. For contextualized word embeddings, we use the XLM-R large pretrained model. The dense layer output size is same as its input size. We use the out-of-the-box pre-trained transformer models, and fine-tune them with the event data, updating all layers wi"
2021.case-1.18,2020.tacl-1.47,0,0.0207513,"xt corpus, can achieve better generalization performance and attain state-ofthe-art performance for many NLP tasks (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020). One key success of transformer-based models is a multi-head self-attention mechanism that can model global dependencies between tokens in input and output sequences. Due to the multilingual nature of this shared task, we have applied several multilingual transformerbased language models, including multilingual BERT (mBERT) (Devlin et al., 2019), XLMRoBERTa (XLM-R) (Conneau et al., 2020), and multilingual BART (mBART) (Liu et al., 2020). Our preliminary experiments showed that XLM-R based models achieved better accuracy than other models. Hence we decided to use XLM-R as the text encoder. We use HuggingFace’s pytorch implementation of transformers (Wolf et al., 2019). XLM-R was pre-trained with unlabeled Wikipedia text and the CommonCrawl Corpus of 100 languages. It uses the SentencePiece tokenizer (Kudo and Richardson, 2018) with a vocabulary size of 250,000. Since XLM-R does not use any cross-lingual resources, it belongs to the unsupervised representation learning framework. For this work, we fine-tune the pre-trained XLM"
2021.case-1.18,2021.ccl-1.108,0,0.067141,"Missing"
2021.case-1.18,H05-1004,0,0.142222,"tuguese and train a model with original English, translated Spanish and translated Portuguese data. The original Spanish and Portuguese data is used as the development set for model selection. return C 24 6.1 S2: This is the English-only model, trained on English data. Spanish and Portuguese are zeroshot. Experiments The evaluation of the event coreference task is based on the CoNLL coref score (Pradhan et al., 2014), which is the unweighted average of the Fscores produced by the link-based MUC (Vilain et al., 1995), the mention-based B 3 (Bagga and Baldwin, 1998), and the entity-based CEAFe (Luo, 2005) metrics. As there is little Spanish and Portuguese data, we use it as a held out development set. Our system uses XLM-R large pretrained model to obtain token and sentence representations. Pairs of sentences are concatenated to each other along with the special begin-of-sentence token and separator token as follows: BOS < si > SEP < sj > We feed the BOS token representation to the binary classification layer to obtain a probabilistic score of the two sentences being coreferent. Once Final Submissions S3: This is an English-only coreference model where the event triggers and place and time arg"
2021.case-1.18,K19-1061,0,0.0381275,"Missing"
2021.case-1.18,N16-1034,0,0.0249073,"0.57 80.25 80.87 es-dev 64.09 - pt-dev 69.67 - Lt = − n X log(P (lwi )) (2) i=1 7.1 Table 5: CoNLL F1 score on the development sets for subtask 4: Event Extraction. in such events. The arguments are to be extracted and classified as one of the following types: time, facility, organizer, participant, place or target of the event. Formally the Event Extraction task can be summarized as follows: given a sentence s = {w1 , w2 , .., wn } and an event label set T = {t1 , t2 ..., tj }, identify contiguous phrases (ws , ..., we ) such that l(ws , .., we ) ∈ T . Most previous work (Chen et al. (2015); Nguyen et al. (2016); Nguyen and Grishman (2018)) for event extraction has treated event and argument extraction as separate tasks. But some systems (Li et al., 2013) treat the problem as structured prediction and train joint models for event triggers and arguments. Lin et al. (2020) built a joint system for many information extraction tasks including event trigger and arguments. Experiments The evaluation of the event extraction task is the CoNLL macro-F1 score. Since there is little Spanish and Portuguese data, we use it either as train in our multilingual model or as a held out development set for our English-"
2021.case-1.18,P14-2006,0,0.0122876,"ach outlined in 6.1. They are: if score > t then Merge ci and cj Update C moreClusters = True S1: This is the multilingual model. To train this we translate the English training data to Spanish and Portuguese and train a model with original English, translated Spanish and translated Portuguese data. The original Spanish and Portuguese data is used as the development set for model selection. return C 24 6.1 S2: This is the English-only model, trained on English data. Spanish and Portuguese are zeroshot. Experiments The evaluation of the event coreference task is based on the CoNLL coref score (Pradhan et al., 2014), which is the unweighted average of the Fscores produced by the link-based MUC (Vilain et al., 1995), the mention-based B 3 (Bagga and Baldwin, 1998), and the entity-based CEAFe (Luo, 2005) metrics. As there is little Spanish and Portuguese data, we use it as a held out development set. Our system uses XLM-R large pretrained model to obtain token and sentence representations. Pairs of sentences are concatenated to each other along with the special begin-of-sentence token and separator token as follows: BOS < si > SEP < sj > We feed the BOS token representation to the binary classification lay"
2021.case-1.18,E99-1023,0,0.0932982,"on the development set. The system took 30 minutes to train on a V100 GPU. 7.2 Final Submission For the final submission to the shared task we explore the following variations: S1: This is the multilingual model trained with all of the English, Spanish and Portuguese training data. The development set is English only. Following the work of M’hamdi et al. (2019); Awasthy et al. (2020), we treat event extraction as a sequence labeling task. Our models are based on the stdBERT baseline in Awasthy et al. (2020), though we extract triggers and arguments at the same time. We use the IOB2 encoding (Sang and Veenstra, 1999) to represent the triggers and the argument labels, where each token is labeled with its label and an indicator of whether it starts or continues a label, or is outside the label boundary by using B-label, I-label and O respesctively. The sentence tokens are converted to token-level contextualized embeddings {h1 , h2 , .., hn }. We pass these through a classification block that is comprised of a dense linear hidden layer followed by a dropout layer, followed by a linear layer mapped to the task label space that produces labels for each token {l1 , l2 , .., ln }. The parameters of the model are"
2021.case-1.18,W15-0812,0,0.0136733,"uses on socio-political and crisis event detection. The workshop defines 3 shared tasks. In this paper we describe our models and systems developed for “Multilingual Protest News Detection - Shared Task 1” (H¨urriyeto˘glu et al., 2021a). Shared task 1 in turn has 4 subtasks: • Subtask 1 - Document Classification: determine whether a news article (document) contains information about a past or ongoing event. 1 Distribution Statement “A” (Approved for Public Release, Distribution Unlimited) Event extraction on news has long been popular, and benchmarks such as ACE (Walker et al., 2006) and ERE (Song et al., 2015) annotate event triggers, arguments and coreference. Most previous work has addressed these tasks separately. H¨urriyeto˘glu et al. (2020) also focused on detecting social-political events, but CASE 2021 has added more subtasks and languages. CASE 2021 addresses event information extraction at different granularity levels, from the coarsest-grained document level to the finestgrained token level. The workshop enables participants to build models for these subtasks and compare similar methods across the subtasks. The task is multilingual, making it even more challenging. In a globally-connected"
2021.case-1.18,Q15-1037,0,0.0130285,"020) use transformers to compute the event trigger and argument representation for the task. Following the recent work on event coreference, our system is comprised of two parts: the classification module and the clustering module. The classification module uses a binary classifier to make pair-wise binary decisions on whether two sentences are coreferent. Once all sentence pairs have been classified as coreferent or not, the clustering module clusters the “closest” sentences with each other with agglomerative clustering, using a certain threshold, a common approach for coreference detection (Yang et al. (2015); Choubey and Huang (2017); Barhom et al. (2019)). Agglomerative clustering is a popular technique for event or entity coreference resolution. At the beginning, all event mentions are assigned their own cluster. In each iteration, clusters are merged based on the average inter-cluster link similarity scores over all mentions in each cluster. The merging procedure stops when the average link similarity falls below a threshold. Formally, given a document D with n sentences {s1 , s2 , ..., sn }, our system follows the procedure outlined in Algorithm 1 while training. The input to the algorithm is"
2021.case-1.24,2021.case-1.23,0,0.202798,"such types for a domain or task a priori is challenging. Fine-grained type systems for a particular domain and task often evolve, with new, previously unseen types emerging. So some types may have many labeled examples available, some types may have few or none. In such a scenario, a flexible text classifier should be able to use training data when available, but also employ few-shot and zero-shot techniques when training data is limited or absent. 1 Distribution Statement “A” (Approved for Public Release, Distribution Unlimited) The Fine-grained Classification of Sociopolitical Events task (Haneczok et al., 2021) at the CASE 2021 workshop (H¨urriyeto˘glu et al., 2021) simulates the scenario of text classification with an evolving, fine-grained type system. There are 25 core, fine-grained event types capturing political violence, demonstrations, and other politically important events. The 25 types are from the ACLED (Armed Conflict Location & Event Data Project) event taxonomy (Raleigh et al., 2010). Copious training data exists for these types. Subtask 1 evaluates classification to these “seen” or “known” types only. Beyond the core types, subtask 2 identifies three new “unseen” types with no training"
2021.case-1.24,2020.acl-main.703,0,0.0628524,"Missing"
2021.case-1.24,2021.ccl-1.108,0,0.0930323,"Missing"
2021.case-1.24,2020.coling-main.584,0,0.0593478,"Missing"
2021.case-1.24,2021.eacl-main.20,0,0.095172,"Missing"
2021.case-1.24,N18-1101,0,0.0109587,"-purpose an NLI model for zero-shot text classification, sentence1 is the text to be classified and sentence2 is some textual representation of a type. The classification score for each type is the NLI score, which represents the extent to which the textual representation of the type is implied by sentence1. Determining implication is not just based on surface lexical overlap between the sentences. In training, the models learn encodings for both sentences, supervised by a large corpus of hand-labeled textual entailment pairs (such as the 433k sentence pairs in the multi-genre RepEval corpus (Williams et al., 2018)). 4.1 Type Representations A crucial design choice when using NLI for zeroshot text classification is the choice of representation of the types. We experimented with full English descriptions, keywords, and type names. Examples of each representation for a sample sentence appear in Table 3. The full English descriptions are the type definitions taken verbatim from source documentation. For the original 25 types, these were taken from ACLED directly. for the five unseen types, definitions were provided by the organizers. The type names were taken from the same source documentation. The keyword"
2021.case-1.24,D19-1404,0,0.0118512,"tream NLP tasks when they are fine-tuned on data specific to the task. Recently, the research community has begun to observe that PLMs fine-tuned on large amounts of data for complex end-to-end tasks can often be leveraged for new tasks without further fine-tuning. Fine-tuning PLMs on complex tasks such as Question Answering (QA) and Natural Language Inference (NLI) infuses models with high-level knowledge useful for other tasks. By choosing an appropriate task representation, QA models and NLI models can be used as “pre-tuned” models for fewshot (Schick and Sch¨utze, 2021) or even zero-shot (Yin et al., 2019) text classification. Typically, an NLI model takes two texts (sentence1 and sentence2) and predicts whether sentence1 implies sentence2, with a given confidence score. To re-purpose an NLI model for zero-shot text classification, sentence1 is the text to be classified and sentence2 is some textual representation of a type. The classification score for each type is the NLI score, which represents the extent to which the textual representation of the type is implied by sentence1. Determining implication is not just based on surface lexical overlap between the sentences. In training, the models"
2021.eacl-main.30,N19-1388,0,0.0277026,"mance over the base models. For concept alignment, we combine the proposed contextual word alignments with previously established alignment techniques utilizing matching rules tailored to AMR as well as machine translation aligners (Flanigan et al., 2014; Pourdamghani et al., 2014). For AMR parser training, we pre-train an AMR parser on the treebanks of different languages simultaneously and subsequently finetune on each language. This is analogous to the techniques used for silver data pre-training (Konstas et al., 2017; van Noord and Bos, 2017) in AMR parsing and multi-lingual pre-training (Aharoni et al., 2019) in machine translation. Finally, we conduct a detailed error analysis of the multilingual AMR parsing. One of the major errors we have found involves synonymous concepts, which share the same meaning as the original concepts in English, but differ in spellings. While this error is mainly caused by the fact that the multilingual word embeddings bridge non-English input tokens to English concepts, it also highlights the highly lexical nature of Smatch scoring (Cai and Knight, 2013) which does not take synonymous concepts into consideration. We also elaborate upon error analysis of the direct co"
2021.eacl-main.30,L18-1157,0,0.222047,"Missing"
2021.eacl-main.30,P17-1042,0,0.0187446,"2020). Word vector alignment techniques. Traditional word alignment methods often use parallel corpora and IBM alignment models (Brown et al., 1990, 1993) as well as improved versions (Och and Ney, 2003; Dyer et al., 2013). More recently, there have been an advent of techniques that align vector representation of words from varying levels of supervision (Ruder et al., 2019). Often word vectors are learned independently for each language and then a mapping from source language vectors to target language vectors with a bilingual dictionary is developed (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). To reduce the need for bilingual supervision, the iterative method of starting from a minimal seed dictionary and alternating with learning the linear map was employed by a recent body of work (Conneau et al., 2018; Schuster et al., 2019; Artetxe et al., 2018). The work most similar to ours is Cao et al. (2020) where the authors obtain contextual embedding alignments from multilingual BERT (Devlin et al., 2018; Pires et al., 2019) and subsequently improve the alignments via finetuning using supervised parallel corpora. Our contextual word alignment between two parallel sentences may be thoug"
2021.eacl-main.30,P18-1073,0,0.023089,"nt of techniques that align vector representation of words from varying levels of supervision (Ruder et al., 2019). Often word vectors are learned independently for each language and then a mapping from source language vectors to target language vectors with a bilingual dictionary is developed (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). To reduce the need for bilingual supervision, the iterative method of starting from a minimal seed dictionary and alternating with learning the linear map was employed by a recent body of work (Conneau et al., 2018; Schuster et al., 2019; Artetxe et al., 2018). The work most similar to ours is Cao et al. (2020) where the authors obtain contextual embedding alignments from multilingual BERT (Devlin et al., 2018; Pires et al., 2019) and subsequently improve the alignments via finetuning using supervised parallel corpora. Our contextual word alignment between two parallel sentences may be thought of as an adaptation of their contextual word retrieval task. However, we refrain from any finetuning of the contextual embeddings and show that the contextual word alignments from the off-the-shelf XLM-R model achieves results competitive to the word alignmen"
2021.eacl-main.30,D17-1130,0,0.0529413,"Missing"
2021.eacl-main.30,W13-2322,0,0.275064,"upervised and relies on the contextualized XLM-R word embeddings. We achieve a highly competitive performance that surpasses the best published results for German, Italian, Spanish and Chinese. 1 Figure 1: AMR graph for The boy wants to go and its German translation Der Junge will gehen. Implicit alignments between the English text and AMR concepts are denoted by dotted arrows. Explicit alignments between English and German texts are denoted by solid arrows. Introduction Abstract Meaning Representation graphs are rooted, labeled, directed, acyclic graphs representing sentence-level semantics (Banarescu et al., 2013). In the example shown in Figure 1, the sentence The boy wants to go is parsed into an AMR graph. The nodes of the AMR graph represent the AMR concepts, which may include normalized surface symbols e.g. boy, Propbank frames (Kingsbury and Palmer, 2002) e.g. want-01, go-02 as well as other AMR-specific constructs. Edges in an AMR graph represent the relations between concepts. In this example :arg0, :arg1 correspond to standard roles of Propbank. One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph and words in the sentences. Since such alignmen"
2021.eacl-main.30,P19-4007,0,0.0263166,"Missing"
2021.eacl-main.30,N18-1104,0,0.15062,"018; Chen and Palmer, 2017). A significant emphasis of this paper is on deriving these alignments for multilingual AMR parsers. Even though by nature AMR is biased towards English, recent work has evaluated the potential of AMR to work as an interlingua. Hajiˇc et al. (2014) and Xue et al. (2014) categorize and propose refinements for divergences in the annotation between English and Chinese as well as Czech AMRs. Anchiˆeta and Pardo (2018) import the corresponding AMR annotation for each sentence from the English annotated corpus and revisit the annotation to adapt it to Portuguese. However, Damonte and Cohen (2018) show that it may be possible to use the original AMR annotations devised for English as representation for equivalent sentences in other languages without any modification despite the translation divergence. This defines the problem of multilingual AMR parsing that we seek to address in this paper - given a sentence in a foreign language, recover the AMR graph originally designed for its English translation. We implement 394 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 394–404 April 19 - 23, 2021. ©2021 Association for Comp"
2021.eacl-main.30,2020.emnlp-main.195,0,0.440068,"lingual AMR parsers by projecting English AMR annotation to target foreign languages (German, Spanish, Italian and Chinese), but we depart from their approach in the specifics of the annotation projection by exploring contextual word alignments directly derived from multilingual contextualized word embeddings. While both procedures utilize parallel corpora, the annotation projection of Damonte and Cohen (2018) requires additional supervised training of their statistical word aligner. Our proposed contextualized word alignment is however unsupervised in nature. Alternatively, a recent study by Blloshmi et al. (2020) showed that one may in fact not need alignmentbased parsers for cross-lingual AMR, rather modelling concept identification as a seq2seq problem. In this paper, we will compare our results to both 395 Damonte and Cohen (2018) and Blloshmi et al. (2020). Word vector alignment techniques. Traditional word alignment methods often use parallel corpora and IBM alignment models (Brown et al., 1990, 1993) as well as improved versions (Och and Ney, 2003; Dyer et al., 2013). More recently, there have been an advent of techniques that align vector representation of words from varying levels of supervisi"
2021.eacl-main.30,N13-1073,0,0.239243,"l word aligner. Our proposed contextualized word alignment is however unsupervised in nature. Alternatively, a recent study by Blloshmi et al. (2020) showed that one may in fact not need alignmentbased parsers for cross-lingual AMR, rather modelling concept identification as a seq2seq problem. In this paper, we will compare our results to both 395 Damonte and Cohen (2018) and Blloshmi et al. (2020). Word vector alignment techniques. Traditional word alignment methods often use parallel corpora and IBM alignment models (Brown et al., 1990, 1993) as well as improved versions (Och and Ney, 2003; Dyer et al., 2013). More recently, there have been an advent of techniques that align vector representation of words from varying levels of supervision (Ruder et al., 2019). Often word vectors are learned independently for each language and then a mapping from source language vectors to target language vectors with a bilingual dictionary is developed (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). To reduce the need for bilingual supervision, the iterative method of starting from a minimal seed dictionary and alternating with learning the linear map was employed by a recent body of work (Conne"
2021.eacl-main.30,J90-2002,0,0.741955,"nd Cohen (2018) requires additional supervised training of their statistical word aligner. Our proposed contextualized word alignment is however unsupervised in nature. Alternatively, a recent study by Blloshmi et al. (2020) showed that one may in fact not need alignmentbased parsers for cross-lingual AMR, rather modelling concept identification as a seq2seq problem. In this paper, we will compare our results to both 395 Damonte and Cohen (2018) and Blloshmi et al. (2020). Word vector alignment techniques. Traditional word alignment methods often use parallel corpora and IBM alignment models (Brown et al., 1990, 1993) as well as improved versions (Och and Ney, 2003; Dyer et al., 2013). More recently, there have been an advent of techniques that align vector representation of words from varying levels of supervision (Ruder et al., 2019). Often word vectors are learned independently for each language and then a mapping from source language vectors to target language vectors with a bilingual dictionary is developed (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). To reduce the need for bilingual supervision, the iterative method of starting from a minimal seed dictionary and alternatin"
2021.eacl-main.30,2020.findings-emnlp.89,1,0.874783,"thought of as an adaptation of their contextual word retrieval task. However, we refrain from any finetuning of the contextual embeddings and show that the contextual word alignments from the off-the-shelf XLM-R model achieves results competitive to the word alignments by fast-align (see Damonte and Cohen (2018)). This suggests the potential for inexpensive, massive scaling of AMR parsing up to 100 languages on which XLM-R is trained. 3 Annotation projection We adopt a transition-based parsing approach for AMR parsing following (Ballesteros and AlOnaizan, 2017; Naseem et al., 2019; Fernandez Astudillo et al., 2020). These produce an AMR graph g from an input sentence s by predicting instead an action sequence a from s as a sequence to sequence problem. This action sequence applied to a state machine M produces then the desired target graph as g = M (a, s). Transition-based parsers require the action sequence for each graph in the training data. This is determined by a rule-based oracle a = O(g, s) which relies on external word-to-node alignments. In all the subsequent experiments we will use the oracle and action set from (Fernandez Astudillo et al., 2020). 3.1 Projection method In order to train AMR pa"
2021.eacl-main.30,J93-2003,0,0.122903,"Missing"
2021.eacl-main.30,P14-1134,0,0.595198,"h may include normalized surface symbols e.g. boy, Propbank frames (Kingsbury and Palmer, 2002) e.g. want-01, go-02 as well as other AMR-specific constructs. Edges in an AMR graph represent the relations between concepts. In this example :arg0, :arg1 correspond to standard roles of Propbank. One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph and words in the sentences. Since such alignments are essential for training many of presentday AMR parsers, there have been various efforts to link the AMR concepts to their corresponding span of words (Flanigan et al., 2014; Pourdamghani ∗ This research was done during an internship at IBM Research AI. et al., 2014; Lyu and Titov, 2018; Chen and Palmer, 2017). A significant emphasis of this paper is on deriving these alignments for multilingual AMR parsers. Even though by nature AMR is biased towards English, recent work has evaluated the potential of AMR to work as an interlingua. Hajiˇc et al. (2014) and Xue et al. (2014) categorize and propose refinements for divergences in the annotation between English and Chinese as well as Czech AMRs. Anchiˆeta and Pardo (2018) import the corresponding AMR annotation for"
2021.eacl-main.30,P13-2131,0,0.642017,"r data pre-training (Konstas et al., 2017; van Noord and Bos, 2017) in AMR parsing and multi-lingual pre-training (Aharoni et al., 2019) in machine translation. Finally, we conduct a detailed error analysis of the multilingual AMR parsing. One of the major errors we have found involves synonymous concepts, which share the same meaning as the original concepts in English, but differ in spellings. While this error is mainly caused by the fact that the multilingual word embeddings bridge non-English input tokens to English concepts, it also highlights the highly lexical nature of Smatch scoring (Cai and Knight, 2013) which does not take synonymous concepts into consideration. We also elaborate upon error analysis of the direct comparison between our proposed annotation projection method using contextual word alignment and a previous baseline, using fast align. The rest of the paper is organized as follows: In Section 2, we discuss related work. In Section 3, we present our main proposal on annotation projection based on contextual word alignments. In Section 4, we describe various combination approaches that improve the multilingual parser performances significantly. These include combining word-toconcept"
2021.eacl-main.30,W14-5808,0,0.678758,"Missing"
2021.eacl-main.30,kingsbury-palmer-2002-treebank,0,0.516216,"German translation Der Junge will gehen. Implicit alignments between the English text and AMR concepts are denoted by dotted arrows. Explicit alignments between English and German texts are denoted by solid arrows. Introduction Abstract Meaning Representation graphs are rooted, labeled, directed, acyclic graphs representing sentence-level semantics (Banarescu et al., 2013). In the example shown in Figure 1, the sentence The boy wants to go is parsed into an AMR graph. The nodes of the AMR graph represent the AMR concepts, which may include normalized surface symbols e.g. boy, Propbank frames (Kingsbury and Palmer, 2002) e.g. want-01, go-02 as well as other AMR-specific constructs. Edges in an AMR graph represent the relations between concepts. In this example :arg0, :arg1 correspond to standard roles of Propbank. One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph and words in the sentences. Since such alignments are essential for training many of presentday AMR parsers, there have been various efforts to link the AMR concepts to their corresponding span of words (Flanigan et al., 2014; Pourdamghani ∗ This research was done during an internship at IBM Resear"
2021.eacl-main.30,E17-1053,0,0.0168677,"specific constructs. Edges in an AMR graph represent the relations between concepts. In this example :arg0, :arg1 correspond to standard roles of Propbank. One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph and words in the sentences. Since such alignments are essential for training many of presentday AMR parsers, there have been various efforts to link the AMR concepts to their corresponding span of words (Flanigan et al., 2014; Pourdamghani ∗ This research was done during an internship at IBM Research AI. et al., 2014; Lyu and Titov, 2018; Chen and Palmer, 2017). A significant emphasis of this paper is on deriving these alignments for multilingual AMR parsers. Even though by nature AMR is biased towards English, recent work has evaluated the potential of AMR to work as an interlingua. Hajiˇc et al. (2014) and Xue et al. (2014) categorize and propose refinements for divergences in the annotation between English and Chinese as well as Czech AMRs. Anchiˆeta and Pardo (2018) import the corresponding AMR annotation for each sentence from the English annotated corpus and revisit the annotation to adapt it to Portuguese. However, Damonte and Cohen (2018) sh"
2021.eacl-main.30,2020.findings-emnlp.288,1,0.775662,"ed from English treebank via annotation projection - we also experiment with combining all the target language treebanks to create a single multilingual treebank. We notice that pre-training an AMR parser on this multilingual treebank with subsequent finetuning on the treebank of each language, improves performance over the parser trained only on each individual treebank. 4.3 Human and synthetic treebank combination We create a synthetic AMR corpus by parsing 85k unlabeled sentences from the context portion of SQuAD-2.0. The resulting synthetic AMR graphs are filtered as per the procedure in (Lee et al., 2020) and combined with the AMR-2.0 training set (LDC2017T10), to produce an expanded AMR2.0 + SQuAD training dataset of 94k sentences. We then project annotations of this expanded English treebank onto each of the target languages, and train the corresponding target language parser. We observe that despite the lower quality of the synthetic AMRs as compared to their human-annotated counterparts, their inclusion in the training set significantly improves parser performance. 5 5.1 Experimental Results AMR Parser and Data For our experiments, we use the stack-Transformer model (Fernandez Astudillo et"
2021.eacl-main.30,W16-1702,0,0.0156732,"d work Multilingual AMR. There have been significant advances in AMR parsing for languages other than English. Previous studies (Hajiˇc et al., 2014; Xue et al., 2014; Migueles-Abraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019) investigated AMR annotations for a variety of different languages such as Chinese, Czech, Spanish and Brazilian Portuguese. Vanderwende et al. (2015) automatically parse the logical representation for sentences in Spanish, Italian, German and Japanese, which is then converted to AMR using a small set of rules. While much of this work, along with studies such as Li et al. (2016); Anchiˆeta and Pardo (2018), produces AMR graphs whose nodes were labeled with words from the target language, Damonte and Cohen (2018) developed AMR parsers for English and used parallel corpora for annotation projection to train Italian, Spanish, German, and Chinese parsers that recover the AMR graph originally designed for the English translation. Their main results showed that the new parsers can overcome certain structural differences between languages. Similar to Damonte and Cohen (2018), we also train multilingual AMR parsers by projecting English AMR annotation to target foreign langu"
2021.eacl-main.30,P18-1037,0,0.183397,"as well as other AMR-specific constructs. Edges in an AMR graph represent the relations between concepts. In this example :arg0, :arg1 correspond to standard roles of Propbank. One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph and words in the sentences. Since such alignments are essential for training many of presentday AMR parsers, there have been various efforts to link the AMR concepts to their corresponding span of words (Flanigan et al., 2014; Pourdamghani ∗ This research was done during an internship at IBM Research AI. et al., 2014; Lyu and Titov, 2018; Chen and Palmer, 2017). A significant emphasis of this paper is on deriving these alignments for multilingual AMR parsers. Even though by nature AMR is biased towards English, recent work has evaluated the potential of AMR to work as an interlingua. Hajiˇc et al. (2014) and Xue et al. (2014) categorize and propose refinements for divergences in the annotation between English and Chinese as well as Czech AMRs. Anchiˆeta and Pardo (2018) import the corresponding AMR annotation for each sentence from the English annotated corpus and revisit the annotation to adapt it to Portuguese. However, Dam"
2021.eacl-main.30,2020.acl-main.167,1,0.895687,"Missing"
2021.eacl-main.30,L18-1486,0,0.0239282,"Missing"
2021.eacl-main.30,P19-1451,1,0.851111,"n two parallel sentences may be thought of as an adaptation of their contextual word retrieval task. However, we refrain from any finetuning of the contextual embeddings and show that the contextual word alignments from the off-the-shelf XLM-R model achieves results competitive to the word alignments by fast-align (see Damonte and Cohen (2018)). This suggests the potential for inexpensive, massive scaling of AMR parsing up to 100 languages on which XLM-R is trained. 3 Annotation projection We adopt a transition-based parsing approach for AMR parsing following (Ballesteros and AlOnaizan, 2017; Naseem et al., 2019; Fernandez Astudillo et al., 2020). These produce an AMR graph g from an input sentence s by predicting instead an action sequence a from s as a sequence to sequence problem. This action sequence applied to a state machine M produces then the desired target graph as g = M (a, s). Transition-based parsers require the action sequence for each graph in the training data. This is determined by a rule-based oracle a = O(g, s) which relies on external word-to-node alignments. In all the subsequent experiments we will use the oracle and action set from (Fernandez Astudillo et al., 2020). 3.1 Project"
2021.eacl-main.30,J03-1002,0,0.0327283,"of their statistical word aligner. Our proposed contextualized word alignment is however unsupervised in nature. Alternatively, a recent study by Blloshmi et al. (2020) showed that one may in fact not need alignmentbased parsers for cross-lingual AMR, rather modelling concept identification as a seq2seq problem. In this paper, we will compare our results to both 395 Damonte and Cohen (2018) and Blloshmi et al. (2020). Word vector alignment techniques. Traditional word alignment methods often use parallel corpora and IBM alignment models (Brown et al., 1990, 1993) as well as improved versions (Och and Ney, 2003; Dyer et al., 2013). More recently, there have been an advent of techniques that align vector representation of words from varying levels of supervision (Ruder et al., 2019). Often word vectors are learned independently for each language and then a mapping from source language vectors to target language vectors with a bilingual dictionary is developed (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). To reduce the need for bilingual supervision, the iterative method of starting from a minimal seed dictionary and alternating with learning the linear map was employed by a recent"
2021.eacl-main.30,P19-1493,0,0.0665407,"Missing"
2021.eacl-main.30,D14-1048,0,0.234354,"del. We show that our proposed procedure achieves competitive results as some of the classical methods for text-to-AMR alignment. Furthermore, such a procedure is easily scalable to the 100 languages that XLM-R is trained on. We also combine different techniques for concept alignments and AMR parser training which significantly improve performance over the base models. For concept alignment, we combine the proposed contextual word alignments with previously established alignment techniques utilizing matching rules tailored to AMR as well as machine translation aligners (Flanigan et al., 2014; Pourdamghani et al., 2014). For AMR parser training, we pre-train an AMR parser on the treebanks of different languages simultaneously and subsequently finetune on each language. This is analogous to the techniques used for silver data pre-training (Konstas et al., 2017; van Noord and Bos, 2017) in AMR parsing and multi-lingual pre-training (Aharoni et al., 2019) in machine translation. Finally, we conduct a detailed error analysis of the multilingual AMR parsing. One of the major errors we have found involves synonymous concepts, which share the same meaning as the original concepts in English, but differ in spellings"
2021.eacl-main.30,N19-1162,0,0.0225805,"there have been an advent of techniques that align vector representation of words from varying levels of supervision (Ruder et al., 2019). Often word vectors are learned independently for each language and then a mapping from source language vectors to target language vectors with a bilingual dictionary is developed (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). To reduce the need for bilingual supervision, the iterative method of starting from a minimal seed dictionary and alternating with learning the linear map was employed by a recent body of work (Conneau et al., 2018; Schuster et al., 2019; Artetxe et al., 2018). The work most similar to ours is Cao et al. (2020) where the authors obtain contextual embedding alignments from multilingual BERT (Devlin et al., 2018; Pires et al., 2019) and subsequently improve the alignments via finetuning using supervised parallel corpora. Our contextual word alignment between two parallel sentences may be thought of as an adaptation of their contextual word retrieval task. However, we refrain from any finetuning of the contextual embeddings and show that the contextual word alignments from the off-the-shelf XLM-R model achieves results competiti"
2021.eacl-main.30,W19-4028,0,0.470697,"describe various combination approaches that improve the multilingual parser performances significantly. These include combining word-toconcept alignments, using multi-lingual treebanks and combining human-annotated and synthetic treebanks. In Section 5, we discuss experimental results. In Sections 6 and 7, we present detailed error analyses. We conclude the paper in Section 8. 2 Related work Multilingual AMR. There have been significant advances in AMR parsing for languages other than English. Previous studies (Hajiˇc et al., 2014; Xue et al., 2014; Migueles-Abraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019) investigated AMR annotations for a variety of different languages such as Chinese, Czech, Spanish and Brazilian Portuguese. Vanderwende et al. (2015) automatically parse the logical representation for sentences in Spanish, Italian, German and Japanese, which is then converted to AMR using a small set of rules. While much of this work, along with studies such as Li et al. (2016); Anchiˆeta and Pardo (2018), produces AMR graphs whose nodes were labeled with words from the target language, Damonte and Cohen (2018) developed AMR parsers for English and used parallel corpora for annotation project"
2021.eacl-main.30,N15-3006,0,0.0166891,"ments, using multi-lingual treebanks and combining human-annotated and synthetic treebanks. In Section 5, we discuss experimental results. In Sections 6 and 7, we present detailed error analyses. We conclude the paper in Section 8. 2 Related work Multilingual AMR. There have been significant advances in AMR parsing for languages other than English. Previous studies (Hajiˇc et al., 2014; Xue et al., 2014; Migueles-Abraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019) investigated AMR annotations for a variety of different languages such as Chinese, Czech, Spanish and Brazilian Portuguese. Vanderwende et al. (2015) automatically parse the logical representation for sentences in Spanish, Italian, German and Japanese, which is then converted to AMR using a small set of rules. While much of this work, along with studies such as Li et al. (2016); Anchiˆeta and Pardo (2018), produces AMR graphs whose nodes were labeled with words from the target language, Damonte and Cohen (2018) developed AMR parsers for English and used parallel corpora for annotation projection to train Italian, Spanish, German, and Chinese parsers that recover the AMR graph originally designed for the English translation. Their main resu"
2021.eacl-main.30,xue-etal-2014-interlingua,0,0.600325,"Missing"
2021.emnlp-main.507,N19-1253,0,0.0173912,"structured approach that guarantees well-formed graphs and yields other desirable sub-products such as alignments. We show that this is not only possible but also attains state-of-the art parsing results without graph re-categorization. Our analysis also shows that contrary to Xu et al. (2020), vocabulary sharing is not necessary for strong performance for our structural fine-tuning. Encoding of the parser state into neural parsers has been undertaken in various works, including seq-to-seq RNN models (Liu and Zhang, 2017; Zhang et al., 2017; Buys and Blunsom, 2017), encoder-only Transformers (Ahmad et al., 2019), seq-to-seq Transformers (Astudillo et al., 2020; Zhou et al., 2021) and pre-trained language models (Qian et al., 2021). Here we explore the application of these approaches to pre-trained seq-to-seq Transformers. Borrowing ideas from Zhou et al. (2021), we encode alignment states into the pre-trained BART attention mechanism, and re-purpose its selfattention as a pointer network. We also rely on a minimal set of actions targeted to utilize BART’s generation with desirable guarantees, such as no unattachable nodes and full recovery of all graphs. We are the first to explore transition-based p"
2021.emnlp-main.507,2020.findings-emnlp.89,1,0.411053,"desirable properties. There are no structural guarantees of graph well-formedness, i.e. the model may predict strings that can not be decoded into valid graphs, and post-processing is required. Furthermore, predicting AMR linearizations ignores the implicit alignments between graph nodes and words, which provide a strong inductive bias and are useful for downstream AMR applications (Mitra and Baral, 2016; Liu et al., 2018; Vlachos et al., 2018; Kapanipathi et al., 2021; Naseem et al., 2021). On the other hand, transition-based AMR parsers (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017a; Astudillo et al., 2020; Zhou et al., 2021) operate over the tokens of the input sentence, generating the graph incrementally. They implicitly model graph structural constraints through transitions and yield alignments by construction, thus guaranteeing graph well-formedness.1 However, it remains unclear whether explicit modeling of structure is still beneficial for AMR parsing in the presence of powerful pre-trained language models and their strong free text generation abilities. In this work, we integrate pre-trained sequenceto-sequence (seq-to-seq) language models with the transition-based approach for AMR parsin"
2021.emnlp-main.507,D17-1130,0,0.0512146,"Missing"
2021.emnlp-main.507,W13-2322,0,0.0474374,"ion system with a small set of basic actions – a generalization of the action-pointer transition system of Zhou et al. (2021). We use BART (Lewis et al., 2019) as our pre-trained language model, since it has shown significant improvements in linearized AMR generation (Bevilacqua et al., 2021). Unlike previous approaches that directly fine-tune the model with The task of Abstract Meaning Representation (AMR) parsing translates a natural sentence into a rooted directed acyclic graph capturing the semantics of the sentence, with nodes representing concepts and edges representing their relations (Banarescu et al., 2013). Recent works utilizing pretrained encoder-decoder language models show great improvements in AMR parsing results (Xu et al., 2020; Bevilacqua et al., 2021). These approaches avoid explicit modeling of the graph structure. Instead, they directly predict the linearized AMR graph treated as free text. While the use 1 of pre-trained Transformer encoders is widely exWith the only exception being disconnected graphs, tended in AMR parsing, the use of pre-trained which happen infrequently in practice. 6279 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages"
2021.emnlp-main.507,P17-1112,0,0.0179208,"#8) works the best. 8 Related Work providing a more structured approach that guarantees well-formed graphs and yields other desirable sub-products such as alignments. We show that this is not only possible but also attains state-of-the art parsing results without graph re-categorization. Our analysis also shows that contrary to Xu et al. (2020), vocabulary sharing is not necessary for strong performance for our structural fine-tuning. Encoding of the parser state into neural parsers has been undertaken in various works, including seq-to-seq RNN models (Liu and Zhang, 2017; Zhang et al., 2017; Buys and Blunsom, 2017), encoder-only Transformers (Ahmad et al., 2019), seq-to-seq Transformers (Astudillo et al., 2020; Zhou et al., 2021) and pre-trained language models (Qian et al., 2021). Here we explore the application of these approaches to pre-trained seq-to-seq Transformers. Borrowing ideas from Zhou et al. (2021), we encode alignment states into the pre-trained BART attention mechanism, and re-purpose its selfattention as a pointer network. We also rely on a minimal set of actions targeted to utilize BART’s generation with desirable guarantees, such as no unattachable nodes and full recovery of all graphs"
2021.emnlp-main.507,2020.acl-main.119,0,0.0183415,"termined cases. tions, but most high performing approaches (Cai Subgraph actions have been used in all transitionand Lam, 2020; Bevilacqua et al., 2021) utilize based AMR systems (Naseem et al., 2019; Asthe re-categorization described in Appendix A.1 of tudillo et al., 2020; Zhou et al., 2021). Zhang et al. (2019a). This version requires of an external Named Entity Recognition (NER) system Aside from NER, past AMR parsers have other to anonymize named entities, both at train time external dependencies such as POS taggers (Zhang and test time. It also makes use of look-up tables et al., 2019a; Cai and Lam, 2020) and lemmatizers for nominalizations (e.g. English to England) and (Cai and Lam, 2020; Naseem et al., 2019; Astudillo other hand-crafted rules. Graph re-categorization et al., 2020). 6280 like-01 ARG 1 ARG 0 ARG 0 person ARG 1-of trip-03 ARG 1 city name name employ-01 op1 Employees Employees Employees liked 1 person 2 employ-01 RA (1, ARG 1-of) liked liked their 5 SHIFT like-01 ROOT LA (1, ARG 0) SHIFT SHIFT Boston Boston 10 city 11 name Boston Boston 13 COPY SHIFT RA (10,name) RA (11,op1) trip trip 16 trip-03 SHIFT LA (10, ARG 1) RA (5, ARG 1) LA (1, ARG 0) Figure 2: From top to bottom: graph"
2021.emnlp-main.507,P13-2131,0,0.0241183,"ess is always guaranteed and no post-processing is needed to return valid graphs, unlike Xu et al. (2020); Bevilacqua et al. (2021). The only post-processing we use is to add wikification nodes as used in all previous parsers.5 5 Experimental Setup Datasets We evaluate our models on 3 AMR benchmark datasets, namely AMR 1.0 (LDC2014T12), AMR 2.0 (LDC2017T10), and AMR 3.0 (LDC2020T02). They have around 10K, 37K, and 56K sentence-AMR pairs for training, respectively.6 Both AMR 2.0 and AMR 3.0 have wikification nodes but AMR 1.0 does not. Evaluation We assess our models with S MATCH (F1) scores7 (Cai and Knight, 2013). We also report the fine-grained evaluation metrics (Damonte et al., 2016) to further investigate different aspects of parsing results, such as concept identification, entity recognition, re-entrancies, etc. Model Configuration We follow the original BART configuration (Lewis et al., 2019) and code. 8 We use the large model configuration as default, and also the base model for ablation studies. The pointer network is always tied with one head of the decoder top layer, and the pointer loss is added to the model cross-entropy loss with 1:1 ratio for training. Transition alignments are used to m"
2021.emnlp-main.507,2020.emnlp-main.413,0,0.0324443,"iques have become dominant in seman- the new state of the art for AMR 2.0. Our results tic parsing. Xu et al. (2020) proposed custom indicate that instead of simply converting the strucmulti-task pre-training and fine-tuning approach tured data into unstructured sequences to fit the for conventional Transformer models (Vaswani need of the pre-trained model, it is possible to efet al., 2017). The massively pre-trained transformer fectively re-purpose a generic pre-trained model BART (Lewis et al., 2019) was used for executable to a structure-aware one achieving strong perforsemantic parsing in Chen et al. (2020) and AMR mance. Similar principles can be applied to adapt parsing in Bevilacqua et al. (2021). The impor- other powerful pre-trained models such as T5 (Raftance of strongly pre-trained decoders seems also fel et al., 2019) and GPT-2 (Radford et al., 2019) for justified as BART gains popularity in various se- structured data predictions. It is worth exploring mantic generation tasks (Chen et al., 2020; Shi thoroughly the pros and cons of introducing strucet al., 2020). Our work aims at capitalizing on the ture to the model compared to removing structure outstanding performance shown by BART, w"
2021.emnlp-main.507,2020.findings-emnlp.288,1,0.836348,"Missing"
2021.emnlp-main.507,2020.acl-main.703,0,0.0789857,"Missing"
2021.emnlp-main.507,W17-6315,0,0.0247298,"m BART with structure-aware fine-tuning (#8) works the best. 8 Related Work providing a more structured approach that guarantees well-formed graphs and yields other desirable sub-products such as alignments. We show that this is not only possible but also attains state-of-the art parsing results without graph re-categorization. Our analysis also shows that contrary to Xu et al. (2020), vocabulary sharing is not necessary for strong performance for our structural fine-tuning. Encoding of the parser state into neural parsers has been undertaken in various works, including seq-to-seq RNN models (Liu and Zhang, 2017; Zhang et al., 2017; Buys and Blunsom, 2017), encoder-only Transformers (Ahmad et al., 2019), seq-to-seq Transformers (Astudillo et al., 2020; Zhou et al., 2021) and pre-trained language models (Qian et al., 2021). Here we explore the application of these approaches to pre-trained seq-to-seq Transformers. Borrowing ideas from Zhou et al. (2021), we encode alignment states into the pre-trained BART attention mechanism, and re-purpose its selfattention as a pointer network. We also rely on a minimal set of actions targeted to utilize BART’s generation with desirable guarantees, such as no unatt"
2021.emnlp-main.507,2021.emnlp-main.714,0,0.0506102,"Missing"
2021.emnlp-main.507,P18-1037,0,0.0206507,"hown to pipelines that are hard to analyze and generalize hurt performance notably on the AMR 2.0 corpus. poorly. This situation has notably improved in the Subgraph actions (Ballesteros and Al-Onaizan, past few years but there are still two main sources 2017b) are used in transition-based systems and of complexity present in almost all recent parsers: play a role similar to re-categorization. Instead of graph re-categorization and subgraph actions. normalizing and reverting, transition-based parsers Graph re-categorization (Wang and Xue, 2017; apply a subgraph action that generates an entire Lyu and Titov, 2018; Zhang et al., 2019a) normal- subgraph at once. This subgraph action coincides with many of the subgraphs collapsed in izes the graph prior to learning. This includes re-categorization. Subgraph actions bring howjoining certain subgraphs such as entities, dates ever fewer external dependencies, since the parser and other constructs into single nodes, removing special types of nodes like polarity and normal- learns to segment and identify subgraphs during training. They still suffer however from data sparizing propbank names. An example of common sity since some subgraphs appear very few times"
2021.emnlp-main.507,P19-1451,1,0.907229,"bles for nominalization and similar pens in this re-categorized space. Re-categorized constructs that hinder generalization. Furthermore, graphs are expanded to normal valid AMR graphs they create the problem of unattachable nodes. This in a post-processing stage. The type and number of subgraphs normalized vary across implementa- was addressed in Zhou et al. (2021) by ignoring subgraphs for a set of heuristically determined cases. tions, but most high performing approaches (Cai Subgraph actions have been used in all transitionand Lam, 2020; Bevilacqua et al., 2021) utilize based AMR systems (Naseem et al., 2019; Asthe re-categorization described in Appendix A.1 of tudillo et al., 2020; Zhou et al., 2021). Zhang et al. (2019a). This version requires of an external Named Entity Recognition (NER) system Aside from NER, past AMR parsers have other to anonymize named entities, both at train time external dependencies such as POS taggers (Zhang and test time. It also makes use of look-up tables et al., 2019a; Cai and Lam, 2020) and lemmatizers for nominalizations (e.g. English to England) and (Cai and Lam, 2020; Naseem et al., 2019; Astudillo other hand-crafted rules. Graph re-categorization et al., 2020)"
2021.emnlp-main.507,N19-4009,0,0.0242225,"n of 4 steps. Learning rate is 1e−4 with 4000 warm-up steps using the inversesqrt scheduling scheme (Vaswani et al., 2017). The hyper-parameters are fixed and not tuned for different models and datasets, as we found results are not sensitive within small ranges. We train sep-voc models for 100 epochs and joint-voc models for 40 epochs as the latter is found to converge faster. The best 5 checkpoints based on development set S MATCH from greedy decoding are averaged, and default beam size of 10 is used for decoding for our final parsing scores. We implement our model9 with the FAIRSEQ toolkit (Ott et al., 2019). More details can be found in the Appendix. 6 Results Main Results We present parsing performances of our model (StructBART) in comparison with previous approaches in Table 1. For each model, we also list its features such as utilization of pre9 Code and model available at https://github.com/ IBM/transition-amr-parser. 6284 Features Transition System Astudillo et al. (2020) Zhou et al. (2021) Ours Model Results on AMR 2.0 APT∗ Model Results on AMR 3.0 #Base Actions Distant Edges Special Subgraph (Zhou et al., 2021) StructBART sep-voc APT∗ (Zhou et al., 2021) StructBART sep-voc 12 10 6 S WAP p"
2021.emnlp-main.507,2021.acl-long.289,1,0.692068,"that this is not only possible but also attains state-of-the art parsing results without graph re-categorization. Our analysis also shows that contrary to Xu et al. (2020), vocabulary sharing is not necessary for strong performance for our structural fine-tuning. Encoding of the parser state into neural parsers has been undertaken in various works, including seq-to-seq RNN models (Liu and Zhang, 2017; Zhang et al., 2017; Buys and Blunsom, 2017), encoder-only Transformers (Ahmad et al., 2019), seq-to-seq Transformers (Astudillo et al., 2020; Zhou et al., 2021) and pre-trained language models (Qian et al., 2021). Here we explore the application of these approaches to pre-trained seq-to-seq Transformers. Borrowing ideas from Zhou et al. (2021), we encode alignment states into the pre-trained BART attention mechanism, and re-purpose its selfattention as a pointer network. We also rely on a minimal set of actions targeted to utilize BART’s generation with desirable guarantees, such as no unattachable nodes and full recovery of all graphs. We are the first to explore transition-based parsing applied on fine-tuning strongly pre-trained seq-toseq models, and we demonstrate that parser state encoding is sti"
2021.emnlp-main.507,D18-1548,0,0.0542695,"Missing"
2021.emnlp-main.507,D18-1086,0,0.0524736,"Missing"
2021.emnlp-main.507,D17-1129,0,0.0203499,", resulting in complex sults without re-categorization, but this is shown to pipelines that are hard to analyze and generalize hurt performance notably on the AMR 2.0 corpus. poorly. This situation has notably improved in the Subgraph actions (Ballesteros and Al-Onaizan, past few years but there are still two main sources 2017b) are used in transition-based systems and of complexity present in almost all recent parsers: play a role similar to re-categorization. Instead of graph re-categorization and subgraph actions. normalizing and reverting, transition-based parsers Graph re-categorization (Wang and Xue, 2017; apply a subgraph action that generates an entire Lyu and Titov, 2018; Zhang et al., 2019a) normal- subgraph at once. This subgraph action coincides with many of the subgraphs collapsed in izes the graph prior to learning. This includes re-categorization. Subgraph actions bring howjoining certain subgraphs such as entities, dates ever fewer external dependencies, since the parser and other constructs into single nodes, removing special types of nodes like polarity and normal- learns to segment and identify subgraphs during training. They still suffer however from data sparizing propbank names"
2021.emnlp-main.507,N15-1040,0,0.0245526,"et al., 2021). These approaches however lack certain desirable properties. There are no structural guarantees of graph well-formedness, i.e. the model may predict strings that can not be decoded into valid graphs, and post-processing is required. Furthermore, predicting AMR linearizations ignores the implicit alignments between graph nodes and words, which provide a strong inductive bias and are useful for downstream AMR applications (Mitra and Baral, 2016; Liu et al., 2018; Vlachos et al., 2018; Kapanipathi et al., 2021; Naseem et al., 2021). On the other hand, transition-based AMR parsers (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017a; Astudillo et al., 2020; Zhou et al., 2021) operate over the tokens of the input sentence, generating the graph incrementally. They implicitly model graph structural constraints through transitions and yield alignments by construction, thus guaranteeing graph well-formedness.1 However, it remains unclear whether explicit modeling of structure is still beneficial for AMR parsing in the presence of powerful pre-trained language models and their strong free text generation abilities. In this work, we integrate pre-trained sequenceto-sequence (seq-to-seq) langua"
2021.emnlp-main.507,2020.emnlp-main.196,0,0.319656,"(Lewis et al., 2019) as our pre-trained language model, since it has shown significant improvements in linearized AMR generation (Bevilacqua et al., 2021). Unlike previous approaches that directly fine-tune the model with The task of Abstract Meaning Representation (AMR) parsing translates a natural sentence into a rooted directed acyclic graph capturing the semantics of the sentence, with nodes representing concepts and edges representing their relations (Banarescu et al., 2013). Recent works utilizing pretrained encoder-decoder language models show great improvements in AMR parsing results (Xu et al., 2020; Bevilacqua et al., 2021). These approaches avoid explicit modeling of the graph structure. Instead, they directly predict the linearized AMR graph treated as free text. While the use 1 of pre-trained Transformer encoders is widely exWith the only exception being disconnected graphs, tended in AMR parsing, the use of pre-trained which happen infrequently in practice. 6279 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6279–6290 c November 7–11, 2021. 2021 Association for Computational Linguistics have-03 linearized graphs, we modify the model str"
2021.emnlp-main.507,P19-1009,0,0.0356805,"Missing"
2021.emnlp-main.507,D19-1392,0,0.032939,"Missing"
2021.emnlp-main.507,D17-1175,0,0.0254667,"-aware fine-tuning (#8) works the best. 8 Related Work providing a more structured approach that guarantees well-formed graphs and yields other desirable sub-products such as alignments. We show that this is not only possible but also attains state-of-the art parsing results without graph re-categorization. Our analysis also shows that contrary to Xu et al. (2020), vocabulary sharing is not necessary for strong performance for our structural fine-tuning. Encoding of the parser state into neural parsers has been undertaken in various works, including seq-to-seq RNN models (Liu and Zhang, 2017; Zhang et al., 2017; Buys and Blunsom, 2017), encoder-only Transformers (Ahmad et al., 2019), seq-to-seq Transformers (Astudillo et al., 2020; Zhou et al., 2021) and pre-trained language models (Qian et al., 2021). Here we explore the application of these approaches to pre-trained seq-to-seq Transformers. Borrowing ideas from Zhou et al. (2021), we encode alignment states into the pre-trained BART attention mechanism, and re-purpose its selfattention as a pointer network. We also rely on a minimal set of actions targeted to utilize BART’s generation with desirable guarantees, such as no unattachable nodes and fu"
2021.emnlp-main.507,2021.naacl-main.443,1,0.0878221,"ere are no structural guarantees of graph well-formedness, i.e. the model may predict strings that can not be decoded into valid graphs, and post-processing is required. Furthermore, predicting AMR linearizations ignores the implicit alignments between graph nodes and words, which provide a strong inductive bias and are useful for downstream AMR applications (Mitra and Baral, 2016; Liu et al., 2018; Vlachos et al., 2018; Kapanipathi et al., 2021; Naseem et al., 2021). On the other hand, transition-based AMR parsers (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017a; Astudillo et al., 2020; Zhou et al., 2021) operate over the tokens of the input sentence, generating the graph incrementally. They implicitly model graph structural constraints through transitions and yield alignments by construction, thus guaranteeing graph well-formedness.1 However, it remains unclear whether explicit modeling of structure is still beneficial for AMR parsing in the presence of powerful pre-trained language models and their strong free text generation abilities. In this work, we integrate pre-trained sequenceto-sequence (seq-to-seq) language models with the transition-based approach for AMR parsing, and explore to wh"
2021.naacl-main.443,P18-1130,0,0.280775,"leading to very long action sequences. This in turn affects both a model’s ability to learn and its decoding speed. In this work, we propose the Action-Pointer Transition (APT) system which combines the advantages of both the transition-based approaches and more general graph-generation approaches. We focus on predicting an action sequence that can build the graph from a source sentence. The core idea is to put the target action sequence to a dual use – as a mechanism for graph generation as well as the representation of the graph itself. Inspired by recent progress in pointer-based parsers (Ma et al., 2018a; Fernández-González and GómezRodríguez, 2020), we replace the stack and buffer by a cursor that moves from left to right and introduce a pointer network (Vinyals et al., 2015) as mechanism for edge creation. Unlike previous works, we use the pointer mechanism on the target side, pointing to past node generation actions to create edges. This eliminates the node generation and attachment restrictions of previous transitionbased parsers. It is also more natural for graph generation, essentially resembling the generation process in the graph-based approaches, but keeping the graph and source ali"
2021.naacl-main.443,1981.tc-1.7,0,0.712125,"Missing"
2021.naacl-main.443,P19-1451,1,0.86871,"gle model yields the second best S MATCH score on AMR 2.0 (81.8), which is further improved to 83.4 with silver data and ensemble decoding. ARG0 boy New ARG1 ARG0 go-02 York op1 ARG4 city name op2 name Figure 1: AMR graph expressing the meaning of the sentence The boy wants to go to New York. actions given the sentence. These actions generate the graph while processing tokens left-to-right through the sentence and store intermediate representations in memories such as stack and buffer (Wang et al., 2015; Damonte et al., 2016; Ballesteros and Al-Onaizan, 2017; Vilares and GómezRodríguez, 2018; Naseem et al., 2019; Astudillo et al., 2020; Lee et al., 2020). General graph-based approaches, on the other hand, directly predict nodes and edges in sequential order from graph traversals such as breath first search or depth first search (Zhang et al., 2019a,b; Cai and Lam, 2019, 2020). While not modeling the local semantic correspondence between graph nodes and source tokens, the approaches achieve strong results without restrictions of transition-based approaches, but often require graph re-categorization, a form of graph normalization, for optimal performance. The strong left-to-right constraint of transiti"
2021.naacl-main.443,N19-4009,0,0.0288532,"ith β1 of 0.9 and β2 of 0.98 for training. Each data batch has 3584 maximum number of tokens, and the learning rate schedule is the same as Vaswani et al. (2017), where we use the maximum learning rate of 5e−4 with 4000 warm-up steps. We use a dropout rate of 0.3 and label smoothing rate of 0.01. We train all the models for a maximum number of 120 epochs, and average the best 5 epoch checkpoints among the last 40 checkpoints based on the S MATCH scores on the development data with greedy decoding. We use a default beam size of 10 for decoding. We implement our model5 with the FAIRSEQ toolkit (Ott et al., 2019). All models are trained and tested on a single Nvidia Titan RTX GPU. Training takes about 10 hours on AMR 2.0 and 3.5 hours on AMR 1.0. 6 6.1 Results and Analysis Main Results Oracle Actions Table 1 compares the oracle data S MATCH and average action sequence length on the AMR 2.0 training set among recent transition systems. Our approach yields much shorter action sequences due to the target-side pointing mechanism. It has also the best coverage on training AMR graphs, due to the flexibility of our transitions that can capture the majority of graph components. We chose not to tackle a number"
2021.naacl-main.443,P18-1171,0,0.0170287,"tions in cross-attention rent node action v for these steps, as illustrated in heads CAm () with m = 1 · · · M at every decoder the input actions in Figure 4. At each intermediate layer. We mask one head of the cross-attention to step τ ∈ [t, t + k], 2 decoder self-attention heads see only the aligned source token at ct , and aug- SAm () are restricted to only attend to the direct ment it with another head masked to see only po- graph neighbors of the current node, represented by sitions &gt; ct . This is similar to the hard attention previous nodes at positions pt , pt+1 , · · · , pτ as well in Peng et al. (2018) and parser state encoding in as the current position τ . This essentially builds Astudillo et al. (2020). sub-sequences of node representations with richer As in prior works, we restrict the output space of graph information step by step, and we use the last our model to only allow valid actions given x, y&lt;t . reference of the same node for pointing positions The restriction is not only enforced at inference, but when generating new edges. Moreover, when prop2 agating this masking pattern along m layers, each Each of these are wrapped around with residual, dropout and layer normalization oper"
2021.naacl-main.443,D14-1048,0,0.03227,"APH (), COPY or PRED () actions. Each node prediction action is followed by edge creation actions. Edges connecting to closer nodes This oracle is expected to work with generic are generated before the farther ones. When multi- AMR aligners. For this work, we use the alignple connected nodes are aligned to one token, they ments generation method of Astudillo et al. (2020), are traversed in pre-order for node generation. A which generates many-to-many alignments. It is a detailed description of oracle algorithm is given in combination of Expectation Maximization based Appendix B. alignments of Pourdamghani et al. (2014) and rule base alignments of Flanigan et al. (2014). Any reThe use of a cursor variable ct decouples node maining unaligned nodes are aligned based on their reference from source tokens, allowing to produce graph proximity to unaligned tokens. For more demultiple nodes and edges (see Figure 3), even the entire AMR graph if necessary, from a single token. tails, we refer the reader to the works of Astudillo et al. (2020) and Naseem et al. (2019). This provides more expressiveness and flexibility than previous transition-based AMR parsers, while keeping a strong inductive bias. The only restric-"
2021.naacl-main.443,D15-1136,0,0.0522611,"Missing"
2021.naacl-main.443,N18-2023,0,0.0277415,"d alignments. Compared to the stack-Transformer (Astudillo et al., 2020), we propose the use of an action pointing mechanism to decouple word and node representation, 8 Conclusion remove the need for stack and buffer and model graph structure on the decoder side. We show that We present an Action-Pointer mechanism that can these improvements yield superior performance naturally handle the generation of arbitrary graph while exploiting the same inductive biases with constructs, including re-entrancies and multiple little train data or small models. nodes per token. Our structural modeling with Vilares and Gómez-Rodríguez (2018) proposed incremental encoding of parser and graph states an AMR - CONVINGTON system for unrestricted non- based on a single Transformer architecture proves projective AMR parsing, comparing the current to be highly effective, obtaining the best results word with all previous words for arc attachment on all AMR corpora among models with similar as we propose. However, their comparison is done learnable parameter sizes. An interesting future with sequential actions whereas we use an efficient exploration is on combining our system with large pointer mechanism to parallelize the process. pre-tra"
2021.naacl-main.443,D17-1129,0,0.196891,"Missing"
2021.naacl-main.443,N15-1040,0,0.092499,"best transition-based AMR parser in very similar conditions. While using no graph re-categorization, our single model yields the second best S MATCH score on AMR 2.0 (81.8), which is further improved to 83.4 with silver data and ensemble decoding. ARG0 boy New ARG1 ARG0 go-02 York op1 ARG4 city name op2 name Figure 1: AMR graph expressing the meaning of the sentence The boy wants to go to New York. actions given the sentence. These actions generate the graph while processing tokens left-to-right through the sentence and store intermediate representations in memories such as stack and buffer (Wang et al., 2015; Damonte et al., 2016; Ballesteros and Al-Onaizan, 2017; Vilares and GómezRodríguez, 2018; Naseem et al., 2019; Astudillo et al., 2020; Lee et al., 2020). General graph-based approaches, on the other hand, directly predict nodes and edges in sequential order from graph traversals such as breath first search or depth first search (Zhang et al., 2019a,b; Cai and Lam, 2019, 2020). While not modeling the local semantic correspondence between graph nodes and source tokens, the approaches achieve strong results without restrictions of transition-based approaches, but often require graph re-categori"
2021.naacl-main.443,2020.emnlp-main.196,0,0.743403,"B,G Astudillo et al. (2020)∗ R 67.1 66.0 68.1 68.3 ±0.4 70.2 ±0.1 71.3 ±0.1 74.0 75.4 76.9 ±0.1 Lee et al. (2020)R (85K silver) 78.2 ±0.1 APT smallR APT baseR 78.2 / 78.2 ±0.0 78.5 / 78.3 ±0.1 APT smallR p.e. APT baseR p.e. 79.7 79.8 Van Noord and Bos (2017) Groschwitz et al. (2018)G Lyu and Titov (2018)G Cai and Lam (2019) Lindemann et al. (2019) Naseem et al. (2019)B Zhang et al. (2019a)B,G Zhang et al. (2019b)B,G Cai and Lam (2020)B Cai and Lam (2020)B,G Astudillo et al. (2020)∗ R Bevilacqua et al. (2021)= 71.0 71.0 74.4 ±0.2 73.2 75.3 ±0.1 75.5 76.3 ±0.1 77.0 ±0.1 78.7 80.2 80.2 ±0.0 83.8 Xu et al. (2020) (4M silver) Lee et al. (2020)R (85K silver) Bevilacqua et al. (2021)= (200K silver) 80.2 81.3 ±0.0 84.3 APT smallR APT baseR 81.7 / 81.5 ±0.2 81.8 / 81.7 ±0.1 APT smallR p.e. APT baseR p.e. APT baseR (70K Silver) APT baseR (70K Silver) p.e. 82.5 82.8 82.8 / 82.6 ±0.1 83.4 Lyu et al. (2020) Bevilacqua et al. (2021)= 75.8 83.0 APT baseR APT baseR p.e. 80.4 / 80.3 ±0.1 81.2 Table 2: S MATCH scores on AMR 1.0, 2.0, and 3.0 test sets. APT is our model. B or R indicates pretrained BERT or RoBERTa embeddings, G use of graph re-categorization, ∗ improved results reported in Lee et al. (2020). = denot"
2021.naacl-main.443,P19-1009,0,0.0949982,"Missing"
2021.naacl-main.443,D19-1392,0,0.0837753,"Missing"
2021.naacl-main.443,D17-1175,0,0.156946,"hu et al., 2019), etc. Most of these works model graph structure in the encoder since the complete source sentence and graph are known. We embed a dynamic graph in the Transformer decoder during parsing. This is similar to broad graph generation approaches (Li et al., 2018) relying on graph neural networks (Li et al., 2019), but our approach is much more efficient as we do not require heavy re-computation of node representations. With the exception of Astudillo et al. (2020), other works introducing stack and buffer information into sequence-to-sequence attention parsers (Liu and Zhang, 2017; Zhang et al., 2017; Buys and Blunsom, 2017), are based on RNNs and do not attain high performances. Liu and Zhang (2017); Zhang et al. (2017) tackle dependency parsing and propose modified attention mechanisms while Buys and Blunsom (2017) predicts semantic graphs jointly with their alignments and compares stackbased with latent and fixed alignments. Compared to the stack-Transformer (Astudillo et al., 2020), we propose the use of an action pointing mechanism to decouple word and node representation, 8 Conclusion remove the need for stack and buffer and model graph structure on the decoder side. We show that We"
2021.naacl-main.443,P19-1503,1,0.862028,"Missing"
2021.naacl-main.443,D19-1548,0,0.0270962,"., 2017). They also model inductive biases indirectly through graph re-categorization, detailed in Section 6.1, which requires a name entity recognition system at test time. Re-categorization was proposed in Lyu and Titov (2018), which reformulated alignments as a differentiable permutation problem, interpretable as another form of inductive bias. Finally, augmenting seq-to-seq models with graph structures has been explored in various NLP areas, including machine translation (Hashimoto and Tsuruoka, 2017; Moussallem et al., 2019), text classification (Lu et al., 2020), AMR to text generation (Zhu et al., 2019), etc. Most of these works model graph structure in the encoder since the complete source sentence and graph are known. We embed a dynamic graph in the Transformer decoder during parsing. This is similar to broad graph generation approaches (Li et al., 2018) relying on graph neural networks (Li et al., 2019), but our approach is much more efficient as we do not require heavy re-computation of node representations. With the exception of Astudillo et al. (2020), other works introducing stack and buffer information into sequence-to-sequence attention parsers (Liu and Zhang, 2017; Zhang et al., 20"
C10-1062,A97-1024,0,0.0490301,"o predict readability: syntactic features, language-model features, and lexical features, as described below. 5.1 We want to answer the question whether a machine can accurately estimate readability as judged by a human. Therefore, we built a machine-learning system that predicts the readFeatures Based on Syntax Many times, a document is found to be unreadable due to unusual linguistic constructs or ungram548 1 http://www.cs.waikato.ac.nz/ml/weka/ matical language that tend to manifest themselves in the syntactic properties of the text. Therefore, syntactic features have been previously used (Bernth, 1997) to gauge the “clarity” of written text, with the goal of helping writers improve their writing skills. Here too, we use several features based on syntactic analyses. Syntactic analyses are obtained from the Sundance shallow parser (Riloff and Phillips, 2004) and from the English Slot Grammar (ESG) (McCord, 1989). Sundance features: The Sundance system is a rule-based system that performs a shallow syntactic analysis of text. We expect that this analysis over readable text would be “well-formed”, adhering to grammatical rules of the English language. Deviations from these rules can be indicati"
C10-1062,N04-1025,0,0.76587,"ability. The evaluation was then designed to compare how well machine and naive human judges predict expert human judgements. In order to make the machine’s predicted score comparable to a human judge’s score (details about our evaluation metrics are in Section 6.1), we also restricted the machine scores to integers. Hence, the task is to predict an integer score from 1 to 5 that measures the readability of the document. This task could be modeled as a multi-class classification problem treating each integer score as a separate class, as done in some of the previous work (Si and Callan, 2001; Collins-Thompson and Callan, 2004). However, since the classes are numerical and not unrelated (for example, the score 2 is in between scores 1 and 3), we decided to model the task as a regression problem and then round the predicted score to obtain the closest integer value. Preliminary results verified that regression performed better than classification. Heilman et al. (2008) also found that it is better to treat the readability scores as ordinal than as nominal. We take the average of the expert judge scores for each document as its goldstandard score. Regression was also used by Kanungo and Orr (2009), although their eval"
C10-1062,N07-1058,0,0.796875,"he documents. Some later methods use pre-determined lists of words to determine the grade level of a document, for example the Lexile measure (Stenner et al., 1988), the Fry Short Passage measure (Fry, 1990) and the Revised Dale-Chall formula (Chall and Dale, 1995). The word lists these methods use may be thought of as very simple language models. More recently, language models have been used for predicting the grade level of documents. Si and Callan (2001) and CollinsThompson and Callan (2004) train unigram language models to predict grade levels of documents. In addition to language models, Heilman et al. (2007) and Schwarm and Ostendorf (2005) also use some syntactic features to estimate the grade level of texts. Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. Their system predicts readability of texts from Wall Street Journal using lexical, syntactic and discourse features. Kanungo and Orr (2009) consider the task of predicting readability of web summary snippets produced by search engines. Using simple surface level features like the number of characters and syllables per word, capitalization, punctuation, ellipses etc. they train a re"
C10-1062,P05-1065,0,0.720084,"ethods use pre-determined lists of words to determine the grade level of a document, for example the Lexile measure (Stenner et al., 1988), the Fry Short Passage measure (Fry, 1990) and the Revised Dale-Chall formula (Chall and Dale, 1995). The word lists these methods use may be thought of as very simple language models. More recently, language models have been used for predicting the grade level of documents. Si and Callan (2001) and CollinsThompson and Callan (2004) train unigram language models to predict grade levels of documents. In addition to language models, Heilman et al. (2007) and Schwarm and Ostendorf (2005) also use some syntactic features to estimate the grade level of texts. Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. Their system predicts readability of texts from Wall Street Journal using lexical, syntactic and discourse features. Kanungo and Orr (2009) consider the task of predicting readability of web summary snippets produced by search engines. Using simple surface level features like the number of characters and syllables per word, capitalization, punctuation, ellipses etc. they train a regression model to predict readabi"
C10-1062,P96-1041,0,0.0157412,"ed features in categorizing text (McCallum and Nigam, 1998; Yang and Liu, 1999) and evaluating readability (Collins-Thompson and Callan, 2004; Heilman et al., 2007) has been investigated in previous work. In our experiments, however, since documents were acquired through several different channels, such as machine translation or web logs, 549 we also build models that try to predict the genre of a document. Since the genre information for many English documents is readily available, we trained a series of genre-specific 5-gram LMs using the modified Kneser-Ney smoothing (Kneser and Ney, 1995; Stanley and Goodman, 1996). Table 1 contains a list of a base LM and genrespecific LMs. Given a document D consisting of tokenized word sequence {wi : i = 1, 2, · · · , |D|}, its perplexity L(D|Mj ) with respect to a LM Mj is computed as: ¢ ¡ 1 P|D| − |D| i=1 log P (wi |hi ;Mj ) , (2) L(D|Mj ) = e where |D |is the number of words in D and hi are the history words for wi , and P (wi |hi ; Mj ) is the probability Mj assigns to wi , when it follows the history words hi . Posterior perplexities from genre-specific language models: While perplexities computed from genre-specific LMs reflect the absolute probability that a d"
C10-1062,W08-0909,0,0.545949,"score from 1 to 5 that measures the readability of the document. This task could be modeled as a multi-class classification problem treating each integer score as a separate class, as done in some of the previous work (Si and Callan, 2001; Collins-Thompson and Callan, 2004). However, since the classes are numerical and not unrelated (for example, the score 2 is in between scores 1 and 3), we decided to model the task as a regression problem and then round the predicted score to obtain the closest integer value. Preliminary results verified that regression performed better than classification. Heilman et al. (2008) also found that it is better to treat the readability scores as ordinal than as nominal. We take the average of the expert judge scores for each document as its goldstandard score. Regression was also used by Kanungo and Orr (2009), although their evaluation did not constrain machine scores to be integers. We tested several regression algorithms available in the Weka1 machine learning package, and in Section 6.2 we report results for several which performed best. The next section describes the numerically-valued features that we used as input for regression. 5 Features for Predicting Readabil"
C10-1062,W02-1028,0,0.00647861,"retations. Sometimes ESG’s grammar rules fail to produce a single complete interpretation of a sentence, in which case it generates partial parses. This typically happens in cases when sentences are ungrammatical, and possibly, less readable. Thus, we use the proportion of such incomplete parses within a document as a readability feature. In case of extremely short documents, this proportion of incomplete parses can be misleading. To account for such short documents, we introduce a variation of the above incomplete parse feature, by weighting it with a log factor as was done in (Riloff, 1996; Thelen and Riloff, 2002). We also experimented with some other syntactic features such as average sentence parse scores from Stanford parser and an in-house maximum entropy statistical parer, average constituent scores etc., however, they slightly degraded the performance in combination with the rest of the features and hence we did not include them in the final set. One possible explanation could be that averaging diminishes the effect of low scores caused by ungrammaticality. 5.2 Features Based on Language Models A probabilistic language model provides a prediction of how likely a given sentence was generated by th"
C10-1062,D08-1020,0,0.778707,"enner et al., 1988), the Fry Short Passage measure (Fry, 1990) and the Revised Dale-Chall formula (Chall and Dale, 1995). The word lists these methods use may be thought of as very simple language models. More recently, language models have been used for predicting the grade level of documents. Si and Callan (2001) and CollinsThompson and Callan (2004) train unigram language models to predict grade levels of documents. In addition to language models, Heilman et al. (2007) and Schwarm and Ostendorf (2005) also use some syntactic features to estimate the grade level of texts. Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. Their system predicts readability of texts from Wall Street Journal using lexical, syntactic and discourse features. Kanungo and Orr (2009) consider the task of predicting readability of web summary snippets produced by search engines. Using simple surface level features like the number of characters and syllables per word, capitalization, punctuation, ellipses etc. they train a regression model to predict readability values. Our work differs from this previous research in several ways. Firstly, the task we h"
D08-1063,P06-1067,0,0.131069,"7 The ACE data has the nice property of being consistent in annotations across these languages. 603 4 Cross-Language Mention Propagation The approach proposed in this article requires a mention detection system build in a resource-rich language, and a translation from the source language to the resource-rich language, together with word alignment. This assumption is realistic: while truly parallel data (humanly created) might be in short supply or harder to acquire, adapting statistical machine translation (SMT) systems from one language-pair to another is not as challenging as it used to be (Al-Onaizan and Papineni, 2006). We also find that there is a large number of parallel corpora available these days which cover many language pairs. For example, for the European Union’s 23 official languages we find 253 language pairs; each document in one language might have to be translated in all other 22 languages. This is in addition to parallel corpora one could get from books, including religious texts such as the Bible, that are translated to a large number of languages. On the other hand, even though mention detection system is important for many natural language processing applications, we still find lack of ment"
D08-1063,J96-1002,0,0.0347385,"ed in the source language, but, still, even when all resources were used, a statistically significant gain was still observed. Similarly to classical NLP tasks such as text chunking (Ramshaw and Marcus, 1995) and named entity recognition (Tjong Kim Sang, 2002), we formulate mention detection as a sequence classification problem, by assigning a label to each token in the text, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. The classification is performed with a statistical approach, built around the maximum entropy (MaxEnt) principle (Berger et al., 1996), that has the advantage of combining arbitrary types of information in making a classification decision. 2 Previous Work There are several investigations in literature that explore using parallel corpora to transfer information content from one language (most of the time English) to another. The earliest investigations of the subject have been performed, on word sense disambiguation (Dagan et al., 1991; P.F.Brown et al., 1991; Gale et al., 1992) (perhaps unsurprisingly given its close connection to machine translation) – all propose and (lightly) evaluate methods to use word sense information"
D08-1063,J94-4003,0,0.0337083,"us Work There are several investigations in literature that explore using parallel corpora to transfer information content from one language (most of the time English) to another. The earliest investigations of the subject have been performed, on word sense disambiguation (Dagan et al., 1991; P.F.Brown et al., 1991; Gale et al., 1992) (perhaps unsurprisingly given its close connection to machine translation) – all propose and (lightly) evaluate methods to use word sense information extracted from the target language to help the sense resolution in the source language and machine translation. (Dagan and Itai, 1994) explicitly suggests performing word sense disambiguation in the target language (English in the article) with the goal of resolving ambiguity in the source language (Hebrew), and show moderate improvement on a small data set3 . More recently, (Diab and Resnik, 2001) presents a method for performing word sense tagging in both the source and target texts of parallel bilingual corpora with the English WordNet sense inventory, by using translation correspondences. On more general cross-language information transfer, (Yarowsky et al., 2001) proposed and evaluated a method of propagating POS taggin"
D08-1063,P91-1017,0,0.0485664,"a specific mention, is inside a specific mention, or is outside any mentions. The classification is performed with a statistical approach, built around the maximum entropy (MaxEnt) principle (Berger et al., 1996), that has the advantage of combining arbitrary types of information in making a classification decision. 2 Previous Work There are several investigations in literature that explore using parallel corpora to transfer information content from one language (most of the time English) to another. The earliest investigations of the subject have been performed, on word sense disambiguation (Dagan et al., 1991; P.F.Brown et al., 1991; Gale et al., 1992) (perhaps unsurprisingly given its close connection to machine translation) – all propose and (lightly) evaluate methods to use word sense information extracted from the target language to help the sense resolution in the source language and machine translation. (Dagan and Itai, 1994) explicitly suggests performing word sense disambiguation in the target language (English in the article) with the goal of resolving ambiguity in the source language (Hebrew), and show moderate improvement on a small data set3 . More recently, (Diab and Resnik, 2001) pre"
D08-1063,N04-1001,0,0.36118,"g various degrees of existent resources in the source language (Arabic, Chinese, Spanish) • the information transfer is performed over machine generated translations and alignments. 3 Mention Detection As mentioned in the introduction, the mention detection problem is formulated as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. Good performance in many natural language processing tasks has been shown to depend heavily on integrating many sources of information (Florian et al., 2004).4 Given this observation, we are interested in algorithms that can easily integrate and make effective use of diverse input types. We select a exponential classifier, the Maximum Entropy (MaxEnt henceforth) classifier that integrates arbitrary types of information and makes a classification decision by aggregating all information available for a given classification. But the reader can replace it with her favorite feature-based classifier throughout the paper. To help with the presentation, we introduce some notations: let Y = {y1 , . . . , yn } be the set of predicted classes, X be the examp"
D08-1063,P02-1002,0,0.117848,"with the features (fj )i , and computes the probability distribution as m 1 Y fj (x,yi ) αij , (1) P (yi |x) = Z(x) j=1 X Y fj (x,yi ) Z(x) = αij i j 4 In fact, the feature set used for classification has a much larger impact on the performance of the resulting system than the classifier method itself. 602 where Z(x) is a normalization factor. The {αij }j=1...m weights are estimated during the training phase to maximize the likelihood of the data (Berger et al., 1996). In this paper, the MaxEnt model is trained using the sequential conditional generalized iterative scaling (SCGIS) technique (Goodman, 2002), and it uses a Gaussian prior for regularization (Chen and Rosenfeld, 2000). Now take xN 1 = (x1 , x2 , . . . xN ), a sequence of contiguous tokens (i.e., a sentence or a document) in the source language. The goal of mention detection system is to find the most likely sequence of labels y1N = (y1 , y2 . . . yN ) that best matches the input xN 1 . In the mention detection case, each token xi in xN 1 is tagged with a label yi as follows:5 • if it’s not part of any entity, yi = O (O for “outside any mentions”) • if it is part of an entity, it is composed of a subtag specifying whether it starts"
D08-1063,1998.amta-tutorials.5,0,0.018029,"in which Chinese dependency parses were induced from English, and show that a parser trained on the resulting trees outperformed simple baselines. (Cabezas et al., 2001) investigates a similar method of propagating syntactic treebank-like annotations from English to Spanish. Finally, a large body of research has been done on cross-language information retrieval, where the goal is to find information in one language (e.g. Chinese newswire) corresponding to a query in a different language (e.g. English) – although the list of relevant papers is too long to be mentioned here (see, for instance, (Grefenstette, 1998)). The work presented here differs from the information extraction investigations presented above in two aspects: 2 While applying this method in the case where the source language has absolutely no resources might be an interesting test case, we don’t see it as being realistic. Resources are build nowadays in a large variety of languages, and not making use of them is rather foolish (a certain big bird and sand comes to mind). 601 • it handles unrestricted text and a full set of 3 Very small by “modern” standards - 137 examples. Probably because at the time the article was written, there were"
D08-1063,D07-1029,0,0.0125077,"h as WordNet (Miller, 1995) and the output 605 of a larger set of information extraction models. 7 Experiments To show the effectiveness of cross-language mention propagation information in improving mention detection system performance in Arabic, Chinese and Spanish, we use three SMT systems with very competitive performance in terms of BLEU 11 (Papineni et al., 2002). To give an idea of the SMT performance, Table 3 shows the performance of the translation systems on the three language pairs, computed on standard test sets. The Arabic to English SMT system is similar to the one described in (Huang and Papineni, 2007); it has 0.55 BLEU score on NIST 2003 Arabic-English machine translation evaluation test set. The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). This system obtains a score of 0.32 cased BLUE on NIST 2003 Arabic-English machine translation evaluation test set. The Spanish to English SMT system is similar to the one described in (Lee et al., 2006); it has a 0.55 BLEU score on the final text edition of the European Parliament Plenary Speech corpus in TC-STAR 2006 evaluation. As mentioned earlier, these three SMT systems have very c"
D08-1063,W03-1026,1,0.872775,"Missing"
D08-1063,koen-2004-pharaoh,0,0.0123905,"any languages. In the approach we propose below, the annotated corpus used to train the mention detection classifier does not have to be part of a parallel corpus. To start the process, we first use a SMT system to translate the source unit (document or sentence) xN 1 into the resource-rich language, yielding the sequence ξ1M = (ξ1 , ξ2 , . . . ξM ). Taking the sequence of tokens ξ1M as input, the MaxEnt classifier assigns a mention label to each token, building the label sequence ψ1M = (ψ1 , ψ2 . . . ψM ). Using the SMTproduced word alignment between source text xN 1 and translated text ξ1M (Koehn, 2004),we propagate the target labels ψ1M to the source language building the label sequence y˜1N = (˜ y1 , y˜2 . . . y˜N ).8 As an example, if a sequence of tokens in the resourcerich language ξi ξ i+1 ξi+2 is aligned to xj xj+1 in the source language and if ξi ξi+1 ξi+2 is tagged as a location mention, then the sequence xj xj+1 can be labeled as a location mention: B-LOC, I-LOC. Hence, each token xi in xN 1 is tagged with a corresponding  propagated label y˜i in y˜1N , y˜i = φ i, A, ψ1M , where A is the alignment between the source and resourcerich languages. In cases when the alignment is 1to-1"
D08-1063,P03-1051,0,0.0901102,"Missing"
D08-1063,P02-1040,0,0.0968223,"em has a better performance when compared to systems dealing with other languages such as Arabic, Chinese and Spanish. These results are not unexpected since the English model has access to a larger training data and uses richer set of information such as WordNet (Miller, 1995) and the output 605 of a larger set of information extraction models. 7 Experiments To show the effectiveness of cross-language mention propagation information in improving mention detection system performance in Arabic, Chinese and Spanish, we use three SMT systems with very competitive performance in terms of BLEU 11 (Papineni et al., 2002). To give an idea of the SMT performance, Table 3 shows the performance of the translation systems on the three language pairs, computed on standard test sets. The Arabic to English SMT system is similar to the one described in (Huang and Papineni, 2007); it has 0.55 BLEU score on NIST 2003 Arabic-English machine translation evaluation test set. The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). This system obtains a score of 0.32 cased BLUE on NIST 2003 Arabic-English machine translation evaluation test set. The Spanish to Engli"
D08-1063,P91-1034,0,0.121254,"nside a specific mention, or is outside any mentions. The classification is performed with a statistical approach, built around the maximum entropy (MaxEnt) principle (Berger et al., 1996), that has the advantage of combining arbitrary types of information in making a classification decision. 2 Previous Work There are several investigations in literature that explore using parallel corpora to transfer information content from one language (most of the time English) to another. The earliest investigations of the subject have been performed, on word sense disambiguation (Dagan et al., 1991; P.F.Brown et al., 1991; Gale et al., 1992) (perhaps unsurprisingly given its close connection to machine translation) – all propose and (lightly) evaluate methods to use word sense information extracted from the target language to help the sense resolution in the source language and machine translation. (Dagan and Itai, 1994) explicitly suggests performing word sense disambiguation in the target language (English in the article) with the goal of resolving ambiguity in the source language (Hebrew), and show moderate improvement on a small data set3 . More recently, (Diab and Resnik, 2001) presents a method for perfo"
D08-1063,W94-0111,0,0.0174446,"d Markov   asj−1 N sumption that the probability P yˆj |x1 , yˆ1 only depends on the previous k classifications. This model is similar to the MEMM model (McCallum et al., 2000), but it does not separate the probability into generation probabilities and transition probabilities, and, crucially, has access to “future” observed features (i.e. it can examine the entire xN 1 sequence, though in practice it will only examine some small part of it) – which is one way of eliminating label 5 The mention encoding is the IOB2 encoding presented in (Tjong Kim Sang and Veenstra, 1999) and introduced by (Ramshaw and Marcus, 1994) for base noun phrase chunking. bias observed by (Lafferty et al., 2001).6 The experiments are run on four languages, part of the ACE-2007 evaluation (NIST, 2007): Arabic, Chinese, English and Spanish.7 Systems across the languages use a large range of features, including lexical (words and morphs in a 3-word window, prefixes and suffixes of length up to 4 characters, WordNet (Miller, 1995) for English), syntactic (POS tags, text chunks), and the output of other information extraction models. These features were described in (Florian et al., 2004), and are not discussed here. In this paper we"
D08-1063,W95-0107,0,0.0238667,"this hypothesis, we conducted experiments on systems build with a varied amount of resources in the receiving language, starting with the case where there are none2 (all information is transferred through translation alignment), and ending with the case where we used all the resources we could gather for that language. The experiments will show that the gain in performance decreases with the amount of resources used in the source language, but, still, even when all resources were used, a statistically significant gain was still observed. Similarly to classical NLP tasks such as text chunking (Ramshaw and Marcus, 1995) and named entity recognition (Tjong Kim Sang, 2002), we formulate mention detection as a sequence classification problem, by assigning a label to each token in the text, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. The classification is performed with a statistical approach, built around the maximum entropy (MaxEnt) principle (Berger et al., 1996), that has the advantage of combining arbitrary types of information in making a classification decision. 2 Previous Work There are several investigations in literature that explore using"
D08-1063,C02-1070,0,0.0241888,"uage information transfer, (Yarowsky et al., 2001) proposed and evaluated a method of propagating POS tagging, named mention, base noun phrase, and morphological information from English into a foreign language, which is very similar to the one presented in this article (experiments were run on French, Chinese, Czech, and Spanish – on human-generated translations). Their results show a significant improvement in performance while building an automatic classifier on the projected annotations over the same automatic classifier trained on a small amount of annotated data in the source language. (Riloff et al., 2002) extends the ideas in (Yarowsky et al., 2001), by showing how it can be used, in conjunction with an automatically trained information extraction system on the source language, to bootstrap the annotation of resources in the target language. They show that they can obtain 48 F-measure on a information extraction task identifying locations, vehicles and victims in plane crashes. (Hwa et al., 2002) proposes a framework that enables the acquisition of syntactic dependency trees for low-resource languages by importing linguistic annotation from rich-resource languages (English). The authors run a"
D08-1063,E99-1023,0,0.0238103,"rm of the type (2). We also used the standard Markov   asj−1 N sumption that the probability P yˆj |x1 , yˆ1 only depends on the previous k classifications. This model is similar to the MEMM model (McCallum et al., 2000), but it does not separate the probability into generation probabilities and transition probabilities, and, crucially, has access to “future” observed features (i.e. it can examine the entire xN 1 sequence, though in practice it will only examine some small part of it) – which is one way of eliminating label 5 The mention encoding is the IOB2 encoding presented in (Tjong Kim Sang and Veenstra, 1999) and introduced by (Ramshaw and Marcus, 1994) for base noun phrase chunking. bias observed by (Lafferty et al., 2001).6 The experiments are run on four languages, part of the ACE-2007 evaluation (NIST, 2007): Arabic, Chinese, English and Spanish.7 Systems across the languages use a large range of features, including lexical (words and morphs in a 3-word window, prefixes and suffixes of length up to 4 characters, WordNet (Miller, 1995) for English), syntactic (POS tags, text chunks), and the output of other information extraction models. These features were described in (Florian et al., 2004),"
D08-1063,W02-2024,0,0.0376911,"varied amount of resources in the receiving language, starting with the case where there are none2 (all information is transferred through translation alignment), and ending with the case where we used all the resources we could gather for that language. The experiments will show that the gain in performance decreases with the amount of resources used in the source language, but, still, even when all resources were used, a statistically significant gain was still observed. Similarly to classical NLP tasks such as text chunking (Ramshaw and Marcus, 1995) and named entity recognition (Tjong Kim Sang, 2002), we formulate mention detection as a sequence classification problem, by assigning a label to each token in the text, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. The classification is performed with a statistical approach, built around the maximum entropy (MaxEnt) principle (Berger et al., 1996), that has the advantage of combining arbitrary types of information in making a classification decision. 2 Previous Work There are several investigations in literature that explore using parallel corpora to transfer information content fro"
D08-1063,H01-1035,0,0.305355,"esolution in the source language and machine translation. (Dagan and Itai, 1994) explicitly suggests performing word sense disambiguation in the target language (English in the article) with the goal of resolving ambiguity in the source language (Hebrew), and show moderate improvement on a small data set3 . More recently, (Diab and Resnik, 2001) presents a method for performing word sense tagging in both the source and target texts of parallel bilingual corpora with the English WordNet sense inventory, by using translation correspondences. On more general cross-language information transfer, (Yarowsky et al., 2001) proposed and evaluated a method of propagating POS tagging, named mention, base noun phrase, and morphological information from English into a foreign language, which is very similar to the one presented in this article (experiments were run on French, Chinese, Czech, and Spanish – on human-generated translations). Their results show a significant improvement in performance while building an automatic classifier on the projected annotations over the same automatic classifier trained on a small amount of annotated data in the source language. (Riloff et al., 2002) extends the ideas in (Yarowsk"
D08-1063,W05-0709,1,0.924677,"Missing"
D08-1063,P02-1033,0,\N,Missing
D08-1063,W03-0419,0,\N,Missing
D10-1033,A97-1029,0,0.0257025,"rror-trade-off (DET) (Martin et al., 1997) analysis, in addition to traditional precision/recall/F measure. This paper is organized as follows. Section 2 discusses previous work. Section 3 describes the baseline maximum-entropy-based MD system. Section 4 introduces enhancements to the system to achieve robustness. Section 5 describes databases used for experiments, which are discussed in Section 6, and Section 7 draws conclusions and plots future work. 2 Previous work on mention detection The MD task has close ties to named-entity recognition, which has been the focus of much recent research (Bikel et al., 1997; Borthwick et al., 1998; Tjong Kim Sang, 2002; Florian et al., 2003; Benajiba et al., 2009), and has been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computationallinguistics literature, a named entity represents an instance of either a location, a person, an organization, and the named-entity-recognition task consists of identifying each individual occurrence of names of such an entity appearing in the text. As stated earlier, in this paper we are interested in identification and classification of textual references to object/abstractio"
D10-1033,W98-1118,0,0.0589296,"(Martin et al., 1997) analysis, in addition to traditional precision/recall/F measure. This paper is organized as follows. Section 2 discusses previous work. Section 3 describes the baseline maximum-entropy-based MD system. Section 4 introduces enhancements to the system to achieve robustness. Section 5 describes databases used for experiments, which are discussed in Section 6, and Section 7 draws conclusions and plots future work. 2 Previous work on mention detection The MD task has close ties to named-entity recognition, which has been the focus of much recent research (Bikel et al., 1997; Borthwick et al., 1998; Tjong Kim Sang, 2002; Florian et al., 2003; Benajiba et al., 2009), and has been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computationallinguistics literature, a named entity represents an instance of either a location, a person, an organization, and the named-entity-recognition task consists of identifying each individual occurrence of names of such an entity appearing in the text. As stated earlier, in this paper we are interested in identification and classification of textual references to object/abstraction mentions, which can be"
D10-1033,J92-1002,0,0.049749,"genres of data, including machine-translation output, informal communications, mixed-language material, varied forms of non-standard database mark-up, etc. We somewhatarbitrarily choose to employ three classifiers as described below. We select a classifier based on a sentence-level determination of the material’s fit to the target language. First, we build an n-gram language model on clean target-language training text. This language model is used to compute the perplexity (P P ) of each sentence during decoding. The P P indicates the quality of the text in the targetlanguage (i.e. English) (Brown et al., 1992); the lower the P P , the cleaner the text. A sentence with a P P lower than a threshold θ1 is considered “clean” and hence the “clean” baseline MD model described in Section 3 is used to detect mentions of this sentence. The clean MD model has access to standard features described in Section 3.1. In the case where a sentence looks particularly badly matched to the target language, defined as P P &gt; θ2 , we use a “gazetteer-based” model based on a dictionary look-up to detect mentions; we retreat to seeking known mentions in a context-independent manner reflecting that most of the context consi"
D10-1033,W03-0425,1,0.82552,"Missing"
D10-1033,N04-1001,0,0.135199,"t a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. We also assign to every nonoutside label a class to specify entity type e.g. person, organization, location, etc. We are interested in a statistical approach that can easily be adapted for several languages and that has the ability to integrate easily and make effective use of diverse sources of information to achieve high system performance. This is because, similar to many NLP tasks, good performance has been shown to depend heavily on integrating many sources of information (Florian et al., 2004). We choose a Maximum Entropy Markov Model (MEMM) as described previously (Florian et al., 2004; Zitouni and Florian, 2009). The maximum-entropy model is trained using the sequential conditional generalized iterative scaling (SCGIS) technique (Goodman, 2002), and it uses a Gaussian prior for regularization (Chen and Rosenfeld, 2000)1 . 3.1 Mention detection: standard features The featues used by our mention detection systems can be divided into the following categories: 1. Lexical Features Lexical features are implemented as token n-grams spanning the current token, both preceding and followin"
D10-1033,P06-1060,1,0.70183,"t rather is a maximum-a-posteriori model. 338 The gazetteers consist of several class of dictionaries: including person names, country names, company names, etc. Dictionaries contain single names such as John or Boston, and also phrases such as Barack Obama, New York City, or The United States. During both training and decoding, when we encounter in the text a token or a sequence of tokens that completely matches an entry in a dictionary, we fire its corresponding class. The use of this framework to build MD systems for clean English text has given very competitive results at ACE evaluations (Florian et al., 2006). Trying other classifiers is always a good experiment, which we didn’t pursue here for two reasons: first, the MEMM system used here is state-of-the-art, as proven in evaluations and competitions – while it is entirely possible that another system might get better results, we don’t think the difference would be large. Second, we are interested in ways of improving performance on noisy data, and we expect any system to observe similar degradation in performance when presented with unexpected input – showing results for multiple classifier types might very well dilute the message, so we stuck t"
D10-1033,P02-1002,0,0.0149425,"approach that can easily be adapted for several languages and that has the ability to integrate easily and make effective use of diverse sources of information to achieve high system performance. This is because, similar to many NLP tasks, good performance has been shown to depend heavily on integrating many sources of information (Florian et al., 2004). We choose a Maximum Entropy Markov Model (MEMM) as described previously (Florian et al., 2004; Zitouni and Florian, 2009). The maximum-entropy model is trained using the sequential conditional generalized iterative scaling (SCGIS) technique (Goodman, 2002), and it uses a Gaussian prior for regularization (Chen and Rosenfeld, 2000)1 . 3.1 Mention detection: standard features The featues used by our mention detection systems can be divided into the following categories: 1. Lexical Features Lexical features are implemented as token n-grams spanning the current token, both preceding and following it. For a token xi , token n-gram features will contain the previous n−1 tokens (xi−n+1 , . . . xi−1 ) and the following n − 1 tokens (xi+1 , . . . xi+n−1 ). Setting n equal to 3 turned out to be a good choice. 2. Gazetteer-based Features The gazetteerbase"
D10-1033,A00-1044,0,0.0348788,"ch can be either named, nominal or pronominal. This task has been a focus of interest in ACE since 2003. The recent ACE evaluation campaign was in 2008. Effort to handle noisy data is still limited, especially for scenarios in which the system at decoding time does not have prior knowledge of the input data source. Previous work dealing with unstructured data assumes the knowledge of the input data source. As an example, E. Minkov et al. (Minkov et al., 2005) assume that the input data is text from e-mails, and define special features to enhance the detection of named entities. Miller et al. (Miller et al., 2000) assume that the input data is the output of a speech or optical character recognition system, and hence extract new features for better named-entity recognition. In a different research problem, L. Yi et al. eliminate the noisy text from the document before performing data mining (Yi et al., 2003). Hence, they do not try to process noisy data; instead, they remove it. The approach we propose in this paper does not assume prior knowledge of the data source. Also we do not want to eliminate the noisy data, but rather attempt to detect the appropriate mentions, if any, that appear in that portio"
D10-1033,H05-1056,0,0.0173144,"n the text. As stated earlier, in this paper we are interested in identification and classification of textual references to object/abstraction mentions, which can be either named, nominal or pronominal. This task has been a focus of interest in ACE since 2003. The recent ACE evaluation campaign was in 2008. Effort to handle noisy data is still limited, especially for scenarios in which the system at decoding time does not have prior knowledge of the input data source. Previous work dealing with unstructured data assumes the knowledge of the input data source. As an example, E. Minkov et al. (Minkov et al., 2005) assume that the input data is text from e-mails, and define special features to enhance the detection of named entities. Miller et al. (Miller et al., 2000) assume that the input data is the output of a speech or optical character recognition system, and hence extract new features for better named-entity recognition. In a different research problem, L. Yi et al. eliminate the noisy text from the document before performing data mining (Yi et al., 2003). Hence, they do not try to process noisy data; instead, they remove it. The approach we propose in this paper does not assume prior knowledge o"
D10-1033,W02-2024,0,0.195402,"n addition to traditional precision/recall/F measure. This paper is organized as follows. Section 2 discusses previous work. Section 3 describes the baseline maximum-entropy-based MD system. Section 4 introduces enhancements to the system to achieve robustness. Section 5 describes databases used for experiments, which are discussed in Section 6, and Section 7 draws conclusions and plots future work. 2 Previous work on mention detection The MD task has close ties to named-entity recognition, which has been the focus of much recent research (Bikel et al., 1997; Borthwick et al., 1998; Tjong Kim Sang, 2002; Florian et al., 2003; Benajiba et al., 2009), and has been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computationallinguistics literature, a named entity represents an instance of either a location, a person, an organization, and the named-entity-recognition task consists of identifying each individual occurrence of names of such an entity appearing in the text. As stated earlier, in this paper we are interested in identification and classification of textual references to object/abstraction mentions, which can be either named, nominal"
D10-1033,D08-1063,1,0.868288,"Section 3.1. The classifier for “mixed”-quality data and the “gazetteer” model were each trained on that set plus the “Latin” training set and the supplemental set. In addition, “mixed” training included the additional features described in Section 4.5. The framework used to build the baseline MD system is similar to the one we used in the ACE evaluation2 . This system has achieved competitive results with an F -measure of 82.7 when trained on the seven main types of ACE data with access to wordnet and part-of-speech-tag information as well as output of other MD and named-entity recognizers (Zitouni and Florian, 2008). It is instructive to evaluate on the individual component systems as well as the combination, despite the fact that the individual components are not wellsuited to all the data sets, for example, the mixed and gazetteer systems being a poorer fit to the English task than the baseline, and vice versa for the 2 NIST’s ACE evaluation http://www.nist.gov/speech/tests/ace/index.htm plan: age animal award cardinal disease event event-award event-communication event-crime event-custody event-demonstration event-disaster event-legal event-meeting event-performance event-personnel event-sports event-"
D10-1033,W03-0419,0,\N,Missing
D16-1135,I08-1071,0,0.0194876,"t annotating multilingual NER data by human is both expensive and time-consuming. Richman and Schone (2008) utilize the category information of Wikipedia to determine the entity type of an entity based on manually constructed rules (e.g., category phrase “Living People” is mapped to entity type PERSON). Such a rule-based entity type mapping is limited both in accuracy and coverage, e.g., (Toral and Muoz, 2006). Nothman et al. (2013) train a Wikipedia entity type classifier using human-annotated Wikipedia pages. Such a supervised-learning based approach has better accuracy and coverage, e.g., (Dakka and Cucerzan, 2008). A number of heuristic rules are developed in both works to label the Wikipedia text to create weakly annotated NER training data. The NER systems trained with the weakly annotated data may achieve similar accuracy compared with systems trained with little human-annotated data (e.g., up to 40K tokens as in (Richman and Schone, 2008)), but they are still significantly worse than well-trained systems (e.g., a drop of 23.9 F1 score on the CoNLL data and a drop of 19.6 F1 score on the BBN data as in (Nothman et al., 2013)). In this paper, we propose a new class of approaches that utilize Wikipedi"
D16-1135,N04-1001,0,0.0205108,"s applied to a new domain (3.6 F1 score improvement on political party articles/English NER) or it is trained with little training data (18.3 F1 score improvement on Japanese NER). We organize the paper as follows. We describe how to build English Wikipedia entity type mapping in Section 2 and extend it to multilingual mappings in Section 3. We present several Wikipedia-based approaches for improving NER systems in Section 4 and evaluate their performance in Section 5. We conclude the paper in Section 6. 2 English Wikipedia Entity Type Mapping We have developed an in-house English NER system (Florian et al., 2004). The system has 51 entity types, and the main motivation of deploying such a fine-grained entity type set is to build cognitive question answering applications on top of the NER system. An important check for a question answering system is the capability to detect whether a particular answer matches the expected type derived from the question. The entity type system used in this paper has been engineered to cover many of the frequent types that are targeted by naturallyphrased questions (such as PERSON, ORGANIZATION, GPE, TITLEWORK, FACILITY, EVENT, DATE, TIME, LOCATION, etc), and it was crea"
D16-1135,D11-1072,0,0.0388986,"Missing"
D16-1135,D07-1073,0,0.0386904,"which happens more frequently for unseen entities as we have observed in our experiments. Wikipedia is an open-access, free-content Internet encyclopedia, which has become the de facto on-line source for general reference. A Wikipedia page about an entity normally includes both structured information and unstructured text information, and such information can be used to help determine the entity type of the referred entity. So far there are two classes of approaches that exploit Wikipedia to improve NER. The first class of approaches use Wikipedia to generate features for NER systems, e.g., (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Radford et al., 2015). Kazama and Torisawa (2007) try to find the Wikipedia entity for each candidate word sequence and then extract a category label from the first sentence of the Wikipedia entity page. A part-of-speech (POS) tagger is used to extract the category label 1 An entity is an unseen entity if it does not appear in the training data used to train the NER model. 1275 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1275–1284, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics feat"
D16-1135,N16-1030,0,0.00594662,"o pre-defined entity types such as persons, organizations, geopolitical entities, locations, events, etc. NER is a fundamental component of many information extraction and knowledge discovery applications, including relation extraction, entity linking, question answering and data mining. The state-of-the-art NER systems are usually statistical machine learning models that are trained with human-annotated data. Popular models include maximum entropy Markov models (MEMM) (McCallum et al., 2000), conditional random fields (CRF) (Lafferty et al., 2001) and neural networks (Collobert et al., 2011; Lample et al., 2016). Such models have strong generalization capability to recognize unseen entities1 based on lexical and contextual information (features). However, a model could still make mistakes if its features favor a wrong entity type, which happens more frequently for unseen entities as we have observed in our experiments. Wikipedia is an open-access, free-content Internet encyclopedia, which has become the de facto on-line source for general reference. A Wikipedia page about an entity normally includes both structured information and unstructured text information, and such information can be used to hel"
D16-1135,D15-1058,0,0.0513951,"as we have observed in our experiments. Wikipedia is an open-access, free-content Internet encyclopedia, which has become the de facto on-line source for general reference. A Wikipedia page about an entity normally includes both structured information and unstructured text information, and such information can be used to help determine the entity type of the referred entity. So far there are two classes of approaches that exploit Wikipedia to improve NER. The first class of approaches use Wikipedia to generate features for NER systems, e.g., (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Radford et al., 2015). Kazama and Torisawa (2007) try to find the Wikipedia entity for each candidate word sequence and then extract a category label from the first sentence of the Wikipedia entity page. A part-of-speech (POS) tagger is used to extract the category label 1 An entity is an unseen entity if it does not appear in the training data used to train the NER model. 1275 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1275–1284, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics features in the training and decoding phase. Ratino"
D16-1135,W09-1119,0,0.408591,"tly for unseen entities as we have observed in our experiments. Wikipedia is an open-access, free-content Internet encyclopedia, which has become the de facto on-line source for general reference. A Wikipedia page about an entity normally includes both structured information and unstructured text information, and such information can be used to help determine the entity type of the referred entity. So far there are two classes of approaches that exploit Wikipedia to improve NER. The first class of approaches use Wikipedia to generate features for NER systems, e.g., (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Radford et al., 2015). Kazama and Torisawa (2007) try to find the Wikipedia entity for each candidate word sequence and then extract a category label from the first sentence of the Wikipedia entity page. A part-of-speech (POS) tagger is used to extract the category label 1 An entity is an unseen entity if it does not appear in the training data used to train the NER model. 1275 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1275–1284, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics features in the training and"
D16-1135,P08-1001,0,0.0660846,"English NER system. Both approaches, however, are language-dependent because (Kazama and Torisawa, 2007) requires a POS tagger and (Ratinov and Roth, 2009) requires manual category aggregation by inspection of the annotation guidelines and the training set. Radford et al. (2015) assume that document-specific knowledge base (e.g., Wikipedia) tags for each document are provided, and they use those tags to build gazetteer type features for improving an English NER system. The second class of approaches use Wikipedia to generate weakly annotated data for training multilingual NER systems, e.g., (Richman and Schone, 2008; Nothman et al., 2013). The motivation is that annotating multilingual NER data by human is both expensive and time-consuming. Richman and Schone (2008) utilize the category information of Wikipedia to determine the entity type of an entity based on manually constructed rules (e.g., category phrase “Living People” is mapped to entity type PERSON). Such a rule-based entity type mapping is limited both in accuracy and coverage, e.g., (Toral and Muoz, 2006). Nothman et al. (2013) train a Wikipedia entity type classifier using human-annotated Wikipedia pages. Such a supervised-learning based appr"
D16-1135,W03-0419,0,0.227907,"Missing"
D16-1135,W02-2024,0,0.48973,"Missing"
D16-1135,W06-2809,0,0.0557241,"NER system. The second class of approaches use Wikipedia to generate weakly annotated data for training multilingual NER systems, e.g., (Richman and Schone, 2008; Nothman et al., 2013). The motivation is that annotating multilingual NER data by human is both expensive and time-consuming. Richman and Schone (2008) utilize the category information of Wikipedia to determine the entity type of an entity based on manually constructed rules (e.g., category phrase “Living People” is mapped to entity type PERSON). Such a rule-based entity type mapping is limited both in accuracy and coverage, e.g., (Toral and Muoz, 2006). Nothman et al. (2013) train a Wikipedia entity type classifier using human-annotated Wikipedia pages. Such a supervised-learning based approach has better accuracy and coverage, e.g., (Dakka and Cucerzan, 2008). A number of heuristic rules are developed in both works to label the Wikipedia text to create weakly annotated NER training data. The NER systems trained with the weakly annotated data may achieve similar accuracy compared with systems trained with little human-annotated data (e.g., up to 40K tokens as in (Richman and Schone, 2008)), but they are still significantly worse than well-t"
D17-1274,P16-1072,0,0.168613,"ler and can indicate slot type. 3 3.1 Regularized Dependency Graph based CNN Regularized Dependency Graph Dependency parsing based features, especially the shortest dependency path between two entities, have been proved to be effective to extract the most important information for identifying the relation between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007). Several recent studies also explored transforming a dependency path into a sequence and applied Neural Networks to the sequence for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015). However, for SF, the shortest dependency path between query and candidate filler is not always sufficient to infer the slot type due to two reasons. First, the most indicative words may not be included in the path. For example, in the following sentence: E2. Survivors include two sons and daughters-inlaw, Troyf iller and Phyllis Perry, Kennyquery and Donna Perry, all of Bluff City. the shortest dependency path between Kenny and Troy is: “Troy ←conj Perry ←conj Kenny”, which 2589 Figure 2: Overview of the Architecture. does not include the most indicative words: sons and dau"
D17-1274,N15-1133,0,0.021171,"nd Ji, 2016; Yu et al., 2016). Some work (Rodriguez et al., 2015; Zhi et al., 2015; Viswanathan et al., 2015; Hong et al., 2015; Rajani and Mooney, 2016a; Yu et al., 2014a; Rajani and Mooney, 2016b; Ma et al., 2015) also attempted to validate slot types by combining results from multiple systems. Our work is also related to dependency path based relation extraction. The effectiveness of dependency features for relation classification has been reported in some previous work (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007; Neville and Jensen, 2003; Ebrahimi and Dou, 2015; Xu et al., 2015; Ji et al., 2014). Liu et al. (2015), Cai et al. (2016) and Xu et al. (2015) applied CNN, bidirectional recurrent CNN and LSTM to CONLL relation extraction and demonstrated that the most important information has been included within the shortest paths between entities. Considering that the indicative words may not be included by the shortest dependency path between query and candidate filler, we enrich it to a regularized dependency graph by adding more contexts. 2595 7 Conclusions and Future Work In this work, we discussed the unique challenges of slot filling compared with"
D17-1274,P05-1053,0,0.453655,"ions to measure the relatedness of each input bigram with each slot type via a transformation matrix. These two attention mechanisms will guide the pooling step to select the information which is related to query and filler and can indicate slot type. 3 3.1 Regularized Dependency Graph based CNN Regularized Dependency Graph Dependency parsing based features, especially the shortest dependency path between two entities, have been proved to be effective to extract the most important information for identifying the relation between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007). Several recent studies also explored transforming a dependency path into a sequence and applied Neural Networks to the sequence for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015). However, for SF, the shortest dependency path between query and candidate filler is not always sufficient to infer the slot type due to two reasons. First, the most indicative words may not be included in the path. For example, in the following sentence: E2. Survivors include two sons and daughters-inlaw, Troyf iller and Phyllis Perry, Kennyquery and Donna Perry"
D17-1274,W09-2415,0,0.0742998,"Missing"
D17-1274,N16-1097,0,0.031215,"Missing"
D17-1274,N07-1015,0,0.2235,"latedness of each input bigram with each slot type via a transformation matrix. These two attention mechanisms will guide the pooling step to select the information which is related to query and filler and can indicate slot type. 3 3.1 Regularized Dependency Graph based CNN Regularized Dependency Graph Dependency parsing based features, especially the shortest dependency path between two entities, have been proved to be effective to extract the most important information for identifying the relation between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007). Several recent studies also explored transforming a dependency path into a sequence and applied Neural Networks to the sequence for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015). However, for SF, the shortest dependency path between query and candidate filler is not always sufficient to infer the slot type due to two reasons. First, the most indicative words may not be included in the path. For example, in the following sentence: E2. Survivors include two sons and daughters-inlaw, Troyf iller and Phyllis Perry, Kennyquery and Donna Perry, all of Bluff City. th"
D17-1274,P15-2047,1,0.951334,"nto one of the 41 types or none. Most previous studies have treated SF in the same way as within-sentence relation extraction tasks in ACE 1 or SemEval (Hendrickx et al., 2009). They created training data based on crowd-sourcing or distant supervision, and then trained a multi-class classifier or multiple binary classifiers for each slot type based on a set of hand-crafted features. Although Deep Neural Networks (DNN) such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) have achieved state-of-the-art results on within-sentence relation extraction (Zeng et al., 2014; Liu et al., 2015; Santos et al., 2015; Nguyen and Grishman, 2015; Yang et al., 2016; Wang et al., 2016), there are limited studies on SF using DNN. Adel and Sch¨utze (2015) and Adel et al. (2016) exploited DNN for SF but did not achieve comparable results as traditional methods. In this paper we aim to answer the following questions: What is the difference between SF and ACE/SemEval relation extraction task? How can we make DNN work for SF? We argue that SF is different and more challenging than traditional relation extraction. First, a query and its candidate filler are usually separated by much wider contex"
D17-1274,D14-1164,0,0.0241327,"milarity scores between column i in F with all slot type vectors, and max{S[i, :]} is the max value among all similarity scores for column i in F . Figure 5: Global Attention. We apply local attention to each convolution output of each subgraph, then take the concatenation of three flattened attentive pooling outputs to a fully connected layer and generate a robust feature representation. Similarly, another feature representation is generated based on global attention. We concatenate these two features to the softmax layer to get the predicted types. 5 Experiments 5.1 Data For model training, Angeli et al. (2014) created some high-quality clean annotations for SF based on crowd-sourcing2 . In addition, Adel et al. (2016) automatically created a larger size of noisy training data based on distant supervision, including about 1,725,891 positive training instances for 41 slot types. We manually assessed the correctness of candidate filler identification and their slot type annotation, and extracted a subset of their noisy annotations and combined it with the clean annotations. Ultimately, we obtain 23,993 positive and 3,000 negative training instances for all slot types. We evaluate our approach in two s"
D17-1274,P14-5010,0,0.00350556,"of birth. Entity types can be inferred by enriching query and filler related contexts. For example, in the following sentence: E3. Merkelquery died in the southern German city of Passauf iller in 1967. we can determine the slot type as city related by incorporating rich contexts (e.g., “city”). To tackle these problems, we propose to regularize the dependency graph, incorporating the shortest dependency path between query and candidate filler, as well as their rich contextual words. Given a sentence s including a query q and candidate filler f , we first apply the Stanford Dependency Parser (Manning et al., 2014) to generate all dependent word pairs: hgovernor, dependenti, then discover the shortest dependency path between query and candidate filler based on BreadthFirst-Search (BFS) algorithm. The regularized dependency graph includes words on the shortest dependency path, as well as words which can be connected to query and filler within n hops. In our experiments, we set n = 1. Figure 3 shows the dependency parsing output for E1 mentioned in Section 1, and the regularized dependency graph with the bold circled nodes. We can see that, the most indicative trigger owns can be found in both the shortes"
D17-1274,H05-1091,0,0.538904,"attention: We use prelearned slot type representations to measure the relatedness of each input bigram with each slot type via a transformation matrix. These two attention mechanisms will guide the pooling step to select the information which is related to query and filler and can indicate slot type. 3 3.1 Regularized Dependency Graph based CNN Regularized Dependency Graph Dependency parsing based features, especially the shortest dependency path between two entities, have been proved to be effective to extract the most important information for identifying the relation between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007). Several recent studies also explored transforming a dependency path into a sequence and applied Neural Networks to the sequence for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015). However, for SF, the shortest dependency path between query and candidate filler is not always sufficient to infer the slot type due to two reasons. First, the most indicative words may not be included in the path. For example, in the following sentence: E2. Survivors include two sons and daughters-inlaw, Troyf ille"
D17-1274,W15-1506,0,0.0896444,"Missing"
D17-1274,D16-1007,0,0.0268318,"Missing"
D17-1274,D16-1201,0,0.104931,"a wide range of features and patterns, especially for slot types that are in the longtail of the quite skewed distribution of slot fills (Ji et al., 2011a). Previous work has mostly focused on compensating the data needs by constructing patterns (Sun et al., 2011; Roth et al., 2014b), automatic annotation by distant supervision (Surdeanu et al., 2011; Roth et al., 2014a; Adel et al., 2016), and constructing trigger lists for unsupervised dependency graph mining (Yu and Ji, 2016; Yu et al., 2016). Some work (Rodriguez et al., 2015; Zhi et al., 2015; Viswanathan et al., 2015; Hong et al., 2015; Rajani and Mooney, 2016a; Yu et al., 2014a; Rajani and Mooney, 2016b; Ma et al., 2015) also attempted to validate slot types by combining results from multiple systems. Our work is also related to dependency path based relation extraction. The effectiveness of dependency features for relation classification has been reported in some previous work (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007; Neville and Jensen, 2003; Ebrahimi and Dou, 2015; Xu et al., 2015; Ji et al., 2014). Liu et al. (2015), Cai et al. (2016) and Xu et al. (2015) applied CNN, bidirectional recurren"
D17-1274,Q16-1019,0,0.0829586,"Missing"
D17-1274,C14-1149,1,0.898473,"Missing"
D17-1274,P15-1061,0,0.0480725,"Missing"
D17-1274,P16-1005,1,0.927171,"er 7–11, 2017. 2017 Association for Computational Linguistics Here, Arcandor and KaDeWe are far separated and it’s difficult to determine the slot type as org:subsidiaries based on the raw wide contexts. Figure 1: Comparison of the Percentage by the # of Words between two entity mentions in ACE05 and SemEval-2010 Task 8 relations, and between query and slot filler in KBP2013 Slot Filling. In addition, compared with relations defined in ACE (18 types) and SemEval (9 types), slot types are more fine-grained and heavily rely on indicative contextual words for disambiguation. Yu et al. (2015) and Yu and Ji (2016) demonstrate that many slot types can be specified by contextual trigger words. Here, a trigger is defined as the word which is related to both the query and candidate filler, and can indicate the type of the target slot. Considering E1 again, owns is a trigger word between Arcandor and KaDeWe, which can indicate the slot type as org:subsidiaries. Most previous work manually constructed trigger lists for each slot type. However, for some slot types, the triggers can be implicit and ambiguous. To address the above challenges, we propose the following new solutions: • To compress wide contexts,"
D17-1274,N15-1126,1,0.901313,"Missing"
D17-1274,C14-1220,0,0.204393,"Missing"
D17-1274,P05-1052,0,0.311731,"ned slot type representations to measure the relatedness of each input bigram with each slot type via a transformation matrix. These two attention mechanisms will guide the pooling step to select the information which is related to query and filler and can indicate slot type. 3 3.1 Regularized Dependency Graph based CNN Regularized Dependency Graph Dependency parsing based features, especially the shortest dependency path between two entities, have been proved to be effective to extract the most important information for identifying the relation between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007). Several recent studies also explored transforming a dependency path into a sequence and applied Neural Networks to the sequence for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015). However, for SF, the shortest dependency path between query and candidate filler is not always sufficient to infer the slot type due to two reasons. First, the most indicative words may not be included in the path. For example, in the following sentence: E2. Survivors include two sons and daughters-inlaw, Troyf iller and Phyllis Perry, Kenn"
D17-1274,P15-1018,0,0.024862,"SF is the lack of labeled data to generalize a wide range of features and patterns, especially for slot types that are in the longtail of the quite skewed distribution of slot fills (Ji et al., 2011a). Previous work has mostly focused on compensating the data needs by constructing patterns (Sun et al., 2011; Roth et al., 2014b), automatic annotation by distant supervision (Surdeanu et al., 2011; Roth et al., 2014a; Adel et al., 2016), and constructing trigger lists for unsupervised dependency graph mining (Yu and Ji, 2016; Yu et al., 2016). Some work (Rodriguez et al., 2015; Zhi et al., 2015; Viswanathan et al., 2015; Hong et al., 2015; Rajani and Mooney, 2016a; Yu et al., 2014a; Rajani and Mooney, 2016b; Ma et al., 2015) also attempted to validate slot types by combining results from multiple systems. Our work is also related to dependency path based relation extraction. The effectiveness of dependency features for relation classification has been reported in some previous work (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007; Neville and Jensen, 2003; Ebrahimi and Dou, 2015; Xu et al., 2015; Ji et al., 2014). Liu et al. (2015), Cai et al. (2016) and Xu et al"
D17-1274,P16-1123,0,0.0357496,"Missing"
D17-1274,D15-1206,0,0.168904,"te slot type. 3 3.1 Regularized Dependency Graph based CNN Regularized Dependency Graph Dependency parsing based features, especially the shortest dependency path between two entities, have been proved to be effective to extract the most important information for identifying the relation between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007). Several recent studies also explored transforming a dependency path into a sequence and applied Neural Networks to the sequence for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015). However, for SF, the shortest dependency path between query and candidate filler is not always sufficient to infer the slot type due to two reasons. First, the most indicative words may not be included in the path. For example, in the following sentence: E2. Survivors include two sons and daughters-inlaw, Troyf iller and Phyllis Perry, Kennyquery and Donna Perry, all of Bluff City. the shortest dependency path between Kenny and Troy is: “Troy ←conj Perry ←conj Kenny”, which 2589 Figure 2: Overview of the Architecture. does not include the most indicative words: sons and daughters for their p"
D19-1038,D16-1250,0,0.111382,"rrent or convolutional neural network layer. The third layer is a summarization layer which summarizes the vectors in a sentence by grouping and pooling. The final layer is the output layer which returns the classification label for the relation type. 3.2.1 Length Normalization and Orthogonal Transformation To ensure that all the training instances in the dictionary D contribute equally to the optimization objective in (4) and to preserve vector norms after projection, we have tried length normalization and orthogonal transformation for learning the bilingual mapping as in (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017). First, we normalize the source-language and target-language word embeddings to be unit vecx tors: x0 = ||x|| for each source-language word emy for each target-language bedding x, and y0 = ||y|| word embedding y. Next, we add an orthogonality constraint to (4) such that M is an orthogonal matrix, i.e., MT M = I where I denotes the identity matrix: MO t→s = arg min M∈Rd×d ,MT M=I D X Neural Network RE Models ||x0i − Myi0 ||2 4.1 i=1 (5) can be computed using singular-value decomposition (SVD). Embedding Layer For an English sentence with n words s = (w1 , w2 , ..., wn ), t"
D19-1038,P17-1042,0,0.105628,"ition (SVD). Embedding Layer For an English sentence with n words s = (w1 , w2 , ..., wn ), the embedding layer maps each word wt to a real-valued vector (word embedding) xt ∈ Rd×1 using the English word embedding model (Section 3.1). In addition, for each entity m in the sentence, the embedding layer maps its entity type to a real-valued vector (entity label embedding) lm ∈ Rdm ×1 (initialized randomly). In our experiments we use d = 300 and dm = 50. MO t→s 3.2.2 Semi-Supervised and Unsupervised Mappings The mapping learned in (4) or (5) requires a seed dictionary. To relax this requirement, Artetxe et al. (2017) proposed a self-learning procedure that can be combined with a dictionary-based mapping technique. Starting with a small seed dictionary, the procedure iteratively 1) learns a mapping using the current dictionary; and 2) computes a new dictionary using the learned mapping. Artetxe et al. (2018) proposed an unsupervised method to learn the bilingual mapping without using a seed dictionary. The method first uses a heuristic to build an initial dictionary that aligns the vocabularies of two languages, and then applies a robust self-learning procedure to iteratively improve the mapping. Another u"
D19-1038,P18-1073,0,0.0812868,"layer maps its entity type to a real-valued vector (entity label embedding) lm ∈ Rdm ×1 (initialized randomly). In our experiments we use d = 300 and dm = 50. MO t→s 3.2.2 Semi-Supervised and Unsupervised Mappings The mapping learned in (4) or (5) requires a seed dictionary. To relax this requirement, Artetxe et al. (2017) proposed a self-learning procedure that can be combined with a dictionary-based mapping technique. Starting with a small seed dictionary, the procedure iteratively 1) learns a mapping using the current dictionary; and 2) computes a new dictionary using the learned mapping. Artetxe et al. (2018) proposed an unsupervised method to learn the bilingual mapping without using a seed dictionary. The method first uses a heuristic to build an initial dictionary that aligns the vocabularies of two languages, and then applies a robust self-learning procedure to iteratively improve the mapping. Another unsuper4.2 Context Layer Given the word embeddings xt ’s of the words in the sentence, the context layer tries to build a sentence-context-aware vector representation for each word. We consider two types of neural network layers that aim to achieve this. 4.2.1 Bi-LSTM Context Layer The first type"
D19-1038,P82-1020,0,0.72639,"Missing"
D19-1038,C10-1064,0,0.378392,"ing models that need to be trained with large amounts of manually annotated RE data to achieve high accuracy. However, annotating RE data by human is expensive and timeconsuming, and can be quite difficult for a new language. Moreover, most RE models require language-specific resources such as dependency parsers and part-of-speech (POS) taggers, which also makes it very challenging to transfer an RE model of a resource-rich language to a resourcepoor language. There are a few existing weakly supervised cross-lingual RE approaches that require no human annotation in the target languages, e.g., Kim et al. (2010); Kim and Lee (2012); Faruqui and Kumar (2015); Zou et al. (2018). However, the existing approaches require aligned parallel corpora or machine translation systems, which may not be readily available in practice. In this paper, we make the following contributions to cross-lingual RE: Relation extraction (RE) seeks to detect and classify semantic relationships between entities, which provides useful information for many NLP applications. Since the state-ofthe-art RE models require large amounts of manually annotated data and language-specific resources to achieve high accuracy, it is very chall"
D19-1038,E14-1049,0,0.0423505,"e to realvalued vectors in Rd×1 . The dimension of the vector space d is normally much smaller than the size of the vocabulary V = |V |for efficient representation. It also aims to capture semantic similarities between the words based on their distributional properties in large samples of monolingual data. Cross-lingual word embedding models try to build word embeddings across multiple languages (Upadhyay et al., 2016; Ruder et al., 2017). One approach builds monolingual word embeddings separately and then maps them to the same vector space using a bilingual dictionary (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Another approach builds multilingual word embeddings in a shared vector space simultaneously, by generating mixed language corpora using aligned sentences (Luong et al., 2015; Gouws et al., 2015). In this paper, we adopt the technique in (Mikolov et al., 2013b) because it only requires a small bilingual dictionary of aligned word pairs, and does not require parallel corpora of aligned sentences which could be more difficult to obtain. We organize the paper as follows. In Section 2 we provide an overview of our approach. In Section 3 we describe how to build monolingual word embeddings and le"
D19-1038,P12-2010,0,0.29151,"d to be trained with large amounts of manually annotated RE data to achieve high accuracy. However, annotating RE data by human is expensive and timeconsuming, and can be quite difficult for a new language. Moreover, most RE models require language-specific resources such as dependency parsers and part-of-speech (POS) taggers, which also makes it very challenging to transfer an RE model of a resource-rich language to a resourcepoor language. There are a few existing weakly supervised cross-lingual RE approaches that require no human annotation in the target languages, e.g., Kim et al. (2010); Kim and Lee (2012); Faruqui and Kumar (2015); Zou et al. (2018). However, the existing approaches require aligned parallel corpora or machine translation systems, which may not be readily available in practice. In this paper, we make the following contributions to cross-lingual RE: Relation extraction (RE) seeks to detect and classify semantic relationships between entities, which provides useful information for many NLP applications. Since the state-ofthe-art RE models require large amounts of manually annotated data and language-specific resources to achieve high accuracy, it is very challenging to transfer a"
D19-1038,N15-1151,0,0.113736,"large amounts of manually annotated RE data to achieve high accuracy. However, annotating RE data by human is expensive and timeconsuming, and can be quite difficult for a new language. Moreover, most RE models require language-specific resources such as dependency parsers and part-of-speech (POS) taggers, which also makes it very challenging to transfer an RE model of a resource-rich language to a resourcepoor language. There are a few existing weakly supervised cross-lingual RE approaches that require no human annotation in the target languages, e.g., Kim et al. (2010); Kim and Lee (2012); Faruqui and Kumar (2015); Zou et al. (2018). However, the existing approaches require aligned parallel corpora or machine translation systems, which may not be readily available in practice. In this paper, we make the following contributions to cross-lingual RE: Relation extraction (RE) seeks to detect and classify semantic relationships between entities, which provides useful information for many NLP applications. Since the state-ofthe-art RE models require large amounts of manually annotated data and language-specific resources to achieve high accuracy, it is very challenging to transfer an RE model of a resource-r"
D19-1038,D15-1205,0,0.0380858,"Missing"
D19-1038,P14-1038,0,0.0321605,"ant information extraction task that seeks to detect and classify semantic relationships between entities like persons, organizations, geo-political entities, locations, and events. It provides useful information for many NLP applications such as knowledge base construction, text mining and question answering. For example, the entity Washington, D.C. and the entity United States have a CapitalOf relationship, and extraction of such relationships can help answer questions like “What is the capital city of the United States?” Traditional RE models (e.g., Zelenko et al. (2003); Kambhatla (2004); Li and Ji (2014)) require careful feature engineering to derive and combine various lexical, syntactic and semantic features. Recently, neural network RE models (e.g., Zeng et al. (2014); dos Santos et al. • We propose a new approach for direct crosslingual RE model transfer based on bilingual word embedding mapping. It projects word embeddings from a target language to a source language (e.g., English), so that a well-trained source-language RE model can be directly applied to the target language, with no manually annotated RE data needed for the target language. 399 Proceedings of the 2019 Conference on Emp"
D19-1038,P17-1004,0,0.0279136,"Missing"
D19-1038,N15-1157,0,0.0240309,"Missing"
D19-1038,W15-1521,0,0.0285525,"capture semantic similarities between the words based on their distributional properties in large samples of monolingual data. Cross-lingual word embedding models try to build word embeddings across multiple languages (Upadhyay et al., 2016; Ruder et al., 2017). One approach builds monolingual word embeddings separately and then maps them to the same vector space using a bilingual dictionary (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Another approach builds multilingual word embeddings in a shared vector space simultaneously, by generating mixed language corpora using aligned sentences (Luong et al., 2015; Gouws et al., 2015). In this paper, we adopt the technique in (Mikolov et al., 2013b) because it only requires a small bilingual dictionary of aligned word pairs, and does not require parallel corpora of aligned sentences which could be more difficult to obtain. We organize the paper as follows. In Section 2 we provide an overview of our approach. In Section 3 we describe how to build monolingual word embeddings and learn a linear mapping between two languages. In Section 4 we present a neural network architecture for the source-language (English). In Section 5 we evaluate the performance of"
D19-1038,P15-1119,0,0.0681277,"Missing"
D19-1038,I17-1068,0,0.0418572,"Missing"
D19-1038,P16-1157,0,0.0206654,"gs, become ubiquitous for many NLP applications (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014). A monolingual word embedding model maps words in the vocabulary V of a language to realvalued vectors in Rd×1 . The dimension of the vector space d is normally much smaller than the size of the vocabulary V = |V |for efficient representation. It also aims to capture semantic similarities between the words based on their distributional properties in large samples of monolingual data. Cross-lingual word embedding models try to build word embeddings across multiple languages (Upadhyay et al., 2016; Ruder et al., 2017). One approach builds monolingual word embeddings separately and then maps them to the same vector space using a bilingual dictionary (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Another approach builds multilingual word embeddings in a shared vector space simultaneously, by generating mixed language corpora using aligned sentences (Luong et al., 2015; Gouws et al., 2015). In this paper, we adopt the technique in (Mikolov et al., 2013b) because it only requires a small bilingual dictionary of aligned word pairs, and does not require parallel corpora of aligned sentence"
D19-1038,P16-1105,0,0.0610568,"Missing"
D19-1038,N16-1103,0,0.0542791,"Missing"
D19-1038,P17-1135,1,0.896197,"Missing"
D19-1038,D18-1034,0,0.0836054,"Missing"
D19-1038,D14-1162,0,0.0872618,"h in-house and the ACE05 datasets (Walker et al., 2006), using a small bilingual dictionary with only 1K word pairs. To the best of our knowledge, this is the first work that includes empirical studies for cross-lingual RE on several languages across a variety of language families, without using aligned parallel corpora or machine translation systems. We will describe each component of our approach in the subsequent sections. 3 In recent years, vector representations of words, known as word embeddings, become ubiquitous for many NLP applications (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014). A monolingual word embedding model maps words in the vocabulary V of a language to realvalued vectors in Rd×1 . The dimension of the vector space d is normally much smaller than the size of the vocabulary V = |V |for efficient representation. It also aims to capture semantic similarities between the words based on their distributional properties in large samples of monolingual data. Cross-lingual word embedding models try to build word embeddings across multiple languages (Upadhyay et al., 2016; Ruder et al., 2017). One approach builds monolingual word embeddings separately and then maps the"
D19-1038,N15-1104,0,0.149584,"ations using a recurrent or convolutional neural network layer. The third layer is a summarization layer which summarizes the vectors in a sentence by grouping and pooling. The final layer is the output layer which returns the classification label for the relation type. 3.2.1 Length Normalization and Orthogonal Transformation To ensure that all the training instances in the dictionary D contribute equally to the optimization objective in (4) and to preserve vector norms after projection, we have tried length normalization and orthogonal transformation for learning the bilingual mapping as in (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017). First, we normalize the source-language and target-language word embeddings to be unit vecx tors: x0 = ||x|| for each source-language word emy for each target-language bedding x, and y0 = ||y|| word embedding y. Next, we add an orthogonality constraint to (4) such that M is an orthogonal matrix, i.e., MT M = I where I denotes the identity matrix: MO t→s = arg min M∈Rd×d ,MT M=I D X Neural Network RE Models ||x0i − Myi0 ||2 4.1 i=1 (5) can be computed using singular-value decomposition (SVD). Embedding Layer For an English sentence with n words s = ("
D19-1038,P13-1147,0,0.0346042,"5 40 50 50 Dev 60 40 63 Test 140 35 55 40 50 50 Test 60 40 63 Table 2: Number of documents in the training/dev/test sets of the in-house and ACE05 datasets. • The third neural network model has a CNN context layer with a window size 3. We call it CNN for short. First we compare our neural network English RE models with the state-of-the-art RE models on the ACE05 English data. The ACE05 English data can be divided to 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). We apply the same data split in (Plank and Moschitti, 2013; Gormley et al., 2015; Nguyen and Grishman, 2016), which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set. We learn the model parameters using Adam (Kingma and Ba, 2015). We apply dropout (Srivastava et al., 2014) to the hidden layers to reduce overfitting. The development set is used for tuning the model hyperparameters and for early stopping. In Table 1 we compare our models with the best models in (Gormley et al., 2015) and (Nguyen and Grishman, 2016). Our Bi-LSTM model outperforms the best model (single or e"
D19-1038,C14-1220,0,0.502344,", and events. It provides useful information for many NLP applications such as knowledge base construction, text mining and question answering. For example, the entity Washington, D.C. and the entity United States have a CapitalOf relationship, and extraction of such relationships can help answer questions like “What is the capital city of the United States?” Traditional RE models (e.g., Zelenko et al. (2003); Kambhatla (2004); Li and Ji (2014)) require careful feature engineering to derive and combine various lexical, syntactic and semantic features. Recently, neural network RE models (e.g., Zeng et al. (2014); dos Santos et al. • We propose a new approach for direct crosslingual RE model transfer based on bilingual word embedding mapping. It projects word embeddings from a target language to a source language (e.g., English), so that a well-trained source-language RE model can be directly applied to the target language, with no manually annotated RE data needed for the target language. 399 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 399–409, c Hong Kong, China, November 3–7,"
D19-1038,P15-1061,0,0.0253526,"tokens of wt in the sentence. The concatenation of the two vectors → − ← − ht = [ h t , h t ] is a good representation of the word wt with both left and right contextual information in the sentence. We perform element-wise max pooling among the vectors in each group: hGi (j) = max h(j), 1 ≤ j ≤ dh , 1 ≤ i ≤ 5 (8) h∈Gi where dh is the dimension of the hidden state vectors. Concatenating the hGi ’s we get a fixedlength vector hs = [hG1 , ..., hG5 ]. 4.4 Output Layer 4.2.2 CNN Context Layer The second type of context layer is based on Convolutional Neural Networks (CNNs) (Zeng et al., 2014; dos Santos et al., 2015), which applies convolution-like operation on successive windows of size k around each word in the sentence. Let zt = [xt−(k−1)/2 , ..., xt+(k−1)/2 ] be the concatenation of k word embeddings around wt . The convolutional layer computes a hidden state vector The output layer receives inputs from the previous layers (the summarization vector hs , the entity label embeddings lm1 and lm2 for the two entities under consideration) and returns a probability distribution over the relation type labels:  p = softmax Ws hs +Wm1 lm1 +Wm2 lm2 +bo (9) ht = tanh(Wzt + b) Given the word embeddings of a sequ"
D19-1038,C18-1037,0,0.103092,"annotated RE data to achieve high accuracy. However, annotating RE data by human is expensive and timeconsuming, and can be quite difficult for a new language. Moreover, most RE models require language-specific resources such as dependency parsers and part-of-speech (POS) taggers, which also makes it very challenging to transfer an RE model of a resource-rich language to a resourcepoor language. There are a few existing weakly supervised cross-lingual RE approaches that require no human annotation in the target languages, e.g., Kim et al. (2010); Kim and Lee (2012); Faruqui and Kumar (2015); Zou et al. (2018). However, the existing approaches require aligned parallel corpora or machine translation systems, which may not be readily available in practice. In this paper, we make the following contributions to cross-lingual RE: Relation extraction (RE) seeks to detect and classify semantic relationships between entities, which provides useful information for many NLP applications. Since the state-ofthe-art RE models require large amounts of manually annotated data and language-specific resources to achieve high accuracy, it is very challenging to transfer an RE model of a resource-rich language to a r"
D19-3006,D16-1244,0,0.0975469,"Missing"
D19-3006,N18-1202,0,0.115417,"span from the document along with a prediction score as its output. Note that CFO’s orchestrator automatically realizes that the IR node produces a list of documents, while the MRC node accepts a single document at a time. So CFO will take care of calling the MRC node for each of the k retrieved documents (using asynchronous calls to parallelize requests if a configuration flag is set). The underlying BERT-for-QA model is based on (Alberti et al., 2019). BERT (Devlin et al., 2018) is one of a series of pre-trained neural models that can be fine tuned to provide state-of-theart results in NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2019) including on the SQuAD (Rajpurkar et al., 2018) and NQ (Kwiatkowski et al., 2019) tasks that align with our MRC based QA. We use the Huggingface PyTorch implementation of BERT 9 which supports starting from a Base (a 12 layer, 768 hidden dimension, 12 atten4 Experiments Prior to integration of the MRC node into GAAMA, we first experiment with data preparation and training of the BERT MRC model as a standalone component using dev sets provided with the NQ and SQuAD data sets (see 5 for more on these data sets). NQ is preferred for evaluat7 10 http"
D19-3006,K18-2016,0,0.012446,"Anserini IR toolkit (Yang et al., 2017) to look for relevant documents for a question, then uses BERT-based techniques (Devlin et al., 2018) to extract the correct answer. However, their reliance on a Lucene based IR toolkit means that constructing a NLP pipeline would either require pipeline components to be written as Lucene based plugins (which comes with a variety of constraints on programming language and structure) or writing custom orchestration code to connect components outside of the toolkit. Similar constraints are imposed by other popular NLP pipeline toolkits such as StanfordNLP (Qi et al., 2018) and Spacy15 (both of which require development in Python with limited flexibility in training neural models with other frameworks such as Tensorflow16 ). In contrast, CFO’s inherent programming lan14 Used 500 random examples from dev set for experiments https://spacy.io/ 16 https://www.tensorflow.org/ 15 35 ity across type definitions. In contrast, CFO only requires consistency in the data model for components that directly connect to each other, and the names of corresponding types and fields do not need to match. These differences make CFO easier to use in cases where components were develo"
D19-3006,P18-2124,0,0.10194,"Missing"
D19-3006,D16-1264,0,0.07123,"en the median latency is greater than one and a half seconds, effectively cementing GPUs as a requirement for deploying to production environments. In future work we intend to explore network pruning or knowledge distillation techniques for potential speedups with the large model. 5 This flexibility is important in a domain such as Machine Reading Comprehension (MRC) where recent advances in language-modeling based pretrained embeddings like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2018) along with large scale open data sets like the Stanford Question Answering Dataset (SQuAD) 1.1 (Rajpurkar et al., 2016) and its successor SQuAD 2.0 (Rajpurkar et al., 2018) have spurred a diverse array of model architecture improvements in a short time span. Recent work has even produced systems that surpass human-level exact match accuracy on the SQuAD datasets, causing us to focus on the challenging new Natural Questions (NQ) dataset (Kwiatkowski et al., 2019) where the questions do not have any observational bias as they were not artificially created. To the best of our knowledge, there is no current software framework paper that shows its analysis on the NQ dataset and displays strong empirical performance"
D19-3006,N19-4013,0,0.442889,"rporates state-of-the-art BERT based MRC (Machine Reading Comprehension) with IR components to enable end-to-end answer retrieval. Results from the demo system are shown to be high quality in both academic and industry domain specific settings. Finally, we discuss best practices when (pre-)training BERT based MRC models for production systems. 1 Introduction Production NLP (Natural Language Processing) and IR (information retrieval) applications often rely on a system flow consisting of multiple components that need to be woven together to build an end-to-end system (A. Ferrucci et al., 2010; Yang et al., 2019). This paper presents a novel approach for defining flow graphs and a toolkit for compiling those definitions into deploy-able production grade systems1 . Though the framework, which we refer to as CFO (C OMPUTATION F LOW O RCHESTRATOR), is well suited to a variety of use cases, we demonstrate it by creating an interactive QA (Question Answering) system that can be used both for academic benchmarking as well as industry specific use cases. The interactive system integrates SOTA (state-of-the-art) BERT-based MRC (Machine Reading Comprehension), an Elasticsearch 2 CFO Architecture The CFO framew"
D19-3006,P18-1031,0,\N,Missing
D19-3006,Q19-1026,0,\N,Missing
D19-3006,N19-1423,0,\N,Missing
N01-1006,W99-0705,0,\N,Missing
N01-1006,W94-0111,0,\N,Missing
N01-1006,J93-2004,0,\N,Missing
N01-1006,P00-1036,0,\N,Missing
N01-1006,W00-1304,1,\N,Missing
N01-1006,A97-1051,0,\N,Missing
N01-1006,C94-2195,0,\N,Missing
N01-1006,W00-0726,0,\N,Missing
N01-1006,J95-4004,0,\N,Missing
N01-1006,J95-2004,0,\N,Missing
N01-1006,P98-2188,0,\N,Missing
N01-1006,C98-2183,0,\N,Missing
N01-1006,P98-1029,0,\N,Missing
N01-1006,C98-1029,0,\N,Missing
N03-4001,C96-1017,0,0.0944467,"Missing"
N03-4001,J93-2003,0,0.00628603,"Missing"
N03-4001,P03-1051,1,0.838841,"Missing"
N03-4001,J03-1005,1,0.874545,"Missing"
N09-2051,W99-0106,0,0.0355879,"Missing"
N09-2051,P07-2027,0,0.0263597,"m B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreference resolution by first analyzing the discourse structure of a document with rhetoric structure theory (RST) (Mann and Thompson, 1987) and then using the resulted discourse structure in coreference resolution. Since obtaining reliably the discourse structure itself is a challenge, they got mixed results compared with a linear structure baseline. Our work presented in this paper concentrates on the structural information represented in metadata, such as turn or speaker informa"
N09-2051,W97-0319,0,0.0240123,"ent in the coreference system B. On the other hand, in System A, “I” is likely to be linked with “Paul” because of its proximity of “Paul” in the absence of speaker information. The result of this experiment suggests that, unsurprisingly, speaker and turn metadata carry structural information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to"
N09-2051,H05-1083,1,0.89406,"Missing"
N09-2051,P04-1018,1,0.880725,"ikely to be linked with “Paul” because of its proximity of “Paul” in the absence of speaker information. The result of this experiment suggests that, unsurprisingly, speaker and turn metadata carry structural information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreference resolution by first analyzing the discourse structure of"
N09-2051,P02-1014,0,0.0388217,"On the other hand, in System A, “I” is likely to be linked with “Paul” because of its proximity of “Paul” in the absence of speaker information. The result of this experiment suggests that, unsurprisingly, speaker and turn metadata carry structural information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreference resolution by first"
N09-2051,J01-4004,0,0.0302956,"eference system B. On the other hand, in System A, “I” is likely to be linked with “Paul” because of its proximity of “Paul” in the absence of speaker information. The result of this experiment suggests that, unsurprisingly, speaker and turn metadata carry structural information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreferenc"
N09-2051,P03-1022,0,0.098125,"l” because of its proximity of “Paul” in the absence of speaker information. The result of this experiment suggests that, unsurprisingly, speaker and turn metadata carry structural information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreference resolution by first analyzing the discourse structure of a document with rhetoric structure"
N09-2051,P08-1096,0,0.0119137,"System A, “I” is likely to be linked with “Paul” because of its proximity of “Paul” in the absence of speaker information. The result of this experiment suggests that, unsurprisingly, speaker and turn metadata carry structural information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreference resolution by first analyzing the disc"
N13-1108,P03-1003,0,0.040442,"e training corpus of question-answer pairs is unavailable for most scenarios. (Monz, 2007) learns term weights for the IR component of a question answering task. His work unlike ours does not aim to find the answers to the questions. Most QA systems in the literature have dealt with answering factoid questions, where the answer is a noun phrase in response to questions of the form “Who,” “Where,” “When.” Most systems have a question analysis component that represents the question as syntactic relations in a parse or as deep semantic relations in a handcrafted ontology (Hermjakob et al., 2000; Chu-carroll et al., 2003; Moldovan et al., 2003). In addition certain systems (Bunescu and Huang, 2010) aim to find the “focus” of the question, that is, the noun-phrases in the question that would co-refer with answers. Additionally, much past work has focused on finding the lexical answer type (Pinchak, 2006; Li and Roth, 2002). Since these papers considered a small number of answer types, rules over the detected relations and answer types could be applied to find the relevant answer. However, since our system answers non-factoid questions that can have answer of arbitrary types, we want to use as few rules as poss"
N13-1108,N04-1001,0,0.0219153,"“American,” “hedge fund,” and “legally avoid taxes” are required elements to find answers and are thus marked as MMP-Musts (signified by enclosing rectangles). We purposely annotate MMPs at the word level and not in the parse tree, because this requires minimal linguistic knowledge. We do, however, employ an automatic procedure to attach MMPs to parse tree nodes when generating MMP training instances. 3.2 MMP Training Questions annotated in Section 3.1 are first processed by an information extraction (IE) pipeline consisting of syntactic parsing, mention detection and coreference resolution (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). After IE, we have access to the syntactic structure represented by a parse tree and semantic information represented by coreferenced mentions (including those of named entities). To take advantage of the availability of the syntactic and semantic information, we first attach the MMP annotations to parse tree nodes of a question, and, if necessary, we augment the parse tree. There are several reasons why we want to embed the MMPs into a parse tree. First, many constituents in parse trees correspond to important phrases we want to capture, especially p"
N13-1108,C02-1150,0,0.0639424,"where the answer is a noun phrase in response to questions of the form “Who,” “Where,” “When.” Most systems have a question analysis component that represents the question as syntactic relations in a parse or as deep semantic relations in a handcrafted ontology (Hermjakob et al., 2000; Chu-carroll et al., 2003; Moldovan et al., 2003). In addition certain systems (Bunescu and Huang, 2010) aim to find the “focus” of the question, that is, the noun-phrases in the question that would co-refer with answers. Additionally, much past work has focused on finding the lexical answer type (Pinchak, 2006; Li and Roth, 2002). Since these papers considered a small number of answer types, rules over the detected relations and answer types could be applied to find the relevant answer. However, since our system answers non-factoid questions that can have answer of arbitrary types, we want to use as few rules as possible. The MMPs therefore become a critical component of our system, both for question analysis and for relevance detection. 3 Question Data and MMP Model To train the MMP model, we first create a set of questions and label their MMPs. The labeled data is then used to train a statistical model to predict MM"
N13-1108,H05-1083,1,0.779252,"avoid taxes” are required elements to find answers and are thus marked as MMP-Musts (signified by enclosing rectangles). We purposely annotate MMPs at the word level and not in the parse tree, because this requires minimal linguistic knowledge. We do, however, employ an automatic procedure to attach MMPs to parse tree nodes when generating MMP training instances. 3.2 MMP Training Questions annotated in Section 3.1 are first processed by an information extraction (IE) pipeline consisting of syntactic parsing, mention detection and coreference resolution (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). After IE, we have access to the syntactic structure represented by a parse tree and semantic information represented by coreferenced mentions (including those of named entities). To take advantage of the availability of the syntactic and semantic information, we first attach the MMP annotations to parse tree nodes of a question, and, if necessary, we augment the parse tree. There are several reasons why we want to embed the MMPs into a parse tree. First, many constituents in parse trees correspond to important phrases we want to capture, especially proper names. Second, after an MMP is attac"
N13-1108,P04-1018,1,0.7069,"nd,” and “legally avoid taxes” are required elements to find answers and are thus marked as MMP-Musts (signified by enclosing rectangles). We purposely annotate MMPs at the word level and not in the parse tree, because this requires minimal linguistic knowledge. We do, however, employ an automatic procedure to attach MMPs to parse tree nodes when generating MMP training instances. 3.2 MMP Training Questions annotated in Section 3.1 are first processed by an information extraction (IE) pipeline consisting of syntactic parsing, mention detection and coreference resolution (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). After IE, we have access to the syntactic structure represented by a parse tree and semantic information represented by coreferenced mentions (including those of named entities). To take advantage of the availability of the syntactic and semantic information, we first attach the MMP annotations to parse tree nodes of a question, and, if necessary, we augment the parse tree. There are several reasons why we want to embed the MMPs into a parse tree. First, many constituents in parse trees correspond to important phrases we want to capture, especially proper names. Secon"
N13-1108,N03-1022,0,0.0481411,"ion-answer pairs is unavailable for most scenarios. (Monz, 2007) learns term weights for the IR component of a question answering task. His work unlike ours does not aim to find the answers to the questions. Most QA systems in the literature have dealt with answering factoid questions, where the answer is a noun phrase in response to questions of the form “Who,” “Where,” “When.” Most systems have a question analysis component that represents the question as syntactic relations in a parse or as deep semantic relations in a handcrafted ontology (Hermjakob et al., 2000; Chu-carroll et al., 2003; Moldovan et al., 2003). In addition certain systems (Bunescu and Huang, 2010) aim to find the “focus” of the question, that is, the noun-phrases in the question that would co-refer with answers. Additionally, much past work has focused on finding the lexical answer type (Pinchak, 2006; Li and Roth, 2002). Since these papers considered a small number of answer types, rules over the detected relations and answer types could be applied to find the relevant answer. However, since our system answers non-factoid questions that can have answer of arbitrary types, we want to use as few rules as possible. The MMPs therefore"
N13-1108,E06-1050,0,0.0242257,"oid questions, where the answer is a noun phrase in response to questions of the form “Who,” “Where,” “When.” Most systems have a question analysis component that represents the question as syntactic relations in a parse or as deep semantic relations in a handcrafted ontology (Hermjakob et al., 2000; Chu-carroll et al., 2003; Moldovan et al., 2003). In addition certain systems (Bunescu and Huang, 2010) aim to find the “focus” of the question, that is, the noun-phrases in the question that would co-refer with answers. Additionally, much past work has focused on finding the lexical answer type (Pinchak, 2006; Li and Roth, 2002). Since these papers considered a small number of answer types, rules over the detected relations and answer types could be applied to find the relevant answer. However, since our system answers non-factoid questions that can have answer of arbitrary types, we want to use as few rules as possible. The MMPs therefore become a critical component of our system, both for question analysis and for relevance detection. 3 Question Data and MMP Model To train the MMP model, we first create a set of questions and label their MMPs. The labeled data is then used to train a statistical"
P06-1060,P98-1080,0,0.0949316,"Missing"
P06-1060,W03-1026,1,0.841045,"references mentions – using the ACE (NIST, 2003) nomenclature – to differentiate them from entities 1 In a pragmatic sense, entities are sets of mentions which co-refer. 2 This last attribute, genericity, depends only loosely on local context. As such, it should be assigned while examining all mentions in an entity, and for this reason is beyond the scope of this article. 473 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 473–480, c Sydney, July 2006. 2006 Association for Computational Linguistics and mention detection (Jing et al., 2003; Florian et al., 2004). At the opposite end of classification methodology space, one can use a cascade model, which performs the sub-tasks sequentially in a predefined order. Under such a model, described in Section 3.3, the user will build separate models for each subtask. For instance, it could first identify the mention boundaries, then assign the entity type, subtype, and mention level information. Such a model has the immediate advantage of having smaller classification spaces, with the drawback that it requires a specific model invocation path. In between the two extremes, one can use a"
P06-1060,N03-5008,0,0.0269034,"ent, though, as these properties tend to have a stronger dependency on the lexical form of the classified word. 5 There is a fourth assigned type – a flag specifying whether a mention is specific (i.e. it refers at a clear entity), generic (refers to a generic type, e.g. “the scientists believe ..”), unspecified (cannot be determined from the text), or negative (e.g. “ no person would do this”). The classification of this type is beyond the goal of this paper. 3 While not wishing to delve too deep into the issue of label bias, we would also like to point out (as it was done, for instance, in (Klein, 2003)) that the label bias of MEMM classifiers can be significantly reduced by allowing them to examine the right context of the classification point - as we have done with our model. 474 3 (MTL) paradigm, where individual related tasks are trained together by sharing a common representation of knowledge, and demonstrated that this strategy yields better results than one-task-ata-time learning strategy. The authors used a backpropagation neural network, and the paradigm was tested on several machine learning tasks. It also contains an excellent discussion on how and why the MTL paradigm is superior"
P06-1060,J93-2004,0,0.0263035,"r work, Section 3.3 presents and contrasts the three meta-classification models. Section 4 outlines the experimental setup and the obtained results, and Section 5 concludes the paper. 2 is noun plural, or starts a noun phrase, etc). There are cases, though, where the labels consist of several related, but not entirely correlated, properties; examples include mention detection—the task we are interested in—, syntactic parsing with functional tag assignment (besides identifying the syntactic parse, also label the constituent nodes with their functional category, as defined in the Penn Treebank (Marcus et al., 1993)), and, to a lesser extent, part-of-speech tagging in highly inflected languages.4 The particular type of mention detection that we are examining in this paper follows the ACE general definition: each mention in the text (a reference to a real-world entity) is assigned three types of information:5 • An entity type, describing the type of the entity it points to (e.g. person, location, organization, etc) • An entity subtype, further detailing the type (e.g. organizations can be commercial, governmental and non-profit, while locations can be a nation, population center, or an international regio"
P06-1060,W94-0111,0,0.0997921,"Missing"
P06-1060,E99-1023,0,0.220391,"Missing"
P06-1060,W02-2024,0,0.019616,"ough computationally expensive, for the model to use additional data that is only partially labeled, with the model change presented later in Section 3.4. • The model cannot directly use data that is only partially labeled (i.e. not all sub-labels are specified). Despite the above disadvantages, this model has performed well in practice: Hajic and Hladk´a (1998) applied it successfully to find POS sequences for Czech and Florian et al. (2004) reports good results on the 2003 ACE task. Most systems that participated in the CoNLL 2002 and 2003 shared tasks on named entity recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) applied this model, as they modeled the identification of mention boundaries and the assignment of mention type at the same time. 3.2 Detect Mention Type The Joint Model 3.3 The joint model differs from the all-in-one model in the fact that the labels are no longer atomic: the features of the system can inspect the constituent sub-labels. This change helps alleviate the data The Cascade Model For some tasks, there might already exist a natural hierarchy among the sub-labels: some sub-labels could benefit from knowing the value of other, primitive, sub-lab"
P06-1060,W01-0701,1,0.809032,"EMM classifiers can be significantly reduced by allowing them to examine the right context of the classification point - as we have done with our model. 474 3 (MTL) paradigm, where individual related tasks are trained together by sharing a common representation of knowledge, and demonstrated that this strategy yields better results than one-task-ata-time learning strategy. The authors used a backpropagation neural network, and the paradigm was tested on several machine learning tasks. It also contains an excellent discussion on how and why the MTL paradigm is superior to single-task learning. Florian and Ngai (2001) used the same multitask learning strategy with a transformation-based learner to show that usually disjointly handled tasks perform slightly better under a joint model; the experiments there were run on POS tagging and text chunking, Chinese word segmentation and POS tagging. Sutton et al. (2004) investigated the multitask classification problem and used a dynamic conditional random fields method, a generalization of linear-chain conditional random fields, which can be viewed as a probabilistic generalization of cascaded, weighted finite-state transducers. The subtasks were represented in a s"
P06-1060,N04-1001,0,0.711093,"s – using the ACE (NIST, 2003) nomenclature – to differentiate them from entities 1 In a pragmatic sense, entities are sets of mentions which co-refer. 2 This last attribute, genericity, depends only loosely on local context. As such, it should be assigned while examining all mentions in an entity, and for this reason is beyond the scope of this article. 473 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 473–480, c Sydney, July 2006. 2006 Association for Computational Linguistics and mention detection (Jing et al., 2003; Florian et al., 2004). At the opposite end of classification methodology space, one can use a cascade model, which performs the sub-tasks sequentially in a predefined order. Under such a model, described in Section 3.3, the user will build separate models for each subtask. For instance, it could first identify the mention boundaries, then assign the entity type, subtype, and mention level information. Such a model has the immediate advantage of having smaller classification spaces, with the drawback that it requires a specific model invocation path. In between the two extremes, one can use a joint model, which mod"
P06-1060,W02-2010,1,0.850417,"et al. (2004) investigated the multitask classification problem and used a dynamic conditional random fields method, a generalization of linear-chain conditional random fields, which can be viewed as a probabilistic generalization of cascaded, weighted finite-state transducers. The subtasks were represented in a single graphical model that explicitly modeled the sub-task dependence and the uncertainty between them. The system, evaluated on POS tagging and base-noun phrase segmentation, improved on the sequential learning strategy. In a similar spirit to the approach presented in this article, Florian (2002) considers the task of named entity recognition as a two-step process: the first is the identification of mention boundaries and the second is the classification of the identified chunks, therefore considering a label for each word being formed from two sub-labels: one that specifies the position of the current word relative in a mention (outside any mentions, starts a mention, is inside a mention) and a label specifying the mention type . Experiments on the CoNLL’02 data show that the two-process model yields considerably higher performance. Hacioglu et al. (2005) explore the same task, inves"
P06-1060,H05-1048,0,0.0134659,"approach presented in this article, Florian (2002) considers the task of named entity recognition as a two-step process: the first is the identification of mention boundaries and the second is the classification of the identified chunks, therefore considering a label for each word being formed from two sub-labels: one that specifies the position of the current word relative in a mention (outside any mentions, starts a mention, is inside a mention) and a label specifying the mention type . Experiments on the CoNLL’02 data show that the two-process model yields considerably higher performance. Hacioglu et al. (2005) explore the same task, investigating the performance of the AIO and the cascade model, and find that the two models have similar performance, with the AIO model having a slight advantage. We expand their study by adding the hybrid joint model to the mix, and further investigate different scenarios, showing that the cascade model leads to superior performance most of the time, with a few ties, and show that the cascade model is especially beneficial in cases where partially-labeled data (only some of the component labels are given) is available. It turns out though, (Hacioglu, 2005) that the c"
P06-1060,C98-1077,0,\N,Missing
P06-1060,W03-0419,0,\N,Missing
P13-1136,P11-1049,0,0.258265,". Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while"
P13-1136,J96-1002,0,0.124803,"Missing"
P13-1136,briscoe-carroll-2002-robust,0,0.0538117,"other systems except the rule-based system). 7 Figure 3: Part of the summary generated by the multiscorer based summarizer for topic D0626H (DUC 2006). Grayed out words are removed. Queryirrelevant phrases, such as temporal information or source of the news, have been removed. dependency-tree based compressor (Martins and Smith, 2009)8 . We adopt the metrics in Martins and Smith (2009) to measure the unigram-level macro precision, recall, and F1-measure with respect to human annotated compression. In addition, we also compute the F1 scores of grammatical relations which are annotated by RASP (Briscoe and Carroll, 2002) according to Clarke and Lapata (2008). In Table 7, our context-aware and head-driven tree-based compression systems show statistically significantly (p < 0.01) higher precisions (Uni8 Thanks to Andr´e F.T. Martins for system outputs. Conclusion We have presented a framework for query-focused multi-document summarization based on sentence compression. We propose three types of compression approaches. Our tree-based compression method can easily incorporate measures of query relevance, content importance, redundancy and language quality into the compression process. By testing on a standard dat"
P13-1136,P11-1050,0,0.017869,"Missing"
P13-1136,P06-1039,0,0.0127762,"Missing"
P13-1136,W03-0501,0,0.238213,"Missing"
P13-1136,N04-1001,0,0.0120169,"and parameter tuning for the multiscorer. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3 . As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3 Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40"
P13-1136,P07-2015,0,0.0521182,"Missing"
P13-1136,N07-1023,0,0.0698946,", 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by using integer linear programming. 3 The Framework We now present our query-focused MDS framework consisting of three steps: Sentence Ranking, Sentence Compression and Post-pr"
P13-1136,W09-1802,0,0.0755725,"gned submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley"
P13-1136,N09-1041,0,0.0486933,"sis applications including openended question answering, recommender systems, and summarization of search engine results. As further evidence of its importance, the Document Understanding Conference (DUC) has used queryfocused MDS as its main task since 2004 to foster new research on automatic summarization in the context of users’ needs. To date, most top-performing systems for multi-document summarization—whether queryspecific or not—remain largely extractive: their summaries are comprised exclusively of sentences selected directly from the documents to be summarized (Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-T¨ur, 2011). Despite their simplicity, extractive approaches have some disadvantages. First, lengthy sentences that are partly relevant are either excluded from the summary or (if selected) can block the selection of other important sentences, due to summary length constraints. In addition, when people write summaries, they tend to abstract the content and seldom use entire sentences taken verbatim from the original documents. In news articles, for example, most sentences are lengthy and contain both potentially useful information for a summary as well as unnecessary"
P13-1136,P11-1052,0,0.00996893,"pervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has bee"
P13-1136,C00-1072,0,0.0769395,"we believe we are the first to successfully show that sentence compression can provide statistically significant improvements over pure extraction-based approaches for queryfocused MDS. 2 Related Work Existing research on query-focused multidocument summarization (MDS) largely relies on extractive approaches, where systems usually take as input a set of documents and select the top relevant sentences for inclusion in the final summary. A wide range of methods have been employed for this task. For unsupervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the sum"
P13-1136,N03-1020,0,0.164068,"Missing"
P13-1136,W03-1101,0,0.0260883,"efully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust proba"
P13-1136,H05-1083,0,0.0143206,". DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3 . As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3 Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt classifiers are used for t"
P13-1136,H05-1115,0,0.0142511,"ments over pure extraction-based approaches for queryfocused MDS. 2 Related Work Existing research on query-focused multidocument summarization (MDS) largely relies on extractive approaches, where systems usually take as input a set of documents and select the top relevant sentences for inclusion in the final summary. A wide range of methods have been employed for this task. For unsupervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (s"
P13-1136,P05-1036,0,0.042728,"candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by u"
P13-1136,P04-1018,0,0.0103922,"or the multiscorer. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3 . As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3 Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt cl"
P13-1136,W09-1801,0,0.781754,"e) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tre"
P13-1136,E06-1038,0,0.216493,"ches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by using integer linear programming. 3 The Framework We now present our query-focused MDS framework consisting of three steps: Sentence Ranking, Sentence Compression and Post-processing. First, sentence ranking determines the importance of each sentence given the query. Then, a sentence compressor iteratively generates the most likely succinct versions of the ranked sentences, which are cumulatively added to the summary, until a length limit is reached. Finally, the postprocessing stage applies coreference resolution and sen"
P13-1136,N04-1019,0,0.0713979,"edundancy within the summary through compression. Furthermore, our H EAD-driven beam search method with M ULTI-scorer beats all systems on DUC 20066 and all systems on DUC 2007 except the best system in terms of R-2 (p < 0.01). Its R-SU4 score is also significantly (p < 0.01) better than extractive methods, rule-based and sequence-based compression methods on both DUC 2006 and 2007. Moreover, our systems with learning-based compression have considerable compression rates, indicating their capability to remove superfluous words as well as improve summary quality. Human Evaluation. The Pyramid (Nenkova and Passonneau, 2004) evaluation was developed to manually assess how many relevant facts or Summarization Content Units (SCUs) are captured by system summaries. We ask a professional annotator (who is not one of the authors, is highly experienced in annotating for various NLP tasks, and is fluent in English) to carry out a Pyramid evaluation on 10 randomly selected topics from 4 We looked at various beam sizes on the heldout data, and observed that the performance peaks around this value. 5 ROUGE-1.5.5.pl -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -a -d 6 The system output from Davis et al. (2012) is n"
P16-1213,E06-1002,0,0.92152,"database, which, in turn, requires the processing system to be able to identify actors across documents by assigning unique identifiers to them. Entity Linking (EL) is the task of mapping specific textual mentions of entities in a text document to an entry in a large catalog of entities, often called a knowledge base or KB, and is one of the major tasks in the Knowledge-Base Population track at the Text Analysis Conference (TAC) (Ji et al., 2014). The task also involves grouping together (clustering) N IL entities which do not have any target referents in the KB. Previous work, pioneered by (Bunescu and Pasca, 2006; Cucerzan, 2007; Sil et al., 2012; Ratinov et al., 2011; Guo et al., 2013), have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. As with many NLP approaches, most of the previous EL research have focused on English, mainly because it has many NLP resources available, it is the most prevalent language on the web, and the fact that the English Wikipedia is the largest among all the Wikipedia datasets. However, there are plenty of web documents in other languages, such as Spanish (Fahrni et al., 2013; Ji et al., 2014)"
P16-1213,D14-1186,0,0.0264534,"7; Sil et al., 2012; Ratinov et al., 2011; Guo et al., 2013), have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. As with many NLP approaches, most of the previous EL research have focused on English, mainly because it has many NLP resources available, it is the most prevalent language on the web, and the fact that the English Wikipedia is the largest among all the Wikipedia datasets. However, there are plenty of web documents in other languages, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; Shi et al., 2014), with a large number of speakers, and there is a need to be able to develop EL systems for these languages (and others!) quickly and inexpensively. In this paper, we investigate the hypothesis that we can train an EL model that is entirely unlexicalized, by only allowing features that compute similarity between the text in the input document and the text/information in the KB. For this purpose, we propose a novel approach to entity linking, which we call Language Independent Entity Linking (henceforth L IE L). We test this hypothesis by applying the English-trained system o"
P16-1213,D13-1184,0,0.288209,"ate-of-the-art To follow the guidelines for the TAC NIST evaluation, we anonymize participant system names as System 1 through 9. Interested readers may look at their system description and scores in (Ji et al., 2014; Fahrni et al., 2013; Miao et al., 2013; Mayfield, 2013; Merhav et al., 2013). Out of these systems, System 1 and System 7 obtained the top score in Spanish and Chinese EL evaluation at TAC 2013 and hence can be treated as the current state-of-the-art for the respective EL tasks. We also compare L IE L with some traditional “wikifiers” like MW08 (Milne and Witten, 2008) and UIUC (Cheng and Roth, 2013) and also N EREL (Sil and Yates, 2013) which is the system which L IE L resembles the most. 3.4 Parameter Settings L IE L has two tuning parameters: σ, the regularization weight; and the number of candidate links per mention we select from the Wikipedia dump. We set the value of σ by trying five possible values in the range [0.1, 10] on held-out data (the TAC 2009 data). We found σ = 0.5 to work best for our experiments. We chose to select a maximum of 40 candidate entities from Wikipedia for each candidate mention (or fewer if the dump had fewer than 40 links with nonzero probability). 11 For"
P16-1213,D07-1074,0,0.848017,", requires the processing system to be able to identify actors across documents by assigning unique identifiers to them. Entity Linking (EL) is the task of mapping specific textual mentions of entities in a text document to an entry in a large catalog of entities, often called a knowledge base or KB, and is one of the major tasks in the Knowledge-Base Population track at the Text Analysis Conference (TAC) (Ji et al., 2014). The task also involves grouping together (clustering) N IL entities which do not have any target referents in the KB. Previous work, pioneered by (Bunescu and Pasca, 2006; Cucerzan, 2007; Sil et al., 2012; Ratinov et al., 2011; Guo et al., 2013), have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. As with many NLP approaches, most of the previous EL research have focused on English, mainly because it has many NLP resources available, it is the most prevalent language on the web, and the fact that the English Wikipedia is the largest among all the Wikipedia datasets. However, there are plenty of web documents in other languages, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (C"
P16-1213,I11-1095,0,0.148421,"thereby not making the system entirely language-independent. Empirically their performance comes close to System 1 which L IE L outperforms. The BASIS system (Merhav et al., 2262 2013), is the state-of-the-art for Chinese EL as it obtained the top score in TAC 2013. The FUJITSU system (Miao et al., 2013) obtained similar scores. It is worth noting that these systems, unlike L IE L, are heavily language dependent, e.g. performing lexicon specific information extraction, using inter-language links to map between the languages or training using labeled Chinese data. In more specialized domains, Dai et al. (2011) employed a Markov logic network for building an EL system with good results in a bio-medical domain; it would be interesting to find out how their techniques might extended to other languages/corpora. Phan et al. (2008) utilize topic models derived from Wikipedia to help classify short text segment, while Guo et al. (2013) investigate methods for disambiguating entities in tweets. Neither of these methods do show how to transfer the EL system developed for short texts to different languages, if at all. The large majority of entity linking research outside of TAC involves a closely related tas"
P16-1213,N13-1122,0,0.176425,"actors across documents by assigning unique identifiers to them. Entity Linking (EL) is the task of mapping specific textual mentions of entities in a text document to an entry in a large catalog of entities, often called a knowledge base or KB, and is one of the major tasks in the Knowledge-Base Population track at the Text Analysis Conference (TAC) (Ji et al., 2014). The task also involves grouping together (clustering) N IL entities which do not have any target referents in the KB. Previous work, pioneered by (Bunescu and Pasca, 2006; Cucerzan, 2007; Sil et al., 2012; Ratinov et al., 2011; Guo et al., 2013), have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. As with many NLP approaches, most of the previous EL research have focused on English, mainly because it has many NLP resources available, it is the most prevalent language on the web, and the fact that the English Wikipedia is the largest among all the Wikipedia datasets. However, there are plenty of web documents in other languages, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; Shi et al., 2014), with a large number of"
P16-1213,P11-1138,0,0.57521,"o be able to identify actors across documents by assigning unique identifiers to them. Entity Linking (EL) is the task of mapping specific textual mentions of entities in a text document to an entry in a large catalog of entities, often called a knowledge base or KB, and is one of the major tasks in the Knowledge-Base Population track at the Text Analysis Conference (TAC) (Ji et al., 2014). The task also involves grouping together (clustering) N IL entities which do not have any target referents in the KB. Previous work, pioneered by (Bunescu and Pasca, 2006; Cucerzan, 2007; Sil et al., 2012; Ratinov et al., 2011; Guo et al., 2013), have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. As with many NLP approaches, most of the previous EL research have focused on English, mainly because it has many NLP resources available, it is the most prevalent language on the web, and the fact that the English Wikipedia is the largest among all the Wikipedia datasets. However, there are plenty of web documents in other languages, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; Shi et al., 2014), with"
P16-1213,P14-2046,0,0.100379,"2; Ratinov et al., 2011; Guo et al., 2013), have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. As with many NLP approaches, most of the previous EL research have focused on English, mainly because it has many NLP resources available, it is the most prevalent language on the web, and the fact that the English Wikipedia is the largest among all the Wikipedia datasets. However, there are plenty of web documents in other languages, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; Shi et al., 2014), with a large number of speakers, and there is a need to be able to develop EL systems for these languages (and others!) quickly and inexpensively. In this paper, we investigate the hypothesis that we can train an EL model that is entirely unlexicalized, by only allowing features that compute similarity between the text in the input document and the text/information in the KB. For this purpose, we propose a novel approach to entity linking, which we call Language Independent Entity Linking (henceforth L IE L). We test this hypothesis by applying the English-trained system on Spanish and Chine"
P16-1213,D12-1011,1,0.907655,"rocessing system to be able to identify actors across documents by assigning unique identifiers to them. Entity Linking (EL) is the task of mapping specific textual mentions of entities in a text document to an entry in a large catalog of entities, often called a knowledge base or KB, and is one of the major tasks in the Knowledge-Base Population track at the Text Analysis Conference (TAC) (Ji et al., 2014). The task also involves grouping together (clustering) N IL entities which do not have any target referents in the KB. Previous work, pioneered by (Bunescu and Pasca, 2006; Cucerzan, 2007; Sil et al., 2012; Ratinov et al., 2011; Guo et al., 2013), have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. As with many NLP approaches, most of the previous EL research have focused on English, mainly because it has many NLP resources available, it is the most prevalent language on the web, and the fact that the English Wikipedia is the largest among all the Wikipedia datasets. However, there are plenty of web documents in other languages, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; S"
P16-1213,P02-1053,0,0.0118142,"ext (m)). KB Link Properties: L IE L can make use of existing relations in the KB, such as inlinks, outlinks, redirects, and categories. Practically, for each such relation l, a KB entry e has an associated set of strings I(l, e)8 ; given a mention-side set M (either T ext(m) or Context(m)), L IE L computes F REQUENCY feature functions for the names of the Categories, Inlinks, Outlinks and Redirects, we compute People etc., since they have lower discriminating power. These are analogous to all WP languages. From the training data, the system first computes point-wise mutual information (PMI) (Turney, 2002) scores for the Wikipedia categories of pairs of entities, (e1 , e2 ): P M I(C(e1 ), C(e2 )) = ntC −1 X X f (e, m, d) = |I(l, e) ∩ M | 1[C(e1 ) = C(eij )] × j Entity-Entity Pair Features Coherence Features: To better model consecutive entity assignments, L IE L computes a coherence feature function called O UTLINK OVER LAP . For every consecutive pair of entities (e1 , e2 ) that belongs to mentions in t, the feature computes Jaccard(Out(e1 ), Out(e2 )), where Out(e) denotes the Outlinks of e. Similarly, we also compute I NLINK OVERLAP. L IE L also uses categories in Wikipedia which exist in al"
P16-1213,D15-1081,0,0.0434028,"use of lexical features from Wikipedia as their model performs joint entity extraction and disambiguation. Some of the other systems which use a graph based algorithm such as partitioning are LCC, NYU (Ji et al., 2014) and HITS (Fahrni et al., 2013) which obtained competitive score in the TAC evaluations. Among all these systems, only the HITS system has ventured beyond English and has obtained the top score in Spanish EL evaluation at TAC 2013. It is the only multilingual EL system in the literature which performs reliably well across a series of languages and benchmark datasets. Recently, (Wang et al., 2015) show a new domain and language-independent EL system but they make use of translation tables for non-English (Chinese) EL; thereby not making the system entirely language-independent. Empirically their performance comes close to System 1 which L IE L outperforms. The BASIS system (Merhav et al., 2262 2013), is the state-of-the-art for Chinese EL as it obtained the top score in TAC 2013. The FUJITSU system (Miao et al., 2013) obtained similar scores. It is worth noting that these systems, unlike L IE L, are heavily language dependent, e.g. performing lexicon specific information extraction, us"
P17-1135,P02-1033,0,0.0604825,"1 58.5 58.0 71.8 Table 5: CoNLL NER test data. supervised learning. 7 Related Work The traditional annotation projection approaches (Yarowsky et al., 2001; Zitouni and Florian, 2008; Ehrmann et al., 2011) project NER tags across language pairs using parallel corpora or translations. Wang and Manning (2014) proposed a variant of annotation projection which projects expectations of tags and uses them as constraints to train a model based on generalized expectation criteria. Annotation projection has also been applied to several other cross-lingual NLP tasks, including word sense disambiguation (Diab and Resnik, 2002), part-of-speech (POS) tagging (Yarowsky et al., 2001) and dependency parsing (Rasooli and Collins, 2015). Wikipedia has been exploited to generate weakly labeled multilingual NER training data. The basic idea is to first categorize Wikipedia pages into entity types, either based on manually constructed rules that utilize the category information of Wikipedia (Richman and Schone, 2008) or Freebase attributes (Al-Rfou et al., 2015), or via a classifier trained with manually labeled Wikipedia pages (Nothman et al., 2013). Heuristic rules are then developed in these works to automatically label t"
P17-1135,R11-1017,0,0.531923,"n a welltrained NER system in a source language (e.g., English), how can one go about extending it to a new language with decent performance and no human annotation in the target language? There are mainly two types of approaches for building weakly supervised cross-lingual NER systems. The first type of approaches create weakly labeled NER training data in a target language. One way to create weakly labeled data is through annotation projection on aligned parallel corpora or translations between a source language and a target language, e.g., (Yarowsky et al., 2001; Zitouni and Florian, 2008; Ehrmann et al., 2011). Another way is to utilize the text and structure of 1470 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1470–1480 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1135 Wikipedia to generate weakly labeled multilingual training annotations, e.g., (Richman and Schone, 2008; Nothman et al., 2013; Al-Rfou et al., 2015). The second type of approaches are based on direct model transfer, e.g., (T¨ackstr¨om et al., 2012; Tsai et al., 2016). The basic idea is to train a single"
P17-1135,E14-1049,0,0.025676,"ross-lingual word clusters were then used to generate universal features. Tsai et al. (2016) applied the cross-lingual wikifier developed in (Tsai and Roth, 2016) and multilingual Wikipedia dump to generate languageindependent labels (FreeBase types and Wikipedia categories) for n-grams in a document, and those labels were used as universal features. Different ways of obtaining cross-lingual embeddings have been proposed in the literature. One approach builds monolingual representations separately and then brings them to the same space typically using a seed dictionary (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Another line of work builds inter-lingual representations simultaneously, often by generating mixed language corpora using the supervision at hand (aligned sentences, documents, etc.) (Vuli´c and Moens, 2015; Gouws et al., 2015). We opt for the first solution in this paper because of its flexibility: we can map all languages to English rather than requiring separate embeddings for each language pair. Additionally we are able to easily add a new language without any constraints on the type of data needed. Note that although we do not specifically create interlingual representations, by traini"
P17-1135,N15-1157,0,0.047155,"Missing"
P17-1135,P15-1119,0,0.149227,"Missing"
P17-1135,H05-1012,0,0.123001,"Missing"
P17-1135,C12-1089,0,0.127632,"Missing"
P17-1135,N16-1030,0,0.383066,"ny information extraction applications, including relation extraction, entity linking, question answering and text mining. Building fast and accurate NER systems is a crucial step towards enabling large-scale automated information extraction and knowledge discovery on the huge volumes of electronic documents existing today. The state-of-the-art NER systems are supervised machine learning models (Nadeau and Sekine, 2007), including maximum entropy Markov models (MEMMs) (McCallum et al., 2000), conditional random fields (CRFs) (Lafferty et al., 2001) and neural networks (Collobert et al., 2011; Lample et al., 2016). To achieve high accuracy, a NER system needs to be trained with a large amount of manually annotated data, and is often supplied with language-specific resources (e.g., gazetteers, word clusters, etc.). Annotating NER data by human is rather expensive and timeconsuming, and can be quite difficult for a new language. This creates a big challenge in building NER systems of multiple languages for supporting multilingual information extraction applications. The difficulty of acquiring supervised annotation raises the following question: given a welltrained NER system in a source language (e.g.,"
P17-1135,N15-1142,0,0.0169383,"ns which can map words in any language into English and we simply use the English NER system to decode. In particular, by mapping all languages into English, we are using one universal NER system and we do not need to re-train the system when a new language is added. 4.1 Monolingual Word Embeddings We first build vector representations of words (word embeddings) for a language using monolingual data. We use a variant of the Continuous Bag-of-Words (CBOW) word2vec model (Mikolov et al., 2013a), which concatenates the context words surrounding a target word instead of adding them (similarly to (Ling et al., 2015)). 1 Additionally, we employ weights w = dist(x,x c) that decay with the distance of a context word xc to a target word x. Tests on word similarity benchmarks show this variant leads to small improvements over the standard CBOW model. We train 300-dimensional word embeddings for English. Following (Mikolov et al., 2013b), we use larger dimensional embeddings for the target languages, namely 800. We train word2vec for 1 epoch for English/Spanish and 5 epochs for the rest of the languages for which we have less data. 4.2 Cross-Lingual Representation Projection We learn cross-lingual word embeddi"
P17-1135,D16-1135,1,0.942858,"4.7 66.0 81.7 R 54.7 41.4 42.7 59.5 59.4 59.9 60.6 64.3 F1 62.1 57.4 53.5 65.5 64.5 60.8 62.6 80.6 F1 67.8 60.3 52.5 69.3 68.8 65.0 67.8 82.3 F1 62.1 54.4 51.4 64.4 64.7 62.0 63.6 71.8 CoNLL development sets. Compared with no data selection, the data selection scheme improves the annotation projection approach by 2.7/2.0/2.7 F1 score on the Spanish/Dutch/German development data. In addition to standard NER features such as n-gram word features, word type features, prefix and suffix features, the target-language NER systems also use the multilingual Wikipedia entity type mappings developed in (Ni and Florian, 2016) to generate dictionary features and as decoding constraints, which improve the annotation projection approach by 3.0/5.4/7.9 F1 score on the Spanish/Dutch/German development data. Table 4: CoNLL NER development data. projection-based systems by 2.2 to 7.4 F1 score. We also provide the performance of supervised learning where the NER system is trained with human-annotated data in the target language (with size shown in the bracket). While the performance of the weakly supervised systems is not as good as supervised learning, it is important to build weakly supervised systems with decent perfor"
P17-1135,D15-1039,0,0.0304598,"notation projection approaches (Yarowsky et al., 2001; Zitouni and Florian, 2008; Ehrmann et al., 2011) project NER tags across language pairs using parallel corpora or translations. Wang and Manning (2014) proposed a variant of annotation projection which projects expectations of tags and uses them as constraints to train a model based on generalized expectation criteria. Annotation projection has also been applied to several other cross-lingual NLP tasks, including word sense disambiguation (Diab and Resnik, 2002), part-of-speech (POS) tagging (Yarowsky et al., 2001) and dependency parsing (Rasooli and Collins, 2015). Wikipedia has been exploited to generate weakly labeled multilingual NER training data. The basic idea is to first categorize Wikipedia pages into entity types, either based on manually constructed rules that utilize the category information of Wikipedia (Richman and Schone, 2008) or Freebase attributes (Al-Rfou et al., 2015), or via a classifier trained with manually labeled Wikipedia pages (Nothman et al., 2013). Heuristic rules are then developed in these works to automatically label the Wikipedia text with NER tags. Ni and Florian (2016) built high-accuracy, high-coverage multilingual Wi"
P17-1135,P08-1001,0,0.352214,"e weakly labeled data is through annotation projection on aligned parallel corpora or translations between a source language and a target language, e.g., (Yarowsky et al., 2001; Zitouni and Florian, 2008; Ehrmann et al., 2011). Another way is to utilize the text and structure of 1470 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1470–1480 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1135 Wikipedia to generate weakly labeled multilingual training annotations, e.g., (Richman and Schone, 2008; Nothman et al., 2013; Al-Rfou et al., 2015). The second type of approaches are based on direct model transfer, e.g., (T¨ackstr¨om et al., 2012; Tsai et al., 2016). The basic idea is to train a single NER system in the source language with language independent features, so the system can be applied to other languages using those universal features. In this paper, we make the following contributions to weakly supervised cross-lingual NER with no human annotation in the target languages. First, for the annotation projection approach, we develop a heuristic, language-independent data selection s"
P17-1135,N12-1052,0,0.410024,"Missing"
P17-1135,K16-1022,0,0.381312,", 2001; Zitouni and Florian, 2008; Ehrmann et al., 2011). Another way is to utilize the text and structure of 1470 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1470–1480 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1135 Wikipedia to generate weakly labeled multilingual training annotations, e.g., (Richman and Schone, 2008; Nothman et al., 2013; Al-Rfou et al., 2015). The second type of approaches are based on direct model transfer, e.g., (T¨ackstr¨om et al., 2012; Tsai et al., 2016). The basic idea is to train a single NER system in the source language with language independent features, so the system can be applied to other languages using those universal features. In this paper, we make the following contributions to weakly supervised cross-lingual NER with no human annotation in the target languages. First, for the annotation projection approach, we develop a heuristic, language-independent data selection scheme that seeks to select good-quality projection-labeled NER data from comparable corpora. Experimental results show that the data selection scheme can significan"
P17-1135,N16-1072,0,0.0253469,"h NER tags. Ni and Florian (2016) built high-accuracy, high-coverage multilingual Wikipedia entity type mappings using weakly labeled data and applied those mappings as decoding constrains or dictionary features to improve multilingual NER systems. For direct NER model transfer, T¨ackstr¨om et al. (2012) built cross-lingual word clusters using monolingual data in source/target languages and aligned parallel data between source and target languages. The cross-lingual word clusters were then used to generate universal features. Tsai et al. (2016) applied the cross-lingual wikifier developed in (Tsai and Roth, 2016) and multilingual Wikipedia dump to generate languageindependent labels (FreeBase types and Wikipedia categories) for n-grams in a document, and those labels were used as universal features. Different ways of obtaining cross-lingual embeddings have been proposed in the literature. One approach builds monolingual representations separately and then brings them to the same space typically using a seed dictionary (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Another line of work builds inter-lingual representations simultaneously, often by generating mixed language corpora using the supervisio"
P17-1135,P15-2118,0,0.0364928,"Missing"
P17-1135,Q14-1005,0,0.0871512,"013) Tsai et al. (2016) Co-Decoding (Conf): AP+NN1 Co-Decoding (Rank): AP+NN1 Supervised (206K) P x x x 64.9 64.6 82.5 P x x x 69.1 69.3 85.1 P x x x 68.5 68.3 79.6 R x x x 65.2 63.9 82.3 R x x x 62.0 61.0 83.9 R x x x 51.0 50.4 65.3 F1 59.3 61.0 60.6 65.1 64.3 82.4 F1 58.4 64.0 61.6 65.4 64.8 84.5 F1 40.4 55.8 48.1 58.5 58.0 71.8 Table 5: CoNLL NER test data. supervised learning. 7 Related Work The traditional annotation projection approaches (Yarowsky et al., 2001; Zitouni and Florian, 2008; Ehrmann et al., 2011) project NER tags across language pairs using parallel corpora or translations. Wang and Manning (2014) proposed a variant of annotation projection which projects expectations of tags and uses them as constraints to train a model based on generalized expectation criteria. Annotation projection has also been applied to several other cross-lingual NLP tasks, including word sense disambiguation (Diab and Resnik, 2002), part-of-speech (POS) tagging (Yarowsky et al., 2001) and dependency parsing (Rasooli and Collins, 2015). Wikipedia has been exploited to generate weakly labeled multilingual NER training data. The basic idea is to first categorize Wikipedia pages into entity types, either based on m"
P17-1135,H01-1035,0,0.932888,"sed annotation raises the following question: given a welltrained NER system in a source language (e.g., English), how can one go about extending it to a new language with decent performance and no human annotation in the target language? There are mainly two types of approaches for building weakly supervised cross-lingual NER systems. The first type of approaches create weakly labeled NER training data in a target language. One way to create weakly labeled data is through annotation projection on aligned parallel corpora or translations between a source language and a target language, e.g., (Yarowsky et al., 2001; Zitouni and Florian, 2008; Ehrmann et al., 2011). Another way is to utilize the text and structure of 1470 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1470–1480 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1135 Wikipedia to generate weakly labeled multilingual training annotations, e.g., (Richman and Schone, 2008; Nothman et al., 2013; Al-Rfou et al., 2015). The second type of approaches are based on direct model transfer, e.g., (T¨ackstr¨om et al., 2012; Tsai e"
P17-1135,D08-1063,1,0.920291,"he following question: given a welltrained NER system in a source language (e.g., English), how can one go about extending it to a new language with decent performance and no human annotation in the target language? There are mainly two types of approaches for building weakly supervised cross-lingual NER systems. The first type of approaches create weakly labeled NER training data in a target language. One way to create weakly labeled data is through annotation projection on aligned parallel corpora or translations between a source language and a target language, e.g., (Yarowsky et al., 2001; Zitouni and Florian, 2008; Ehrmann et al., 2011). Another way is to utilize the text and structure of 1470 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1470–1480 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1135 Wikipedia to generate weakly labeled multilingual training annotations, e.g., (Richman and Schone, 2008; Nothman et al., 2013; Al-Rfou et al., 2015). The second type of approaches are based on direct model transfer, e.g., (T¨ackstr¨om et al., 2012; Tsai et al., 2016). The basic ide"
P17-1135,W02-2024,0,0.803845,"target language without the need for re-training. Finally, we design two co-decoding schemes that combine the outputs (views) of the two projection-based systems to produce an output that is more accurate than the outputs of individual systems. We evaluate the performance of the proposed approaches on both in-house and open NER data sets for a number of target languages. The results show that the combined systems outperform the state-of-the-art cross-lingual NER approaches proposed in T¨ackstr¨om et al. (2012), Nothman et al. (2013) and Tsai et al. (2016) on the CoNLL NER test data (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). We organize the paper as follows. In Section 2 we introduce three NER models that are used in the paper. In Section 3 we present an annotation projection approach with effective data selection. In Section 4 we propose a representation projection approach for direct NER model transfer. In Section 5 we describe two co-decoding schemes that effectively combine the outputs of two projection-based approaches. In Section 6 we evaluate the performance of the proposed approaches. We describe related work in Section 7 and conclude the paper in Section 8. 2 NER Mo"
P17-1135,W03-0419,0,0.448734,"Missing"
P18-2063,P02-1014,0,0.133074,"In the extrinsic evaluation, we show that our English model helps achieve superior entity linking accuracy on Chinese and Spanish test sets than the top 2015 TAC system without using any annotated data from Chinese or Spanish. 1 Introduction Cross-lingual models for NLP tasks are important since they can be used on data from a new language without requiring annotation from the new language (Ji et al., 2014, 2015). This paper investigates the use of multi-lingual embeddings (Faruqui and Dyer, 2014; Upadhyay et al., 2016) for building cross-lingual models for the task of coreference resolution (Ng and Cardie, 2002; Pradhan et al., 2012). Consider the following text from a Spanish news article: “Tormenta de nieve afecta a 100 millones de personas en EEUU. Unos 100 millones de personas enfrentaban el s´abado nuevas dificultades tras la enorme tormenta de nieve de hace d´ıas en la costa este de Estados Unidos.” The mentions “EEUU” (“US” in English) and “Estados Unidos” (“United States” in English) are coreferent. A coreference model trained on English data is unlikely to coreference these two 395 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages"
P18-2063,D08-1031,0,0.0375538,"er on Es and 2.6 points below Zh, implying coreference is a vital task for EL. Our “EL + En Coref” outperforms the 2015 TAC best system by 0.7 points on Es and 0.8 points on Zh, without requiring any training data for coreference on Es and Zh respectively. Finally, we show the SOTA results on these two data sets recently reported by S IL 18. Although their EL model does not use any supervision from Es or Zh, their coreference resolution model is trained on a large internal data set on the same language as 5 Related Work Rule based (Raghunathan et al., 2010) and statistical coreference models (Bengtson and Roth, 2008; Rahman and Ng, 2009; Fernandes et al., 2012; Durrett et al., 2013; Clark and Manning, 2015; Martschat and Strube, 2015; Bj¨orkelund and Kuhn, 2014) are hard to transfer across languages due to their use of lexical features or patterns in the rules. Neural coreference is promising since it allows cross-lingual transfer using multilingual embedding. However, most of the recent neural coreference models (Wiseman et al., 2015, 2016; Clark and Manning, 2015, 2016; Lee et al., 2017) have focused on training and testing on the same language. In contrast, our model performs cross-lingual coreference"
P18-2063,P17-1135,1,0.831619,"d and Kuhn, 2014) are hard to transfer across languages due to their use of lexical features or patterns in the rules. Neural coreference is promising since it allows cross-lingual transfer using multilingual embedding. However, most of the recent neural coreference models (Wiseman et al., 2015, 2016; Clark and Manning, 2015, 2016; Lee et al., 2017) have focused on training and testing on the same language. In contrast, our model performs cross-lingual coreference. There have been some recent promising results regarding such cross-lingual models for other tasks, most notably mention detection(Ni et al., 2017) and EL (Tsai and Roth, 2016; Sil and Florian, 2016). In this work, we show that such promise exists for coreference also. The tasks of EL and coreference are intrinsically related, prompting joint models (Durrett and Klein, 2014; Hajishirzi et al., 2013). However, the recent SOTA was obtained using pipeline models of coreference and EL (Sil et al., 2018). Compared to a joint model, pipeline models are easier to implement, improve and adapt to a new domain. 6 Conclusion The proposed cross-lingual coreference model was found to be empirically strong in both intrinsic and extrinsic evaluations i"
P18-2063,P14-1005,0,0.0554291,"Missing"
P18-2063,P15-1136,0,0.0166124,"Coref” outperforms the 2015 TAC best system by 0.7 points on Es and 0.8 points on Zh, without requiring any training data for coreference on Es and Zh respectively. Finally, we show the SOTA results on these two data sets recently reported by S IL 18. Although their EL model does not use any supervision from Es or Zh, their coreference resolution model is trained on a large internal data set on the same language as 5 Related Work Rule based (Raghunathan et al., 2010) and statistical coreference models (Bengtson and Roth, 2008; Rahman and Ng, 2009; Fernandes et al., 2012; Durrett et al., 2013; Clark and Manning, 2015; Martschat and Strube, 2015; Bj¨orkelund and Kuhn, 2014) are hard to transfer across languages due to their use of lexical features or patterns in the rules. Neural coreference is promising since it allows cross-lingual transfer using multilingual embedding. However, most of the recent neural coreference models (Wiseman et al., 2015, 2016; Clark and Manning, 2015, 2016; Lee et al., 2017) have focused on training and testing on the same language. In contrast, our model performs cross-lingual coreference. There have been some recent promising results regarding such cross-lingual models for othe"
P18-2063,W12-4501,0,0.0475821,"uation, we show that our English model helps achieve superior entity linking accuracy on Chinese and Spanish test sets than the top 2015 TAC system without using any annotated data from Chinese or Spanish. 1 Introduction Cross-lingual models for NLP tasks are important since they can be used on data from a new language without requiring annotation from the new language (Ji et al., 2014, 2015). This paper investigates the use of multi-lingual embeddings (Faruqui and Dyer, 2014; Upadhyay et al., 2016) for building cross-lingual models for the task of coreference resolution (Ng and Cardie, 2002; Pradhan et al., 2012). Consider the following text from a Spanish news article: “Tormenta de nieve afecta a 100 millones de personas en EEUU. Unos 100 millones de personas enfrentaban el s´abado nuevas dificultades tras la enorme tormenta de nieve de hace d´ıas en la costa este de Estados Unidos.” The mentions “EEUU” (“US” in English) and “Estados Unidos” (“United States” in English) are coreferent. A coreference model trained on English data is unlikely to coreference these two 395 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 395–400 c Melbourne, Au"
P18-2063,P16-1061,0,0.0292772,"odel based on dev set. Coreference Results: For each language, we follow the official train-test splits made in the TAC 2015 competition. Except, a small portion of the training set is held out as development set for tuning the models. All experimental results on all languages reported in this paper were obtained on the official test sets. We used the official CoNLL 2012 evaluation script and report MUC, B3 and CEAF scores and their average (CONLL score). See Pradhan et al. (2011, 2012). To test the competitiveness of our model with other SOTA models, we train the publicly available system of Clark and Manning (2016) (henceforth, C&M16) on the TAC 15 En training set and test on the TAC 15 En test set. The C&M16 system normally outputs both noun phrase mentions and their coreference and is trained on Ontonotes. To ensure a fair comparison, we changed the configuration of the system to accept gold mention boundaries both during training and testing. Since the system was unable to deal with partially overlapping mentions, we excluded such mentions in the evaluation. Table 2 shows that our model outperforms C&M16 by 8 points. For cross-lingual experiments, we build monolingual embeddings for En, Zh and Es usi"
P18-2063,P13-1012,0,0.0195413,"for EL. Our “EL + En Coref” outperforms the 2015 TAC best system by 0.7 points on Es and 0.8 points on Zh, without requiring any training data for coreference on Es and Zh respectively. Finally, we show the SOTA results on these two data sets recently reported by S IL 18. Although their EL model does not use any supervision from Es or Zh, their coreference resolution model is trained on a large internal data set on the same language as 5 Related Work Rule based (Raghunathan et al., 2010) and statistical coreference models (Bengtson and Roth, 2008; Rahman and Ng, 2009; Fernandes et al., 2012; Durrett et al., 2013; Clark and Manning, 2015; Martschat and Strube, 2015; Bj¨orkelund and Kuhn, 2014) are hard to transfer across languages due to their use of lexical features or patterns in the rules. Neural coreference is promising since it allows cross-lingual transfer using multilingual embedding. However, most of the recent neural coreference models (Wiseman et al., 2015, 2016; Clark and Manning, 2015, 2016; Lee et al., 2017) have focused on training and testing on the same language. In contrast, our model performs cross-lingual coreference. There have been some recent promising results regarding such cros"
P18-2063,W11-1901,0,0.311104,"and Linking (EDL) Task (Ji et al., 2015), achieves competitive results to models trained directly on Chinese and Spanish respectively. Secondly, a pipeline consisting of this coreference model and an Entity Linking (henceforth EL) model can achieve superior linking accuracy than the official top ranking system in 2015 on Chinese and Spanish test sets, without using any supervision in Chinese or Spanish. Although most of the active coreference research is on solving the problem of noun phrase coreference resolution in the Ontonotes data set, invigorated by the 2011 and 2012 CoNLL shared task (Pradhan et al., 2011, 2012), there are many important applications/end tasks where the mentions of interest are not noun phrases. Consider the sentence, “(U.S. president Barack Obama who started ((his) political career) in (Illinois)), was born in (Hawaii).” The bracketing represents the Ontonotes style We propose an entity-centric neural crosslingual coreference model that builds on multi-lingual embeddings and languageindependent features. We perform both intrinsic and extrinsic evaluations of our model. In the intrinsic evaluation, we show that our model, when trained on English and tested on Chinese and Spani"
P18-2063,Q14-1037,0,0.0351832,"Missing"
P18-2063,D10-1048,0,0.0796872,"ollowed by the EL model. The former is 3 points below the latter on Es and 2.6 points below Zh, implying coreference is a vital task for EL. Our “EL + En Coref” outperforms the 2015 TAC best system by 0.7 points on Es and 0.8 points on Zh, without requiring any training data for coreference on Es and Zh respectively. Finally, we show the SOTA results on these two data sets recently reported by S IL 18. Although their EL model does not use any supervision from Es or Zh, their coreference resolution model is trained on a large internal data set on the same language as 5 Related Work Rule based (Raghunathan et al., 2010) and statistical coreference models (Bengtson and Roth, 2008; Rahman and Ng, 2009; Fernandes et al., 2012; Durrett et al., 2013; Clark and Manning, 2015; Martschat and Strube, 2015; Bj¨orkelund and Kuhn, 2014) are hard to transfer across languages due to their use of lexical features or patterns in the rules. Neural coreference is promising since it allows cross-lingual transfer using multilingual embedding. However, most of the recent neural coreference models (Wiseman et al., 2015, 2016; Clark and Manning, 2015, 2016; Lee et al., 2017) have focused on training and testing on the same languag"
P18-2063,E14-1049,0,0.221558,"on Chinese and Spanish, achieves competitive results to the models trained directly on Chinese and Spanish respectively. In the extrinsic evaluation, we show that our English model helps achieve superior entity linking accuracy on Chinese and Spanish test sets than the top 2015 TAC system without using any annotated data from Chinese or Spanish. 1 Introduction Cross-lingual models for NLP tasks are important since they can be used on data from a new language without requiring annotation from the new language (Ji et al., 2014, 2015). This paper investigates the use of multi-lingual embeddings (Faruqui and Dyer, 2014; Upadhyay et al., 2016) for building cross-lingual models for the task of coreference resolution (Ng and Cardie, 2002; Pradhan et al., 2012). Consider the following text from a Spanish news article: “Tormenta de nieve afecta a 100 millones de personas en EEUU. Unos 100 millones de personas enfrentaban el s´abado nuevas dificultades tras la enorme tormenta de nieve de hace d´ıas en la costa este de Estados Unidos.” The mentions “EEUU” (“US” in English) and “Estados Unidos” (“United States” in English) are coreferent. A coreference model trained on English data is unlikely to coreference these"
P18-2063,D09-1101,0,0.0610857,"elow Zh, implying coreference is a vital task for EL. Our “EL + En Coref” outperforms the 2015 TAC best system by 0.7 points on Es and 0.8 points on Zh, without requiring any training data for coreference on Es and Zh respectively. Finally, we show the SOTA results on these two data sets recently reported by S IL 18. Although their EL model does not use any supervision from Es or Zh, their coreference resolution model is trained on a large internal data set on the same language as 5 Related Work Rule based (Raghunathan et al., 2010) and statistical coreference models (Bengtson and Roth, 2008; Rahman and Ng, 2009; Fernandes et al., 2012; Durrett et al., 2013; Clark and Manning, 2015; Martschat and Strube, 2015; Bj¨orkelund and Kuhn, 2014) are hard to transfer across languages due to their use of lexical features or patterns in the rules. Neural coreference is promising since it allows cross-lingual transfer using multilingual embedding. However, most of the recent neural coreference models (Wiseman et al., 2015, 2016; Clark and Manning, 2015, 2016; Lee et al., 2017) have focused on training and testing on the same language. In contrast, our model performs cross-lingual coreference. There have been som"
P18-2063,P16-1213,1,0.894321,"he candidate link’s Wikipedia page which are fed to a am type(mi ),m type(mj ) r vmi ,mj N Here N is a normalizing constant given by: s X N= a2m type(mi ),m type(mj ) (mi ,mj )∈M (e1 ,e2 ) This layer represents attention over the mention pair embeddings where attention weights are based on the m types of the mention pairs. Sigmoid Layer: ves1 ,e2 = σ(W (2) vea1 ,e2 ) Output Layer: 1 P (y12 = 1|e1 , e2 ) = −ws .ves1 ,e2 1+e 397 feed-forward neural layer which acts as a binary classifier to predict the correct link for m. Specifically, the feature abstraction layer computes cosine similarities (Sil and Florian, 2016) between the representations of the source query document and the target Wikipedia pages over various granularities. These representations are computed by performing CNNs and LSTMs over the context of the entities. Then these similarities are fed into a Multi-perspective Binning layer which maps each similarity into a higher dimensional vector. We also train fine-grained similarities and dissimilarities between the query and candidate document from multiple perspectives, combined with convolution and tensor networks. The model achieves state-of-the-art (SOTA) results on English benchmark EL da"
P18-2063,D13-1029,0,0.0573345,"Missing"
P18-2063,N16-1072,0,0.0260668,"d to transfer across languages due to their use of lexical features or patterns in the rules. Neural coreference is promising since it allows cross-lingual transfer using multilingual embedding. However, most of the recent neural coreference models (Wiseman et al., 2015, 2016; Clark and Manning, 2015, 2016; Lee et al., 2017) have focused on training and testing on the same language. In contrast, our model performs cross-lingual coreference. There have been some recent promising results regarding such cross-lingual models for other tasks, most notably mention detection(Ni et al., 2017) and EL (Tsai and Roth, 2016; Sil and Florian, 2016). In this work, we show that such promise exists for coreference also. The tasks of EL and coreference are intrinsically related, prompting joint models (Durrett and Klein, 2014; Hajishirzi et al., 2013). However, the recent SOTA was obtained using pipeline models of coreference and EL (Sil et al., 2018). Compared to a joint model, pipeline models are easier to implement, improve and adapt to a new domain. 6 Conclusion The proposed cross-lingual coreference model was found to be empirically strong in both intrinsic and extrinsic evaluations in the context of an entity l"
P18-2063,D17-1018,0,0.0620839,"the same language as 5 Related Work Rule based (Raghunathan et al., 2010) and statistical coreference models (Bengtson and Roth, 2008; Rahman and Ng, 2009; Fernandes et al., 2012; Durrett et al., 2013; Clark and Manning, 2015; Martschat and Strube, 2015; Bj¨orkelund and Kuhn, 2014) are hard to transfer across languages due to their use of lexical features or patterns in the rules. Neural coreference is promising since it allows cross-lingual transfer using multilingual embedding. However, most of the recent neural coreference models (Wiseman et al., 2015, 2016; Clark and Manning, 2015, 2016; Lee et al., 2017) have focused on training and testing on the same language. In contrast, our model performs cross-lingual coreference. There have been some recent promising results regarding such cross-lingual models for other tasks, most notably mention detection(Ni et al., 2017) and EL (Tsai and Roth, 2016; Sil and Florian, 2016). In this work, we show that such promise exists for coreference also. The tasks of EL and coreference are intrinsically related, prompting joint models (Durrett and Klein, 2014; Hajishirzi et al., 2013). However, the recent SOTA was obtained using pipeline models of coreference and"
P18-2063,P16-1157,0,0.0280005,"achieves competitive results to the models trained directly on Chinese and Spanish respectively. In the extrinsic evaluation, we show that our English model helps achieve superior entity linking accuracy on Chinese and Spanish test sets than the top 2015 TAC system without using any annotated data from Chinese or Spanish. 1 Introduction Cross-lingual models for NLP tasks are important since they can be used on data from a new language without requiring annotation from the new language (Ji et al., 2014, 2015). This paper investigates the use of multi-lingual embeddings (Faruqui and Dyer, 2014; Upadhyay et al., 2016) for building cross-lingual models for the task of coreference resolution (Ng and Cardie, 2002; Pradhan et al., 2012). Consider the following text from a Spanish news article: “Tormenta de nieve afecta a 100 millones de personas en EEUU. Unos 100 millones de personas enfrentaban el s´abado nuevas dificultades tras la enorme tormenta de nieve de hace d´ıas en la costa este de Estados Unidos.” The mentions “EEUU” (“US” in English) and “Estados Unidos” (“United States” in English) are coreferent. A coreference model trained on English data is unlikely to coreference these two 395 Proceedings of t"
P18-2063,Q15-1029,0,0.0140115,"15 TAC best system by 0.7 points on Es and 0.8 points on Zh, without requiring any training data for coreference on Es and Zh respectively. Finally, we show the SOTA results on these two data sets recently reported by S IL 18. Although their EL model does not use any supervision from Es or Zh, their coreference resolution model is trained on a large internal data set on the same language as 5 Related Work Rule based (Raghunathan et al., 2010) and statistical coreference models (Bengtson and Roth, 2008; Rahman and Ng, 2009; Fernandes et al., 2012; Durrett et al., 2013; Clark and Manning, 2015; Martschat and Strube, 2015; Bj¨orkelund and Kuhn, 2014) are hard to transfer across languages due to their use of lexical features or patterns in the rules. Neural coreference is promising since it allows cross-lingual transfer using multilingual embedding. However, most of the recent neural coreference models (Wiseman et al., 2015, 2016; Clark and Manning, 2015, 2016; Lee et al., 2017) have focused on training and testing on the same language. In contrast, our model performs cross-lingual coreference. There have been some recent promising results regarding such cross-lingual models for other tasks, most notably mentio"
P18-2063,N16-1114,0,0.0291735,"Missing"
P18-2063,P15-1137,0,0.0193623,"resolution model is trained on a large internal data set on the same language as 5 Related Work Rule based (Raghunathan et al., 2010) and statistical coreference models (Bengtson and Roth, 2008; Rahman and Ng, 2009; Fernandes et al., 2012; Durrett et al., 2013; Clark and Manning, 2015; Martschat and Strube, 2015; Bj¨orkelund and Kuhn, 2014) are hard to transfer across languages due to their use of lexical features or patterns in the rules. Neural coreference is promising since it allows cross-lingual transfer using multilingual embedding. However, most of the recent neural coreference models (Wiseman et al., 2015, 2016; Clark and Manning, 2015, 2016; Lee et al., 2017) have focused on training and testing on the same language. In contrast, our model performs cross-lingual coreference. There have been some recent promising results regarding such cross-lingual models for other tasks, most notably mention detection(Ni et al., 2017) and EL (Tsai and Roth, 2016; Sil and Florian, 2016). In this work, we show that such promise exists for coreference also. The tasks of EL and coreference are intrinsically related, prompting joint models (Durrett and Klein, 2014; Hajishirzi et al., 2013). However, the recent SO"
P19-1451,D17-1130,1,0.927908,"beled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and"
P19-1451,D15-1041,1,0.831875,"nd concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM dependency parser by Dyer et al. (2015). This approach allows unbounded lookahead and makes use of greedy inference. BO also learns character-level word representations to capitalize on morphosyntactic regularities (Ballesteros et al., 2015). BO uses recurrent neural networks to represent the stack data structures that underlie many linear-time parsing algorithms. It follows transition-based parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2003, 2008); words are read from a buffer and they are incrementally combined, in a stack, with a set of actions towards producing the final parse. The input is a sentence and the output is a complete AMR graph without 4586 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4586–4592 c Florence, Italy, July 28 - August 2, 2019. 2019 Association"
P19-1451,D16-1211,1,0.880434,"oracle action sequences.(As mentioned above, the oracle upper bound is only 93.3 F1. With the enhanced alignments, BO reported 89.5 F1 in the LDC2014 development set). Second, even for the perfectly aligned sentences, the oracle action sequence is not the only or the best action sequence that can lead to the gold graph; there could be shorter sequences that are easier to learn. Therefore, strictly binding the training objective to the oracle action sequences can lead to suboptimal performance, as evidenced in (Daumé III and Marcu, 2005; Daumé III et al., 2009; Goldberg and Nivre, 2012, 2013; Ballesteros et al., 2016) among others. To circumvent these issues, we resort to a Reinforcement Learning (RL) objective where the Smatch score of the predicted graph for a given sentence is used as reward. This alleviates the strong dependency on hard alignment and leaves room to training with exploration of the action space. This line of work is also motivated by Goodman et al. (2016), who used imitation learning to build AMR parses from dependency trees. We use the self-critical policy gradient training algorithm by Rennie et al. (2017) which is a special case of the REINFORCE algorithm of Williams (1992) with a ba"
P19-1451,W13-2322,0,0.430589,"ck-LSTM transition-based AMR parser (Ballesteros and Al-Onaizan, 2017) by augmenting training with Policy Learning and rewarding the Smatch score of sampled graphs. In addition, we also combined several AMR-to-text alignments with an attention mechanism and we supplemented the parser with pre-processed concept identification, named entities and contextualized embeddings. We achieve a highly competitive performance that is comparable to the best published results. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-"
P19-1451,P13-2131,0,0.656993,"tural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also c"
P19-1451,P15-3007,0,0.0187689,"d Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros a"
P19-1451,E17-1053,0,0.0949736,"TMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it"
P19-1451,P15-1033,1,0.866449,"sentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017;"
P19-1451,S16-1186,0,0.0728509,"ents AMR annotations do not provide alignments between the nodes of an AMR graph and the tokens in the corresponding sentence. We need such alignments to generate action sequences with an oracle for training. The parser is then trained to generate these action sequences. The quality of word-to-graph alignments has a direct impact in the accuracy of the parser. In previous work, both rule-based (Flanigan et al., 2014) and machine learning (Pourdamghani et al., 2014) methods have been used to produce word-to-graph alignments. Once generated, the alignments are often not updated during training (Flanigan et al., 2016; Damonte et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017). More recently, Lyu and Titov (2018) learn these alignments as latent variables. In this work, we combine pre-learned (hard) alignments with an attention mechanism. As shown in section 4, the combination has a synergistic effect. In the following, we first explain our method for producing hard alignments and then we elaborate on the attention mechanism. Hard Alignments Generation: In order to produce word-to-graph alignments, we combine the outputs of the symmetrized Expectation Maximization approach (SEM) of Pourdamghani et"
P19-1451,P14-1134,0,0.723706,"ild upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM trans"
P19-1451,P17-1043,0,0.0327329,"of an AMR graph and the tokens in the corresponding sentence. We need such alignments to generate action sequences with an oracle for training. The parser is then trained to generate these action sequences. The quality of word-to-graph alignments has a direct impact in the accuracy of the parser. In previous work, both rule-based (Flanigan et al., 2014) and machine learning (Pourdamghani et al., 2014) methods have been used to produce word-to-graph alignments. Once generated, the alignments are often not updated during training (Flanigan et al., 2016; Damonte et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017). More recently, Lyu and Titov (2018) learn these alignments as latent variables. In this work, we combine pre-learned (hard) alignments with an attention mechanism. As shown in section 4, the combination has a synergistic effect. In the following, we first explain our method for producing hard alignments and then we elaborate on the attention mechanism. Hard Alignments Generation: In order to produce word-to-graph alignments, we combine the outputs of the symmetrized Expectation Maximization approach (SEM) of Pourdamghani et al. (2014) with those of the rule-based algorithm (JAMR) of Flanigan"
P19-1451,C12-1059,0,0.0596808,"graph nodes introduce noise into oracle action sequences.(As mentioned above, the oracle upper bound is only 93.3 F1. With the enhanced alignments, BO reported 89.5 F1 in the LDC2014 development set). Second, even for the perfectly aligned sentences, the oracle action sequence is not the only or the best action sequence that can lead to the gold graph; there could be shorter sequences that are easier to learn. Therefore, strictly binding the training objective to the oracle action sequences can lead to suboptimal performance, as evidenced in (Daumé III and Marcu, 2005; Daumé III et al., 2009; Goldberg and Nivre, 2012, 2013; Ballesteros et al., 2016) among others. To circumvent these issues, we resort to a Reinforcement Learning (RL) objective where the Smatch score of the predicted graph for a given sentence is used as reward. This alleviates the strong dependency on hard alignment and leaves room to training with exploration of the action space. This line of work is also motivated by Goodman et al. (2016), who used imitation learning to build AMR parses from dependency trees. We use the self-critical policy gradient training algorithm by Rennie et al. (2017) which is a special case of the REINFORCE algor"
P19-1451,Q13-1033,0,0.0548568,"Missing"
P19-1451,P16-1001,0,0.127283,"lts. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contr"
P19-1451,D18-1198,0,0.620886,"h study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1)"
P19-1451,N15-1142,0,0.0320197,"tities from the AMR dataset (there are more than 100 entity types in the AMR language) and we trained a neural network NER model (Ni et al., 2017) to predict NER labels for the AMR parser. In the NER model, the target word and its surrounding words and tags are used as features. We jackknifed (90/10) the training data, to train the AMR parser. The ten jackknifed models got an average NER F1 score of 79.48 on the NER dev set. 2.4 Contextualized Vectors Recent work has shown that the use of pre-trained networks improves the performance of downstream tasks. BO uses pre-trained word embeddings by Ling et al. (2015) along with learned character embeddings. In this work, we explore the effect of using contextualized word vectors as pre-trained word embeddings. We experiment with recent context based embedding obtained with BERT (Devlin et al., 2018). We use average of last 4 layers of BERT Large model with hidden representations of size 1024. We produce the word representation by mean pooling the representations of word piece tokens obtained using BERT. We only use the contextualized word vectors as input to our model, we do not back-propagate through the BERT layers. 2.5 Wikification Given that BO does n"
P19-1451,D18-1264,0,0.347593,"with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM depend"
P19-1451,D15-1166,0,0.0127201,"ed and fill in intermediate nodes again. Soft Alignments via Attention: The parser state is represented by the STACK, BUFFER and a list with the history of actions (which are encoded as LSTMs, the first two being Stack-LSTMs (Dyer et al., 2015)). This forms the vector st that represents the state: st = max {0, W[stt ; bt ; at ] + d} . (1) This vector st is used to predict the best action (and concept to add, if applicable) to take, given the state with a softmax. We complement the state with an attention over the input sentence (Bahdanau et al., 2014). In particular, we use general attention (Luong et al., 2015). In order to do so, we add a bidirectional LSTM encoder to the BO parsing model and we run attention over it in each time step. More formally, the attention weights αi (for position i) are calculated based on the actions predicted so far (represented as aj ), the encoder representation of the sentence (hi ) and a projection weight matrix Wa : ei = a> j Wa hi (2) exp(ei ) . αi = P k exp(ek ) (3) A vector representation (cj ) is computed by a weighted sum of the encoded sentence word representations and the α values. X cj = αi · hi . (4) i 2 https://isi.edu/~damghani/papers/ Aligner.zip 3 When"
P19-1451,P18-1037,0,0.310823,"ach of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of diffe"
P19-1451,S16-1166,0,0.0434133,"trics represent the structure and semantic parsing task. For all the remaining metrics, our parser consistently achieves the second best results. Also, our best single model (16) achieves more than 9 Smatch points on top of BO (0). Guo and Lu (2018)’s parser is a reimplementation of BO with a refined search space (which we did not attempt) and we beat their performance by 5 points. 5 BO reported results on the 2014 dataset. LDC2016E25 and LDC2017T10 contain the same AMR annotations as of March 2016. LDC2017T10 is the general release while LDC2016E25 was released for Semeval 2016 participants (May, 2016). 4589 6 Id 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Experiment BO (JAMR) BO + Label (JAMR) BO + Label 2 + POS 3 + DEP 4 + NER 5 + Concepts 6 + BERT 1 + Attention 8 + POS 9 + DEP 10 + NER 11 + Concepts 12 + BERT11 13 + Smatch 8 + BERT 14 + RL Zhang et al. (2019) Lyu and Titov (2018) van Noord and Bos (2017) Guo and Lu (2018) Smatch 65.9 67.0 68.3 69.0 69.4 69.8 70.9 72.9 69.8 70.4 70.7 70.8 71.8 73.1 73.6 73.4 75.5 76.3 74.4 71.0 69.8 Unlabeled 71 72 73 74 75 75 76 78 75 75 75 76 77 78 78 78 80 79 77 74 74 No WSD 66 68 69 70 70 70 71 73 70 71 71 71 72 74 74 74 76 77 76 72 72 Named Entities 80"
P19-1451,P17-1135,1,0.902876,"Missing"
P19-1451,W03-3017,0,0.289676,"Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM dependency parser by Dyer et al. (2015). This approach allows unbounded lookahead and makes use of greedy inference. BO also learns character-level word representations to capitalize on morphosyntactic regularities (Ballesteros et al., 2015). BO uses recurrent neural networks to represent the stack data structures that underlie many linear-time parsing algorithms. It follows transition-based parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2003, 2008); words are read from a buffer and they are incrementally combined, in a stack, with a set of actions towards producing the final parse. The input is a sentence and the output is a complete AMR graph without 4586 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4586–4592 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics any preprocessing required.1 We use Dynet (Neubig et al., 2017) to implement the parser. In what follows, we present several additions to the original BO model that improved the resul"
P19-1451,J08-4003,0,0.114297,"Missing"
P19-1451,N18-1202,0,0.0509141,"Missing"
P19-1451,D14-1048,0,0.535358,"ased parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of B"
P19-1451,D17-1129,0,0.197202,"s between the nodes of an AMR graph and the tokens in the corresponding sentence. We need such alignments to generate action sequences with an oracle for training. The parser is then trained to generate these action sequences. The quality of word-to-graph alignments has a direct impact in the accuracy of the parser. In previous work, both rule-based (Flanigan et al., 2014) and machine learning (Pourdamghani et al., 2014) methods have been used to produce word-to-graph alignments. Once generated, the alignments are often not updated during training (Flanigan et al., 2016; Damonte et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017). More recently, Lyu and Titov (2018) learn these alignments as latent variables. In this work, we combine pre-learned (hard) alignments with an attention mechanism. As shown in section 4, the combination has a synergistic effect. In the following, we first explain our method for producing hard alignments and then we elaborate on the attention mechanism. Hard Alignments Generation: In order to produce word-to-graph alignments, we combine the outputs of the symmetrized Expectation Maximization approach (SEM) of Pourdamghani et al. (2014) with those of the rule-based al"
P19-1451,P15-2141,0,0.142384,"e performance that is comparable to the best published results. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition,"
P19-1451,N15-1040,0,0.231316,"e performance that is comparable to the best published results. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition,"
P19-1451,W03-3023,0,0.169656,"as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM dependency parser by Dyer et al. (2015). This approach allows unbounded lookahead and makes use of greedy inference. BO also learns character-level word representations to capitalize on morphosyntactic regularities (Ballesteros et al., 2015). BO uses recurrent neural networks to represent the stack data structures that underlie many linear-time parsing algorithms. It follows transition-based parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2003, 2008); words are read from a buffer and they are incrementally combined, in a stack, with a set of actions towards producing the final parse. The input is a sentence and the output is a complete AMR graph without 4586 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4586–4592 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics any preprocessing required.1 We use Dynet (Neubig et al., 2017) to implement the parser. In what follows, we present several additions to the original BO model that impro"
P19-1451,P19-1009,0,0.337492,"Missing"
P19-1451,D16-1065,0,0.058624,"s comparable to the best published results. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several"
P19-1451,N18-1106,0,0.0653911,"Missing"
P19-1451,N18-2023,0,0.0336166,"ents of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has b"
P99-1022,J90-2002,0,0.16842,"Missing"
P99-1022,P98-1035,0,0.0314858,"ce wi... wj ; a common size for m is 3 (trigram language models). Even if n-grams were proved to be very powerful and robust in various tasks involving language models, they have a certain handicap: because of the Markov assumption, the dependency is limited to very short local context. Cache language models (Kuhn and de Mori (1992),Rosenfeld (1994)) try to overcome this limitation by boosting the probability of the words already seen in the history; trigger models (Lau et al. (1993)), even more general, try to capture the interrelationships between words. Models based on syntactic structure (Chelba and Jelinek (1998), Wright et al. (1993)) effectively estimate intra-sentence syntactic word dependencies. The approach we present here is based on the observation that certain words tend to have different probability distributions in different topics. We propose to compute the conditional language model probability as a dynamic mixture model of K topicspecific language models: 167 2 The Data The data used in this research is the Broadcast News (BN94) corpus, consisting of radio and TV news transcripts form the year 1994. From the total of 30226 documents, 20226 were used for training and the other 10000 were u"
P99-1022,H94-1014,0,0.0481707,"Missing"
P99-1022,H94-1013,0,0.0183234,"peace given major news topic distinctions (e.g. BUSINESS and INTERNATIONAL news) as illustrated in Figure 1. There is substantial subtopic probability variation for peace within INTERNATIONAL news (the word usage is 50-times more likely where w{ denotes the sequence wi... wj ; a common size for m is 3 (trigram language models). Even if n-grams were proved to be very powerful and robust in various tasks involving language models, they have a certain handicap: because of the Markov assumption, the dependency is limited to very short local context. Cache language models (Kuhn and de Mori (1992),Rosenfeld (1994)) try to overcome this limitation by boosting the probability of the words already seen in the history; trigger models (Lau et al. (1993)), even more general, try to capture the interrelationships between words. Models based on syntactic structure (Chelba and Jelinek (1998), Wright et al. (1993)) effectively estimate intra-sentence syntactic word dependencies. The approach we present here is based on the observation that certain words tend to have different probability distributions in different topics. We propose to compute the conditional language model probability as a dynamic mixture model"
P99-1022,C98-1035,0,\N,Missing
Q16-1009,S13-1004,0,0.0360441,"f the two words or their lemmas are identical, a value ppdbSim ∈ (0, 1) if the word pair is present in PPDB (the XXXL database)2 , and 0 117 4 Answer Sentence Ranking Features Instead of reinventing similarity features for our QA ranker, we derive our feature set from the winning system (Sultan et al., 2015) at the SemEval 2015 Semantic Textual Similarity (STS) task (Agirre et al., 2015). STS is an annually held SemEval competition, where systems output real-valued similarity scores for input sentence pairs. Hundreds of systems have been evaluated over the past few years (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015); our chosen system was shown to outperform all other systems from all years of SemEval STS (Sultan et al., 2015). In order to compute the degree of semantic similarity between a question Q and a candidate answer sentence S (i) , we draw features from two sources: (1) lexical alignment between Q and S (i) , and (2) vector representations of Q and S (i) , derived from their word embeddings. While the original STS system employs ridge regression, we use these features within a logistic regression model for QA ranking. 2 Alignment Features http://www.cis"
Q16-1009,S14-2010,0,0.0365511,"Missing"
Q16-1009,S15-2045,0,0.0243241,"Missing"
Q16-1009,P14-1023,0,0.0323876,") ) simA (Q, S ) = c nc (Q) + nc (S (i) ) (i) where nac (·) and nc (·) represent the number of aligned content words and the total number of content words in a sentence, respectively. S (i) can be arbitrarily long and still contain an answer to Q. In the above similarity measure, longer 118 answer sentences are penalized due to a larger number of unaligned words. To counter this phenomenon, we add a measure of coverage of Q by S (i) to the original feature set of Sultan et al. (2015): covA (Q, S (i) ) = 4.2 nac (Q) nc (Q) A Semantic Vector Feature Neural word embeddings (Mikolov et al., 2013; Baroni et al., 2014; Pennington, 2014) have been highly successful as distributional word representations in the recent past. We utilize the 400-dimensional word embeddings developed by Baroni et al. (2014)3 to construct sentence-level embeddings for Q and S (i) , which we then compare to compute a similarity score. To construct the vector representation VS of a given sentence S, we first extract the content word (1) (M ) lemmas CS = {CS , ..., CS } in S. The vectors representing these lemmas are then added to generate the sentence vector: VS = M X i=1 VC (i) S Finally, a similarity measure for Q and S (i) is de"
Q16-1009,N13-1092,0,0.11667,"Missing"
Q16-1009,N10-1145,0,0.0223249,"nking model must learn to rank test answer sentences from such binary annotations in the training data. Existing models accomplish this by learning to assign a relevance score to each (Q, S (i) ) pair; these scores then can be used to rank the sentences. QA rankers predominantly operate under the hypothesis that this relevance score is a function of the syntactic and/or semantic similarities between Q and S (i) . Wang et al. (2007), for example, learn the probability of generating Q from S (i) using syntactic transformations under a quasi-synchronous grammar formalism. The tree edit models of Heilman and Smith (2010) and Yao et al. (2013a) compute minimal tree edit sequences to align S (i) to Q, and use logistic regression to map features of edit sequences to a relevance score. Wang and Manning (2010) employ structured prediction to compute probabilities for tree edit sequences. Yao et al. (2013b) align related phrases in Q and each S (i) using a semi-Markov CRF model and rank candidates based on their decoding scores. Yih et al. (2013) use an array of lexical semantic similarity resources, from which they derive features for a binary classifier. Convolutional neural network models proposed by Yu et al. ("
Q16-1009,P05-1012,0,0.0180005,"tern for the corresponding question. TRAIN is a much smaller subset of TRAIN-ALL, containing pairs that are manually corrected for errors. Manual judgment is produced for DEV and TEST pairs, too. For answer extraction, Yao et al. (2013a) add to each QA pair the correct answer chunk(s). The gold TREC patterns are used to first identify relevant chunks in each answer sentence. TRAIN, DEV and TEST are then manually corrected for errors. The Wang et al. (2007) dataset also comes with POS/DEP/NER tags for each sentence. They use the MXPOST tagger (Ratnaparkhi, 1996) for POS tagging, the MSTParser (McDonald et al., 2005) to generate typed dependency trees, and the BBN Identifinder (Bikel et al., 1999) for NER tagging. Although we have access to a state-of-the-art information pipeline that produces better tags, this paper aims to study the effect of the proposed models and of our features on system performance, rather than on additional variables; therefore, to support comparison with prior work, we rely on the tags provided with the dataset for all our experiments. 6.2 Answer Sentence Ranking We adopt the standard evaluation procedure and metrics for QA rankers reported in the literature. 6.2.1 Evaluation Met"
Q16-1009,D14-1162,0,0.0782812,"Missing"
Q16-1009,W96-0213,0,0.593029,"positive example if it matches the gold answer pattern for the corresponding question. TRAIN is a much smaller subset of TRAIN-ALL, containing pairs that are manually corrected for errors. Manual judgment is produced for DEV and TEST pairs, too. For answer extraction, Yao et al. (2013a) add to each QA pair the correct answer chunk(s). The gold TREC patterns are used to first identify relevant chunks in each answer sentence. TRAIN, DEV and TEST are then manually corrected for errors. The Wang et al. (2007) dataset also comes with POS/DEP/NER tags for each sentence. They use the MXPOST tagger (Ratnaparkhi, 1996) for POS tagging, the MSTParser (McDonald et al., 2005) to generate typed dependency trees, and the BBN Identifinder (Bikel et al., 1999) for NER tagging. Although we have access to a state-of-the-art information pipeline that produces better tags, this paper aims to study the effect of the proposed models and of our features on system performance, rather than on additional variables; therefore, to support comparison with prior work, we rely on the tags provided with the dataset for all our experiments. 6.2 Answer Sentence Ranking We adopt the standard evaluation procedure and metrics for QA r"
Q16-1009,D13-1044,0,0.0944471,"ng algorithms operate under the assumption that the degree of syntactic and/or semantic similarity between questions and answer sentences is a sufficiently strong predictor of answer sentence relevance (Wang et al., 2007; Yih et al., 2013; Yu et al., 2014; Severyn and Moschitti, 2015). On the other hand, answer extraction algorithms frequently assess candidate answer phrases based primarily on their own properties relative to the question (e.g., whether the question is a who question and the phrase refers to a person), making inadequate or no use of sentence-level evidence (Yao et al., 2013a; Severyn and Moschitti, 2013). Both these assumptions, however, are simplistic, and fail to capture the core requirements of the two tasks. Table 1 shows a question, and three candidate answer sentences only one of which (S (1) ) actually answers the question. Ranking models that rely solely on text similarity are highly likely to incorrectly assign similar ranks to S (1) and S (2) . Such models would fail to utilize the key piece of evidence against S (2) that it does not contain any temporal information, necessary to answer a when question. Similarly, an extraction model that relies only on the features of a candidate p"
Q16-1009,Q14-1018,1,0.943938,"n the Wang et al. (2007) dataset. For standalone extraction, we use (i) for training the gold chunk annotations Cg associated with (Q, S (i) ) pairs: a candidate NP chunk in S (i) is considered a positive example for (Q, S (i) ) iff (i) it contains Cg and S (i) is an actual answer sentence. For both ranking and extraction, the corresponding weight vector θ is learned by minimizing the following L2 -regularized loss function: T  1 X (i) J(θ) = − y log(P (i) ) + T i=1  (i) (i) (1 − y ) log(1 − P ) + λkθk2 We align related words in Q and S (i) using a monolingual aligner originally proposed by Sultan et al. (2014). Here we give a brief description of our implementation, which employs arguably more principled methods to solve a set of subproblems. See the original article for further details. The aligner computes for each word pair across Q and S (i) a semantic similarity score simW ∈ [0, 1] using PPDB—a large database of lexical paraphrases developed using bilingual pivoting (Ganitkevitch et al., 2013). Specifically, it allows three different levels of similarity: 1 if the two words or their lemmas are identical, a value ppdbSim ∈ (0, 1) if the word pair is present in PPDB (the XXXL database)2 , and 0"
Q16-1009,S15-2027,1,0.903633,"article for further details. The aligner computes for each word pair across Q and S (i) a semantic similarity score simW ∈ [0, 1] using PPDB—a large database of lexical paraphrases developed using bilingual pivoting (Ganitkevitch et al., 2013). Specifically, it allows three different levels of similarity: 1 if the two words or their lemmas are identical, a value ppdbSim ∈ (0, 1) if the word pair is present in PPDB (the XXXL database)2 , and 0 117 4 Answer Sentence Ranking Features Instead of reinventing similarity features for our QA ranker, we derive our feature set from the winning system (Sultan et al., 2015) at the SemEval 2015 Semantic Textual Similarity (STS) task (Agirre et al., 2015). STS is an annually held SemEval competition, where systems output real-valued similarity scores for input sentence pairs. Hundreds of systems have been evaluated over the past few years (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015); our chosen system was shown to outperform all other systems from all years of SemEval STS (Sultan et al., 2015). In order to compute the degree of semantic similarity between a question Q and a candidate answer sentence S (i) , we draw features"
Q16-1009,D07-1003,0,0.0948727,"er hand, is the task of assigning a rank to each sentence so that the ones that are more likely to contain an answer are ranked higher. In this form, QA is similar to information retrieval and presents greater opportunities for further exploration and learning. In this article, we propose a novel approach to jointly solving these two well-studied yet open QA problems. Most answer sentence ranking algorithms operate under the assumption that the degree of syntactic and/or semantic similarity between questions and answer sentences is a sufficiently strong predictor of answer sentence relevance (Wang et al., 2007; Yih et al., 2013; Yu et al., 2014; Severyn and Moschitti, 2015). On the other hand, answer extraction algorithms frequently assess candidate answer phrases based primarily on their own properties relative to the question (e.g., whether the question is a who question and the phrase refers to a person), making inadequate or no use of sentence-level evidence (Yao et al., 2013a; Severyn and Moschitti, 2013). Both these assumptions, however, are simplistic, and fail to capture the core requirements of the two tasks. Table 1 shows a question, and three candidate answer sentences only one of which"
Q16-1009,C10-1131,0,0.0549232,"Missing"
Q16-1009,N13-1106,0,0.0295938,"Missing"
Q16-1009,D13-1056,0,0.0288665,"Missing"
Q16-1009,P13-2029,0,0.0586155,"Missing"
Q16-1009,P13-1171,0,0.184809,"k of assigning a rank to each sentence so that the ones that are more likely to contain an answer are ranked higher. In this form, QA is similar to information retrieval and presents greater opportunities for further exploration and learning. In this article, we propose a novel approach to jointly solving these two well-studied yet open QA problems. Most answer sentence ranking algorithms operate under the assumption that the degree of syntactic and/or semantic similarity between questions and answer sentences is a sufficiently strong predictor of answer sentence relevance (Wang et al., 2007; Yih et al., 2013; Yu et al., 2014; Severyn and Moschitti, 2015). On the other hand, answer extraction algorithms frequently assess candidate answer phrases based primarily on their own properties relative to the question (e.g., whether the question is a who question and the phrase refers to a person), making inadequate or no use of sentence-level evidence (Yao et al., 2013a; Severyn and Moschitti, 2013). Both these assumptions, however, are simplistic, and fail to capture the core requirements of the two tasks. Table 1 shows a question, and three candidate answer sentences only one of which (S (1) ) actually"
Q16-1009,S12-1051,0,\N,Missing
S01-1040,P00-1035,1,0.889368,"Missing"
S01-1040,P98-1080,0,0.0285784,"d context, local bigram and trigram collocations and several syntactic relationships based on predicate-argument structure (described in Section 1.2). Their use is illustrated on a sample English sentence for train in Figure 1. 1.1 Part-of-Speech Tagging and Lemmatization Part-of-speech tagger availability varied across the languages included in this sense-disambiguation system evaluation. Transformation-based taggers (Ngai and Florian, 2001) were trained on standard data for English (Penn Treebank), Swedish (SUC-1 corpus) and Estonian (MultextEast corpus). For Czech, an available POS tagger (Hajic and Hladka, 1998), which includes lemmatization, was used. The remaining languages - Spanish, Italian and Basque were tagged using an unsupervised tagger (Cucerzan 163 Syntactic {predicate-argument) features Object Prep ObjPrep children until age NNS IN NN child/N until/I age/N Ngram collocational features and Yarowsky, 2000). Lemmatization was performed using a combination of supervised and unsupervised methods (Yarowsky and Wicentowski, 2000), and using existing trie-based supervised models for English. 1.2 Syntactic Features Extracted syntactic relationships in the feature space depended on the keyword's pa"
S01-1040,P98-1081,0,0.0696734,"Missing"
S01-1040,P00-1027,1,0.878851,"Missing"
S01-1040,N01-1006,1,\N,Missing
S01-1040,C98-1077,0,\N,Missing
S01-1040,C98-1078,0,\N,Missing
W00-1304,J93-2004,0,0.0253443,"Table 2: Example of a sentence with chunk tags Experiments Three experiments that demonstrate the effectiveness and appropriateness of our probability estimates are presented in this section. The experiments are performed on text chunking, a subproblem of syntactic parsing. Unlike full parsing, the sentences are divided into non-overlapping phrases, where each word belongs to the lowest parse constituent that dominates it. The data used in all of these experiments is the CoNLL-2000 phrase chunking corpus (CoNLL, 2000). The corpus consists of sections 15-18 and section 20 of the Penn Treebank (Marcus et al., 1993), and is pre-divided into a 8936-sentence (211727 tokens) training set and a 2012-sentence (47377 tokens) test set. The chunk tags are derived from the parse tree constituents, and the part-of-speech tags were generated by the Brill tagger (Brill, 1995). As was noted by Ramshaw & Marcus (1999), text chunking can be mapped to a tagging task, where each word is tagged with a chunk tag representing the phrase that it belongs to. An example sentence from the corpus is shown in Table 4. As a contrasting system, our results are compared with those produced by a C4.5 decision tree system (henceforth"
W00-1304,P00-1016,1,0.829382,"Missing"
W00-1304,J95-2004,0,0.166162,"of the probabilistic classifier on unseen data and the degree to which the output probabilities from the classifier can be used to estimate confidences in its classification decisions. The results of these experiments show that, for the task of text chunking 1, the estimates produced by this technique are more informative than those generated by a state-of-the-art decision tree. Transformation-based learning (TBL) (Brill, 1995) is a successful rule-based machine learning algorithm in natural language processing. It has been applied to a wide variety of tasks, including part of speech tagging (Roche and Schabes, 1995; Brill,1995), noun phrase chvnklng (Ramshaw and Marcus, 1999), parsing (Brill, 1996; Vilain and Day, 1996), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brilland Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable listof rules. 1 Introduction In natural language processing, a great amount of work has gone into the development of machine learning algorithms which extract useful linguistic information from resources su"
W00-1304,P98-2188,0,0.0939659,"Missing"
W00-1304,C96-1047,0,0.0222069,"sifier can be used to estimate confidences in its classification decisions. The results of these experiments show that, for the task of text chunking 1, the estimates produced by this technique are more informative than those generated by a state-of-the-art decision tree. Transformation-based learning (TBL) (Brill, 1995) is a successful rule-based machine learning algorithm in natural language processing. It has been applied to a wide variety of tasks, including part of speech tagging (Roche and Schabes, 1995; Brill,1995), noun phrase chvnklng (Ramshaw and Marcus, 1999), parsing (Brill, 1996; Vilain and Day, 1996), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brilland Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable listof rules. 1 Introduction In natural language processing, a great amount of work has gone into the development of machine learning algorithms which extract useful linguistic information from resources such as dictionaries, newswire feeds, manually annotated corpora and web pages. Most of the effective methods"
W00-1304,C94-2195,0,0.173096,"Missing"
W00-1304,J95-4004,0,0.557337,"Missing"
W00-1304,A97-1051,0,0.0621309,"rmative than those generated by a state-of-the-art decision tree. Transformation-based learning (TBL) (Brill, 1995) is a successful rule-based machine learning algorithm in natural language processing. It has been applied to a wide variety of tasks, including part of speech tagging (Roche and Schabes, 1995; Brill,1995), noun phrase chvnklng (Ramshaw and Marcus, 1999), parsing (Brill, 1996; Vilain and Day, 1996), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brilland Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable listof rules. 1 Introduction In natural language processing, a great amount of work has gone into the development of machine learning algorithms which extract useful linguistic information from resources such as dictionaries, newswire feeds, manually annotated corpora and web pages. Most of the effective methods can be roughly divided into rule-based and probabilistic algorithms. In general, the rule-based methods have the advantage of capturing the necessary information in a small and concise set of rules. In"
W00-1304,P96-1042,0,0.0746229,"Missing"
W00-1304,C98-2183,0,\N,Missing
W01-0701,W99-0621,0,\N,Missing
W01-0701,J96-3004,0,\N,Missing
W01-0701,W94-0111,0,\N,Missing
W01-0701,J93-2004,0,\N,Missing
W01-0701,A00-2007,0,\N,Missing
W01-0701,W96-0213,0,\N,Missing
W01-0701,W00-1304,1,\N,Missing
W01-0701,A94-1030,0,\N,Missing
W01-0701,N01-1006,1,\N,Missing
W01-0701,W00-0726,0,\N,Missing
W01-0701,J95-4004,0,\N,Missing
W01-0701,A00-1031,0,\N,Missing
W01-0701,P00-1015,0,\N,Missing
W01-0701,P97-1041,0,\N,Missing
W02-0810,S01-1040,1,\N,Missing
W02-0810,S01-1027,0,\N,Missing
W02-0810,P00-1027,1,\N,Missing
W02-0810,P95-1026,0,\N,Missing
W02-0810,J01-3001,0,\N,Missing
W02-0810,J98-1004,0,\N,Missing
W02-0810,P98-1081,0,\N,Missing
W02-0810,C98-1078,0,\N,Missing
W02-0810,P98-1029,0,\N,Missing
W02-0810,C98-1029,0,\N,Missing
W02-0810,S01-1029,0,\N,Missing
W02-0810,roventini-etal-2000-italwordnet,0,\N,Missing
W02-1004,P98-1029,0,0.407309,"Missing"
W02-1004,J95-4004,0,0.0345552,"Missing"
W02-1004,P00-1035,1,0.921822,"Missing"
W02-1004,W02-1005,1,0.880993,"Missing"
W02-1004,S01-1001,0,0.0215764,"Missing"
W02-1004,W99-0623,0,0.111867,"Missing"
W02-1004,N01-1006,1,0.325593,"Missing"
W02-1004,A00-2009,0,0.16041,"Missing"
W02-1004,C00-2124,0,0.0656361,"Missing"
W02-1004,J01-3001,0,0.0297114,"Missing"
W02-1004,P98-1081,0,0.107306,"Missing"
W02-1004,J01-2002,0,0.0972506,"Missing"
W02-1004,P00-1027,1,0.886884,"Missing"
W02-1004,C98-1078,0,\N,Missing
W02-1004,C98-1029,0,\N,Missing
W02-2010,M95-1012,0,0.0606995,"ature identity (e.g.     ), membership in a set (e.g. B  ORG     ¿       ), etc. TBL has some attractive qualities that make it suitable for the language-related tasks: it can automatically integrate heterogenous types of knowledge, without the need for explicit modeling (similar to Snow, Maximum Entropy, decision trees, etc); it is error–driven, therefore directly minimizes the  ¬ 66.72 69.04 65.94 68.06 71.36 71.88 73.49 ultimate evaluation measure: the error rate; and it has an inherently dynamic behavior1 . TBL has been previously applied to the English NER task (Aberdeen et al., 1995), with good results. The fnTBL-based NER system is designed in the same way as Brill’s POS tagger (Brill, 1995), consisting of a morphological stage, where unknown words’ chunks are guessed based on their morphological and capitalization representation, followed by a contextual stage, in which the full interaction between the words’ features is leveraged for learning. The feature templates used are based on a combination of word, chunk and capitalization information of words in a 7-word window around the target word. The entire template list (133 templates) will be made available from the auth"
W02-2010,A97-1029,0,0.0349565,"ch as lists of countries/cities/regions, organizations, people names, etc.), due to the short exposition space, we decided to restrict them to this feature space. Table 2 presents the results obtained by running off-the-shelf part-of-speech/text chunking classifiers; all of them use just word information, albeit in different ways. The leader of the pack is the MXPOST tagger (Ratnaparkhi, 1996). The measure of choice for the NER task is F-measure, the harmonic  mean of precision and recall:     · , usually computed with   . As observed by participants in the MUC-6 and -7 tasks (Bikel et al., 1997; Borthwick, 1999; Miller et 1: Capitalization information first_cap, all_caps, all_lower, number, punct, other 2: Presence in dictionary upper, lower, both, none Table 1: Capitalization information al., 1998), an important feature for the NER task is information relative to word capitalization. In an approach similar to Zhou and Su (2002), we extracted for each word a 2-byte code, as summarized in Table 1. The first byte specifies the capitalization of the word (first letter capital, etc), while the second specifies whether the word is present in the dictionary in lower case, upper case, both"
W02-2010,J95-4004,0,0.0580798,"active qualities that make it suitable for the language-related tasks: it can automatically integrate heterogenous types of knowledge, without the need for explicit modeling (similar to Snow, Maximum Entropy, decision trees, etc); it is error–driven, therefore directly minimizes the  ¬ 66.72 69.04 65.94 68.06 71.36 71.88 73.49 ultimate evaluation measure: the error rate; and it has an inherently dynamic behavior1 . TBL has been previously applied to the English NER task (Aberdeen et al., 1995), with good results. The fnTBL-based NER system is designed in the same way as Brill’s POS tagger (Brill, 1995), consisting of a morphological stage, where unknown words’ chunks are guessed based on their morphological and capitalization representation, followed by a contextual stage, in which the full interaction between the words’ features is leveraged for learning. The feature templates used are based on a combination of word, chunk and capitalization information of words in a 7-word window around the target word. The entire template list (133 templates) will be made available from the author’s web page after the conclusion of the shared task. Snow Snow – Sparse Network of Winnows – is an architectu"
W02-2010,W01-0726,0,0.0137477,"evelopment sets Table 4: Forward-Backward results (F-measure) on the development sets finally converging after 10 iterations to a F-measure value of 73.49. For each marked entity  , the goal is to determine its most likely type:4 3 Breaking-Up the Task Muñoz et al. (1999) examine a different method of chunking, called Open/Close (O/C) method: 2 classifiers are used, one predicting open brackets and one predicting closed brackets. A final optimization stage pairs open and closed brackets through a global search. We propose here a method that is similar in spirit to the O/C method, and also to Carreras and Màrquez (2001), Arévalo et al. (2002): 1. In the first stage, detect only the entity boundaries, without identifying their type, using the fnTBL system3 ; 2. Using a forward-backward type algorithm (FB henceforth), determine the most probable type of each entity detected in the first step. This method has some enticing properties:  Detecting only the entity boundaries is a simpler problem, as different entity types share common features; Table 3 shows the performance obtained by the fnTBL system – the performance is sensibly higher than the one shown in Table 2;  The FB algorithm allows for a global searc"
W02-2010,M98-1009,0,0.143366,"Missing"
W02-2010,W99-0621,0,0.0159969,"Missing"
W02-2010,N01-1006,1,0.31529,"mportant feature for the NER task is information relative to word capitalization. In an approach similar to Zhou and Su (2002), we extracted for each word a 2-byte code, as summarized in Table 1. The first byte specifies the capitalization of the word (first letter capital, etc), while the second specifies whether the word is present in the dictionary in lower case, upper case, both or neither forms. These two codes are extracted in order to offer both a way of backing-off in sparse data cases (unknown words) and a way of encouraging generalization. Table 2 shows the performance of the fnTBL (Ngai and Florian, 2001) and Snow systems when using the capitalization information, both systems displaying considerably better performance. 2.2 Transformation-Based Learning Transformation-based learning (TBL henceforth) is an error-driven machine learning technique which works by first assigning an initial classification to the data, and then automatically proposing, evaluating and selecting the transformations that maximally decrease the number of errors. Each such transformation, or rule, consists of a predicate and a target. In our implementation of TBL – fnTBL – predicates consist of a conjunction of atomic pr"
W02-2010,W96-0213,0,0.0926083,"this paper are using only information that can be extracted directly from the training data: the words, their capitalization information and the chunk tags. While they can definitely incorporate additional information (such as lists of countries/cities/regions, organizations, people names, etc.), due to the short exposition space, we decided to restrict them to this feature space. Table 2 presents the results obtained by running off-the-shelf part-of-speech/text chunking classifiers; all of them use just word information, albeit in different ways. The leader of the pack is the MXPOST tagger (Ratnaparkhi, 1996). The measure of choice for the NER task is F-measure, the harmonic  mean of precision and recall:     · , usually computed with   . As observed by participants in the MUC-6 and -7 tasks (Bikel et al., 1997; Borthwick, 1999; Miller et 1: Capitalization information first_cap, all_caps, all_lower, number, punct, other 2: Presence in dictionary upper, lower, both, none Table 1: Capitalization information al., 1998), an important feature for the NER task is information relative to word capitalization. In an approach similar to Zhou and Su (2002), we extracted for each word a 2-byte"
W02-2010,E99-1023,0,0.0191797,"rill, 1995), Snow (sparse network of winnows (Muñoz et al., 1999)) and a forward-backward algorithm are stacked (the output of one classifier is passed as input to the next classifier), yielding considerable improvement in performance. In addition, in agreement with other studies on the same problem, the enhancement of the feature space (in the form of capitalization information) is shown to be especially beneficial to this task. 2 Computational Approaches All approaches to the NER task presented in this paper, except the one presented in Section 3, use the IOB chunk tagging method (Tjong Kim Sang and Veenstra, 1999) for identifying the named entities. 2.1 Feature Space and Baselines A careful selection of the feature space is a very important part of classifier design. The algorithms presented in this paper are using only information that can be extracted directly from the training data: the words, their capitalization information and the chunk tags. While they can definitely incorporate additional information (such as lists of countries/cities/regions, organizations, people names, etc.), due to the short exposition space, we decided to restrict them to this feature space. Table 2 presents the results ob"
W02-2010,P02-1060,0,0.0436439,"er of the pack is the MXPOST tagger (Ratnaparkhi, 1996). The measure of choice for the NER task is F-measure, the harmonic  mean of precision and recall:     · , usually computed with   . As observed by participants in the MUC-6 and -7 tasks (Bikel et al., 1997; Borthwick, 1999; Miller et 1: Capitalization information first_cap, all_caps, all_lower, number, punct, other 2: Presence in dictionary upper, lower, both, none Table 1: Capitalization information al., 1998), an important feature for the NER task is information relative to word capitalization. In an approach similar to Zhou and Su (2002), we extracted for each word a 2-byte code, as summarized in Table 1. The first byte specifies the capitalization of the word (first letter capital, etc), while the second specifies whether the word is present in the dictionary in lower case, upper case, both or neither forms. These two codes are extracted in order to offer both a way of backing-off in sparse data cases (unknown words) and a way of encouraging generalization. Table 2 shows the performance of the fnTBL (Ngai and Florian, 2001) and Snow systems when using the capitalization information, both systems displaying considerably bette"
W03-0425,J96-1002,0,0.00841485,"ta. 1 Introduction This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt). This particular set of classifiers is diverse across multiple dimensions, making it suitable for combination: • fnTBL is a discriminant classifier – it bases its classification decision only on the few most discriminant features active on an example – while HMM, RRM and MaxEnt are agglomerative classifiers – their decision is based on the combination of all features active for the particular example. • In dealing with the data sparseness problem, fnTBL, MaxEnt and RRM investigate and integrate in their decision arbitrary feature types, while HMM is depen"
W03-0425,J95-4004,0,0.172091,"Missing"
W03-0425,W02-2010,1,0.509993,"mally decrease the number of errors. 2 Usually, document titles, but also table headers, etc. HMM TBL MaxEnt RRM English (a) (b) 82.0 74.6 88.1 81.2 90.8 85.6 92.1 85.5 German (a) (b) 69.5 68.6 68.0 67.3 70.7 71.3 Tab. 1: Individual classifier results on the two test sets. TBL has some attractive qualities that make it suitable for the language-related tasks: it can automatically integrate heterogeneous types of knowledge, without the need for explicit modeling, it is error–driven, and has an inherently dynamic behavior. The particular setup in which fnTBL is used in this work is described in Florian (2002a): in a first phase, TBL is used to identify the entity boundaries, followed by a sequence classification stage, where the entities identified at the first step are classified using internal and external clues3 . 3.4 The Hidden Markov Model Classifier The HMM classifier used in the experiments in Section 4 follows the system description in (Bikel et al., 1999), and it performs sequence classification by assigning each word either one of the named entity types or the label NOT-A-NAME to represent ""not a named entity"". The states in the HMM are organized into regions, one region for each type o"
W03-0425,C00-2124,0,0.0385379,"uation (2), the provided training data was split into 5 equal parts, and each classifier was trained, in a round-robin fashion, on 4 fifths of the data and applied on the remaining fifth. This way, the entire training data can be used to estimate the weight parameters λi (w) and Pi (C|w, Ci ) but, at decoding time, the individual classifier outputs Ci are computed by using the entire training data. Table 2 presents the combination results, for different ways of estimating the interpolation parameters. A simple combination method is the equal voting method (van Halteren et al., 2001; Tjong Kim Sang et al., 2000), where the parameters are computed as λi (w) = n1 and Pi (C|w, Ci ) = δ (C, Ci ), where δ is the Kronecker operator (δ (x, y) := (x = y?1 : 0)) – each of the classifiers votes with equal weight for the class that is most likely under its model, and the class receiving the largest number of votes wins. However, this procedure may lead to ties, where some classes receive the same number of votes – one usually resorts to randomly selecting one of the tied candidates in this case – Table 2 presents the average results obtained by this method, together with the variance obtained over 30 trials. To"
W03-0425,J01-2002,0,0.0436444,"Missing"
W03-0425,W03-0434,1,0.318643,"richer tagset data (32 named categories), used in the IBM question answering system (Ittycheriah et al., 2001) In addition, a ngram-based capitalization restoration algorithm has been applied on the sentences that appear in all caps2 , for the English task. 3 The Algorithms This section describes only briefly the classifiers used in combination in Section 4; a full description of the algorithms and their properties is beyond the scope of this paper – the reader is instead referred to the original articles. 3.1 The Robust Risk Minimization Classifier This classifier is described in detail in (Zhang and Johnson, 2003, this volume), along with a comprehensive evaluation of its performance, and therefore is not presented here. 3.2 The Maximum Entropy Classifier The MaxEnt classifier computes the posterior class probability of an example by evaluating the normalized product of the weights active for the particular example. The model weights are trained using the improved iterative scaling algorithm (Berger et al., 1996). To avoid running in severe over-training problems, a feature cutoff of 4 is applied before the model weights are learned. At decoding time, the best sequence of classifications is identified"
W03-1026,W97-0306,0,\N,Missing
W03-1026,E99-1023,0,\N,Missing
W03-1026,W02-0811,0,\N,Missing
W03-1026,C00-2167,0,\N,Missing
W03-1026,W03-0425,1,\N,Missing
W03-1026,C02-1012,0,\N,Missing
W03-1026,J95-4004,0,\N,Missing
W03-1026,W03-0419,0,\N,Missing
W03-1026,C00-2124,0,\N,Missing
W03-1026,J01-2002,0,\N,Missing
W03-1026,P98-1029,0,\N,Missing
W03-1026,C98-1029,0,\N,Missing
W03-1026,W02-2024,0,\N,Missing
W03-1026,W02-1036,0,\N,Missing
W05-0709,E99-1001,0,0.103507,"Missing"
W05-0709,N04-1001,0,0.466946,"from the 63 Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 63–70, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word. It is these orthographic variations and complex morphological structure that make Arabic language processing challenging (Xu et al., 2001; Xu et al., 2002). Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in (Luo et al., 2004). Both systems are built around from the maximum-entropy technique (Berger et al., 1996). We formulate the mention detection task as a sequence classification problem. While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language. The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes. We begin with a segmentation of the written text before starting the classification. This segmentation process consi"
W05-0709,M98-1009,0,0.0552198,"Missing"
W05-0709,P02-1014,0,0.0143573,"Missing"
W05-0709,J96-1002,0,0.0458627,"rivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word. It is these orthographic variations and complex morphological structure that make Arabic language processing challenging (Xu et al., 2001; Xu et al., 2002). Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in (Luo et al., 2004). Both systems are built around from the maximum-entropy technique (Berger et al., 1996). We formulate the mention detection task as a sequence classification problem. While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language. The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes. We begin with a segmentation of the written text before starting the classification. This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens). The resulting granularity"
W05-0709,A97-1029,0,0.0285189,"Missing"
W05-0709,J01-4004,0,0.0229922,"Missing"
W05-0709,J96-3004,0,0.0743884,"Missing"
W05-0709,P03-1051,0,\N,Missing
W05-0709,P04-1018,1,\N,Missing
W15-3806,S14-2007,0,0.0314435,"Missing"
W15-3806,D12-1042,1,0.843557,"Missing"
W15-3806,D13-1136,0,0.0219564,"Missing"
