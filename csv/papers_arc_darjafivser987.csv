2021.wassa-1.16,Exploring Stylometric and Emotion-Based Features for Multilingual Cross-Domain Hate Speech Detection,2021,-1,-1,3,0.714286,442,ilia markov,"Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"In this paper, we describe experiments designed to evaluate the impact of stylometric and emotion-based features on hate speech detection: the task of classifying textual content into hate or non-hate speech classes. Our experiments are conducted for three languages {--} English, Slovene, and Dutch {--} both in in-domain and cross-domain setups, and aim to investigate hate speech using features that model two linguistic phenomena: the writing style of hateful social media content operationalized as function word usage on the one hand, and emotion expression in hateful messages on the other hand. The results of experiments with features that model different combinations of these phenomena support our hypothesis that stylometric and emotion-based features are robust indicators of hate speech. Their contribution remains persistent with respect to domain and language variation. We show that the combination of features that model the targeted phenomena outperforms words and character n-gram features under cross-domain conditions, and provides a significant boost to deep learning models, which currently obtain the best results, when combined with them in an ensemble."
2020.peoples-1.15,"The {L}i{L}a{H} Emotion Lexicon of {C}roatian, {D}utch and {S}lovene",2020,-1,-1,3,0,264,nikola ljubevsic,"Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media",0,"In this paper, we present emotion lexicons of Croatian, Dutch and Slovene, based on manually corrected automatic translations of the English NRC Emotion lexicon. We evaluate the impact of the translation changes by measuring the change in supervised classification results of socially unacceptable utterances when lexicon information is used for feature construction. We further showcase the usage of the lexicons by calculating the difference in emotion distributions in texts containing and not containing socially unacceptable discourse, comparing them across four languages (English, Croatian, Dutch, Slovene) and two topics (migrants and LGBT). We show significant and consistent improvements in automatic classification across all languages and topics, as well as consistent (and expected) emotion distributions across all languages and topics, proving for the manually corrected lexicons to be a useful addition to the severely lacking area of emotion lexicons, the crucial resource for emotive analysis of text."
2020.lrec-1.417,Interoperability in an Infrastructure Enabling Multidisciplinary Research: The case of {CLARIN},2020,-1,-1,3,0.919974,17553,franciska jong,Proceedings of the 12th Language Resources and Evaluation Conference,0,"CLARIN is a European Research Infrastructure providing access to language resources and technologies for researchers in the humanities and social sciences. It supports the use and study of language data in general and aims to increase the potential for comparative research of cultural and societal phenomena across the boundaries of languages and disciplines, all in line with the European agenda for Open Science. Data infrastructures such as CLARIN have recently embarked on the emerging frameworks for the federation of infrastructural services, such as the European Open Science Cloud and the integration of services resulting from multidisciplinary collaboration in federated services for the wider SSH domain. In this paper we describe the interoperability requirements that arise through the existing ambitions and the emerging frameworks. The interoperability theme will be addressed at several levels, including organisation and ecosystem, design of workflow services, data curation, performance measurement and collaboration."
2020.iwltp-1.5,{CLARIN}: Distributed Language Resources and Technology in a {E}uropean Infrastructure,2020,-1,-1,4,0,18394,maria eskevich,Proceedings of the 1st International Workshop on Language Technology Platforms,0,CLARIN is a European Research Infrastructure providing access to digital language resources and tools from across Europe and beyond to researchers in the humanities and social sciences. This paper focuses on CLARIN as a platform for the sharing of language resources. It zooms in on the service offer for the aggregation of language repositories and the value proposition for a number of communities that benefit from the enhanced visibility of their data and services as a result of integration in CLARIN. The enhanced findability of language resources is serving the social sciences and humanities (SSH) community at large and supports research communities that aim to collaborate based on virtual collections for a specific domain. The paper also addresses the wider landscape of service platforms based on language technologies which has the potential of becoming a powerful set of interoperable facilities to a variety of communities of use.
W18-5116,Datasets of {S}lovene and {C}roatian Moderated News Comments,2018,-1,-1,3,0.346655,264,nikola ljubevsic,Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2),0,"This paper presents two large newly constructed datasets of moderated news comments from two highly popular online news portals in the respective countries: the Slovene RTV MCC and the Croatian 24sata. The datasets are analyzed by performing manual annotation of the types of the content which have been deleted by moderators and by investigating deletion trends among users and threads. Next, initial experiments on automatically detecting the deleted content in the datasets are presented. Both datasets are published in encrypted form, to enable others to perform experiments on detecting content to be deleted without revealing potentially inappropriate content. Finally, the baseline classification models trained on the non-encrypted datasets are disseminated as well to enable real-world use."
W18-3028,Predicting Concreteness and Imageability of Words Within and Across Languages via Word Embeddings,2018,0,1,2,0.346655,264,nikola ljubevsic,Proceedings of The Third Workshop on Representation Learning for {NLP},0,"The notions of concreteness and imageability, traditionally important in psycholinguistics, are gaining significance in semantic-oriented natural language processing tasks. In this paper we investigate the predictability of these two concepts via supervised learning, using word embeddings as explanatory variables. We perform predictions both within and across languages by exploiting collections of cross-lingual embeddings aligned to a single vector space. We show that the notions of concreteness and imageability are highly predictable both within and across languages, with a moderate loss of up to 20{\%} in correlation when predicting across languages. We further show that the cross-lingual transfer via word embeddings is more efficient than the simple transfer via bilingual dictionaries."
L18-1210,{CLARIN}{'}s Key Resource Families,2018,0,0,1,1,443,darja fivser,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1515,{CLARIN}: Towards {FAIR} and Responsible Data Science Using Language Resources,2018,0,2,4,0.919974,17553,franciska jong,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"CLARIN is a European Research Infrastructure providing access to language resources and technologies for researchers in the humanities and social sciences. It supports the study of language data in general and aims to increase the potential for comparative research of cultural and societal phenomena across the boundaries of languages. This paper outlines the CLARIN vision and strategy, and it explains how the design and implementation of CLARIN are compliant with the FAIR principles: findability, accessibility, interoperability and reusability of data. The paper also explains the approach of CLARIN towards the enabling of responsible data science. Attention is paid to (i) the development of measures for increasing the transparency and explainability of the results from applying CLARIN technologies, in particular in the context of multidisciplinary research, and (ii) stimulating the uptake of its resources, tools and services by the various communities of use, all in accordance with the principles for Open Science."
W17-3007,"Legal Framework, Dataset and Annotation Schema for Socially Unacceptable Online Discourse Practices in {S}lovene",2017,4,12,1,1,443,darja fivser,Proceedings of the First Workshop on Abusive Language Online,0,"In this paper we present the legal framework, dataset and annotation schema of socially unacceptable discourse practices on social networking platforms in Slovenia. On this basis we aim to train an automatic identification and classification system with which we wish contribute towards an improved methodology, understanding and treatment of such practices in the contemporary, increasingly multicultural information society."
W17-2901,Language-independent Gender Prediction on {T}witter,2017,6,2,2,0.388079,264,nikola ljubevsic,Proceedings of the Second Workshop on {NLP} and Computational Social Science,0,"In this paper we present a set of experiments and analyses on predicting the gender of Twitter users based on language-independent features extracted either from the text or the metadata of users{'} tweets. We perform our experiments on the TwiSty dataset containing manual gender annotations for users speaking six different languages. Our classification results show that, while the prediction model based on language-independent features performs worse than the bag-of-words model when training and testing on the same language, it regularly outperforms the bag-of-words model when applied to different languages, showing very stable results across various languages. Finally we perform a comparative analysis of feature effect sizes across the six languages and show that differences in our features correspond to cultural distances."
W17-1410,Adapting a State-of-the-Art Tagger for {S}outh {S}lavic Languages to Non-Standard Text,2017,9,3,3,0.388079,264,nikola ljubevsic,Proceedings of the 6th Workshop on {B}alto-{S}lavic Natural Language Processing,0,"In this paper we present the adaptations of a state-of-the-art tagger for South Slavic languages to non-standard texts on the example of the Slovene language. We investigate the impact of introducing in-domain training data as well as additional supervision through external resources or tools like word clusters and word normalization. We remove more than half of the error of the standard tagger when applied to non-standard texts by training it on a combination of standard and non-standard training data, while enriching the data representation with external resources removes additional 11 percent of the error. The final configuration achieves tagging accuracy of 87.41{\%} on the full morphosyntactic description, which is, nevertheless, still quite far from the accuracy of 94.27{\%} achieved on standard text."
W16-3904,Private or Corporate? Predicting User Types on {T}witter,2016,12,1,2,0.417356,264,nikola ljubevsic,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"In this paper we present a series of experiments on discriminating between private and corporate accounts on Twitter. We define features based on Twitter metadata, morphosyntactic tags and surface forms, showing that the simple bag-of-words model achieves single best results that can, however, be improved by building a weighted soft ensemble of classifiers based on each feature type. Investigating the time and language dependence of each feature type delivers quite unexpecting results showing that features based on metadata are neither time- nor language-insensitive as the way the two user groups use the social network varies heavily through time and space."
W16-2610,A Global Analysis of Emoji Usage,2016,3,30,2,0.417356,264,nikola ljubevsic,Proceedings of the 10th Web as Corpus Workshop,0,"Emojis are a quickly spreading and rather unknown communication phenomenon which occasionally receives attention in the mainstream press, but lacks the scientific exploration it deserves. This paper is a first attempt at investigating the global distribution of emojis. We perform our analysis of the spatial distribution of emojis on a dataset of xe2x88xbc17 million (and growing) geo-encoded tweets containing emojis by running a cluster analysis over countries represented as emoji distributions and performing correlation analysis of emoji distributions and World Development Indicators. We show that emoji usage tends to draw quite a realistic picture of the living conditions in various parts of our world."
L16-1573,Corpus-Based Diacritic Restoration for {S}outh {S}lavic Languages,2016,6,5,3,0.417356,264,nikola ljubevsic,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In computer-mediated communication, Latin-based scripts users often omit diacritics when writing. Such text is typically easily understandable to humans but very difficult for computational processing because many words become ambiguous or unknown. Letter-level approaches to diacritic restoration generalise better and do not require a lot of training data but word-level approaches tend to yield better results. However, they typically rely on a lexicon which is an expensive resource, not covering non-standard forms, and often not available for less-resourced languages. In this paper we present diacritic restoration models that are trained on easy-to-acquire corpora. We test three different types of corpora (Wikipedia, general web, Twitter) for three South Slavic languages (Croatian, Serbian and Slovene) and evaluate them on two types of text: standard (Wikipedia) and non-standard (Twitter). The proposed approach considerably outperforms charlifter, so far the only open source tool available for this task. We make the best performing systems freely available."
R15-1049,Predicting the Level of Text Standardness in User-generated Content,2015,11,6,2,0.525461,264,nikola ljubevsic,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Non-standard language as it appears in user-generated content has recently attracted much attention. This paper proposes that non-standardness comes in two basic varieties, technical and linguistic, and develops a machine-learning method to discriminate between standard and nonstandard texts in these two dimensions. We describe the manual annotation of a dataset of Slovene user-generated content and the features used to build our regression models. We evaluate and discuss the results, where the mean absolute error of the best performing method on a three-point scale is 0.38 for technical and 0.42 for linguistic standardness prediction. Even when using no language-dependent information sources, our predictor still outperforms an OOVratio baseline by a wide margin. In addition, we show that very little manually annotated training data is required to perform good prediction. Predicting standardness can help decide when to attempt to normalise the data to achieve better annotation results with standard tools, and provide linguists who are interested in nonstandard language with a simple way of selecting only such texts for their research."
fiser-etal-2014-slowcrowd,slo{WC}rowd: A crowdsourcing tool for lexicographic tasks,2014,3,1,1,1,443,darja fivser,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The paper presents sloWCrowd, a simple tool developed to facilitate crowdsourcing lexicographic tasks, such as error correction in automatically generated wordnets and semantic annotation of corpora. The tool is open-source, language-independent and can be adapted to a broad range of crowdsourcing tasks. Since volunteers who participate in our crowdsourcing tasks are not trained lexicographers, the tool has been designed to obtain multiple answers to the same question and compute the majority vote, making sure individual unreliable answers are discarded. We also make sure unreliable volunteers, who systematically provide unreliable answers, are not taken into account. This is achieved by measuring their accuracy against a gold standard, the questions from which are posed to the annotators on a regular basis in between the real question. We tested the tool in an extensive crowdsourcing task, i.e. error correction of the Slovene wordnet, the results of which are encouraging, motivating us to use the tool in other annotation tasks in the future as well."
ljubesic-etal-2014-tweetcat,{T}weet{C}a{T}: a tool for building {T}witter corpora of smaller languages,2014,8,12,2,0.595238,264,nikola ljubevsic,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents TweetCaT, an open-source Python tool for building Twitter corpora that was designed for smaller languages. Using the Twitter search API and a set of seed terms, the tool identifies users tweeting in the language of interest together with their friends and followers. By running the tool for 235 days we tested it on the task of collecting two monitor corpora, one for Croatian and Serbian and the other for Slovene, thus also creating new and valuable resources for these languages. A post-processing step on the collected corpus is also described, which filters out users that tweet predominantly in a foreign language thus further cleans the collected corpora. Finally, an experiment on discriminating between Croatian and Serbian Twitter users is reported."
W13-2501,Cross-lingual {WSD} for Translation Extraction from Comparable Corpora,2013,22,1,3,0,2673,marianna apidianaki,Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,0,"We propose a data-driven approach to enhance translation extraction from comparable corpora. Instead of resorting to an external dictionary, we translate source vector features by using a cross-lingual Word Sense Disambiguation method. The candidate senses for a feature correspond to sense clusters of its translations in a parallel corpus and the context used for disambiguation consists of the vector that contains the feature. The translations found in the disambiguation output convey the sense of the features in the source vector, while the use of translation clusters permits to expand their translation with several variants. As a consequence, the translated vectors are less noisy and richer, and allow for the extraction of higher quality lexicons compared to simpler methods."
W13-2411,Identifying false friends between closely related languages,2013,10,1,2,0.595238,264,nikola ljubevsic,Proceedings of the 4th Biennial International Workshop on {B}alto-{S}lavic Natural Language Processing,0,"In this paper we present a corpus-based approach to automatic identification of false friends for Slovene and Croatian, a pair of closely related languages. By taking advantage of the lexical overlap between the two languages, we focus on measuring the difference in meaning between identically spelled words by using frequency and distributional information. We analyze the impact of corpora of different origin and size together with different association and similarity measures and compare them to a simple frequency-based baseline. With the best performing setting we obtain very good average precision of 0.973 and 0.883 on different gold standards. The presented approach works on non-parallel datasets, is knowledge-lean and language-independent, which makes it attractive for natural language processing tasks that often lack the lexical resources and cannot afford to build them by hand."
W12-1001,Lexicon Construction and Corpus Annotation of Historical Language with the {C}o{B}a{LT} Editor,2012,6,6,4,0,23107,tom kenter,"Proceedings of the 6th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"This paper describes a Web-based editor called CoBaLT (Corpus-Based Lexicon Tool), developed to construct corpus-based computational lexica and to correct word-level annotations and transcription errors in corpora. The paper describes the tool as well as our experience in using it to annotate a reference corpus and compile a large lexicon of historical Slovene. The annotations used in our project are modern-day word form equivalent, lemma, part-of-speech tag and optional gloss. The CoBaLT interface is word form oriented and compact. It enables wildcard word searching and sorting according to several criteria, which makes the editing process flexible and efficient. The tool accepts preannotated corpora in TEI P5 format and is able to export the corpus and lexicon in TEI P5 as well. The tool is implemented using the LAMP architecture and is freely available for research purposes."
W12-0112,Were the clocks striking or surprising? Using {WSD} to improve {MT} performance,2012,14,3,2,1,21021,vspela vintar,Proceedings of the Joint Workshop on Exploiting Synergies between Information Retrieval and Machine Translation ({ESIRMT}) and Hybrid Approaches to Machine Translation ({H}y{T}ra),0,"We report on a series of experiments aimed at improving the machine translation of ambiguous lexical items by using wordnet-based unsupervised Word Sense Disambiguation (WSD) and comparing its results to three MT systems. Our experiments are performed for the English-Slovene language pair using UKB, a freely available graph-based word sense disambiguation system. Results are evaluated in three ways: a manual evaluation of WSD performance from MT perspective, an analysis of agreement between the WSD-proposed equivalent and those suggested by the three systems, and finally by computing BLEU, NIST and METEOR scores for all translation versions. Our results show that WSD performs with a MT-relevant precision of 71% and that 21% of sense-related MT errors could be prevented by using unsupervised WSD."
fiser-etal-2012-addressing,Addressing polysemy in bilingual lexicon extraction from comparable corpora,2012,11,6,1,1,443,darja fivser,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents an approach to extract translation equivalents from comparable corpora for polysemous nouns. As opposed to the standard approaches that build a single context vector for all occurrences of a given headword, we first disambiguate the headword with third-party sense taggers and then build a separate context vector for each sense of the headword. Since state-of-the-art word sense disambiguation tools are still far from perfect, we also tried to improve the results by combining the sense assignments provided by two different sense taggers. Evaluation of the results shows that we outperform the baseline (0.473) in all the settings we experimented with, even when using only one sense tagger, and that the best-performing results are indeed obtained by taking into account the intersection of both sense taggers (0.720)."
sagot-fiser-2012-cleaning,Cleaning noisy wordnets,2012,9,3,2,0.0749817,250,benoit sagot,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Automatic approaches to creating and extending wordnets, which have become very popular in the past decade, inadvertently result in noisy synsets. This is why we propose an approach to detect synset outliers in order to eliminate the noise and improve accuracy of the developed wordnets, so that they become more useful lexico-semantic resources for natural language applications. The approach compares the words that appear in the synset and its surroundings with the contexts of the literals in question they are used in based on large monolingual corpora. By fine-tuning the outlier threshold we can influence how many outlier candidates will be eliminated. Although the proposed approach is language-independent we test it on Slovene and French that were created automatically from bilingual resources and contain plenty of disambiguation errors. Manual evaluation of the results shows that by applying a threshold similar to the estimated error rate in the respective wordnets, 67{\%} of the proposed outlier candidates are indeed incorrect for French and a 64{\%} for Slovene. This is a big improvement compared to the estimated overall error rates in the resources, which are 12{\%} for French and 15{\%} for Slovene."
W11-1204,Building and Using Comparable Corpora for Domain-Specific Bilingual Lexicon Extraction,2011,17,14,1,1,443,darja fivser,Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,0,"This paper presents a series of experiments aimed at inducing and evaluating domain-specific bilingual lexica from comparable corpora. First, a small English-Slovene comparable corpus from health magazines was manually constructed and then used to compile a large comparable corpus on health-related topics from web corpora. Next, a bilingual lexicon for the domain was extracted from the corpus by comparing context vectors in the two languages. Evaluation of the results shows that a 2-way translation of context vectors significantly improves precision of the extracted translation equivalents. We also show that it is sufficient to increase the corpus for one language in order to obtain a higher recall, and that the increase of the number of new words is linear in the size of the corpus. Finally, we demonstrate that by lowering the frequency threshold for context vectors, the drop in precision is much slower than the increase of recall."
R11-1018,Bilingual lexicon extraction from comparable corpora for closely related languages,2011,12,20,1,1,443,darja fivser,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"In this paper we present a knowledge-light approach to extract a bilingual lexicon for closely related languages from comparable corpora. While in most related work an existing dictionary is used to translate context vectors, we take advantage of the similarities between languages instead and build a seed lexicon from words that are identical in both languages and then further extend it with context-based cognates and translations of the most frequent words. We also use cognates for reranking translation candidates obtained via context similarity and extract translation equivalents for all content words, not just nouns as in most related work. The results are very encouraging, suggesting that other similar languages could benefit from the same approach. By enlarging the seed lexicon with cognates and translations of the most frequent words and by cognate-based reranking of translation candidates we were able to improve the average baseline precision from 0.592 to 0.797 on the mean reciprocal rank for the ten top-ranking translation candidates for nouns, verbs and adjectives with a 46% recall on the gold standard of 1000 random entries from a traditional dictionary."
erjavec-etal-2010-jos,The {JOS} Linguistically Tagged Corpus of {S}lovene,2010,6,34,2,0.472026,15753,tomavz erjavec,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The JOS language resources are meant to facilitate developments of HLT and corpus linguistics for the Slovene language and consist of the morphosyntactic specifications, defining the Slovene morphosyntactic features and tagset; two annotated corpora (jos100k and jos1M); and two web services (a concordancer and text annotation tool). The paper introduces these components, and concentrates on jos100k, a 100,000 word sampled balanced monolingual Slovene corpus, manually annotated for three levels of linguistic description. On the morphosyntactic level, each word is annotated with its morphosyntactic description and lemma; on the syntactic level the sentences are annotated with dependency links; on the semantic level, all the occurrences of 100 top nouns in the corpus are annotated with their wordnet synset from the Slovene semantic lexicon sloWNet. The JOS corpora and specifications have a standardised encoding (Text Encoding Initiative Guidelines TEI P5) and are available for research from http://nl.ijs.si/jos/ under the Creative Commons licence."
fiser-etal-2010-learning,Learning to Mine Definitions from {S}lovene Structured and Unstructured Knowledge-Rich Resources,2010,20,6,1,1,443,darja fivser,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The paper presents an innovative approach to extract Slovene definition candidates from domain-specific corpora using morphosyntactic patterns, automatic terminology recognition and semantic tagging with wordnet senses. First, a classification model was trained on examples from Slovene Wikipedia which was then used to find well-formed definitions among the extracted candidates. The results of the experiment are encouraging, with accuracy ranging from 67{\%} to 71{\%}. The paper also addresses some drawbacks of the approach and suggests ways to overcome them in future work."
vintar-fiser-2008-harvesting,Harvesting Multi-Word Expressions from Parallel Corpora,2008,13,14,2,1,21021,vspela vintar,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The paper presents a set of approaches to extend the automatically created Slovene wordnet with nominal multi-word expressions. In the first approach multi-word expressions from Princeton WordNet are translated with a technique that is based on word-alignment and lexico-syntactic patterns. This is followed by extracting new terms from a monolingual corpus using keywordness ranking and contextual patterns. Finally, the multi-word expressions are assigned a hypernym and added to our wordnet. Manual evaluation and comparison of the results shows that the translation approach is the most straightforward and accurate. However, it is successfully complemented by the two monolingual approaches which are able to identify more term candidates in the corpus that would otherwise go unnoticed. Some weaknesses of the proposed wordnet extension techniques are also addressed."
2008.jeptalnrecital-long.18,Construction d{'}un wordnet libre du fran{\\c{c}}ais {\\`a} partir de ressources multilingues,2008,-1,-1,2,0.0749817,250,benoit sagot,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article d{\'e}crit la construction d{'}un Wordnet Libre du Fran{\c{c}}ais (WOLF) {\`a} partir du Princeton WordNet et de diverses ressources multilingues. Les lex{\`e}mes polys{\'e}miques ont {\'e}t{\'e} trait{\'e}s au moyen d{'}une approche reposant sur l{'}alignement en mots d{'}un corpus parall{\`e}le en cinq langues. Le lexique multilingue extrait a {\'e}t{\'e} d{\'e}sambigu{\""\i}s{\'e} s{\'e}mantiquement {\`a} l{'}aide des wordnets des langues concern{\'e}es. Par ailleurs, une approche bilingue a {\'e}t{\'e} suffisante pour construire de nouvelles entr{\'e}es {\`a} partir des lex{\`e}mes monos{\'e}miques. Nous avons pour cela extrait des lexiques bilingues {\`a} partir deWikip{\'e}dia et de th{\'e}saurus. Le wordnet obtenu a {\'e}t{\'e} {\'e}valu{\'e} par rapport au wordnet fran{\c{c}}ais issu du projet EuroWordNet. Les r{\'e}sultats sont encourageants, et des applications sont d{'}ores et d{\'e}j{\`a} envisag{\'e}es."
erjavec-fiser-2006-building,Building {S}lovene {W}ord{N}et,2006,19,18,2,0,15753,tomavz erjavec,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"A WordNet is a lexical database in which nouns, verbs, adjectives and adverbs are organized in a conceptual hierarchy, linking semantically and lexically related concepts. Such semantic lexicons have become oneof the most valuable resources for a wide range of NLP research and applications, such as semantic tagging, automatic word-sense disambiguation, information retrieval and document summarisation. Following the WordNet design for the English languagedeveloped at Princeton, WordNets for a number of other languages havebeen developed in the past decade, taking the idea into the domain ofmultilingual processing. This paper reports on the prototype SloveneWordNet which currently contains about 5,000 top-level concepts. Theresource has been automatically translated from the Serbian WordNet, with the help of a bilingual dictionary, synset literals ranked according to the frequency of corpus occurrence, and results manually corrected. The paper presents the results obtained, discusses some problems encountered along the way and points out some possibilitiesof automated acquisition and refinement of synsets in the future."
