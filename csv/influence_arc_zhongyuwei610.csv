2020.acl-main.528,N13-1006,0,0.112491,"Missing"
2020.acl-main.528,W06-0130,0,0.562999,"ls Yang et al., 2016 Yang et al., 2016∗† Che et al., 2013∗ Wang et al., 2013∗ Word-based (LSTM) + char + bichar Word-based (LSTM) + char + bichar Char-based (LSTM) + bichar + softword + ExSoftword + bichar + ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT P 65.59 72.98 77.71 76.43 76.66 78.62 72.84 73.36 68.79 74.36 69.90 73.80 76.35 76.40 77.28 77.13 76.01 81.99 83.41 R 71.84 80.15 72.51 72.32 63.60 73.13 59.72 70.12 60.35 69.43 66.46 71.05 71.56 72.60 74.07 75.22 79.96 81.65 82.21 Models Chen et al., 2006 Zhang et al. 2006∗ Zhou et al. 2013 Lu et al. 2016 Dong et al. 2016 Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT F1 68.57 76.40 75.02 74.32 69.52 75.77 65.63 71.70 64.30 71.89 68.13 72.40 73.88 74.45 75.64 76.16 77.93 81.82 82.81 Models Peng and Dredze, 2015 Peng and Dredze, 2016∗ He and Sun, 2017a He and Sun, 2017b∗ Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLe"
2020.acl-main.528,P15-1017,0,0.059364,"Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-ofthe-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT. 1 1 Introduction Named Entity Recognition (NER) is concerned with the identification of named entities, such as persons, locations, and organizations, in unstructured text. NER plays an important role in many downstream tasks, including knowledge base construction (Riedel et al., 2013), information retrieval (Chen et al., 2015), and question answering (Diefenbach et al., 2018). In languages where words are naturally separated (e.g., English), NER has been conventionally formulated as a sequence ∗ Equal contribution. The source code of this paper is publicly available at https://github.com/v-mipeng/ LexiconAugmentedNER. 1 labeling problem, and the state-of-the-art results have been achieved using neural-network-based models (Huang et al., 2015; Chiu and Nichols, 2016; Liu et al., 2018). Compared with NER in English, Chinese NER is more difficult since sentences in Chinese are not naturally segmented. Thus, a common p"
2020.acl-main.528,Q16-1026,0,0.127253,"unstructured text. NER plays an important role in many downstream tasks, including knowledge base construction (Riedel et al., 2013), information retrieval (Chen et al., 2015), and question answering (Diefenbach et al., 2018). In languages where words are naturally separated (e.g., English), NER has been conventionally formulated as a sequence ∗ Equal contribution. The source code of this paper is publicly available at https://github.com/v-mipeng/ LexiconAugmentedNER. 1 labeling problem, and the state-of-the-art results have been achieved using neural-network-based models (Huang et al., 2015; Chiu and Nichols, 2016; Liu et al., 2018). Compared with NER in English, Chinese NER is more difficult since sentences in Chinese are not naturally segmented. Thus, a common practice for Chinese NER is to first perform word segmentation using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to perf"
2020.acl-main.528,N19-1423,0,0.0703552,"Missing"
2020.acl-main.528,P19-1141,0,0.280767,"on using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this consideration, Zhang and Yang, (2018) proposed Lattice-LSTM for incorporating word lexicons into the character-based NER model. Moreover, rather than heuristically choosing a word for the character when it matches multiple words in the lexicon, the authors proposed to preserve all words that match the character, leaving the subsequent NER model to determine which word to apply. To realize this idea, they introduced an elaborate modification to the sequence modeling layer of the"
2020.acl-main.528,D19-1096,1,0.748725,"Missing"
2020.acl-main.528,E17-2113,0,0.452886,"on. The source code of this paper is publicly available at https://github.com/v-mipeng/ LexiconAugmentedNER. 1 labeling problem, and the state-of-the-art results have been achieved using neural-network-based models (Huang et al., 2015; Chiu and Nichols, 2016; Liu et al., 2018). Compared with NER in English, Chinese NER is more difficult since sentences in Chinese are not naturally segmented. Thus, a common practice for Chinese NER is to first perform word segmentation using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this considerati"
2020.acl-main.528,I08-4022,0,0.050302,"nese are not naturally segmented. Thus, a common practice for Chinese NER is to first perform word segmentation using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this consideration, Zhang and Yang, (2018) proposed Lattice-LSTM for incorporating word lexicons into the character-based NER model. Moreover, rather than heuristically choosing a word for the character when it matches multiple words in the lexicon, the authors proposed to preserve all words that match the character, leaving the subsequent NER model to determine which word to"
2020.acl-main.528,W06-0115,0,0.621713,"highest conditional probability given the input sequence s: y ∗ =y p(y|s; θ), (14) which can be efficiently solved using the Viterbi algorithm (Forney, 1973). 4 4.1 Experiments Experiment Setup Most experimental settings in this work followed the protocols of Lattice-LSTM (Zhang and Yang, 2018), including tested datasets, compared baselines, evaluation metrics (P, R, F1), and so on. To make this work self-completed, we concisely illustrate some primary settings of this work. Datasets The methods were evaluated on four Chinese NER datasets, including OntoNotes (Weischedel et al., 2011), MSRA (Levow, 2006), Weibo NER (Peng 5955 OntoNotes 1× 2.23× 2.56× 2.77× 6.15× 6.08× 2.74× MSRA 1× 1.57× 2.55× 2.32× 5.78× 5.95× 2.33× Weibo 1× 2.41× 4.45× 2.84× 6.10× 5.91× 2.85× Resume 1× 1.44× 3.12× 2.38× 6.13× 6.45× 2.32× 175 Table 2: Inference speed (average sentences per second, the larger the better) of our method with LSTM layer compared with Lattice-LSTM, LR-CNN and BERT. and Dredze, 2015; He and Sun, 2017a), and Resume NER (Zhang and Yang, 2018). OntoNotes and MSRA are from the newswire domain, where gold-standard segmentation is available for training data. For OntoNotes, gold segmentation is also ava"
2020.acl-main.528,li-etal-2014-comparison,0,0.270291,"Missing"
2020.acl-main.528,N19-1247,0,0.295037,"ice for Chinese NER is to first perform word segmentation using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this consideration, Zhang and Yang, (2018) proposed Lattice-LSTM for incorporating word lexicons into the character-based NER model. Moreover, rather than heuristically choosing a word for the character when it matches multiple words in the lexicon, the authors proposed to preserve all words that match the character, leaving the subsequent NER model to determine which word to apply. To realize this idea, they introduced an elab"
2020.acl-main.528,L16-1138,0,0.210153,"013∗ Wang et al., 2013∗ Word-based (LSTM) + char + bichar Word-based (LSTM) + char + bichar Char-based (LSTM) + bichar + softword + ExSoftword + bichar + ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT P 65.59 72.98 77.71 76.43 76.66 78.62 72.84 73.36 68.79 74.36 69.90 73.80 76.35 76.40 77.28 77.13 76.01 81.99 83.41 R 71.84 80.15 72.51 72.32 63.60 73.13 59.72 70.12 60.35 69.43 66.46 71.05 71.56 72.60 74.07 75.22 79.96 81.65 82.21 Models Chen et al., 2006 Zhang et al. 2006∗ Zhou et al. 2013 Lu et al. 2016 Dong et al. 2016 Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT F1 68.57 76.40 75.02 74.32 69.52 75.77 65.63 71.70 64.30 71.89 68.13 72.40 73.88 74.45 75.64 76.16 77.93 81.82 82.81 Models Peng and Dredze, 2015 Peng and Dredze, 2016∗ He and Sun, 2017a He and Sun, 2017b∗ Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagge"
2020.acl-main.528,D15-1064,0,0.353453,"Missing"
2020.acl-main.528,W06-0126,0,0.660456,"16 Yang et al., 2016∗† Che et al., 2013∗ Wang et al., 2013∗ Word-based (LSTM) + char + bichar Word-based (LSTM) + char + bichar Char-based (LSTM) + bichar + softword + ExSoftword + bichar + ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT P 65.59 72.98 77.71 76.43 76.66 78.62 72.84 73.36 68.79 74.36 69.90 73.80 76.35 76.40 77.28 77.13 76.01 81.99 83.41 R 71.84 80.15 72.51 72.32 63.60 73.13 59.72 70.12 60.35 69.43 66.46 71.05 71.56 72.60 74.07 75.22 79.96 81.65 82.21 Models Chen et al., 2006 Zhang et al. 2006∗ Zhou et al. 2013 Lu et al. 2016 Dong et al. 2016 Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT F1 68.57 76.40 75.02 74.32 69.52 75.77 65.63 71.70 64.30 71.89 68.13 72.40 73.88 74.45 75.64 76.16 77.93 81.82 82.81 Models Peng and Dredze, 2015 Peng and Dredze, 2016∗ He and Sun, 2017a He and Sun, 2017b∗ Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftL"
2020.acl-main.528,P18-1144,0,0.753284,"However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this consideration, Zhang and Yang, (2018) proposed Lattice-LSTM for incorporating word lexicons into the character-based NER model. Moreover, rather than heuristically choosing a word for the character when it matches multiple words in the lexicon, the authors proposed to preserve all words that match the character, leaving the subsequent NER model to determine which word to apply. To realize this idea, they introduced an elaborate modification to the sequence modeling layer of the LSTM-CRF model (Huang et al., 2015). Experimental studies on four Chinese NER datasets have verified the effectiveness of Lattice-LSTM. However, the model"
2020.acl-main.528,I08-4017,0,0.0546155,"al., 2018). We performed experiments on four public Chinese NER datasets. The experimental results show that when implementing the sequence modeling layer with a single-layer Bi-LSTM, our method achieves considerable improvements over the state-of-theart methods in both inference speed and sequence labeling performance. 2 Background In this section, we introduce several previous works that influenced our work, including the Softword technique and Lattice-LSTM. 2.1 Softword Feature The Softword technique was originally used for incorporating word segmentation information into downstream tasks (Zhao and Kit, 2008; Peng and Dredze, 2016). It augments the character representation with the embedding of its corresponding segmentation label: xcj ← [xcj ; eseg (seg(cj ))]. (1) Here, seg(cj ) ∈ Yseg denotes the segmentation label of the character cj predicted by the word segmentor, eseg denotes the segmentation label embedding lookup table, and typically Yseg = {B, M, E, S}. However, gold segmentation is not provided in most datasets, and segmentation results obtained by a segmenter can be incorrect. Therefore, segmentation errors will inevitably be introduced through this approach. 2.2 Lattice-LSTM Lattice-"
2020.acl-main.528,P16-2025,0,0.595518,"rmed experiments on four public Chinese NER datasets. The experimental results show that when implementing the sequence modeling layer with a single-layer Bi-LSTM, our method achieves considerable improvements over the state-of-theart methods in both inference speed and sequence labeling performance. 2 Background In this section, we introduce several previous works that influenced our work, including the Softword technique and Lattice-LSTM. 2.1 Softword Feature The Softword technique was originally used for incorporating word segmentation information into downstream tasks (Zhao and Kit, 2008; Peng and Dredze, 2016). It augments the character representation with the embedding of its corresponding segmentation label: xcj ← [xcj ; eseg (seg(cj ))]. (1) Here, seg(cj ) ∈ Yseg denotes the segmentation label of the character cj predicted by the word segmentor, eseg denotes the segmentation label embedding lookup table, and typically Yseg = {B, M, E, S}. However, gold segmentation is not provided in most datasets, and segmentation results obtained by a segmenter can be incorrect. Therefore, segmentation errors will inevitably be introduced through this approach. 2.2 Lattice-LSTM Lattice-LSTM designs to incorpor"
2020.acl-main.528,N13-1008,0,0.0185885,"ation. Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-ofthe-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT. 1 1 Introduction Named Entity Recognition (NER) is concerned with the identification of named entities, such as persons, locations, and organizations, in unstructured text. NER plays an important role in many downstream tasks, including knowledge base construction (Riedel et al., 2013), information retrieval (Chen et al., 2015), and question answering (Diefenbach et al., 2018). In languages where words are naturally separated (e.g., English), NER has been conventionally formulated as a sequence ∗ Equal contribution. The source code of this paper is publicly available at https://github.com/v-mipeng/ LexiconAugmentedNER. 1 labeling problem, and the state-of-the-art results have been achieved using neural-network-based models (Huang et al., 2015; Chiu and Nichols, 2016; Liu et al., 2018). Compared with NER in English, Chinese NER is more difficult since sentences in Chinese ar"
2020.acl-main.528,D19-1396,0,0.83911,"Missing"
2020.coling-main.182,W05-0909,0,0.214213,"Missing"
2020.coling-main.182,D15-1075,0,0.0395072,"he effectiveness of our proposed approach. To dig into our approach, we perform ablation studies to explore the different effects of scaling module and prototype position indicator. 3.1 Prototype Collection In-Domain Corpus Din CommonGen is to describe a common scenario in our daily life, datasets of image captioning or video captioning would contain more knowledge about spatial relations, object properties, physical rules, temporal event knowledge and social conventions that contribute to build the target scene contains the these provided concepts. We utilize VaTeX (Wang et al., 2019), SNLI (Bowman et al., 2015), Activity (Krishna et al., 2017) and the training set of CommonGen as the external plain text knowledge datasets and retrieve prototype according to the concepts appear in the sentence. Out-of-Domain Corpus Dout In-domain corpus Din may only suitable for these description sentence for daily scenario and has difficulty in generalizing toother domains, thus we also employ wikipedia as our external knowledge dataset to retrieve prototypes to test the generalization of our model. The number of retrieved prototypes concepts that co-occur in ground truth sentence across different external knowledge"
2020.coling-main.182,P18-1015,0,0.105109,"ackground knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. Song et al. (2016)"
2020.coling-main.182,P16-1154,0,0.0292584,"1024 and 5k. The dropout rate is 0.1. We set the standard deviation of initialization in group embedding, scaling module and prototype position indicator to 5e-3. The optimizer of model is Adam (Kingma and Ba, 2014) with β1 = 0.9 and β2 = 0.999. During decoding, the size of beam search is 5 and the length penalty is 0.0. 3.3 Results For the compared methods, we classify them into four groups. Group 1 Models without pretraining. bRNN-CopyNet and Trans-CopyNet are based on the best popular architecture Bidirectional RNNs and Transformers (Vaswani et al., 2017) with attention and copy mechanism (Gu et al., 2016). MeanPooling-CopyNet is employed to deal with the influence of the concept ordering in the sequential based methods, where the input concepts is randomly permuted multiple times and decoding is with a mean pooling based MLP network. Levenshtein Transformer (Gu et al., 2019) is an edit-based non-autoregressive generation model, where the generated sentences go through multiple refinement. Group 2 Pretrained language generation models including GPT-2 (Radford et al., 2019), UniLM (Dong et al., 2019), UniLM-v2 (Bao et al., 2020), BERT-Gen (Bao et al., 2020), BART (Lewis et al., 2019), and T5 (Ra"
2020.coling-main.182,Q18-1031,0,0.0270196,"dy this problem. Consider quite a few relationship reasoning over these concepts require a variety of background knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devl"
2020.coling-main.182,2020.acl-main.228,0,0.0155863,"g dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. Song et al. (2016) introduces an extra encoder for the retrieved response, and the output of the encoder, together with that of the query encoder, is utilized to feed the decoder. Weston et al. (2018) simply concatenates the original query and the retrieves response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduces to encodes the lexical differences between the current que"
2020.coling-main.182,N03-1020,0,0.349131,"Missing"
2020.coling-main.182,D19-1282,0,0.104528,"nation and complete the scenario with introducing additional concepts. We propose to use two kind of corpus as out of domain and in domain external knowledge to retrieve the prototypes respectively. To better model the prototypes, we design two attention mechanisms to enhance the knowledge injection procedure. We conduct experiment on CommonGen benchmark, experimental results show that our method significantly improves the performance on all the metrics. 1 Introduction Recently, commonsense reasoning tasks such as SWAG (Zellers et al., 2018), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are presented to investigate the model’s ability to make acceptable and logical assumptions about ordinary scenes in our daily life. SWAG requires to infer the probable subsequent event based on the given textual description of an event. CommonsenseQA focuses on commonsense question answering that collects commonsense questions at scale by describing the relation between concepts from CONCEPTNET. Different from these discriminative tasks, CommonGen is a generation task that not only needs to use background commonsense knowledge to conduct relational reasoning, but also compositional based g"
2020.coling-main.182,S19-1012,0,0.0196311,"missing of EKI-BART Din is more than that of BART Din , which shows that BART Din is more likely to ignore the provided concepts than BART Din and being dominated by noises in prototype. This also verifies that the ability of BART Dout in dealing with prototype noises is stronger than BART Din , and removing these noises benefits to finding out a more plausible scenario with these concepts. 4 4.1 Related Work Commonsense Reasoning Recently, there are emerging works to investigate machine commonsense reasoning ability. ATOMIC (Sap et al., 2019), Event2Mind (Rashkin et al., 2018), MCScript 2.0 (Ostermann et al., 2019), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), Story Cloze Test (Mostafazadeh et al., 2017), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are released to reasoning over external knowledge besides the inputs for question answering or generation. Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019a) constructs 2021 schema graphs from ConceptNet to reason over relevant commonsense knowledge. lv et al. (2020) focuses on automatically extracting evidence from heterogeneous external knowledge and reasoning"
2020.coling-main.182,P18-1123,0,0.0227478,"etrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. Song et al. (2016) introduces an extra encoder for the retrieved response, and the output of the encoder, together with that of the query encoder, is utilized to feed the decoder. Weston et al. (2018) simply concatenates the original query and the retrieves response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduces to encodes the lexical differences between the current query and the retrieved query. Pandey et al. (2018) proposes to weight different training instances by context similarity. Different from these work, We explore the retrieve-and-edit framework on the basis of pretrained encoder-decoder model, and identify the importance of each token in prototype in a more fine-grained manner. 5 Conclusion and Future Work In this paper, we have proposed a pretraining enhanced retrieve-and-edit model for commonsense generation. The key of CommonGen is to identify the priority of the scene based on the concept combination, we have scaling module to softly reduce the impact of prototype noises on generation and p"
2020.coling-main.182,P02-1040,0,0.10919,"Missing"
2020.coling-main.182,N19-1263,0,0.0107029,"such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. Song et al. (2016) introduces an extra e"
2020.coling-main.182,P19-1487,0,0.0145731,"hese noises benefits to finding out a more plausible scenario with these concepts. 4 4.1 Related Work Commonsense Reasoning Recently, there are emerging works to investigate machine commonsense reasoning ability. ATOMIC (Sap et al., 2019), Event2Mind (Rashkin et al., 2018), MCScript 2.0 (Ostermann et al., 2019), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), Story Cloze Test (Mostafazadeh et al., 2017), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are released to reasoning over external knowledge besides the inputs for question answering or generation. Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019a) constructs 2021 schema graphs from ConceptNet to reason over relevant commonsense knowledge. lv et al. (2020) focuses on automatically extracting evidence from heterogeneous external knowledge and reasoning over the extracted evidence to study this problem. Consider quite a few relationship reasoning over these concepts require a variety of background knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any exis"
2020.coling-main.182,P18-1043,0,0.0246896,"e number of instance with no concept missing of EKI-BART Din is more than that of BART Din , which shows that BART Din is more likely to ignore the provided concepts than BART Din and being dominated by noises in prototype. This also verifies that the ability of BART Dout in dealing with prototype noises is stronger than BART Din , and removing these noises benefits to finding out a more plausible scenario with these concepts. 4 4.1 Related Work Commonsense Reasoning Recently, there are emerging works to investigate machine commonsense reasoning ability. ATOMIC (Sap et al., 2019), Event2Mind (Rashkin et al., 2018), MCScript 2.0 (Ostermann et al., 2019), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), Story Cloze Test (Mostafazadeh et al., 2017), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are released to reasoning over external knowledge besides the inputs for question answering or generation. Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019a) constructs 2021 schema graphs from ConceptNet to reason over relevant commonsense knowledge. lv et al. (2020) focuses on automatically extracting evidence from heteroge"
2020.coling-main.182,D15-1044,0,0.0470787,"uire a variety of background knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. S"
2020.coling-main.182,W18-5713,0,0.0167139,"l knowledge and reasoning over the extracted evidence to study this problem. Consider quite a few relationship reasoning over these concepts require a variety of background knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework"
2020.coling-main.182,2020.findings-emnlp.217,1,0.871963,"Missing"
2020.coling-main.182,D18-1009,0,0.103227,"rpus would benefit to discriminate the priority of different concept combination and complete the scenario with introducing additional concepts. We propose to use two kind of corpus as out of domain and in domain external knowledge to retrieve the prototypes respectively. To better model the prototypes, we design two attention mechanisms to enhance the knowledge injection procedure. We conduct experiment on CommonGen benchmark, experimental results show that our method significantly improves the performance on all the metrics. 1 Introduction Recently, commonsense reasoning tasks such as SWAG (Zellers et al., 2018), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are presented to investigate the model’s ability to make acceptable and logical assumptions about ordinary scenes in our daily life. SWAG requires to infer the probable subsequent event based on the given textual description of an event. CommonsenseQA focuses on commonsense question answering that collects commonsense questions at scale by describing the relation between concepts from CONCEPTNET. Different from these discriminative tasks, CommonGen is a generation task that not only needs to use background commonsense know"
2020.coling-main.182,P19-1472,0,0.0167693,"ws that BART Din is more likely to ignore the provided concepts than BART Din and being dominated by noises in prototype. This also verifies that the ability of BART Dout in dealing with prototype noises is stronger than BART Din , and removing these noises benefits to finding out a more plausible scenario with these concepts. 4 4.1 Related Work Commonsense Reasoning Recently, there are emerging works to investigate machine commonsense reasoning ability. ATOMIC (Sap et al., 2019), Event2Mind (Rashkin et al., 2018), MCScript 2.0 (Ostermann et al., 2019), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), Story Cloze Test (Mostafazadeh et al., 2017), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are released to reasoning over external knowledge besides the inputs for question answering or generation. Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019a) constructs 2021 schema graphs from ConceptNet to reason over relevant commonsense knowledge. lv et al. (2020) focuses on automatically extracting evidence from heterogeneous external knowledge and reasoning over the extracted evidence to study this problem. Consider qui"
2020.coling-main.204,W05-0909,0,0.0914806,"nt for sentence generation . VST: This is the baseline version of our model without using topic information as guidance. TAVST w/o IU: This is our proposed TAVST method without IU module, which only equipped with initial topic description generator. TAVST: This is our full model. TAVST (MLE) is trained using MLE loss, while TAVST (RL) is trained via RL loss. 3.4 Automatic Evaluation Results We evaluate our model on two generation tasks i.e., story generation and topic description generation, in terms of four automatic metrics: BLEU (Papineni et al., 2002), ROUGE-L (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Story Generation The overall experimental results are shown in Table 1. TAVST (MLE) outperforms all of the baseline models trained with MLE. This confirms the effectivness of topic information for generating better stories. Noticeably, compared with the RL-based models, our TAVST (MLE) has already achieved a competitive performance and outperforms other RL models (i.e., AREL and HSRL) in terms of METEOR and BLEU@[2, 3, 4] metrics. After equipped with RL, our TAVST (RL) model is able to further improve the performance, outperforming the two RL models in term"
2020.coling-main.204,D13-1128,0,0.0232593,". 3.7 Case Study Figure 4 shows an example of the ground-truth story and stories generated automatically by different models. The words in red, blue and yellow color represent the topic, subject, and emotion, respectively. Our model shows promising results according to topic consistency, which further confirms that our model can extract appropriate topic which serves as the guidance of generating a topic-consistent story. 2257 4 Related work This paper is related to the fields of image captioning, visual storytelling and multi-task learning. Image Captioning In early works (Yang et al., 2011; Elliott and Keller, 2013), image captioning is treated as a ranking problem, which is based on retrieval models to identify similar captions from the database. Later, the end-to-end frameworks based on the CNN and RNN are adopted by researchers (Xu et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals et al., 2017; Dai et al., 2017). Such works focus on the literal description of image content, while the generated texts is limited in a single sentence. Visual Storytelling Visual storytelling is the task of generating a narrative paragraph for an image stream. Huang et al. (2016) introduce the first dataset (VIST) for visu"
2020.coling-main.204,N16-1147,0,0.520831,"g (Antol et al., 2015; Yu et al., 2017b; Singh et al., 2019), aiming at generating a short sentence or a phrase conditioned on certain visual information. With the development of deep learning and reinforcement learning models, recent years witness promising improvement of these tasks for single-image-to-single-sentence generation. Visual storytelling moves one step further, extending the input and output dimension to a sequence of images and a sequence of sentences. It requires the model to understand the main idea of an image stream and generate coherent sentences. Most of existing methods (Huang et al., 2016; Liu et al., 2017; Yu et al., 2017a; Wang et al., 2018a; Wang et al., 2020) for visual storytelling extend approaches of image captioning without considering topic information of the image sequence, which causes the problem of generating semantically incoherent content. An example of visual storytelling can be seen in Figure 1. An image stream with five images about a car accident is presented accompanied with two stories. One is constructed by a human annotator and the other is produced by an automatic storytelling approach. There are two problems with the machine generated story. First, the"
2020.coling-main.204,P18-1240,0,0.103348,"he decoder produces the a hidden state hti . Once the last topic hidden state t hM is obtained, we concatenate all topic hidden states ht = [ht1 , ..., htM ], M &gt; 1 as the topic memory, which are fed into the story generation module. 2.3 Initial Story Generator with Co-attention Network The initial story generator is responsible for generating the story with the guidance of the topic description constructed by the initial topic description generator. Co-attention Encoding In order to combine both visual information and topic information for story generation, we adopt a co-attention mechanism (Jing et al., 2018) for context information encoding. Specifically, given visual context vectors hv and topic vectors ht , the affinity matrix C is calculated by T C = tanh(ht Wb hv ) (2) where Wb is the weight parameter. After calculating this matrix, we compute attentions weights over the visual context vectors and the topic vectors via the following operations: H v = tanh(Wv hv + (Wt ht )C) H t = tanh(Wt ht + (Wv hv )C T ) T av = softmax(whv Hv) (3) T at = softmax(wht H t) T , w T are the weight parameters. Based on the attention weights, the visual and where Wv , Wt , whv ht semantic attentions are calculate"
2020.coling-main.204,P04-1077,0,0.0985818,"dance to the lower level agent for sentence generation . VST: This is the baseline version of our model without using topic information as guidance. TAVST w/o IU: This is our proposed TAVST method without IU module, which only equipped with initial topic description generator. TAVST: This is our full model. TAVST (MLE) is trained using MLE loss, while TAVST (RL) is trained via RL loss. 3.4 Automatic Evaluation Results We evaluate our model on two generation tasks i.e., story generation and topic description generation, in terms of four automatic metrics: BLEU (Papineni et al., 2002), ROUGE-L (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Story Generation The overall experimental results are shown in Table 1. TAVST (MLE) outperforms all of the baseline models trained with MLE. This confirms the effectivness of topic information for generating better stories. Noticeably, compared with the RL-based models, our TAVST (MLE) has already achieved a competitive performance and outperforms other RL models (i.e., AREL and HSRL) in terms of METEOR and BLEU@[2, 3, 4] metrics. After equipped with RL, our TAVST (RL) model is able to further improve the performance, outp"
2020.coling-main.204,P02-1040,0,0.108952,"concept for each image as the guidance to the lower level agent for sentence generation . VST: This is the baseline version of our model without using topic information as guidance. TAVST w/o IU: This is our proposed TAVST method without IU module, which only equipped with initial topic description generator. TAVST: This is our full model. TAVST (MLE) is trained using MLE loss, while TAVST (RL) is trained via RL loss. 3.4 Automatic Evaluation Results We evaluate our model on two generation tasks i.e., story generation and topic description generation, in terms of four automatic metrics: BLEU (Papineni et al., 2002), ROUGE-L (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Story Generation The overall experimental results are shown in Table 1. TAVST (MLE) outperforms all of the baseline models trained with MLE. This confirms the effectivness of topic information for generating better stories. Noticeably, compared with the RL-based models, our TAVST (MLE) has already achieved a competitive performance and outperforms other RL models (i.e., AREL and HSRL) in terms of METEOR and BLEU@[2, 3, 4] metrics. After equipped with RL, our TAVST (RL) model is able to further"
2020.coling-main.204,P18-1083,0,0.0760091,"019), aiming at generating a short sentence or a phrase conditioned on certain visual information. With the development of deep learning and reinforcement learning models, recent years witness promising improvement of these tasks for single-image-to-single-sentence generation. Visual storytelling moves one step further, extending the input and output dimension to a sequence of images and a sequence of sentences. It requires the model to understand the main idea of an image stream and generate coherent sentences. Most of existing methods (Huang et al., 2016; Liu et al., 2017; Yu et al., 2017a; Wang et al., 2018a; Wang et al., 2020) for visual storytelling extend approaches of image captioning without considering topic information of the image sequence, which causes the problem of generating semantically incoherent content. An example of visual storytelling can be seen in Figure 1. An image stream with five images about a car accident is presented accompanied with two stories. One is constructed by a human annotator and the other is produced by an automatic storytelling approach. There are two problems with the machine generated story. First, the sentiment expressed in the text is inappropriate. In f"
2020.coling-main.204,P19-1240,0,0.128394,"o BLEU@N all the time (Vedantam et al., 2015; Wang et al., 2018a). During the test stage, we generate the stories by performing a beam-search with a beam size of 3. 3.3 Models for Comparison We compare our proposed methods with several baselines for visual storytelling as follows: seq2seq (Huang et al., 2016): It generates caption for each single model via classic sequence-tosequence model and concatenate all captions to form the final story. h-attn-rank (Yu et al., 2017a): On top of the classic sequence-to-sequence model, it adds an additional RNN to select photos for story generation. HPSR (Wang et al., 2019a): It introduces an additional RNN stacked on the RNN-based photo encoder to detect the scene change. Information from both RNNs are fed into an RNN for story generation. 2254 Methods MLE seq2seq (Huang et al., 2016) h-attn-rank (Yu et al., 2017a) HPSR (Wang et al., 2019a) VST (MLE) TAVST w/o IU (MLE) TAVST (MLE) RL AREL (Wang et al., 2018b) HSRL (Huang et al., 2019) TAVST w/o IU (RL) TAVST (RL) B-1 B-2 B-3 B-4 R-L C M − − 61.9 62.3 63.1 63.6 − − 37.8 38.0 38.6 39.3 − 21.0 21.5 21.8 22.9 23.4 3.5 − 12.2 12.7 14.0 14.2 − 29.5 31.2 29.7 29.7 30.3 6.8 7.5 8.0 7.8 8.5 8.7 31.4 34.1 34.4 34.3 35.1"
2020.coling-main.204,H05-1044,0,0.0348739,"hoose which story is better in terms of a certain factor. Results are shown in Table 4. Our model performs better than the other two models in terms of relevance and topic consistency. The advantage of topic consistency is more promising. This proves that the topic description generator can help the story generation agent construct a more consistent story. 3.6 Further Analysis on Topic Consistency We further evaluate the quality of the generated story in terms of topic consistency from the perspective of sentiment. Specifically, we employ a lexicon-based approach using a subjectivity lexicon (Wilson et al., 2005). We count the number of sentiment words in each sentence for the polarity evaluation. The score will be 1,0,-1 if a sentence is positive, neutral and negative, respectively. Based on the score for each sentence, two qualitative experiments are designed to measure the in-story sentiment consistency and topic-story sentiment consistency. In-story Sentiment Consistency We argue that the sentiment of sentences in a story should be consistent given the album is related to a certain topic. For each story, we obtain a vector with 5 sentiment scores in correspondence to 5 sentences. We then calculate"
2020.coling-main.204,D18-1397,0,0.0158067,"ing text generation model via introducing automatic metrics (e.g., METEOR) to guide the training process (Wang et al., 2018b). We also explore the RL-based approach to train our generator. The reinforcement learning (RL) loss can be written as: ∗ 2 Linit (9) story(rl) (θ1 ) = −Ey∼pθ1 (r(y; y ) − b) where r is a sentence-level metric for the sampled sentence y and the ground-truth y ∗ ; b is the baseline which can be an arbitrary function but a linear layer in our experiments for simply. To stabilize the RL training process, a simple way is to linearly combine MLE and RL objectives as follows (Wu et al., 2018): init init Linit (10) story(com) = αLstory(rl) + (1 − α)Lstory(mle) where hyper-parameter α is employed to control the trade-off between MLE and RL objectives. init In the initial stage, a combined loss function of Linit story(com) and Ltopic is computed through: init Linit = λ1 Linit story(com) + (1 − λ1 )Ltopic(mle) (11) where hyper-parameter λ1 is employed to balance these losses. 2.4 Iterative Updating Module Considering that the generated story would also be helpful for the generation of topic description, we design an iterative updating module for the two agents to interact with each ot"
2020.coling-main.204,D17-1101,0,0.241617,"earn them simultaneously via iterative updating mechanism. We validate our approach on VIST dataset, where quantitative results, ablations, and human evaluation demonstrate our method’s good ability in generating stories with higher quality compared to state-of-the-art methods. 1 Introduction Image-to-text generation is an important topic in artificial intelligence (AI) which connects computer vision (CV) and natural language processing (NLP). Popular tasks include image captioning (Karpathy and Fei-Fei, 2015; Ren et al., 2017; Vinyals et al., 2017) and question answering (Antol et al., 2015; Yu et al., 2017b; Singh et al., 2019), aiming at generating a short sentence or a phrase conditioned on certain visual information. With the development of deep learning and reinforcement learning models, recent years witness promising improvement of these tasks for single-image-to-single-sentence generation. Visual storytelling moves one step further, extending the input and output dimension to a sequence of images and a sequence of sentences. It requires the model to understand the main idea of an image stream and generate coherent sentences. Most of existing methods (Huang et al., 2016; Liu et al., 2017;"
2020.coling-main.561,K16-1002,0,0.045755,"Missing"
2020.coling-main.561,C18-1288,0,0.203055,"ses the factuality of the message. Besides, disagreement and query towards descriptive statements are able to trigger drastic discussion and result in validity modification. Although some researchers explore propagation structure of rumor proliferation (Ma et al., 2017; Kumar and Carley, 2019), they typically rely on rough aggregation of locally successional messages. Moreover, the evolution of message interaction depicts the global characteristic of rumor cascades which improves the performance of verification. Figure 1 (b) illustrates the intuition using statistics drawn from PHEME dataset (Kochkina et al., 2018). It can be seen that denial tweets with supportive parent posts appear frequently in false rumors especially in an early stage, while unverified rumors constantly stimulate queries behind positive messages along with time. As rumor cascade evolves, with more dialogue context and auxiliary evidence, assessing the message credibility comprehensively becomes possible. ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6377 Proceedings of the 28th International Conference on Co"
2020.coling-main.561,P19-1498,0,0.211651,"pened during the message diffusion which is deemed to be important for the identification of rumors. Figure 1 (a) shows a rumor cascade which is identified as false for devilishly suspect Ray Radley’s role in the appalling Sydney siege. As can be seen, denial to false rumor tends to evoke affirmative replies which further confuses the factuality of the message. Besides, disagreement and query towards descriptive statements are able to trigger drastic discussion and result in validity modification. Although some researchers explore propagation structure of rumor proliferation (Ma et al., 2017; Kumar and Carley, 2019), they typically rely on rough aggregation of locally successional messages. Moreover, the evolution of message interaction depicts the global characteristic of rumor cascades which improves the performance of verification. Figure 1 (b) illustrates the intuition using statistics drawn from PHEME dataset (Kochkina et al., 2018). It can be seen that denial tweets with supportive parent posts appear frequently in false rumors especially in an early stage, while unverified rumors constantly stimulate queries behind positive messages along with time. As rumor cascade evolves, with more dialogue con"
2020.coling-main.561,P17-1066,0,0.154828,"interactions happened during the message diffusion which is deemed to be important for the identification of rumors. Figure 1 (a) shows a rumor cascade which is identified as false for devilishly suspect Ray Radley’s role in the appalling Sydney siege. As can be seen, denial to false rumor tends to evoke affirmative replies which further confuses the factuality of the message. Besides, disagreement and query towards descriptive statements are able to trigger drastic discussion and result in validity modification. Although some researchers explore propagation structure of rumor proliferation (Ma et al., 2017; Kumar and Carley, 2019), they typically rely on rough aggregation of locally successional messages. Moreover, the evolution of message interaction depicts the global characteristic of rumor cascades which improves the performance of verification. Figure 1 (b) illustrates the intuition using statistics drawn from PHEME dataset (Kochkina et al., 2018). It can be seen that denial tweets with supportive parent posts appear frequently in false rumors especially in an early stage, while unverified rumors constantly stimulate queries behind positive messages along with time. As rumor cascade evolve"
2020.coling-main.561,D14-1162,0,0.085117,"nfirm the effectiveness of message interaction. Among these two tasks, verification is labeled on cascade-level while stance belongs to tweet-level annotations. PHEME is undoubtedly suitable for our exploration of message interaction as it is constructed by a large amount of conversational threads in which participants tend to launch discussion other than judge on the source tweet. 4.2 Preprocessing and Training Details We preprocess each tweet by the NLTK toolkit (Bird et al., 2009) and follow a procedure of removing url and @, tokenizing, lemmatizing, and removing all the stop words. Glove (Pennington et al., 2014) word embeddings with dimension of 300 are adopted without being fine-tuned. As for training process, we 6382 perform leave-one-event-out (LOEO) cross validation (Kochkina et al., 2018). Although it suffers a lot to handle problems such as evil-balanced instances for each event and semantic inconsistency between events, LOEO is much more representative of real world and has been adopted by latest researches (Kumar and Carley, 2019; Wei et al., 2019). Hyperparameters performing best in development set are fixed and recorded. The network is trained with back propagation using the Adagrad update"
2020.coling-main.561,D19-1485,0,0.334562,"at are ambiguous at the time of posting, then explore how users share and discuss rumors and finally assess their veracity as true, false or unverified. This can be represented as a pipeline of sub-tasks, including rumor detection, stance classification and rumor verification (Zubiaga et al., 2018a). Identifying and debunking rumors automatically has been extensively studied in the past few years. State-of-the-art approaches construct sequential representations following a chronological order and then utilize temporal features to capture dynamic signals (Zubiaga et al., 2016; Ma et al., 2016; Wei et al., 2019). Although the source content stays invariable, time-series modeling successfully locates modifiers who might import evidence to correct misinformation or stir up enmity to discredit truth (Zhang et al., 2013). These models generate promising results, however, they ignore local interactions happened during the message diffusion which is deemed to be important for the identification of rumors. Figure 1 (a) shows a rumor cascade which is identified as false for devilishly suspect Ray Radley’s role in the appalling Sydney siege. As can be seen, denial to false rumor tends to evoke affirmative rep"
2020.coling-main.561,P18-1101,0,0.10894,"poses the discrete variational autoencoders (DVAEs) which assume that the corresponding prior distribution over the latent space is characterized by independent categorical distributions. Especially for text mining, discrete variables are adaptive to holistic properties of text and much more friendly for interpreting categories of natural language such as style, topic and high-level syntactic features. For instance, in neural dialog generation, DVAE is able to learn underlying dialogue intentions that can be interpreted as actions guiding the generation of machine responses (Wen et al., 2017; Zhao et al., 2018). In this paper, we learn discrete latent variables between inherited post pairs and incorporate them with textual information to model message interaction. 3 Proposed Model Resolution of rumor cascades can be formulated as a supervised classification problem. Given a treestuctured T WITTER cascade C which corresponds to a root tweet r0 and its relevant responsive tweets {r1 , r2 , ..., rT }, the goal is to recognize the stance of each tweet Yis as support, comment, deny or query, as well as determine the class of the cascade Yv as true, false or unverified. From our dataset, for each tweet ri"
2020.coling-main.561,N19-1163,0,0.0121094,"ssages, dynamic time series structure (Ma et al., 2015) and tree model using propagation pattern (Ma et al., 2017) is effective of depicting global difference between rumor and non-rumor claims. To avoid the effort and bias of feature engineering, methods based on deep neural networks are massively applied and have demonstrated great efficacy of discovering data representation automatically. Ma 6378 et al. (2016) employ recurrent neural networks (RNNs) to capture dynamic temporal signals. Yu et al. (2017) use convolutional neural networks (CNNs) to flexibly extract evidential posts. Recently, Zhou et al. (2019) integrate reinforcement learning to select the minimum number of posts required for early rumor detection. Ma et al. (2019) generate less indicative semantic representation via generative adversarial networks to gain better generalization for rumor detection. Besides, since rumor resolution is a coherent process, researchers also combine detection and stance classification with verification under the framework of multi-task learning (Ma et al., 2018; Kochkina et al., 2018; Kumar and Carley, 2019; Wei et al., 2019). In summary, deep learning approaches for rumor resolution involves three criti"
2020.emnlp-main.320,2020.acl-main.499,0,0.0209775,"rounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural network, whose predictions are used to teach the student model. Wang and Poon (2018) generalize virtual evidence (Pearl, 2014) to arbitrary potential functions over inputs and outputs, and use deep probabilistic logic to integrate indirection supervision into neural networks. More recently, Asai and Hajishirzi (2020) regularize question answering systems with symmetric consistency and symmetric consistency. The former creates a symmetric question by replacing words with their antonyms in comparison question, while the latter is for causal reasoning questions through creating new examples when positive causal relationship between two cause-effect questions holds. The second group is to incorporate logic-specific modules into the inference process (Yang et al., 2017; Dong et al., 2019). For example, Rockt¨aschel and Riedel (2017) target at the problem of knowledge base completion, and use neural unification"
2020.emnlp-main.320,D19-1565,0,0.0399722,"Missing"
2020.emnlp-main.320,D16-1146,0,0.0382263,"Missing"
2020.emnlp-main.320,N19-1423,0,0.173838,"ency between sentencelevel and token-level predictions (§3.2) and textual knowledge from literal definitions of propaganda techniques (§3.3). At last, we describe the training and inference procedures (§3.4). 3.1 Base Model To better exploit the sentence-level information and further benefit token-level prediction, we develop a fine-grained multi-task method as our base model, which makes predictions for 18 propaganda techniques at both sentence level and token level. Inspired by the success of pre-trained language models on various natural language processing downstream tasks, we adopt BERT (Devlin et al., 2019) as the backbone model here. For each input sentence, the sequence is modified as “[CLS]sentence tokens[SEP ]”. Specifically, on top of BERT, we add 19 binary classifiers for finegrained sentence-level predictions, and one 19-way classifier for token-level predictions, where all classifiers are implemented as linear layers. At sentence level, we perform multiple binary classifications and this can further support leveraging declarative knowledge. The last representation of the special token [CLS] which is regarded as a summary of the semantic content of the input, is adopted to perform multipl"
2020.emnlp-main.320,Q15-1027,0,0.0123863,"ta. It is appealing to leverage the advantages from both worlds. In NLP community, the injection of logic to neural network can be generally divided into two groups. Methods in the first group regularize neural network with logicdriven loss functions (Xu et al., 2018; Fischer et al., 2019; Li et al., 2019). For example, Rockt¨aschel et al. (2015) target on the problem of knowledge base completion. After extracting and annotating propositional logical rules about relations in knowledge graph, they ground these rules to facts from knowledge graph and add a differentiable training loss function. Kruszewski et al. (2015) map text to Boolean representations, and derive loss functions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number of grounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural networ"
2020.emnlp-main.320,D19-1405,0,0.0184536,"h first-order logic and natural language. Neural networks have the merits of convenient end-to-end training and good generalization, however, they typically need a lot of training data and are not interpretable. On the other hand, logicbased expert systems are interpretable and require less or no training data. It is appealing to leverage the advantages from both worlds. In NLP community, the injection of logic to neural network can be generally divided into two groups. Methods in the first group regularize neural network with logicdriven loss functions (Xu et al., 2018; Fischer et al., 2019; Li et al., 2019). For example, Rockt¨aschel et al. (2015) target on the problem of knowledge base completion. After extracting and annotating propositional logical rules about relations in knowledge graph, they ground these rules to facts from knowledge graph and add a differentiable training loss function. Kruszewski et al. (2015) map text to Boolean representations, and derive loss functions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number o"
2020.emnlp-main.320,P19-1028,0,0.154388,"ugh creating new examples when positive causal relationship between two cause-effect questions holds. The second group is to incorporate logic-specific modules into the inference process (Yang et al., 2017; Dong et al., 2019). For example, Rockt¨aschel and Riedel (2017) target at the problem of knowledge base completion, and use neural unification modules to recursively construct model similar to the backward chaining algorithm of Prolog. Evans and Grefenstette (2018) develop a differentiable model of forward chaining inference, where weights represent a probability distribution over clauses. Li and Srikumar (2019) inject logic-driven neurons to existing neural networks by measuring the degree of the head being true measured by probabilistic soft logic (Kimmig et al., 2012). Our approach belongs to the first direction, and to the best of knowledge our work is the first one that augments neural network with logical knowledge for propaganda detection. 6 Conclusion In this paper, we propose a fine-grained multitask learning approach, which leverages declarative knowledge to detect propaganda techniques in news articles. Specifically, the declarative knowledge is expressed in both first-order logic and natu"
2020.emnlp-main.320,D17-1317,0,0.0842897,"fluence an audience. 4. Slogan: A brief and striking phrase that may include labeling and stereotyping. Figure 1: Examples of propagandistic texts, and definitions of corresponding propaganda techniques (Bold denotes propagandistic texts). Introduction ∗ “To all Americans tonight, in all our cities and towns, I make this promise: We Will Make America Strong Again. We Will Make America Proud Again. We Will Make America Safe Again. And We Will Make America Great Again 3, 4.” thanks to the recent release of Propaganda Techniques Corpus (Da San Martino et al., 2019). Different from earlier works (Rashkin et al., 2017; Wang, 2017) that mainly study propaganda detection at a coarse-grained level, namely predicting whether a document is propagandistic or not, the fine-grained propaganda detection requires to identify the tokens of particular propaganda techniques in news articles. Da San Martino et al. (2019) propose strong baselines in a multi-task learning manner, which are trained by binary detection of propaganda at sentence level and fine-grained propaganda detection over 18 techniques at token level. Such data-driven methods have the merits of convenient end-to-end learning and strong generalization, h"
2020.emnlp-main.320,N15-1118,0,0.421682,"Missing"
2020.emnlp-main.320,N18-1074,0,0.0442108,"njection of first-order logic into neural networks. We will describe related studies in these two directions. Fake news detection draws growing attention as the spread of misinformation on social media becomes easier and leads to stronger influence. Various types of fake news detection problems are intro3901 duced. For example, there are 4-way classification of news documents (Rashkin et al., 2017), and 6way classification of short statements (Wang, 2017). There are also sentence-level fact checking problems with various genres of evidence, including natural language sentences from Wikipedia (Thorne et al., 2018), semi-structured tables (Chen et al., 2020), and images (Zlatkova et al., 2019; Nakamura et al., 2019). Our work studies propaganda detection, a fine-grained problem that requires tokenlevel prediction over 18 fine-grained propaganda techniques. The release of a large manually annotated dataset (Da San Martino et al., 2019) makes the development of large neural models possible, and also triggers our work, which improves a standard multi-task learning approach by augmenting declarative knowledge expressed in both first-order logic and natural language. Neural networks have the merits of conven"
2020.emnlp-main.320,D18-1215,0,0.0256681,"tions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number of grounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural network, whose predictions are used to teach the student model. Wang and Poon (2018) generalize virtual evidence (Pearl, 2014) to arbitrary potential functions over inputs and outputs, and use deep probabilistic logic to integrate indirection supervision into neural networks. More recently, Asai and Hajishirzi (2020) regularize question answering systems with symmetric consistency and symmetric consistency. The former creates a symmetric question by replacing words with their antonyms in comparison question, while the latter is for causal reasoning questions through creating new examples when positive causal relationship between two cause-effect questions holds. The second gr"
2020.emnlp-main.320,P17-2067,0,0.193758,". Slogan: A brief and striking phrase that may include labeling and stereotyping. Figure 1: Examples of propagandistic texts, and definitions of corresponding propaganda techniques (Bold denotes propagandistic texts). Introduction ∗ “To all Americans tonight, in all our cities and towns, I make this promise: We Will Make America Strong Again. We Will Make America Proud Again. We Will Make America Safe Again. And We Will Make America Great Again 3, 4.” thanks to the recent release of Propaganda Techniques Corpus (Da San Martino et al., 2019). Different from earlier works (Rashkin et al., 2017; Wang, 2017) that mainly study propaganda detection at a coarse-grained level, namely predicting whether a document is propagandistic or not, the fine-grained propaganda detection requires to identify the tokens of particular propaganda techniques in news articles. Da San Martino et al. (2019) propose strong baselines in a multi-task learning manner, which are trained by binary detection of propaganda at sentence level and fine-grained propaganda detection over 18 techniques at token level. Such data-driven methods have the merits of convenient end-to-end learning and strong generalization, however, they"
2020.emnlp-main.320,D19-1216,0,0.0257362,"tudies in these two directions. Fake news detection draws growing attention as the spread of misinformation on social media becomes easier and leads to stronger influence. Various types of fake news detection problems are intro3901 duced. For example, there are 4-way classification of news documents (Rashkin et al., 2017), and 6way classification of short statements (Wang, 2017). There are also sentence-level fact checking problems with various genres of evidence, including natural language sentences from Wikipedia (Thorne et al., 2018), semi-structured tables (Chen et al., 2020), and images (Zlatkova et al., 2019; Nakamura et al., 2019). Our work studies propaganda detection, a fine-grained problem that requires tokenlevel prediction over 18 fine-grained propaganda techniques. The release of a large manually annotated dataset (Da San Martino et al., 2019) makes the development of large neural models possible, and also triggers our work, which improves a standard multi-task learning approach by augmenting declarative knowledge expressed in both first-order logic and natural language. Neural networks have the merits of convenient end-to-end training and good generalization, however, they typically need"
2020.emnlp-main.320,P16-1228,0,\N,Missing
2020.emnlp-main.729,N10-1086,0,0.532672,"ted ones (GTQs). Phrases underlined are the answers to ground-truth questions. (b) Knowledge graph constructed based on the input text shown in top sub-figure. Two colored ellipsoid are two query paths related to two ground truth questions in sub-figure (a) respectively. Nodes in green are covered by ground-truth questions. Figure 1: A sample paragraph from SQuAD with machine generated questions (Zhou et al., 2017) (a), ground truth questions (a) and corresponding knowledge graph (b). Introduction Question Generation (QG) from text aims to automatically construct questions from textual input (Heilman and Smith, 2010). It receives increasing attentions from research communities recently, due to its broad applications in scenarios of dialogue system and educational reading comprehension (Piwek et al., 2007; Duan et al., 2017). It can also help to augment the question set to enhance the performance of question answering systems. ∗ Corresponding author Our code is available at https://github.com/ WangsyGit/PathQG. 1 Current QG systems mainly follow the sequenceto-sequence structure with an encoder for modeling the textual input and a decoder for text generation (Du et al., 2017). These neural-based models hav"
2020.emnlp-main.729,W05-0909,0,0.0144766,"eration. - PathQG is our proposed generation framework consisting of a query representation learner and a query-based question generator. PathQG-V is the variational version of PathQG with an additional posterior query learner. - NQG++ is an oracle model that is aware of all path information contained in the target question and encode them via BIO scheme. It can be treated as the upper bound of NQG+ (pl). We present this result for reference. 5.4 Automatic Evaluation Results For the automatic evaluation, we utlize some widely adopted metrics including BLEU 1-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGEL (Lin and Hovy, 2003). Besides, we also compare results in the semantic content level by using a metric named SPICE (Anderson et al., 2016). It evaluates the similarity of scene graphs generated from candidate and reference questions. Evaluation results on both whole and complex datasets are shown in the Table 2 and 3. We have several findings: - PathQG-V outperforms other models in terms of all metrics on both datasets by a considerable margin. This indicates the effectiveness of our 9071 variational inference framework for modeling the query path for better question generation. -"
2020.emnlp-main.729,N18-1165,0,0.0203742,"tuitively, we divide the task of question generation from a query path into two steps, namely, query representation learning and query-based question generation. We formulate the former step as a sequence labeling problem for identifying the involved facts to form a query. For query-based question generation, an RNN-based generator is used to generate the question word by word. We first train the two modules jointly in an end-to-end fashion (PathQG in Section 3). In order to further enforce the interaction between theses two modules, we employ a variational framework to train the two modules (Chen et al., 2018; Zhang et al., 2018) that treats query representation learning as an inference process from the query path taking the generated question as the target (PathQG-V in Section 4). For model evaluation, we build the experimental environment on top of the benchmark dataset SQuAD (Rajpurkar et al., 2016). In specific, we automatically construct the KG for each piece of input text, and pair ground-truth questions with corresponding query paths from the KG. Experimental results show that our generation model outperforms other state-of-the-art QG models, especially when the questions are more complicat"
2020.emnlp-main.729,D19-1317,0,0.214464,"et al., 2018a,b), has attracted increasing attention in recent years. Most previous studies on textual question generation are rule-based and transform a declarative sentence into an interrogative sentence according to hand-crafted patterns (Heilman and Smith, 2010; Heilman, 2011). With the advance of neural network, Du et al. (2017) propose to apply a seq2seq structure with attention for automatic question generation. As follow-up, Zhou et al. (2017); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (2019) explore to use answer-relevant context to guide question generation. Besides, some studies (Wang et al., 2017; Tang et al., 2017; Wang et al., 2019) take question generation as a subtask, and jointly learn it with other tasks, such as question answering and phrase extraction, which also help to alleviate the uncertainty and improve the generation performance. Another stream of research for question generation is from KG to question. Reddy et al. (2017); Elsahar et al. (2018) explore to generate questions from a single KG triple using text as context information. It is close to our setting, bu"
2020.emnlp-main.729,P17-1123,0,0.422642,"ns from textual input (Heilman and Smith, 2010). It receives increasing attentions from research communities recently, due to its broad applications in scenarios of dialogue system and educational reading comprehension (Piwek et al., 2007; Duan et al., 2017). It can also help to augment the question set to enhance the performance of question answering systems. ∗ Corresponding author Our code is available at https://github.com/ WangsyGit/PathQG. 1 Current QG systems mainly follow the sequenceto-sequence structure with an encoder for modeling the textual input and a decoder for text generation (Du et al., 2017). These neural-based models have shown promising performance, however, they suffer from generating irrelevant and uninformative questions. Figure 1a presents two sample questions generated by a nueral QG model. Q2 contains irrelevant information “Everton Fc”. Although Q1 is correct, it is a safe play without mentioning any 9066 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9066–9075, c November 16–20, 2020. 2020 Association for Computational Linguistics specific information in the input text. One possible reason causing the problem is that curren"
2020.emnlp-main.729,N03-1020,0,0.165225,"Missing"
2020.emnlp-main.729,D17-1090,0,0.0396691,"ground truth questions in sub-figure (a) respectively. Nodes in green are covered by ground-truth questions. Figure 1: A sample paragraph from SQuAD with machine generated questions (Zhou et al., 2017) (a), ground truth questions (a) and corresponding knowledge graph (b). Introduction Question Generation (QG) from text aims to automatically construct questions from textual input (Heilman and Smith, 2010). It receives increasing attentions from research communities recently, due to its broad applications in scenarios of dialogue system and educational reading comprehension (Piwek et al., 2007; Duan et al., 2017). It can also help to augment the question set to enhance the performance of question answering systems. ∗ Corresponding author Our code is available at https://github.com/ WangsyGit/PathQG. 1 Current QG systems mainly follow the sequenceto-sequence structure with an encoder for modeling the textual input and a decoder for text generation (Du et al., 2017). These neural-based models have shown promising performance, however, they suffer from generating irrelevant and uninformative questions. Figure 1a presents two sample questions generated by a nueral QG model. Q2 contains irrelevant informat"
2020.emnlp-main.729,N18-1020,0,0.0182037,"; Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (2019) explore to use answer-relevant context to guide question generation. Besides, some studies (Wang et al., 2017; Tang et al., 2017; Wang et al., 2019) take question generation as a subtask, and jointly learn it with other tasks, such as question answering and phrase extraction, which also help to alleviate the uncertainty and improve the generation performance. Another stream of research for question generation is from KG to question. Reddy et al. (2017); Elsahar et al. (2018) explore to generate questions from a single KG triple using text as context information. It is close to our setting, but we are different in two aspects. First, we propose to form a query path consisting of multiple triples for question generation instead of a single triple. Second, the context we process is where the extracted triples from. This setting is more natural and different from using retrieved text as context as they did. 7 Conclusion and Future Work In this paper, we propose to model facts in the input text as knowledge graph for question generation. We present a novel task of gen"
2020.emnlp-main.729,C18-1150,1,0.710092,"o cases of input texts, paths, answers and questions generated by human, NQG+ and PathQG-V. Phrases underlined are irrelevant to the input text. model PathQG-V is more informative and specific, which consists of information “plymouth” and “late 18th”. In sample 2, our generated question is consistent to the input text while the one from NQG+ contains irrelevant phrase “swazi economy”. 6 Related Work Question Generation, aiming at generating questions from a range of inputs, such as raw text (Heilman and Smith, 2010), structured data (Serban et al., 2016) and images (Mostafazadeh et al., 2016; Fan et al., 2018a,b), has attracted increasing attention in recent years. Most previous studies on textual question generation are rule-based and transform a declarative sentence into an interrogative sentence according to hand-crafted patterns (Heilman and Smith, 2010; Heilman, 2011). With the advance of neural network, Du et al. (2017) propose to apply a seq2seq structure with attention for automatic question generation. As follow-up, Zhou et al. (2017); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (20"
2020.emnlp-main.729,P16-1170,0,0.0988213,"estion by 9072 Figure 6: Two cases of input texts, paths, answers and questions generated by human, NQG+ and PathQG-V. Phrases underlined are irrelevant to the input text. model PathQG-V is more informative and specific, which consists of information “plymouth” and “late 18th”. In sample 2, our generated question is consistent to the input text while the one from NQG+ contains irrelevant phrase “swazi economy”. 6 Related Work Question Generation, aiming at generating questions from a range of inputs, such as raw text (Heilman and Smith, 2010), structured data (Serban et al., 2016) and images (Mostafazadeh et al., 2016; Fan et al., 2018a,b), has attracted increasing attention in recent years. Most previous studies on textual question generation are rule-based and transform a declarative sentence into an interrogative sentence according to hand-crafted patterns (Heilman and Smith, 2010; Heilman, 2011). With the advance of neural network, Du et al. (2017) propose to apply a seq2seq structure with attention for automatic question generation. As follow-up, Zhou et al. (2017); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018)"
2020.emnlp-main.729,P02-1040,0,0.109663,"sing BIO scheme for question generation. - PathQG is our proposed generation framework consisting of a query representation learner and a query-based question generator. PathQG-V is the variational version of PathQG with an additional posterior query learner. - NQG++ is an oracle model that is aware of all path information contained in the target question and encode them via BIO scheme. It can be treated as the upper bound of NQG+ (pl). We present this result for reference. 5.4 Automatic Evaluation Results For the automatic evaluation, we utlize some widely adopted metrics including BLEU 1-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGEL (Lin and Hovy, 2003). Besides, we also compare results in the semantic content level by using a metric named SPICE (Anderson et al., 2016). It evaluates the similarity of scene graphs generated from candidate and reference questions. Evaluation results on both whole and complex datasets are shown in the Table 2 and 3. We have several findings: - PathQG-V outperforms other models in terms of all metrics on both datasets by a considerable margin. This indicates the effectiveness of our 9071 variational inference framework for modeling the query path"
2020.emnlp-main.729,D14-1162,0,0.0838358,"esponding query paths from the KG for ground truth questions. In practice, a path can be determined by a start node and an end node. We thus use answer entity of the question as the start node and use the entity identified in the question as the end node. If the question contains multiple entities, we take the one farthest to the start node in the KG as the end node. We ignore the edge directions to simplify the modeling of query path. 5.2 Implementation Details We construct different vocabularies for input texts and questions respectively by keeping words which appear more than twice. Glove (Pennington et al., 2014) is used to initialize word embedding with dimension 300 and the embedding for BIO tag is randomly initialized of size 20. The size of hidden units in LSTM cell in all encoders is 300 while the size of the generation decoder is 1200. The hyperparameters to balance weights of losses are chosen as λ = 0.5 and β = 0.1. We evaluate our model on validation set to choose parameters. During test 2 The constructed KGs and complex question index can be downloaded from https://www.disc.fudan.edu. cn/data/fudan_pathqg_data.zip. 9070 Model NQG+ AFPA ASs2s NQG+ (pl) PathQG PathQG-V NQG++ BLEU 1 49.89 50.05"
2020.emnlp-main.729,D16-1264,0,0.0492678,"stion generation, an RNN-based generator is used to generate the question word by word. We first train the two modules jointly in an end-to-end fashion (PathQG in Section 3). In order to further enforce the interaction between theses two modules, we employ a variational framework to train the two modules (Chen et al., 2018; Zhang et al., 2018) that treats query representation learning as an inference process from the query path taking the generated question as the target (PathQG-V in Section 4). For model evaluation, we build the experimental environment on top of the benchmark dataset SQuAD (Rajpurkar et al., 2016). In specific, we automatically construct the KG for each piece of input text, and pair ground-truth questions with corresponding query paths from the KG. Experimental results show that our generation model outperforms other state-of-the-art QG models, especially when the questions are more complicated. Human evaluation also proves the effectiveness of our model in terms of both relevance and informativeness. 2 Task Definition We first introduce some notations in our task: - x = (x1 , ..., xn ): an input text with n tokens, where xi is the ith token; - G: a knowledge graph constructed from x,"
2020.emnlp-main.729,E17-1036,0,0.0170504,"7); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (2019) explore to use answer-relevant context to guide question generation. Besides, some studies (Wang et al., 2017; Tang et al., 2017; Wang et al., 2019) take question generation as a subtask, and jointly learn it with other tasks, such as question answering and phrase extraction, which also help to alleviate the uncertainty and improve the generation performance. Another stream of research for question generation is from KG to question. Reddy et al. (2017); Elsahar et al. (2018) explore to generate questions from a single KG triple using text as context information. It is close to our setting, but we are different in two aspects. First, we propose to form a query path consisting of multiple triples for question generation instead of a single triple. Second, the context we process is where the extracted triples from. This setting is more natural and different from using retrieved text as context as they did. 7 Conclusion and Future Work In this paper, we propose to model facts in the input text as knowledge graph for question generation. We pres"
2020.emnlp-main.729,W15-2812,0,0.0136549,"tructed by other methods. Experiments Experimental Dataset Our experiments are conducted on SQuAD (Rajpurkar et al., 2016) consisting of 61,623 sentences. Each sentence is annotated with several questions together with their answers extracted from the input text. We build our experimental dataset on top of SQuAD. We construct knowledge graph for each sentence automatically and identify query paths for ground truth questions for evaluation. The resulted dataset consists of 89,976 tuples (input sentence x, query path s, ground truth question y). KG construction We employ the scene graph parser (Schuster et al., 2015) for KG construction from a textual description. It identifies entities and their relationships from a text and build a scene graph. It turns out that the generated scene graph usually misses some key information in the text, thus we employ the part-of-speech tagger to extract verb phrases between entities to further enrich relationship labels. The extended scene graph is used as the knowledge graph for the input text. The average quantities of entities and facts in each KG are dataset train valid test complex whole 12,828 68,704 1,895 10,313 1,855 10,959 len. of ques. 14.7 13.3 Table 1: Stati"
2020.emnlp-main.729,P16-1056,0,0.040077,"Missing"
2020.emnlp-main.729,N18-2090,0,0.09269,"zadeh et al., 2016; Fan et al., 2018a,b), has attracted increasing attention in recent years. Most previous studies on textual question generation are rule-based and transform a declarative sentence into an interrogative sentence according to hand-crafted patterns (Heilman and Smith, 2010; Heilman, 2011). With the advance of neural network, Du et al. (2017) propose to apply a seq2seq structure with attention for automatic question generation. As follow-up, Zhou et al. (2017); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (2019) explore to use answer-relevant context to guide question generation. Besides, some studies (Wang et al., 2017; Tang et al., 2017; Wang et al., 2019) take question generation as a subtask, and jointly learn it with other tasks, such as question answering and phrase extraction, which also help to alleviate the uncertainty and improve the generation performance. Another stream of research for question generation is from KG to question. Reddy et al. (2017); Elsahar et al. (2018) explore to generate questions from a single KG triple using text as context information. It is clo"
2020.findings-emnlp.422,D15-1106,0,0.0144959,"1) with a dimension of 300 and updated during training. The dimension of the hidden units for GRU (Chung et al., 2014) and GCN is 300. We initialize the parameters according to a uniform distribution with the Xavier scheme (Kumar, 2017), and the dropout rate is set to 0.5. The Adam (Kingma and Ba, 2014) method with a learning rate of 1e-3 is used for training. Baseline Methods. To evaluate the effectiveness of our proposed model, we apply the advanced baselines in two categories for comparison: (1) TF-IDF; (2) LexRank (Erkan and Radev, 2004); (3) Seq2Seq (Sutskever et al., 2014); (4) HRNNLM (Lin et al., 2015); (5) Transformer (Vaswani et al., 2017). The former two are extractive models which extract words from the gene text as the term name, and the latter three are generative models which generate words from the vocabulary space as the term name. 4.2 Experimental Results The experimental results are shown in Table 2. It is observed that the generative models perform better than the extractive models by incorporating the language probability into generation, which makes the generated term name more coherent. Whereas, the extractive models usually extract keywords independently, which are hard to f"
2020.findings-emnlp.422,P02-1040,0,0.121521,"Missing"
2020.findings-emnlp.422,P16-1154,0,0.0331563,"yer of GCN. 3.2 Graph Attention based Decoder Motivated by the effectiveness of the attention mechanism for generation (Bahdanau et al., 2014), we adopt a graph attention based decoder to generate the term name. The attentive word node representation by GCN is utilized and formulated as: n X at = αj wj0 (2) j=1 αj = sof tmax(v T tanh(Wa [ht−1 ; w0 j ])) where ht−1 is the previous hidden state, w0 j is the word node representation by GCN, v is a parameter vector, and Wa is a parameter matrix. Given the word overlaps between the gene text and term name, we utilize the copy mechanism in CopyNet (Gu et al., 2016) for decoding, making it possible to generate the word from either the vocabulary of the training set or the current gene text. The initial hidden state h0 is the term node representation (i.e., t0 ) obtained by GCN, and the hidden state is updated as: 0 ht = f ([ht−1 ; wt−1 ; at ; wSR ]) (3) where f is the RNN function, wt−1 is the word 0 embedding of the previous generated word, wSR is a selective read (SR) vector in CopyNet. When the previous generated word appears in the gene text, the next word will also probably come from it, and 0 thus wSR is the previous word node representation; other"
2021.acl-demo.41,D18-2029,0,0.0379272,"Missing"
2021.acl-demo.41,N19-1423,0,0.207492,"re rated on a 1-5 scale (5 denotes the best). in a general way, TextFlint supports generating massive and comprehensive transformed samples with just one command. By default, TextFlint performs all single transformations on the original dataset to form the corresponding transformed datasets, and the performance of the target models is tested on these datasets. The evaluation report provides a comparative view of model performance on datasets before and after certain types of transformations, which supports model weakness analyses and guides particular improvements. For example, take BERT base(Devlin et al., 2019) as the target model to verify its robustness on the CONLL2003 dataset(Tjong Kim Sang and De Meulder, 2003), whose robustness report is shown in Figure 5. The performance of BERT base decreases significantly in some morphology transformations, such as OCR, Keyboard, Typos, and Spelling Error. To combat these errors of input texts and improve the robustness of the model, we suggest that placing a word correction model(Pruthi et al., 2019) before BERT would be beneficial. 0.8 Case 2: Customized Evaluation For users who want to test model performance on specific aspects, they demand a customized"
2021.acl-demo.41,D18-1380,0,0.107351,"Missing"
2021.acl-demo.41,2021.naacl-demos.6,0,0.061782,"Missing"
2021.acl-demo.41,D14-1181,0,0.00437317,"Missing"
2021.acl-demo.41,2020.emnlp-main.500,1,0.864395,"like to teach kids in the kindergarten. The storm destroyed many houses in the village. ✘ Figure 1: Examples of three main generation functions. The transformation example is from ABSA (Aspectbased Sentiment Analysis) task, where the italic bold RevTgt (short for reverse target) denotes task-specific transformations, and the bold Typos denotes universal transformation. Introduction The detection of model robustness has been attracting increasing attention in recent years, given that deep neural networks (DNNs) of high accuracy can still be vulnerable to carefully crafted adversarial examples (Li et al., 2020), distribution shift (Miller et al., 2020), data transformation (Xing et al., 2020), and shortcut learning (Geirhos et al., 2020). Existing approaches to textual robustness evaluation focus on slightly modifying the input data, which maintains the original meaning and results in a different prediction. However, these methods often concentrate on either universal or task-specific generalization capabilities, which is difficult to comprehensively evaluate. In response to the shortcomings of recent works, we introduce TextFlint, a unified, multilingual, and analyzable robustness evaluation toolki"
2021.acl-demo.41,P18-1087,0,0.0272118,"Missing"
2021.acl-demo.41,P02-1040,0,0.116092,"are implemented based on TextAttack (Morris et al., 2020). Validator It is crucial to verify the quality of the samples generated by Transformation and AttackRecipe. TextFlint provides several metrics to evaluate the quality of the generated text, including (1) language model perplexity calculated based on the GPT2 model (Radford et al., 2019), (2) word replacement ratio in generated text compared with its original text, (3) edit distance between original text and generated text, (4) semantic 349 similarity calculated based on Universal Sentence Encoder (Cer et al., 2018), and (5) BLEU score (Papineni et al., 2002). 2.3 Reporter Layer Generation Layer yields three types of adversarial samples and verifies the robustness of the target model. Based on the evaluation results from Generation Layer, Report Layer aims to provide users with a standard analysis report from syntax, morphology, pragmatics, and paradigmatic relation aspects. The running process of Report Layer can be regarded as a pipeline from Analyzer to ReportGenerator. 3 Figure 4: Screenshot of TextFlint’s web interface running Ocr transformation for ABSA task. Usage Using TextFlint to verify the robustness of a specific model is as simple as"
2021.acl-demo.41,P19-1561,0,0.0287018,"Missing"
2021.acl-demo.41,2020.acl-main.442,0,0.114738,"methods equipped with pre-training LMs showed better performance than other models on the task-specific transformations, e.g., AddDiff , where the accuracy score of BERT-Aspect dropped from 90.32 to merely 81.58. All the evaluation results and comprehensive robustness analysis are available at textflint.io. 5 Related Work Our work is related to many existing open-source tools and works in different areas. Robustness Evaluation Many tools include evaluation methods for robustness, including NLPAug (Ma, 2019), Errudite (Wu et al., 2019), AllenNLP Interpret (Wallace et al., 2019), and Checklist (Ribeiro et al., 2020), which are only applicable to limited parts of robustness evaluations, while TextFlint supports comprehensive evaluation methods, e.g., subpopulation, adversarial attacks, transformations, and so on. Besides the common transformation methods like synonym substitution and typos, various task-specific transformations are tailored for each of the 12 NLP tasks, which is peculiar to TextFlint. Moreover, we are the first to provide linguistic support for the transformations, the designs for which were inspired by linguistics and have been proved plausible and readable by human annotators. Several t"
2021.acl-demo.41,C16-1311,0,0.0414402,"Missing"
2021.acl-demo.41,D16-1021,0,0.0913955,"Missing"
2021.acl-demo.41,D19-3002,0,0.0392234,"Missing"
2021.acl-demo.41,D16-1058,0,0.0373143,"Missing"
2021.acl-demo.41,P19-1073,0,0.133542,"underlying patterns about model robustness. As for the ABSA task (Table 2), methods equipped with pre-training LMs showed better performance than other models on the task-specific transformations, e.g., AddDiff , where the accuracy score of BERT-Aspect dropped from 90.32 to merely 81.58. All the evaluation results and comprehensive robustness analysis are available at textflint.io. 5 Related Work Our work is related to many existing open-source tools and works in different areas. Robustness Evaluation Many tools include evaluation methods for robustness, including NLPAug (Ma, 2019), Errudite (Wu et al., 2019), AllenNLP Interpret (Wallace et al., 2019), and Checklist (Ribeiro et al., 2020), which are only applicable to limited parts of robustness evaluations, while TextFlint supports comprehensive evaluation methods, e.g., subpopulation, adversarial attacks, transformations, and so on. Besides the common transformation methods like synonym substitution and typos, various task-specific transformations are tailored for each of the 12 NLP tasks, which is peculiar to TextFlint. Moreover, we are the first to provide linguistic support for the transformations, the designs for which were inspired by lingu"
2021.acl-demo.41,2020.emnlp-main.292,1,0.893206,"Missing"
2021.acl-long.455,P19-1635,0,0.0200443,"l. (2020b) connects each 5866 number in the problem with nearby nouns to enrich the problem representations. However, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties prediction mechani"
2021.acl-long.455,C10-3014,0,0.00817697,"est set, respectively. We report answer accuracy as the main evaluation metrics of the math word problem solving task. 3.2 Implementation Details In this paper, we truncate the problem to a max sequence length of 150, and the expression to a max sequence length of 50. We select 4,000 words that appear most frequently in the training set of each dataset as the vocabulary, and replace the remaining words with a special token UNK. We initialize the word embedding with the pretrained 300-dimension word vectors3 . The problem encoder used two external knowledge bases: Cilin (Mei, 1985) and Hownet (Dong et al., 2010). The number of heads T in GAT is 8. The hidden size is 512 and the batch size is 64. We use the Adam optimizer (Kingma and Ba, 2014) to optimize the models an the learning rate is 0.001. We compute the final loss function with β1 , β2 , β3 of 0.5. Dropout (Srivastava et al., 2014) is set to 0.5. Models are trained in 80 epoches for the Math23K dataset and 50 epoches for the Ape210K dataset. During testing, the beam size is set to 5. Once all internal nodes in the expression tree have two child nodes, the decoder stops generating the next word. The hyper-parameters are tuned on the valid set."
2021.acl-long.455,2020.findings-emnlp.262,0,0.0207101,"eplace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties prediction mechanism. 5 Conclusion In this study, we proposed a novel approach called NumS2T, that better captures numerical value information and utilizes numerical properties. In this model, we use a digi"
2021.acl-long.455,P18-1039,0,0.0158951,"te-of-the-art models. 1 1 Figure 1: Example of a math word problem. The same problem with different numerical values may correspond to different math expressions. Without numerical value information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate a pre-order traversal"
2021.acl-long.455,P19-1619,0,0.0121074,"h word problem solving tasks (Ling et al., 2017; Wang et al., 2017b, 2018a). To better utilize expression structure information, recent studies have used Seq2Tree models (Liu et al., 2019; Zhang et al., 2020a). Xie and Sun (2019) proposed a tree structured decoder that uses a goal-driven approach to generate expression trees. Wu et al. (2020) proposed a knowledge-aware Seq2Tree model with a state aggregation mechanism that incorporates common-sense knowledge from external knowledge bases. Recently, several methods have attempted to use the contextual information of the numbers in the problem. Li et al. (2019) propose a group attention mechanism to extract quantity-related features and quantitypair features. Zhang et al. (2020b) connects each 5866 number in the problem with nearby nouns to enrich the problem representations. However, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations"
2021.acl-long.455,P17-1015,0,0.107963,"e than existing state-of-the-art models. 1 1 Figure 1: Example of a math word problem. The same problem with different numerical values may correspond to different math expressions. Without numerical value information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate"
2021.acl-long.455,D19-1241,0,0.0365,"Missing"
2021.acl-long.455,P16-1202,0,0.0198685,"ieves better performance than existing state-of-the-art models. 1 1 Figure 1: Example of a math word problem. The same problem with different numerical values may correspond to different math expressions. Without numerical value information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary"
2021.acl-long.455,P19-1329,0,0.0250924,"eatures. Zhang et al. (2020b) connects each 5866 number in the problem with nearby nouns to enrich the problem representations. However, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties"
2021.acl-long.455,D15-1202,0,0.0210073,"e that our model achieves better performance than existing state-of-the-art models. 1 1 Figure 1: Example of a math word problem. The same problem with different numerical values may correspond to different math expressions. Without numerical value information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target ex"
2021.acl-long.455,P18-1196,0,0.0571618,"Missing"
2021.acl-long.455,D19-1534,0,0.018856,"each 5866 number in the problem with nearby nouns to enrich the problem representations. However, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties prediction mechanism. 5 Conclusion In thi"
2021.acl-long.455,D18-1132,0,0.0325612,"Missing"
2021.acl-long.455,P17-1018,0,0.225624,"lue information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate a pre-order traversal sequence of this expression tree based on the parent and sibling nodes of each node. Although promising results have been achieved, previous methods rarely take numerical values in"
2021.acl-long.455,D17-1088,0,0.0519152,"Missing"
2021.acl-long.455,2020.emnlp-main.579,1,0.866795,"growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate a pre-order traversal sequence of this expression tree based on the parent and sibling nodes of each node. Although promising results have been achieved, previous methods rarely take numerical values into consideration, despite the fact that in math word problem solving, numerical values provide vital information. As an infinite number of numerals can appear in math word problems, it is impossible to list them all in the vocabulary. Previous methods replace all the numbers in the problems with number symbols (e.g.,"
2021.acl-long.455,2020.acl-main.362,0,0.0258773,"nerates the sub-expression “80-52”. However, this problem is about the fares that have already been sold rather than how many tickets are left. With numerical properties, NumS2T is able to realize that 80 is not related to the target expression and should not appear in the generated result. 4 Related Work Math Word Problem Solving: In recent years, Seq2Seq (Sutskever et al., 2014) has been widely used in math word problem solving tasks (Ling et al., 2017; Wang et al., 2017b, 2018a). To better utilize expression structure information, recent studies have used Seq2Tree models (Liu et al., 2019; Zhang et al., 2020a). Xie and Sun (2019) proposed a tree structured decoder that uses a goal-driven approach to generate expression trees. Wu et al. (2020) proposed a knowledge-aware Seq2Tree model with a state aggregation mechanism that incorporates common-sense knowledge from external knowledge bases. Recently, several methods have attempted to use the contextual information of the numbers in the problem. Li et al. (2019) propose a group attention mechanism to extract quantity-related features and quantitypair features. Zhang et al. (2020b) connects each 5866 number in the problem with nearby nouns to enrich"
2021.acl-long.99,N15-1171,0,0.018176,"Although roll call data is the major resource for legislator behavior modeling, it has two limitations. Firstly, it fails to uncover detailed opinions of legislators towards legislative issues. Therefore, we have no clue about the motivation behind their voting. Secondly, it is unable to model the behavior of newly-elected legislators because their historical voting records are not available (i.e., coldstart problem). Meanwhile, researchers explore to use public statements to characterize the ideology of legislators with the guidance of framing theory (Entman, 1993; Chong and Druckman, 2007; Baumer et al., 2015; Vafa et al., 2020). Vafa et al. (2020) propose a text-based ideal point model to analyze tweets of legislators independent of roll call data. Experiment results show some correlations between distributions of ideal points learned 1236 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1236–1246 August 1–6, 2021. ©2021 Association for Computational Linguistics from legislative data and public statements. However, they treat the two resources separately and fail to uncover dee"
2021.acl-long.99,2020.acl-main.476,0,0.0273936,"oth legislators and legislation in the same space, and voting behavior is characterized as the distance between them. However, this simple spatial model fails to predict votes on new legislation. Text-based models have emerged to address this issue. Gerrish and Blei (2011, 2012); Gu et al. (2014); Nguyen et al. (2015) extended ideal point model with latent topics and issue-adjusted methods. Some embedding methods (Kraft et al., 2016) also promote learning of legislators. More recently, external context information including party, sponsor and donors (Kornilova et al., 2018; Yang et al., 2020; Davoodi et al., 2020) have been introduced to better describe the legislative process. Since votes are not the only way to express political preferences, other sources of data including speech and knowledge graph (Budhwar et al., 2018; Gentzkow et al., 2019; Patil et al., 2019; Vafa et al., 2020) have been applied to estimate ideology. Although previous studies (Bruns and Highfield, 2013; Golbeck and Hansen, 2014; Barber´a, 2015; Peng et al., 2016; Wong et al., 2016; Boutyline and Willer, 2017; Johnson et al., 2017) have incorporated social network of following or retweeting on Twitter to learn legislators, fine-g"
2021.acl-long.99,N19-1423,0,0.00883048,"nodes and six types of relations with two categories (relations between homogeneous nodes and relations between heterogeneous nodes). We will introduce the structure of the graph in this subsection. 3.1.1 Initialization of Nodes Legislator Nodes We follow Yang et al. (2020) to map each legislator to a continuous low-dimension vector, utilizing information of member ID, state and party. The legislator representation is Xm = eID ⊕ eP arty ⊕ eState Legislation Nodes For legislation, we pay attention to title and description and represent each legislation by sentence embedding generated by BERT (Devlin et al., 2019). Thus, the legislation representation is Xl = BERT (title + description) Hashtag Nodes To represent a hashtag, we randomly choose K tweets with the tag and use BERT to get sentence embedding of each tweet text. After that, we take the average of these vectors, Xt = Avg(BERT (tweeti )) i = 1, 2, ...K 3.1.2 Relations between Homogeneous Nodes R1: Co-sponsorship of Legislators Each legislation is initialized by a sponsor and several cosponsors. Previous study (Yang et al., 2020) has proved the effectiveness of modeling cosponsorship in legislator representation learning. Obviously, more legislat"
2021.acl-long.99,P17-1069,0,0.0253998,"Missing"
2021.acl-long.99,P18-2081,0,0.0746966,"ur model with some state-of-the-art approaches. - majority is a baseline which assumes all legislators vote yea. - ideal-point-wf (Gerrish and Blei, 2011): a regression model that takes the word frequency of legislation text as features. The training paradigm follows the traditional ideal point model. Thus, it can only predict on legislators present in the training data. - ideal-point-tfidf : similar to ideal-point-wf, it uses TFIDF of legislation text as features instead. - ideal-vector (Kraft et al., 2016): it learns multidimensional ideal vectors for legislators based on bill texts. - CNN (Kornilova et al., 2018): it uses CNN to encode legislation. - CNN+meta (Kornilova et al., 2018): on the basis of CNN, it adds percentage of sponsors of different parties as bill’s authorship information. - LSTM+GCN (Yang et al., 2020): it uses LSTM to encode legislation and applies a GCN to update representations of legislators. - Vote: the single task of roll call vote in our framework. - Ours: our framework. 4.2 Overall Performance We report the average accuracy of all experiment sets following Kornilova et al. (2018); Yang et al. (2020). Besides, macro F1 score is also provided for more information. Table 1 shows"
2021.acl-long.99,D16-1221,0,0.401345,"tional graph neural networks to learn the representation of legislators with the guidance of historical records of their voting and hashtag usage. Experiment results indicate that our model yields significant improvements for the task of roll call vote prediction. Further analysis further demonstrates that legislator representation we learned captures nuances in statements. 1 Figure 1: An illustration of correspondence of vote behavior and public statements on Twitter. Supporters of the abortion-banning legislation frequently mention the tag life while opponents focus on choice. et al., 2014; Kraft et al., 2016) and report positive results for roll call vote prediction. Introduction Modeling the behavior of legislators is one of the most important topics of quantitative political science. Existing researches largely rely on roll call data, i.e. historical voting records, to estimate the political preference of legislators. The most widely used approach for roll call data analysis is ideal point model (Clinton et al., 2004) that represents legislators and legislation as points in a one-dimension latent space. Researchers enhance ideal point model by incorporating textual information of legislation (Ge"
2021.acl-long.99,P15-1139,0,0.0101923,"ccurately related to a specific bill in an automatic and complete way, we will explore the frequency of inconsistency in the future. 6 Related Work Ideal point estimation has become a mainstream approach to model ideology of legislators. Classical ideal point model (Clinton et al., 2004) represents both legislators and legislation in the same space, and voting behavior is characterized as the distance between them. However, this simple spatial model fails to predict votes on new legislation. Text-based models have emerged to address this issue. Gerrish and Blei (2011, 2012); Gu et al. (2014); Nguyen et al. (2015) extended ideal point model with latent topics and issue-adjusted methods. Some embedding methods (Kraft et al., 2016) also promote learning of legislators. More recently, external context information including party, sponsor and donors (Kornilova et al., 2018; Yang et al., 2020; Davoodi et al., 2020) have been introduced to better describe the legislative process. Since votes are not the only way to express political preferences, other sources of data including speech and knowledge graph (Budhwar et al., 2018; Gentzkow et al., 2019; Patil et al., 2019; Vafa et al., 2020) have been applied to"
2021.acl-long.99,K19-1053,0,0.0400706,"nd Blei (2011, 2012); Gu et al. (2014); Nguyen et al. (2015) extended ideal point model with latent topics and issue-adjusted methods. Some embedding methods (Kraft et al., 2016) also promote learning of legislators. More recently, external context information including party, sponsor and donors (Kornilova et al., 2018; Yang et al., 2020; Davoodi et al., 2020) have been introduced to better describe the legislative process. Since votes are not the only way to express political preferences, other sources of data including speech and knowledge graph (Budhwar et al., 2018; Gentzkow et al., 2019; Patil et al., 2019; Vafa et al., 2020) have been applied to estimate ideology. Although previous studies (Bruns and Highfield, 2013; Golbeck and Hansen, 2014; Barber´a, 2015; Peng et al., 2016; Wong et al., 2016; Boutyline and Willer, 2017; Johnson et al., 2017) have incorporated social network of following or retweeting on Twitter to learn legislators, fine-grained attitudes of legislators remain unknown since the texts themselves have not been mined. Until recently, Preot¸iuc-Pietro et al. (2017) started to analyze linguistic differences between ideologically different groups using a broad range of handcrafte"
2021.acl-long.99,P17-1068,0,0.0606298,"Missing"
2021.acl-long.99,2020.emnlp-main.46,0,0.224288,"crease in the Federal minimum wage”, the vote-based model predicts that Senator Harry Reid will vote nay, which is also the ground truth. But our model wrongly predicts that he will vote yea. We probe into his tweets and find that he 1243 used #raisethewage frequently to call for raise in minimum wage, as those who support the bill. On the one hand, hashtags may have difficulty capturing more fine-grained decisions, which can be influenced by various factors; on the other hand, legislators may behave differently from what they say, since they may make certain statements to get public support (Spell et al., 2020). When legislators do not accord their words to deed, our model may be misled by legislators’ statements. As it’s difficult to find hashtags directly and accurately related to a specific bill in an automatic and complete way, we will explore the frequency of inconsistency in the future. 6 Related Work Ideal point estimation has become a mainstream approach to model ideology of legislators. Classical ideal point model (Clinton et al., 2004) represents both legislators and legislation in the same space, and voting behavior is characterized as the distance between them. However, this simple spati"
2021.acl-long.99,2020.acl-main.475,0,0.0676393,"ata is the major resource for legislator behavior modeling, it has two limitations. Firstly, it fails to uncover detailed opinions of legislators towards legislative issues. Therefore, we have no clue about the motivation behind their voting. Secondly, it is unable to model the behavior of newly-elected legislators because their historical voting records are not available (i.e., coldstart problem). Meanwhile, researchers explore to use public statements to characterize the ideology of legislators with the guidance of framing theory (Entman, 1993; Chong and Druckman, 2007; Baumer et al., 2015; Vafa et al., 2020). Vafa et al. (2020) propose a text-based ideal point model to analyze tweets of legislators independent of roll call data. Experiment results show some correlations between distributions of ideal points learned 1236 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1236–1246 August 1–6, 2021. ©2021 Association for Computational Linguistics from legislative data and public statements. However, they treat the two resources separately and fail to uncover deep relationships of b"
2021.ccl-1.74,C12-1047,0,0.0900021,"Missing"
2021.ccl-1.74,P18-2023,0,0.0357094,"Missing"
2021.emnlp-main.17,P14-1038,0,0.223441,"ns. Conventionally, Named Entity Recognition (NER) and Relation Extraction (RE) are performed in a pipelined manner (Zelenko et al., 2002; Chan and Roth, 2011). These approaches are flawed in that they do not consider the intimate connection between NER and RE. Also, error propagation is another drawback of pipeline methods. In order to conquer these issues, joint extracting entity and relation is proposed and demonstrates stronger performance on both tasks. In early work, joint methods mainly rely on elaborate feature engineering to establish interaction between NER and RE (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014). Recently, end-to-end neural network has shown to be successful in extracting relational triples (Zeng et al., 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Shen et al., 2021) and has since become the mainstream of joint 1 Introduction entity and relation extraction. According to their differences in encoding taskJoint entity and relation extraction intend to specific features, most of the existing methods simultaneously extract entity and relation facts can be divided into two categories: sequential in the given text to form relational triples as encoding and pa"
2021.emnlp-main.17,P19-1129,0,0.0348847,"Missing"
2021.emnlp-main.17,D18-1360,0,0.0536971,"Missing"
2021.emnlp-main.17,P16-1105,0,0.050881,"Missing"
2021.emnlp-main.17,D14-1200,0,0.170678,"y, Named Entity Recognition (NER) and Relation Extraction (RE) are performed in a pipelined manner (Zelenko et al., 2002; Chan and Roth, 2011). These approaches are flawed in that they do not consider the intimate connection between NER and RE. Also, error propagation is another drawback of pipeline methods. In order to conquer these issues, joint extracting entity and relation is proposed and demonstrates stronger performance on both tasks. In early work, joint methods mainly rely on elaborate feature engineering to establish interaction between NER and RE (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014). Recently, end-to-end neural network has shown to be successful in extracting relational triples (Zeng et al., 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Shen et al., 2021) and has since become the mainstream of joint 1 Introduction entity and relation extraction. According to their differences in encoding taskJoint entity and relation extraction intend to specific features, most of the existing methods simultaneously extract entity and relation facts can be divided into two categories: sequential in the given text to form relational triples as encoding and parallel encoding. In sequ"
2021.emnlp-main.17,N13-1008,0,0.0406422,"of joint 1 Introduction entity and relation extraction. According to their differences in encoding taskJoint entity and relation extraction intend to specific features, most of the existing methods simultaneously extract entity and relation facts can be divided into two categories: sequential in the given text to form relational triples as encoding and parallel encoding. In sequential (s, r, o). The extracted information provides a encoding, task-specific features are generated supplement to many studies, such as knowledge sequentially, which means features extracted first graph construction (Riedel et al., 2013), question are not affected by those that are extracted later. ∗ Corresponding author. Zeng et al. (2018) and Wei et al. (2020) are 185 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 185–197 c November 7–11, 2021. 2021 Association for Computational Linguistics typical examples of this category. Their methods 2. We conduct extensive experiments on six extract features for different tasks in a predefined datasets. The main results show that our order. In parallel encoding, task-specific features method is superior to other baseline apare generated i"
2021.emnlp-main.17,2021.acl-demo.41,1,0.598002,"Ori → Aug Decline 82.9→64.2 18.7 87.4→79.0 8.4 88.7→74.6 14.1 84.7→51.5 33.2 84.6→81.3 3.3 89.0→80.4 8.6 SwapLonger Average Ori → Aug Decline Decline 82.9→67.7 15.2 16.6 87.4→82.1 5.3 11.6 88.7→78.5 10.2 14.6 84.7→31.1 53.6 28.1 84.6→73.1 11.5 11.9 89.0→84.3 4.7 5.1 Table 4: Robustness test of NER against input perturbation in ACE05, baseline results and test files are copied from https://www.textflint.io/ 6.2 Robustness Test on Named Entity Recognition We use robustness test to evaluate our model under adverse circumstances. In this case, we use the domain transformation methods of NER from (Wang et al., 2021). The compared baselines are all relation-free models, including BiLSTMCRF (Huang et al., 2015), BERT (Devlin et al., 2019), TENER (Yan et al., 2019) and FlairEmbeddings (Akbik et al., 2019). Descriptions of the transformation methods can be found in Appendix D From table 4, we observe that our model is mostly more resilient against input perturbations compared to other baselines, especially in the category of CrossCategory, which is probably attributed to the fact that relation signals used in our training impose type constraints on entities, thus inference of entity types is less affected by"
2021.emnlp-main.17,2020.emnlp-main.132,0,0.156794,"nd Sasaki, 2014) to handle each relation separately. specific ones, are formed through concerted efforts Task interaction modeling, however, has not of entity and relation gates, allowing for interaction been well handled by most of the previous between the formation of entity and relation features determined by these partitions. Second, work. In some of the previous approaches, Task interaction is achieved with entity and relation the shared partition, which represents information prediction sharing the same features (Tran and useful to both task, is equally accessible to the Kavuluru, 2019; Wang et al., 2020b). This could formation of both task-specific features, ensuring be problematic as information about entity and balanced two-way interaction. The contributions relation could sometimes be contradictory. Also, of our work are summarized below: 1. We propose partition filter network, a frame- as models that use sequential encoding (Bekoulis work designed specifically for joint encoding. et al., 2018b; Eberts and Ulges, 2019; Wei et al., This method is capable of encoding task- 2020) or parallel encoding (Fu et al., 2019) lack specific features and guarantees proper two- proper two-way interacti"
2021.emnlp-main.17,2020.coling-main.138,0,0.0424825,"Missing"
2021.emnlp-main.17,2020.acl-main.136,0,0.494739,"xtraction intend to specific features, most of the existing methods simultaneously extract entity and relation facts can be divided into two categories: sequential in the given text to form relational triples as encoding and parallel encoding. In sequential (s, r, o). The extracted information provides a encoding, task-specific features are generated supplement to many studies, such as knowledge sequentially, which means features extracted first graph construction (Riedel et al., 2013), question are not affected by those that are extracted later. ∗ Corresponding author. Zeng et al. (2018) and Wei et al. (2020) are 185 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 185–197 c November 7–11, 2021. 2021 Association for Computational Linguistics typical examples of this category. Their methods 2. We conduct extensive experiments on six extract features for different tasks in a predefined datasets. The main results show that our order. In parallel encoding, task-specific features method is superior to other baseline apare generated independently using shared input. proaches, and the ablation study provides inCompared with sequential encoding, models build si"
2021.emnlp-main.17,C10-2160,0,0.0484055,"two task partitions. Conventionally, Named Entity Recognition (NER) and Relation Extraction (RE) are performed in a pipelined manner (Zelenko et al., 2002; Chan and Roth, 2011). These approaches are flawed in that they do not consider the intimate connection between NER and RE. Also, error propagation is another drawback of pipeline methods. In order to conquer these issues, joint extracting entity and relation is proposed and demonstrates stronger performance on both tasks. In early work, joint methods mainly rely on elaborate feature engineering to establish interaction between NER and RE (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014). Recently, end-to-end neural network has shown to be successful in extracting relational triples (Zeng et al., 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Shen et al., 2021) and has since become the mainstream of joint 1 Introduction entity and relation extraction. According to their differences in encoding taskJoint entity and relation extraction intend to specific features, most of the existing methods simultaneously extract entity and relation facts can be divided into two categories: sequential in the given text to form relational triples a"
2021.emnlp-main.17,W02-1010,0,0.0646134,"entity prediction in a non-negligible way. The source code can be found at https://github.com/ Coopercoppers/PFN. answering (Diefenbach et al., 2018) and text summarization (Gupta and Lehal, 2010). ✂ Abstract NER-Specific Cell State NER RE Partition Figure 1: Partition process of cell neurons. Entity and relation gate are used to divide neurons into taskrelated and task-unrelated ones. Neurons relating to both tasks form the shared partition while the rest form two task partitions. Conventionally, Named Entity Recognition (NER) and Relation Extraction (RE) are performed in a pipelined manner (Zelenko et al., 2002; Chan and Roth, 2011). These approaches are flawed in that they do not consider the intimate connection between NER and RE. Also, error propagation is another drawback of pipeline methods. In order to conquer these issues, joint extracting entity and relation is proposed and demonstrates stronger performance on both tasks. In early work, joint methods mainly rely on elaborate feature engineering to establish interaction between NER and RE (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014). Recently, end-to-end neural network has shown to be successful in extracting relational triples"
2021.emnlp-main.17,C14-1220,0,0.0530811,"Chan and Roth, 2011). These approaches are flawed in that they do not consider the intimate connection between NER and RE. Also, error propagation is another drawback of pipeline methods. In order to conquer these issues, joint extracting entity and relation is proposed and demonstrates stronger performance on both tasks. In early work, joint methods mainly rely on elaborate feature engineering to establish interaction between NER and RE (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014). Recently, end-to-end neural network has shown to be successful in extracting relational triples (Zeng et al., 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Shen et al., 2021) and has since become the mainstream of joint 1 Introduction entity and relation extraction. According to their differences in encoding taskJoint entity and relation extraction intend to specific features, most of the existing methods simultaneously extract entity and relation facts can be divided into two categories: sequential in the given text to form relational triples as encoding and parallel encoding. In sequential (s, r, o). The extracted information provides a encoding, task-specific features are generated supplement to"
2021.emnlp-main.22,D17-1047,0,0.14349,"espectively. We adopt Adam (Kingma and Ba, 2015) with warm-up to optimize our models with learning rate 1e−3 for Transformer encoder and 5e−5 for BERT. The pre-trained models are fine-tuned by aspect-aware fine-tuning with 5e−5 learning rate. The hyper-parameters are set as α = β = 1 for combining objectives in SCAPT, and τ = 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning BERT for Sentence Pair Classification, in which t"
2021.emnlp-main.22,P19-1052,0,0.0157185,"5 learning rate. The hyper-parameters are set as α = β = 1 for combining objectives in SCAPT, and τ = 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning BERT for Sentence Pair Classification, in which the input review is transformed to “[CLS] + context + [SEP] + aspect + [SEP]” and fed into BERT for classification (Song et al., 2019). 250 Method Attention GNN Knowledge Enhanced Ours ATAE-LSTM (Wang et al., 2016a) IAN (Ma et al"
2021.emnlp-main.22,D14-1125,0,0.0711583,"Missing"
2021.emnlp-main.22,2021.naacl-main.146,0,0.0153807,", 2018) was further proposed to identify corresponding sentiment expression for aspects. Recent efforts (He et al., 2018; Tang et al., 2020) used syntax information from dependency trees to enhance attention-based models. A lot of works (Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020) make use of graph neural networks to incorporate tree-structured syntactic information and capture aspect-related information in text. Another line in ABSA concentrated on utilizing external corpus and pre-trained knowledge to enhance semantic awareness of models (Xu et al., 2019; Rietzler et al., 2020; Dai et al., 2021). Contrastive Representation Learning Our work adopts contrastive method in representation learning to acquire discriminating instance representations. Recent work on contrastive representation learning of instances usually based on estimating representation similarities on similar and dissimilar pairs, which are usually composed in a self-supervised manner (Chen et al., 2020; He et al., 2020). Specially, Khosla et al. (2020) illustrated a supervised contrastive method to build positive pairs between instances with same class label, and put their representations together. In this work, our mod"
2021.emnlp-main.22,D19-1006,0,0.0221846,"ng scheme. Aspect-Aware Fine-tuning Our proposed models are fine-tuned on ABSA benchmarks by aspect-aware fine-tuning, to fully leverage their ability of sentiment identification. They also learn to capture aspect-related sentiment information during fine-tuning. Specifically, given a sentence xab = ab {w1 , . . . , wa , . . . wn } in ABSA dataset D , and wa is one of the aspects occurring in xab . In fine-tuning, models predict aspect-level sentiment orientation yaab according to aspect-based ¯ ab and sentiment representation representation h a sab . Aspect-based Representation The research (Ethayarajh, 2019) on pre-trained contextualized word representation has demonstrated that it can capture context information related to the word. Thus, in spite of using laborious methods to embed the aspect information, we extract aspect-based ¯ ab by collecting final hidden representation h a ¯ ab states that correspond to wa . In fine-tuning, h a would focus on aspect-related words in context, which we believe would enhance the perception of aspect-specific opinion words and bring the model with a good view of explicit sentiment. Specifically, let Ia be the token index in aspect xa , we average the hidden s"
2021.emnlp-main.22,D18-1380,0,0.0309841,"Missing"
2021.emnlp-main.22,N19-1259,0,0.0476879,"Missing"
2021.emnlp-main.22,C18-1096,0,0.0888715,"Aspect-level sentiment analysis (ABSA) is a fine- contain implicit sentiment among Restaurant and grained variant aiming to identify the sentiment Laptop datasets. However, most of the previous polarity of one or more mentioned aspects in methods generally pay little attention on modeling product reviews. Recent studies tackle the task implicit sentiment expressions. This motivates us by either employing attention mechanisms (Wang to better solve the task of ABSA by capturing et al., 2016b; Ma et al., 2017) or incorporating implicit sentiment in an advanced way. syntax-aware graph structures (He et al., 2018; To equip current models with the ability to Tang et al., 2020; Zhang et al., 2019; Sun et al., capture implicit sentiment, inadequate ABSA 2019; Wang et al., 2020). Both methodologies aim datasets are the main challenge. With only a to capture the corresponding sentiment expression few thousand labeled data, models could hardly towards a particular aspect, which is usually an recognize comprehensive patterns of sentiment opinion word that explicitly expresses sentiment expressions, and are unable to capture enough polarity. For instance, given the review on a commonsense knowledge, which is"
2021.emnlp-main.22,D19-1654,0,0.21183,"sidering the sentiment annotations of retrieved corpora are noisy, supervised contrastive learning enhances noise immunity of the pretraining process. Also, SCAPT contains review reconstruction and masked aspect predication objectives. The former requires representation encoding review context besides sentiment polarity, and the latter adds the model’s ability to capture the sentiment target. Overall, the pre-training process captures both implicit and explicit sentiment orientation towards aspects in reviews. Experimental evaluations conducted on SemEval-2014 (Pontiki et al., 2014) and MAMS (Jiang et al., 2019) datasets show that proposed SCAPT outperforms baseline models by a large margin. The results on partitioned datasets demonstrate the effectiveness of both implicit sentiment expression and explicit sentiment expression. Moreover, the ablation study verifies that SCAPT efficiently learns implicit sentiment expression on the external noisy corpora. Codes and datasets are publicly available1 . The contributions of this work include: • We reveal that ABSA was only marginally tackled by previous studies since they paid little attention to implicit sentiment. • We propose Supervised Contrastive Pre"
2021.emnlp-main.22,2020.acl-main.703,0,0.0512512,"Missing"
2021.emnlp-main.22,S14-2004,0,0.316571,"ent labels are pushed apart. Considering the sentiment annotations of retrieved corpora are noisy, supervised contrastive learning enhances noise immunity of the pretraining process. Also, SCAPT contains review reconstruction and masked aspect predication objectives. The former requires representation encoding review context besides sentiment polarity, and the latter adds the model’s ability to capture the sentiment target. Overall, the pre-training process captures both implicit and explicit sentiment orientation towards aspects in reviews. Experimental evaluations conducted on SemEval-2014 (Pontiki et al., 2014) and MAMS (Jiang et al., 2019) datasets show that proposed SCAPT outperforms baseline models by a large margin. The results on partitioned datasets demonstrate the effectiveness of both implicit sentiment expression and explicit sentiment expression. Moreover, the ablation study verifies that SCAPT efficiently learns implicit sentiment expression on the external noisy corpora. Codes and datasets are publicly available1 . The contributions of this work include: • We reveal that ABSA was only marginally tackled by previous studies since they paid little attention to implicit sentiment. • We prop"
2021.emnlp-main.22,2020.lrec-1.607,0,0.135445,"tive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning BERT for Sentence Pair Classification, in which the input review is transformed to “[CLS] + context + [SEP] + aspect + [SEP]” and fed into BERT for classification (Song et al., 2019). 250 Method Attention GNN Knowledge Enhanced Ours ATAE-LSTM (Wang et al., 2016a) IAN (Ma et al., 2017) RAM (Chen et al., 2017) MGAN (Fan et al., 2018) ASGCN (Zhang et al., 2019) BiGCN (Zhang and Qian, 2020) CDT (Sun et al.,"
2021.emnlp-main.22,S15-2077,0,0.0243514,"2017). We denote the retrieved review corpus used in SCAPT as D = {x1 , x2 , . . . , xn } including n sentences. The i-th sentence xi is labeled with yi . For each input sentence xi , following Devlin et al. (2019), we format the input sentence as Ii = [CLS] + xi + [SEP] to feed into the model. The output vector of [CLS] token encodes the ¯ i: sentence representation h ¯ i , · · · = TransEnc(Ii ) h (1) Implicit Sentiment As sentiment that can only be inferred within the context of reviews, many researches address the presence of implicit sentiment in sentiment analysis. Toprak et al. (2010); Russo et al. (2015) proposed similar terminologies (as implicit polarity or polar facts), and provided corpora containing implicit sentiment. Deng and Wiebe (2014) detected implicit sentiment via inference over explicit sentiment expressions and so-called 1 https://github.com/Tribleave/SCAPT-ABSA 3 Methodology In this section, we introduce the pre-training and fine-tuning scheme of our models. In pre-training, we introduce Supervised ContrAstive Pre-Training (SCAPT) for ABSA, which learns the polarity of sentiment expressions by leveraging retrieved review corpus. In fine-tuning, aspect-aware finetuning is adopt"
2021.emnlp-main.22,D19-1569,0,0.076785,"5e−5 for BERT. The pre-trained models are fine-tuned by aspect-aware fine-tuning with 5e−5 learning rate. The hyper-parameters are set as α = β = 1 for combining objectives in SCAPT, and τ = 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning BERT for Sentence Pair Classification, in which the input review is transformed to “[CLS] + context + [SEP] + aspect + [SEP]” and fed into BERT for classification (Song et al., 2019). 2"
2021.emnlp-main.22,D16-1021,0,0.0157601,"ons can models pre-trained with SCAPT are more robust be found in BERTAsp. The visualization also for aspect-level perturbations, which attribute to shows that BERTAsp+SCAPT tightly clusters the better modeling for sentiment and context the representations of both implicit and explicit information with the enhancement of in-domain sentiment expressions. sentiment knowledge. 253 6 Related Work Neural Network Methods for ABSA The early neural network methods (Wang et al., 2016b; Ma et al., 2017) in ABSA employed various of attention mechanisms to identify aspect-related context. Memory Network (Tang et al., 2016; Chen et al., 2017; Wang et al., 2018) was further proposed to identify corresponding sentiment expression for aspects. Recent efforts (He et al., 2018; Tang et al., 2020) used syntax information from dependency trees to enhance attention-based models. A lot of works (Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020) make use of graph neural networks to incorporate tree-structured syntactic information and capture aspect-related information in text. Another line in ABSA concentrated on utilizing external corpus and pre-trained knowledge to enhance semantic awareness of models (Xu et al"
2021.emnlp-main.22,2020.acl-main.588,0,0.103833,"licit sentiment among Restaurant and grained variant aiming to identify the sentiment Laptop datasets. However, most of the previous polarity of one or more mentioned aspects in methods generally pay little attention on modeling product reviews. Recent studies tackle the task implicit sentiment expressions. This motivates us by either employing attention mechanisms (Wang to better solve the task of ABSA by capturing et al., 2016b; Ma et al., 2017) or incorporating implicit sentiment in an advanced way. syntax-aware graph structures (He et al., 2018; To equip current models with the ability to Tang et al., 2020; Zhang et al., 2019; Sun et al., capture implicit sentiment, inadequate ABSA 2019; Wang et al., 2020). Both methodologies aim datasets are the main challenge. With only a to capture the corresponding sentiment expression few thousand labeled data, models could hardly towards a particular aspect, which is usually an recognize comprehensive patterns of sentiment opinion word that explicitly expresses sentiment expressions, and are unable to capture enough polarity. For instance, given the review on a commonsense knowledge, which is required in restaurant “Great food but the service is dreadful”"
2021.emnlp-main.22,2021.acl-demo.41,1,0.717486,"Ori → New Decline 67.55→9.87 -57.68 72.41→19.91 -52.50 77.12→25.86 -51.46 77.59→50.94 -26.65 78.53→53.29 -25.24 76.80→52.52 -24.28 82.76→76.13 -6.63 Table 6: Model performance on aspect robustness test sets. We compare the model accuracy on the original and new test sets, and the decline of prediction on new examples are reported. 5.6 Aspect Robustness We analyze the robustness of our proposed models on aspect robustness test sets. Aspect robustness of ABSA was first emphasized and tested in Xing et al. (2020) by applying several perturbations on reviews from Restaurant and Laptop. TextFlint (Wang et al., 2021) extended these transformations by introducing transformations from various linguistic perspectives. The test sets are designed to probe whether models could distinguish the sentiment of the target aspect from the non-target aspects and unrelated information. 5.5 Hidden Sentiment Representations Table 6 lists the performance of tested modFor better understanding the behavior of our els, in which the robustness of our proposed proposed methods, we further perform a visual- models is convincingly proved. Comparing to ization of the sentiment representation using t-SNE obvious performance drop in"
2021.emnlp-main.22,2020.acl-main.374,0,0.0192792,"ked input token at k-th position, its contextualized hidden representation hik is fed into a softmax layer to predict the original word: P map (k) = softmax(Wo hik ) (6) Specific to the above equation, hik is the output of Transformer encoder at k-th position, Wo is a trainable parameter matrix, and P map (k) indicates the predict probability of the original word at k-th position. The masked aspect prediction loss is an accumulation of log-likelihood on predictions of each masked position: Lmap = i X − log P map (k) (7) xik =wMASK Different from MLM (Devlin et al., 2019) or sentiment masking (Tian et al., 2020), masked aspect prediction focuses more on modeling aspectrelated context information in aspect-based representations, which complements the other pretraining objectives and purposefully benefits our fine-tuning scheme. Aspect-Aware Fine-tuning Our proposed models are fine-tuned on ABSA benchmarks by aspect-aware fine-tuning, to fully leverage their ability of sentiment identification. They also learn to capture aspect-related sentiment information during fine-tuning. Specifically, given a sentence xab = ab {w1 , . . . , wa , . . . wn } in ABSA dataset D , and wa is one of the aspects occurrin"
2021.emnlp-main.22,P10-1059,0,0.0455267,"coder (Vaswani et al., 2017). We denote the retrieved review corpus used in SCAPT as D = {x1 , x2 , . . . , xn } including n sentences. The i-th sentence xi is labeled with yi . For each input sentence xi , following Devlin et al. (2019), we format the input sentence as Ii = [CLS] + xi + [SEP] to feed into the model. The output vector of [CLS] token encodes the ¯ i: sentence representation h ¯ i , · · · = TransEnc(Ii ) h (1) Implicit Sentiment As sentiment that can only be inferred within the context of reviews, many researches address the presence of implicit sentiment in sentiment analysis. Toprak et al. (2010); Russo et al. (2015) proposed similar terminologies (as implicit polarity or polar facts), and provided corpora containing implicit sentiment. Deng and Wiebe (2014) detected implicit sentiment via inference over explicit sentiment expressions and so-called 1 https://github.com/Tribleave/SCAPT-ABSA 3 Methodology In this section, we introduce the pre-training and fine-tuning scheme of our models. In pre-training, we introduce Supervised ContrAstive Pre-Training (SCAPT) for ABSA, which learns the polarity of sentiment expressions by leveraging retrieved review corpus. In fine-tuning, aspect-awar"
2021.emnlp-main.22,2020.acl-main.295,0,0.109745,". However, most of the previous polarity of one or more mentioned aspects in methods generally pay little attention on modeling product reviews. Recent studies tackle the task implicit sentiment expressions. This motivates us by either employing attention mechanisms (Wang to better solve the task of ABSA by capturing et al., 2016b; Ma et al., 2017) or incorporating implicit sentiment in an advanced way. syntax-aware graph structures (He et al., 2018; To equip current models with the ability to Tang et al., 2020; Zhang et al., 2019; Sun et al., capture implicit sentiment, inadequate ABSA 2019; Wang et al., 2020). Both methodologies aim datasets are the main challenge. With only a to capture the corresponding sentiment expression few thousand labeled data, models could hardly towards a particular aspect, which is usually an recognize comprehensive patterns of sentiment opinion word that explicitly expresses sentiment expressions, and are unable to capture enough polarity. For instance, given the review on a commonsense knowledge, which is required in restaurant “Great food but the service is dreadful”, sentiment identification. It reveals that external current models attempt to find “great” for aspect"
2021.emnlp-main.22,P18-1088,0,0.0177396,"are more robust be found in BERTAsp. The visualization also for aspect-level perturbations, which attribute to shows that BERTAsp+SCAPT tightly clusters the better modeling for sentiment and context the representations of both implicit and explicit information with the enhancement of in-domain sentiment expressions. sentiment knowledge. 253 6 Related Work Neural Network Methods for ABSA The early neural network methods (Wang et al., 2016b; Ma et al., 2017) in ABSA employed various of attention mechanisms to identify aspect-related context. Memory Network (Tang et al., 2016; Chen et al., 2017; Wang et al., 2018) was further proposed to identify corresponding sentiment expression for aspects. Recent efforts (He et al., 2018; Tang et al., 2020) used syntax information from dependency trees to enhance attention-based models. A lot of works (Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020) make use of graph neural networks to incorporate tree-structured syntactic information and capture aspect-related information in text. Another line in ABSA concentrated on utilizing external corpus and pre-trained knowledge to enhance semantic awareness of models (Xu et al., 2019; Rietzler et al., 2020; Dai et"
2021.emnlp-main.22,D16-1058,0,0.261289,"nsformer encoder and BERT takes 80 and 8 epochs respectively. We adopt Adam (Kingma and Ba, 2015) with warm-up to optimize our models with learning rate 1e−3 for Transformer encoder and 5e−5 for BERT. The pre-trained models are fine-tuned by aspect-aware fine-tuning with 5e−5 learning rate. The hyper-parameters are set as α = β = 1 for combining objectives in SCAPT, and τ = 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning"
2021.emnlp-main.22,2020.emnlp-main.292,1,0.84704,"Missing"
2021.emnlp-main.22,N19-1242,0,0.101511,"= 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning BERT for Sentence Pair Classification, in which the input review is transformed to “[CLS] + context + [SEP] + aspect + [SEP]” and fed into BERT for classification (Song et al., 2019). 250 Method Attention GNN Knowledge Enhanced Ours ATAE-LSTM (Wang et al., 2016a) IAN (Ma et al., 2017) RAM (Chen et al., 2017) MGAN (Fan et al., 2018) ASGCN (Zhang et al., 2019) BiGCN (Zhang"
2021.emnlp-main.22,D19-1464,0,0.0267047,"Missing"
2021.emnlp-main.22,2020.emnlp-main.286,0,0.0167801,"for Transformer encoder and 5e−5 for BERT. The pre-trained models are fine-tuned by aspect-aware fine-tuning with 5e−5 learning rate. The hyper-parameters are set as α = β = 1 for combining objectives in SCAPT, and τ = 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning BERT for Sentence Pair Classification, in which the input review is transformed to “[CLS] + context + [SEP] + aspect + [SEP]” and fed into BERT for classificatio"
2021.emnlp-main.22,D16-1059,0,0.162078,"nsformer encoder and BERT takes 80 and 8 epochs respectively. We adopt Adam (Kingma and Ba, 2015) with warm-up to optimize our models with learning rate 1e−3 for Transformer encoder and 5e−5 for BERT. The pre-trained models are fine-tuned by aspect-aware fine-tuning with 5e−5 learning rate. The hyper-parameters are set as α = β = 1 for combining objectives in SCAPT, and τ = 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning"
2021.findings-acl.121,P19-1279,0,0.0206704,"dopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token “@” before and after the first entity, adding “#” before and after the second entity. Then the token representations of the first special token “@” and “#” are concatenated to perform relation classification. We adopt micro F1 score as the metric to represent the model performance as previous works. Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1. Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-A DAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In particular, (1) K-A DAPTER models outperform RoBERTa, which proves the effectiveness of infusing knowledge in"
2021.findings-acl.121,P18-1009,0,0.0216531,"d analyses with the case study and probing experiments to explore the effectiveness and ability of models for learning factual knowledge. The notations of K-A DAPTER (F+L), K-A DAPTER (F), and K-A DAPTER (L) denote our model which consists of both factual adapter and linguistic adapter, only factual adapter and only linguistic adapter, respectively. Implementation details, and statistics of datasets are in the Appendix. 4.1 Entity Typing We conduct experiments on fine-grained entity typing which aims to predict the types of a given entity and its context. We evaluate our models on OpenEntity (Choi et al., 2018) and FIGER (Ling et al., 2015) following the same split setting as Zhang et al. (2019). To fine-tune our models for entity typing, we modify the input token sequence by adding the special token “@” before and after a certain entity, then the first “@” special token representation is adopted to perform classification. As for OpenEntity, we adopt micro F1 score as the final metric to represent the model performance. As for FIGER, we adopt strict accuracy, loose macro, loose micro F1 scores (Ling and Weld, 2012) for evaluation following the same evaluation criteria used in previous works. Baselin"
2021.findings-acl.121,P19-1285,0,0.0618623,"Missing"
2021.findings-acl.121,N19-1423,0,0.0429486,"texttriplets on Wikipedia and Wikidata and (2) linguistic knowledge obtained via dependency parsing. Results on three knowledge-driven tasks, including relation classification, entity typing, and question answering, demonstrate that each adapter improves the performance and the combination of both adapters brings further improvements. Further analysis indicates that K-A DAPTER captures versatile knowledge than RoBERTa. 1 1 Introduction Language representation models, which are pretrained on large-scale text corpus through unsupervised objectives like (masked) language modeling, such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018, 2019), XLNet (Yang et al., ∗ Work is done during internship at Microsoft. Zhongyu Wei and Duyu Tang are corresponding authors. 1 Codes are publicly available at https://github. com/microsoft/K-Adapter 2019), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020), have established state-of-the-art performances on various NLP downstream tasks. Despite the huge success of these pre-trained models in empirical studies, recent studies suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) sugges"
2021.findings-acl.121,L18-1544,0,0.0249766,"ayer, we denote the number of transformer layer as N , the hidden dimension of transformer layer as HA , the number of self-attention heads as AA , the hidden dimension of down-projection and up-projection layers as Hd and Hu . In detail, we have the following adapter size: N = 2, HA = 768, AA = 12, Hu = 1024 and Hd = 768. The RoBERTa lay2 3.3 Factual Adapter Factual knowledge can be described as the basic information that is concerned with facts. In this work, we acquire factual knowledge from the relationships among entities in natural language. We extract a sub-dataset T-REx-rc from T-REx (ElSahar et al., 2018) which is a large scale alignment dataset between Wikipedia abstracts and Wikidata triples. We discard all relations having less than 50 entity pairs, collecting 430 relations and 5.5M sentences. In order to inject factual knowledge, we propose to pre-train a knowledge-specific adapter called facAdapter on the relation classification task. This task requires a model to classify relation labels of given entity pairs based on context. Specifically, the last hidden features of RoBERTa and facAdapter are concatenated as the input representation, and the pooling layer is applied to the input repres"
2021.findings-acl.121,2020.findings-emnlp.71,0,0.198207,"Missing"
2021.findings-acl.121,W16-1313,0,0.118964,"-the-shell dependency parser from Stanford Parser3 on a part of Book Corpus (Zhu et al., 2015). To inject linguistic knowledge, we pre-train another knowledge-specific adapter called linAdapter on the task of dependency relation prediction. This task aims to predict the head index of each token in the given sentence. We concatenate the last hidden features of RoBERTa and linAdapter as the input representation, and then apply a linear layer 3 https://github.com/huggingface/transformers 1408 http://nlp.stanford.edu/software/lex-parser.html OpenEntity Model FIGER P R Mi-F1 Acc Ma-F1 Mi-F1 NFGEC (Shimaoka et al., 2016) BERT-base (Zhang et al., 2019) ERNIE (Zhang et al., 2019) KnowBERT (Peters et al., 2019) KEPLER (Wang et al., 2021) WKLM (Xiong et al., 2020) 68.80 76.37 78.42 78.60 77.20 - 53.30 70.96 72.90 73.70 74.20 - 60.10 73.56 75.56 76.10 75.70 - 55.60 52.04 57.19 60.21 75.15 75.16 75.61 81.99 71.73 71.63 73.39 77.00 RoBERTa RoBERTa + multitask K-A DAPTER (w/o knowledge) K-A DAPTER (F) K-A DAPTER (L) K-A DAPTER (F+L) 77.55 77.96 74.47 79.30 80.01 78.99 74.95 76.00 74.91 75.84 74.00 76.27 76.23 76.97 76.17 77.53 76.89 77.61 56.31 59.86 56.93 59.50 61.10 61.81 82.43 84.45 82.56 84.52 83.61 84.87 77.83 7"
2021.findings-acl.121,D18-1244,0,0.0240646,"ion classification aims to determine the correct relation between two entities in a given sentence. We adopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token “@” before and after the first entity, adding “#” before and after the second entity. Then the token representations of the first special token “@” and “#” are concatenated to perform relation classification. We adopt micro F1 score as the metric to represent the model performance as previous works. Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1. Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-A DAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In part"
2021.findings-acl.121,D17-1004,0,0.069357,"Missing"
2021.findings-acl.121,P19-1139,0,0.231713,"s suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) suggest that although language models do well in reasoning about the surface form of entity names, they fail in capturing rich factual knowledge. Kassner and Sch¨utze (2020) observe that BERT mostly did not learn the meaning of negation (e.g. “not”). These observations motivate us to study the injection of knowledge into pre-trained models like BERT and RoBERTa. Recently, some efforts have been made to exploit injecting knowledge into pre-trained language models (Zhang et al., 2019; Lauscher et al., 2019; Levine et al., 2020; Peters et al., 2019; He et al., 2020; Xiong et al., 2020). Most previous works (as shown in Table 1) augment the standard language modeling objective with knowledge-driven objectives and update the entire model parameters. Although these methods obtain better performance on downstream tasks, they struggle at supporting the development of versatile models with multiple kinds of knowledge injected (Kirkpatrick et al., 2017). When new kinds of knowledge are injected, model parameters need to be retrained so that previously injected knowledge would fad"
2021.findings-acl.121,P17-1018,1,0.80079,"elected. We report accuracy scores obtained from the leaderboard. Open-domain QA aims to answer questions using external resources such as collections of documents and webpages. We evaluate our modes on two public datasets, i.e., Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017). Specifically, we first retrieve paragraphs corresponding to the question using the information retrieval system and then extract the answer from these retrieved paragraphs through the reading comprehension technique. Following previous work(Lin et al., 2018), we use the retrieved paragraphs provided by Wang et al. (2017) for these two datasets. To fine-tune our models for this task, the input token sequence is modified as “<SEP>question </SEP>paragraph</SEP>”. We apply linear layers over the last hidden features of our model to predict the start and end position of the answer span. We adopt two metrics including ExactMatch (EM) and loose F1 (Ling and Weld, 2012) scores to evaluate our models. Baselines BERT-FTRACE+SW AG (Huang et al., 2019) is the BERT model sequentially fine-tuned on both RACE and SWAG datasets. BiDAF (Seo et al., 2017) adopts a bi-directional attention network. AQA (Buck et al., 2018) propo"
2021.findings-acl.203,2020.acl-main.632,0,0.0998607,"Missing"
2021.findings-acl.203,2020.acl-main.371,0,0.0529097,"Missing"
2021.findings-acl.203,2020.emnlp-main.3,0,0.0695418,"Missing"
2021.findings-acl.203,2020.emnlp-main.569,0,0.205178,"responding author. logical context like student essays, public speeches, etc., where only one participant is involved. Online forums such as idebate1 and changemyview2 , enable people to exchange opinions on some specific topics freely. The user generated dataset of interactive arguments also motivates another line of research for argumentation in dialogical context (Asterhan and Schwarz, 2007). Initial researches in this filed focused on analyzing the ChangeMyView data (Tan et al., 2016; Wei et al., 2016) to summarize the key factors of persuasive arguments. Furthermore, Ji et al. (2019) and Cheng et al. (2020) propose the task of identifying and extracting interactive arguments. Ji et al. (2019) formulate this task as a problem of sentence pair scoring and computes the textual similarity 1 https://idebate.org/ https://www.reddit.com/r/ changemyview/ 2 2310 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2310–2319 August 1–6, 2021. ©2021 Association for Computational Linguistics between the two arguments as the result. Such task is then further applied to other fields such as legal domain. For instance, Yuan et al. (2021) organize a challenge aimed to identify the i"
2021.findings-acl.203,D14-1125,0,0.0706235,"Missing"
2021.findings-acl.203,N19-1423,0,0.00667796,"represent the weight vector and the bias respectively. After obtaining the matching score for each argument pair, we treat the task as a sentence pair ranking problem, and use MarginRankingLoss for training: 2313 L= 4 X i=1 max(0, γ − S(q, r+ ) + S(q, ri− )), (12) where S(q, r+ ) refers to the matching score of the positive argument pair while S(q, ri− ) refers to the matching score of the i-th negative argument pair, and γ is the margin hyperparameter. 4 Experiments In this section, we will introduce the dataset, the evaluation metrics, comparative models and experiment results. 4.1 - BERT (Devlin et al., 2019): This method finetunes the pre-trained BERT model for sentencepair classification. Note that this model is not only a baseline model but also a sub-module of our proposed model. Experiment Setup Experimental Dataset We use the dataset constructed in (Ji et al., 2019) for evaluation. The authors find that in the ChangeMyView dataset (Tan et al., 2016), there exist replies that quote sentences from the original post. They extract all these quotation-reply pair q, r from posts in ChangeMyView dataset (Tan et al., 2016). For every interactive argument pair, they randomly sample four negative repl"
2021.findings-acl.203,2020.acl-main.287,0,0.0359314,"Missing"
2021.findings-acl.203,P14-1145,0,0.0580711,"Missing"
2021.findings-acl.203,2020.emnlp-main.716,0,0.0131216,"ath-level). Experiment results indicate that our model achieves state-of-the-art performance in the benchmark dataset. Further analysis demonstrates the effectiveness of our model for enforcing knowledge reasoning through paths in the knowledge graph. 1 Figure 1: Two instances of interactive argument pairs, the related concepts are colored same, and the corresponding knowledge is visualised in the right side. Introduction Argumentation Mining aims at analyzing the semantic and logical structure of argumentative texts. Existing research covers argument structure prediction (Morio et al., 2020; Li et al., 2020), persuasiveness evaluation (Al Khatib et al., 2020; El Baff et al., 2020) and argument summarization (BarHaim et al., 2020b,a). Most of them focus on mono∗ Corresponding author. logical context like student essays, public speeches, etc., where only one participant is involved. Online forums such as idebate1 and changemyview2 , enable people to exchange opinions on some specific topics freely. The user generated dataset of interactive arguments also motivates another line of research for argumentation in dialogical context (Asterhan and Schwarz, 2007). Initial researches in this filed focused"
2021.findings-acl.203,D19-1282,0,0.0112976,"annotation stage to obtain an automatically generated knowledge graph. Leveraging external knowledge in NLU Our work also lies in the general context of using external knowledge to encode sentences and paragraphs. Yang and Mitchell (2017) are among the first researches that retrieve the related entities in the external knowledge base and merge them into an LSTM encoder. Afterward, Weissenborn et al. (2017), Mihaylov and Frank (2018) and Zhang et al. (2020) mainly follows the main idea of the work to incorporate external word-level lexical knowledge to enhance the sentence embedding. Moreover, Lin et al. (2019) propose a knowledge-aware network(KagNet) that utilizes that graph knowledge from ConceptNet to answer the commonsense questions. Compared with these methods, our work utilizes conceptual information from dialogical argumentation lexicons and conducts a reasoning process resembling human beings, which is then encoded by a path transformer, and finally aligned with the semantic information through a hierarchical attention mechanism. 2317 7 Conclusion and Future Work We propose a framework that imitates human’s reasoning process in debating. Practically, we first construct a dialogical argument"
2021.findings-acl.203,P18-1076,0,0.0136484,"wledge graph. Our work obtains inspiration from the construction of Al-Khatib’s knowledge graph, but adapting their method to the dialogical debating forum settings, and removing the human annotation stage to obtain an automatically generated knowledge graph. Leveraging external knowledge in NLU Our work also lies in the general context of using external knowledge to encode sentences and paragraphs. Yang and Mitchell (2017) are among the first researches that retrieve the related entities in the external knowledge base and merge them into an LSTM encoder. Afterward, Weissenborn et al. (2017), Mihaylov and Frank (2018) and Zhang et al. (2020) mainly follows the main idea of the work to incorporate external word-level lexical knowledge to enhance the sentence embedding. Moreover, Lin et al. (2019) propose a knowledge-aware network(KagNet) that utilizes that graph knowledge from ConceptNet to answer the commonsense questions. Compared with these methods, our work utilizes conceptual information from dialogical argumentation lexicons and conducts a reasoning process resembling human beings, which is then encoded by a path transformer, and finally aligned with the semantic information through a hierarchical att"
2021.findings-acl.203,2020.acl-main.298,0,0.0323231,"h entity-level and path-level). Experiment results indicate that our model achieves state-of-the-art performance in the benchmark dataset. Further analysis demonstrates the effectiveness of our model for enforcing knowledge reasoning through paths in the knowledge graph. 1 Figure 1: Two instances of interactive argument pairs, the related concepts are colored same, and the corresponding knowledge is visualised in the right side. Introduction Argumentation Mining aims at analyzing the semantic and logical structure of argumentative texts. Existing research covers argument structure prediction (Morio et al., 2020; Li et al., 2020), persuasiveness evaluation (Al Khatib et al., 2020; El Baff et al., 2020) and argument summarization (BarHaim et al., 2020b,a). Most of them focus on mono∗ Corresponding author. logical context like student essays, public speeches, etc., where only one participant is involved. Online forums such as idebate1 and changemyview2 , enable people to exchange opinions on some specific topics freely. The user generated dataset of interactive arguments also motivates another line of research for argumentation in dialogical context (Asterhan and Schwarz, 2007). Initial researches in t"
2021.findings-acl.203,P16-1030,0,0.0612931,"Missing"
2021.findings-acl.203,P16-2032,1,0.817327,"l Baff et al., 2020) and argument summarization (BarHaim et al., 2020b,a). Most of them focus on mono∗ Corresponding author. logical context like student essays, public speeches, etc., where only one participant is involved. Online forums such as idebate1 and changemyview2 , enable people to exchange opinions on some specific topics freely. The user generated dataset of interactive arguments also motivates another line of research for argumentation in dialogical context (Asterhan and Schwarz, 2007). Initial researches in this filed focused on analyzing the ChangeMyView data (Tan et al., 2016; Wei et al., 2016) to summarize the key factors of persuasive arguments. Furthermore, Ji et al. (2019) and Cheng et al. (2020) propose the task of identifying and extracting interactive arguments. Ji et al. (2019) formulate this task as a problem of sentence pair scoring and computes the textual similarity 1 https://idebate.org/ https://www.reddit.com/r/ changemyview/ 2 2310 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2310–2319 August 1–6, 2021. ©2021 Association for Computational Linguistics between the two arguments as the result. Such task is then further applied to othe"
2021.findings-acl.203,P17-1132,0,0.019062,"such discussion; Khatib et al. (2020) constructs a monological argumentation graph by extracting knowledge from Debatepedia.org and use human annotation to further improve the quality of their knowledge graph. Our work obtains inspiration from the construction of Al-Khatib’s knowledge graph, but adapting their method to the dialogical debating forum settings, and removing the human annotation stage to obtain an automatically generated knowledge graph. Leveraging external knowledge in NLU Our work also lies in the general context of using external knowledge to encode sentences and paragraphs. Yang and Mitchell (2017) are among the first researches that retrieve the related entities in the external knowledge base and merge them into an LSTM encoder. Afterward, Weissenborn et al. (2017), Mihaylov and Frank (2018) and Zhang et al. (2020) mainly follows the main idea of the work to incorporate external word-level lexical knowledge to enhance the sentence embedding. Moreover, Lin et al. (2019) propose a knowledge-aware network(KagNet) that utilizes that graph knowledge from ConceptNet to answer the commonsense questions. Compared with these methods, our work utilizes conceptual information from dialogical argu"
2021.findings-acl.203,2020.acl-main.291,0,0.0277117,"inspiration from the construction of Al-Khatib’s knowledge graph, but adapting their method to the dialogical debating forum settings, and removing the human annotation stage to obtain an automatically generated knowledge graph. Leveraging external knowledge in NLU Our work also lies in the general context of using external knowledge to encode sentences and paragraphs. Yang and Mitchell (2017) are among the first researches that retrieve the related entities in the external knowledge base and merge them into an LSTM encoder. Afterward, Weissenborn et al. (2017), Mihaylov and Frank (2018) and Zhang et al. (2020) mainly follows the main idea of the work to incorporate external word-level lexical knowledge to enhance the sentence embedding. Moreover, Lin et al. (2019) propose a knowledge-aware network(KagNet) that utilizes that graph knowledge from ConceptNet to answer the commonsense questions. Compared with these methods, our work utilizes conceptual information from dialogical argumentation lexicons and conducts a reasoning process resembling human beings, which is then encoded by a path transformer, and finally aligned with the semantic information through a hierarchical attention mechanism. 2317 7"
2021.findings-acl.203,N19-1421,0,0.0226791,".3), which are finally fed into a multi-layer perceptron (MLP) to calculate the final matching score (§3.4). 3.1 Sentence Encoding As for the quotation and reply arguments, it is critical to use the semantic information implied in the texts. Various works have already proved the outstanding performance of pre-trained models in semantic modeling. In our work, we use the BERT model to generate the encoding s for the given argument pair by simply creating a sentence that takes the form of ”[CLS] q [SEP] r [SEP]” and taking the embedding for the ”[CLS]” token, just as suggested by previous works (Talmor et al., 2019). 3.2 Concept Encoding For entities in the argumentation knowledge graph, we need to obtain the representation for each node. We use the BERT model with average pooling to get the initial representation for each entity. Then we encode the conceptual information in both entity-level and path-level with graph networks to enforce the background knowledge modeling and reasoning. 3.2.1 Entity Level Representation To utilize the structural information entailed in the knowledge graph, we apply a 2-layer Graph Convolutional Network (GCN) to it. Here we adopt GCN as it has proved to be both effective a"
2021.findings-emnlp.127,2020.findings-emnlp.211,0,0.0792033,"Missing"
2021.findings-emnlp.127,C10-3014,0,0.0327931,"Missing"
2021.findings-emnlp.127,P16-1014,0,0.0704491,"Missing"
2021.findings-emnlp.127,P18-1039,0,0.0159639,"ategory neighbor same . category If a box of fruit weighs 24 kilograms , how many boxes of pears are less than apples ? Expression: ( 360 / 24 ) – ( 240 / 24 ) – – / 240 / / 24 360 24 360 / 24 240 24 pear boxes apple boxes 240/24 360/24 - / 240 24 / 360 34 (a) Graph2Tree - / 360 24 / 240 24 (b) EEH-G2T Figure 1: An example of a math word problem. The top part of the figure shows the different types of edges connected to the word “pear” in the graph. The bottom part of the figure shows the expressions generated by Graph2Tree (Zhang et al., 2020b) and EEH-G2T. Previous works (Wang et al., 2017; Huang et al., 2018; Wang et al., 2019) used sequenceto-sequence (seq2seq) methods with an attention mechanism (Bahdanau et al., 2014) to generate math expression sequences from math word prob1 Introduction lems. To capture the structural information of math Math word problem solving is an important natural expressions, many works (Liu et al., 2019; Xie language processing (NLP) task that has recently and Sun, 2019; Zhang et al., 2020a) treat math been attracting increasing research interests. Math expressions as binary trees and propose several word problems are narrative text that describe sequence-to-tree (se"
2021.findings-emnlp.127,N16-1136,0,0.0684508,"Missing"
2021.findings-emnlp.127,2020.findings-emnlp.255,0,0.0329546,"Missing"
2021.findings-emnlp.127,P17-1015,0,0.0683501,"Missing"
2021.findings-emnlp.127,D19-1241,0,0.0403052,"Missing"
2021.findings-emnlp.127,P14-5010,0,0.00261207,"nal knowledge bases. An illustrative example is shown in Figure 3. Specifically, given a math word problem X, its dependency tree, and word category information, our model constructs a graph according to the following steps. Edge-labeled Graph 2.2.1 Graph Construction This section introduces how to construct an edgelabeled graph that contains both the local relations between nodes within a sentence and the longrange relations between nodes across sentences. Our model extracts these relations from the problem’s dependency tree and external knowledge base. We use the Stanford Corenlp toolkit 2 (Manning et al., 2014) to parse each math word problem into a dependency tree. The toolkit analyzes the grammatical structure of a sentence and establishes relationships between “head” words and words which modify those heads. In addition, inspired by Wu et al. (2020), we collected word 2 https://stanfordnlp.github.io/CoreNLP/ 1475 • Self node & Neighbor: We define each word xi in the problem X as a node. Each word node xi is connected to its adjacent word nodes (xi−1 , xi+1 ) in the problem. These edges are labeled as “neighbor”. Also, to incorporate the node’s own information into the problem representations, we"
2021.findings-emnlp.127,D14-1162,0,0.090807,"L=− T X log P(yt |y&lt;t ,X). (9) t=1 During the inference, we use beam search to generate final expression. At time step t, if yt is an operator, the current node is an internal node, and the model continues to generate its child nodes. If yt is a number, it represents a leaf node with no child node. Once the children of all the internal nodes have been generated, the generated expression sequence Y= {y1 , y2 , . . . , yT } is transformed into an expression tree, and the decoding process is terminated. 3.1 We used Pytorch for our implementation 3 . We used 300-dimensional Glove word embeddings (Pennington et al., 2014). The hidden size is 512. The batch size is 64. The number of heads M in problem-level aggregation is 8. The number K of split attention vectors is 2. We set the learning rate of the Adam optimizer (Kingma and Ba, 2014) to 0.001, and the dropout is 0.5. During training, it took 120 epochs to train the model. During decoding, we used a beam search with a beam size of 5. We used the same parameter settings for both Math23K and MAWPS datasets. The hyper-parameters are tuned on the valid set. Training We train the model with the cross-entropy loss, defined as: 3 Implementation Details Experiments"
2021.findings-emnlp.127,P19-1423,0,0.0373921,"Missing"
2021.findings-emnlp.127,2020.acl-main.362,0,0.190558,"tate-of-the-art methods. 1 240 kilograms of category neighbor pears category neighbor same . category If a box of fruit weighs 24 kilograms , how many boxes of pears are less than apples ? Expression: ( 360 / 24 ) – ( 240 / 24 ) – – / 240 / / 24 360 24 360 / 24 240 24 pear boxes apple boxes 240/24 360/24 - / 240 24 / 360 34 (a) Graph2Tree - / 360 24 / 240 24 (b) EEH-G2T Figure 1: An example of a math word problem. The top part of the figure shows the different types of edges connected to the word “pear” in the graph. The bottom part of the figure shows the expressions generated by Graph2Tree (Zhang et al., 2020b) and EEH-G2T. Previous works (Wang et al., 2017; Huang et al., 2018; Wang et al., 2019) used sequenceto-sequence (seq2seq) methods with an attention mechanism (Bahdanau et al., 2014) to generate math expression sequences from math word prob1 Introduction lems. To capture the structural information of math Math word problem solving is an important natural expressions, many works (Liu et al., 2019; Xie language processing (NLP) task that has recently and Sun, 2019; Zhang et al., 2020a) treat math been attracting increasing research interests. Math expressions as binary trees and propose severa"
2021.findings-emnlp.127,D18-1132,0,0.015132,"014) to 0.001, and the dropout is 0.5. During training, it took 120 epochs to train the model. During decoding, we used a beam search with a beam size of 5. We used the same parameter settings for both Math23K and MAWPS datasets. The hyper-parameters are tuned on the valid set. Training We train the model with the cross-entropy loss, defined as: 3 Implementation Details Experiments Datasets 3.3 Baselines We compare the performance of our model with the following baselines: DNS (Wang et al., 2017) is a seq2seq model that consists of a two-layer GRU encoder and a two-layer LSTM decoder. MathEN (Wang et al., 2018) is a seq2seq model with a bidirectional LSTM encoder and an attention mechanism. Recu-RNN (Wang et al., 2019) uses recursive neural networks on the predicted tree structure templates. Tree-Dec (Liu et al., 2019) is a seq2tree model with a tree-structured decoder, which generates each node based on its parent and sibling node. GTS (Xie and Sun, 2019) is a seq2tree model that generates expression trees in a goal-driven manner. It generates each node based on its parent node and its left sibling subtree embedding. KA-S2T (Wu et al., 2020) is a graphto-tree model with commonsense knowledge from t"
2021.findings-emnlp.127,D18-1244,0,0.0407093,"Missing"
2021.findings-emnlp.127,D17-1088,0,0.0639743,"Missing"
2021.findings-emnlp.127,2020.emnlp-main.579,1,0.902406,"raph Construction This section introduces how to construct an edgelabeled graph that contains both the local relations between nodes within a sentence and the longrange relations between nodes across sentences. Our model extracts these relations from the problem’s dependency tree and external knowledge base. We use the Stanford Corenlp toolkit 2 (Manning et al., 2014) to parse each math word problem into a dependency tree. The toolkit analyzes the grammatical structure of a sentence and establishes relationships between “head” words and words which modify those heads. In addition, inspired by Wu et al. (2020), we collected word 2 https://stanfordnlp.github.io/CoreNLP/ 1475 • Self node & Neighbor: We define each word xi in the problem X as a node. Each word node xi is connected to its adjacent word nodes (xi−1 , xi+1 ) in the problem. These edges are labeled as “neighbor”. Also, to incorporate the node’s own information into the problem representations, we connect each node to itself and label the edge as “self node”. • Dependency (edges within sentences): The dependency tree is a structured representation that contains various grammatical relationships between word pairs. Following Zhang et al. (2"
2021.naacl-main.135,P19-1285,0,0.0508064,"Missing"
2021.naacl-main.135,N19-1122,0,0.0178446,"ion as a new self-attention network. Zhao et al. (2019) explores parallel multi-scale representation learning to capture both long-range and short-range language structures with combination of convolution and self-attention. In our work, DMAN, SAN and FFN are unified in Mask Attention Networks, where DMAN is a supplement of SAN and FFN that specializes in localness modeling. Moreover, we investigate different collaboration mechanisms. Related Work Recently, there is a large body of work on im- 6 Conclusion proving Transformer (Vaswani et al., 2017) for various issues. For recurrence modeling, Hao et al. (2019) introduces a novel attentive recurrent netIn this paper, we introduce Mask Attention Network to leverage the strengths of both attention and works and reformulate SAN and FFN to point recurrent networks. For context modeling, Yang out they are two special cases with static mask et al. (2019a) focuses on improving self-attention in MANs. We analyze the the deficiency of through capturing the richness of context and pro- SAN and FFN in localness modeling. Dynamic poses to contextualize the transformations of the Mask Attention Network is derived from MANs query and key layers. Wu et al. (2019)"
2021.naacl-main.135,N03-1020,0,0.173018,"Missing"
2021.naacl-main.135,W18-6301,0,0.0396285,"Missing"
2021.naacl-main.135,P02-1040,0,0.110664,"warmup steps is 8k and the total updates is 50k. with 0.1 label smoothing rate. Inverse-sqrt learning The optimizer of model is Adam with (0.9,0.98). rate scheduler are employed, the peak learning rates The dropout and clip-norm are both 0.1. During are 1.5e-2, 1e-2 and 7e-3 with 8k warmup, 50k decoding, the beam size are both 5, the max length update, 80k update and 80k update for transformer and length penalty are 50 and 2.0 for CNN/Daily 1696 Mail, 30 and 1.0 for Gigaword. The models are trained on 4 P40 GPUs. 3.2 Experimental Results 3.2.1 Machine Translation In machine translation, BLEU (Papineni et al., 2002) is employed as the evaluation measure. Following common practice, we use tokenized casesensitive BLEU and case-insensitive BLEU for WMT14 En-De and IWSLT14 De-En, respectively. We take Transformer (Vaswani et al., 2017) as the baseline and compare with other concurrent methods. Convolutional Transformer (Yang et al., 2019b) restricts the attention scope to a window of neighboring elements in order to model locality for self-attention model. Local Transformer (Yang et al., 2018) casts localness modeling as a learnable Gaussian bias, which indicates the central and scope of the local region to"
2021.naacl-main.135,D15-1044,0,0.145263,"Missing"
2021.naacl-main.135,P17-1099,0,0.221759,"IWSLT14 German-to-English (De-En) and WMT14 EnglishAutomatic summarization aims to produce a conto-German (En-De). IWSLT14 De-En dataset con- cise and fluent summary conveying the key inforsists of about 153K/7K/7K sentence pairs for train- mation in the input text. We focus on abstractive ing/validation/testing. WMT14 En-De dataset con- summarization, a generation task where the sumsists of about 4.5M sentence pairs, and the models mary is not limited in reusing the phrases or senwere validated on newstest2013 and examined on tences in the input text. We use the CNN/Daily newstest2014. Mail (See et al., 2017) and Gigaword (Rush et al., Our data processing follows Lu et al. (2019). 2015) for model evaluation. For IWSLT2014, we set our model into the small Following Song et al. (2019), we set the hidden one, the hidden size, embeddings and attention size, embeddings and attention heads to 768, 768, heads to 512, 512, and 4 respectively. For the and 12 respectively. Our model consists of a 6-layer WMT14 dataset, following the Transformer setting encoder and 6-layer decoder. For the convenience of Vaswani et al. (2017), we set our model into the of comparison, the training follows classic seq2seq base"
2021.naacl-main.135,N18-2074,0,0.445559,"rforms the original Transformer. 1 Introduction Recently, Transformer (Vaswani et al., 2017) has been widely applied in various natural language processing tasks, such as neural machine translation (Vaswani et al., 2017) and text summarization (Zhang et al., 2019). To further improve the performance of the text representation, Transformer-based variants have attracted a lot of attention (Lu et al., 2019; Sukhbaatar et al., 2019a,b; Bugliarello and Okazaki, 2019; Ma et al., 2020). Each building block of Transformer has two sublayers: Self-Attention Network (SAN) and Feed-Forward Network (FFN). Shaw et al. (2018) Figure 1: The mask matrices of (a) SAN, (b) DMAN and (c) FFN in Mask Attention Networks. Color that fades from black to white means the values in mask matrices decrease from 1 to 0. presents an extension to SAN which incorporates the relative positional information for the sequence. Sukhbaatar et al. (2019a) proposes attention span to control the maximum context size used in SAN and scales Transformer to long-range (∼ 8192 tokens) language modeling. Recently, some works targeting on FFN have been proposed. Lu et al. (2019) gives a new understanding of Transformer from a multi-particle dynamic"
2021.naacl-main.135,P19-1032,0,0.107927,"ial layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer. 1 Introduction Recently, Transformer (Vaswani et al., 2017) has been widely applied in various natural language processing tasks, such as neural machine translation (Vaswani et al., 2017) and text summarization (Zhang et al., 2019). To further improve the performance of the text representation, Transformer-based variants have attracted a lot of attention (Lu et al., 2019; Sukhbaatar et al., 2019a,b; Bugliarello and Okazaki, 2019; Ma et al., 2020). Each building block of Transformer has two sublayers: Self-Attention Network (SAN) and Feed-Forward Network (FFN). Shaw et al. (2018) Figure 1: The mask matrices of (a) SAN, (b) DMAN and (c) FFN in Mask Attention Networks. Color that fades from black to white means the values in mask matrices decrease from 1 to 0. presents an extension to SAN which incorporates the relative positional information for the sequence. Sukhbaatar et al. (2019a) proposes attention span to control the maximum context size used in SAN and scales Transformer to long"
2021.naacl-main.135,K16-1028,0,0.0242906,"n MANs. Under our design principles, there are three elements: FFN, SAN, and DMAN. For the convenience of comparison, we take FFN as the last component in the sequential layered structure. We try different collaboration methods and test them on IWSLT2014 German-to-English (De-En). The results are shown in the Table 3. We conclude that: Abstractive Summarization We use the F1 score of ROUGE (Lin and Hovy, 2003) as the evaluation metric1 . In Table 2, we compare our model against the baseline Transformer (Vaswani et al., 2017) and several generation models on CNN/Daily Mail and Gigaword. LEAD3 (Nallapati et al., 2016) extracts the first three sentences in a document as its summary. PTGEN+Converage (See et al., 2017) is a sequenceto-sequence model based on the pointer-generator network. As shown in Table 2, our model outperforms Transformer by 1.4 in ROUGE-1, 2.2 in 1 Further Analysis 1. Our proposed C#5 achieves the best performance that verify the effectiveness of our proposed sequential layered structure. 2. All of C#3, C#4 and C#5 outperform C#1 and C#2, and the least improvement in BLEU is 0.2. This shows that no matter what collaboration method, models with the participation of DMAN perform better tha"
2021.naacl-main.135,D18-1475,0,0.336421,"ng and capture the global 1692 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1692–1701 June 6–11, 2021. ©2021 Association for Computational Linguistics semantics. In contrast, mask of FFN disables it to perceive the information of other tokens and forces it into self-evolution. We believe that these two specialties endowed by two mask matrices make the success of Transformer in text representation. Although positive results of Transformer have been reported, recent works (Shaw et al., 2018; Yang et al., 2018; Guo et al., 2019) have shown that modeling localness would further improve the performance through experiments. We argue that deficiency of Transformer in local structure modeling is caused by the attention computation with static mask matrix. In the framework of MANs, we find a problem that irrelevant tokens with overlapping neighbors incorrectly attend to each other with relatively large attention scores. For example “a black dog jump to catch the frisbee”, though “catch” and “black” are neither relevant nor neighbors, for the reason that both of them are highly related to their common nei"
2021.naacl-main.135,N19-1407,0,0.0934995,"ild a distance-dependent then globalness, and take the step for self-evolution mask matrix SM. If each token only model the in the end. relationship with those tokens within b units of itself, we can set 3 Experiments  0, |t − s |&gt; b SM[t, s] = (11) In this section, we introduce our experiments. 1, |t − s |≤ b We first describe the experimental details in § 3.1. where t, s are the positions of query and key, and Then we show the experimental results in § 3.2. 1695 Model IWSLT14 De-En small params WMT14 En-De params big params base Transformer (Vaswani et al., 2017) Convolutional Transformer (Yang et al., 2019b) Weighted Transformer (Ahmed et al., 2017) Local Transformer (Yang et al., 2018) Relative Transformer (Shaw et al., 2018) Scaling NMT (Ott et al., 2018) Dynamic Conv (Wu et al., 2019) 34.4 35.2 36M - 27.3 28.2 28.4 28.5 26.8 - 62M 88M 65M 89M - 28.4 28.7 28.9 29.2 29.2 29.3 29.7 213M 213M 268M 213M 213M Ours 36.3 37M 29.1 63M 30.4 215M Table 1: Translation performance (BLEU) on IWSLT14 De-En and WMT14 En-De testsets. Finally we conduct the ablation study and analysis in § 4. 3.1 3.1.1 Experimental Setting Machine Translation big, base and small model with max-tokens 4096, 12288 and 8192 per"
2021.naacl-main.135,K19-1074,0,0.021388,"tion network (DMAN) with a learnable mask matrix which is able to model localness adaptively. To incorporate advantages of DMAN, SAN, and FFN, we propose a sequential layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer. 1 Introduction Recently, Transformer (Vaswani et al., 2017) has been widely applied in various natural language processing tasks, such as neural machine translation (Vaswani et al., 2017) and text summarization (Zhang et al., 2019). To further improve the performance of the text representation, Transformer-based variants have attracted a lot of attention (Lu et al., 2019; Sukhbaatar et al., 2019a,b; Bugliarello and Okazaki, 2019; Ma et al., 2020). Each building block of Transformer has two sublayers: Self-Attention Network (SAN) and Feed-Forward Network (FFN). Shaw et al. (2018) Figure 1: The mask matrices of (a) SAN, (b) DMAN and (c) FFN in Mask Attention Networks. Color that fades from black to white means the values in mask matrices decrease from 1 to 0. presents an extension to SAN which incorporates the relative po"
2021.naacl-main.431,P16-2085,0,0.0264799,"tation consists of two posts from change my view, a sub-forum of Reddit.com. Different types of underlines are used to highlight the interactive argument pairs. Introduction Arguments play a central role in decision making on social issues. Striving to automatically understand human arguments, computational argumentation becomes a growing field in natural language processing. It can be analyzed at two levels — monological argumentation and dialogical argumentation. Existing research on monological argumentation covers argument structure prediction (Stab and Gurevych, 2014), claims generation (Bilu and Slonim, 2016), essay scoring (Taghipour and Ng, 2016), etc. Recently, dialogical argumentation becomes an active topic. In the process of dialogical arguments, participants exchange arguments on a given topic (Asterhan and Schwarz, 2007; Hunter, 2013). With the popularity of online debating forums, large volume of dialogical arguments are daily formed, concerning wide range of topics. A social media dialogical argumentation example from ChangeMyView subreddit is shown in Figure 1. There we show two ∗ *Corresponding author posts holding opposite stances over the same topic. One is the original post and the"
2021.naacl-main.431,P16-1150,0,0.0235027,"ise ranking problem. The performance of different models are shown in Table 4. Note that we use the original CMV dataset and follow the previous setup in Tan et al. (2016); Ji et al. (2018). We find that our model outperforms the state-ofthe-art method (Ji et al., 2018) by a large margin, which indicates that our learned representation can well help downstream tasks. 6 Related Work In this section, we will introduce two major areas related to our work, which are dialogical argumentation and argument representation learning. 6.1 Dialogical Argumentation ing the quality of persuasive arguments (Habernal and Gurevych, 2016). Gottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performanc"
2021.naacl-main.431,W18-5211,0,0.0272452,"s reply. As can be seen, opinions from both sides are voiced with multiple arguments and the reply post B is organized in-line with post A’s arguments. Here we define an interactive argument pair formed with two arguments from both sides (with the same underline), which focuses on the same perspective of the discussion topic. The automatic identification of these pairs will be a fundamental step towards the understanding of dialogical argumentative structure. Moreover, it can benefit downstream tasks, such as debate summarization (Sanchan et al., 2017) and logical chain extraction in debates (Botschen et al., 2018). However, it is non-trivial to extract the interactive argument pairs holding opposite stances. Back to the example. Given argument b1 with only four words contained, it is difficult, without richer contextual information, to understand why it has interactive relationship with a1. In addition, without modeling the debating focuses of arguments, it is likely for models to wrongly predict that b2 has interactive relationship with a4 for sharing more words. Motivated by these observations, we pro5467 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computat"
2021.naacl-main.431,N16-1162,0,0.0465159,"Missing"
2021.naacl-main.431,K16-1002,0,0.0151145,"ention mechanism to learn sentence represening the relationship between arguments (Wang and tations (Wang et al., 2017a) and their relations with Cardie, 2014; Persing and Ng, 2017) and evaluat- others (Wang et al., 2017b). Our task is inherently 5474 different from theirs because our target arguments naturally occur in the complex interaction context of dialogues, which requires additional efforts for understanding the discourse structure therein. 6.2 Argument Representation Learning the encoder to encode a particular argument and then using the decoder to decode words in adjacent arguments. Bowman et al. (2016) introduce variational autoencoders to incorporate distributed latent representations of entire arguments. In addition, Hill et al. (2016) propose the FastSent model, using bag-of-words of arguments to predict the adjacent arguments. Logeswaran and Lee (2018) propose the Quick Thoughts to exploit the closeness of adjacent arguments. They formulate the argument representation learning as a classification problem. Argument representation learning for natural language has been studied widely in the past few years. Previous work discuss prior approaches to learning argument representations from la"
2021.naacl-main.431,J96-2004,0,0.875242,"Missing"
2021.naacl-main.431,D17-1070,0,0.0181352,"and Blunsom (2013) explore a language-specific contexts of arguments and induce latent representaencoder applied to each argument and represent tions via discrete variational autoencoders. Experithe argument by the mean vector of the words mental results on the dataset show that our model involved. They consider minimizing the inner significantly outperforms the competitive baselines. product between paired arguments in different Further analyses reveal why our model yields sulanguages as the training objective and do not perior performance and prove the usefulness of rely on word alignments. Conneau et al. (2017) discrete argument representations. propose a model called InferSent, which is used The future work will be carried out in two direcas the baseline as it served as the inspiration for tions. First, we will study the usage of our model the inclusion of the SNLI task in the multitask for applying to other dialogical argumentation remodel. They prove that NLI is an effective task lated tasks, such as debate summarization. Second, for pre-training and transfer learning in obtaining we will utilize neural topic model for learning disgeneric argument representations. They train crete argument repres"
2021.naacl-main.431,K17-1017,0,0.0196911,"the probability distribution of zi over K categories, which contains salient features of the argument on varying aspects. Therefore, we obtain the discrete argument representation by the posterior distribution of discrete latent variables z. R= M X Wei q(zi |x) (7) i=1 3.2 Argumentative Context Modeling Here, we introduce contextual information of the quotation and the reply to help identify the interactive argument pairs. The argumentative context contains a list of arguments. Following previous setting in Ji et al. (2018), we consider each sentence as an argument in the context. Inspired by Dong et al. (2017), we employ a hierarchical architecture to obtain argumentative context representations. Argument-level CNN. Given an argument and their embedding forms {e1 , e2 , ..., en }, we employ a convolution layer to incorporate the context information on word level. si = f (Ws · [ei : ei+ws−1 ] + bs ) where Ws and bs are weight matrix and bias vector. ws is the window size in the convolution layer and si is the feature representation. Then, we conduct an attention pooling operation over all the words to get argument embedding vectors. mi = tanh(Wm · si + bm ) eWu ·mi ui = P eWu ·mj (9) (10) j a= X ui"
2021.naacl-main.431,D13-1191,0,0.0328623,"rmance of different models are shown in Table 4. Note that we use the original CMV dataset and follow the previous setup in Tan et al. (2016); Ji et al. (2018). We find that our model outperforms the state-ofthe-art method (Ji et al., 2018) by a large margin, which indicates that our learned representation can well help downstream tasks. 6 Related Work In this section, we will introduce two major areas related to our work, which are dialogical argumentation and argument representation learning. 6.1 Dialogical Argumentation ing the quality of persuasive arguments (Habernal and Gurevych, 2016). Gottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014)"
2021.naacl-main.431,C18-1314,1,0.788169,"do you believe those aspects to be? ... Figure 2: An example illustrating the formation process of a quotation-reply pair in CMV. extract interactive argument pairs with the relation of quotation-reply. In general, the content of posts 2 Task Definition and Dataset Collection in CMV is informal, making it difficult to parse an In this section, we first define our task of inter- argument in a finer-grain with premise, conclusion and other components. Therefore, following previactive argument pair identification, followed by a description of how we collect the data for this task. ous setting in Ji et al. (2018), we treat each sentence as an argument. Specifically, we only consider the quotation containing one argument and view the 2.1 Task Definition first sentence after the quotation as the reply. We Given a argument q from the original post, a candi- treat the quotation-reply pairs extracted as posidate set of replies consisting of one positive reply tive samples and randomly select four replies from r+ , several negative replies r1− ∼ ru− , and their cor- other posts that are also related to the original post responding argumentative contexts, our goal is to to pair with the quotation as negative"
2021.naacl-main.431,P18-1040,0,0.0257888,"structure in texts. Recently, the dialogical argumen- teractions between two arguments in debate. Howtation has become an active topic. ever, there is limited research on the interactions Dialogical argumentation refers to a series of between posts. In this work, we propose a novel interactive arguments related to a given topic, in- task of identifying interactive argument pairs from volving argument retraction, view exchange, and argumentative posts to further understand the inso on. Existing research covers discourse struc- teractions between posts. Our work is also related ture prediction (Liu et al., 2018), dialog summa- with some similar tasks, such as question answering rization (Hsueh and Moore, 2007), etc. There are and sentence alignment. They focus on the design several attempts to address tasks related to analyz- of attention mechanism to learn sentence represening the relationship between arguments (Wang and tations (Wang et al., 2017a) and their relations with Cardie, 2014; Persing and Ng, 2017) and evaluat- others (Wang et al., 2017b). Our task is inherently 5474 different from theirs because our target arguments naturally occur in the complex interaction context of dialogues, which r"
2021.naacl-main.431,P11-1035,0,0.0212355,"find that our model outperforms the state-ofthe-art method (Ji et al., 2018) by a large margin, which indicates that our learned representation can well help downstream tasks. 6 Related Work In this section, we will introduce two major areas related to our work, which are dialogical argumentation and argument representation learning. 6.1 Dialogical Argumentation ing the quality of persuasive arguments (Habernal and Gurevych, 2016). Gottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014) create a dispute corpus from Wikipedia and use a sentiment analysis to predict the dispute label of arguments. Wei et al. (2016) collect a dataset from CMV an"
2021.naacl-main.431,D14-1162,0,0.0856575,"Missing"
2021.naacl-main.431,sanchan-etal-2017-automatic,0,0.0177199,"s over the same topic. One is the original post and the other is reply. As can be seen, opinions from both sides are voiced with multiple arguments and the reply post B is organized in-line with post A’s arguments. Here we define an interactive argument pair formed with two arguments from both sides (with the same underline), which focuses on the same perspective of the discussion topic. The automatic identification of these pairs will be a fundamental step towards the understanding of dialogical argumentative structure. Moreover, it can benefit downstream tasks, such as debate summarization (Sanchan et al., 2017) and logical chain extraction in debates (Botschen et al., 2018). However, it is non-trivial to extract the interactive argument pairs holding opposite stances. Back to the example. Given argument b1 with only four words contained, it is difficult, without richer contextual information, to understand why it has interactive relationship with a1. In addition, without modeling the debating focuses of arguments, it is likely for models to wrongly predict that b2 has interactive relationship with a4 for sharing more words. Motivated by these observations, we pro5467 Proceedings of the 2021 Conferen"
2021.naacl-main.431,P15-1012,0,0.0276404,"ion can well help downstream tasks. 6 Related Work In this section, we will introduce two major areas related to our work, which are dialogical argumentation and argument representation learning. 6.1 Dialogical Argumentation ing the quality of persuasive arguments (Habernal and Gurevych, 2016). Gottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014) create a dispute corpus from Wikipedia and use a sentiment analysis to predict the dispute label of arguments. Wei et al. (2016) collect a dataset from CMV and analyze the correlation between disputing quality and disputation behaviors. analyze the disputation action in the online debate. Given an or"
2021.naacl-main.431,D14-1006,0,0.0284413,"1 Figure 1: An example of dialogical argumentation consists of two posts from change my view, a sub-forum of Reddit.com. Different types of underlines are used to highlight the interactive argument pairs. Introduction Arguments play a central role in decision making on social issues. Striving to automatically understand human arguments, computational argumentation becomes a growing field in natural language processing. It can be analyzed at two levels — monological argumentation and dialogical argumentation. Existing research on monological argumentation covers argument structure prediction (Stab and Gurevych, 2014), claims generation (Bilu and Slonim, 2016), essay scoring (Taghipour and Ng, 2016), etc. Recently, dialogical argumentation becomes an active topic. In the process of dialogical arguments, participants exchange arguments on a given topic (Asterhan and Schwarz, 2007; Hunter, 2013). With the popularity of online debating forums, large volume of dialogical arguments are daily formed, concerning wide range of topics. A social media dialogical argumentation example from ChangeMyView subreddit is shown in Figure 1. There we show two ∗ *Corresponding author posts holding opposite stances over the sa"
2021.naacl-main.431,D16-1193,0,0.0203612,"e my view, a sub-forum of Reddit.com. Different types of underlines are used to highlight the interactive argument pairs. Introduction Arguments play a central role in decision making on social issues. Striving to automatically understand human arguments, computational argumentation becomes a growing field in natural language processing. It can be analyzed at two levels — monological argumentation and dialogical argumentation. Existing research on monological argumentation covers argument structure prediction (Stab and Gurevych, 2014), claims generation (Bilu and Slonim, 2016), essay scoring (Taghipour and Ng, 2016), etc. Recently, dialogical argumentation becomes an active topic. In the process of dialogical arguments, participants exchange arguments on a given topic (Asterhan and Schwarz, 2007; Hunter, 2013). With the popularity of online debating forums, large volume of dialogical arguments are daily formed, concerning wide range of topics. A social media dialogical argumentation example from ChangeMyView subreddit is shown in Figure 1. There we show two ∗ *Corresponding author posts holding opposite stances over the same topic. One is the original post and the other is reply. As can be seen, opinions"
2021.naacl-main.431,P14-2113,0,0.0260124,"ottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014) create a dispute corpus from Wikipedia and use a sentiment analysis to predict the dispute label of arguments. Wei et al. (2016) collect a dataset from CMV and analyze the correlation between disputing quality and disputation behaviors. analyze the disputation action in the online debate. Given an original argument and an argument disputing it, they aims to evaluate the quality of a disputing comment based on the original argument and the discussed topic. Habernal and Gurevych (2016) crowdsource the UKPConvArg1 corpus to study what makes an informal social media argument convincing. They crow"
2021.naacl-main.431,S18-1073,0,0.0409519,"Missing"
2021.naacl-main.431,P17-1018,0,0.0168005,"identifying interactive argument pairs from volving argument retraction, view exchange, and argumentative posts to further understand the inso on. Existing research covers discourse struc- teractions between posts. Our work is also related ture prediction (Liu et al., 2018), dialog summa- with some similar tasks, such as question answering rization (Hsueh and Moore, 2007), etc. There are and sentence alignment. They focus on the design several attempts to address tasks related to analyz- of attention mechanism to learn sentence represening the relationship between arguments (Wang and tations (Wang et al., 2017a) and their relations with Cardie, 2014; Persing and Ng, 2017) and evaluat- others (Wang et al., 2017b). Our task is inherently 5474 different from theirs because our target arguments naturally occur in the complex interaction context of dialogues, which requires additional efforts for understanding the discourse structure therein. 6.2 Argument Representation Learning the encoder to encode a particular argument and then using the decoder to decode words in adjacent arguments. Bowman et al. (2016) introduce variational autoencoders to incorporate distributed latent representations of entire ar"
2021.naacl-main.431,W16-2820,1,0.80987,"arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014) create a dispute corpus from Wikipedia and use a sentiment analysis to predict the dispute label of arguments. Wei et al. (2016) collect a dataset from CMV and analyze the correlation between disputing quality and disputation behaviors. analyze the disputation action in the online debate. Given an original argument and an argument disputing it, they aims to evaluate the quality of a disputing comment based on the original argument and the discussed topic. Habernal and Gurevych (2016) crowdsource the UKPConvArg1 corpus to study what makes an informal social media argument convincing. They crowdsource the UKPConvArg1 corpus and use SVM and bidirectional LSTM to experiment on their annotated datasets. Tan et al. (2016) pa"
2021.naacl-main.431,P17-1190,0,0.038311,"Missing"
2021.naacl-main.431,D17-1026,0,0.0153416,"ork discuss prior approaches to learning argument representations from labelled and unlabelled data. There have been attempts to use laPrevious work focuses on learning continuous arbeled/structured data to learn argument repgument representations with no interpretability. In resentations. Wieting et al. (2016) and Wieting this work, we study the discrete argument represenand Gimpel (2017) introduce a large sentential tations, capturing varying aspects in argumentation paraphrase dataset and use paraphrase data to languages. learn an encoder that maps synonymous phrases to similar embeddings. Wieting et al. (2017) explore the use of machine translation to obtain more 7 Conclusion and Future Work paraphrase data via back-translation of bilingual argument pairs for learning paraphrastic embed- In this paper, we propose a novel task of interactive dings. They show how neural backtranslation argument pair identification from two posts with could be used to generate paraphrases. Hermann opposite stances on a certain topic. We examine and Blunsom (2013) explore a language-specific contexts of arguments and induce latent representaencoder applied to each argument and represent tions via discrete variational a"
2021.naacl-main.431,P18-1101,0,0.0232342,"the two WOF (Ji et al., 2018), the state-of-the-art model to loss terms. The first loss term is defined on the evaluate the persuasiveness of argumentative comDVAE and cross entropy loss is defined as the re- ments, which is tailored to fit our task. In addition, construction loss. We apply the regularization on we compare with some ablations to study the contriKL cost term to solve posterior collapse issue. Due bution from our components. Here we first consider to the space limitation, we leave out the derivation M ATCHrnn , which uses BiGRU to learn argument details and refer the readers to Zhao et al. (2018). representations and explore the match of arguments LDV AE = Eq(z|x) [log p(x|z)]−KL(q(z|x)||p(z)) without modeling the context therein. Then we (19) compare with other ablations that adopt varying The second loss term is defined on the argument argument context modeling methods. Here we conmatching. We formalize this issue as a ranking task sider BiGRU (henceforth M ATCHrnn +Cb ), which 5471 3.4 Joint Learning P@1 MRR 28.36* 28.70* 51.66* 52.03* 31.26* 52.97* 56.04* 73.03* 0.60 0.59 0.58 0.57 0 51.52* 55.98* 57.46* 58.27‡ 58.61‡ 61.17 70.57* 73.20* 73.72* 74.16* 74.66‡ 76.16 Table 2: The per"
C12-2138,al-saif-markert-2010-leeds,0,0.186853,"Missing"
C12-2138,D11-1068,0,0.0230127,"Missing"
C12-2138,W01-1605,0,0.519818,"Missing"
C12-2138,C10-3004,0,0.0258654,"Missing"
C12-2138,W11-0401,0,0.0653223,"Missing"
C12-2138,miltsakaki-etal-2004-penn,0,0.159515,"Missing"
C12-2138,J03-1002,0,0.00537512,"Missing"
C12-2138,P09-2004,0,0.133057,"Missing"
C12-2138,C08-2022,0,0.0926301,"Missing"
C12-2138,prasad-etal-2008-penn,0,0.615867,"Missing"
C12-2138,I08-7010,0,0.1145,"Missing"
C12-2138,W05-0312,0,0.515718,"Missing"
C12-2138,D11-1015,1,0.868,"Missing"
C12-2138,P12-1008,0,0.376064,"Missing"
C14-1083,W97-0703,0,0.0978216,"Missing"
C14-1083,C12-1047,0,0.0204472,"d representative description of a target event. Sharifi et al. (2010) proposed a graph-based phrase reinforcement 873 algorithm (PRA) to generate a one-sentence summary from a collection of tweets. By using linguistic features, Judd and Kalita (2013) improved the performance of PRA. Sharifi et al. (2010) and Inouye et al. (2011) presented a hybrid TF-IDF approach for extracting tweets with the presence of important terms. More fine-grained summarization was proposed by considering sub-events and combining the summaries extracted from each sub-topic (Nichols et al., 2012; Zubiaga et al., 2012; Duan et al., 2012). The research for coupling news and microblogs attracted much attention recently. Suba˘si´c and Berendt (2011) and Zhao et al. (2011) independently compared tweets to online news to identify features for news detection in tweets. Phelan et al. (2011) used tweets to recommend news articles based on user preferences. Gao et al. (2012) produced cross-media news summaries by capturing the complemen˘ tary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Guo et al. (2013) proposed"
C14-1083,P13-1024,0,0.0761818,"., 2012; Duan et al., 2012). The research for coupling news and microblogs attracted much attention recently. Suba˘si´c and Berendt (2011) and Zhao et al. (2011) independently compared tweets to online news to identify features for news detection in tweets. Phelan et al. (2011) used tweets to recommend news articles based on user preferences. Gao et al. (2012) produced cross-media news summaries by capturing the complemen˘ tary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Guo et al. (2013) proposed a graphical model to identify news for a given tweet to provide contextual support for NLP tasks. Some work attempted to use different kinds of resources to help document summarization, such as Wikipedia and query log of search engine (Svore et al., 2007), clickthrough data (Sun et al., 2005), users’ comments on news (Hu et al., 2008), and social media context of the articles (Yang et al., 2011). Our work is closely related to Svore et al. (2007) that considered incorporating third-party resource in the ranking process, but the access to query logs is extremely limited, and Wikipedia"
C14-1083,P11-1038,0,0.0290498,"Missing"
C14-1083,D13-1158,0,0.00444978,"ds outperform some strong baselines for single-document summarization. 2 Related Work Our work intersects the summarization of single document and microblogs. Single-document summarization has been studied for years starting from Luhn and Peter (1958). Based on local content information of a document (Wong et al., 2008; Barzilay et al., 1997; Marcu, 1997), researchers proposed various statistical or semantic approaches using classification (Wong et al., 2008), Integer Linear Programming (ILP) (Li et al., 2013), sequential models (Shen et al., 2007) and graphical models (Litvak and Last, 2008; Hirao et al., 2013). For the concision of summary, sentence compression or word deletion was used (Knight and Marcu, 2002) for preprocessing. Joint models combining compression and selection of sentences were also studied (Woodsend and Lapata, 2010; Li et al., 2013). Summarizing microblog content is to distill the large quantities of tweets into a concise and representative description of a target event. Sharifi et al. (2010) proposed a graph-based phrase reinforcement 873 algorithm (PRA) to generate a one-sentence summary from a collection of tweets. By using linguistic features, Judd and Kalita (2013) improved"
C14-1083,N13-1047,0,0.0192019,"Last, 2008; Hirao et al., 2013). For the concision of summary, sentence compression or word deletion was used (Knight and Marcu, 2002) for preprocessing. Joint models combining compression and selection of sentences were also studied (Woodsend and Lapata, 2010; Li et al., 2013). Summarizing microblog content is to distill the large quantities of tweets into a concise and representative description of a target event. Sharifi et al. (2010) proposed a graph-based phrase reinforcement 873 algorithm (PRA) to generate a one-sentence summary from a collection of tweets. By using linguistic features, Judd and Kalita (2013) improved the performance of PRA. Sharifi et al. (2010) and Inouye et al. (2011) presented a hybrid TF-IDF approach for extracting tweets with the presence of important terms. More fine-grained summarization was proposed by considering sub-events and combining the summaries extracted from each sub-topic (Nichols et al., 2012; Zubiaga et al., 2012; Duan et al., 2012). The research for coupling news and microblogs attracted much attention recently. Suba˘si´c and Berendt (2011) and Zhao et al. (2011) independently compared tweets to online news to identify features for news detection in tweets. P"
C14-1083,P13-1099,0,0.0236889,"s-tweets pairing corpus, the results of experiments following both directions indicate that our methods outperform some strong baselines for single-document summarization. 2 Related Work Our work intersects the summarization of single document and microblogs. Single-document summarization has been studied for years starting from Luhn and Peter (1958). Based on local content information of a document (Wong et al., 2008; Barzilay et al., 1997; Marcu, 1997), researchers proposed various statistical or semantic approaches using classification (Wong et al., 2008), Integer Linear Programming (ILP) (Li et al., 2013), sequential models (Shen et al., 2007) and graphical models (Litvak and Last, 2008; Hirao et al., 2013). For the concision of summary, sentence compression or word deletion was used (Knight and Marcu, 2002) for preprocessing. Joint models combining compression and selection of sentences were also studied (Woodsend and Lapata, 2010; Li et al., 2013). Summarizing microblog content is to distill the large quantities of tweets into a concise and representative description of a target event. Sharifi et al. (2010) proposed a graph-based phrase reinforcement 873 algorithm (PRA) to generate a one-sen"
C14-1083,W04-1013,0,0.0470301,"Missing"
C14-1083,W08-1404,0,0.0295271,"indicate that our methods outperform some strong baselines for single-document summarization. 2 Related Work Our work intersects the summarization of single document and microblogs. Single-document summarization has been studied for years starting from Luhn and Peter (1958). Based on local content information of a document (Wong et al., 2008; Barzilay et al., 1997; Marcu, 1997), researchers proposed various statistical or semantic approaches using classification (Wong et al., 2008), Integer Linear Programming (ILP) (Li et al., 2013), sequential models (Shen et al., 2007) and graphical models (Litvak and Last, 2008; Hirao et al., 2013). For the concision of summary, sentence compression or word deletion was used (Knight and Marcu, 2002) for preprocessing. Joint models combining compression and selection of sentences were also studied (Woodsend and Lapata, 2010; Li et al., 2013). Summarizing microblog content is to distill the large quantities of tweets into a concise and representative description of a target event. Sharifi et al. (2010) proposed a graph-based phrase reinforcement 873 algorithm (PRA) to generate a one-sentence summary from a collection of tweets. By using linguistic features, Judd and K"
C14-1083,W97-0713,0,0.235689,"we extract top-ranked tweets (with the help of news sentences) as a substitute of sentences extraction since they are typically shorter. Based on our news-tweets pairing corpus, the results of experiments following both directions indicate that our methods outperform some strong baselines for single-document summarization. 2 Related Work Our work intersects the summarization of single document and microblogs. Single-document summarization has been studied for years starting from Luhn and Peter (1958). Based on local content information of a document (Wong et al., 2008; Barzilay et al., 1997; Marcu, 1997), researchers proposed various statistical or semantic approaches using classification (Wong et al., 2008), Integer Linear Programming (ILP) (Li et al., 2013), sequential models (Shen et al., 2007) and graphical models (Litvak and Last, 2008; Hirao et al., 2013). For the concision of summary, sentence compression or word deletion was used (Knight and Marcu, 2002) for preprocessing. Joint models combining compression and selection of sentences were also studied (Woodsend and Lapata, 2010; Li et al., 2013). Summarizing microblog content is to distill the large quantities of tweets into a concise"
C14-1083,D07-1047,0,0.85741,"n et al. (2011) used tweets to recommend news articles based on user preferences. Gao et al. (2012) produced cross-media news summaries by capturing the complemen˘ tary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Guo et al. (2013) proposed a graphical model to identify news for a given tweet to provide contextual support for NLP tasks. Some work attempted to use different kinds of resources to help document summarization, such as Wikipedia and query log of search engine (Svore et al., 2007), clickthrough data (Sun et al., 2005), users’ comments on news (Hu et al., 2008), and social media context of the articles (Yang et al., 2011). Our work is closely related to Svore et al. (2007) that considered incorporating third-party resource in the ranking process, but the access to query logs is extremely limited, and Wikipedia content is relatively static which cannot reflect timely information like social media. We also share the same testbed with Woodsend and Lapata (2010). They selected and compressed news sentences with a joint model using ILP by considering phrase as basic extract"
C14-1083,C08-1124,0,0.0513353,"f news sentences for extraction; secondly, we extract top-ranked tweets (with the help of news sentences) as a substitute of sentences extraction since they are typically shorter. Based on our news-tweets pairing corpus, the results of experiments following both directions indicate that our methods outperform some strong baselines for single-document summarization. 2 Related Work Our work intersects the summarization of single document and microblogs. Single-document summarization has been studied for years starting from Luhn and Peter (1958). Based on local content information of a document (Wong et al., 2008; Barzilay et al., 1997; Marcu, 1997), researchers proposed various statistical or semantic approaches using classification (Wong et al., 2008), Integer Linear Programming (ILP) (Li et al., 2013), sequential models (Shen et al., 2007) and graphical models (Litvak and Last, 2008; Hirao et al., 2013). For the concision of summary, sentence compression or word deletion was used (Knight and Marcu, 2002) for preprocessing. Joint models combining compression and selection of sentences were also studied (Woodsend and Lapata, 2010; Li et al., 2013). Summarizing microblog content is to distill the larg"
C14-1083,P10-1058,0,0.0882413,"in Figure 1 (marked in red rectangle) that are written in a compact, almost telegraphic style. In contrast to the original content of the article, significant compression is obtained by shortening and paraphrasing. Unfortunately, the production of such good-quality highlights needs to be done manually which is very expensive. Existing methods face grand technical challenges for automating the process. The task is complex in nature due to a broad range of linguistic constraints which ultimately requires wide-coverage of language understanding beyond the capabilities of current NLP technology (Woodsend and Lapata, 2010). Most automatic systems simplify the problem using extractive approach. By using linguistic or statistical information or both, the key units or concepts can be identified from sentences or across multiple documents, and then the sentences are scored and extracted according to their informativeness with the presence of the key components. The extractive approach has two salient problems: (1) it is commonly ineffective to locate key sentences, meaning that the presence of linguistically and/or statistically important units does not necessarily indicate a highlight sentence. This is evidenced b"
C16-1054,C12-1029,0,0.0193289,"d rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and"
C16-1054,W08-1105,0,0.0909924,"Missing"
C16-1054,C12-1056,0,0.0179621,"t al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compress"
C16-1054,D14-1076,1,0.850198,"n summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for joint sentence compression and summarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user"
C16-1054,N15-1145,1,0.827258,"related to the following aspects: ILP based summarization method, dependency tree based sentence compression by considering extra information, and mining social media for document summarization. Recently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post"
C16-1054,N15-1079,1,0.837783,"related to the following aspects: ILP based summarization method, dependency tree based sentence compression by considering extra information, and mining social media for document summarization. Recently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post"
C16-1054,N10-1134,0,0.0299569,"s kind. 2 Related Work Our work is closely related to the following aspects: ILP based summarization method, dependency tree based sentence compression by considering extra information, and mining social media for document summarization. Recently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summa"
C16-1054,W04-1013,0,0.0084025,"Missing"
C16-1054,W09-1801,0,0.0273222,"mance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for joint sentence compression and summarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used"
C16-1054,D07-1047,0,0.024065,"Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user preferences. (Gao et al., 2012) produced cross-media news summaries by capturing the ˇ complementary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Wei and Gao (2014) derived external features based on a collection of relevant tweets to assist the ranking of the original sentences for highlight generation. In addition to tweets, Svore et al. (2007) leveraged Wikipedia and query log of search engines to help document summarization. Tsukamoto et al. (2015) proposed a method for efficiently collecting posts that are only implicitly related to an announcement post, taking into account retweets on Twitter in particular. Our work involves the two aspects when using post information: one is that we utilize post information to help choose sentences from new articles and compress them to form a summary, and the other is that we directly use sentences from the posts as the summary. 3 Corpus Construction For our work, we manually collected popular"
C16-1054,P13-1136,0,0.0210716,"arization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for j"
C16-1054,C14-1083,1,0.847348,"mmarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user preferences. (Gao et al., 2012) produced cross-media news summaries by capturing the ˇ complementary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Wei and Gao (2014) derived external features based on a collection of relevant tweets to assist the ranking of the original sentences for highlight generation. In addition to tweets, Svore et al. (2007) leveraged Wikipedia and query log of search engines to help document summarization. Tsukamoto et al. (2015) proposed a method for efficiently collecting posts that are only implicitly related to an announcement post, taking into account retweets on Twitter in particular. Our work involves the two aspects when using post information: one is that we utilize post information to help choose sentences from new articl"
C16-1054,C08-1124,0,0.0362832,"ttention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for joint sentence compression and summarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user preferences. (Gao et al., 2012) produced cross-media news summaries by capturing the ˇ complementary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Wei and Gao (2014) derived external features based on a collection of relevant tweets to assist the ranking of the original sentences for highlight generation. In addition to tweets, Svore et al. (2007) leveraged Wikipedia and query log of search engines"
C16-1054,D12-1022,0,0.0161237,"ently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of"
C16-1054,P11-1049,0,\N,Missing
C18-1150,W05-0909,0,0.240213,", and we compress it to a 300-dimension vector using 2-layer fully-connected layer. The upper settings are the same for all neural network models. We set batch size, rollout size, D1-step, G1-step, G2-step and G3-step as 64, 16, 5, 1, 2 and 1, respectively. γ for the training of D2 is set to 2.0 empirically. 3.4 Automatic Evaluation There is no direct evaluation metric to determine whether a question is natural or not. We thus use several relevance scores for the automatic evaluation following the setting of existing researches, including Corpus BLEU-4, BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr (Vedantam et al., 2015). The overall experiment results in terms of five relevance scores are shown in Table 1. We have some findings as follows: 1 The original dataset contains three parts, namely, MS-COCO, Flickr and Bing, VQG-MSCOCO. However, images from Flickr and Bing are quite different from those in Visual Question Answering (VQA) dataset (Antol et al., 2015), and this makes it difficult to use questions from VQA as negative samples to train D2 . 1768 Model BLEU-4 KNN Img2Seq Img2Seqpre−train MIXER-BLEU-4 ReinforceD1 ReinforceD2 ReinforceD1 +D2 37."
C18-1150,D14-1179,0,0.0420663,"Missing"
C18-1150,P17-1123,0,0.0666076,"is paper locates in the research filed of question generation and reinforcement learning for sequence generation. We will focus on related works from these two domains. Question Generation Question generation has been researched for years from textual input (Rus et al., 2010; Heilman, 2011). Researchers start from rule-based method that extracts key aspects from the input text and then insert these aspects into human generated templates for interrogative sentence generation (Heilman, 2011). Recently, sequence-to-sequence model is utilized for question generation in description-question pairs (Du et al., 2017; Tang et al., 2017; Serban et al., 2016). Although these models generate better performance, the characteristics of question is still ignored. On the other hand, research about visual question generation is much less (Ren et al., 2015; Vijayakumar et al., 2018; 1770 Mostafazadeh et al., 2016; Shijie et al., 2017). Diversity as another important characteristic of question also draws much attention. Li et al. (2016) proposed to use Maximum Mutual Information (MMI) as the objective function for result diversification. Vijayakumar et al. (2018) proposed a diverse beam search for generated multipl"
C18-1150,N16-1014,0,0.0721589,"Missing"
C18-1150,N03-1020,0,0.326693,"nsion vector using 2-layer fully-connected layer. The upper settings are the same for all neural network models. We set batch size, rollout size, D1-step, G1-step, G2-step and G3-step as 64, 16, 5, 1, 2 and 1, respectively. γ for the training of D2 is set to 2.0 empirically. 3.4 Automatic Evaluation There is no direct evaluation metric to determine whether a question is natural or not. We thus use several relevance scores for the automatic evaluation following the setting of existing researches, including Corpus BLEU-4, BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr (Vedantam et al., 2015). The overall experiment results in terms of five relevance scores are shown in Table 1. We have some findings as follows: 1 The original dataset contains three parts, namely, MS-COCO, Flickr and Bing, VQG-MSCOCO. However, images from Flickr and Bing are quite different from those in Visual Question Answering (VQA) dataset (Antol et al., 2015), and this makes it difficult to use questions from VQA as negative samples to train D2 . 1768 Model BLEU-4 KNN Img2Seq Img2Seqpre−train MIXER-BLEU-4 ReinforceD1 ReinforceD2 ReinforceD1 +D2 37.062 36.744 37.522 41.674 38."
C18-1150,P16-1170,0,0.0939854,"Missing"
C18-1150,P02-1040,0,0.100776,"riginal dimension of fc7 is 4096, and we compress it to a 300-dimension vector using 2-layer fully-connected layer. The upper settings are the same for all neural network models. We set batch size, rollout size, D1-step, G1-step, G2-step and G3-step as 64, 16, 5, 1, 2 and 1, respectively. γ for the training of D2 is set to 2.0 empirically. 3.4 Automatic Evaluation There is no direct evaluation metric to determine whether a question is natural or not. We thus use several relevance scores for the automatic evaluation following the setting of existing researches, including Corpus BLEU-4, BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr (Vedantam et al., 2015). The overall experiment results in terms of five relevance scores are shown in Table 1. We have some findings as follows: 1 The original dataset contains three parts, namely, MS-COCO, Flickr and Bing, VQG-MSCOCO. However, images from Flickr and Bing are quite different from those in Visual Question Answering (VQA) dataset (Antol et al., 2015), and this makes it difficult to use questions from VQA as negative samples to train D2 . 1768 Model BLEU-4 KNN Img2Seq Img2Seqpre−train MIXER-BLEU-4 Reinforc"
C18-1150,D14-1162,0,0.0820416,"rial learning network to better train the generator under the reinforcement learning framework. - ReinforceD2 : it uses the score of D2 as the reward to guide the training of the generator. This model is comparable to MIXER-BLEU-4 because both models utilize a static way to produce reward (BLEU score with ground truth questions in MIXER-BLEU-4 and classification confidence in ReinforceD2 ) - ReinforceD1 +D2 : this is our proposed model. 3.3 Training Details For the generator, we use GRU cell and the number of cells is 512; the dimension of word embedding is 300 and is pre-trained using GloVe (Pennington et al., 2014). The image feature, fc7 is the output of the 7th fully-connected layer in VGGNet. The original dimension of fc7 is 4096, and we compress it to a 300-dimension vector using 2-layer fully-connected layer. The upper settings are the same for all neural network models. We set batch size, rollout size, D1-step, G1-step, G2-step and G3-step as 64, 16, 5, 1, 2 and 1, respectively. γ for the training of D2 is set to 2.0 empirically. 3.4 Automatic Evaluation There is no direct evaluation metric to determine whether a question is natural or not. We thus use several relevance scores for the automatic ev"
C18-1150,W10-4234,0,0.035598,"iddle-level. This indicates that optimizing a single evaluation metric is not sufficient enough for generating high quality natural questions. - The gap between ground-truth questions and machine generated questions is still large. This indicates that there is still a large room for question generation system to improve. 4 Related Works This paper locates in the research filed of question generation and reinforcement learning for sequence generation. We will focus on related works from these two domains. Question Generation Question generation has been researched for years from textual input (Rus et al., 2010; Heilman, 2011). Researchers start from rule-based method that extracts key aspects from the input text and then insert these aspects into human generated templates for interrogative sentence generation (Heilman, 2011). Recently, sequence-to-sequence model is utilized for question generation in description-question pairs (Du et al., 2017; Tang et al., 2017; Serban et al., 2016). Although these models generate better performance, the characteristics of question is still ignored. On the other hand, research about visual question generation is much less (Ren et al., 2015; Vijayakumar et al., 201"
C18-1150,P16-1056,0,0.080342,"Missing"
C18-1150,D17-1090,0,0.0513244,"in the research filed of question generation and reinforcement learning for sequence generation. We will focus on related works from these two domains. Question Generation Question generation has been researched for years from textual input (Rus et al., 2010; Heilman, 2011). Researchers start from rule-based method that extracts key aspects from the input text and then insert these aspects into human generated templates for interrogative sentence generation (Heilman, 2011). Recently, sequence-to-sequence model is utilized for question generation in description-question pairs (Du et al., 2017; Tang et al., 2017; Serban et al., 2016). Although these models generate better performance, the characteristics of question is still ignored. On the other hand, research about visual question generation is much less (Ren et al., 2015; Vijayakumar et al., 2018; 1770 Mostafazadeh et al., 2016; Shijie et al., 2017). Diversity as another important characteristic of question also draws much attention. Li et al. (2016) proposed to use Maximum Mutual Information (MMI) as the objective function for result diversification. Vijayakumar et al. (2018) proposed a diverse beam search for generated multiple questions. Fan et"
C18-1314,N16-1165,0,0.0352315,"Missing"
C18-1314,D16-1053,0,0.0182811,"lignment matrix based on two inputs, which can model complex interactions between the two inputs. Xiong et al. (2016) present a co-attention encoder to focus on relevant parts of the representations of the question and document and use a dynamic pointing decoder to locate the answer. Cui et al. (2016) propose a two-way attention mechanism to encode the passage and question mutually and induce attended attention for final answer predictions. Self-attention mechanism is an attention mechanism aiming at aligning the sequence with itself, which has been successfully used in a variety of tasks. In Cheng et al. (2016), both encoder and decoder are modeled as LSTMs with self-attention for extractive summarization of documents. In Lin et al. (2017), the authors conduct a self-attention over the hidden states of a BiLSTM to extract the sentence embedding. Instead of sentence vector, they use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. In this work, we employ a co-attention mechanism to capture the interactions between the original post and the reply on argument level. What’s more, we use a self-attention mechanism to obtain the argument r"
C18-1314,W14-4012,0,0.0128942,"Missing"
C18-1314,K17-1017,0,0.0774691,"Matrix Alignment Matrix Post to reply argument attention + Weights GRU GRU GRU u OP Argument Vector Original Post Softmax r1R r1O P uOP GRU r r ... r Co-Attention Network Attention Post argument to reply argument attention R m OP 2 n {U*i }i=1 Co-Attention Network r ... R 2 r1O P Softmax Aggregation Network r X feat O* Attention Pooling R 1 r1R Original Post Reply ... r2R rmR Reply Figure 2: Overall architecture of the proposed model. The left part is the main framework of this work. The right part is the detailed structure of the co-attention network. 2.1 Argument Representation Inspired by Dong et al. (2017), we employ a hierarchical architecture to obtain two different representations for each single argument. For simplicity, we consider each sentence as an argument. Representation based on internal words given an argument with words w1 , w2 , ..., wT , we first map each word to a dense vector obtaining x1 , x2 , ..., xT correspondingly. We then employ a convolution layer to incorporate the context information on word level. zi = f (Wz · [xi : xi+hw −1 ] + bz ) (1) where Wz and bz are weight matrix and bias vector. hw is the window size in the convolution layer and zi is the feature representati"
C18-1314,C14-1089,0,0.0293406,"results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a numeric score, considering features of content, grammar, d"
C18-1314,D16-1129,0,0.350129,"nt, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First, viewpoints of the original poster and repliers"
C18-1314,P16-1150,0,0.458692,"nt, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First, viewpoints of the original poster and repliers"
C18-1314,P15-1107,0,0.0352229,"xt similarity. The interactions among argument pairs are ignored. In this work, we evaluate the quality of debate comments through the interactions among them on argument level. 5.2 Attention mechanism Attention mechanism allows models to focus on specific parts of inputs at each step of a task. Moreover, attention mechanism has been proved to be significantly effective in natural language processing tasks. Co-attention mechanism has recently attracted lots of research interest in the fields of machine translation (Bahdanau et al., 2014), question answering (Wu et al., 2017), text generation (Li et al., 2015), etc. It is computed as an alignment matrix based on two inputs, which can model complex interactions between the two inputs. Xiong et al. (2016) present a co-attention encoder to focus on relevant parts of the representations of the question and document and use a dynamic pointing decoder to locate the answer. Cui et al. (2016) propose a two-way attention mechanism to encode the passage and question mutually and induce attended attention for final answer predictions. Self-attention mechanism is an attention mechanism aiming at aligning the sequence with itself, which has been successfully us"
C18-1314,D15-1110,0,0.0767786,"network to capture the interactions between participants on argument level. Experimental results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as"
C18-1314,D14-1162,0,0.0817283,"as a ranking task and utilize a pairwise hinge loss for training. Given a triple (OP, R+ , R− ), where R+ and R− respectively denote the positive and the negative reply for OP . The loss function is defined in Equation 18. L = max(0, 1 − S(OP, R+ ) + S(OP, R− )) (18) where S(OP, R+ ) and S(OP, R− ) are the corresponding persuasiveness scores. The model is trained by stochastic gradient descent on 105 epochs, and evaluated on the development set at every epoch to select the best model. Dropout (Srivastava et al., 2014) has proved to be an effective method and is used in our work. We use Glove (Pennington et al., 2014) word embeddings, which are 50-dimension word vectors trained with a crawled large corpus with 840 billion tokens. Embeddings for words not present are randomly initialized with sampled numbers from a uniform distribution [0.25,0.25]. We set initial learning rate to 0.1, batch size to 20, filter sizes to 5, filter numbers to 100 and the hidden unit of BiGRU to 200. Early stopping was used with a patience of 15 epochs. We implemented our model using TensorFlow. The model converged in 23 hours on an NVIDIA Titan X machine. 3707 Original post Positive reply Negative reply Avew 10 10 10 Training S"
C18-1314,D14-1006,0,0.0506881,"eractions between participants on argument level. Experimental results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a n"
C18-1314,D16-1193,0,0.0383443,"tructure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a numeric score, considering features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality"
C18-1314,E17-1017,0,0.0698165,"and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a numeric score, considering features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on ("
C18-1314,Q17-1016,0,0.22711,"all of the attention representations in a linear function to get the integrated information. The detail is illustrated in Equation 12. n m U = f (U 1 , U 2 , U 3 , {riOP }i=1 , {rjR }j=1 ) (12) where f is a simple linear function, U 3 is a matrix that is tiled n times by u3 . 2.3 Aggregation Network After acquiring the local alignment representation by the co-attention network, we employ a filtration gate to hold the interactive information. Then, we fuse the interactive information via a bi-directional GRU and compute the persuasiveness score. Filtration Gate We utilize the filtration gate (Wang et al., 2017b) to hold the information that helps to understand the argument-level interactions between the original post and the reply. The formulas are in Equation 13 and 14. gt = sigmoid(Wg U + b) (13) U ∗ = gt U (14) We fuse the interactive information reserved by the filtration gate via a bi-directional GRU. The calculation is described in Equation 15. Ot = BiGRU (Ot−1 , Ut∗ ) (15) Then, we use an attention pooling operation over the whole hidden states of this BiGRU to summarize the interactive features into a dense vector O∗ . Scoring Tay et al. (2017) prove that adding some manual features such as"
C18-1314,P17-1018,0,0.170286,"all of the attention representations in a linear function to get the integrated information. The detail is illustrated in Equation 12. n m U = f (U 1 , U 2 , U 3 , {riOP }i=1 , {rjR }j=1 ) (12) where f is a simple linear function, U 3 is a matrix that is tiled n times by u3 . 2.3 Aggregation Network After acquiring the local alignment representation by the co-attention network, we employ a filtration gate to hold the interactive information. Then, we fuse the interactive information via a bi-directional GRU and compute the persuasiveness score. Filtration Gate We utilize the filtration gate (Wang et al., 2017b) to hold the information that helps to understand the argument-level interactions between the original post and the reply. The formulas are in Equation 13 and 14. gt = sigmoid(Wg U + b) (13) U ∗ = gt U (14) We fuse the interactive information reserved by the filtration gate via a bi-directional GRU. The calculation is described in Equation 15. Ot = BiGRU (Ot−1 , Ut∗ ) (15) Then, we use an attention pooling operation over the whole hidden states of this BiGRU to summarize the interactive features into a dense vector O∗ . Scoring Tay et al. (2017) prove that adding some manual features such as"
C18-1314,P16-2032,1,0.941255,"score, considering features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First,"
C18-1314,W16-2820,1,0.734769,"features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First, viewpoints of the"
D11-1015,N07-1054,0,0.0288585,"Missing"
D11-1015,W01-1605,0,0.129796,"Missing"
D11-1015,C10-3004,0,0.0119562,"nding named entity tags and part-of-speech tags, respectively. (4) Added sentiment tag (P : Positive; N : Negative) to all sentiment words. By applying above rules, the SSRs for D1 and D2 would be: d1 : [PER V|Ja01 RB|Ka01 JJ|Ee14|P IN NN|Dk03]s , [PRP V|Ja01 DT JJ|Ga16|N NN|Ae13 ]n d2 : [PER V|Ja01 JJ|Ee14|P IN NN|Bp12]s , [PRP V|He15|N NN|Di10 NN|Dd08 ]n Refer to d1 and d2 , ""Boris"" could match ""John"" in SSRs because they were converted to ""PER"" and they all appeared at the beginning of discourse instances. ""Ja01"", ""Ee14"" etc. were semantic labels from Chinese synonym list extended version (Che et al., 2010). There were similar resources in other languages such as Wordnet(Fellbaum, 1998) in English. The next problem became how to start from current SSRs and generate new SSRs for recognizing discourse relations without cue phrases. 4.2 Mining common SSRs Recall assumption (2), in order to incorporate lexical, structural and semantic information for the similarity calculation of two SSRs holding the same discourse relation, three types of matches were defined for {(u, v)|u ∈ di[k] , v ∈ dj[k] , k = 1, 2}: (1)Full match: (i) u = v or (ii) u.pos = v.pos and u.semlabel=v.semlabel or (iii) u.pos=v.pos"
D11-1015,P09-1075,0,0.0161866,"odeled the scheme to improve opinion polarity classification. However, opinion frames was difficult to be implemented because the recognition of opinion target was very challenging in general text. Our work differs from their approaches in two key aspects: (1) we distinguished nucleus and satellite in discourse but opinion frames did not; (2) our method for discourse discovery was unsupervised while their method needed annotated data. Most research works about discourse classification were not related to sentiment analysis. Supervised discourse classification methods (Soricut and Marcu, 2003; Duverle and Prendinger, 2009) needed manually annotated data. Marcu and Echihabi (2002) presented an unsupervised method to recognize discourse relations held between arbitrary spans of text. They showed that lexical pairs extracted from massive amount of data can have a major impact on discourse classification. BlairGoldensohn et al. (2007) extended Marcu&apos;s work by using parameter opitimization, topic segmentation and syntactic parsing. However, syntactic parsers were usually costly and impractical when dealing with large scale of text. Thus, in additional to lexical features, we incorporated sequential and semantic info"
D11-1015,P02-1047,0,0.611789,"However, opinion frames was difficult to be implemented because the recognition of opinion target was very challenging in general text. Our work differs from their approaches in two key aspects: (1) we distinguished nucleus and satellite in discourse but opinion frames did not; (2) our method for discourse discovery was unsupervised while their method needed annotated data. Most research works about discourse classification were not related to sentiment analysis. Supervised discourse classification methods (Soricut and Marcu, 2003; Duverle and Prendinger, 2009) needed manually annotated data. Marcu and Echihabi (2002) presented an unsupervised method to recognize discourse relations held between arbitrary spans of text. They showed that lexical pairs extracted from massive amount of data can have a major impact on discourse classification. BlairGoldensohn et al. (2007) extended Marcu&apos;s work by using parameter opitimization, topic segmentation and syntactic parsing. However, syntactic parsers were usually costly and impractical when dealing with large scale of text. Thus, in additional to lexical features, we incorporated sequential and semantic information in proposed method for discourse relation classifi"
D11-1015,miltsakaki-etal-2004-penn,0,0.0931139,"2 means CUE2 Relation Table 1: Examples of cue phrases 4 Methods The proposed methods were based on two assumptions: (1) Cue-phrase-based patterns could be used to find limited number of high quality discourse instances; (2) discourse relations were determined by lexical, structural and semantic information between two segments. Cue-phrase-based patterns could find only limited number of discourse instances with high precision (Marcu and Echihabi, 2002). Therefore, we could not rely on cue-phrase-based patterns alone. Moreover, there was no annotated corpus similar to Penn Discourse TreeBank (Miltsakaki et al., 2004) in other languages such as Chinese. Thus, we proposed a language independent unsupervised method to identify discourse relations without cue phrases while maintaining relatively high precision. For each discourse relation, we started with several cuephrase-based patterns and collected a large number of discourse instances from raw corpus. Then, discourse instances were converted to semantic sequential representations (SSRs). Finally, an unsupervised method was adopted to generate, weigh and filter common SSRs without cue phrases. The mined common SSRs could be directly used in our SSR-based c"
D11-1015,W02-1011,0,0.0193578,"t the proposed methods could effectively recognize the defined discourse relations and achieve significant improvement in sentence-level polarity classification comparing to BPC. The remainder of this paper is organized as follows. Section 2 introduces the related work. Section 3 presents the discourse scheme with discourse constraints on polarity. Section 4 gives the detail of proposed method. Experimental results are reported and discussed in Section 5 and Section 6 concludes this paper. 2 Related Work Research on polarity classification were generally conducted on 4 levels: document-level (Pang et al., 2002), sentence-level (Riloff et al., 2003), phraselevel (Wilson et al., 2009) and feature-level (Hu and Liu, 2004; Xia et al., 2007). There was little research focusing on the automatic recognition of intra-sentence level discourse 163 relations for sentiment analysis in the literature. Polanyi and Zaenen (2006) argued that valence calculation is critically affected by discourse structure. Asher et al. (2008) proposed a shallow semantic representation using a feature structure and use five types of rhetorical relations to build a finegrained corpus for deep contextual sentiment analysis. Neverthel"
D11-1015,W03-0404,0,0.0376784,"vely recognize the defined discourse relations and achieve significant improvement in sentence-level polarity classification comparing to BPC. The remainder of this paper is organized as follows. Section 2 introduces the related work. Section 3 presents the discourse scheme with discourse constraints on polarity. Section 4 gives the detail of proposed method. Experimental results are reported and discussed in Section 5 and Section 6 concludes this paper. 2 Related Work Research on polarity classification were generally conducted on 4 levels: document-level (Pang et al., 2002), sentence-level (Riloff et al., 2003), phraselevel (Wilson et al., 2009) and feature-level (Hu and Liu, 2004; Xia et al., 2007). There was little research focusing on the automatic recognition of intra-sentence level discourse 163 relations for sentiment analysis in the literature. Polanyi and Zaenen (2006) argued that valence calculation is critically affected by discourse structure. Asher et al. (2008) proposed a shallow semantic representation using a feature structure and use five types of rhetorical relations to build a finegrained corpus for deep contextual sentiment analysis. Nevertheless, they did not propose a computatio"
D11-1015,sadamitsu-etal-2008-sentiment,0,0.0605118,"ructure. Asher et al. (2008) proposed a shallow semantic representation using a feature structure and use five types of rhetorical relations to build a finegrained corpus for deep contextual sentiment analysis. Nevertheless, they did not propose a computational model for their discourse scheme. Snyder and Barzilay (2007) combined an agreement model based on contrastive RST relations with a local aspect model to make a more informed overall decision for sentiment classification. Nonetheless, contrastive relations were only one type of discourse relations which may help polarity classification. Sadamitsu et al. (2008) modeled polarity reversal using HCRFs integrated with inter-sentence discourse structures. However, our work is on intrasentence level and our purpose is not to find polarity reversals but trying to adapt general discourse schemes (e.g., RST) to help determine the overall polarity of ambiguous sentences. The most closely related works were (Somasundaran et al., 2008) and (Somasundaran et al., 2009), which proposed opinion frames as a representation of discourse-level associations on dialogue and modeled the scheme to improve opinion polarity classification. However, opinion frames was difficu"
D11-1015,N07-1038,0,0.0149574,"d feature-level (Hu and Liu, 2004; Xia et al., 2007). There was little research focusing on the automatic recognition of intra-sentence level discourse 163 relations for sentiment analysis in the literature. Polanyi and Zaenen (2006) argued that valence calculation is critically affected by discourse structure. Asher et al. (2008) proposed a shallow semantic representation using a feature structure and use five types of rhetorical relations to build a finegrained corpus for deep contextual sentiment analysis. Nevertheless, they did not propose a computational model for their discourse scheme. Snyder and Barzilay (2007) combined an agreement model based on contrastive RST relations with a local aspect model to make a more informed overall decision for sentiment classification. Nonetheless, contrastive relations were only one type of discourse relations which may help polarity classification. Sadamitsu et al. (2008) modeled polarity reversal using HCRFs integrated with inter-sentence discourse structures. However, our work is on intrasentence level and our purpose is not to find polarity reversals but trying to adapt general discourse schemes (e.g., RST) to help determine the overall polarity of ambiguous sen"
D11-1015,C08-1101,0,0.328817,"gnized the defined discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification. (a) [Although Fujimori was criticized by the international community]，[he was loved by the domestic population]， [because people hated the corrupted ruling class]. (儘管 國際間對藤森口誅筆伐，他在國內一直深受百姓愛 戴，原因是百姓對腐化的統治階級早就深惡痛絕。) 1 Introduction As an important task of sentiment analysis, polarity classification is critically affected by discourse structure (Polanyi and Zaenen, 2006). Previous research developed discourse schema (Asher et al., 2008) (Somasundaran et al., 2008) and proved that the utilization of discourse relations could improve the performance of polarity classification on dialogues (Somasundaran et al., 2009). However, cur1 Defined as ambiguous sentences in this paper Example (a) is a positive sentence holding a Contrast relation between first two segments and a Cause relation between last two segments. The polarity of ""criticized"", ""hated"" and ""corrupted"" are recognized as negative expressions while ""loved"" is recognized as a positive expression. Example (a) is difficult for existing polarity classification methods for two reasons: (1) the number"
D11-1015,D09-1018,0,0.124863,"cation. (a) [Although Fujimori was criticized by the international community]，[he was loved by the domestic population]， [because people hated the corrupted ruling class]. (儘管 國際間對藤森口誅筆伐，他在國內一直深受百姓愛 戴，原因是百姓對腐化的統治階級早就深惡痛絕。) 1 Introduction As an important task of sentiment analysis, polarity classification is critically affected by discourse structure (Polanyi and Zaenen, 2006). Previous research developed discourse schema (Asher et al., 2008) (Somasundaran et al., 2008) and proved that the utilization of discourse relations could improve the performance of polarity classification on dialogues (Somasundaran et al., 2009). However, cur1 Defined as ambiguous sentences in this paper Example (a) is a positive sentence holding a Contrast relation between first two segments and a Cause relation between last two segments. The polarity of ""criticized"", ""hated"" and ""corrupted"" are recognized as negative expressions while ""loved"" is recognized as a positive expression. Example (a) is difficult for existing polarity classification methods for two reasons: (1) the number of positive expressions is less than negative expressions; (2) the importance of each sentiment expression is unknown. However, consider Figure 1, if we"
D11-1015,N03-1030,0,0.0814805,"iations on dialogue and modeled the scheme to improve opinion polarity classification. However, opinion frames was difficult to be implemented because the recognition of opinion target was very challenging in general text. Our work differs from their approaches in two key aspects: (1) we distinguished nucleus and satellite in discourse but opinion frames did not; (2) our method for discourse discovery was unsupervised while their method needed annotated data. Most research works about discourse classification were not related to sentiment analysis. Supervised discourse classification methods (Soricut and Marcu, 2003; Duverle and Prendinger, 2009) needed manually annotated data. Marcu and Echihabi (2002) presented an unsupervised method to recognize discourse relations held between arbitrary spans of text. They showed that lexical pairs extracted from massive amount of data can have a major impact on discourse classification. BlairGoldensohn et al. (2007) extended Marcu&apos;s work by using parameter opitimization, topic segmentation and syntactic parsing. However, syntactic parsers were usually costly and impractical when dealing with large scale of text. Thus, in additional to lexical features, we incorporat"
D11-1015,J09-3003,0,0.0288966,"e relations and achieve significant improvement in sentence-level polarity classification comparing to BPC. The remainder of this paper is organized as follows. Section 2 introduces the related work. Section 3 presents the discourse scheme with discourse constraints on polarity. Section 4 gives the detail of proposed method. Experimental results are reported and discussed in Section 5 and Section 6 concludes this paper. 2 Related Work Research on polarity classification were generally conducted on 4 levels: document-level (Pang et al., 2002), sentence-level (Riloff et al., 2003), phraselevel (Wilson et al., 2009) and feature-level (Hu and Liu, 2004; Xia et al., 2007). There was little research focusing on the automatic recognition of intra-sentence level discourse 163 relations for sentiment analysis in the literature. Polanyi and Zaenen (2006) argued that valence calculation is critically affected by discourse structure. Asher et al. (2008) proposed a shallow semantic representation using a feature structure and use five types of rhetorical relations to build a finegrained corpus for deep contextual sentiment analysis. Nevertheless, they did not propose a computational model for their discourse schem"
D11-1015,C08-2002,0,\N,Missing
D14-1123,P11-1016,0,0.0366737,"ent analysis and event detection in microblog, respectively. Sentiment Analysis in Microblog: Sentiment analysis (Pang and Lee, 2008; Liu, 2012) is mainly about analyzing people’s opinions, sentiments and emotions towards a given event, topic, product, etc. Microblog platforms like Twitter and Weibo, provide people a convenient way to post their emotional reactions towards social events in almost real time. This leads to increasing number of interests on sentiment analysis in microblog data (Davidov et al., 2010; Liu et al., 2012; Go et al., 2009; Agarwal et al., 2011; Pak and Paroubek, 2010; Jiang et al., 2011; Speriosu et al., 2011; Bermingham and Smeaton, 2010). The training data for microblog sentiment analysis are usually obtained in an automatic manner by utilizing emoticons, hashtags and smileys. Davidov et al. (2010) propose an approach to automatically obtain labeled training examples by exploiting hashtags and smileys. Liu et al. (2012) proposed an emoticon smoothed method to integrate both manually labeled data and noisy labeled data for Twitter sentiment classification. Different from existing microblog sentiment analysis work, which aims at discovering sentiments and emotions for an eve"
D14-1123,W11-0705,0,0.0440545,"section, we review the related work on sentiment analysis and event detection in microblog, respectively. Sentiment Analysis in Microblog: Sentiment analysis (Pang and Lee, 2008; Liu, 2012) is mainly about analyzing people’s opinions, sentiments and emotions towards a given event, topic, product, etc. Microblog platforms like Twitter and Weibo, provide people a convenient way to post their emotional reactions towards social events in almost real time. This leads to increasing number of interests on sentiment analysis in microblog data (Davidov et al., 2010; Liu et al., 2012; Go et al., 2009; Agarwal et al., 2011; Pak and Paroubek, 2010; Jiang et al., 2011; Speriosu et al., 2011; Bermingham and Smeaton, 2010). The training data for microblog sentiment analysis are usually obtained in an automatic manner by utilizing emoticons, hashtags and smileys. Davidov et al. (2010) propose an approach to automatically obtain labeled training examples by exploiting hashtags and smileys. Liu et al. (2012) proposed an emoticon smoothed method to integrate both manually labeled data and noisy labeled data for Twitter sentiment classification. Different from existing microblog sentiment analysis work, which aims at di"
D14-1123,C10-2028,0,0.267155,"f precision, recall and F-measure. 2 Related Work In this section, we review the related work on sentiment analysis and event detection in microblog, respectively. Sentiment Analysis in Microblog: Sentiment analysis (Pang and Lee, 2008; Liu, 2012) is mainly about analyzing people’s opinions, sentiments and emotions towards a given event, topic, product, etc. Microblog platforms like Twitter and Weibo, provide people a convenient way to post their emotional reactions towards social events in almost real time. This leads to increasing number of interests on sentiment analysis in microblog data (Davidov et al., 2010; Liu et al., 2012; Go et al., 2009; Agarwal et al., 2011; Pak and Paroubek, 2010; Jiang et al., 2011; Speriosu et al., 2011; Bermingham and Smeaton, 2010). The training data for microblog sentiment analysis are usually obtained in an automatic manner by utilizing emoticons, hashtags and smileys. Davidov et al. (2010) propose an approach to automatically obtain labeled training examples by exploiting hashtags and smileys. Liu et al. (2012) proposed an emoticon smoothed method to integrate both manually labeled data and noisy labeled data for Twitter sentiment classification. Different from exi"
D14-1123,pak-paroubek-2010-twitter,0,0.0435762,"e related work on sentiment analysis and event detection in microblog, respectively. Sentiment Analysis in Microblog: Sentiment analysis (Pang and Lee, 2008; Liu, 2012) is mainly about analyzing people’s opinions, sentiments and emotions towards a given event, topic, product, etc. Microblog platforms like Twitter and Weibo, provide people a convenient way to post their emotional reactions towards social events in almost real time. This leads to increasing number of interests on sentiment analysis in microblog data (Davidov et al., 2010; Liu et al., 2012; Go et al., 2009; Agarwal et al., 2011; Pak and Paroubek, 2010; Jiang et al., 2011; Speriosu et al., 2011; Bermingham and Smeaton, 2010). The training data for microblog sentiment analysis are usually obtained in an automatic manner by utilizing emoticons, hashtags and smileys. Davidov et al. (2010) propose an approach to automatically obtain labeled training examples by exploiting hashtags and smileys. Liu et al. (2012) proposed an emoticon smoothed method to integrate both manually labeled data and noisy labeled data for Twitter sentiment classification. Different from existing microblog sentiment analysis work, which aims at discovering sentiments and"
D14-1123,W11-2207,0,0.0183136,"nt detection in microblog, respectively. Sentiment Analysis in Microblog: Sentiment analysis (Pang and Lee, 2008; Liu, 2012) is mainly about analyzing people’s opinions, sentiments and emotions towards a given event, topic, product, etc. Microblog platforms like Twitter and Weibo, provide people a convenient way to post their emotional reactions towards social events in almost real time. This leads to increasing number of interests on sentiment analysis in microblog data (Davidov et al., 2010; Liu et al., 2012; Go et al., 2009; Agarwal et al., 2011; Pak and Paroubek, 2010; Jiang et al., 2011; Speriosu et al., 2011; Bermingham and Smeaton, 2010). The training data for microblog sentiment analysis are usually obtained in an automatic manner by utilizing emoticons, hashtags and smileys. Davidov et al. (2010) propose an approach to automatically obtain labeled training examples by exploiting hashtags and smileys. Liu et al. (2012) proposed an emoticon smoothed method to integrate both manually labeled data and noisy labeled data for Twitter sentiment classification. Different from existing microblog sentiment analysis work, which aims at discovering sentiments and emotions for an event or topic given in ad"
D14-1123,D12-1134,0,0.340478,"UDRQDOGR     tion on March 12 indicates a public event:3.11 EDFHOROD   Earthquake in Japan. We can see that emotional changes immediately reflect the occurring of realworld events, thus it is reasonable to use them to perform event detection. Most existing research on microblog event detection (Weng and Lee, 2011; Sakaki et al., 2010; Becker et al., 2010) only account for the factual information (e.g., burstness of topic keywords). They usually ignore the importance of emotion information for event detection. Although there have recently been a few papers (Zhao et al., 2012a; Nguyen et al., 2013; Akcora et al., 2010) in this direction, they have a number of disadvantages. Firstly, they can not detect communityrelated events. Since they all aggregate emotion at global level, they can only discover national attention events, such as public holidays ( “Christmas” and “Spring Festival”) or natural disasters. In many applications, discovering events related to a certain group of users or a certain topic is more meaningful. Consider the following questions: “what happened in the football community last week?” and “what are the most significant events in the lawyer com"
D15-1259,W04-1013,0,0.00881063,"maries for each repost tree. To ensure the quality of reference summaries, we first extracted a list of frequent nouns from each repost tree and generalized 7 to 10 topics based on the nouns list, which provided a high-level overview of a repost tree to editors. Then, our guideline required editors to read all repost microblogs ordered sequentially on a repost tree. For every message, its entire repost tree path was also provided as supplementary context information. When finished reading, editors wrote down one or two sentences to summarize each topic in the list. We utilized ROUGE-N metric (Lin, 2004) for benchmark, which is a standard for evaluating automatic summaries based on N-gram overlapping between a generated summary and a reference. Specifically, ROUGE-1 and ROUGE-2 F1-measure were used as our evaluation metrics. Lin et al. (2004) has demonstrated that ROUGE-2 correlates well with humans in summarizing formal texts. And ROUGE-1 is a better alternative in evaluating summaries for short and informal 4 All descriptions are English translations of the root microblogs originally in Chinese. microblog messages (Inouye and Kalita, 2011; Chang et al., 2013). In our human-generated summari"
D15-1259,W11-0709,0,0.0876751,"f social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2169 then considered social signals like user following relations and retweet count (Duan et al., 2012; Liu et al., 2012), and reported such features useful to help summarize microblog posts. Our work studies repost tree summarization by leveraging content-level structure to enrich context of messages, which is a different kin"
D15-1259,C12-1104,0,0.167824,"ome works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2169 then considered social signals like user following relations and retweet count (Duan et al., 2012; Liu et al., 2012), and reported such features useful to help summarize microblog posts. Our work studies repost tree summarization by leveraging content-level structure to enrich context of messages, which is a different kind of signal. Chang et al. (2013) proposed a task to summarize Twitter context trees consisting of an original tweet and all its reposts (i.e., replies and retweets). They combined user influence signals into a supervised summarization framework. Our work is different from theirs: 1) They simply treat a context tree as a tweets stream while we consider repost tree structures in summarization"
D15-1259,C12-1047,0,0.0623679,"portant information (Radev et al., 2002). Generally, text summarization techniques can be categorized into extractive and abstractive methods (Das and Martins, 2007). Extractive approaches focus on how to identify and distill salient contents from original texts whereas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are in"
D15-1259,P13-4009,0,0.0757462,"Sort all v ∈ V by RN (v) in descending order 19: Pick the top-n messages as summary WALK-2 sampling from leader probabilities, it also reduces the risk of including real followers into summary. 5 Experiments and Results To evaluate the two modules in our repost tree summarization system, i.e., CRF-based model for leader detection and LeadSum model for summarization, we conducted two sets of experiments based on microblog posts data collected from Sina Weibo, which has a similar market penetration as Twitter (Rapoza, 2011)2 . Microblog messages on Sina Weibo are in Chinese and we use FudanNLP (Qiu et al., 2013) for text preprocessing including word segmentation and POS tagging. 5.1 Experiment for Leader Detection In this experiment, we evaluated the performance of CRF model for leader detection task. 5.1.1 Data Collection and Setup We first crawled 1,300 different repost trees using the public PKUVIS toolkit (Ren et al., 2014). Given an original microblog post, the toolkit can automatically crawl its complete repost tree. For each tree, we randomly selected one path and further formed a set with 1,300 repost tree paths, 2 The datasets are available at http://www1.se. cuhk.edu.hk/˜lijing/data/repost_"
D15-1259,J02-4001,0,0.176959,". • We identify a novel problem of leader detection for summarization, which aims to reduce noise on repost trees, and present a CRF-based method for effectively detecting leaders by utilizing the tree structure and message contents. • We incorporate the leader detection result into an unsupervised summarization model based on random walk and substantially enhance the model to reduce the impact of leader detection errors on summarization. 2 Related Work The goal of text summarization is to automatically produce a succinct summary for one or more documents that preserves important information (Radev et al., 2002). Generally, text summarization techniques can be categorized into extractive and abstractive methods (Das and Martins, 2007). Extractive approaches focus on how to identify and distill salient contents from original texts whereas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or"
D15-1259,radev-etal-2004-mead,0,0.286211,"Missing"
D15-1259,N13-1135,0,0.106426,"Missing"
D15-1259,D11-1040,0,0.0316202,"ount based on user following relations. • LeadProSum: LeadProSum ranks and selects reposting messages by their marginal probabilities as leaders determined by our CRF-based leader detection model. • SVDSum: SVDSum adopts the Singular Value Decomposition (SVD) to discover hidden sub-topics for summarization (Gong and Liu, 2001). Reposting messages are ranked according to latent semantic analysis with SVD on termmessage matrix. • DivRankSum: DivRankSum directly applies DivRank (Mei et al., 2010) algorithm to rank all messages unaware of leaders and followers. A similar model is also reported in Yan et al. (2011). Following their work, we set damping weight as 0.85. • UserInfSum: Chang et al. (2013) ranks messages utilizing Gradient Boosted Decision Tree (GBDT) algorithm with text, popularity, temporal and user influence signals to summarize Twitter context tree. In particular, without the interaction data with external users, we utilize users’ fol2174 Name Tree (I) Tree (II) Tree (III) Tree (IV) Tree (V) Tree (VI) Tree (VII) Tree (VIII) Tree (IX) Tree (X) # of nodes 21,353 9,616 13,087 12,865 10,666 21,127 18,974 2,021 9,230 10,052 # of nodes with comments 15,409 6,073 9,583 7,083 7,129 15,057 12,399"
D18-1090,D15-1049,0,0.0541644,"of the future directions can be exploring other kinds of scoring actions than classification under the reinforcement learning framework. Text Quality Evaluation Traditionally, AES models are usually divided into three categories: classification, regression and ranking. Naive Bayes models are mostly used in classification tasks. Larkey (1998) use bagof-word features. Following that, Rudner and Liang (2002) develop a system based on multinomial Bernoulli Naive Bayes, using content and style features. E-rater (Attali and Burstein, 2004) is one of the earliest systems to adopt regression methods. Phandi et al. (2015) use correlated Bayesian Linear Ridge Regression (cBLRR) focusing on domain-adaptation tasks. Ranking models use linguistic features. Yannakoudakis et al. (2011) formulate AES as a pair-wise ranking problem by ranking the order of pair essays. Chen and He (2013) formulate AES into a list-wise ranking problem by considering the order relation among the whole essays. Argument quality evaluation is a task closely related to AES, which involves evaluation of argumentative texts with various grains (argumentlevel, post-level, etc.). Tan et al. (2016); Wei et al. (2016a); Wang et al. (2017) make use"
D18-1090,D13-1180,0,0.0930244,"king. Naive Bayes models are mostly used in classification tasks. Larkey (1998) use bagof-word features. Following that, Rudner and Liang (2002) develop a system based on multinomial Bernoulli Naive Bayes, using content and style features. E-rater (Attali and Burstein, 2004) is one of the earliest systems to adopt regression methods. Phandi et al. (2015) use correlated Bayesian Linear Ridge Regression (cBLRR) focusing on domain-adaptation tasks. Ranking models use linguistic features. Yannakoudakis et al. (2011) formulate AES as a pair-wise ranking problem by ranking the order of pair essays. Chen and He (2013) formulate AES into a list-wise ranking problem by considering the order relation among the whole essays. Argument quality evaluation is a task closely related to AES, which involves evaluation of argumentative texts with various grains (argumentlevel, post-level, etc.). Tan et al. (2016); Wei et al. (2016a); Wang et al. (2017) make use of linguistic features to evaluate the persuasiveness of arAcknowledgments Thanks for the constructive comments from anonymous reviewers. This work is partially supported by National Natural Science Foundation of China (Grant No. 61702106), Shanghai Science and"
D18-1090,D16-1115,0,0.0126277,"esults on benchmark datasets show the effectiveness of our framework. 1 Introduction In recent years, neural networks have been widely used to grade student essays automatically and achieve state-of-the-art performance. In particular, a distributed representation is learned for an essay with variant neural networks and a linear layer is then used to produce the final score. Existing researches focus on learning better essay representation using different neural networks, including long short-term memory (LSTM) network (Taghipour and Ng, 2016), hierarchical convolutional neural networks (CNN) (Dong and Zhang, 2016), hierarchical CNN-LSTM structure with attention mechanism (Dong et al., 2017), and SKIPFLOW LSTM (Tay et al., 2017). The major evaluation metric for AES is quadratic weighted kappa (QWK), which is also the official metric of Automated Student Assessment Prize1 (ASAP). It evaluates the scoring results by taking rating schema into consideration. Because QWK is not differentiable, it is hard to train systems via optimizing this metric directly. Alternatively, existing AES systems are typically trained to predict the score for a single essay and optimized using mean square error (MSE). The 2 Typi"
D18-1090,D16-1193,0,0.302353,"tic weighted kappa as guidance to optimize the scoring system. Experiment results on benchmark datasets show the effectiveness of our framework. 1 Introduction In recent years, neural networks have been widely used to grade student essays automatically and achieve state-of-the-art performance. In particular, a distributed representation is learned for an essay with variant neural networks and a linear layer is then used to produce the final score. Existing researches focus on learning better essay representation using different neural networks, including long short-term memory (LSTM) network (Taghipour and Ng, 2016), hierarchical convolutional neural networks (CNN) (Dong and Zhang, 2016), hierarchical CNN-LSTM structure with attention mechanism (Dong et al., 2017), and SKIPFLOW LSTM (Tay et al., 2017). The major evaluation metric for AES is quadratic weighted kappa (QWK), which is also the official metric of Automated Student Assessment Prize1 (ASAP). It evaluates the scoring results by taking rating schema into consideration. Because QWK is not differentiable, it is hard to train systems via optimizing this metric directly. Alternatively, existing AES systems are typically trained to predict the score f"
D18-1090,K17-1017,0,0.160871,"ion In recent years, neural networks have been widely used to grade student essays automatically and achieve state-of-the-art performance. In particular, a distributed representation is learned for an essay with variant neural networks and a linear layer is then used to produce the final score. Existing researches focus on learning better essay representation using different neural networks, including long short-term memory (LSTM) network (Taghipour and Ng, 2016), hierarchical convolutional neural networks (CNN) (Dong and Zhang, 2016), hierarchical CNN-LSTM structure with attention mechanism (Dong et al., 2017), and SKIPFLOW LSTM (Tay et al., 2017). The major evaluation metric for AES is quadratic weighted kappa (QWK), which is also the official metric of Automated Student Assessment Prize1 (ASAP). It evaluates the scoring results by taking rating schema into consideration. Because QWK is not differentiable, it is hard to train systems via optimizing this metric directly. Alternatively, existing AES systems are typically trained to predict the score for a single essay and optimized using mean square error (MSE). The 2 Typically, an essay scorer contains two components, namely, essay representation a"
D18-1090,C18-1314,1,0.81003,"ession-based scorer B0. The rating ranges for set 1,2,7,8 are much greater than set 3,4,5,6 (see Table 1). The performance difference between B1 and B0 decreases (from positive to negative) when the number of rating categories increases. This is because when the number of categories get larger, it requires much more parameters for the classification-based scorer to be well trained. Given N categories, the classification layer should output N probabilities for each category per essay, costing N times more parameters than regression-based scoring. 4 guments in online forums. Wei et al. (2016b); Ji et al. (2018) consider features from the perspectives of argumentation interaction between participants. Persing and Ng (2017) construct their model based on error types for argumentation. 4.2 Being able to optimize non-differentiable quality metrics, reinforcement learning has been widely used in natural language processing tasks such as machine translation (Bahdanau et al., 2016), image captioning (Rennie et al., 2016; Zhang et al., 2017) and text summarization (Ranzato et al., 2015). To the best of our knowledge, this paper is the first attempt to optimize the scorer by QWK that considers rating schema."
D18-1090,Q17-1016,0,0.0533373,"Missing"
D18-1090,P16-2032,1,0.834915,") compared with regression-based scorer B0. The rating ranges for set 1,2,7,8 are much greater than set 3,4,5,6 (see Table 1). The performance difference between B1 and B0 decreases (from positive to negative) when the number of rating categories increases. This is because when the number of categories get larger, it requires much more parameters for the classification-based scorer to be well trained. Given N categories, the classification layer should output N probabilities for each category per essay, costing N times more parameters than regression-based scoring. 4 guments in online forums. Wei et al. (2016b); Ji et al. (2018) consider features from the perspectives of argumentation interaction between participants. Persing and Ng (2017) construct their model based on error types for argumentation. 4.2 Being able to optimize non-differentiable quality metrics, reinforcement learning has been widely used in natural language processing tasks such as machine translation (Bahdanau et al., 2016), image captioning (Rennie et al., 2016; Zhang et al., 2017) and text summarization (Ranzato et al., 2015). To the best of our knowledge, this paper is the first attempt to optimize the scorer by QWK that cons"
D18-1090,W16-2820,1,0.86136,") compared with regression-based scorer B0. The rating ranges for set 1,2,7,8 are much greater than set 3,4,5,6 (see Table 1). The performance difference between B1 and B0 decreases (from positive to negative) when the number of rating categories increases. This is because when the number of categories get larger, it requires much more parameters for the classification-based scorer to be well trained. Given N categories, the classification layer should output N probabilities for each category per essay, costing N times more parameters than regression-based scoring. 4 guments in online forums. Wei et al. (2016b); Ji et al. (2018) consider features from the perspectives of argumentation interaction between participants. Persing and Ng (2017) construct their model based on error types for argumentation. 4.2 Being able to optimize non-differentiable quality metrics, reinforcement learning has been widely used in natural language processing tasks such as machine translation (Bahdanau et al., 2016), image captioning (Rennie et al., 2016; Zhang et al., 2017) and text summarization (Ranzato et al., 2015). To the best of our knowledge, this paper is the first attempt to optimize the scorer by QWK that cons"
D18-1090,P11-1019,0,0.16244,"uation Traditionally, AES models are usually divided into three categories: classification, regression and ranking. Naive Bayes models are mostly used in classification tasks. Larkey (1998) use bagof-word features. Following that, Rudner and Liang (2002) develop a system based on multinomial Bernoulli Naive Bayes, using content and style features. E-rater (Attali and Burstein, 2004) is one of the earliest systems to adopt regression methods. Phandi et al. (2015) use correlated Bayesian Linear Ridge Regression (cBLRR) focusing on domain-adaptation tasks. Ranking models use linguistic features. Yannakoudakis et al. (2011) formulate AES as a pair-wise ranking problem by ranking the order of pair essays. Chen and He (2013) formulate AES into a list-wise ranking problem by considering the order relation among the whole essays. Argument quality evaluation is a task closely related to AES, which involves evaluation of argumentative texts with various grains (argumentlevel, post-level, etc.). Tan et al. (2016); Wei et al. (2016a); Wang et al. (2017) make use of linguistic features to evaluate the persuasiveness of arAcknowledgments Thanks for the constructive comments from anonymous reviewers. This work is partially"
D18-1090,P17-1172,0,0.01471,"ize non-differentiable quality metrics, reinforcement learning has been widely used in natural language processing tasks such as machine translation (Bahdanau et al., 2016), image captioning (Rennie et al., 2016; Zhang et al., 2017) and text summarization (Ranzato et al., 2015). To the best of our knowledge, this paper is the first attempt to optimize the scorer by QWK that considers rating schema. Skip connections in RNNs are capable of capturing long-term dependencies in sequences. Vezhnevets et al. (2017) introduces dilated LSTM to allow its manager to operate at a low temporal resolution. Yu et al. (2017) propose a reinforcement learning method to let the network learn how long to skip. Related Work There are two lines of research related to our work including text quality evaluation and reinforcement learning for natural language processing. 4.1 Reinforcement Learning for Natural Language Processing 5 Conclusion and Future Work In this paper, we propose a reinforcement learning framework incorporating QWK metric as the reward to train the essay scoring system directly. A packed evaluation strategy is used for QWK computation and the scoring of each essay is treated as a single action. In part"
D19-1096,N13-1006,0,0.385532,"rent domains and the baseline methods applied for comparison. We also detail the hyperparameter configuration of the proposed model. Our codes and datasets can be found at https: //github.com/RowitZou/LGN. 4.1 Data We conducted experiments on four Chinese NER datasets. (1) OntoNotes 4.0 (Weischedel et al., 2011): OntoNotes is a manually annotated multilingual corpus in the news domain that contains various text annotations, including Chinese named entity labels. Gold-standard segmentation is available. We only use Chinese documents (about 16k sentences) and process the data in the same way as Che et al. (2013). (2) MSRA (Levow, 2006): MSRA is also a dataset in the news domain and contains three types of named entities: LOC, PER, and ORG. Chinese word segmentation is available in the training set but not in the test set. (3) Weibo NER (Peng and Dredze, 2015): It consists of annotated NER messages drawn from the social media Sina Weibo2 . The corpus contains PER, ORG, GPE, and LOC for both named entity and nominal mention. (4) Resume NER (Zhang and Yang, 2018): It is composed of resumes collected from Sina Finance3 and is annotated with 8 types of named entities. Both Weibo and Resume datasets do not"
D19-1096,W06-0130,0,0.804173,"Missing"
D19-1096,D15-1141,1,0.763206,"Missing"
D19-1096,P15-1017,0,0.443298,"equence 河流 River Figure 1: Example of word character lattice with partial input. Because of the characteristic of chain structure, RNN-based methods must predict the label “度” using only previous partial sequences “印度 (India)”, which may suffer from word ambiguities without global sentence semantics. The task of named entity recognition (NER) involves determining entity boundaries and recognizing categories of named entities, which is a fundamental task in the field of natural language processing (NLP). NER plays an important role in many downstream NLP tasks, including information retrieval (Chen et al., 2015b), relation extraction (Bunescu and Mooney, 2005), question answering systems (Diefenbach et al., 2018), and other applications. Compared with English NER, Chinese named entities are more difficult to identify due to their uncertain boundaries, complex composition, and NE definitions within the nest (Duan and Zheng, 2011). One intuitive way to alleviate word boundary problems is to first perform word segmentation Equal contribution. M-LOC ? Yin Introduction ∗ B-LOC E-GPE B-GPE and then apply word sequence labeling (Yang et al., 2016; He and Sun, 2017). However, the rare gold-standard segmenta"
D19-1096,N19-1423,0,0.052104,"Missing"
D19-1096,li-etal-2014-comparison,0,0.377738,"Missing"
D19-1096,L16-1138,0,0.203827,"Missing"
D19-1096,D15-1104,0,0.0258868,"vely aggregating mechanism; 3) several experimental results demonstrate the effectiveness of the proposed method in different aspects. 2 2.1 Related Work Chinese NER with Lexicon. Some previous Chinese NER researches have compared word-based and character-based methods (Li et al., 2014) and show that due to the limited performance of the current Chinese word segmentation, character-based name taggers can outperform their word-based counterparts (He and Wang, 2008; Liu et al., 2010). Lexicon features have been widely used to better leverage word information for Chinese NER (Huang et al., 2015; Luo et al., 2015; Gui et al., 2019). Especially, Zhang and Yang (2018) proposed a lattice LSTM to model characters and potential words simultaneously. However, their lattice LSTM used a concatenation of independently trained left-toright and right-to-left LSTM to represent features, which was also limited (Devlin et al., 2018). In this work, we propose a novel character-based method that treats the named entities as a node classification task. The proposed method can utilize global information (both the left and the right context) (Dong et al., 2019) to tackle word ambiguities. 2.2 Graph Neural Networks on Te"
D19-1096,I08-4022,0,0.0863472,"on task; 2) the proposed model can capture global context information and local compositions to tackle Chinese word ambiguity problems through recursively aggregating mechanism; 3) several experimental results demonstrate the effectiveness of the proposed method in different aspects. 2 2.1 Related Work Chinese NER with Lexicon. Some previous Chinese NER researches have compared word-based and character-based methods (Li et al., 2014) and show that due to the limited performance of the current Chinese word segmentation, character-based name taggers can outperform their word-based counterparts (He and Wang, 2008; Liu et al., 2010). Lexicon features have been widely used to better leverage word information for Chinese NER (Huang et al., 2015; Luo et al., 2015; Gui et al., 2019). Especially, Zhang and Yang (2018) proposed a lattice LSTM to model characters and potential words simultaneously. However, their lattice LSTM used a concatenation of independently trained left-toright and right-to-left LSTM to represent features, which was also limited (Devlin et al., 2018). In this work, we propose a novel character-based method that treats the named entities as a node classification task. The proposed method"
D19-1096,D14-1181,0,0.00888242,"Missing"
D19-1096,W06-0115,0,0.819159,"methods applied for comparison. We also detail the hyperparameter configuration of the proposed model. Our codes and datasets can be found at https: //github.com/RowitZou/LGN. 4.1 Data We conducted experiments on four Chinese NER datasets. (1) OntoNotes 4.0 (Weischedel et al., 2011): OntoNotes is a manually annotated multilingual corpus in the news domain that contains various text annotations, including Chinese named entity labels. Gold-standard segmentation is available. We only use Chinese documents (about 16k sentences) and process the data in the same way as Che et al. (2013). (2) MSRA (Levow, 2006): MSRA is also a dataset in the news domain and contains three types of named entities: LOC, PER, and ORG. Chinese word segmentation is available in the training set but not in the test set. (3) Weibo NER (Peng and Dredze, 2015): It consists of annotated NER messages drawn from the social media Sina Weibo2 . The corpus contains PER, ORG, GPE, and LOC for both named entity and nominal mention. (4) Resume NER (Zhang and Yang, 2018): It is composed of resumes collected from Sina Finance3 and is annotated with 8 types of named entities. Both Weibo and Resume datasets do not contain the gold-standa"
D19-1096,W14-1609,0,0.0468043,"nest (Duan and Zheng, 2011). One intuitive way to alleviate word boundary problems is to first perform word segmentation Equal contribution. M-LOC ? Yin Introduction ∗ B-LOC E-GPE B-GPE and then apply word sequence labeling (Yang et al., 2016; He and Sun, 2017). However, the rare gold-standard segmentation in NER datasets and incorrectly segmented entity boundaries both negatively impact the identification of named entities (Peng and Dredze, 2015; He and Sun, 2016). Hence, character-level Chinese NER using lexicon features to better leverage word information has attracted research attention (Passos et al., 2014; Zhang and Yang, 2018). In particular, Zhang and Yang (2018) introduced a variant of a long short-term memory network (latticestructured LSTM) that encodes all potential words matching a sentence to exploit explicit word information, achieving state-of-the-art results. However, these methods are usually based on RNN or CRF to sequentially encode a sentence, while the underlying structure of language is not strictly sequential (Shen et al., 2019). As a result, these models would encounter serious word ambiguity problems (Mich et al., 2000). Especially in Chinese texts, the recognition of named"
D19-1096,D15-1064,0,0.488836,"ns. Compared with English NER, Chinese named entities are more difficult to identify due to their uncertain boundaries, complex composition, and NE definitions within the nest (Duan and Zheng, 2011). One intuitive way to alleviate word boundary problems is to first perform word segmentation Equal contribution. M-LOC ? Yin Introduction ∗ B-LOC E-GPE B-GPE and then apply word sequence labeling (Yang et al., 2016; He and Sun, 2017). However, the rare gold-standard segmentation in NER datasets and incorrectly segmented entity boundaries both negatively impact the identification of named entities (Peng and Dredze, 2015; He and Sun, 2016). Hence, character-level Chinese NER using lexicon features to better leverage word information has attracted research attention (Passos et al., 2014; Zhang and Yang, 2018). In particular, Zhang and Yang (2018) introduced a variant of a long short-term memory network (latticestructured LSTM) that encodes all potential words matching a sentence to exploit explicit word information, achieving state-of-the-art results. However, these methods are usually based on RNN or CRF to sequentially encode a sentence, while the underlying structure of language is not strictly sequential ("
D19-1096,W06-0126,0,0.781484,"Missing"
D19-1096,P18-1030,0,0.0335756,"Missing"
D19-1096,P18-1144,0,0.484748,", 2011). One intuitive way to alleviate word boundary problems is to first perform word segmentation Equal contribution. M-LOC ? Yin Introduction ∗ B-LOC E-GPE B-GPE and then apply word sequence labeling (Yang et al., 2016; He and Sun, 2017). However, the rare gold-standard segmentation in NER datasets and incorrectly segmented entity boundaries both negatively impact the identification of named entities (Peng and Dredze, 2015; He and Sun, 2016). Hence, character-level Chinese NER using lexicon features to better leverage word information has attracted research attention (Passos et al., 2014; Zhang and Yang, 2018). In particular, Zhang and Yang (2018) introduced a variant of a long short-term memory network (latticestructured LSTM) that encodes all potential words matching a sentence to exploit explicit word information, achieving state-of-the-art results. However, these methods are usually based on RNN or CRF to sequentially encode a sentence, while the underlying structure of language is not strictly sequential (Shen et al., 2019). As a result, these models would encounter serious word ambiguity problems (Mich et al., 2000). Especially in Chinese texts, the recognition of named entities with overlapp"
D19-1096,D18-1244,0,0.383117,"er, their lattice LSTM used a concatenation of independently trained left-toright and right-to-left LSTM to represent features, which was also limited (Devlin et al., 2018). In this work, we propose a novel character-based method that treats the named entities as a node classification task. The proposed method can utilize global information (both the left and the right context) (Dong et al., 2019) to tackle word ambiguities. 2.2 Graph Neural Networks on Texts Graph neural networks have been successfully applied to several text classification tasks (Veliˇckovi´c et al., 2017; Yao et al., 2018; Zhang et al., 2018b). Peng et al. (2018) proposed a GCNbased deep learning model for text classification. Zhang et al. (2018c) proposed using the dependency parse trees to construct a graph for relation extraction. Recently, multi-head attention mechanisms (Vaswani et al., 2017) have been widely used by graph neural networks during the fusion process (Zhang et al., 2018a; Lee et al., 2018), which can aggregate graph information by assigning different weights to neighboring nodes or associated edges. Given a set of vectors H ∈ ˆ ∈ R1×d , and a set of Rn×d , a query vector q trainable parameters W, this mechanism"
D19-1096,W03-1728,0,0.122763,"hese methods are based on character sequences. We applied the bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kim, 2014) as classic baseline methods. Character-level methods + bichar + softword: Character bigrams are useful for capturing adjacent features and representing characters. We concatenated bigram embeddings with character embeddings to better leverage the bigram information. In addition, we added the segmentation information by incorporating segmentation label embeddings into the character representation. The BMES scheme is used for representing the word segmentation (Xue and Shen, 2003). Word-level methods: For the datasets with gold segmentation, we directly employed wordlevel NER methods to evaluate the performance, which are denoted as Gold seg. Otherwise, we first used open source segmentation toolkit6 to automatically segment the datasets. Then wordlevel NER methods are applied, which are denoted as Auto seg. The bi-directional LSTM and CNN are also applied as baselines. Word-level methods + char + bichar: For characters in the subsequence wb,e , we first used a bi-directional LSTM to learn their hidden states and bigram states. We then augmented the wordlevel methods w"
D19-1508,P15-1033,0,0.0337745,"n, the performance can be slightly improved by incorporating the character level information with CNN. Furthermore, by integrating either our corpus-level attention or document-level attention into the existing models, the performance can be signiﬁcantly boosted. In particular, our model with global attention achieves the best performance in terms of all metrics. Symptom recognition is the basis of symptom inference. We report the results of the recent advanced baselines as well as the variants of our proposed method. Speciﬁcally, we compare the performance of the following models: • Bi-RNNs (Dyer et al., 2015): The models use LSTM or GRU for the sentence encoder, and treat symptom entity recognition as a classiﬁcation problem with the softmax function. From now on, we use RNNs to denote LSTM or GRU for ease of description. Model Bi-GRU Bi-LSTM Bi-GRU-CRF Bi-LSTM-CRF CNNs-Bi-GRU-CRF CNNs-Bi-LSTM-CRF Corpus-level attention Document-level attention Both Corpus and Document-level attention • Corpus-level Attention: It is a Bi-LSTMCRF model that incorporates corpus-level features via our corpus-level attention. • Document-level Attention: It is a BiLSTM-CRF model that incorporates document-level feature"
D19-1508,P16-1101,0,0.102499,"Missing"
D19-1508,W03-0419,0,0.327077,"Missing"
D19-5514,N13-1092,0,0.0656318,"Missing"
D19-5514,S07-1091,0,0.0103781,"Seq(f+a2a) ours (f2a) ours (a2a) ours (f+a2a) BLEU-1 0.251 0.581 0.715 0.796 0.859 0.881 BLEU-2 0.167 0.526 0.661 0.722 0.808 0.835 BLEU-3 0.110 0.482 0.619 0.656 0.763 0.791 5 Related Work There are two lines of research for paraphrase generation including knowledge based ones and neural network based ones. Some researchers provide rules (Bhagat and Hovy, 2013) or corpus including knowledge (Fader et al., 2013; Ganitkevitch et al., 2013; Pavlick et al., 2015). Other researchers try to make use of templates (Berant and Liang, 2014), semantic information (Kozlowski et al., 2003) and thesaurus (Hassan et al., 2007) for paraphrase generation. Rush (2015) have applied Seq2Seq model with attention mechanism for text summarization. Prakash (2016) employ a residual net in Seq2Seq model to generate paraphrases. Cao (2017) combine a copying decoder and a generative decoder for paraphrase generation. Cao(2018) try to utilize template information to help text summarization, however, the template is vague in that paper. We hope to utilize the special structure of question and extract the template explicitly from questions. BLEU-4 0.085 0.441 0.580 0.598 0.720 0.747 Table 4: Transfer learning performance of our pi"
D19-5514,W03-1601,0,0.0345783,"t. Model Seq2Seq(f2a) Seq2Seq(a2a) Seq2Seq(f+a2a) ours (f2a) ours (a2a) ours (f+a2a) BLEU-1 0.251 0.581 0.715 0.796 0.859 0.881 BLEU-2 0.167 0.526 0.661 0.722 0.808 0.835 BLEU-3 0.110 0.482 0.619 0.656 0.763 0.791 5 Related Work There are two lines of research for paraphrase generation including knowledge based ones and neural network based ones. Some researchers provide rules (Bhagat and Hovy, 2013) or corpus including knowledge (Fader et al., 2013; Ganitkevitch et al., 2013; Pavlick et al., 2015). Other researchers try to make use of templates (Berant and Liang, 2014), semantic information (Kozlowski et al., 2003) and thesaurus (Hassan et al., 2007) for paraphrase generation. Rush (2015) have applied Seq2Seq model with attention mechanism for text summarization. Prakash (2016) employ a residual net in Seq2Seq model to generate paraphrases. Cao (2017) combine a copying decoder and a generative decoder for paraphrase generation. Cao(2018) try to utilize template information to help text summarization, however, the template is vague in that paper. We hope to utilize the special structure of question and extract the template explicitly from questions. BLEU-4 0.085 0.441 0.580 0.598 0.720 0.747 Table 4: Tra"
D19-5514,N03-1003,0,0.217767,"e have two observations. First, words in a question can be easily divided into two types, namely, topic words and template words. Template words define the information need of the question while topic words are related to some specific entities or events. Second, for a pair of paraphrase questions, they tend to share the Introduction Paraphrase means sentences or phrases that convey the same meaning with different expressions. Popular tasks about paraphrases are paraphrase identification (Yin and Sch¨utze, 2015), paraphrase generation (Li et al., 2018; Gupta et al., 2018), sentence rewriting (Barzilay and Lee, 2003), etc. As a special case of paraphrase generation, question paraphrasing (McKeown, 1983) aims to restate an input question. It can be applied in a question answering system for the expansion of question set to enhance the coverage of candidate answers. Besides, it is able to probe the need of users within an interactive system by rephrasing questions. Traditional approaches for paraphrase generation are mostly based on external knowledge, including manually constructed templates (McKeown, 1983), or external thesaurus (Hassan et al., * Corresponding author 109 Proceedings of the 2019 EMNLP Work"
D19-5514,D18-1421,0,0.0203855,"example of question paraphrasing can be seen in Table 1. We have two observations. First, words in a question can be easily divided into two types, namely, topic words and template words. Template words define the information need of the question while topic words are related to some specific entities or events. Second, for a pair of paraphrase questions, they tend to share the Introduction Paraphrase means sentences or phrases that convey the same meaning with different expressions. Popular tasks about paraphrases are paraphrase identification (Yin and Sch¨utze, 2015), paraphrase generation (Li et al., 2018; Gupta et al., 2018), sentence rewriting (Barzilay and Lee, 2003), etc. As a special case of paraphrase generation, question paraphrasing (McKeown, 1983) aims to restate an input question. It can be applied in a question answering system for the expansion of question set to enhance the coverage of candidate answers. Besides, it is able to probe the need of users within an interactive system by rephrasing questions. Traditional approaches for paraphrase generation are mostly based on external knowledge, including manually constructed templates (McKeown, 1983), or external thesaurus (Hassan et"
D19-5514,P14-1133,0,0.0345115,"et and then fine-tuned on the automotive dataset. Model Seq2Seq(f2a) Seq2Seq(a2a) Seq2Seq(f+a2a) ours (f2a) ours (a2a) ours (f+a2a) BLEU-1 0.251 0.581 0.715 0.796 0.859 0.881 BLEU-2 0.167 0.526 0.661 0.722 0.808 0.835 BLEU-3 0.110 0.482 0.619 0.656 0.763 0.791 5 Related Work There are two lines of research for paraphrase generation including knowledge based ones and neural network based ones. Some researchers provide rules (Bhagat and Hovy, 2013) or corpus including knowledge (Fader et al., 2013; Ganitkevitch et al., 2013; Pavlick et al., 2015). Other researchers try to make use of templates (Berant and Liang, 2014), semantic information (Kozlowski et al., 2003) and thesaurus (Hassan et al., 2007) for paraphrase generation. Rush (2015) have applied Seq2Seq model with attention mechanism for text summarization. Prakash (2016) employ a residual net in Seq2Seq model to generate paraphrases. Cao (2017) combine a copying decoder and a generative decoder for paraphrase generation. Cao(2018) try to utilize template information to help text summarization, however, the template is vague in that paper. We hope to utilize the special structure of question and extract the template explicitly from questions. BLEU-4 0"
D19-5514,P16-1101,0,0.0237392,"nto template and topic ones. We treat the problem as a supervised sequence labeling task and modify the classical BIO tagging strategy to fit our scenario. Specifically, we use “O” to specify the template part, and treat “B” and “I” as the topic part. As Bi-LSTM has been 1 http://www.sdspeople.fudan.edu.cn/ zywei/data/paraphrase.zip 110 Figure 1: The overview of the proposed framework. We can also train them together to ease the error propagation problem resulted from separate training. The loss function here is the sum of each module. proved to be effective for the task of sequence labeling (Ma and Hovy, 2016), we also utilize such structure for template extraction. Cross-entropy (CE) is used for training and the loss is JT E . 3.2 Template Transforming J(θ) = JT E (θ) + JT T (θ) + JT F (θ) Take the extracted template from previous module as input, template transforming module searches for candidate templates for paraphrasing. We utilize a retrieval-based approach to search for candidate templates. We first build an index for all the templates in our dataset. Then we use a score function (e.g. cosine similarity) to evaluate the similarity between original template and candidate templates to find ou"
D19-5514,J83-1001,0,0.745432,"topic words and template words. Template words define the information need of the question while topic words are related to some specific entities or events. Second, for a pair of paraphrase questions, they tend to share the Introduction Paraphrase means sentences or phrases that convey the same meaning with different expressions. Popular tasks about paraphrases are paraphrase identification (Yin and Sch¨utze, 2015), paraphrase generation (Li et al., 2018; Gupta et al., 2018), sentence rewriting (Barzilay and Lee, 2003), etc. As a special case of paraphrase generation, question paraphrasing (McKeown, 1983) aims to restate an input question. It can be applied in a question answering system for the expansion of question set to enhance the coverage of candidate answers. Besides, it is able to probe the need of users within an interactive system by rephrasing questions. Traditional approaches for paraphrase generation are mostly based on external knowledge, including manually constructed templates (McKeown, 1983), or external thesaurus (Hassan et al., * Corresponding author 109 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 109–114 c Hong Kong, No"
D19-5514,P02-1040,0,0.103625,"s poor. The loss for training seq-to-seq model is JT T . 3.3 4 4.1 Experiments Experimental Setup We test our model on datasets described in Section 2. Both datasets are divided into training, validation and test with split ratio of 7:2:1. We use Adam as our optimization method and set the learning rate as 0.0001. We set the dimension of hidden state as 128. For padding, we set the max length as 64. We use BERT-Chinese tokenizer(Devlin et al., 2018) to separate characters. For the general evaluation, we evaluate the quality of the generated paraphrase questions. BLEU1, BLEU-2, BLEU-3, BLEU-4((Papineni et al., 2002)) are used as evaluation measures. Three models are compared. seq2seq (Bahdanau et al., 2014) uses an encoder-decoder structure with attention for generation. ours (separate) this is our pipeline model consisting of three modules. Each module is trained separately. ours (joint) this is our pipeline model consisting of three modules and joint training is used. Template Filling Take a candidate template and topic words as input, template filling module fills each slot in the template with topic words to form a new question. In practice, we use two encoders to encode subsequence of topic part and"
D19-5514,P18-1015,0,0.0319809,"are topic words and others are templates. 2007). The generated paraphrases are usually fluent and informative. However, it is very timeconsuming to construct templates by human and external thesaurus are always absent for some languages. Recently, researchers start to use neural network based approaches by formulating the generation task in a fashion of sequence-tosequence (Sutskever et al., 2014; Bahdanau et al., 2014; Prakash et al., 2016). However, these models tend to “lose control” generating some unpredictable results. In order to alleviate the uncertainty in sequenceto-sequence model, Cao et al. (2018) propose to search for similar sentences as soft template to back up the neural generation model in the scenario of text summarization. With this inspiration, we also try to bridge neural-based models and template-based approaches for question paraphrasing. An example of question paraphrasing can be seen in Table 1. We have two observations. First, words in a question can be easily divided into two types, namely, topic words and template words. Template words define the information need of the question while topic words are related to some specific entities or events. Second, for a pair of par"
D19-5514,P15-2070,0,0.0308568,"Missing"
D19-5514,C16-1275,0,0.0137895,"odel in a large margin and the advantage is more promising when the size of training sample is small. 1 Table 1: Example of an question and its paraphrases. Underlined phrases are topic words and others are templates. 2007). The generated paraphrases are usually fluent and informative. However, it is very timeconsuming to construct templates by human and external thesaurus are always absent for some languages. Recently, researchers start to use neural network based approaches by formulating the generation task in a fashion of sequence-tosequence (Sutskever et al., 2014; Bahdanau et al., 2014; Prakash et al., 2016). However, these models tend to “lose control” generating some unpredictable results. In order to alleviate the uncertainty in sequenceto-sequence model, Cao et al. (2018) propose to search for similar sentences as soft template to back up the neural generation model in the scenario of text summarization. With this inspiration, we also try to bridge neural-based models and template-based approaches for question paraphrasing. An example of question paraphrasing can be seen in Table 1. We have two observations. First, words in a question can be easily divided into two types, namely, topic words"
D19-5514,P13-1158,0,0.0222087,"nly. It is the same joint model as we used in the previous section. f+a2a: Model is pre-trained on the financial dataset and then fine-tuned on the automotive dataset. Model Seq2Seq(f2a) Seq2Seq(a2a) Seq2Seq(f+a2a) ours (f2a) ours (a2a) ours (f+a2a) BLEU-1 0.251 0.581 0.715 0.796 0.859 0.881 BLEU-2 0.167 0.526 0.661 0.722 0.808 0.835 BLEU-3 0.110 0.482 0.619 0.656 0.763 0.791 5 Related Work There are two lines of research for paraphrase generation including knowledge based ones and neural network based ones. Some researchers provide rules (Bhagat and Hovy, 2013) or corpus including knowledge (Fader et al., 2013; Ganitkevitch et al., 2013; Pavlick et al., 2015). Other researchers try to make use of templates (Berant and Liang, 2014), semantic information (Kozlowski et al., 2003) and thesaurus (Hassan et al., 2007) for paraphrase generation. Rush (2015) have applied Seq2Seq model with attention mechanism for text summarization. Prakash (2016) employ a residual net in Seq2Seq model to generate paraphrases. Cao (2017) combine a copying decoder and a generative decoder for paraphrase generation. Cao(2018) try to utilize template information to help text summarization, however, the template is vague in th"
D19-5514,N15-1091,0,0.035459,"Missing"
he-etal-2012-quantising,W11-0705,0,\N,Missing
he-etal-2012-quantising,pak-paroubek-2010-twitter,0,\N,Missing
J18-4008,W05-0613,0,0.0178369,"auses, namely, elementary discourse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the complex structure and informal language style, discourse parsing on conversations is still a challenging problem (Perret et al. 2016). Most research focuses o"
J18-4008,P06-1026,0,0.123548,"Missing"
J18-4008,D14-1226,0,0.0636155,"Missing"
J18-4008,W04-3240,0,0.130138,"Missing"
J18-4008,W09-3951,0,0.0825383,"Missing"
J18-4008,P15-1077,0,0.0674667,"Missing"
J18-4008,P06-1039,0,0.142338,"Missing"
J18-4008,P16-2044,0,0.0311334,"Another potential line is to combine our work with representation learning on social media. Although some previous studies have provided intriguing approaches to learning representations at the level of words (Mikolov et al. 2013; Mikolov, Yih, and Zweig 2013), sentences (Le and Mikolov 2014), and paragraphs (Kiros et al. 2015), they are limited in modeling social media content with colloquial relations. Following similar ideas in this work, where discourse and topics are jointly explored, we can conduct other types of representation learning, embeddings for words (Li et al. 2017b), messages (Dhingra et al. 2016), or users (Ding, Bickel, and Pan 2017), in the context of conversations, which should complement social media representation learning and vice versa. Appendix A In this section, we present the key steps for inferring our joint model of conversational discourse and latent topics. Its generation process has been described in Section 3. As described in Section 3, we use collapsed Gibbs sampling (Griffiths et al. 2004) for model inference. Before providing the formula of sampling steps, we first define the notations of all variables used in the formulations of Gibbs sampling, described in Table A"
J18-4008,D17-1241,0,0.0428534,"Missing"
J18-4008,C12-1047,0,0.0996001,"hy content and noncontent background (general information) (Daum´e and Marcu 2006; Haghighi and ¨ 2010), and (2) to cluster sentences Vanderwende 2009; C ¸ elikyilmaz and Hakkani-Tur 724 Li et al. A Joint Model of Discourse and Topics on Microblogs or documents into topics, with summaries then generated from each topic cluster for minimizing content redundancy (Salton et al. 1997; McKeown et al. 1999; Siddharthan, Nenkova, and McKeown 2004). Similar techniques have also been applied to summarize events or opinions on microblogs (Chakrabarti and Punera 2011; Long et al. 2011; Rosa et al. 2011; Duan et al. 2012; Shen et al. 2013; Meng et al. 2012). Our downstream application on microblog summarization lies in the research line of point (1), whereas we integrate the effects of discourse on key content identification, which has not been studied in any prior work. Also it is worth noting that, following point (2) to cluster messages before summarization is beyond the scope of this work because we are focusing on summarizing a single conversation tree, on which there are limited topics. We leave the potential of using our model to segment topics for multiconversation summarization to future work. 2.2 Di"
J18-4008,C10-1034,0,0.371052,"microblog conversations. 1. Introduction Over the past two decades, the Internet has been revolutionizing the way we communicate. Microblogging, a social networking channel over the Internet, further accelerates communication and information exchange. Popular microblog platforms, such as Twitter1 and Sina Weibo,2 have become important outlets for individuals to share information and voice opinions, which further benefit downstream applications such as instant detection of breaking events (Lin et al. 2010; Weng and Lee 2011; Peng et al. 2015), real-time and ad hoc search of microblog messages (Duan et al. 2010; Li et al. 2015b), public opinions and user behavior understanding on societal issues (Pak and Paroubek 2010; Popescu and Pennacchiotti 2010; Kouloumpis, Wilson, and Moore 2011), and so forth. However, the explosive growth of microblog data far outpaces human beings’ speed of reading and understanding. As a consequence, there is a pressing need for effective natural language processing (NLP) systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content (Farzindar and Inkpen 2015). As one of the important and fundamen"
J18-4008,P14-1048,0,0.0169086,"nd so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the complex structure and informal language style, discourse parsing on conversations is still a challenging problem (Perret et al. 2016). Most research focuses on the detection of dialogue acts (DAs),4 which are 4 Dialogue act can be used interchangeably with speech act (Stolcke et"
J18-4008,P07-1062,0,0.0392749,"urse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the complex structure and informal language style, discourse parsing on conversations is still a challenging problem (Perret et al. 2016). Most research focuses on the detection of dial"
J18-4008,P11-2008,0,0.133607,"Missing"
J18-4008,N09-1041,0,0.248132,"as conversation starter. Topic Assignments. Messages on one conversation tree focus on related topics. To exploit such intuition in topic assignments, the topic of each message m on conversation tree c (i.e., zc,m ) is sampled from the topic mixture θc of conversation tree c. 3.2 Word-Level Modeling To distinguish varying types of word distributions to separately capture discourse, topic, and background representations, we follow the solutions from previous work to assign each word as a discrete and exact source that reflects one particular type of word representation (Daum´e and Marcu 2006; Haghighi and Vanderwende 2009; Ritter, Cherry, and Dolan 2010). To this end, for each word n in message m and tree c, a ternary variable xc,m,n ∈ {DISC, TOPIC, BACK} controls word n to fall into one of the three types: discourse, topic, and background word. In doing so, words in the given collection are explicitly separated into three types, based on which the word distributions representing discourse, topic, and background components are separated accordingly. Discourse words. (DISC) indicate the discourse role of a message; for example, in Figure 1, “How” and the question mark “?” reflect that [R1] should be assigned th"
J18-4008,P14-1002,0,0.0234826,"ded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the complex structure and informal language style, discourse parsing on conversations is still a challenging problem (Perret et al. 2016). Most research focuses on the detection of dialogue acts (DAs),4 which are 4 Dialogue act can be used interchangeably with speech act (Stolcke et al. 2000). 725 Computational Linguistics Volume 44,"
J18-4008,D12-1083,0,0.0384174,"Missing"
J18-4008,P13-1160,0,0.0181528,"Missing"
J18-4008,D15-1259,1,0.704119,"tions. 1. Introduction Over the past two decades, the Internet has been revolutionizing the way we communicate. Microblogging, a social networking channel over the Internet, further accelerates communication and information exchange. Popular microblog platforms, such as Twitter1 and Sina Weibo,2 have become important outlets for individuals to share information and voice opinions, which further benefit downstream applications such as instant detection of breaking events (Lin et al. 2010; Weng and Lee 2011; Peng et al. 2015), real-time and ad hoc search of microblog messages (Duan et al. 2010; Li et al. 2015b), public opinions and user behavior understanding on societal issues (Pak and Paroubek 2010; Popescu and Pennacchiotti 2010; Kouloumpis, Wilson, and Moore 2011), and so forth. However, the explosive growth of microblog data far outpaces human beings’ speed of reading and understanding. As a consequence, there is a pressing need for effective natural language processing (NLP) systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content (Farzindar and Inkpen 2015). As one of the important and fundamental text analyti"
J18-4008,P16-1199,1,0.313837,"t al. 2013). Quan et al. (2015) propose self-aggregation-based topic modeling (SATM) that aggregates texts jointly with topic inference. Another popular solution is to take into account word relations to alleviate document-level word sparseness. Biterm topic model (BTM) directly models the generation of word-pair co-occurrence patterns in each individual message (Yan et al. 2013; Cheng et al. 2014). More recently, word embeddings trained by large-scale external data are leveraged to capture word relations and improve topic models on short texts (Das, Zaheer, and Dyer 2015; Nguyen et al. 2015; Li et al. 2016a, 2017a; Shi et al. 2017; Xun et al. 2017). To date, most efforts focus on content in messages, but ignore the rich discourse structure embedded in ubiquitous user interactions on microblog platforms. On microblogs, which were originally built for user communication and interaction, conversations are freely formed on issues of interests by reposting messages and replying to others. When joining a conversation, users generally post topically related content, which naturally provide effective contextual information for topic discovery. AlvarezMelis and Saveski (2016) have shown that simply aggr"
J18-4008,D14-1220,0,0.0410723,"Missing"
J18-4008,W04-1013,0,0.0251576,"scourse and latent topics is fully unsupervised, therefore does not require any manual annotation. For evaluation, we conduct quantitative and qualitative analysis on large-scale Twitter and Sina Weibo corpora. Experimental results show that topics induced by our model are more coherent than existing models. Qualitative analysis on discourse further shows that our model can yield meaningful clusters of words related to manually crafted discourse categories. In addition, we present an empirical study on downstream application of microblog conversation summarization. Empirical results on ROUGE (Lin 2004) show that summaries produced based on our joint model contain more salient information than state-of-the-art summarization systems. Human evaluation also indicates that our output summaries are competitive with existing unsupervised summarization systems in the aspects of informativeness, conciseness, and readability. In summary, our contributions in this article are 3-fold: • 722 Microblog posts organized as conversation trees for topic modeling. We propose a novel concept of representing microblog posts as conversation trees by connecting microblog posts based on reposting and replying Li e"
J18-4008,D09-1036,0,0.0944317,"Missing"
J18-4008,W11-0709,0,0.0644897,"Missing"
J18-4008,C12-1104,0,0.0289953,"ev et al. 2004), TF-IDF (Inouye and Kalita 2011), integer linear programming (Liu, Liu, and Weng 2011; Takamura, Yokono, and Okumura 2011), graph learning (Sharifi, Hutton, and Kalita 2010), and so on. Later, researchers found that standard summarization models are not suitable for microblog posts because of the severe redundancy, noise, and sparsity problems exhibited in short and colloquial messages (Chang et al. 2013; Li et al. 2015a). To solve these problems, one common solution is to use social signals such as the user influence and retweet counts to help summarization (Duan et al. 2012; Liu et al. 2012; Chang et al. 2013). Different from the aforementioned studies, we do not include external features such 726 Li et al. A Joint Model of Discourse and Topics on Microblogs as the social network structure, which ensures the general applicability of our approach when applied to domains without such information. Discourse has been reported useful to microblog summarization. Zhang et al. (2013) and Li et al. (2015a) leverage dialogue acts to indicate summary-worthy messages. In the field of conversation summarization from other domains (e.g., meetings, forums, and e-mails), it is also popular to l"
J18-4008,J00-3005,0,0.250894,"re theory (RST) (Mann and Thompson 1988) is one of the most influential discourse theories. According to its assumption, a coherent document can be represented by text units at different levels (e.g., clauses, sentences, paragraphs) in a hierarchical tree structure. In particular, the minimal units in RST (i.e., leaves of the tree structure) are defined as sub-sentential clauses, namely, elementary discourse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became"
J18-4008,N13-1090,0,0.156114,"s regardless of the different discourse roles of messages. The work by Li et al. (2016b) serves as another prior effort to leverage conversation structure, captured by a supervised discourse tagger, on topic induction. Different from them, our model learns discourse structure for conversations in a fully unsupervised manner, which does not require annotated data. Another line of research tackles data sparseness by modeling word relations rather than word occurrences in documents. For example, recent research work has shown that distributional similarities of words captured by word embeddings (Mikolov et al. 2013; Mikolov, Yih, and Zweig 2013) are useful in recognizing interpretable topic word clusters from short texts (Das, Zaheer, and Dyer 2015; Nguyen et al. 2015; Li et al. 2016a, 2017a; Shi et al. 2017; Xun et al. 2017). These topic models heavily rely on meaningful word embeddings needed to be trained on a large-scale, high-quality external corpus, which should be both in the same domain and the same language as the data for topic modeling (Bollegala, Maehara, and Kawarabayashi 2015). However, such external resource is not always available. For example, to the best of our knowledge, there current"
J18-4008,D11-1024,0,0.0306012,"ocessing, which retains the same common settings to ensure comparable performance.18 Evaluation Metrics. Topic model evaluation is inherently difficult. Although in many previous studies perplexity is a popular metric to evaluate the predictive abilities of topic models given held-out data set with unseen words (Blei, Ng, and Jordan 2003), we do not consider perplexity here because high perplexity does not necessarily indicate semantically coherent topics in human perception (Chang et al. 2009). The quality of topics is commonly measured by UCI (Newman et al. 2010) and UMass coherence scores (Mimno et al. 2011), assuming that words representing a coherent topic are likely to co-occur within the same document. We only consider UMass coherence here as UMass and UCI generally agree with each other, according to Stevens et al. (2012). We also consider a newer evaluation metric, the CV coherence ¨ measure (Roder, Both, and Hinneburg 2015), as it has been proven to provide the scores closest to human evaluation compared with other widely used topic coherence metrics, including UCI and UMass scores.19 For the CV coherence measure, in brief, given a word list for topic representations (i.e., the top N words"
J18-4008,N06-1047,0,0.0509304,"we do not include external features such 726 Li et al. A Joint Model of Discourse and Topics on Microblogs as the social network structure, which ensures the general applicability of our approach when applied to domains without such information. Discourse has been reported useful to microblog summarization. Zhang et al. (2013) and Li et al. (2015a) leverage dialogue acts to indicate summary-worthy messages. In the field of conversation summarization from other domains (e.g., meetings, forums, and e-mails), it is also popular to leverage the pre-detected discourse structure for summarization (Murray et al. 2006; McKeown, Shrestha, and Rambow 2007; Wang and Cardie 2013; Bhatia, Biyani, and Mitra 2014; Bokaei, Sameti, and Liu 2016). Oya and Carenini (2014) and Qin, Wang, and Kim (2017) address discourse tagging together with salient content discovery on e-mails and meetings, and show the usefulness of their relations in summarization. For all the systems mentioned here, manually crafted tags and annotated data are required for discourse modeling. Instead, the discourse structure is discovered in a fully unsupervised manner in our model, which is represented by word distributions and can be different f"
J18-4008,N10-1012,0,0.234042,"(Phan, Nguyen, and Horiguchi 2008; Zeng et al. 2018a), and recommendation on microblogs (Zeng et al. 2018b). Conventionally, probabilistic topic models (e.g., probabilistic latent semantic analysis [Hofmann 1999] and latent Dirichlet allocation [Blei et al. 2003]) have achieved huge success over the past decade, owing to their fully unsupervised manner and ease of extension. The semantic structure discovered by these topic models have facilitated the progress of many research fields, for example, information retrieval (Boyd-Graber, Hu, and Mimno 2017), data mining (Lin et al. 2015), and NLP (Newman et al. 2010). Nevertheless, ascribing to their reliance on document-level word co-occurrence patterns, the progress is still limited to formal conventional documents such as news reports (Blei, Ng, and Jordan 2003) and scientific articles (Rosen-Zvi et al. 2004). The aforementioned models work poorly when directly applied to short and colloquial texts (e.g., microblog posts) owing to severe sparsity exhibited in such text genre (Wang and McCallum 2006; Hong and Davison 2010). Previous research has proposed several methods to deal with the sparsity issue in short texts. One common approach is to aggregate"
J18-4008,Q15-1022,0,0.575772,"ing 2010; Mehrotra et al. 2013). Quan et al. (2015) propose self-aggregation-based topic modeling (SATM) that aggregates texts jointly with topic inference. Another popular solution is to take into account word relations to alleviate document-level word sparseness. Biterm topic model (BTM) directly models the generation of word-pair co-occurrence patterns in each individual message (Yan et al. 2013; Cheng et al. 2014). More recently, word embeddings trained by large-scale external data are leveraged to capture word relations and improve topic models on short texts (Das, Zaheer, and Dyer 2015; Nguyen et al. 2015; Li et al. 2016a, 2017a; Shi et al. 2017; Xun et al. 2017). To date, most efforts focus on content in messages, but ignore the rich discourse structure embedded in ubiquitous user interactions on microblog platforms. On microblogs, which were originally built for user communication and interaction, conversations are freely formed on issues of interests by reposting messages and replying to others. When joining a conversation, users generally post topically related content, which naturally provide effective contextual information for topic discovery. AlvarezMelis and Saveski (2016) have shown"
J18-4008,N13-1039,0,0.0942168,"Missing"
J18-4008,W14-4318,0,0.0172156,"which ensures the general applicability of our approach when applied to domains without such information. Discourse has been reported useful to microblog summarization. Zhang et al. (2013) and Li et al. (2015a) leverage dialogue acts to indicate summary-worthy messages. In the field of conversation summarization from other domains (e.g., meetings, forums, and e-mails), it is also popular to leverage the pre-detected discourse structure for summarization (Murray et al. 2006; McKeown, Shrestha, and Rambow 2007; Wang and Cardie 2013; Bhatia, Biyani, and Mitra 2014; Bokaei, Sameti, and Liu 2016). Oya and Carenini (2014) and Qin, Wang, and Kim (2017) address discourse tagging together with salient content discovery on e-mails and meetings, and show the usefulness of their relations in summarization. For all the systems mentioned here, manually crafted tags and annotated data are required for discourse modeling. Instead, the discourse structure is discovered in a fully unsupervised manner in our model, which is represented by word distributions and can be different from any human designed discourse inventory. The effects of such discourse representations on salient content identification have not been explored"
J18-4008,pak-paroubek-2010-twitter,0,0.131623,"Missing"
J18-4008,N16-1013,0,0.0483436,"Missing"
J18-4008,prasad-etal-2008-penn,0,0.0237269,"n a hierarchical tree structure. In particular, the minimal units in RST (i.e., leaves of the tree structure) are defined as sub-sentential clauses, namely, elementary discourse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the comple"
J18-4008,P17-1090,0,0.0965772,"Missing"
J18-4008,P13-4009,0,0.0372628,"Missing"
J18-4008,radev-etal-2004-mead,0,0.135384,"Missing"
J18-4008,J02-4001,0,0.172797,"Missing"
J18-4008,N10-1020,0,0.38114,"Missing"
J18-4008,N13-1135,0,0.0207025,"content background (general information) (Daum´e and Marcu 2006; Haghighi and ¨ 2010), and (2) to cluster sentences Vanderwende 2009; C ¸ elikyilmaz and Hakkani-Tur 724 Li et al. A Joint Model of Discourse and Topics on Microblogs or documents into topics, with summaries then generated from each topic cluster for minimizing content redundancy (Salton et al. 1997; McKeown et al. 1999; Siddharthan, Nenkova, and McKeown 2004). Similar techniques have also been applied to summarize events or opinions on microblogs (Chakrabarti and Punera 2011; Long et al. 2011; Rosa et al. 2011; Duan et al. 2012; Shen et al. 2013; Meng et al. 2012). Our downstream application on microblog summarization lies in the research line of point (1), whereas we integrate the effects of discourse on key content identification, which has not been studied in any prior work. Also it is worth noting that, following point (2) to cluster messages before summarization is beyond the scope of this work because we are focusing on summarizing a single conversation tree, on which there are limited topics. We leave the potential of using our model to segment topics for multiconversation summarization to future work. 2.2 Discourse Analysis D"
J18-4008,C04-1129,0,0.0301286,"Missing"
J18-4008,N03-1030,0,0.291035,"Missing"
J18-4008,D12-1087,0,0.0833286,"Missing"
J18-4008,J00-3003,0,0.760268,"Missing"
J18-4008,N09-1064,0,0.0691813,"Missing"
J18-4008,C04-1048,0,0.10585,"Missing"
J18-4008,P13-1137,0,0.0670602,"Missing"
J18-4008,J05-2005,0,0.082294,"rent levels (e.g., clauses, sentences, paragraphs) in a hierarchical tree structure. In particular, the minimal units in RST (i.e., leaves of the tree structure) are defined as sub-sentential clauses, namely, elementary discourse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversati"
J18-4008,D18-1351,1,0.865754,") systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content (Farzindar and Inkpen 2015). As one of the important and fundamental text analytic approaches, topic models extract key components embedded in microblog content by clustering words that describe similar semantic meanings to form latent “topics.” The derived intermediate topic representations have proven beneficial to many NLP applications for social media, such as summarization (Harabagiu and Hickl 2011), classification (Phan, Nguyen, and Horiguchi 2008; Zeng et al. 2018a), and recommendation on microblogs (Zeng et al. 2018b). Conventionally, probabilistic topic models (e.g., probabilistic latent semantic analysis [Hofmann 1999] and latent Dirichlet allocation [Blei et al. 2003]) have achieved huge success over the past decade, owing to their fully unsupervised manner and ease of extension. The semantic structure discovered by these topic models have facilitated the progress of many research fields, for example, information retrieval (Boyd-Graber, Hu, and Mimno 2017), data mining (Lin et al. 2015), and NLP (Newman et al. 2010). Nevertheless, ascribing to thei"
J18-4008,N18-1035,1,0.810545,") systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content (Farzindar and Inkpen 2015). As one of the important and fundamental text analytic approaches, topic models extract key components embedded in microblog content by clustering words that describe similar semantic meanings to form latent “topics.” The derived intermediate topic representations have proven beneficial to many NLP applications for social media, such as summarization (Harabagiu and Hickl 2011), classification (Phan, Nguyen, and Horiguchi 2008; Zeng et al. 2018a), and recommendation on microblogs (Zeng et al. 2018b). Conventionally, probabilistic topic models (e.g., probabilistic latent semantic analysis [Hofmann 1999] and latent Dirichlet allocation [Blei et al. 2003]) have achieved huge success over the past decade, owing to their fully unsupervised manner and ease of extension. The semantic structure discovered by these topic models have facilitated the progress of many research fields, for example, information retrieval (Boyd-Graber, Hu, and Mimno 2017), data mining (Lin et al. 2015), and NLP (Newman et al. 2010). Nevertheless, ascribing to thei"
J18-4008,P15-1071,0,\N,Missing
J18-4008,P10-1084,0,\N,Missing
P13-2011,J96-2004,0,0.0239954,"Missing"
P13-2011,W10-3001,0,0.664687,"Missing"
P13-2011,P09-2044,0,0.554532,"Missing"
P13-2011,W10-3004,0,0.0563479,"ork on uncertainty identification focused on classifying sentences into uncertain or definite categories. Existing approaches are mainly based on supervised methods (Light et al., 2004; Medlock and Briscoe, 2007; Medlock, 2008; Szarvas, 2008) using the annotated corpus with different types of features including Part-OfSpeech (POS) tags, stems, n-grams, etc.. Classification of uncertain sentences was consolidated as a task in the 2010 edition of CoNLL shared task on learning to detect hedge cues and their scope in natural language text (Farkas et al., 2010). The best system for Wikipedia data (Georgescul, 2010) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional 2 http://www.timeml.org/site/timebank/ timebank.html Random Fields (CRF). In our work, we conduct an empirical study of uncertainty identification on tweets dataset and explore the effectiveness of different types of features (i.e., content-based, user-based and Twitterspecific) from social media context. 3 Uncertainty corpus for microblogs 3.1 Types of uncertainty in microblogs Traditionally, uncertainty can be divided into two categories, namely Epistemic and Hypothetical ("
P13-2011,W08-0607,0,0.0429344,"al media. Although uncertainty has been studied theoretically for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise q"
P13-2011,D11-1147,0,0.0933427,"Missing"
P13-2011,J12-2004,0,0.617265,"Missing"
P13-2011,P08-1033,0,0.113025,"cally for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to external information"
P13-2011,W10-3002,0,0.449553,"gories. Existing approaches are mainly based on supervised methods (Light et al., 2004; Medlock and Briscoe, 2007; Medlock, 2008; Szarvas, 2008) using the annotated corpus with different types of features including Part-OfSpeech (POS) tags, stems, n-grams, etc.. Classification of uncertain sentences was consolidated as a task in the 2010 edition of CoNLL shared task on learning to detect hedge cues and their scope in natural language text (Farkas et al., 2010). The best system for Wikipedia data (Georgescul, 2010) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional 2 http://www.timeml.org/site/timebank/ timebank.html Random Fields (CRF). In our work, we conduct an empirical study of uncertainty identification on tweets dataset and explore the effectiveness of different types of features (i.e., content-based, user-based and Twitterspecific) from social media context. 3 Uncertainty corpus for microblogs 3.1 Types of uncertainty in microblogs Traditionally, uncertainty can be divided into two categories, namely Epistemic and Hypothetical (Kiefer, 2005). For Epistemic, there are two sub-classes Possible and Probable. For Hypothetical, th"
P13-2011,W04-3103,0,0.203939,"g time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to external information when making uncertain"
P13-2011,P07-1125,0,0.270428,"y has been studied theoretically for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to extern"
P13-2011,J12-2001,0,0.0854876,"ed as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to external information when making uncertain statements. But, neither of the uncertainty expressions can be represented based on the existing types of uncertainty defined in the literature. Therefore, a different uncertainty classification scheme is needed in social media context. In this pap"
P13-2011,W08-0606,0,\N,Missing
P15-2009,D14-1076,1,0.849007,"r than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to guide sentence compression for compressive summary generation. We extend an unsupervised dependency"
P15-2009,C12-1029,0,0.0185575,"y reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to guide sentence compressio"
P15-2009,P06-1048,0,0.0250579,"subsequent compression component. Framework 2.2 We adopt a pipeline approach for compressive news highlights generation. The framework integrates a sentence extraction component and a post-sentence compression component. Each is described below. 2.1 tfw,x tfw,y (idfw )2 qP 2 2 ∈x (tfwi ,x idfwi ) × w ∈y (tfwi ,y idfwi ) P sim(x, y) = qP Dependency Tree Based Sentence Compression We use an unsupervised dependency tree based compression framework (Filippova and Strube, 2008) as our baseline. This method achieved a higher F-score (Riezler et al., 2003) than other systems on the Edinburgh corpus (Clarke and Lapata, 2006). We will introduce the baseline in this part and describe our extended model that leverages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w1 , w2 , ..., wm , identify a subset of the words of s, such that it is grammatical and preserves essential information of s. In the baseline framework, a dependency graph for an original sentence is first generated and then the compression is done by deleting edges of the dependency graph. The goal is to find a subtree with the highest score: X f (X) = xe × winf o"
P15-2009,W03-1101,0,0.0224033,"the gist of the document, and can dramatically reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while"
P15-2009,W04-1013,0,0.00405067,"performs the corresponding original baseline, LexRank and HGRW. • The improvement obtained by LexRank+SC+both compared to LexRank is more promising than that obtained by HGRW+SC+both compared to HGRW. This may be because HGRW has used tweet information already, and leaves limited room for improvement for the sentence compression model when using the same source of information. Table 2: Overall Performance. Bold: the best value in each group in terms of different metrics. Following (Wei and Gao, 2014), we output 4 sentences for each news article as the highlights and report the ROUGE-1 scores (Lin, 2004) using human-generated highlights as the reference. The sentence compression rates are set to 0.8 for short sentences containing fewer than 9 words, and 0.5 for long sentences with more than 9 words, following (Filippova and Strube, 2008). We empirically use 0.8 for α, β and  such that tweets have more impact for both sentence selection and compression. We leveraged The New York Times Annotated Corpus (LDC Catalog No: LDC2008T19) as the background news corpus. It has both the original news articles and human generated summaries. The Stanford Parser4 is used to obtain dependency trees. The bac"
P15-2009,D13-1156,1,0.858019,"usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to guide sentence compression for compressive summary generation. We extend an unsup"
P15-2009,D13-1155,0,0.114691,"elevant tweets. For a tweet winf o (e) = 51 Psummary (n) Particle (n) (4) wsyn (e) = P (l|h) PrelevantT (n) and PbackgroundT (n) are the unigram probabilities of word n in two language models trained on the relevant tweet dataset and a background tweet dataset respectively. The new syntactic importance score is: (5) where Psummary (n) and Particle (n) are the unigram probabilities of word n in the two language models trained on human generated summaries and the original articles respectively. P (l|h) is the conditional probability of label l given head h. Note that here we use the formula in (Filippova and Altun, 2013) for winf o (e), which was shown to be more effective for sentence compression than the original formula in (Filippova and Strube, 2008). The optimization problem can be solved under the tree structure and length constraints by integer linear programming1 . Given that L is the maximum number of words permitted for the compression, the length constraint is simply represented as: X xe ≤ L (6) T wsyn (e) = 3 Setup We evaluate our pipeline news highlights generation framework on a public corpus based on CNN/USAToday news (Wei and Gao, 2014). This corpus was constructed via an event-oriented strate"
P15-2009,N03-1026,0,0.0556107,"ut the top news sentences as the highlights, and the input to the subsequent compression component. Framework 2.2 We adopt a pipeline approach for compressive news highlights generation. The framework integrates a sentence extraction component and a post-sentence compression component. Each is described below. 2.1 tfw,x tfw,y (idfw )2 qP 2 2 ∈x (tfwi ,x idfwi ) × w ∈y (tfwi ,y idfwi ) P sim(x, y) = qP Dependency Tree Based Sentence Compression We use an unsupervised dependency tree based compression framework (Filippova and Strube, 2008) as our baseline. This method achieved a higher F-score (Riezler et al., 2003) than other systems on the Edinburgh corpus (Clarke and Lapata, 2006). We will introduce the baseline in this part and describe our extended model that leverages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w1 , w2 , ..., wm , identify a subset of the words of s, such that it is grammatical and preserves essential information of s. In the baseline framework, a dependency graph for an original sentence is first generated and then the compression is done by deleting edges of the dependency graph. The go"
P15-2009,W08-1105,0,0.386495,"DF value. Although both types of nodes can be ranked in this framework, we only output the top news sentences as the highlights, and the input to the subsequent compression component. Framework 2.2 We adopt a pipeline approach for compressive news highlights generation. The framework integrates a sentence extraction component and a post-sentence compression component. Each is described below. 2.1 tfw,x tfw,y (idfw )2 qP 2 2 ∈x (tfwi ,x idfwi ) × w ∈y (tfwi ,y idfwi ) P sim(x, y) = qP Dependency Tree Based Sentence Compression We use an unsupervised dependency tree based compression framework (Filippova and Strube, 2008) as our baseline. This method achieved a higher F-score (Riezler et al., 2003) than other systems on the Edinburgh corpus (Clarke and Lapata, 2006). We will introduce the baseline in this part and describe our extended model that leverages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w1 , w2 , ..., wm , identify a subset of the words of s, such that it is grammatical and preserves essential information of s. In the baseline framework, a dependency graph for an original sentence is first generated and"
P15-2009,N10-1131,0,0.0613254,"f the document, and can dramatically reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to gu"
P15-2009,P13-1136,0,0.0247074,"mation load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to guide sentence compression for compressive s"
P15-2009,C14-1083,1,0.8058,"er together with their comments. The availability of cross-media information provides new opportunities for traditional tasks of Natural Language Processing (Zhao et al., 2011; Subaˇsi´c and Berendt, 2011; Gao et al., 2012; Kothari et al., 2013; ˇ Stajner et al., 2013). In this paper, we propose to use relevant tweets of a news article to guide the sentence compression process in a pipeline framework for generating compressive news highlights. This is a pioneer study for using such parallel data to guide sentence compression for document summarization. Our work shares some similar ideas with (Wei and Gao, 2014; Wei and Gao, 2015). They also attempted to use tweets to help news highlights generation. Wei and Gao (2014) derived external features based on the relevant tweet collection to assist the ranking of the original sentences for extractive summarization in a fashion of supervised machine learning. Wei and Gao (2015) proposed a graph-based approach to simultaneously rank the We explore using relevant tweets of a given news article to help sentence compression for generating compressive news highlights. We extend an unsupervised dependency-tree based sentence compression approach by incorporating"
P15-2009,D13-1047,1,0.889893,"Missing"
P16-2032,N15-1172,0,0.0433705,"s have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisitcs. First, persuasive text contains argument that is not common in other genres. Second, beside the text itself, the interplay between a comment and what it responds to is crucial. Third, the community reaction to the comment also needs to be taken into consideration. In this paper, we propose several sets of features to capture the above mentioned characteristics for persuasive comment identification in the online forum. We constructed a dataset fr"
P16-2032,D14-1083,0,0.0113119,"lated features. Our experiments show that the surface textual features do not perform well compared to the argumentation based features, and the social interaction based features are effective especially when more users participate in the discussion. 1 Introduction With the popularity of online forums such as idebate1 and convinceme2 , researchers have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisitcs. First, persuasive text contains argument that is not common in other genres. Second, beside the te"
P16-2032,D15-1239,0,0.026047,"(2014) looked into the effect of wording while predicting the popularity of social media content. Park et al. (2016) developed an interactive system to assist human moderators to select high quality news. Guerini et al. (2015) modeled a notion of euphony and explored the impact of sounds on different forms of persuasiveness. Their research focused on the phonetic aspect instead of language usage. Reddit based research: Reddit has been used recently for research on social news analysis and recommendation (e.g., (Buntain and Golbeck, 2014)). Researchers also analyzed the language use on Reddit. Jaech et al. (2015) studied how language use affects community reaction to comments in Reddit. Tan et al. (2016) analyzed the interaction dynamics and persuasion strategies in CMV. 6 Conclusion In this paper, we studied the impact of different sets of features on the identification of persuasive comments in the online forum. Our experiment results show that argumentation based features work the best in the early stage of the discussion, while the effectiveness of social interaction based features increases when the number of comments in the thread grows. There are three major future directions for this research."
P16-2032,W13-4008,0,0.0432121,"experiments show that the surface textual features do not perform well compared to the argumentation based features, and the social interaction based features are effective especially when more users participate in the discussion. 1 Introduction With the popularity of online forums such as idebate1 and convinceme2 , researchers have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisitcs. First, persuasive text contains argument that is not common in other genres. Second, beside the text itself, the interp"
P16-2032,W10-0214,0,0.00858958,"ents and social interaction related features. Our experiments show that the surface textual features do not perform well compared to the argumentation based features, and the social interaction based features are effective especially when more users participate in the discussion. 1 Introduction With the popularity of online forums such as idebate1 and convinceme2 , researchers have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisitcs. First, persuasive text contains argument that is not common in other genres. S"
P16-2032,D14-1006,0,0.0517899,"Manning et al., 2014) was used to preprocess the text (i.e., comment splitting, sentence tokenization, POS tagging and NER recognition.). • Argumentation Related Features: We believe a comment’s argumentation quality is a good indicator of its persuasiveness. In order to capture the argumentation related information, we propose two sub-groups of features based on the comment itself and the interplay between the comment and other comments in the discussion. a) Local features: we trained a binary classifier to classify sentences as argumentative and non-argumentative using features proposed in (Stab and Gurevych, 2014). We then use the number and percentage of argumentative sen9 197 It is a comment that replies to the original post directly. Approach random author entry-order LTRtext LTRsocial LTRarg LTRtext+social LTRtext+arg LTRsocial+arg LTRT +S+A LTRall NDCG@1 0.258 0.382 0.460 0.372 0.475† 0.475† 0.494† 0.485† 0.502† ‡ 0.508† ‡ 0.521† ‡ NDCG@5 0.440 0.567 0.600 0.558 0.650† 0.652† 0.666† 0.654† 0.674† ‡ 0.676† ‡ 0.685† ‡ NDCG@10 0.564 0.664 0.689 0.658 0.718† 0.725† 0.733† 0.729† 0.740† 0.743† ‡ 0.752† ‡ Table 3: Performance of first-10 comments ranking (T+S+A: the combination of the three sets of feat"
P16-2032,P14-1017,0,0.208326,"idebate1 and convinceme2 , researchers have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisitcs. First, persuasive text contains argument that is not common in other genres. Second, beside the text itself, the interplay between a comment and what it responds to is crucial. Third, the community reaction to the comment also needs to be taken into consideration. In this paper, we propose several sets of features to capture the above mentioned characteristics for persuasive comment identification in the"
P16-2032,W14-1305,0,0.038754,"k comments for their persuasive scores, including textual information in the comments and social interaction related features. Our experiments show that the surface textual features do not perform well compared to the argumentation based features, and the social interaction based features are effective especially when more users participate in the discussion. 1 Introduction With the popularity of online forums such as idebate1 and convinceme2 , researchers have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisi"
P16-2032,Q13-1028,0,0.0366981,"of online forums such as idebate1 and convinceme2 , researchers have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisitcs. First, persuasive text contains argument that is not common in other genres. Second, beside the text itself, the interplay between a comment and what it responds to is crucial. Third, the community reaction to the comment also needs to be taken into consideration. In this paper, we propose several sets of features to capture the above mentioned characteristics for persuasive comment ide"
P16-2032,P14-5010,0,0.00207461,".2 Features We propose several key features that we hypothesize are predictive of persuasive comments. The full feature list is given in Table 2. • Surface Text Features8 : In order to capture the basic textual information, we use the comment length and content diversity represented as the number of words, POS tags, URLs, and punctuation marks. We also explored unigram features and named entity based features, but they did not improve system performance and are thus not included. • Social Interaction Features: We hypothesize that if a comment attracts more social attention 8 Stanford CoreNLP (Manning et al., 2014) was used to preprocess the text (i.e., comment splitting, sentence tokenization, POS tagging and NER recognition.). • Argumentation Related Features: We believe a comment’s argumentation quality is a good indicator of its persuasiveness. In order to capture the argumentation related information, we propose two sub-groups of features based on the comment itself and the interplay between the comment and other comments in the discussion. a) Local features: we trained a binary classifier to classify sentences as argumentative and non-argumentative using features proposed in (Stab and Gurevych, 20"
P16-3019,W09-2209,0,0.0255686,"ssification tasks. We show that our model achieves a comparable and even better performance than the traditional MT-based method. 1 Introduction With the rapid growth of global Internet, huge amounts of information are created in different languages. It is important to develop cross-lingual NLP systems in order to leverage information from other languages, especially languages with rich annotations. Traditionally, cross-lingual systems rely highly on machine translation (MT) systems (Wan et al., 2011; Wan, 2011; Rigutini et al., 2005; Ling et al., 2008; Amini et al., 2009; Guo and Xiao, 2012; Chen and Ji, 2009; Duh et al., 2011). They translate data in one language into the other, and then apply monolingual models. One problem of such cross-lingual systems is that there is hardly any decent MT system for resourcepoor languages. Another problem is the lack of high quality parallel corpora for resource-poor languages, which is required by MT systems. Other work tried to address these problems by developping language independent representation learning and structural correspondence learning (SCL) (Prettenhofer and Stein, 2010; Xiao and Guo, 2013). They showed some promising results on document level c"
P16-3019,P04-1035,0,0.030122,"idation and test sets. Question classification (QC) aims to determine the category of a given question sentence. For English, we use the TREC1 dataset. For Chinese, we use a QA corpus from HIT-IRLab2 . We kept the six overlapped question types for both English and Chinese corpora. The final corpus includes 4,313 English questions and 4,031 Chinese questions (859 for testing, 859 for validation and 2,313 for training3 ). Sentiment classification on movie review (SC-M) aims to classify a piece of given movie review into positive or negative. For English, we use IMDB polarity movie reviews from (Pang and Lee, 2004) (5,331 positive and 5,331 negative). For Chinese, we use the short Chinese movie reviews from Douban4 . Like IMDB, users from Douban leave their comments along with a score for the movie. We collected 250 one star reviews (lowest score), and 250 five star reviews (highest score). We randomly split the 500 reviews into 200 for validation and 300 for testing. Sentiment classification on product review (SC-P) aims to classify a piece of given product review into positive or negative. We use corpora from (Wan, 2011). Their Chinese dataset contains mostly short reviews. However, their English Amaz"
P16-3019,P10-1114,0,0.0149825,"2011; Rigutini et al., 2005; Ling et al., 2008; Amini et al., 2009; Guo and Xiao, 2012; Chen and Ji, 2009; Duh et al., 2011). They translate data in one language into the other, and then apply monolingual models. One problem of such cross-lingual systems is that there is hardly any decent MT system for resourcepoor languages. Another problem is the lack of high quality parallel corpora for resource-poor languages, which is required by MT systems. Other work tried to address these problems by developping language independent representation learning and structural correspondence learning (SCL) (Prettenhofer and Stein, 2010; Xiao and Guo, 2013). They showed some promising results on document level classification tasks. However, their methods require carefully designed language specific features and find the “pivot features” across languages, which can be very expensive and inefficient. To solve these problems, we develop an efficient and feasible cross-lingual sentence model that is based on convolutional neural network (CNN). Sentence modeling using CNN has shown its great potential in recent years (Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015). One of the advantages is that CNN requires much less expe"
P16-3019,P11-2075,0,0.0126748,"We show that our model achieves a comparable and even better performance than the traditional MT-based method. 1 Introduction With the rapid growth of global Internet, huge amounts of information are created in different languages. It is important to develop cross-lingual NLP systems in order to leverage information from other languages, especially languages with rich annotations. Traditionally, cross-lingual systems rely highly on machine translation (MT) systems (Wan et al., 2011; Wan, 2011; Rigutini et al., 2005; Ling et al., 2008; Amini et al., 2009; Guo and Xiao, 2012; Chen and Ji, 2009; Duh et al., 2011). They translate data in one language into the other, and then apply monolingual models. One problem of such cross-lingual systems is that there is hardly any decent MT system for resourcepoor languages. Another problem is the lack of high quality parallel corpora for resource-poor languages, which is required by MT systems. Other work tried to address these problems by developping language independent representation learning and structural correspondence learning (SCL) (Prettenhofer and Stein, 2010; Xiao and Guo, 2013). They showed some promising results on document level classification tasks"
P16-3019,P14-1062,0,0.0548184,"ge independent representation learning and structural correspondence learning (SCL) (Prettenhofer and Stein, 2010; Xiao and Guo, 2013). They showed some promising results on document level classification tasks. However, their methods require carefully designed language specific features and find the “pivot features” across languages, which can be very expensive and inefficient. To solve these problems, we develop an efficient and feasible cross-lingual sentence model that is based on convolutional neural network (CNN). Sentence modeling using CNN has shown its great potential in recent years (Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015). One of the advantages is that CNN requires much less expertise knowledge than traditional feature based models. The only input of the model, word embeddings, can be learned automatically from large unlabeled text data. There are roughly two main differences between different languages, lexicon and grammar. Lexicon can be seen as a set of symbols with each symbol representing certain meanings. A bilingual dictionary easily enables us to map from one symbol set to another. As for grammar, it decides the organization of lexical symbols, i.e., word order. Different l"
P16-3019,D14-1181,0,0.0208467,"Missing"
P16-3019,J11-3005,0,0.046285,"poor languages. We evaluate our model using English and Chinese data on several sentence classification tasks. We show that our model achieves a comparable and even better performance than the traditional MT-based method. 1 Introduction With the rapid growth of global Internet, huge amounts of information are created in different languages. It is important to develop cross-lingual NLP systems in order to leverage information from other languages, especially languages with rich annotations. Traditionally, cross-lingual systems rely highly on machine translation (MT) systems (Wan et al., 2011; Wan, 2011; Rigutini et al., 2005; Ling et al., 2008; Amini et al., 2009; Guo and Xiao, 2012; Chen and Ji, 2009; Duh et al., 2011). They translate data in one language into the other, and then apply monolingual models. One problem of such cross-lingual systems is that there is hardly any decent MT system for resourcepoor languages. Another problem is the lack of high quality parallel corpora for resource-poor languages, which is required by MT systems. Other work tried to address these problems by developping language independent representation learning and structural correspondence learning (SCL) (Pret"
P16-3019,D13-1153,0,0.0183419,"ing et al., 2008; Amini et al., 2009; Guo and Xiao, 2012; Chen and Ji, 2009; Duh et al., 2011). They translate data in one language into the other, and then apply monolingual models. One problem of such cross-lingual systems is that there is hardly any decent MT system for resourcepoor languages. Another problem is the lack of high quality parallel corpora for resource-poor languages, which is required by MT systems. Other work tried to address these problems by developping language independent representation learning and structural correspondence learning (SCL) (Prettenhofer and Stein, 2010; Xiao and Guo, 2013). They showed some promising results on document level classification tasks. However, their methods require carefully designed language specific features and find the “pivot features” across languages, which can be very expensive and inefficient. To solve these problems, we develop an efficient and feasible cross-lingual sentence model that is based on convolutional neural network (CNN). Sentence modeling using CNN has shown its great potential in recent years (Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015). One of the advantages is that CNN requires much less expertise knowledge than"
P16-3019,P15-2029,0,0.177819,"and structural correspondence learning (SCL) (Prettenhofer and Stein, 2010; Xiao and Guo, 2013). They showed some promising results on document level classification tasks. However, their methods require carefully designed language specific features and find the “pivot features” across languages, which can be very expensive and inefficient. To solve these problems, we develop an efficient and feasible cross-lingual sentence model that is based on convolutional neural network (CNN). Sentence modeling using CNN has shown its great potential in recent years (Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015). One of the advantages is that CNN requires much less expertise knowledge than traditional feature based models. The only input of the model, word embeddings, can be learned automatically from large unlabeled text data. There are roughly two main differences between different languages, lexicon and grammar. Lexicon can be seen as a set of symbols with each symbol representing certain meanings. A bilingual dictionary easily enables us to map from one symbol set to another. As for grammar, it decides the organization of lexical symbols, i.e., word order. Different languages organize their words"
P16-3019,P14-5010,0,0.00354431,"7 +Lex+Phrase 82.19 79.22 83.60 85.01 Table 1: Results of different systems. +Lex: lexical features are used; +Phrase: phrase-based bilingual word embeddings and grammar are used. gradient descent (SGD) learning method. We apply random dropout (Hinton et al., 2012) on the last fully connected layer for regularization. We use ADADELTA (Zeiler, 2012) algorithm to automatically control the learning rate and progress. The batch size for SGD and feature maps are tuned on the validation set for each task and fixed across different configurations. We preprocess all our corpora with Stanford CoreNLP (Manning et al., 2014), including word segmentation, sentence segmentation and dependency parsing. 4.2 Results Table 1 shows the results of different systems. When using the MT based methods, the basic CNN achieves better results than DCNN. One possible reason is that the translation system produces errors, which may affect the performance of dependency parsing. For our method using bilingual word embeddings, basic CNN encodes only lexicon mapping information, and is not good at capturing grammar patterns. Therefore, it is natural this system has the lowest result. DCNN performs better than CNN, because it is able"
P18-2033,D17-1237,1,0.889592,"Missing"
P18-2033,N07-2038,0,0.0554368,"the turn information. In terms of the representation vector of symptoms, it’s dimension is equal to the number of all symptoms, whose elements for positive symptoms are 1, negative symptoms are -1, notsure symptoms are −2 and not-mentioned sympUser Simulator At the beginning of each dialogue session, a user simulator samples a user goal from the experiment dataset. At each turn t, the user takes an action au,t according to the current user state su,t and the previous agent action at−1 , and transits into the next user state su,t+1 . In practice, the user state su is factored into an agenda A (Schatzmann et al., 2007) and a goal G, noted as su = (A, G). During the course of the dialogue, the goal G ensures that the user behaves in a consistent, goal-oriented manner. And the agenda contains a list of symptoms and their status (whether or not they are requested) to track the progress of the conversation. Every dialogue session is initiated by the user 203 4 toms are 0. Each state s ∈ S is the concatenation of these four vectors. Actions A. An action a ∈ A is composed of a dialogue act (e.g., inform, request, deny and confirm) and a slot (i.e., normalized symptoms or a special slot disease). In addition, than"
P18-2033,I17-1074,0,0.222406,"agnose tests, vital signs and medical image. And it is collected accumulatively following a diagnostic procedure in clinic, which involves interactions between patients and doctors and some complicated medical tests. Therefore, it is very expensive to collect EHRs for different diseases. How to collect the information from patient automatically remains the challenge for automatic diagnosis. Recently, due to its promising potentials and alluring commercial values, research about taskoriented dialogue system (DS) has attracted increasing attention in different domains, including ticket booking (Li et al., 2017; Peng et al., 2017a), online shopping (Yan et al., 2017) and restaurant searching (Wen et al., 2017). We believe that applying DS in the medical domain has great potential to reduce the cost of collecting data from patients. However, there is a gap to fill for applying DS in disease identification. There are basically two major challenges. First, the lack of annotated medical dialogue dataset. Second, no available DS framework for disease identification. By addressing these two problems, we make the first move to build a dialogue system facilitating automatic information collection and diagno"
P18-2033,E17-1042,0,0.0993595,"Missing"
P19-1652,W05-0909,0,0.10726,"NIC+WC uses the hard word constraints (WC). NIC+WC+RL is trained with reinforcement learning using the image-grounded vocabulary as the action space. - NIC+WC+WA employs the soft word-aware (WA) mechanism on top of NIC+WC. NIC+WC+WA+RL is trained with reinforcement learning using the image-grounded vocabulary as the action space. - NIC+WC(GT) utilizes the ground-truth vocabufi as the word constraints instead of Wi . lary W This is an oracle. 3.4 Overall Performance We report scores of several widely used metrics for image captioning evaluation, including BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr-D (Vedantam et al., 2015). The overall performance is shown in Table 1. Several findings stand out: - Both NIC+WC and NIC+WC+RL perform better than their counter-part models NIC and NIC+RL across all metrics. This shows the effectiveness of using the word constraint mechanism for reducing irrelevant words for a given image. - Both NIC+WC+WA and NIC+WC+WA+RL outperform NIC+WC and NIC+WC+RL respectively. This shows that the word-aware mechanism effectively guides the generator to better capturing the semantics of a given image. - Compared to NIC+WC and NIC+"
P19-1652,P15-2017,0,0.022324,"baseline model that takes visual features as input and employs a sinEffectiveness of generating novel captions Novel caption generation is crucial for automatic image captioning because retrieval-based models that simply retrieve existing captions from the training set often produce less human results though they can achieve high scores in terms of auFigure 5: Mean CIDEr-D scores with standard derivation of |Wi |= 64 versus |Wi |= |V |, for NIC+WC+WA+RL (3 seeds) on the validation set. Xaxis: the number of training iterations (2×104 ), Y-axis: CIDEr-D scores. 6520 tomatic evaluation metrics (Devlin et al., 2015b). The worst case of N-gram problem is that the model directly generated the same frequent captions in the training set (Devlin et al., 2015a). Thus the capability of generating novel captions for an image that is not seen in the training set indicates that the generator is able to understand a given image better instead of simply generating frequent Ngrams found in the training set. In this experiment, we consider captions generated by models that are not seen in the training set as novel captions. We show the ratio of novel captions generated by different models in Figure 6. Our proposed mo"
P19-1652,C18-1150,1,0.691852,"ord (2015). Visual question answering (Antol et al., 2015; Goyal et al., 2017) aims to provide an answer to a question related to a given image. Existing architectures designed for VQA (Malinowski et al., 2015) utilize an RNN to encode the question, and a CNN to encode the image. Most efforts are made to align the visual and text information for generating the answer. Visual question generation is a relatively new task that generates natural questions about an image (Mostafazadeh et al., 2016). Approaches have been explored to generate diverse questions (Tang et al., 2017; Zhang et al., 2017; Fan et al., 2018a) and questions with a specific property (Fan et al., 2018b). Instead of using high-level visual features extracted from the image for text generation, some researchers explore identifying fine-grained information from the image, i.e. objects and attributes, to guide the process of text generation. Traditionally, template-based approaches are used to compose the caption (Farhadi et al., 2010; Kulkarni et al., 2013; Lin et al., 2015). After that, different attention mechanisms are proposed to align visual information and text for better generation (You et al., 2016; Lu et al., 2017; Anderson e"
P19-1652,N03-1020,0,0.0542235,"Missing"
P19-1652,P16-1170,0,0.0191424,"al., 2016; Lu et al., 2017; Anderson et al., 2018) employ CNN to extract visual features and RNN to generate captions word by word (2015). Visual question answering (Antol et al., 2015; Goyal et al., 2017) aims to provide an answer to a question related to a given image. Existing architectures designed for VQA (Malinowski et al., 2015) utilize an RNN to encode the question, and a CNN to encode the image. Most efforts are made to align the visual and text information for generating the answer. Visual question generation is a relatively new task that generates natural questions about an image (Mostafazadeh et al., 2016). Approaches have been explored to generate diverse questions (Tang et al., 2017; Zhang et al., 2017; Fan et al., 2018a) and questions with a specific property (Fan et al., 2018b). Instead of using high-level visual features extracted from the image for text generation, some researchers explore identifying fine-grained information from the image, i.e. objects and attributes, to guide the process of text generation. Traditionally, template-based approaches are used to compose the caption (Farhadi et al., 2010; Kulkarni et al., 2013; Lin et al., 2015). After that, different attention mechanisms"
P19-1652,P02-1040,0,0.110523,"with the two-layer LSTM. 6518 - NIC+WC uses the hard word constraints (WC). NIC+WC+RL is trained with reinforcement learning using the image-grounded vocabulary as the action space. - NIC+WC+WA employs the soft word-aware (WA) mechanism on top of NIC+WC. NIC+WC+WA+RL is trained with reinforcement learning using the image-grounded vocabulary as the action space. - NIC+WC(GT) utilizes the ground-truth vocabufi as the word constraints instead of Wi . lary W This is an oracle. 3.4 Overall Performance We report scores of several widely used metrics for image captioning evaluation, including BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr-D (Vedantam et al., 2015). The overall performance is shown in Table 1. Several findings stand out: - Both NIC+WC and NIC+WC+RL perform better than their counter-part models NIC and NIC+RL across all metrics. This shows the effectiveness of using the word constraint mechanism for reducing irrelevant words for a given image. - Both NIC+WC+WA and NIC+WC+WA+RL outperform NIC+WC and NIC+WC+RL respectively. This shows that the word-aware mechanism effectively guides the generator to better capturing the semantics of a given i"
P19-1652,D14-1162,0,0.0831222,"for the training, validation and test sets is 29,000, 1,000 and 1,000, respectively. Each image contains 5 human annotated captions. We split the dataset following the process described in (Karpathy and Fei-Fei, 2015). 3.2 Implementation Details For image representation, we rescale the image to 224 × 224 and use ResNet-152 (He et al., 2016) pre-trained on ImageNet (Russakovsky et al., 2015) to extract features of dimension 2,048. The mini-batch size is 64. The dimensions of LSTM hidden unit and the word embedding are 512 and 300, respectively, and the word embedding is initialized with GloVe (Pennington et al., 2014)2 which is pretrained on Wikipedia 2014 and Gigaword 5. We prune the vocabulary by dropping words appear less than five times. For the generator, We train the model with cross-entropy using Adam (Kingma and Ba, 2014) with an initial learning rate 1 × 10−3 which decreases by a factor of 0.8 every 2 × 104 iterations. Then we train the generator with reinforcement learning but without 2 http://nlp.stanford.edu/data/glove. 6B.zip hard constraints using Adam with an initial learning rate 5 × 10−5 which decreases by a factor of 0.8 every 3 × 104 iterations. Finally, we train the generator with reinf"
P19-1652,D17-1090,0,0.0241213,"and RNN to generate captions word by word (2015). Visual question answering (Antol et al., 2015; Goyal et al., 2017) aims to provide an answer to a question related to a given image. Existing architectures designed for VQA (Malinowski et al., 2015) utilize an RNN to encode the question, and a CNN to encode the image. Most efforts are made to align the visual and text information for generating the answer. Visual question generation is a relatively new task that generates natural questions about an image (Mostafazadeh et al., 2016). Approaches have been explored to generate diverse questions (Tang et al., 2017; Zhang et al., 2017; Fan et al., 2018a) and questions with a specific property (Fan et al., 2018b). Instead of using high-level visual features extracted from the image for text generation, some researchers explore identifying fine-grained information from the image, i.e. objects and attributes, to guide the process of text generation. Traditionally, template-based approaches are used to compose the caption (Farhadi et al., 2010; Kulkarni et al., 2013; Lin et al., 2015). After that, different attention mechanisms are proposed to align visual information and text for better generation (You et"
W16-2820,W13-4008,0,0.0520225,"Missing"
W16-2820,W10-0214,0,0.0388071,"nt features for the prediction of highly voted comments in terms of delta score and karma score respectively. Although they considered some sorts of argumentation related features, such features are merely based on lexical similarity, without modeling persuasion behaviors. With the popularity of the online debating forum such as idebate1 , convinceme2 and createdebate3 , researchers have been paying increasing attention to analyze debating content, including identification of arguing expressions in online debate (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, there is still little research about quality evaluation of debating content. Tan et al. (2016) and Wei and Liu (2016) studied the persuasiveness of comments in sub-reddit change my view of Reddit.com. They evaluated In this paper, we focus on a particular action in the online debating forum, i.e., disputation. Within debate, disputation happens when a user disagrees with a specific comment. Figure 1 gives a disputation example from the online debating forum createdebate. It presents an origin"
W16-2820,W14-1305,0,0.0195494,"c is “Should the Gorilla have died?”) 1 Introduction the effectiveness of different features for the prediction of highly voted comments in terms of delta score and karma score respectively. Although they considered some sorts of argumentation related features, such features are merely based on lexical similarity, without modeling persuasion behaviors. With the popularity of the online debating forum such as idebate1 , convinceme2 and createdebate3 , researchers have been paying increasing attention to analyze debating content, including identification of arguing expressions in online debate (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, there is still little research about quality evaluation of debating content. Tan et al. (2016) and Wei and Liu (2016) studied the persuasiveness of comments in sub-reddit change my view of Reddit.com. They evaluated In this paper, we focus on a particular action in the online debating forum, i.e., disputation. Within debate, disputation happens when a user disagrees with a specific comment. Figure 1 gives a dis"
W16-2820,P16-2032,1,0.678326,"ity, without modeling persuasion behaviors. With the popularity of the online debating forum such as idebate1 , convinceme2 and createdebate3 , researchers have been paying increasing attention to analyze debating content, including identification of arguing expressions in online debate (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, there is still little research about quality evaluation of debating content. Tan et al. (2016) and Wei and Liu (2016) studied the persuasiveness of comments in sub-reddit change my view of Reddit.com. They evaluated In this paper, we focus on a particular action in the online debating forum, i.e., disputation. Within debate, disputation happens when a user disagrees with a specific comment. Figure 1 gives a disputation example from the online debating forum createdebate. It presents an original argument and an argument disputing it. Our study aims to evaluate the quality of a disputing comment given its original argument and the discussed topic. In order to have a deep understanding of disputation, we analyz"
W16-2820,D14-1083,0,\N,Missing
zhou-etal-2014-cuhk,miltsakaki-etal-2004-penn,0,\N,Missing
zhou-etal-2014-cuhk,D07-1075,0,\N,Missing
zhou-etal-2014-cuhk,W11-0401,0,\N,Missing
zhou-etal-2014-cuhk,J93-2004,0,\N,Missing
zhou-etal-2014-cuhk,D11-1015,1,\N,Missing
zhou-etal-2014-cuhk,D09-1018,0,\N,Missing
zhou-etal-2014-cuhk,mladova-etal-2008-sentence,0,\N,Missing
zhou-etal-2014-cuhk,W05-0312,0,\N,Missing
zhou-etal-2014-cuhk,H05-1033,0,\N,Missing
zhou-etal-2014-cuhk,al-saif-markert-2010-leeds,0,\N,Missing
zhou-etal-2014-cuhk,W01-1605,0,\N,Missing
zhou-etal-2014-cuhk,W09-3006,0,\N,Missing
zhou-etal-2014-cuhk,N03-1030,0,\N,Missing
zhou-etal-2014-cuhk,P09-1075,0,\N,Missing
zhou-etal-2014-cuhk,prasad-etal-2008-penn,0,\N,Missing
zhou-etal-2014-cuhk,I08-7009,0,\N,Missing
zhou-etal-2014-cuhk,P12-1008,0,\N,Missing
zhou-etal-2014-cuhk,I11-1170,0,\N,Missing
zhou-etal-2014-cuhk,I08-7010,0,\N,Missing
zhou-etal-2014-cuhk,W10-1844,0,\N,Missing
zhou-etal-2014-cuhk,C12-2138,1,\N,Missing
zhou-etal-2014-cuhk,D12-1084,0,\N,Missing
